File: service/src/java/org/apache/hive/service/servlet/OTELExporter.java
Patch:
@@ -34,6 +34,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.hive.common.OTELJavaMetrics;
 import org.apache.hadoop.hive.ql.QueryDisplay;
 import org.apache.hadoop.hive.ql.QueryInfo;
@@ -86,8 +87,8 @@ public void exposeMetricsToOTEL() {
 
     LOG.debug("Found {} liveQueries and {} historicalQueries", liveQueries.size(), historicalQueries.size());
 
-    for (QueryInfo lQuery: liveQueries){
-      if(lQuery.getQueryDisplay() == null){
+    for (QueryInfo lQuery : liveQueries) {
+      if (lQuery.getQueryDisplay() == null || StringUtils.isEmpty(lQuery.getQueryDisplay().getQueryId())) {
         continue;
       }
       String queryID = lQuery.getQueryDisplay().getQueryId();

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/utils/TestMetaStoreServerUtils.java
Patch:
@@ -964,6 +964,8 @@ public void testConversionToSignificantNumericTypes() {
     assertEquals("-1.01", MetaStoreServerUtils.getNormalisedPartitionValue("-0001.010000", "double"));
     assertEquals("1.01", MetaStoreServerUtils.getNormalisedPartitionValue("0001.0100", "decimal"));
     assertEquals("-1.01", MetaStoreServerUtils.getNormalisedPartitionValue("-0001.0100", "decimal"));
+    assertEquals("__HIVE_DEFAULT_PARTITION__", MetaStoreServerUtils.getNormalisedPartitionValue(
+        "__HIVE_DEFAULT_PARTITION__", "decimal"));
   }
 
   @Test

File: serde/src/java/org/apache/hadoop/hive/serde2/JsonSerDe.java
Patch:
@@ -139,7 +139,6 @@ private void initialize(final Configuration conf, final Properties tbl,
     this.jsonReader = new HiveJsonReader(this.soi, tsParser);
     this.jsonWriter = new HiveJsonWriter(this.binaryEncoding, getColumnNames());
 
-    this.jsonReader.setBinaryEncoding(binaryEncoding);
     this.jsonReader.enable(HiveJsonReader.Feature.COL_INDEX_PARSING);
 
     if (writeablePrimitivesDeserialize) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -42,6 +42,7 @@
 import java.util.stream.Collectors;
 
 import com.google.common.annotations.VisibleForTesting;
+import org.apache.commons.lang3.StringUtils;
 import org.apache.commons.lang3.ArrayUtils;
 import org.apache.commons.lang3.tuple.ImmutablePair;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedInputFormatInterface;
@@ -1830,7 +1831,8 @@ private ImmutablePair<Boolean, Boolean> validateInputFormatAndSchemaEvolution(Ma
         // (e.g. Avro provides the table schema and ignores the partition schema..).
         //
         String nextDataColumnsString = ObjectInspectorUtils.getFieldNames(partObjectInspector);
-        String[] nextDataColumns = nextDataColumnsString.split(",");
+        String[] nextDataColumns = StringUtils.isBlank(nextDataColumnsString) ?
+            new String[0] : nextDataColumnsString.split(",");
         List<String> nextDataColumnList = Arrays.asList(nextDataColumns);
 
         /*

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ImportTableDesc.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.hooks.ReadEntity;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.parse.HiveTableName;
 import org.apache.hadoop.hive.ql.parse.ReplicationSpec;
@@ -175,7 +176,7 @@ public TableType tableType() {
     return TableType.MANAGED_TABLE;
   }
 
-  public Table toTable(HiveConf conf) throws Exception {
+  public Table toTable(HiveConf conf) throws HiveException {
     return createTblDesc.toTable(conf);
   }
 

File: llap-common/src/gen/protobuf/gen-java/org/apache/hadoop/hive/llap/daemon/rpc/LlapDaemonProtocolProtos.java
Patch:
@@ -1,6 +1,7 @@
 // Generated by the protocol buffer compiler.  DO NOT EDIT!
 // source: LlapDaemonProtocol.proto
 
+// Protobuf Java Version: 3.25.5
 package org.apache.hadoop.hive.llap.daemon.rpc;
 
 public final class LlapDaemonProtocolProtos {

File: llap-common/src/gen/protobuf/gen-java/org/apache/hadoop/hive/llap/plugin/rpc/LlapPluginProtocolProtos.java
Patch:
@@ -1,6 +1,7 @@
 // Generated by the protocol buffer compiler.  DO NOT EDIT!
 // source: LlapPluginProtocol.proto
 
+// Protobuf Java Version: 3.25.5
 package org.apache.hadoop.hive.llap.plugin.rpc;
 
 public final class LlapPluginProtocolProtos {

File: ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/hooks/proto/HiveHookEvents.java
Patch:
@@ -1,6 +1,7 @@
 // Generated by the protocol buffer compiler.  DO NOT EDIT!
 // source: HiveEvents.proto
 
+// Protobuf Java Version: 3.25.5
 package org.apache.hadoop.hive.ql.hooks.proto;
 
 public final class HiveHookEvents {

File: ql/src/gen/protobuf/gen-test/org/apache/hadoop/hive/ql/io/protobuf/SampleProtos.java
Patch:
@@ -1,6 +1,7 @@
 // Generated by the protocol buffer compiler.  DO NOT EDIT!
 // source: SampleProtos.proto
 
+// Protobuf Java Version: 3.25.5
 package org.apache.hadoop.hive.ql.io.protobuf;
 
 public final class SampleProtos {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java
Patch:
@@ -34,7 +34,7 @@ public enum UnsupportedFeature {
     Having_clause_without_any_groupby, Invalid_column_reference, Invalid_decimal,
     Less_than_equal_greater_than, Others, Same_name_in_multiple_expressions,
     Schema_less_table, Select_alias_in_having_clause, Select_transform, Subquery,
-    Table_sample_clauses, UDTF, Union_type, Unique_join,
+    Table_sample_clauses, UDTF, Unique_join,
     HighPrecisionTimestamp // CALCITE-1690
   };
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCExpandExpressionsRule.java
Patch:
@@ -35,7 +35,6 @@
 import org.apache.calcite.rex.RexUtil;
 import org.apache.calcite.sql.SqlKind;
 import org.apache.calcite.sql.fun.SqlStdOperatorTable;
-import org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil;
 import org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -187,7 +186,7 @@ public RexNode visitCall(RexCall inputCall) {
             break;
         }
       }
-      return node;
+      return RexUtil.isFlat(node) ? node : RexUtil.flatten(rexBuilder, node);
     }
 
     private RexNode transformIntoOrAndClause(RexBuilder rexBuilder, RexCall expression) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HivePartitionPruneRule.java
Patch:
@@ -50,6 +50,9 @@ protected void perform(RelOptRuleCall call, Filter filter,
       HiveTableScan tScan) {
     // Original table
     RelOptHiveTable hiveTable = (RelOptHiveTable) tScan.getTable();
+    if (hiveTable.getHiveTableMD().isMaterializedTable()) {
+      return;
+    }
 
     // Copy original table scan and table
     HiveTableScan tScanCopy = tScan.copyIncludingTable(tScan.getRowType());

File: common/src/java/org/apache/hadoop/hive/conf/Constants.java
Patch:
@@ -99,7 +99,8 @@ public class Constants {
   public static final String ORC_INPUT_FORMAT = "org.apache.hadoop.hive.ql.io.orc.OrcInputFormat";
   public static final String ORC_OUTPUT_FORMAT = "org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat";
   
-  public static final Pattern COMPACTION_POOLS_PATTERN = Pattern.compile("hive\\.compactor\\.worker\\.(.*)\\.threads");
+  public static final Pattern COMPACTION_POOLS_PATTERN = Pattern.compile("^hive\\.compactor\\.worker\\.(.+)\\.threads$");
+  public static final String COMPACTION_DEFAULT_POOL = "default";
   public static final String HIVE_COMPACTOR_WORKER_POOL = "hive.compactor.worker.pool";
 
   public static final String HTTP_HEADER_REQUEST_TRACK = "X-Request-ID";

File: common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -491,6 +491,7 @@ public enum ErrorMsg {
   UNEXPECTED_PARTITION_TRANSFORM_SPEC(10437, "Partition transforms are only supported by Iceberg storage handler", true),
   NONICEBERG_COMPACTION_WITH_FILTER_NOT_SUPPORTED(10440, "Compaction with filter is not allowed on non-Iceberg table {0}.{1}", true),
   ICEBERG_COMPACTION_WITH_PART_SPEC_AND_FILTER_NOT_SUPPORTED(10441, "Compaction command with both partition spec and filter is not supported on Iceberg table {0}.{1}", true),
+  COMPACTION_THREAD_INITIALIZATION(10442, "Compaction thread failed during initialization", false),
 
   //========================== 20000 range starts here ========================//
 

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveShell.java
Patch:
@@ -24,6 +24,7 @@
 import java.util.stream.Collectors;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;
 import org.apache.hadoop.metrics2.lib.DefaultMetricsSystem;
@@ -227,6 +228,7 @@ private HiveConf initializeConf() {
     // set lifecycle hooks
     hiveConf.setVar(HiveConf.ConfVars.HIVE_QUERY_LIFETIME_HOOKS, HiveIcebergQueryLifeTimeHook.class.getName());
 
+    MetastoreConf.setBoolVar(hiveConf, MetastoreConf.ConfVars.TRY_DIRECT_SQL, true);
     return hiveConf;
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/Compiler.java
Patch:
@@ -113,7 +113,7 @@ public QueryPlan compile(String rawCommand, boolean deferClose) throws CommandPr
       plan = createPlan(sem);
       
       if (HiveOperation.START_TRANSACTION == queryState.getHiveOperation()
-          || plan.hasAcidResources()) {
+          || plan.isRequiresOpenTransaction()) {
         openTxnAndGetValidTxnList();
       }
       verifyTxnState();

File: ql/src/java/org/apache/hadoop/hive/ql/Executor.java
Patch:
@@ -219,7 +219,7 @@ private void preExecutionCacheActions() throws Exception {
     if (driverContext.getCacheUsage().getStatus() == CacheUsage.CacheStatus.CAN_CACHE_QUERY_RESULTS &&
         driverContext.getPlan().getFetchTask() != null) {
       ValidTxnWriteIdList txnWriteIdList = null;
-      if (driverContext.getPlan().hasAcidReadWrite()) {
+      if (driverContext.getPlan().hasAcidResourcesInQuery()) {
         txnWriteIdList = AcidUtils.getValidTxnWriteIdList(driverContext.getConf());
       }
       // The results of this query execution might be cacheable.

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/database/drop/DropDatabaseAnalyzer.java
Patch:
@@ -116,7 +116,7 @@ public void analyzeInternal(ASTNode root) throws SemanticException {
   }
 
   @Override
-  public boolean hasAcidResources() {
+  public boolean isRequiresOpenTransaction() {
     // check DB tags once supported (i.e. ICEBERG_ONLY, ACID_ONLY, EXTERNAL_ONLY)
     return true;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/storage/compact/AlterTableCompactAnalyzer.java
Patch:
@@ -27,7 +27,6 @@
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.QueryState;
-import org.apache.hadoop.hive.ql.ddl.DDLDesc.DDLDescWithWriteId;
 import org.apache.hadoop.hive.ql.ddl.DDLWork;
 import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;
 import org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableAnalyzer;
@@ -120,7 +119,7 @@ protected void analyzeCommand(TableName tableName, Map<String, String> partition
   }
   
   @Override
-  protected void setAcidDdlDesc(DDLDescWithWriteId desc) {
-    // doesn't need an open txn
+  public boolean isRequiresOpenTransaction() {
+    return false; // doesn't need an open txn
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -1571,12 +1571,12 @@ public Set<FileSinkDesc> getAcidFileSinks() {
     return acidFileSinks;
   }
 
-  public boolean hasAcidReadWrite() {
+  public boolean hasTransactionalInQuery() {
     return transactionalInQuery;
   }
 
-  public boolean hasAcidResources() {
-    return hasAcidReadWrite() || getAcidDdlDesc() != null ||
+  public boolean isRequiresOpenTransaction() {
+    return hasTransactionalInQuery() || getAcidDdlDesc() != null ||
       Stream.of(getInputs(), getOutputs()).flatMap(Collection::stream)
         .filter(entity -> entity.getType() == Entity.Type.TABLE || entity.getType() == Entity.Type.PARTITION)
         .flatMap(entity -> {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ExecuteStatementAnalyzer.java
Patch:
@@ -225,7 +225,7 @@ public void analyzeInternal(ASTNode root) throws SemanticException {
 
       this.queryProperties = cachedPlan.getQueryProperties();
       this.setAutoCommitValue(cachedPlan.getAutoCommitValue());
-      this.transactionalInQuery = cachedPlan.hasAcidReadWrite();
+      this.transactionalInQuery = cachedPlan.hasTransactionalInQuery();
       this.acidFileSinks.addAll(cachedPlan.getAcidFileSinks());
       this.initCtx(cachedPlan.getCtx());
       this.ctx.setCboInfo(cachedPlan.getCboInfo());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java
Patch:
@@ -129,7 +129,7 @@ static Task<ExportWork> analyzeExport(ASTNode ast, @Nullable String acidTableNam
   }
   
   @Override
-  public boolean hasAcidReadWrite() {
+  public boolean hasTransactionalInQuery() {
     return isMmExport; // Full ACID export goes through UpdateDelete analyzer.
   }
 }

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnExIm.java
Patch:
@@ -345,11 +345,11 @@ private void testImport(boolean isVectorized, boolean existingTarget) throws Exc
     TestTxnCommands2.runWorker(hiveConf);
     String[][] expected3 = new String[][] {
         {"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t1\t2",
-            ".*t/delta_0000001_0000002_v000001[5-6]/bucket_00000"},
+            ".*t/delta_0000001_0000002_v000001[4-5]/bucket_00000"},
         {"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":1}\t3\t4",
-            ".*t/delta_0000001_0000002_v000001[5-6]/bucket_00000"},
+            ".*t/delta_0000001_0000002_v000001[4-5]/bucket_00000"},
         {"{\"writeid\":2,\"bucketid\":536870913,\"rowid\":0}\t0\t6",
-            ".*t/delta_0000001_0000002_v000001[5-6]/bucket_00000"}};
+            ".*t/delta_0000001_0000002_v000001[4-5]/bucket_00000"}};
     checkResult(expected3, testQuery, isVectorized, "minor compact imported table");
 
   }

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java
Patch:
@@ -888,7 +888,6 @@ public void testLockingExternalInStrictModeInsert() throws Exception {
 
     conf.setBoolVar(HiveConf.ConfVars.HIVE_TXN_EXT_LOCKING_ENABLED, true);
     HiveTxnManager txnMgr = TxnManagerFactory.getTxnManagerFactory().getTxnManager(conf);
-    txnMgr.openTxn(ctx, "T1");
     driver.compileAndRespond("insert into tab_not_acid partition(np='blah') values(7,8)", true);
     ((DbTxnManager)txnMgr).acquireLocks(driver.getPlan(), ctx, "T1", false);
     List<ShowLocksResponseElement> locks = getLocks(txnMgr);

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java
Patch:
@@ -3384,7 +3384,7 @@ long updateTableParam(Table table, String key, String expectedValue, String newV
     String statement = TxnUtils.createUpdatePreparedStmt(
         "\"TABLE_PARAMS\"",
         ImmutableList.of("\"PARAM_VALUE\""),
-        ImmutableList.of("\"TBL_ID\"", "\"PARAM_KEY\"", "\"PARAM_VALUE\""));
+        ImmutableList.of("\"TBL_ID\"", "\"PARAM_KEY\"", dbType.toVarChar("\"PARAM_VALUE\"")));
     Query query = pm.newQuery("javax.jdo.query.SQL", statement);
     return (long) query.executeWithArray(newValue, table.getId(), key, expectedValue);
   }

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java
Patch:
@@ -2087,7 +2087,7 @@ public List<Partition> getPartitionsByExpr(org.apache.hadoop.hive.ql.metadata.Ta
       tasks.forEach(task -> {
         DataFile file = task.file();
         PartitionSpec spec = task.spec();
-        if ((latestSpecOnly && file.specId() == tableSpecId) || (!latestSpecOnly && file.specId() != tableSpecId)) {
+        if (latestSpecOnly && file.specId() == tableSpecId || !latestSpecOnly && file.specId() != tableSpecId) {
           PartitionData partitionData = IcebergTableUtil.toPartitionData(task.partition(), spec.partitionType());
           String partName = spec.partitionToPath(partitionData);
           Map<String, String> partSpecMap = Maps.newLinkedHashMap();

File: iceberg/patched-iceberg-core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java
Patch:
@@ -278,7 +278,7 @@ void cleanAllTooOldDirtyCommit(FileSystem fs, int previousVersionsMax) throws IO
     for (FileStatus file : files) {
       long modificationTime = file.getModificationTime();
       Path path = file.getPath();
-      if ((currentMaxVersion - version(path.getName()) > previousVersionsMax) &&
+      if (currentMaxVersion - version(path.getName()) > previousVersionsMax &&
             (now - modificationTime) > ttl) {
         dirtyCommits.add(path);
       }

File: iceberg/iceberg-catalog/src/main/java/org/apache/iceberg/hive/HiveClientPool.java
Patch:
@@ -88,8 +88,8 @@ protected IMetaStoreClient reconnect(IMetaStoreClient client) {
 
   @Override
   protected boolean isConnectionException(Exception e) {
-    return super.isConnectionException(e) || (e != null && e instanceof MetaException &&
-        e.getMessage().contains("Got exception: org.apache.thrift.transport.TTransportException"));
+    return super.isConnectionException(e) || e != null && e instanceof MetaException &&
+        e.getMessage().contains("Got exception: org.apache.thrift.transport.TTransportException");
   }
 
   @Override

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java
Patch:
@@ -1019,8 +1019,8 @@ private void setOrcOnlyFilesParam(org.apache.hadoop.hive.metastore.api.Table hms
 
   private boolean isOrcOnlyFiles(org.apache.hadoop.hive.metastore.api.Table hmsTable) {
     return !"FALSE".equalsIgnoreCase(hmsTable.getParameters().get(ORC_FILES_ONLY)) &&
-        ((hmsTable.getSd().getInputFormat() != null &&
-            hmsTable.getSd().getInputFormat().toUpperCase().contains(org.apache.iceberg.FileFormat.ORC.name())) ||
+        (hmsTable.getSd().getInputFormat() != null &&
+            hmsTable.getSd().getInputFormat().toUpperCase().contains(org.apache.iceberg.FileFormat.ORC.name()) ||
             org.apache.iceberg.FileFormat.ORC.name()
                 .equalsIgnoreCase(hmsTable.getSd().getSerdeInfo().getParameters().get("write.format.default")) ||
             org.apache.iceberg.FileFormat.ORC.name()

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergSelects.java
Patch:
@@ -118,7 +118,7 @@ public void testCBOWithSelfJoin() throws IOException {
   public void testJoinTablesSupportedTypes() throws IOException {
     for (int i = 0; i < SUPPORTED_TYPES.size(); i++) {
       Type type = SUPPORTED_TYPES.get(i);
-      if ((type == Types.TimestampType.withZone()) && isVectorized && fileFormat == FileFormat.ORC) {
+      if (type == Types.TimestampType.withZone() && isVectorized && fileFormat == FileFormat.ORC) {
         // ORC/TIMESTAMP_INSTANT is not supported vectorized types for Hive
         continue;
       }
@@ -145,7 +145,7 @@ public void testJoinTablesSupportedTypes() throws IOException {
   public void testSelectDistinctFromTable() throws IOException {
     for (int i = 0; i < SUPPORTED_TYPES.size(); i++) {
       Type type = SUPPORTED_TYPES.get(i);
-      if ((type == Types.TimestampType.withZone()) &&
+      if (type == Types.TimestampType.withZone() &&
           isVectorized && fileFormat == FileFormat.ORC) {
         // ORC/TIMESTAMP_INSTANT is not supported vectorized types for Hive
         continue;

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/storage/concatenate/AlterTableConcatenateOperation.java
Patch:
@@ -76,7 +76,9 @@ private MergeFileWork getMergeFileWork(CompilationOpContext opContext) throws Hi
 
     Map<Path, List<String>> pathToAliases = new LinkedHashMap<>();
     List<String> inputDirStr = Lists.newArrayList(inputDirList.toString());
-    pathToAliases.put(desc.getInputDir(), inputDirStr);
+    for (Path path: mergeWork.getInputPaths()) {
+      pathToAliases.put(path, inputDirStr);
+    }
     mergeWork.setPathToAliases(pathToAliases);
 
     FileMergeDesc fmd = getFileMergeDesc();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ExprNodeConverter.java
Patch:
@@ -361,6 +361,9 @@ public static ExprNodeConstantDesc toExprNodeConstantDesc(RexLiteral literal) {
             "char values must use NlsString for correctness");
         int precision = lType.getPrecision();
         HiveChar value = new HiveChar((String) literal.getValue3(), precision);
+        if (value.getCharacterLength() == 0) {
+          return new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, null);
+        }
         return new ExprNodeConstantDesc(new CharTypeInfo(precision), value);
       }
       case VARCHAR: {

File: hplsql/src/main/java/org/apache/hive/hplsql/Utils.java
Patch:
@@ -213,8 +213,9 @@ public static String convertSqlDatetimeFormat(String in) {
     int len = in.length();
     int i = 0;
     while (i < len) {
-      if (i + 4 <= len && in.substring(i, i + 4).compareTo("YYYY") == 0) {
-        out.append("yyyy");
+      if (i + 4 <= len && (in.substring(i, i + 4).compareTo("YYYY") == 0 || in.substring(i, i + 4)
+          .compareTo("yyyy") == 0)) {
+        out.append("uuuu");
         i += 4;
       }
       else if (i + 2 <= len && in.substring(i, i + 2).compareTo("mm") == 0) {

File: parser/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java
Patch:
@@ -78,7 +78,7 @@ public Object dupNode(Object t) {
 
     @Override
     public Object dupTree(Object t, Object parent) {
-      // Overriden to copy start index / end index, that is needed through optimization,
+      // Overridden to copy start index / end index, that is needed through optimization,
       // e.g., for masking/filtering
       ASTNode astNode = (ASTNode) t;
       ASTNode astNodeCopy = (ASTNode) super.dupTree(t, parent);

File: parser/src/test/org/apache/hadoop/hive/ql/parse/HqlParser.java
Patch:
@@ -91,7 +91,7 @@ public Object dupNode(Object t) {
 
     @Override
     public Object dupTree(Object t, Object parent) {
-      // Overriden to copy start index / end index, that is needed through
+      // Overridden to copy start index / end index, that is needed through
       // optimization, e.g., for masking/filtering
       ASTNode astNode = (ASTNode) t;
       ASTNode astNodeCopy = (ASTNode) super.dupTree(t, parent);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
Patch:
@@ -685,7 +685,7 @@ public void close(boolean abort) throws HiveException {
     // call the operator specific close routine
     closeOp(abort);
 
-    // closeOp can be overriden
+    // closeOp can be Overridden
     if (conf != null && conf.getRuntimeStatsTmpDir() != null) {
       publishRunTimeStats();
     }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEvaluator.java
Patch:
@@ -147,7 +147,7 @@ public void configure(MapredContext mapredContext) {
    *         compilation time,
    */
   public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException {
-    // This function should be overriden in every sub class
+    // This function should be Overridden in every sub class
     // And the sub class should call super.init(m, parameters) to get mode set.
     mode = m;
     partitionEvaluator = null;

File: ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java
Patch:
@@ -77,7 +77,7 @@ public void testResolvingDriverAlias() throws Exception {
 
     conf.setLongVar(HiveConf.ConfVars.HIVE_SMALL_TABLES_FILESIZE, 65536);
 
-    // alias1, alias2, alias3 all can be selected but overriden by biggest one (alias3)
+    // alias1, alias2, alias3 all can be selected but Overridden by biggest one (alias3)
     resolved = resolver.resolveMapJoinTask(ctx, conf);
     Assert.assertEquals("alias3", resolved.getId());
 

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/IcebergTableUtil.java
Patch:
@@ -327,8 +327,8 @@ public static void cherryPick(Table table, long snapshotId) {
   }
 
   public static boolean isV2Table(Map<String, String> props) {
-    return props != null &&
-        "2".equals(props.get(TableProperties.FORMAT_VERSION));
+    return props == null ||
+        "2".equals(props.get(TableProperties.FORMAT_VERSION)) || props.get(TableProperties.FORMAT_VERSION) == null;
   }
 
   public static boolean isCopyOnWriteMode(Context.Operation operation, BinaryOperator<String> props) {

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java
Patch:
@@ -310,7 +310,9 @@ public long evictEntity(LlapDaemonProtocolProtos.EvictEntityRequestProto protoRe
 
     long markedBytes = dataCache.markBuffersForProactiveEviction(predicate, isInstantDeallocation);
     markedBytes += fileMetadataCache.markBuffersForProactiveEviction(predicate, isInstantDeallocation);
-    markedBytes += serdeCache.markBuffersForProactiveEviction(predicate, isInstantDeallocation);
+    if (serdeCache != null) {
+      markedBytes += serdeCache.markBuffersForProactiveEviction(predicate, isInstantDeallocation);
+    }
 
     // Signal mark phase of proactive eviction was done
     if (markedBytes > 0) {

File: itests/qtest/src/test/java/org/apache/hadoop/hive/cli/TestNegativeLlapCliDriver.java
Patch:
@@ -33,7 +33,7 @@
 @RunWith(Parameterized.class)
 public class TestNegativeLlapCliDriver {
 
-  static CliAdapter adapter = new CliConfigs.NegativeLlapCliDriver().getCliAdapter();
+  static CliAdapter adapter = new CliConfigs.NegativeLlapCliConfig().getCliAdapter();
 
   @Parameters(name = "{0}")
   public static List<Object[]> getParameters() throws Exception {

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliConfigs.java
Patch:
@@ -361,8 +361,8 @@ public NegativeLlapLocalCliConfig() {
     }
   }
 
-  public static class NegativeLlapCliDriver extends AbstractCliConfig {
-    public NegativeLlapCliDriver() {
+  public static class NegativeLlapCliConfig extends AbstractCliConfig {
+    public NegativeLlapCliConfig() {
       super(CoreNegativeCliDriver.class);
       try {
         setQueryDir("ql/src/test/queries/clientnegative");

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyStruct.java
Patch:
@@ -300,7 +300,7 @@ public void parseMultiDelimit(byte[] rawRow, byte[] fieldDelimit) {
     // first field always starts from 0, even when missing
     startPosition[0] = 0;
     for (int i = 1; i <= fields.length; i++) {
-      if (fields.length > 1 && delimitIndexes[i - 1] != -1) {
+      if (delimitIndexes[i - 1] != -1) {
         int start = delimitIndexes[i - 1] + fieldDelimit.length;
         startPosition[i] = start - i * diff;
       } else {
@@ -313,7 +313,7 @@ public void parseMultiDelimit(byte[] rawRow, byte[] fieldDelimit) {
 
   // find all the indexes of the sub byte[]
   private int[] findIndexes(byte[] array, byte[] target) {
-    if (fields.length <= 1) {
+    if (fields.length < 1) {
       return new int[0];
     }
     int[] indexes = new int[fields.length];

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2575,7 +2575,7 @@ public static enum ConfVars {
         "and the original filter is removed. If this config is false, the original filter \n" +
         "is also left in the operator tree at the original place."),
     HIVE_JOIN_DISJ_TRANSITIVE_PREDICATES_PUSHDOWN("hive.optimize.join.disjunctive.transitive.predicates.pushdown",
-        true, "Whether to transitively infer disjunctive predicates across joins. \n"
+        false, "Whether to transitively infer disjunctive predicates across joins. \n"
         + "Disjunctive predicates are hard to simplify and pushing them down might lead to infinite rule matching "
         + "causing stackoverflow and OOM errors"),
     HIVE_POINT_LOOKUP_OPTIMIZER("hive.optimize.point.lookup", true,

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInternalRecordWrapper.java
Patch:
@@ -142,7 +142,7 @@ private Map<String, Integer> buildFieldPositionMap(StructType schema) {
   private static Function<Object, Object> converter(Type type) {
     switch (type.typeId()) {
       case TIMESTAMP:
-        return timestamp -> DateTimeUtil.timestamptzFromMicros((Long) timestamp);
+        return timestamp -> DateTimeUtil.timestampFromMicros((Long) timestamp);
       case DATE:
         return date -> DateTimeUtil.dateFromDays((Integer) date);
       case STRUCT:

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/StageIDsRearranger.java
Patch:
@@ -48,7 +48,7 @@ public PhysicalContext resolve(PhysicalContext pctx) throws SemanticException {
     for (Task task : getExplainOrder(pctx)) {
       task.setId(PREFIX + (++counter));
     }
-    return null;
+    return pctx;
   }
 
   private static List<Task> getExplainOrder(PhysicalContext pctx) {

File: cli/src/test/org/apache/hadoop/hive/cli/TestCliSessionState.java
Patch:
@@ -19,7 +19,7 @@
 
 import static org.junit.Assert.assertEquals;
 
-import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.Warehouse;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.junit.Test;
@@ -33,8 +33,8 @@ public class TestCliSessionState {
    * test default db name
    */
   @Test
-  public void testgetDbName() throws Exception {
-    SessionState.start(new HiveConf());
+  public void testGetDbName() throws Exception {
+    SessionState.start(new HiveConfForTest(getClass()));
     assertEquals(Warehouse.DEFAULT_DATABASE_NAME,
         SessionState.get().getCurrentDatabase());
   }

File: hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseQueries.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hbase.MiniHBaseCluster;
 import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.DriverFactory;
 import org.apache.hadoop.hive.ql.IDriver;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
@@ -52,7 +53,7 @@ public class TestHBaseQueries {
    * databases, etc.), otherwise those will be visible for subsequent test methods too.
    */
   public TestHBaseQueries() throws Exception {
-    baseConf = new HiveConf(HBaseConfiguration.create(), TestHBaseQueries.class);
+    baseConf = new HiveConfForTest(HBaseConfiguration.create(), TestHBaseQueries.class);
     baseConf.set(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER.varname, SQLStdHiveAuthorizerFactory.class.getName());
 
     // set up Zookeeper

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/TestUseDatabase.java
Patch:
@@ -40,8 +40,9 @@ public class TestUseDatabase {
 
   @Before
   public void setUp() throws Exception {
-
     HiveConf hcatConf = new HiveConf(this.getClass());
+    //TODO: HIVE-27998: hcatalog tests on Tez
+    hcatConf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
     hcatConf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     hcatConf.set(ConfVars.PRE_EXEC_HOOKS.varname, "");

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatBaseTest.java
Patch:
@@ -79,6 +79,8 @@ public void setUp() throws Exception {
    */
   protected void setUpHiveConf() {
     hiveConf = new HiveConf(this.getClass());
+    //TODO: HIVE-27998: hcatalog tests on Tez
+    hiveConf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
     Path workDir = new Path(System.getProperty("test.tmp.dir",
         "target" + File.separator + "test" + File.separator + "tmp"));
     hiveConf.set("mapred.local.dir", workDir + File.separator + this.getClass().getSimpleName()

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestE2EScenarios.java
Patch:
@@ -84,6 +84,8 @@ public void setUp() throws Exception {
     }
 
     HiveConf hiveConf = new HiveConf(this.getClass());
+    //TODO: HIVE-27998: hcatalog tests on Tez
+    hiveConf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
     hiveConf.set(HiveConf.ConfVars.PRE_EXEC_HOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.POST_EXEC_HOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderComplexSchema.java
Patch:
@@ -62,9 +62,7 @@
 @RunWith(Parameterized.class)
 public class TestHCatLoaderComplexSchema {
 
-  //private static MiniCluster cluster = MiniCluster.buildCluster();
   private static IDriver driver;
-  //private static Properties props;
   private static final Logger LOG = LoggerFactory.getLogger(TestHCatLoaderComplexSchema.class);
 
   private static final Map<String, Set<String>> DISABLED_STORAGE_FORMATS =
@@ -106,6 +104,8 @@ private void createTable(String tablename, String schema) throws Exception {
   @BeforeClass
   public static void setUpBeforeClass() throws Exception {
     HiveConf hiveConf = new HiveConf(TestHCatLoaderComplexSchema.class);
+    //TODO: HIVE-27998: hcatalog tests on Tez
+    hiveConf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
     Path workDir = new Path(System.getProperty("test.tmp.dir",
         "target" + File.separator + "test" + File.separator + "tmp"));
     hiveConf.set("mapred.local.dir", workDir + File.separator + "TestHCatLoaderComplexSchema"

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderEncryption.java
Patch:
@@ -159,6 +159,8 @@ public void setup() throws Exception {
     }
 
     HiveConf hiveConf = new HiveConf(this.getClass());
+    //TODO: HIVE-27998: hcatalog tests on Tez
+    hiveConf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
     hiveConf.set(HiveConf.ConfVars.PRE_EXEC_HOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.POST_EXEC_HOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatStorerMulti.java
Patch:
@@ -95,6 +95,8 @@ public void setUp() throws Exception {
 
     if (driver == null) {
       HiveConf hiveConf = new HiveConf(this.getClass());
+      //TODO: HIVE-27998: hcatalog tests on Tez
+      hiveConf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
       hiveConf.set(HiveConf.ConfVars.PRE_EXEC_HOOKS.varname, "");
       hiveConf.set(HiveConf.ConfVars.POST_EXEC_HOOKS.varname, "");
       hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");

File: hcatalog/webhcat/java-client/src/test/java/org/apache/hive/hcatalog/api/repl/commands/TestCommands.java
Patch:
@@ -76,6 +76,8 @@ public static void setUpBeforeClass() throws Exception {
 
     TestHCatClient.startMetaStoreServer();
     hconf = TestHCatClient.getConf();
+    //TODO: HIVE-27998: hcatalog tests on Tez
+    hconf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
     hconf.set(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK.varname,"");
     hconf.set(HiveConf.ConfVars.REPL_RUN_DATA_COPY_TASKS_ON_TARGET.varname, "false");
     hconf

File: itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationCleanup.java
Patch:
@@ -65,7 +65,8 @@ public class TestDbNotificationCleanup {
     @BeforeClass
     public static void connectToMetastore() throws Exception {
         conf = new HiveConf();
-
+        //TODO: HIVE-27998: hcatalog tests on Tez
+        conf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
         MetastoreConf.setVar(conf,MetastoreConf.ConfVars.TRANSACTIONAL_EVENT_LISTENERS,
                 "org.apache.hive.hcatalog.listener.DbNotificationListener");
         conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);

File: itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java
Patch:
@@ -269,6 +269,8 @@ public void onBatchAcidWrite(BatchAcidWriteEvent batchAcidWriteEvent) throws Met
   @BeforeClass
   public static void connectToMetastore() throws Exception {
     HiveConf conf = new HiveConf();
+    //TODO: HIVE-27998: hcatalog tests on Tez
+    conf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
     conf.setVar(HiveConf.ConfVars.METASTORE_TRANSACTIONAL_EVENT_LISTENERS,
         DbNotificationListener.class.getName());
     conf.setVar(HiveConf.ConfVars.METASTORE_EVENT_LISTENERS, MockMetaStoreEventListener.class.getName());

File: itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java
Patch:
@@ -70,9 +70,10 @@ public void setup() throws Exception {
     dataDir = new File(System.getProperty("java.io.tmpdir") + File.separator +
         TestSequenceFileReadWrite.class.getCanonicalName() + "-" + System.currentTimeMillis());
     hiveConf = new HiveConf(this.getClass());
+    //TODO: HIVE-27998: hcatalog tests on Tez
+    hiveConf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
     warehouseDir = HCatUtil.makePathASafeFileName(dataDir + File.separator + "warehouse");
     inputFileName = HCatUtil.makePathASafeFileName(dataDir + File.separator + "input.data");
-    hiveConf = new HiveConf(this.getClass());
     hiveConf.set(HiveConf.ConfVars.PRE_EXEC_HOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.POST_EXEC_HOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java
Patch:
@@ -28,6 +28,7 @@
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext;
 import org.apache.hadoop.hive.ql.hooks.HookContext;
 import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
@@ -138,7 +139,7 @@ public void postAnalyze(HiveSemanticAnalyzerHookContext context,
    */
   @BeforeClass
   public static void setUpBeforeClass() throws Exception {
-    HiveConf hiveConf = new HiveConf();
+    HiveConf hiveConf = new HiveConfForTest(TestHs2Hooks.class);
     hiveConf.setVar(ConfVars.PRE_EXEC_HOOKS,
         PreExecHook.class.getName());
     hiveConf.setVar(ConfVars.POST_EXEC_HOOKS,

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreMetrics.java
Patch:
@@ -19,6 +19,7 @@
 
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.metastore.metrics.Metrics;
 import org.apache.hadoop.hive.metastore.metrics.MetricsConstants;
@@ -41,7 +42,7 @@ public class TestMetaStoreMetrics {
 
   @BeforeClass
   public static void before() throws Exception {
-    hiveConf = new HiveConf(TestMetaStoreMetrics.class);
+    hiveConf = new HiveConfForTest(TestMetaStoreMetrics.class);
     hiveConf.setIntVar(HiveConf.ConfVars.METASTORE_THRIFT_CONNECTION_RETRIES, 3);
     hiveConf.setBoolVar(HiveConf.ConfVars.METASTORE_METRICS, true);
     hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/tools/metatool/TestHiveMetaTool.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
@@ -91,7 +92,7 @@ public void setUp() throws Exception {
       os = new ByteArrayOutputStream();
       System.setOut(new PrintStream(os));
 
-      hiveConf = new HiveConf(HiveMetaTool.class);
+      hiveConf = new HiveConfForTest(HiveMetaTool.class);
       client = new HiveMetaStoreClient(hiveConf);
 
       createDatabase();

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestCreateUdfEntities.java
Patch:
@@ -20,6 +20,7 @@
 import static org.junit.Assert.*;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.hooks.Entity;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
 import org.apache.hadoop.hive.ql.reexec.ReExecDriver;
@@ -34,8 +35,7 @@ public class TestCreateUdfEntities {
 
   @Before
   public void setUp() throws Exception {
-
-    HiveConf conf = new HiveConf(IDriver.class);
+    HiveConf conf = new HiveConfForTest(getClass());
     SessionState.start(conf);
     driver = DriverFactory.newDriver(conf);
   }

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestDDLWithRemoteMetastoreSecondNamenode.java
Patch:
@@ -28,9 +28,9 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.MetaStoreTestUtils;
 import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.ql.exec.mr.ExecDriver;
 import org.apache.hadoop.hive.ql.metadata.*;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
@@ -79,7 +79,7 @@ public void setUp() throws Exception {
     }
     tests = new JUnit4TestAdapter(this.getClass()).countTestCases();
     try {
-      conf = new HiveConf(ExecDriver.class);
+      conf = new HiveConfForTest(getClass());
       SessionState.start(conf);
 
       // Test with remote metastore service

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestDatabaseTableDefault.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf.ConfVars;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
@@ -86,7 +87,7 @@ public String toString() {
 
     @Before
     public void setUp() throws Exception {
-        hiveConf = new HiveConf(this.getClass());
+        hiveConf = new HiveConfForTest(getClass());
         HiveConf.setBoolVar(hiveConf, HiveConf.ConfVars.CREATE_TABLES_AS_ACID, true);
         HiveConf.setBoolVar(hiveConf, HiveConf.ConfVars.HIVE_CREATE_TABLES_AS_INSERT_ONLY, true);
         HiveConf.setBoolVar(hiveConf, HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, true);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestLocationQueries.java
Patch:
@@ -130,7 +130,7 @@ public void testAlterTablePartitionLocation_alter5() throws Exception {
     QTestUtil[] qt = new QTestUtil[qfiles.length];
 
     for (int i = 0; i < qfiles.length; i++) {
-      qt[i] = new CheckResults(resDir, logDir, MiniClusterType.NONE, "parta");
+      qt[i] = new CheckResults(resDir, logDir, MiniClusterType.TEZ_LOCAL, "parta");
       qt[i].postInit();
       qt[i].newSession();
       qt[i].setInputFile(qfiles[i]);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestMetaStoreLimitPartitionRequest.java
Patch:
@@ -33,6 +33,7 @@
 import java.util.Set;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.HMSHandler;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hive.jdbc.miniHS2.MiniHS2;
@@ -64,7 +65,7 @@ public class TestMetaStoreLimitPartitionRequest {
   @BeforeClass
   public static void beforeTest() throws Exception {
     Class.forName(MiniHS2.getJdbcDriverName());
-    conf = new HiveConf();
+    conf = new HiveConfForTest(TestMetaStoreLimitPartitionRequest.class);
     DriverManager.setLoginTimeout(0);
 
     conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/metadata/TestAlterTableMetadata.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.ql.metadata;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.api.PrincipalType;
 import org.apache.hadoop.hive.ql.DriverFactory;
 import org.apache.hadoop.hive.ql.IDriver;
@@ -35,7 +36,7 @@ public void testAlterTableOwner() throws HiveException, CommandProcessorExceptio
      * owner metadata of the table in HMS.
      */
 
-    HiveConf conf = new HiveConf(this.getClass());
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     SessionState.start(conf);
     IDriver driver = DriverFactory.newDriver(conf);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java
Patch:
@@ -22,6 +22,7 @@
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.Warehouse;
 import org.apache.hadoop.hive.ql.DriverFactory;
 import org.apache.hadoop.hive.ql.IDriver;
@@ -38,7 +39,7 @@ public class TestSemanticAnalyzerHookLoading {
   @Test
   public void testHookLoading() throws Exception {
 
-    HiveConf conf = new HiveConf(this.getClass());
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.set(ConfVars.SEMANTIC_ANALYZER_HOOK.varname, DummySemanticAnalyzerHook.class.getName());
     conf.set(ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     SessionState.start(conf);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestExportImport.java
Patch:
@@ -18,9 +18,9 @@
 
 package org.apache.hadoop.hive.ql.parse;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.shims.Utils;
 import org.junit.AfterClass;
 import org.junit.Before;
@@ -51,7 +51,7 @@ public class TestExportImport {
 
   @BeforeClass
   public static void classLevelSetup() throws Exception {
-    Configuration conf = new Configuration();
+    HiveConf conf = new HiveConfForTest(TestExportImport.class);
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     conf.set("hive.repl.include.external.tables", "false");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestMetaStoreEventListenerInRepl.java
Patch:
@@ -19,14 +19,13 @@
 
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
-import org.apache.hadoop.hive.metastore.events.AlterDatabaseEvent;
 import org.apache.hadoop.hive.metastore.events.AlterTableEvent;
 import org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent;
 import org.apache.hadoop.hive.metastore.events.CreateTableEvent;
 import org.apache.hadoop.hive.metastore.events.DropTableEvent;
 import org.apache.hadoop.hive.shims.Utils;
-
 import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.rules.TestName;
@@ -62,7 +61,7 @@ public class TestMetaStoreEventListenerInRepl {
 
   @BeforeClass
   public static void internalBeforeClassSetup() throws Exception {
-    TestMetaStoreEventListenerInRepl.conf = new HiveConf(TestMetaStoreEventListenerInRepl.class);
+    TestMetaStoreEventListenerInRepl.conf = new HiveConfForTest(TestMetaStoreEventListenerInRepl.class);
     TestMetaStoreEventListenerInRepl.conf.set("dfs.client.use.datanode.hostname", "true");
     TestMetaStoreEventListenerInRepl.conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     MiniDFSCluster miniDFSCluster =

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestParseUtilsStagingDir.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hdfs.DFSTestUtil;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.session.SessionState;
@@ -61,7 +62,7 @@ public static void beforeClassSetup() throws Exception {
     System.setProperty("jceks.key.serialFilter", "java.lang.Enum;java.security.KeyRep;" +
         "java.security.KeyRep$Type;javax.crypto.spec.SecretKeySpec;" +
         "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata;!*");
-    conf = new HiveConf();
+    conf = new HiveConfForTest(TestParseUtilsStagingDir.class);
     conf.set("hadoop.security.key.provider.path", "jceks://file" + jksFile);
 
     miniDFSCluster =

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplWithJsonMessageFormat.java
Patch:
@@ -25,8 +25,6 @@
 import org.junit.rules.TestRule;
 
 import java.util.ArrayList;
-import java.util.List;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOfHiveStreaming.java
Patch:
@@ -19,6 +19,7 @@
 
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.metastore.messaging.json.gzip.GzipJSONMessageEncoder;
 import org.apache.hadoop.hive.shims.Utils;
@@ -69,7 +70,7 @@ public static void classLevelSetup() throws Exception {
   static void internalBeforeClassSetup(Map<String, String> overrides,
       Class clazz) throws Exception {
 
-    HiveConf conf = new HiveConf(clazz);
+    HiveConf conf = new HiveConfForTest(TestReplicationOfHiveStreaming.class);
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     MiniDFSCluster miniDFSCluster =

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
Patch:
@@ -193,6 +193,8 @@ public static void setUpBeforeClass() throws Exception {
   static void internalBeforeClassSetup(Map<String, String> additionalProperties)
       throws Exception {
     hconf = new HiveConf(TestReplicationScenarios.class);
+    //TODO: HIVE-28044: Replication tests to run on Tez
+    hconf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
     String metastoreUri = System.getProperty("test."+MetastoreConf.ConfVars.THRIFT_URIS.getHiveName());
     if (metastoreUri != null) {
       hconf.set(MetastoreConf.ConfVars.THRIFT_URIS.getHiveName(), metastoreUri);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.common.repl.ReplConst;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.TableType;
 import org.apache.hadoop.hive.metastore.ReplChangeManager;
 import org.apache.hadoop.hive.metastore.Warehouse;
@@ -137,7 +138,7 @@ public static void classLevelSetup() throws Exception {
   static void internalBeforeClassSetup(Map<String, String> overrides,
       Class clazz) throws Exception {
 
-    conf = new HiveConf(clazz);
+    conf = new HiveConfForTest(clazz);
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("metastore.warehouse.tenant.colocation", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosIncrementalLoadAcidTables.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.metastore.messaging.json.gzip.GzipJSONMessageEncoder;
 import org.apache.hadoop.hive.ql.metadata.HiveMetaStoreClientWithLocalCache;
@@ -72,7 +73,7 @@ public static void classLevelSetup() throws Exception {
 
   static void internalBeforeClassSetup(Map<String, String> overrides, Class clazz)
       throws Exception {
-    conf = new HiveConf(clazz);
+    conf = new HiveConfForTest(TestReplicationScenariosIncrementalLoadAcidTables.class);
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     MiniDFSCluster miniDFSCluster =

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenarios.java
Patch:
@@ -20,6 +20,7 @@
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hive.common.StatsSetupConst;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.Warehouse;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.NotificationEvent;
@@ -92,7 +93,7 @@ static void internalBeforeClassSetup(Map<String, String> primaryOverrides,
                                        Map<String, String> replicaOverrides, Class clazz,
                                        boolean autogather, AcidTableKind acidTableKind)
       throws Exception {
-    conf = new HiveConf(clazz);
+    conf = new HiveConfForTest(TestStatsReplicationScenarios.class);
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     MiniDFSCluster miniDFSCluster =

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestTimedOutTxnNotificationLogging.java
Patch:
@@ -20,6 +20,7 @@
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.common.repl.ReplScope;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.MetastoreTaskThread;
@@ -98,7 +99,7 @@ public void setUp() throws Exception {
   }
 
   private void setConf() {
-    hiveConf = new HiveConf();
+    hiveConf = new HiveConfForTest(getClass());
     MetastoreConf.setBoolVar(hiveConf, MetastoreConf.ConfVars.HIVE_IN_TEST, true);
     MetastoreConf.setVar(hiveConf, MetastoreConf.ConfVars.WAREHOUSE, "/tmp");
     MetastoreConf.setTimeVar(hiveConf, MetastoreConf.ConfVars.TXN_TIMEOUT, 1, TimeUnit.SECONDS);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestAuthorizationPreEventListener.java
Patch:
@@ -24,6 +24,7 @@
 
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.metastore.MetaStoreTestUtils;
 import org.apache.hadoop.hive.metastore.api.Database;
@@ -65,7 +66,7 @@ public void setUp() throws Exception {
 
     int port = MetaStoreTestUtils.startMetaStoreWithRetry();
 
-    clientHiveConf = new HiveConf(this.getClass());
+    clientHiveConf = new HiveConfForTest(this.getClass());
 
     clientHiveConf.setVar(HiveConf.ConfVars.METASTORE_URIS, "thrift://localhost:" + port);
     clientHiveConf.setIntVar(HiveConf.ConfVars.METASTORE_THRIFT_CONNECTION_RETRIES, 3);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestMetastoreClientSideAuthorizationProvider.java
Patch:
@@ -21,6 +21,7 @@
 import com.google.common.collect.Lists;
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.metastore.MetaStoreTestUtils;
 import org.apache.hadoop.hive.metastore.api.TableMeta;
@@ -54,7 +55,7 @@ public void setUp() throws Exception {
 
         int port = MetaStoreTestUtils.startMetaStoreWithRetry();
 
-        clientHiveConf = new HiveConf(this.getClass());
+        clientHiveConf = new HiveConfForTest(this.getClass());
 
         // Turn on client-side authorization
         clientHiveConf.setBoolVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED,true);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestMultiAuthorizationPreEventListener.java
Patch:
@@ -24,6 +24,7 @@
 
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.metastore.MetaStoreTestUtils;
 import org.apache.hadoop.hive.metastore.api.Database;
@@ -60,7 +61,7 @@ public static void setUp() throws Exception {
 
     int port = MetaStoreTestUtils.startMetaStoreWithRetry();
 
-    clientHiveConf = new HiveConf();
+    clientHiveConf = new HiveConfForTest(TestMultiAuthorizationPreEventListener.class);
 
     clientHiveConf.setVar(HiveConf.ConfVars.METASTORE_URIS, "thrift://localhost:" + port);
     clientHiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/authorization/plugin/TestHiveAuthorizerCheckInvocation.java
Patch:
@@ -38,6 +38,7 @@
 import org.apache.commons.lang3.tuple.Pair;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.utils.TestTxnDbUtil;
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.QueryState;
@@ -97,7 +98,7 @@ public HiveAuthorizer createHiveAuthorizer(HiveMetastoreClientFactory metastoreC
 
   @BeforeClass
   public static void beforeTest() throws Exception {
-    conf = new HiveConf();
+    conf = new HiveConfForTest(TestHiveAuthorizerCheckInvocation.class);
 
     // Turn on mocked authorization
     conf.setVar(ConfVars.HIVE_AUTHORIZATION_MANAGER, MockedHiveAuthorizerFactory.class.getName());

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/authorization/plugin/TestHiveAuthorizerShowFilters.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hadoop.hive.UtilsForTest;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.DriverFactory;
 import org.apache.hadoop.hive.ql.IDriver;
 import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;
@@ -113,7 +114,7 @@ public HiveAuthorizer createHiveAuthorizer(HiveMetastoreClientFactory metastoreC
 
   @BeforeClass
   public static void beforeTest() throws Exception {
-    conf = new HiveConf();
+    conf = new HiveConfForTest(TestHiveAuthorizerShowFilters.class);
 
     // Turn on mocked authorization
     conf.setVar(ConfVars.HIVE_AUTHORIZATION_MANAGER, MockedHiveAuthorizerFactory.class.getName());

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/session/TestClearDanglingScratchDir.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.junit.AfterClass;
 import org.junit.Assert;
@@ -51,7 +52,7 @@ public class TestClearDanglingScratchDir {
   @BeforeClass
   static public void oneTimeSetup() throws Exception {
     m_dfs = new MiniDFSCluster.Builder(new Configuration()).numDataNodes(1).format(true).build();
-    conf = new HiveConf();
+    conf = new HiveConfForTest(TestClearDanglingScratchDir.class);
     conf.set(HiveConf.ConfVars.HIVE_SCRATCH_DIR_LOCK.toString(), "true");
     conf.set(HiveConf.ConfVars.METASTORE_AUTO_CREATE_ALL.toString(), "true");
     LoggerFactory.getLogger("SessionState");
@@ -145,7 +146,7 @@ public void testClearDanglingScratchDir() throws Exception {
    */
   @Test
   public void testLocalDanglingFilesCleaning() throws Exception {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.set("fs.default.name", "file:///");
     String tmpDir = System.getProperty("test.tmp.dir");
     conf.set("hive.exec.scratchdir", tmpDir + "/hive-27317-hdfsscratchdir");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactorBase.java
Patch:
@@ -81,6 +81,8 @@ public void setup() throws Exception {
     }
 
     HiveConf hiveConf = new HiveConf(this.getClass());
+    // this test is mr specific, for Tez-based compaction look at CompactorOnTezTest
+    hiveConf.set(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE.varname, "mr");
     hiveConf.setVar(HiveConf.ConfVars.PRE_EXEC_HOOKS, "");
     hiveConf.setVar(HiveConf.ConfVars.POST_EXEC_HOOKS, "");
     hiveConf.setVar(HiveConf.ConfVars.METASTORE_WAREHOUSE, TEST_WAREHOUSE_DIR);

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniHA.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hdfs.HAUtil;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hive.jdbc.miniHS2.MiniHS2;
 import org.apache.hive.service.cli.HiveSQLException;
 import org.apache.hive.service.cli.session.HiveSessionHook;
@@ -67,7 +68,7 @@ public void run(HiveSessionHookContext sessionHookContext) throws HiveSQLExcepti
   @BeforeClass
   public static void beforeTest() throws Exception {
     Class.forName(MiniHS2.getJdbcDriverName());
-    conf = new HiveConf();
+    conf = new HiveConfForTest(TestJdbcWithMiniHA.class);
     conf.setBoolVar(ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     String dataFileDir = conf.get("test.data.files").replace('\\', '/')
         .replace("c:", "");

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/authorization/TestCLIAuthzSessionContext.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.common.io.SessionStream;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;
 import org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer;
@@ -60,7 +61,7 @@ public HiveAuthorizer createHiveAuthorizer(HiveMetastoreClientFactory metastoreC
 
   @BeforeClass
   public static void beforeTest() throws Exception {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(TestCLIAuthzSessionContext.class);
     conf.setVar(ConfVars.HIVE_AUTHORIZATION_MANAGER, MockedHiveAuthorizerFactory.class.getName());
     conf.setVar(ConfVars.HIVE_AUTHENTICATOR_MANAGER, SessionStateUserAuthenticator.class.getName());
     conf.setBoolVar(ConfVars.HIVE_AUTHORIZATION_ENABLED, true);

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/authorization/TestJdbcWithSQLAuthUDFBlacklist.java
Patch:
@@ -28,6 +28,7 @@
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;
 import org.apache.hive.jdbc.miniHS2.MiniHS2;
@@ -64,7 +65,7 @@ public void shutDownHS2() throws Exception {
 
   @Test
   public void testBlackListedUdfUsage() throws Exception {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.setVar(ConfVars.HIVE_SERVER2_BUILTIN_UDF_BLACKLIST, "sqrt");
     startHS2(conf);
 

File: itests/hive-unit/src/test/java/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hadoop.hive.UtilsForTest;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService;
 import org.apache.hive.service.cli.thrift.ThriftCLIService;
 import org.apache.hive.service.cli.thrift.ThriftCLIServiceClient;
@@ -37,7 +38,7 @@ public class TestEmbeddedThriftBinaryCLIService extends CLIServiceTest {
   @BeforeClass
   public static void setUpBeforeClass() throws Exception {
     service = new EmbeddedThriftBinaryCLIService();
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(TestEmbeddedThriftBinaryCLIService.class);
     conf.setBoolean("datanucleus.schema.autoCreateTables", true);
     conf.setVar(HiveConf.ConfVars.HIVE_MAPRED_MODE, "nonstrict");
     UtilsForTest.expandHiveConfParams(conf);

File: itests/hive-unit/src/test/java/org/apache/hive/service/cli/operation/TestOperationLoggingAPIWithMr.java
Patch:
@@ -64,6 +64,8 @@ public static void setUpBeforeClass() throws Exception {
     };
     hiveConf = new HiveConf();
     hiveConf.set(ConfVars.HIVE_SERVER2_LOGGING_OPERATION_LEVEL.varname, "verbose");
+    // this test is mr specific
+    hiveConf.set(ConfVars.HIVE_EXECUTION_ENGINE.varname, "mr");
     miniHS2 = new MiniHS2(hiveConf);
     confOverlay = new HashMap<String, String>();
     confOverlay.put(ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");

File: itests/hive-unit/src/test/java/org/apache/hive/service/cli/operation/TestOperationLoggingLayout.java
Patch:
@@ -24,8 +24,8 @@
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 
-import org.apache.hadoop.hive.common.LogUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender;
 import org.apache.hadoop.hive.ql.log.LogDivertAppender;
 import org.apache.hadoop.hive.ql.log.LogDivertAppenderForTest;
@@ -69,7 +69,7 @@ public class TestOperationLoggingLayout {
   @BeforeClass
   public static void setUpBeforeClass() throws Exception {
     tableName = "TestOperationLoggingLayout_table";
-    hiveConf = new HiveConf();
+    hiveConf = new HiveConfForTest(TestOperationLoggingLayout.class);
     hiveConf.set(HiveConf.ConfVars.HIVE_SERVER2_LOGGING_OPERATION_LEVEL.varname, "execution");
     miniHS2 = new MiniHS2(hiveConf);
     confOverlay = new HashMap<String, String>();

File: itests/hive-unit/src/test/java/org/apache/hive/service/server/InformationSchemaWithPrivilegeTestBase.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator;
 import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;
@@ -193,7 +194,7 @@ public static void setupInternal(boolean zookeeperSSLEnabled) throws Exception {
     zkCluster = new MiniZooKeeperCluster(zookeeperSSLEnabled);
     int zkPort = zkCluster.startup(zkDataDir);
 
-    miniHS2 = new MiniHS2(new HiveConf());
+    miniHS2 = new MiniHS2(new HiveConfForTest(InformationSchemaWithPrivilegeTestBase.class));
     confOverlay = new HashMap<String, String>();
     Path workDir = new Path(System.getProperty("test.tmp.dir",
         "target" + File.separator + "test" + File.separator + "tmp"));
@@ -219,7 +220,7 @@ public static void setupInternal(boolean zookeeperSSLEnabled) throws Exception {
     if(zookeeperSSLEnabled) {
       String dataFileDir = !System.getProperty("test.data.files", "").isEmpty() ?
           System.getProperty("test.data.files") :
-          (new HiveConf()).get("test.data.files").replace('\\', '/').replace("c:", "");
+          (new HiveConfForTest(InformationSchemaWithPrivilegeTestBase.class)).get("test.data.files").replace('\\', '/').replace("c:", "");
       confOverlay.put(ConfVars.HIVE_ZOOKEEPER_SSL_KEYSTORE_LOCATION.varname,
           dataFileDir + File.separator + LOCALHOST_KEY_STORE_NAME);
       confOverlay.put(ConfVars.HIVE_ZOOKEEPER_SSL_KEYSTORE_PASSWORD.varname,

File: itests/hive-unit/src/test/java/org/apache/hive/service/server/TestGracefulStopHS2.java
Patch:
@@ -49,6 +49,8 @@ public static void setupBeforeClass() throws Exception {
     MiniHS2.cleanupLocalDir();
     try {
       HiveConf conf = new HiveConf();
+      //TODO: HIVE-28296: TestGracefulStopHS2 to run on Tez
+      conf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
       conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
       conf.setBoolVar(HiveConf.ConfVars.HIVE_SERVER2_LOGGING_OPERATION_ENABLED, false);
       conf.setBoolVar(HiveConf.ConfVars.HIVE_STATS_COL_AUTOGATHER, false);

File: itests/hive-unit/src/test/java/org/apache/hive/service/server/TestHS2ClearDanglingScratchDir.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.shims.Utils;
 import org.junit.Assert;
@@ -34,7 +35,7 @@ public class TestHS2ClearDanglingScratchDir {
   @Test
   public void testScratchDirCleared() throws Exception {
     MiniDFSCluster m_dfs = new MiniDFSCluster.Builder(new Configuration()).numDataNodes(1).format(true).build();
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.addResource(m_dfs.getConfiguration(0));
     conf.set(HiveConf.ConfVars.HIVE_SCRATCH_DIR_LOCK.toString(), "true");
     conf.set(HiveConf.ConfVars.HIVE_SERVER2_CLEAR_DANGLING_SCRATCH_DIR.toString(), "true");

File: itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloQTestUtil.java
Patch:
@@ -26,15 +26,15 @@
  */
 public class AccumuloQTestUtil extends QTestUtil {
 
-  public AccumuloQTestUtil(String outDir, String logDir, MiniClusterType miniMr,
-      AccumuloTestSetup setup, String initScript, String cleanupScript) throws Exception {
+  public AccumuloQTestUtil(String outDir, String logDir, MiniClusterType miniMr, AccumuloTestSetup setup,
+      String hiveConfDir, String initScript, String cleanupScript) throws Exception {
 
     super(
         QTestArguments.QTestArgumentsBuilder.instance()
           .withOutDir(outDir)
           .withLogDir(logDir)
           .withClusterType(miniMr)
-          .withConfDir(null)
+          .withConfDir(hiveConfDir)
           .withInitScript(initScript)
           .withCleanupScript(cleanupScript)
           .withLlapIo(false)

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CoreAccumuloCliDriver.java
Patch:
@@ -44,11 +44,12 @@ public CoreAccumuloCliDriver(AbstractCliConfig cliConfig) {
   @BeforeClass
   public void beforeClass() throws Exception {
     MiniClusterType miniMR = cliConfig.getClusterType();
+    String hiveConfDir = cliConfig.getHiveConfDir();
     String initScript = cliConfig.getInitScript();
     String cleanupScript = cliConfig.getCleanupScript();
 
     qt = new AccumuloQTestUtil(cliConfig.getResultsDir(), cliConfig.getLogDir(), miniMR, new AccumuloTestSetup(),
-        initScript, cleanupScript);
+        hiveConfDir, initScript, cleanupScript);
   }
 
   @Override

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CoreBeeLineDriver.java
Patch:
@@ -58,6 +58,7 @@
 import com.google.common.collect.ObjectArrays;
 
 public class CoreBeeLineDriver extends CliAdapter {
+
   private final File hiveRootDirectory = new File(AbstractCliConfig.HIVE_ROOT);
   private final File queryDirectory;
   private final File logDirectory;
@@ -107,6 +108,8 @@ public CoreBeeLineDriver(AbstractCliConfig testCliConfig) {
 
   private static MiniHS2 createMiniServer() throws Exception {
     HiveConf hiveConf = new HiveConf();
+    // TODO: HIVE-28031: Adapt some cli driver tests to Tez where it's applicable
+    hiveConf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
     // We do not need Zookeeper at the moment
     hiveConf.set(HiveConf.ConfVars.HIVE_LOCK_MANAGER.varname,
         "org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager");

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CoreHBaseCliDriver.java
Patch:
@@ -43,11 +43,12 @@ public CoreHBaseCliDriver(AbstractCliConfig testCliConfig) {
   @BeforeClass
   public void beforeClass() throws Exception {
     MiniClusterType miniMR = cliConfig.getClusterType();
+    String hiveConfDir = cliConfig.getHiveConfDir();
     String initScript = cliConfig.getInitScript();
     String cleanupScript = cliConfig.getCleanupScript();
 
-    qt = new HBaseQTestUtil(cliConfig.getResultsDir(), cliConfig.getLogDir(), miniMR, new HBaseTestSetup(), initScript,
-        cleanupScript);
+    qt = new HBaseQTestUtil(cliConfig.getResultsDir(), cliConfig.getLogDir(), miniMR, new HBaseTestSetup(), hiveConfDir,
+        initScript, cleanupScript);
   }
 
   @Override

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CoreHBaseNegativeCliDriver.java
Patch:
@@ -42,11 +42,12 @@ public CoreHBaseNegativeCliDriver(AbstractCliConfig testCliConfig) {
   @Override
   public void beforeClass() throws Exception {
     MiniClusterType miniMR = cliConfig.getClusterType();
+    String hiveConfDir = cliConfig.getHiveConfDir();
     String initScript = cliConfig.getInitScript();
     String cleanupScript = cliConfig.getCleanupScript();
 
-    qt = new HBaseQTestUtil(cliConfig.getResultsDir(), cliConfig.getLogDir(), miniMR, new HBaseTestSetup(), initScript,
-        cleanupScript);
+    qt = new HBaseQTestUtil(cliConfig.getResultsDir(), cliConfig.getLogDir(), miniMR, new HBaseTestSetup(), hiveConfDir,
+        initScript, cleanupScript);
   }
 
   @Override

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/parse/CoreParseNegative.java
Patch:
@@ -47,6 +47,7 @@ public CoreParseNegative(AbstractCliConfig testCliConfig) {
   @BeforeClass
   public void beforeClass() throws Exception {
     MiniClusterType miniMR = cliConfig.getClusterType();
+    String hiveConfDir = cliConfig.getHiveConfDir();
     String initScript = cliConfig.getInitScript();
     String cleanupScript = cliConfig.getCleanupScript();
 
@@ -55,7 +56,7 @@ public void beforeClass() throws Exception {
           .withOutDir(cliConfig.getResultsDir())
           .withLogDir(cliConfig.getLogDir())
           .withClusterType(miniMR)
-          .withConfDir(null)
+          .withConfDir(hiveConfDir)
           .withInitScript(initScript)
           .withCleanupScript(cleanupScript)
           .withLlapIo(false)

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HivePreWarmProcessor.java
Patch:
@@ -86,7 +86,7 @@ public void run(Map<String, LogicalInput> inputs,
     ReadaheadPool rpool = ReadaheadPool.getInstance();
     ShimLoader.getHadoopShims();
 
-    URL hiveurl = new URL("jar:" + DagUtils.getInstance().getExecJarPathLocal(conf) + "!/");
+    URL hiveurl = new URL("jar:file:" + DagUtils.getInstance().getExecJarPathLocal(conf) + "!/");
     JarURLConnection hiveconn = (JarURLConnection)hiveurl.openConnection();
     JarFile hivejar = hiveconn.getJarFile();
     try {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java
Patch:
@@ -283,7 +283,7 @@ protected void openInternal(String[] additionalFilesNotFromConf,
     } else {
       this.resources = new HiveResources(createTezDir(sessionId, "resources"));
       ensureLocalResources(conf, additionalFilesNotFromConf);
-      LOG.info("Created new resources: " + resources);
+      LOG.info("Created new resources: " + this.resources);
     }
 
     // unless already installed on all the cluster nodes, we'll have to

File: ql/src/test/org/apache/hadoop/hive/ql/TxnCommandsBaseForTests.java
Patch:
@@ -106,6 +106,8 @@ public void setUp() throws Exception {
   }
   void initHiveConf() {
     hiveConf = new HiveConf(this.getClass());
+    //TODO: HIVE-28029: Make unit tests based on TxnCommandsBaseForTests run on Tez
+    hiveConf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
   }
   void setUpInternal() throws Exception {
     initHiveConf();

File: ql/src/test/org/apache/hadoop/hive/ql/ddl/table/partition/show/TestShowPartitionAnalyzer.java
Patch:
@@ -24,12 +24,12 @@
 import java.util.Map;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.TableType;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.QueryState;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
-import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner;
 import org.apache.hadoop.hive.ql.parse.ASTNode;
@@ -56,7 +56,7 @@ public class TestShowPartitionAnalyzer {
 
   @Before
   public void before() throws Exception {
-    conf = new HiveConf();
+    conf = new HiveConfForTest(getClass());
     SessionState.start(conf);
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestContext.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.junit.Before;
@@ -31,7 +32,7 @@
 import static org.mockito.Mockito.*;
 
 public class TestContext {
-    private static HiveConf conf = new HiveConf();
+    private static HiveConf conf = new HiveConfForTest(TestContext.class);
 
     private Context context;
 

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestExecDriver.java
Patch:
@@ -89,6 +89,8 @@ public class TestExecDriver {
     try {
       queryState = new QueryState.Builder().withHiveConf(new HiveConf(ExecDriver.class)).build();
       conf = queryState.getConf();
+      // this test is mr specific
+      conf.set(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE.varname, "mr");
       conf.setBoolVar(HiveConf.ConfVars.SUBMIT_VIA_CHILD, true);
       conf.setBoolVar(HiveConf.ConfVars.SUBMIT_LOCAL_TASK_VIA_CHILD, true);
       conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java
Patch:
@@ -27,6 +27,7 @@
 
 import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.exec.FunctionInfo.FunctionResource;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
@@ -96,7 +97,8 @@ public void setUp() {
     varchar5 = TypeInfoFactory.getPrimitiveTypeInfo("varchar(5)");
     char10 = TypeInfoFactory.getPrimitiveTypeInfo("char(10)");
     char5 = TypeInfoFactory.getPrimitiveTypeInfo("char(5)");
-    SessionState.start(new HiveConf());
+    HiveConf conf = new HiveConfForTest(getClass());
+    SessionState.start(conf);
   }
 
   private void implicit(TypeInfo a, TypeInfo b, boolean convertible) {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestGetInputSummary.java
Patch:
@@ -46,6 +46,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.io.ContentSummaryInputFormat;
@@ -79,7 +80,7 @@ public class TestGetInputSummary {
   @Before
   public void setup() throws Exception {
     // creates scratch directories needed by the Context object
-    SessionState.start(new HiveConf());
+    SessionState.start(new HiveConfForTest(getClass()));
 
     this.jobConf = new JobConf();
     this.properties = new Properties();

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.CompilationOpContext;
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;
@@ -421,7 +422,7 @@ public InputSplit[] getSplits(JobConf job, int splits) throws IOException {
 
   @Test
   public void testFetchOperatorContext() throws Exception {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.set("hive.support.concurrency", "false");
     conf.setVar(HiveConf.ConfVars.HIVE_MAPRED_MODE, "nonstrict");
     conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,

File: ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hadoop.hive.common.metrics.common.Metrics;
 import org.apache.hadoop.hive.common.metrics.common.MetricsConstant;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.QueryPlan;
 import org.apache.hadoop.hive.ql.exec.Operator;
@@ -170,7 +171,7 @@ public Edge answer(InvocationOnMock invocation) throws Throwable {
     conf = new JobConf();
     appLr = createResource("foo.jar");
 
-    HiveConf hiveConf = new HiveConf();
+    HiveConf hiveConf = new HiveConfForTest(getClass());
     hiveConf
         .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
             "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
@@ -356,7 +357,7 @@ public void testParseRightmostXmx() throws Exception {
   }
 
   @Test
-  public void tezTask_updates_Metrics() throws IOException {
+  public void testTezTaskUpdatesMetrics() throws IOException {
 
     Metrics mockMetrics = Mockito.mock(Metrics.class);
 

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorUDFUnixTimeStampString.java
Patch:
@@ -17,6 +17,7 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
@@ -72,7 +73,7 @@ public static Collection<String[]> readInputs() throws IOException, CsvException
 
   @Test
   public void testEvaluate() throws HiveException, InterruptedException, CharacterCodingException {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.setVar(HiveConf.ConfVars.HIVE_DATETIME_FORMATTER, formatter);
     conf.setVar(HiveConf.ConfVars.HIVE_LOCAL_TIME_ZONE, zone);
     SessionState state = SessionState.start(conf);

File: ql/src/test/org/apache/hadoop/hive/ql/hooks/TestQueryHooks.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.hooks;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
 import org.apache.hadoop.hive.ql.session.SessionState;
@@ -41,7 +42,7 @@ public class TestQueryHooks {
 
   @BeforeClass
   public static void setUpBeforeClass() {
-    conf = new HiveConf(TestQueryHooks.class);
+    conf = new HiveConfForTest(TestQueryHooks.class);
     conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
             "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");

File: ql/src/test/org/apache/hadoop/hive/ql/io/TestSymlinkTextInputFormat.java
Patch:
@@ -111,7 +111,6 @@ public void tearDown() throws IOException {
   @Test
   public void testCombine() throws Exception {
     JobConf newJob = new JobConf(job);
-    FileSystem fs = dataDir1.getFileSystem(newJob);
 
     Path dir1_file1 = new Path(dataDir1, "combinefile1_1");
     writeTextFile(dir1_file1,
@@ -132,6 +131,8 @@ public void testCombine() throws Exception {
 
 
     HiveConf hiveConf = new HiveConf(TestSymlinkTextInputFormat.class);
+    // TODO: HIVE-28032: TestSymlinkTextInputFormat.testCombine to run on Tez
+    hiveConf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
     hiveConf
     .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/DbTxnManagerEndToEndTestBase.java
Patch:
@@ -54,6 +54,8 @@ public abstract class DbTxnManagerEndToEndTestBase {
   protected TxnStore txnHandler;
 
   public DbTxnManagerEndToEndTestBase() {
+    //TODO: HIVE-28029: Make unit tests based on DbTxnManagerEndToEndTestBase run on Tez
+    conf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
     HiveConf.setVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
       "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, false);

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager.java
Patch:
@@ -19,6 +19,7 @@
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.ThreadPool;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse;
@@ -67,7 +68,7 @@
  */
 public class TestDbTxnManager {
   private static final int TEST_TIMED_OUT_TXN_ABORT_BATCH_SIZE = 1000;
-  private final HiveConf conf = new HiveConf();
+  private final HiveConf conf = new HiveConfForTest(getClass());
   private HiveTxnManager txnMgr;
   private AcidHouseKeeperService houseKeeperService = null;
   private final Context ctx;
@@ -76,8 +77,7 @@ public class TestDbTxnManager {
   HashSet<WriteEntity> writeEntities;
 
   public TestDbTxnManager() throws Exception {
-    conf
-    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+    conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     TestTxnDbUtil.setConfValues(conf);
     SessionState.start(conf);

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDummyTxnManager.java
Patch:
@@ -24,6 +24,7 @@
 import org.junit.Assert;
 import org.junit.Before;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.ErrorMsg;
@@ -50,7 +51,7 @@
 
 @RunWith(MockitoJUnitRunner.class)
 public class TestDummyTxnManager {
-  private final HiveConf conf = new HiveConf();
+  private final HiveConf conf = new HiveConfForTest(getClass());
   private HiveTxnManager txnMgr;
   private Context ctx;
   private int nextInput = 1;

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSessionHiveMetastoreClientAddPartitionsFromSpecTempTable.java
Patch:
@@ -19,6 +19,7 @@
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.annotation.MetastoreCheckinTest;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Partition;
@@ -66,7 +67,7 @@ public void setUp() throws Exception {
   }
 
   private void initHiveConf() throws HiveException {
-    conf = Hive.get().getConf();
+    conf = new HiveConfForTest(Hive.get().getConf(), getClass());
     conf.setBoolVar(HiveConf.ConfVars.METASTORE_FASTPATH, true);
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSessionHiveMetastoreClientAddPartitionsTempTable.java
Patch:
@@ -20,6 +20,7 @@
 import com.google.common.collect.Lists;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.annotation.MetastoreCheckinTest;
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.hive.metastore.api.Table;
@@ -73,7 +74,7 @@ public void setUp() throws Exception {
   }
 
   private void initHiveConf() throws HiveException {
-    conf = Hive.get().getConf();
+    conf = new HiveConfForTest(Hive.get().getConf(), getClass());
     conf.setBoolVar(HiveConf.ConfVars.METASTORE_FASTPATH, true);
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSessionHiveMetastoreClientAlterPartitionsTempTable.java
Patch:
@@ -19,6 +19,7 @@
 
 import com.google.common.collect.Lists;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.MetaStoreTestUtils;
 import org.apache.hadoop.hive.metastore.annotation.MetastoreCheckinTest;
@@ -71,7 +72,7 @@ public void setUp() throws Exception {
   }
 
   private void initHiveConf() throws HiveException {
-    conf = Hive.get().getConf();
+    conf = new HiveConfForTest(Hive.get().getConf(), getClass());
     conf.setBoolVar(HiveConf.ConfVars.METASTORE_FASTPATH, true);
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSessionHiveMetastoreClientAppendPartitionTempTable.java
Patch:
@@ -19,6 +19,7 @@
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.annotation.MetastoreCheckinTest;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Partition;
@@ -62,7 +63,7 @@ public void setUp() throws Exception {
   }
 
   private void initHiveConf() throws HiveException {
-    conf = Hive.get().getConf();
+    conf = new HiveConfForTest(Hive.get().getConf(), getClass());
     conf.setBoolVar(HiveConf.ConfVars.METASTORE_FASTPATH, true);
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSessionHiveMetastoreClientDropPartitionsTempTable.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.annotation.MetastoreCheckinTest;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Partition;
@@ -72,7 +73,7 @@ public void setUp() throws Exception {
   }
 
   private void initHiveConf() throws HiveException {
-    conf = Hive.get().getConf();
+    conf = new HiveConfForTest(Hive.get().getConf(), getClass());
     conf.setBoolVar(HiveConf.ConfVars.METASTORE_FASTPATH, true);
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSessionHiveMetastoreClientExchangePartitionsTempTable.java
Patch:
@@ -20,6 +20,7 @@
 import com.google.common.collect.Lists;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.Warehouse;
 import org.apache.hadoop.hive.metastore.annotation.MetastoreCheckinTest;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
@@ -69,7 +70,7 @@ public void setUp() throws Exception {
   }
 
   private void initHiveConf() throws Exception{
-    conf = Hive.get().getConf();
+    conf = new HiveConfForTest(Hive.get().getConf(), getClass());
     conf.setBoolVar(HiveConf.ConfVars.METASTORE_FASTPATH, true);
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSessionHiveMetastoreClientGetPartitionsTempTable.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.google.common.collect.Lists;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.annotation.MetastoreCheckinTest;
 import org.apache.hadoop.hive.metastore.api.Partition;
@@ -75,7 +76,7 @@ public void setUp() throws Exception {
   }
 
   private void initHiveConf() throws HiveException {
-    conf = Hive.get().getConf();
+    conf = new HiveConfForTest(Hive.get().getConf(), getClass());
     conf.setBoolVar(HiveConf.ConfVars.METASTORE_FASTPATH, true);
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSessionHiveMetastoreClientListPartitionsTempTable.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.google.common.collect.Lists;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.TestMetastoreExpr;
 import org.apache.hadoop.hive.metastore.annotation.MetastoreCheckinTest;
@@ -94,7 +95,7 @@ public void setUp() throws Exception {
   }
 
   private void initHiveConf() throws HiveException {
-    conf = Hive.get().getConf();
+    conf = new HiveConfForTest(Hive.get().getConf(), getClass());
     conf.setBoolVar(HiveConf.ConfVars.METASTORE_FASTPATH, true);
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/optimizer/TestGenMapRedUtilsCreateConditionalTask.java
Patch:
@@ -64,6 +64,8 @@ public class TestGenMapRedUtilsCreateConditionalTask {
   @BeforeClass
   public static void initializeSessionState() {
     hiveConf = new HiveConf();
+    // GenMapRedUtils is an MR compiler class
+    hiveConf.set(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE.varname, "mr");
   }
 
   @Before

File: ql/src/test/org/apache/hadoop/hive/ql/optimizer/physical/TestNullScanTaskDispatcher.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.StorageStatistics;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.CompilationOpContext;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
@@ -83,7 +84,7 @@ public class TestNullScanTaskDispatcher {
 
   @Before
   public void setup() {
-    hiveConf = new HiveConf();
+    hiveConf = new HiveConfForTest(getClass());
     hiveConf.set("fs.mock.impl", MockFileSystem.class.getName());
     hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_METADATA_ONLY_QUERIES, true);
     sessionState = SessionState.start(hiveConf);

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestDMLSemanticAnalyzer.java
Patch:
@@ -229,6 +229,8 @@ public void testInsertValuesPartitioned() throws Exception {
   public void setup() throws Exception {
     queryState = new QueryState.Builder().build();
     conf = queryState.getConf();
+    // the test doesn't involve DAG execution, skip TezSessionState initialization
+    conf.setBoolean(HiveConf.ConfVars.HIVE_CLI_TEZ_INITIALIZE_SESSION.varname, false);
     conf
     .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestGenTezWork.java
Patch:
@@ -22,13 +22,13 @@
 import static org.junit.Assert.assertSame;
 import static org.junit.Assert.assertTrue;
 
-import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.LinkedHashMap;
 import java.util.Properties;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.CompilationOpContext;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
@@ -68,8 +68,7 @@ public class TestGenTezWork {
   @SuppressWarnings("unchecked")
   @Before
   public void setUp() throws Exception {
-    // Init conf
-    final HiveConf conf = new HiveConf(SemanticAnalyzer.class);
+    final HiveConf conf = new HiveConfForTest(getClass());
     SessionState.start(conf);
 
     // Init parse context

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestHiveDecimalParse.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.parse;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.QueryPlan;
@@ -166,7 +167,7 @@ public void testDecimalType9() throws ParseException {
   }
 
   private Driver createDriver() {
-    HiveConf conf = new HiveConf(Driver.class);
+    HiveConf conf = new HiveConfForTest(getClass());
     conf
     .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestMacroSemanticAnalyzer.java
Patch:
@@ -44,6 +44,8 @@ public class TestMacroSemanticAnalyzer {
   public void setup() throws Exception {
     queryState = new QueryState.Builder().build();
     conf = queryState.getConf();
+    // the test doesn't involve DAG execution, skip TezSessionState initialization
+    conf.setBoolean(HiveConf.ConfVars.HIVE_CLI_TEZ_INITIALIZE_SESSION.varname, false);
     SessionState.start(conf);
     context = new Context(conf);
   }

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestParseUtils.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.api.TxnType;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.io.AcidUtils;
@@ -49,7 +50,7 @@ public class TestParseUtils {
   public TestParseUtils(String query, TxnType txnType) {
     this.query = query;
     this.txnType = txnType;
-    this.conf = new HiveConf();
+    this.conf = new HiveConfForTest(getClass());
   }
 
   @Before

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestReplicationSemanticAnalyzer.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.ql.parse;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.junit.Test;
 import org.junit.experimental.runners.Enclosed;
@@ -32,7 +33,7 @@ public class TestReplicationSemanticAnalyzer {
   private static HiveConf hiveConf = buildHiveConf();
 
   public static HiveConf buildHiveConf() {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(TestReplicationSemanticAnalyzer.class);
     conf.setVar(HIVE_QUOTEDID_SUPPORT, Quotation.NONE.stringValue());
     return conf;
   }

File: ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java
Patch:
@@ -98,6 +98,8 @@ public static void reset() {
   public void setup() throws Exception {
     queryState = new QueryState.Builder().build();
     HiveConf conf = queryState.getConf();
+    // the test doesn't involve DAG execution, skip TezSessionState initialization
+    conf.setBoolean(HiveConf.ConfVars.HIVE_CLI_TEZ_INITIALIZE_SESSION.varname, false);
     conf.setVar(ConfVars.HIVE_AUTHORIZATION_TASK_FACTORY,
         TestHiveAuthorizationTaskFactory.DummyHiveAuthorizationTaskFactoryImpl.class.getName());
     conf

File: ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestPrivilegesV1.java
Patch:
@@ -42,6 +42,8 @@ public void setup() throws Exception {
     queryState = new QueryState.Builder().build();
     db = Mockito.mock(Hive.class);
     HiveConf hiveConf = queryState.getConf();
+    // the test doesn't involve DAG execution, skip TezSessionState initialization
+    hiveConf.setBoolean(HiveConf.ConfVars.HIVE_CLI_TEZ_INITIALIZE_SESSION.varname, false);
     table = new Table(DB, TABLE);
     partition = new Partition(table);
     hiveConf

File: ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestPrivilegesV2.java
Patch:
@@ -43,6 +43,8 @@ public void setup() throws Exception {
     queryState = new QueryState.Builder().build();
     //set authorization mode to V2
     HiveConf conf = queryState.getConf();
+    // the test doesn't involve DAG execution, skip TezSessionState initialization
+    conf.setBoolean(HiveConf.ConfVars.HIVE_CLI_TEZ_INITIALIZE_SESSION.varname, false);
     conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         SQLStdHiveAuthorizerFactory.class.getName());
     db = Mockito.mock(Hive.class);

File: ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestSessionUserName.java
Patch:
@@ -20,6 +20,7 @@
 import org.junit.Assert;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;
@@ -132,7 +133,7 @@ private void setupDataNucleusFreeHive(HiveConf hiveConf) throws MetaException {
    * that captures the given user name
    */
   private HiveConf getAuthV2HiveConf() {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         HiveAuthorizerStoringUserNameFactory.class.getName());
     conf.setVar(HiveConf.ConfVars.HIVE_AUTHENTICATOR_MANAGER,

File: ql/src/test/org/apache/hadoop/hive/ql/parse/type/TestDecimalStringValidation.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.ql.parse.type;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.exec.FunctionInfo;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
@@ -100,7 +101,7 @@ public static Collection<FunctionCall> params() throws Exception {
 
   @Test
   public void testValidationDecimalWithCharacterFailsWhenStrictChecksEnabled() {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.setBoolVar(HiveConf.ConfVars.HIVE_STRICT_CHECKS_TYPE_SAFETY, true);
     try {
       validateCall(conf);
@@ -112,7 +113,7 @@ public void testValidationDecimalWithCharacterFailsWhenStrictChecksEnabled() {
 
   @Test
   public void testValidationDecimalWithCharacterSucceedsWhenStrictChecksDisabled() throws SemanticException {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.setBoolVar(HiveConf.ConfVars.HIVE_STRICT_CHECKS_TYPE_SAFETY, false);
     validateCall(conf);
   }

File: ql/src/test/org/apache/hadoop/hive/ql/processors/TestCommandProcessorFactory.java
Patch:
@@ -21,6 +21,7 @@
 import java.sql.SQLException;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.junit.Assert;
 import org.junit.Before;
@@ -34,7 +35,7 @@ public class TestCommandProcessorFactory {
 
   @Before
   public void setUp() throws Exception {
-    conf = new HiveConf();
+    conf = new HiveConfForTest(getClass());
   }
 
   @Test

File: ql/src/test/org/apache/hadoop/hive/ql/session/TestAddResource.java
Patch:
@@ -32,6 +32,7 @@
 import java.util.Set;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.session.SessionState.ResourceType;
 
 import org.junit.Before;
@@ -44,12 +45,13 @@
 public class TestAddResource {
 
   private static final String TEST_JAR_DIR = System.getProperty("test.tmp.dir", ".") + File.separator;
+
   private HiveConf conf;
   private ResourceType t;
 
   @Before
   public void setup() throws IOException {
-    conf = new HiveConf();
+    conf = new HiveConfForTest(getClass());
     t = ResourceType.JAR;
 
     //Generate test jar files
@@ -165,8 +167,6 @@ public void testDuplicateAdds() throws URISyntaxException, IOException {
   // test when two jars with shared dependencies are added, the classloader contains union of the dependencies
   @Test
   public void testUnion() throws URISyntaxException, IOException {
-
-    HiveConf conf = new HiveConf();
     SessionState ss = Mockito.spy(SessionState.start(conf).get());
     ResourceType t = ResourceType.JAR;
     String query1 = "testQuery1";

File: ql/src/test/org/apache/hadoop/hive/ql/tool/TestLineageInfo.java
Patch:
@@ -17,12 +17,12 @@
  */
 
 package org.apache.hadoop.hive.ql.tool;
-
 import static org.junit.Assert.fail;
 
 import java.util.TreeSet;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.tools.LineageInfo;
@@ -40,7 +40,7 @@ public class TestLineageInfo {
 
   @Before
   public void before() {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(getClass());
     SessionState.start(conf);
     ctx = new Context(conf);
   }

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFDateFormatEvaluate.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.udf.generic;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredJavaObject;
@@ -51,6 +52,7 @@
  */
 @RunWith(Parameterized.class)
 public class TestGenericUDFDateFormatEvaluate {
+
   private final GenericUDFDateFormat udf = new GenericUDFDateFormat();
   private final String value;
   private final String pattern;
@@ -84,7 +86,7 @@ public static Collection<String[]> readInputs() throws IOException, CsvException
 
   @Test
   public void testEvaluate() throws HiveException, InterruptedException {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.setVar(HiveConf.ConfVars.HIVE_DATETIME_FORMATTER, formatter);
     conf.setVar(HiveConf.ConfVars.HIVE_LOCAL_TIME_ZONE, zone);
     SessionState state = SessionState.start(conf);

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFFromUnixTimeEvaluate.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.udf.generic;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredJavaObject;
@@ -84,7 +85,7 @@ public static Collection<String[]> readInputs() throws IOException, CsvException
 
   @Test
   public void testEvaluate() throws HiveException, InterruptedException {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.setVar(HiveConf.ConfVars.HIVE_DATETIME_FORMATTER, formatter);
     conf.setVar(HiveConf.ConfVars.HIVE_LOCAL_TIME_ZONE, zone);
     SessionState state = SessionState.start(conf);

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFToUnixTimestampEvaluateStringString.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.udf.generic;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredJavaObject;
@@ -92,7 +93,7 @@ public void testEvaluateUnixTimeStamp() throws HiveException, InterruptedExcepti
   }
 
   private void testEvaluateWithUDF(GenericUDF udfToTest) throws HiveException, InterruptedException {
-    HiveConf conf = new HiveConf();
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.setVar(HiveConf.ConfVars.HIVE_DATETIME_FORMATTER, formatter);
     conf.setVar(HiveConf.ConfVars.HIVE_LOCAL_TIME_ZONE, zone);
     conf.setVar(HiveConf.ConfVars.HIVE_DATETIME_RESOLVER_STYLE, resolverStyle);

File: ql/src/test/org/apache/hive/testutils/HiveTestEnvSetup.java
Patch:
@@ -361,4 +361,4 @@ public HiveTestEnvContext getTestCtx() {
     return testEnvContext;
   }
 
-}
+}
\ No newline at end of file

File: service/src/test/org/apache/hive/service/cli/CLIServiceTest.java
Patch:
@@ -301,6 +301,8 @@ private void syncThreadStart(final CountDownLatch cdlIn, final CountDownLatch cd
   @Test
   public void testExecuteStatementParallel() throws Exception {
     Map<String, String> confOverlay = new HashMap<String, String>();
+    //TODO: HIVE-28283: CliServiceTest.testExecuteStatementParallel to run on Tez
+    confOverlay.put(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE.varname, "mr");
     String tableName = "TEST_EXEC_PARALLEL";
     String columnDefinitions = "(ID STRING)";
 

File: service/src/test/org/apache/hive/service/cli/operation/TestCommandWithSpace.java
Patch:
@@ -21,7 +21,7 @@
 import com.google.common.collect.ImmutableMap;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.hooks.TestQueryHooks;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.processors.DfsProcessor;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hive.service.cli.HiveSQLException;
@@ -36,7 +36,7 @@ public class TestCommandWithSpace {
     @Test
     public void testCommandWithPrefixSpace() throws IllegalAccessException, ClassNotFoundException, InstantiationException, HiveSQLException {
         String query = " dfs -ls /";
-        HiveConf conf = new HiveConf();
+        HiveConf conf = new HiveConfForTest(getClass());
         conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
         conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
                 "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");

File: service/src/test/org/apache/hive/service/cli/operation/TestQueryLifeTimeHooksWithSQLOperation.java
Patch:
@@ -21,9 +21,9 @@
 import com.google.common.collect.ImmutableMap;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHookContext;
 import org.apache.hadoop.hive.ql.hooks.QueryLifeTimeHookWithParseHooks;
-import org.apache.hadoop.hive.ql.hooks.TestQueryHooks;
 import org.apache.hadoop.hive.ql.session.SessionState;
 
 import org.apache.hive.service.cli.HandleIdentifier;
@@ -48,7 +48,7 @@ public class TestQueryLifeTimeHooksWithSQLOperation {
 
   @Test
   public void testQueryInfoInHookContext() throws IllegalAccessException, ClassNotFoundException, InstantiationException, HiveSQLException {
-    HiveConf conf = new HiveConf(TestQueryHooks.class);
+    HiveConf conf = new HiveConfForTest(getClass());
     conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
             "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");

File: service/src/test/org/apache/hive/service/cli/thrift/ThriftCLIServiceTest.java
Patch:
@@ -25,6 +25,7 @@
 import java.util.Map;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.MetaStoreTestUtils;
 import org.apache.hive.service.Service;
 import org.apache.hive.service.cli.OperationHandle;
@@ -62,7 +63,7 @@ public static void setUpBeforeClass() throws Exception {
     // Find a free port
     port = MetaStoreTestUtils.findFreePort();
     hiveServer2 = new HiveServer2();
-    hiveConf = new HiveConf();
+    hiveConf = new HiveConfForTest(ThriftCLIServiceTest.class);
   }
 
   /**

File: streaming/src/test/org/apache/hive/streaming/TestStreamingDynamicPartitioning.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConfForTest;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
@@ -126,7 +127,7 @@ public FileStatus getFileStatus(Path path) throws IOException {
   private final static String dbName2 = "testing2";
 
   public TestStreamingDynamicPartitioning() throws Exception {
-    conf = new HiveConf(this.getClass());
+    conf = new HiveConfForTest(getClass());
     conf.set("fs.raw.impl", RawFileSystem.class.getName());
     conf
       .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFHex.java
Patch:
@@ -39,7 +39,7 @@
     + "If the argument is a number or binary, returns the hexadecimal representation.\n"
     + "Example:\n"
     + "  > SELECT _FUNC_(17) FROM src LIMIT 1;\n"
-    + "  'H1'\n"
+    + "  '11'\n"
     + "  > SELECT _FUNC_('Facebook') FROM src LIMIT 1;\n"
     + "  '46616365626F6F6B'")
 @VectorizedExpressions({FuncHex.class})

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/monitoring/TezJobMonitor.java
Patch:
@@ -124,17 +124,17 @@ public TezJobMonitor(List<BaseWork> topSortedWorks, final DAGClient dagClient, H
     this.dag = dag;
     this.context = ctx;
     console = SessionState.getConsole();
+    this.perfLogger = perfLogger;
     updateFunction = updateFunction();
     this.counters = counters;
-    this.perfLogger = perfLogger;
   }
 
   private RenderStrategy.UpdateFunction updateFunction() {
     return InPlaceUpdate.canRenderInPlace(hiveConf)
         && !SessionState.getConsole().getIsSilent()
         && !SessionState.get().isHiveServerQuery()
-        ? new RenderStrategy.InPlaceUpdateFunction(this)
-        : new RenderStrategy.LogToFileFunction(this);
+        ? new RenderStrategy.InPlaceUpdateFunction(this, perfLogger)
+        : new RenderStrategy.LogToFileFunction(this, perfLogger);
   }
 
   private boolean isProfilingEnabled() {

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java
Patch:
@@ -47,7 +47,7 @@
  * An implementation of {@link HiveTxnManager} that does not support
  * transactions.  This provides default Hive behavior.
  */
-class DummyTxnManager extends HiveTxnManagerImpl {
+public class DummyTxnManager extends HiveTxnManagerImpl {
   static final private Logger LOG =
       LoggerFactory.getLogger(DummyTxnManager.class.getName());
 

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorFilterCompare.java
Patch:
@@ -84,6 +84,7 @@ public TestVectorFilterCompare() {
     // Arithmetic operations rely on getting conf from SessionState, need to initialize here.
     SessionState ss = new SessionState(new HiveConf());
     ss.getConf().setVar(HiveConf.ConfVars.HIVE_COMPAT, "latest");
+    ss.getConf().setBoolVar(HiveConf.ConfVars.HIVE_STRICT_TIMESTAMP_CONVERSION, false);
     SessionState.setCurrentSessionState(ss);
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java
Patch:
@@ -102,8 +102,7 @@ private ColumnStatistics constructColumnStatsFromInput()
 
     ColumnStatisticsData statsData = new ColumnStatisticsData();
 
-    if (columnType.equalsIgnoreCase("long")
-        || columnType.equalsIgnoreCase(serdeConstants.TINYINT_TYPE_NAME)
+    if (columnType.equalsIgnoreCase(serdeConstants.TINYINT_TYPE_NAME)
         || columnType.equalsIgnoreCase(serdeConstants.SMALLINT_TYPE_NAME)
         || columnType.equalsIgnoreCase(serdeConstants.INT_TYPE_NAME)
         || columnType.equalsIgnoreCase(serdeConstants.BIGINT_TYPE_NAME)) {

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/show/ShowPartitionsOperation.java
Patch:
@@ -58,7 +58,8 @@ public int execute() throws HiveException {
     if (tbl.isNonNative() && tbl.getStorageHandler().supportsPartitionTransform()) {
       parts = tbl.getStorageHandler().showPartitions(context, tbl);
     } else if (!tbl.isPartitioned()) {
-      throw new HiveException(ErrorMsg.TABLE_NOT_PARTITIONED, desc.getTabName());
+      context.getTask().setException(new HiveException(ErrorMsg.TABLE_NOT_PARTITIONED, desc.getTabName()));
+      return ErrorMsg.TABLE_NOT_PARTITIONED.getErrorCode();
     } else if (desc.getCond() != null || desc.getOrder() != null) {
       parts = getPartitionNames(tbl);
     } else if (desc.getPartSpec() != null) {

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HMSHandler.java
Patch:
@@ -1677,8 +1677,8 @@ private void drop_database_core(RawStore ms, DropDatabaseRequest req) throws NoS
             }
             // For each partition in each table, drop the partitions and get a list of
             // partitions' locations which might need to be deleted
-            partitionPaths = dropPartitionsAndGetLocations(ms, req.getCatalogName(), req.getName(), table.getTableName(),
-                tablePath, tableDataShouldBeDeleted);
+            partitionPaths.addAll(dropPartitionsAndGetLocations(ms, req.getCatalogName(), req.getName(), table.getTableName(),
+                tablePath, tableDataShouldBeDeleted));
             
             EnvironmentContext context = null;
             if (isSoftDelete) {

File: llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
Patch:
@@ -1716,7 +1716,7 @@ protected void registerRunningTask(TaskInfo taskInfo) {
         isGuaranteed ? guaranteedTasks : speculativeTasks;
     writeLock.lock();
     try {
-      WM_LOG.info("Registering " + taskInfo.attemptId + "; " + taskInfo.isGuaranteed);
+      WM_LOG.info("Registering {}, guaranteed: {}", taskInfo.attemptId, taskInfo.isGuaranteed);
       addToRunningTasksMap(runningTasks, taskInfo);
       if (metrics != null) {
         metrics.decrPendingTasksCount();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveTableScan.java
Patch:
@@ -150,7 +150,7 @@ private HiveTableScan(RelOptCluster cluster, RelTraitSet traitSet, RelOptHiveTab
       String alias, String concatQbIDAlias, RelDataType newRowtype, boolean useQBIdInDigest, boolean insideView,
       HiveTableScanTrait tableScanTrait) {
     super(cluster, TraitsUtil.getDefaultTraitSet(cluster), table);
-    assert getConvention() == HiveRelNode.CONVENTION;
+    assert getTraitSet().containsIfApplicable(HiveRelNode.CONVENTION);
     this.tblAlias = alias;
     this.concatQbIDAlias = concatQbIDAlias;
     this.hiveTableScanRowType = newRowtype;

File: hplsql/src/main/java/org/apache/hive/hplsql/Exec.java
Patch:
@@ -1635,7 +1635,7 @@ String createLocalUdf() {
    */
   @Override 
   public Integer visitAssignment_stmt_single_item(HplsqlParser.Assignment_stmt_single_itemContext ctx) { 
-    String name = ctx.ident().getText();
+    String name = ctx.qident().getText();
     visit(ctx.expr());    
     Var var = setVariable(name);
     StringBuilder assignments = new StringBuilder();

File: jdbc/src/java/org/apache/hive/jdbc/Utils.java
Patch:
@@ -617,8 +617,8 @@ public static JdbcConnectionParams extractURLComponents(String uri, Properties i
           if (port <= 0) {
             port = Integer.parseInt(Utils.DEFAULT_PORT);
           }
-          connParams.setHost(jdbcBaseURI.getHost());
-          connParams.setPort(jdbcBaseURI.getPort());
+          connParams.setHost(host);
+          connParams.setPort(port);
         }
         // We check for invalid host, port while configuring connParams with configureConnParams()
         authorityStr = connParams.getHost() + ":" + connParams.getPort();

File: jdbc/src/test/org/apache/hive/jdbc/TestHiveConnection.java
Patch:
@@ -73,5 +73,8 @@ public void testHiveConnectionParameters() throws SQLException, ZooKeeperHiveCli
     Assert.assertEquals("cliservice", params.getSessionVars().get(JdbcConnectionParams.HTTP_PATH));
     Assert.assertEquals("60", params.getSessionVars().get(JdbcConnectionParams.SOCKET_TIMEOUT));
     Assert.assertEquals("true", params.getSessionVars().get(JdbcConnectionParams.JDBC_PARAM_REQUEST_TRACK));
+
+    JdbcConnectionParams nonPortParams = Utils.parseURL("jdbc:hive2://hello.host/default");
+    Assert.assertEquals(Integer.parseInt(Utils.DEFAULT_PORT), nonPortParams.getPort());
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/HiveTotalOrderPartitioner.java
Patch:
@@ -1,6 +1,4 @@
 /*
- * Copyright 2010 The Apache Software Foundation
- *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -17,7 +15,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.hadoop.hive.ql.exec;
 
 import org.slf4j.Logger;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java
Patch:
@@ -1,6 +1,4 @@
 /*
- * Copyright 2010 The Apache Software Foundation
- *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -17,7 +15,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.hadoop.hive.ql.exec;
 
 import java.io.IOException;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTotalOrderPartitioner.java
Patch:
@@ -1,6 +1,4 @@
 /*
- * Copyright 2010 The Apache Software Foundation
- *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -17,7 +15,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.hadoop.hive.ql.exec.tez;
 
 import org.slf4j.Logger;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SamplingOptimizer.java
Patch:
@@ -1,6 +1,4 @@
 /*
- * Copyright 2010 The Apache Software Foundation
- *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -17,7 +15,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.hadoop.hive.ql.optimizer.physical;
 
 import org.apache.hadoop.hive.ql.exec.GroupByOperator;

File: serde/src/java/org/apache/hadoop/hive/serde2/MultiDelimitSerDe.java
Patch:
@@ -1,6 +1,4 @@
 /*
- * Copyright 2010 The Apache Software Foundation
- *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/jdbc/functions/HeartbeatTxnRangeFunction.java
Patch:
@@ -82,7 +82,7 @@ public HeartbeatTxnRangeResponse execute(MultiDataSourceJdbcResource jdbcResourc
     }
     if (updateCnt == numTxnsToHeartbeat) {
       //fast pass worked, i.e. all txns we were asked to heartbeat were Open as expected
-      context.rollbackToSavepoint(savePoint);
+      context.createSavepoint();
       return rsp;
     }
     //if here, do the slow path so that we can return info txns which were not in expected state

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLPlanUtils.java
Patch:
@@ -89,6 +89,7 @@
 
 import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE;
 import static org.apache.hadoop.hive.ql.metadata.HiveUtils.unparseIdentifier;
+import static org.apache.hadoop.hive.serde.serdeConstants.UNION_TYPE_NAME;
 
 public class DDLPlanUtils {
   private static final String EXTERNAL = "external";
@@ -964,7 +965,7 @@ public static String formatType(TypeInfo typeInfo) {
           String unionElementType = formatType(unionElementTypeInfo);
           unionFormattedType.append(unionElementType);
         }
-        return "uniontype<" + unionFormattedType.toString() + ">";
+        return UNION_TYPE_NAME + "<" + unionFormattedType.toString() + ">";
       default:
         throw new RuntimeException("Unknown type: " + typeInfo.getCategory());
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedBatchUtil.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.common.type.Timestamp;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DateWritableV2;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@@ -116,7 +117,7 @@ public static ColumnVector createColumnVector(String typeName,
     typeName = typeName.toLowerCase();
 
     // Allow undecorated CHAR and VARCHAR to support scratch column type names.
-    if (typeName.equals("char") || typeName.equals("varchar")) {
+    if (typeName.equals(serdeConstants.CHAR_TYPE_NAME) || typeName.equals(serdeConstants.VARCHAR_TYPE_NAME)) {
       return new BytesColumnVector(VectorizedRowBatch.DEFAULT_SIZE);
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDateToBoolean.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
+import org.apache.hadoop.hive.serde.serdeConstants;
 
 /*
  * Comment from BooleanWritable evaluate(DateWritable d)
@@ -54,7 +55,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("date"))
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.DATE_TYPE_NAME))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDateToTimestamp.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.io.DateWritableV2;
 
 public class CastDateToTimestamp extends VectorExpression {
@@ -145,7 +146,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("date"))
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.DATE_TYPE_NAME))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDoubleToTimestamp.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.util.TimestampUtils;
+import org.apache.hadoop.hive.serde.serdeConstants;
 
 public class CastDoubleToTimestamp extends VectorExpression {
   private static final long serialVersionUID = 1L;
@@ -155,7 +156,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("double"))
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.DOUBLE_TYPE_NAME))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToBoolean.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
 
 public class CastTimestampToBoolean extends VectorExpression {
   private static final long serialVersionUID = 1L;
@@ -121,7 +122,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"))
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.TIMESTAMP_TYPE_NAME))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDouble.java
Patch:
@@ -22,6 +22,7 @@
 
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
 
 public class CastTimestampToDouble extends VectorExpression {
   private static final long serialVersionUID = 1L;
@@ -142,7 +143,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"))
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.TIMESTAMP_TYPE_NAME))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToLong.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 
@@ -183,7 +184,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"))
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.TIMESTAMP_TYPE_NAME))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateColSubtractDateColumn.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.io.DateWritableV2;
 
 // A type date (LongColumnVector storing epoch days) minus a type date produces a
@@ -159,8 +160,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("date"),
-            VectorExpressionDescriptor.ArgumentType.getType("date"))
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.DATE_TYPE_NAME),
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.DATE_TYPE_NAME))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateColSubtractDateScalar.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.io.DateWritableV2;
 
 // A type date (LongColumnVector storing epoch days) minus a type date produces a
@@ -170,8 +171,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("date"),
-            VectorExpressionDescriptor.ArgumentType.getType("date"))
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.DATE_TYPE_NAME),
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.DATE_TYPE_NAME))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateScalarSubtractDateColumn.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.io.DateWritableV2;
 
 // A type date (LongColumnVector storing epoch days) minus a type date produces a
@@ -153,8 +154,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("date"),
-            VectorExpressionDescriptor.ArgumentType.getType("date"))
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.DATE_TYPE_NAME),
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.DATE_TYPE_NAME))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.SCALAR,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprDoubleColumnDoubleColumn.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
 
 /**
  * Compute IF(expr1, expr2, expr3) for 3 input column expressions.
@@ -159,8 +160,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
         .setNumArguments(3)
         .setArgumentTypes(
             VectorExpressionDescriptor.ArgumentType.getType("long"),
-            VectorExpressionDescriptor.ArgumentType.getType("double"),
-            VectorExpressionDescriptor.ArgumentType.getType("double"))
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.DOUBLE_TYPE_NAME),
+            VectorExpressionDescriptor.ArgumentType.getType(serdeConstants.DOUBLE_TYPE_NAME))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.COLUMN,

File: ql/src/java/org/apache/hadoop/hive/ql/io/IOConstants.java
Patch:
@@ -39,7 +39,7 @@ public final class IOConstants {
 
   /**
    * The desired TABLE column names and types for input format schema evolution.
-   * This is different than COLUMNS and COLUMNS_TYPES, which are based on individual partition
+   * This is different from COLUMNS and COLUMNS_TYPES, which are based on individual partition
    * metadata.
    *
    * Virtual columns and partition columns are not included

File: ql/src/java/org/apache/hadoop/hive/ql/io/RCFileOutputFormat.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
@@ -124,7 +125,7 @@ public org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter getHiveRecor
       boolean isCompressed, Properties tableProperties, Progressable progress) throws IOException {
 
     String[] cols = null;
-    String columns = tableProperties.getProperty("columns");
+    String columns = tableProperties.getProperty(serdeConstants.LIST_COLUMNS);
     if (columns == null || columns.trim().equals("")) {
       cols = new String[0];
     } else {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSerde.java
Patch:
@@ -97,7 +97,7 @@ public void initialize(Configuration configuration, Properties tableProperties,
   }
 
   /**
-   * NOTE: if "columns.types" is missing, all columns will be of String type.
+   * NOTE: if {@link serdeConstants#LIST_COLUMN_TYPES} is missing, all columns will be of String type.
    */
   @Override
   protected List<TypeInfo> parseColumnTypes() {
@@ -163,7 +163,7 @@ private String convertOrcTypeToFieldType(TypeDescription fieldType) {
   }
 
   private String convertPrimitiveType(TypeDescription fieldType) {
-    if (fieldType.getCategory().getName().equals("timestamp with local time zone")) {
+    if (fieldType.getCategory().getName().equals(serdeConstants.TIMESTAMPLOCALTZ_TYPE_NAME)) {
       throw new IllegalArgumentException("Unhandled ORC type " + fieldType.getCategory().getName());
     }
     return fieldType.toString();

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveCollectionConverter.java
Patch:
@@ -31,6 +31,8 @@
 import org.apache.parquet.schema.GroupType;
 import org.apache.parquet.schema.Type;
 
+import static org.apache.hadoop.hive.serde.serdeConstants.LIST_TYPE_NAME;
+
 public class HiveCollectionConverter extends HiveGroupConverter {
   private final GroupType collectionType;
   private final ConverterParent parent;
@@ -184,7 +186,7 @@ private static boolean isElementType(Type repeatedType, String parentName) {
     if (repeatedType.isPrimitive() ||
         (repeatedType.asGroupType().getFieldCount() != 1)) {
       return true;
-    } else if (repeatedType.getName().equals("array")) {
+    } else if (repeatedType.getName().equals(LIST_TYPE_NAME)) {
       return true; // existing avro data
     } else if (repeatedType.getName().equals(parentName + "_tuple")) {
       return true; // existing thrift data

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
Patch:
@@ -422,7 +422,7 @@ public List<FieldSchema> readSchema(Configuration conf, String file) throws SerD
       MessageType msg = metadata.getSchema();
       List<FieldSchema> schema = new ArrayList<>();
       String inferBinaryAsStringValue = conf.get(HiveConf.ConfVars.HIVE_PARQUET_INFER_BINARY_AS.varname);
-      boolean inferBinaryAsString = "string".equalsIgnoreCase(inferBinaryAsStringValue);
+      boolean inferBinaryAsString = serdeConstants.STRING_TYPE_NAME.equalsIgnoreCase(inferBinaryAsStringValue);
 
       for (Type field: msg.getFields()) {
         FieldSchema fieldSchema = convertParquetTypeToFieldSchema(field, inferBinaryAsString);

File: ql/src/java/org/apache/hadoop/hive/ql/io/sarg/ConvertAstToSearchArg.java
Patch:
@@ -51,6 +51,7 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotNull;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
@@ -439,7 +440,7 @@ private void parse(ExprNodeDesc expression) {
       // if it is a reference to a boolean column, covert it to a truth test.
       if (expression instanceof ExprNodeColumnDesc) {
         ExprNodeColumnDesc columnDesc = (ExprNodeColumnDesc) expression;
-        if (columnDesc.getTypeString().equals("boolean")) {
+        if (columnDesc.getTypeString().equals(serdeConstants.BOOLEAN_TYPE_NAME)) {
           builder.equals(columnDesc.getColumn(), PredicateLeaf.Type.BOOLEAN,
               true);
           return;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
Patch:
@@ -38,6 +38,7 @@
 import org.apache.hadoop.fs.RemoteIterator;
 import org.apache.hadoop.hive.common.StatsSetupConst;
 import org.apache.hadoop.hive.ql.stats.StatsUtils;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.fs.FileSystem;
@@ -231,7 +232,7 @@ private FetchData checkTree(boolean aggressive, ParseContext pctx, String alias,
       if (op instanceof FilterOperator) {
         ExprNodeDesc predicate = ((FilterOperator) op).getConf().getPredicate();
         if (predicate instanceof ExprNodeConstantDesc
-                && "boolean".equals(predicate.getTypeInfo().getTypeName())) {
+                && serdeConstants.BOOLEAN_TYPE_NAME.equals(predicate.getTypeInfo().getTypeName())) {
           continue;
         } else if (PartitionPruner.onlyContainsPartnCols(table, predicate)) {
           continue;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -1954,8 +1954,8 @@ protected FetchTask createFetchTask(String tableSchema) {
     prop.setProperty(serdeConstants.SERIALIZATION_FORMAT, Integer.toString(Utilities.tabCode));
     prop.setProperty(serdeConstants.SERIALIZATION_NULL_FORMAT, " ");
     String[] colTypes = schema.split("#");
-    prop.setProperty("columns", colTypes[0]);
-    prop.setProperty("columns.types", colTypes[1]);
+    prop.setProperty(serdeConstants.LIST_COLUMNS, colTypes[0]);
+    prop.setProperty(serdeConstants.LIST_COLUMN_TYPES, colTypes[1]);
     prop.setProperty(serdeConstants.SERIALIZATION_LIB, LazySimpleSerDe.class.getName());
     prop.setProperty(hive_metastoreConstants.TABLE_BUCKETING_VERSION, "-1");
     FetchWork fetch =

File: ql/src/java/org/apache/hadoop/hive/ql/parse/rewrite/MergeRewriter.java
Patch:
@@ -41,6 +41,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.function.UnaryOperator;
+import org.apache.hadoop.hive.serde.serdeConstants;
 
 import static org.apache.commons.lang3.StringUtils.isNotBlank;
 
@@ -141,7 +142,7 @@ private void handleCardinalityViolation(
         Table table = db.newTable(tableName);
         table.setSerializationLib(format.getSerde());
         List<FieldSchema> fields = new ArrayList<>();
-        fields.add(new FieldSchema("val", "int", null));
+        fields.add(new FieldSchema("val", serdeConstants.INT_TYPE_NAME, null));
         table.setFields(fields);
         table.setDataLocation(Warehouse.getDnsPath(new Path(SessionState.get().getTempTableSpace(),
             tableName), conf));

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java
Patch:
@@ -217,7 +217,7 @@ public Map getPropertiesExplain() {
   }
 
   public void setProperties(final Properties properties) {
-    properties.remove("columns.comments");
+    properties.remove(serdeConstants.LIST_COLUMN_COMMENTS);
     if (properties instanceof CopyOnFirstWriteProperties) {
       this.properties = properties;
     } else {

File: ql/src/java/org/apache/hadoop/hive/ql/processors/DfsProcessor.java
Patch:
@@ -23,6 +23,7 @@
 import java.util.ArrayList;
 import java.util.Map;
 
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -56,7 +57,7 @@ public DfsProcessor(Configuration conf) {
   public DfsProcessor(Configuration conf, boolean addSchema) {
     dfs = new FsShell(conf);
     dfsSchema = new Schema();
-    dfsSchema.addToFieldSchemas(new FieldSchema(DFS_RESULT_HEADER, "string", ""));
+    dfsSchema.addToFieldSchemas(new FieldSchema(DFS_RESULT_HEADER, serdeConstants.STRING_TYPE_NAME, ""));
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/processors/LlapCacheResourceProcessor.java
Patch:
@@ -51,6 +51,7 @@
 import org.apache.hadoop.hive.metastore.api.Schema;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType;
 import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.io.retry.RetryPolicies;
 import org.apache.hadoop.io.retry.RetryPolicy;
 import org.apache.hadoop.net.NetUtils;
@@ -122,8 +123,8 @@ private CommandProcessorResponse llapCacheCommandHandler(SessionState ss, String
 
   private Schema getSchema() {
     Schema sch = new Schema();
-    sch.addToFieldSchemas(new FieldSchema("hostName", "string", ""));
-    sch.addToFieldSchemas(new FieldSchema("purgedMemoryBytes", "string", ""));
+    sch.addToFieldSchemas(new FieldSchema("hostName", serdeConstants.STRING_TYPE_NAME, ""));
+    sch.addToFieldSchemas(new FieldSchema("purgedMemoryBytes", serdeConstants.STRING_TYPE_NAME, ""));
     sch.putToProperties(SERIALIZATION_NULL_FORMAT, defaultNullString);
     return sch;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/esri/serde/BaseJsonSerDe.java
Patch:
@@ -28,6 +28,7 @@
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.core.JsonToken;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeStats;
@@ -110,7 +111,7 @@ public void initialize(Configuration cfg, Properties tbl, Properties partitionPr
         throw new SerDeException("Only primitive field types are accepted");
       }
 
-      if (colTypeInfo.getTypeName().equals("binary")) {
+      if (colTypeInfo.getTypeName().equals(serdeConstants.BINARY_TYPE_NAME)) {
 
         if (geometryColumn >= 0) {
           // only one column can be defined as binary for geometries

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBetween.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
@@ -39,7 +40,7 @@ public class GenericUDFBetween extends GenericUDF {
 
   @Override
   public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
-    if (!arguments[0].getTypeName().equals("boolean")) {
+    if (!arguments[0].getTypeName().equals(serdeConstants.BOOLEAN_TYPE_NAME)) {
       throw new UDFArgumentTypeException(0, "First argument for BETWEEN should be boolean type");
     }
     egt.initialize(new ObjectInspector[] {arguments[1], arguments[2]});

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDirectSqlUtils.java
Patch:
@@ -554,7 +554,6 @@ static Boolean extractSqlBoolean(Object value) throws MetaException {
         return true;
       }
     }
-    LOG.debug("Value is of type {}", value.getClass());
     throw new MetaException("Cannot extract boolean from column value " + value);
   }
 
@@ -590,8 +589,8 @@ else if (value instanceof byte[]) {
       return (byte[]) value;
     }
 	else {
-      // this may happen when enablebitvector is false
-      LOG.debug("Expected blob type but got " + value.getClass().getName());
+      // org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getStatsList(enableBitVector,enableKll)
+      // We get here when enableBitvector or enableKll is false
       return null;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java
Patch:
@@ -51,7 +51,7 @@
  * A class to initiate compactions.  This will run in a separate thread.
  * It's critical that there exactly 1 of these in a given warehouse.
  */
-public class Initiator extends InitiatorBase {
+public class Initiator extends MetaStoreCompactorThread {
   static final private String CLASS_NAME = Initiator.class.getName();
   static final private Logger LOG = LoggerFactory.getLogger(CLASS_NAME);
 
@@ -153,11 +153,11 @@ public void run() {
                * Therefore, using a thread pool here and running checkForCompactions in parallel */
               String tableName = ci.getFullTableName();
               String partition = ci.getFullPartitionName();
-
+              ci.initiatorVersion = this.runtimeVersion;
               CompletableFuture<Void> asyncJob =
                   CompletableFuture.runAsync(
                           CompactorUtil.ThrowingRunnable.unchecked(() ->
-                              scheduleCompactionIfRequired(ci, t, p, runAs, metricsEnabled)), compactionExecutor)
+                                  CompactorUtil.scheduleCompactionIfRequired(ci, t, p, runAs, metricsEnabled, hostName, txnHandler, conf)), compactionExecutor)
                       .exceptionally(exc -> {
                         LOG.error("Error while running scheduling the compaction on the table {} / partition {}.", tableName, partition, exc);
                         return null;

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestMaterializedViewsCache.java
Patch:
@@ -39,6 +39,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable;
+import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.IncrementalRebuildMode;
 import org.apache.hadoop.hive.ql.parse.ASTNode;
 import org.apache.hadoop.hive.ql.parse.HiveParser;
 import org.apache.hadoop.hive.ql.parse.ParseDriver;
@@ -158,7 +159,7 @@ private static HiveRelOptMaterialization createMaterialization(Table table) thro
     return new HiveRelOptMaterialization(
             new DummyRel(table), new DummyRel(table), null, asList(table.getDbName(), table.getTableName()),
             RewriteAlgorithm.ALL,
-            HiveRelOptMaterialization.IncrementalRebuildMode.AVAILABLE, ParseUtils.parse(table.getViewExpandedText(), null));
+            IncrementalRebuildMode.AVAILABLE, ParseUtils.parse(table.getViewExpandedText(), null));
   }
 
   @Test

File: ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/TestHivePushdownSnapshotFilterRule.java
Patch:
@@ -43,7 +43,7 @@ public class TestHivePushdownSnapshotFilterRule extends TestRuleBase {
 
   @Test
   public void testFilterIsRemovedAndVersionIntervalFromIsSetWhenFilterHasSnapshotIdPredicate() {
-    RelNode tableScan = createT2IcebergTS();
+    RelNode tableScan = createNonNativeTSSupportingSnapshots();
 
     RelBuilder relBuilder = HiveRelFactories.HIVE_BUILDER.create(relOptCluster, schemaMock);
     RelNode root = relBuilder.push(tableScan)
@@ -64,7 +64,7 @@ public void testFilterIsRemovedAndVersionIntervalFromIsSetWhenFilterHasSnapshotI
 
   @Test
   public void testFilterLeftIntactWhenItDoesNotHaveSnapshotIdPredicate() {
-    RelNode tableScan = createT2IcebergTS();
+    RelNode tableScan = createNonNativeTS();
 
     RelBuilder relBuilder = HiveRelFactories.HIVE_BUILDER.create(relOptCluster, schemaMock);
     RelNode root = relBuilder.push(tableScan)

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/FixedBucketPruningOptimizer.java
Patch:
@@ -202,7 +202,7 @@ protected void generatePredicate(NodeProcessorCtx procCtx,
       BitSet bs = new BitSet(numBuckets);
       bs.clear();
       PrimitiveObjectInspector bucketOI = (PrimitiveObjectInspector)bucketField.getFieldObjectInspector();
-      PrimitiveObjectInspector constOI = PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(bucketOI.getPrimitiveCategory());
+      PrimitiveObjectInspector constOI = PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(bucketOI.getTypeInfo());
       // Fetch the bucketing version from table scan operator
       int bucketingVersion = top.getConf().getTableMetadata().getBucketingVersion();
 

File: common/src/test/org/apache/hadoop/hive/ql/log/PerfLoggerTest.java
Patch:
@@ -55,7 +55,6 @@ public void testMT() throws InterruptedException {
     AtomicInteger count = new AtomicInteger(0);
     // getEndTimes in a loop
     executorService.execute(() -> {
-      PerfLogger.setPerfLogger(pl);
       try {
         count.incrementAndGet();
         snooze(100);
@@ -76,7 +75,6 @@ public void testMT() throws InterruptedException {
       executorService.execute(() -> {
         try {
           int cnt = count.incrementAndGet();
-          PerfLogger.setPerfLogger(pl);
           for (int i = 0; i < 64; ++i) {
             pl.perfLogBegin("test", PerfLogger.COMPILE + "_ "+  cnt + "_" + i);
             snooze(50);

File: service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
Patch:
@@ -322,7 +322,7 @@ public Object run() throws HiveSQLException {
           Hive.set(parentHive);
           // TODO: can this result in cross-thread reuse of session state?
           SessionState.setCurrentSessionState(parentSessionState);
-          PerfLogger.setPerfLogger(SessionState.getPerfLogger());
+          SessionState.getPerfLogger();
           if (!embedded) {
             LogUtils.registerLoggingContext(queryState.getConf());
           }

File: service/src/java/org/apache/hive/service/cli/operation/hplsql/HplSqlOperation.java
Patch:
@@ -199,7 +199,7 @@ public void run() {
         assert (!parentHive.allowClose());
         Hive.set(parentHive);
         SessionState.setCurrentSessionState(parentSessionState);
-        PerfLogger.setPerfLogger(SessionState.getPerfLogger());
+        SessionState.getPerfLogger();
         LogUtils.registerLoggingContext(queryState.getConf());
         ShimLoader.getHadoopShims()
             .setHadoopQueryContext(String.format(USER_ID, queryState.getQueryId(), parentSessionState.getUserName()));

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4652,7 +4652,7 @@ public static enum ConfVars {
         "The default value is true."),
     HIVE_VECTORIZATION_USE_ROW_DESERIALIZE("hive.vectorized.use.row.serde.deserialize", true,
         "This flag should be set to true to enable vectorizing using row deserialize.\n" +
-        "The default value is false."),
+        "The default value is true."),
     HIVE_VECTORIZATION_ROW_DESERIALIZE_INPUTFORMAT_EXCLUDES(
         "hive.vectorized.row.serde.inputformat.excludes",
         "org.apache.parquet.hadoop.ParquetInputFormat,org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat",

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1503,7 +1503,7 @@ public static enum ConfVars {
     @Deprecated
     METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES(
         "hive.metastore.disallow.incompatible.col.type.changes", true,
-        "If true (default is false), ALTER TABLE operations which change the type of a\n" +
+        "If true (default is true), ALTER TABLE operations which change the type of a\n" +
         "column (say STRING) to an incompatible type (say MAP) are disallowed.\n" +
         "RCFile default SerDe (ColumnarSerDe) serializes the values in such a way that the\n" +
         "datatypes can be converted from string to any type. The map is also serialized as\n" +

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java
Patch:
@@ -125,7 +125,7 @@ public void updateExpressionProxy(String proxyClass) throws TException {
    * @param msckInfo Information about the tables and partitions we want to check for.
    * @return Returns 0 when execution succeeds and above 0 if it fails.
    */
-  public int repair(MsckInfo msckInfo) {
+  public int repair(MsckInfo msckInfo) throws TException, MetastoreException, IOException {
     CheckResult result = null;
     List<String> repairOutput = new ArrayList<>();
     String qualifiedTableName = null;
@@ -271,6 +271,7 @@ public int repair(MsckInfo msckInfo) {
     } catch (Exception e) {
       LOG.warn("Failed to run metacheck: ", e);
       success = false;
+      throw e;
     } finally {
       if (result != null) {
         logResult(result);

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
Patch:
@@ -391,7 +391,7 @@ private CloseableIterable<T> openVectorized(FileScanTask task, Schema readSchema
           "Vectorized read is unsupported for Hive 2 integration.");
 
       Path path = new Path(task.file().path().toString());
-      Map<Integer, ?> idToConstant = constantsMap(task, IdentityPartitionConverters::convertConstant);
+      Map<Integer, ?> idToConstant = constantsMap(task, HiveIdentityPartitionConverters::convertConstant);
       Expression residual = HiveIcebergInputFormat.residualForTask(task, context.getConfiguration());
 
       // TODO: We have to take care of the EncryptionManager when LLAP and vectorization is used
@@ -544,7 +544,8 @@ private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask ta
         Types.StructType partitionType = Partitioning.partitionType(table);
         return PartitionUtil.constantsMap(task, partitionType, converter);
       } else if (projectsIdentityPartitionColumns) {
-        return PartitionUtil.constantsMap(task, converter);
+        Types.StructType partitionType = Partitioning.partitionType(table);
+        return PartitionUtil.constantsMap(task, partitionType, converter);
       } else {
         return Collections.emptyMap();
       }

File: llap-server/src/java/org/apache/hadoop/hive/llap/cli/service/AsyncTaskCopyAuxJars.java
Patch:
@@ -45,8 +45,8 @@ class AsyncTaskCopyAuxJars implements Callable<Void> {
 
   private static final String[] DEFAULT_AUX_CLASSES =
       new String[] {"org.apache.hive.hcatalog.data.JsonSerDe", "org.apache.hadoop.hive.druid.DruidStorageHandler",
-          "org.apache.hive.storage.jdbc.JdbcStorageHandler", "org.apache.commons.dbcp.BasicDataSourceFactory",
-          "org.apache.commons.pool.impl.GenericObjectPool", "org.apache.hadoop.hive.kafka.KafkaStorageHandler",
+          "org.apache.hive.storage.jdbc.JdbcStorageHandler", "org.apache.commons.dbcp2.BasicDataSourceFactory",
+          "org.apache.commons.pool2.impl.GenericObjectPool", "org.apache.hadoop.hive.kafka.KafkaStorageHandler",
           "org.apache.hadoop.hive.kudu.KuduStorageHandler"};
   private static final String HBASE_SERDE_CLASS = "org.apache.hadoop.hive.hbase.HBaseSerDe";
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java
Patch:
@@ -384,8 +384,8 @@ private void addToCurrentFunctions(String functionName, FunctionInfo functionInf
   public WindowFunctionInfo getWindowFunctionInfo(String functionName) throws SemanticException {
     // First try without qualifiers - would resolve builtin/temp functions
     FunctionInfo info = getFunctionInfo(WINDOW_FUNC_PREFIX + functionName);
-    // Try qualifying with current db name for permanent functions
-    if (info == null) {
+    // Try qualifying with current db name for permanent functions and try register function to session
+    if (info == null && FunctionRegistry.getFunctionInfo(functionName) != null) {
       String qualifiedName = FunctionUtils.qualifyFunctionName(
               functionName, SessionState.get().getCurrentDatabase().toLowerCase());
       info = getFunctionInfo(WINDOW_FUNC_PREFIX + qualifiedName);

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/tools/schematool/TestSchemaToolForMetastore.java
Patch:
@@ -46,6 +46,7 @@
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.metastore.dbinstall.rules.DatabaseRule;
 import org.apache.hadoop.hive.metastore.dbinstall.rules.Derby;
+import org.apache.hadoop.hive.metastore.dbinstall.rules.Mariadb;
 import org.apache.hadoop.hive.metastore.dbinstall.rules.Mssql;
 import org.apache.hadoop.hive.metastore.dbinstall.rules.Mysql;
 import org.apache.hadoop.hive.metastore.dbinstall.rules.Oracle;
@@ -85,7 +86,7 @@ public static Collection<Object[]> databases() {
     dbs.add(new Object[] { new Mysql() });
     dbs.add(new Object[] { new Oracle() });
     dbs.add(new Object[] { new Postgres() });
-//    dbs.add(new Object[] { new Mariadb() }); Disabled due to HIVE-27749
+    dbs.add(new Object[] { new Mariadb() });
     dbs.add(new Object[] { new Mssql() });
     return dbs;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
Patch:
@@ -258,7 +258,7 @@ public int execute() {
         LogUtils.putToMDC(LogUtils.DAGID_KEY, dagId);
 
         // finally monitor will print progress until the job is done
-        TezJobMonitor monitor = new TezJobMonitor(work.getAllWork(), dagClient, conf, dag, ctx, counters);
+        TezJobMonitor monitor = new TezJobMonitor(work.getAllWork(), dagClient, conf, dag, ctx, counters, perfLogger);
         rc = monitor.monitorExecution();
 
         if (rc != 0) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/monitoring/TezJobMonitor.java
Patch:
@@ -77,7 +77,7 @@ public class TezJobMonitor {
   private static final int MAX_RETRY_INTERVAL = 2500;
   private static final int MAX_RETRY_FAILURES = (MAX_RETRY_INTERVAL / MAX_CHECK_INTERVAL) + 1;
 
-  private final PerfLogger perfLogger = SessionState.getPerfLogger();
+  private final PerfLogger perfLogger;
   private static final List<DAGClient> shutdownList;
   private final List<BaseWork> topSortedWorks;
 
@@ -117,7 +117,7 @@ public static void initShutdownHook() {
   private final TezCounters counters;
 
   public TezJobMonitor(List<BaseWork> topSortedWorks, final DAGClient dagClient, HiveConf conf, DAG dag,
-    Context ctx, final TezCounters counters) {
+    Context ctx, final TezCounters counters, PerfLogger perfLogger) {
     this.topSortedWorks = topSortedWorks;
     this.dagClient = dagClient;
     this.hiveConf = conf;
@@ -126,6 +126,7 @@ public TezJobMonitor(List<BaseWork> topSortedWorks, final DAGClient dagClient, H
     console = SessionState.getConsole();
     updateFunction = updateFunction();
     this.counters = counters;
+    this.perfLogger = perfLogger;
   }
 
   private RenderStrategy.UpdateFunction updateFunction() {

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDefaultTransformer.java
Patch:
@@ -772,7 +772,7 @@ private Database getDbForTable(Table oldTable) throws MetaException {
       return hmsHandler.get_database_core(oldTable.getCatName(), oldTable.getDbName());
     } catch (NoSuchObjectException e) {
       throw new MetaException(
-          "Database " + oldTable.getTableName() + " for table " + oldTable.getTableName() + " could not be found");
+          "Database " + oldTable.getDbName() + " for table " + oldTable.getTableName() + " could not be found");
     }
   }
 
@@ -895,7 +895,7 @@ private Table validateTablePaths(Table table) throws MetaException {
     try {
       db = hmsHandler.get_database_core(table.getCatName(), table.getDbName());
     } catch (NoSuchObjectException e) {
-      throw new MetaException("Database " + table.getTableName() + " for table " + table.getTableName() + " could not be found");
+      throw new MetaException("Database " + table.getDbName() + " for table " + table.getTableName() + " could not be found");
     }
 
     if (TableType.MANAGED_TABLE.name().equals(table.getTableType())) {

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatExternalDynamicPartitioned.java
Patch:
@@ -28,7 +28,7 @@ public TestHCatExternalDynamicPartitioned(String formatName, String serdeClass,
       throws Exception {
     super(formatName, serdeClass, inputFormatClass, outputFormatClass);
     tableName = "testHCatExternalDynamicPartitionedTable_" + formatName;
-    generateWriteRecords(NUM_RECORDS, NUM_PARTITIONS, 0);
+    generateWriteRecords(NUM_RECORDS, NUM_TOP_PARTITIONS, 0);
     generateDataColumns();
   }
 
@@ -43,7 +43,7 @@ protected Boolean isTableExternal() {
    */
   @Test
   public void testHCatExternalDynamicCustomLocation() throws Exception {
-    runHCatDynamicPartitionedTable(true, "mapred/externalDynamicOutput/${p1}");
+    runHCatDynamicPartitionedTable(true, "mapred/externalDynamicOutput/${p1}/${p2}");
   }
 
 }

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatExternalDynamicPartitioned.java
Patch:
@@ -28,7 +28,7 @@ public TestHCatExternalDynamicPartitioned(String formatName, String serdeClass,
       throws Exception {
     super(formatName, serdeClass, inputFormatClass, outputFormatClass);
     tableName = "testHCatExternalDynamicPartitionedTable_" + formatName;
-    generateWriteRecords(NUM_RECORDS, NUM_TOP_PARTITIONS, 0);
+    generateWriteRecords(NUM_RECORDS, NUM_PARTITIONS, 0);
     generateDataColumns();
   }
 
@@ -43,7 +43,7 @@ protected Boolean isTableExternal() {
    */
   @Test
   public void testHCatExternalDynamicCustomLocation() throws Exception {
-    runHCatDynamicPartitionedTable(true, "mapred/externalDynamicOutput/${p1}/{p2}");
+    runHCatDynamicPartitionedTable(true, "mapred/externalDynamicOutput/${p1}");
   }
 
 }

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatExternalDynamicPartitioned.java
Patch:
@@ -28,7 +28,7 @@ public TestHCatExternalDynamicPartitioned(String formatName, String serdeClass,
       throws Exception {
     super(formatName, serdeClass, inputFormatClass, outputFormatClass);
     tableName = "testHCatExternalDynamicPartitionedTable_" + formatName;
-    generateWriteRecords(NUM_RECORDS, NUM_PARTITIONS, 0);
+    generateWriteRecords(NUM_RECORDS, NUM_TOP_PARTITIONS, 0);
     generateDataColumns();
   }
 
@@ -43,7 +43,7 @@ protected Boolean isTableExternal() {
    */
   @Test
   public void testHCatExternalDynamicCustomLocation() throws Exception {
-    runHCatDynamicPartitionedTable(true, "mapred/externalDynamicOutput/${p1}");
+    runHCatDynamicPartitionedTable(true, "mapred/externalDynamicOutput/${p1}/{p2}");
   }
 
 }

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergFilterFactory.java
Patch:
@@ -110,9 +110,6 @@ private static Expression translateLeaf(PredicateLeaf leaf) {
         return in(column, leafToLiteralList(leaf));
       case BETWEEN:
         List<Object> icebergLiterals = leafToLiteralList(leaf);
-        if (icebergLiterals.size() < 2) {
-          throw new UnsupportedOperationException("Missing leaf literals: " + leaf);
-        }
         if (icebergLiterals.size() == 2) {
           return and(greaterThanOrEqual(column, icebergLiterals.get(0)),
               lessThanOrEqual(column, icebergLiterals.get(1)));

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -8052,8 +8052,9 @@ protected Operator genFileSinkPlan(String dest, QB qb, Operator input)
         loadFileDesc.setMoveTaskId(moveTaskId);
         loadFileWork.add(loadFileDesc);
         try {
+          FileSystem fs = isDfsDir ?  destinationPath.getFileSystem(conf) : FileSystem.getLocal(conf);
           Path qualifiedPath = conf.getBoolVar(ConfVars.HIVE_RANGER_USE_FULLY_QUALIFIED_URL) ?
-                  destinationPath.getFileSystem(conf).makeQualified(destinationPath) : destinationPath;
+              fs.makeQualified(destinationPath) : destinationPath;
           if (!outputs.add(new WriteEntity(qualifiedPath, !isDfsDir, isDestTempFile))) {
             throw new SemanticException(ErrorMsg.OUTPUT_SPECIFIED_MULTIPLE_TIMES
                     .getMsg(destinationPath.toUri().toString()));

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2019,10 +2019,10 @@ public static enum ConfVars {
     HIVEMULTIGROUPBYSINGLEREDUCER("hive.multigroupby.singlereducer", true,
         "Whether to optimize multi group by query to generate single M/R  job plan. If the multi group by query has \n" +
         "common group by keys, it will be optimized to generate single M/R job."),
-    HIVE_MAP_GROUPBY_SORT("hive.map.groupby.sorted", true,
+    HIVE_MAP_GROUPBY_SORT("hive.map.groupby.sorted", false,
         "If the bucketing/sorting properties of the table exactly match the grouping key, whether to perform \n" +
-        "the group by in the mapper by using BucketizedHiveInputFormat. The only downside to this\n" +
-        "is that it limits the number of mappers to the number of files."),
+        "the group by in the mapper by using BucketizedHiveInputFormat. This can only work if the number of files to be\n" +
+        "processed is exactly 1. The downside to this is that it limits the number of mappers to the number of files."),
     HIVE_DEFAULT_NULLS_LAST("hive.default.nulls.last", true,
         "Whether to set NULLS LAST as the default null ordering for ASC order and " +
             "NULLS FIRST for DESC order."),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -486,6 +486,9 @@ public final class FunctionRegistry {
     system.registerGenericUDF("!", GenericUDFOPNot.class);
     system.registerGenericUDF("between", GenericUDFBetween.class);
     system.registerGenericUDF("in_bloom_filter", GenericUDFInBloomFilter.class);
+    system.registerGenericUDF("toMap", GenericUDFToMap.class);
+    system.registerGenericUDF("toArray", GenericUDFToArray.class);
+    system.registerGenericUDF("toStruct", GenericUDFToStruct.class);
 
     // Utility UDFs
     system.registerUDF("version", UDFVersion.class, false);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeConstantDesc.java
Patch:
@@ -173,6 +173,9 @@ public String getExprString() {
     if (typeInfo.getCategory() == Category.PRIMITIVE) {
       return getFormatted(typeInfo, value);
     } else if (typeInfo.getCategory() == Category.STRUCT) {
+      if (getWritableObjectInspector().getWritableConstantValue() == null) {
+        return getFormatted(typeInfo, value);
+      }
       StringBuilder sb = new StringBuilder();
       sb.append("const struct(");
       List<?> items = (List<?>) getWritableObjectInspector().getWritableConstantValue();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
Patch:
@@ -1306,7 +1306,8 @@ public static MapJoinDesc getMapJoinDesc(HiveConf hconf,
         List<ExprNodeDesc> keyExprList =
             ExprNodeDescUtils.resolveJoinKeysAsRSColumns(mapEntry.getValue(), rsParent);
         if (keyExprList == null) {
-          throw new SemanticException("Error resolving join keys");
+          LOG.warn("Error resolving join keys {} in {} {}", mapEntry.getValue(), rsParent, rsParent.getColumnExprMap());
+          return null;
         }
         newKeyExprMap.put(pos, keyExprList);
       }

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
Patch:
@@ -166,7 +166,7 @@ protected void initialize(Table table,
       return;
     }
 
-    if (table.isPartitioned()) {
+    if (table.isPartitioned() && tPartition.isSetSd()) {
       try {
         if (tPartition.getSd().getLocation() == null) {
           // set default if location is not set and this is a physical

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java
Patch:
@@ -1064,7 +1064,7 @@ static String encodeString(String rawString) {
 
   String getPathForAuth(String locationProperty) {
     return getPathForAuth(locationProperty,
-        SessionStateUtil.getProperty(conf, hive_metastoreConstants.DEFAULT_TABLE_LOCATION).orElse(null));
+        SessionStateUtil.getProperty(conf, SessionStateUtil.DEFAULT_TABLE_LOCATION).orElse(null));
   }
 
   String getPathForAuth(String locationProperty, String defaultTableLocation) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -25,7 +25,6 @@
 import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.HIVEARCHIVEENABLED;
 import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.HIVE_DEFAULT_STORAGE_HANDLER;
 import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.HIVESTATSDBCLASS;
-import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.DEFAULT_TABLE_LOCATION;
 import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_LOCATION;
 import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE;
 import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.TABLE_IS_CTAS;
@@ -13803,7 +13802,7 @@ private Map<String, String> validateAndAddDefaultProperties(
     }
 
     if (isIcebergTable(retValue)) {
-      SessionStateUtil.addResourceOrThrow(conf, hive_metastoreConstants.DEFAULT_TABLE_LOCATION,
+      SessionStateUtil.addResourceOrThrow(conf, SessionStateUtil.DEFAULT_TABLE_LOCATION,
           getDefaultLocation(qualifiedTabName[0], qualifiedTabName[1], true));
     }
     return retValue;

File: ql/src/java/org/apache/hadoop/hive/ql/session/SessionStateUtil.java
Patch:
@@ -31,6 +31,7 @@ public class SessionStateUtil {
 
   private static final Logger LOG = LoggerFactory.getLogger(SessionStateUtil.class);
   private static final String COMMIT_INFO_PREFIX = "COMMIT_INFO.";
+  public static final String DEFAULT_TABLE_LOCATION = "defaultLocation";
 
   private SessionStateUtil() {
 

File: standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/hive_metastoreConstants.java
Patch:
@@ -97,6 +97,4 @@
 
   public static final java.lang.String EXPECTED_PARAMETER_VALUE = "expected_parameter_value";
 
-  public static final java.lang.String DEFAULT_TABLE_LOCATION = "defaultLocation";
-
 }

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.java
Patch:
@@ -24,6 +24,7 @@
 import java.net.URISyntaxException;
 import java.util.Collections;
 import org.apache.hadoop.conf.Configurable;
+import org.apache.hadoop.hive.common.TableName;
 import org.apache.hadoop.hive.common.classification.InterfaceAudience;
 import org.apache.hadoop.hive.common.classification.InterfaceStability;
 import org.apache.hadoop.hive.common.type.SnapshotContext;

File: standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/hive_metastoreConstants.java
Patch:
@@ -97,4 +97,6 @@
 
   public static final java.lang.String EXPECTED_PARAMETER_VALUE = "expected_parameter_value";
 
+  public static final java.lang.String DEFAULT_TABLE_LOCATION = "defaultLocation";
+
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -14263,14 +14263,13 @@ ASTNode analyzeCreateTable(
 
           isTransactional, isManaged, new String[]{qualifiedTabName.getDb(), qualifiedTabName.getTable()}, isDefaultTableTypeChanged);
       tblProps.put(hive_metastoreConstants.TABLE_IS_CTLT, "true");
-      isExt = isIcebergTable(tblProps) ||
-          isExternalTableChanged(tblProps, isTransactional, isExt, isDefaultTableTypeChanged);
+      isExt = isExternalTableChanged(tblProps, isTransactional, isExt, isDefaultTableTypeChanged);
       addDbAndTabToOutputs(new String[] {qualifiedTabName.getDb(), qualifiedTabName.getTable()},
           TableType.MANAGED_TABLE, isTemporary, tblProps, storageFormat);
 
       Table likeTable = getTable(likeTableName, false);
       if (likeTable != null) {
-        if (isTemporary || isExt) {
+        if (isTemporary || isExt || isIcebergTable(tblProps)) {
           updateDefaultTblProps(likeTable.getParameters(), tblProps,
               new ArrayList<>(Arrays.asList(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL,
                   hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES)));

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
Patch:
@@ -254,9 +254,9 @@ public void alterTable(RawStore msdb, Warehouse wh, String catName, String dbnam
       boolean renamedTranslatedToExternalTable = rename && MetaStoreUtils.isTranslatedToExternalTable(oldt)
           && MetaStoreUtils.isTranslatedToExternalTable(newt);
       boolean renamedExternalTable = rename && MetaStoreUtils.isExternalTable(oldt)
-          && !MetaStoreUtils.isPropertyTrue(oldt.getParameters(), "TRANSLATED_TO_EXTERNAL");
+          && !MetaStoreUtils.isPropertyTrue(oldt.getParameters(), HiveMetaHook.TRANSLATED_TO_EXTERNAL);
       boolean isRenameIcebergTable =
-          rename && HiveMetaHook.ICEBERG.equalsIgnoreCase(newt.getParameters().get(HiveMetaHook.TABLE_TYPE));
+          rename && MetaStoreUtils.isIcebergTable(newt.getParameters());
 
       List<ColumnStatistics> columnStatistics = getColumnStats(msdb, oldt);
       columnStatistics = deleteTableColumnStats(msdb, oldt, newt, columnStatistics);

File: service/src/java/org/apache/hive/service/cli/thrift/RetryingThriftCLIServiceClient.java
Patch:
@@ -311,7 +311,8 @@ protected synchronized TTransport connect(HiveConf conf) throws HiveSQLException
     String host = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST);
     int port = conf.getIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_PORT);
     int maxThriftMessageSize = (int) conf.getSizeVar(HiveConf.ConfVars.HIVE_THRIFT_CLIENT_MAX_MESSAGE_SIZE);
-    LOG.info("Connecting to " + host + ":" + port);
+    LOG.info("Connecting to {}:{} using a thrift max message of size: {}",
+        host, port, maxThriftMessageSize);
 
     transport = HiveAuthUtils.getSocketTransport(host, port, 0, maxThriftMessageSize);
     ((TSocket) transport).setTimeout((int) conf.getTimeVar(HiveConf.ConfVars.SERVER_READ_SOCKET_TIMEOUT,

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -612,7 +612,8 @@ public void renamePartition(String catName, String dbname, String tableName, Lis
   }
 
   private <T extends TTransport> T configureThriftMaxMessageSize(T transport) {
-    int maxThriftMessageSize = (int) MetastoreConf.getSizeVar(conf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE);
+    int maxThriftMessageSize = (int) MetastoreConf.getSizeVar(
+            conf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE);
     if (maxThriftMessageSize > 0) {
       if (transport.getConfiguration() == null) {
         LOG.warn("TTransport {} is returning a null Configuration, Thrift max message size is not getting configured",

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.hadoop.hive.metastore.parser;
 
 import java.sql.Timestamp;
-import java.util.Date;
+import java.sql.Date;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -486,9 +486,9 @@ private String getJdoFilterPushdownParam(int partColIndex,
       // columns have been excluded above, so it will either compare w/string or fail.
       Object val = value;
       if (colType.equals("date") && value instanceof Date) {
-        val = MetaStoreUtils.PARTITION_DATE_FORMAT.get().format((Date)value);
+        val = MetaStoreUtils.convertDateToString((Date)value);
       } else if (colType.equals("timestamp") && value instanceof Timestamp) {
-        val = MetaStoreUtils.PARTITION_TIMESTAMP_FORMAT.get().format(((Timestamp)value).toLocalDateTime());
+        val = MetaStoreUtils.convertTimestampToString((Timestamp)value);
       }
       boolean isStringValue = val instanceof String;
       if (!isStringValue && (!isIntegralSupported || !(val instanceof Long))) {

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -6771,7 +6771,8 @@ public StorageHandlerInfo getStorageHandlerInfo(Table table)
 
   public void alterTableExecuteOperation(Table table, AlterTableExecuteSpec executeSpec) throws HiveException {
     try {
-      HiveStorageHandler storageHandler = createStorageHandler(table.getTTable());
+      HiveStorageHandler storageHandler = Optional.ofNullable(createStorageHandler(table.getTTable())).orElseThrow(() ->
+          new UnsupportedOperationException(String.format("ALTER EXECUTE is not supported for table %s", table.getTableName())));
       storageHandler.executeOperation(table, executeSpec);
     } catch (MetaException e) {
       throw new HiveException(e);

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
Patch:
@@ -1362,7 +1362,8 @@ public enum ConfVars {
             "org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe," +
             "org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe," +
             "org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe," +
-            "org.apache.hadoop.hive.serde2.OpenCSVSerde",
+            "org.apache.hadoop.hive.serde2.OpenCSVSerde," +
+            "org.apache.iceberg.mr.hive.HiveIcebergSerDe",
         "SerDes retrieving schema from metastore. This is an internal parameter."),
     SERDES_WITHOUT_FROM_DESERIALIZER("metastore.serdes.without.from.deserializer",
         "hive.metastore.serdes.without.from.deserializer",

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestRestrictedList.java
Patch:
@@ -105,8 +105,10 @@ public static void startServices() throws Exception {
     addToExpectedRestrictedMap("hive.driver.parallel.compilation.global.limit");
     addToExpectedRestrictedMap("hive.zookeeper.ssl.keystore.location");
     addToExpectedRestrictedMap("hive.zookeeper.ssl.keystore.password");
+    addToExpectedRestrictedMap("hive.zookeeper.ssl.keystore.type");
     addToExpectedRestrictedMap("hive.zookeeper.ssl.truststore.location");
     addToExpectedRestrictedMap("hive.zookeeper.ssl.truststore.password");
+    addToExpectedRestrictedMap("hive.zookeeper.ssl.truststore.type");
 
     checkRestrictedListMatch();
   }

File: jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java
Patch:
@@ -1009,7 +1009,9 @@ SSLConnectionSocketFactory getTwoWaySSLSocketFactory() throws SQLException {
         JdbcConnectionParams.SUNJSSE_ALGORITHM_STRING);
       String keyStorePath = sessConfMap.get(JdbcConnectionParams.SSL_KEY_STORE);
       String keyStorePassword = Utils.getPassword(sessConfMap, JdbcConnectionParams.SSL_KEY_STORE_PASSWORD);
-      KeyStore sslKeyStore = KeyStore.getInstance(JdbcConnectionParams.SSL_KEY_STORE_TYPE);
+      String keyStoreType = sessConfMap.get(JdbcConnectionParams.SSL_KEY_STORE_TYPE);
+      keyStoreType = (!StringUtils.isBlank(keyStoreType)) ? keyStoreType : KeyStore.getDefaultType();
+      KeyStore sslKeyStore = KeyStore.getInstance(keyStoreType);
 
       if (keyStorePath == null || keyStorePath.isEmpty()) {
         throw new IllegalArgumentException(JdbcConnectionParams.SSL_KEY_STORE

File: llap-client/src/java/org/apache/hadoop/hive/registry/impl/ZkRegistryBase.java
Patch:
@@ -238,8 +238,10 @@ private CuratorFramework getZookeeperClient(Configuration conf, String namespace
         .sslEnabled(HiveConf.getBoolVar(conf, ConfVars.HIVE_ZOOKEEPER_SSL_ENABLE))
         .keyStoreLocation(HiveConf.getVar(conf, ConfVars.HIVE_ZOOKEEPER_SSL_KEYSTORE_LOCATION))
         .keyStorePassword(keyStorePassword)
+        .keyStoreType(HiveConf.getVar(conf, ConfVars.HIVE_ZOOKEEPER_SSL_KEYSTORE_TYPE))
         .trustStoreLocation(HiveConf.getVar(conf, ConfVars.HIVE_ZOOKEEPER_SSL_TRUSTSTORE_LOCATION))
         .trustStorePassword(trustStorePassword)
+        .trustStoreType(HiveConf.getVar(conf, ConfVars.HIVE_ZOOKEEPER_SSL_TRUSTSTORE_TYPE))
         .build().getNewZookeeperClient(zooKeeperAclProvider, namespace);
   }
 

File: common/src/java/org/apache/hadoop/hive/common/type/TimestampTZ.java
Patch:
@@ -100,4 +100,7 @@ public int getNanos() {
     return zonedDateTime.toInstant().getNano();
   }
 
+  public Instant toInstant() {
+    return zonedDateTime.toInstant();
+  }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/InstantFormatterCache.java
Patch:
@@ -27,14 +27,14 @@
  * </p>
  * @param <T> the type of the underlying datetime formatter
  */
-abstract class UnixTimeFormatterCache<T> implements UnixTimeFormatter {
+abstract class InstantFormatterCache<T> implements InstantFormatter {
 
   protected final ZoneId zoneId;
   protected final Function<String, T> loader;
   protected String lastPattern;
   protected T formatter;
 
-  protected UnixTimeFormatterCache(ZoneId zoneId, Function<String, T> loader) {
+  protected InstantFormatterCache(ZoneId zoneId, Function<String, T> loader) {
     this.zoneId = zoneId;
     this.loader = loader;
   }

File: itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestRemoteHiveMetaStoreKerberos.java
Patch:
@@ -77,7 +77,7 @@ public void testThriftMaxMessageSize() throws Throwable {
     MetastoreConf.setVar(clientConf, ConfVars.THRIFT_URIS, "thrift://localhost:" + port);
     // set to a low value to prove THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE is being honored
     // (it should throw an exception)
-    MetastoreConf.setLongVar(clientConf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE, 1024L);
+    MetastoreConf.setVar(clientConf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE, "1024");
     HiveMetaStoreClient limitedClient = new HiveMetaStoreClient(clientConf);
     Exception expectedException = assertThrows(TTransportException.class, () -> {
       limitedClient.listPartitions(dbName, tblName, (short)-1);

File: itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestSSLWithMiniKdc.java
Patch:
@@ -98,7 +98,7 @@ public void testHmsThriftMaxMessageSize() throws Exception {
     MetastoreConf.setVar(clientConf, MetastoreConf.ConfVars.THRIFT_URIS, "thrift://localhost:" + miniHS2.getHmsPort());
     // set to a low value to prove THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE is being honored
     // (it should throw an exception)
-    MetastoreConf.setLongVar(clientConf, MetastoreConf.ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE, 512L);
+    MetastoreConf.setVar(clientConf, MetastoreConf.ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE, "512");
     HiveMetaStoreClient limitedClient = new HiveMetaStoreClient(clientConf);
     String dbName = "default";
     String tableName = "testThriftMaxMessageSize";

File: service/src/java/org/apache/hive/service/cli/thrift/RetryingThriftCLIServiceClient.java
Patch:
@@ -310,8 +310,8 @@ protected synchronized TTransport connect(HiveConf conf) throws HiveSQLException
 
     String host = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST);
     int port = conf.getIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_PORT);
-    int maxThriftMessageSize = (int) Math.min(conf.getLongVar(HiveConf.ConfVars.HIVE_THRIFT_CLIENT_MAX_MESSAGE_SIZE),Integer.MAX_VALUE);
-    LOG.info("Connecting to " + host + ":" + port+ " using a thrift max message of size: "+ maxThriftMessageSize);
+    int maxThriftMessageSize = (int) conf.getSizeVar(HiveConf.ConfVars.HIVE_THRIFT_CLIENT_MAX_MESSAGE_SIZE);
+    LOG.info("Connecting to " + host + ":" + port);
 
     transport = HiveAuthUtils.getSocketTransport(host, port, 0, maxThriftMessageSize);
     ((TSocket) transport).setTimeout((int) conf.getTimeVar(HiveConf.ConfVars.SERVER_READ_SOCKET_TIMEOUT,

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -610,8 +610,7 @@ public void renamePartition(String catName, String dbname, String tableName, Lis
   }
 
   private <T extends TTransport> T configureThriftMaxMessageSize(T transport) {
-    int maxThriftMessageSize = Math.min(MetastoreConf.getIntVar(
-            conf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE),Integer.MAX_VALUE);
+    int maxThriftMessageSize = (int) MetastoreConf.getSizeVar(conf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE);
     if (maxThriftMessageSize > 0) {
       if (transport.getConfiguration() == null) {
         LOG.warn("TTransport {} is returning a null Configuration, Thrift max message size is not getting configured",

File: itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestRemoteHiveMetaStoreKerberos.java
Patch:
@@ -77,7 +77,7 @@ public void testThriftMaxMessageSize() throws Throwable {
     MetastoreConf.setVar(clientConf, ConfVars.THRIFT_URIS, "thrift://localhost:" + port);
     // set to a low value to prove THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE is being honored
     // (it should throw an exception)
-    MetastoreConf.setVar(clientConf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE, "1024");
+    MetastoreConf.setLongVar(clientConf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE, 1024L);
     HiveMetaStoreClient limitedClient = new HiveMetaStoreClient(clientConf);
     Exception expectedException = assertThrows(TTransportException.class, () -> {
       limitedClient.listPartitions(dbName, tblName, (short)-1);

File: itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestSSLWithMiniKdc.java
Patch:
@@ -98,7 +98,7 @@ public void testHmsThriftMaxMessageSize() throws Exception {
     MetastoreConf.setVar(clientConf, MetastoreConf.ConfVars.THRIFT_URIS, "thrift://localhost:" + miniHS2.getHmsPort());
     // set to a low value to prove THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE is being honored
     // (it should throw an exception)
-    MetastoreConf.setVar(clientConf, MetastoreConf.ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE, "512");
+    MetastoreConf.setLongVar(clientConf, MetastoreConf.ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE, 512L);
     HiveMetaStoreClient limitedClient = new HiveMetaStoreClient(clientConf);
     String dbName = "default";
     String tableName = "testThriftMaxMessageSize";

File: service/src/java/org/apache/hive/service/cli/thrift/RetryingThriftCLIServiceClient.java
Patch:
@@ -310,8 +310,8 @@ protected synchronized TTransport connect(HiveConf conf) throws HiveSQLException
 
     String host = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST);
     int port = conf.getIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_PORT);
-    int maxThriftMessageSize = (int) conf.getSizeVar(HiveConf.ConfVars.HIVE_THRIFT_CLIENT_MAX_MESSAGE_SIZE);
-    LOG.info("Connecting to " + host + ":" + port);
+    int maxThriftMessageSize = (int) Math.min(conf.getLongVar(HiveConf.ConfVars.HIVE_THRIFT_CLIENT_MAX_MESSAGE_SIZE),Integer.MAX_VALUE);
+    LOG.info("Connecting to " + host + ":" + port+ " using a thrift max message of size: "+ maxThriftMessageSize);
 
     transport = HiveAuthUtils.getSocketTransport(host, port, 0, maxThriftMessageSize);
     ((TSocket) transport).setTimeout((int) conf.getTimeVar(HiveConf.ConfVars.SERVER_READ_SOCKET_TIMEOUT,

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -610,7 +610,8 @@ public void renamePartition(String catName, String dbname, String tableName, Lis
   }
 
   private <T extends TTransport> T configureThriftMaxMessageSize(T transport) {
-    int maxThriftMessageSize = (int) MetastoreConf.getSizeVar(conf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE);
+    int maxThriftMessageSize = Math.min(MetastoreConf.getIntVar(
+            conf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE),Integer.MAX_VALUE);
     if (maxThriftMessageSize > 0) {
       if (transport.getConfiguration() == null) {
         LOG.warn("TTransport {} is returning a null Configuration, Thrift max message size is not getting configured",

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3536,8 +3536,8 @@ public static enum ConfVars {
     HIVEFETCHTASKCACHING("hive.fetch.task.caching", true,
         "Enabling the caching of the result of fetch tasks eliminates the chance of running into a failing read." +
             " On the other hand, if enabled, the hive.fetch.task.conversion.threshold must be adjusted accordingly. That" +
-            " is 1GB by default which must be lowered in case of enabled caching to prevent the consumption of too much memory."),
-    HIVEFETCHTASKCONVERSIONTHRESHOLD("hive.fetch.task.conversion.threshold", 1073741824L,
+            " is 200MB by default which must be lowered in case of enabled caching to prevent the consumption of too much memory."),
+    HIVEFETCHTASKCONVERSIONTHRESHOLD("hive.fetch.task.conversion.threshold", 209715200L,
         "Input threshold for applying hive.fetch.task.conversion. If target table is native, input length\n" +
         "is calculated by summation of file lengths. If it's not native, storage handler for the table\n" +
         "can optionally implement org.apache.hadoop.hive.ql.metadata.InputEstimator interface."),

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHive.java
Patch:
@@ -838,6 +838,9 @@ public void testPartition() throws Throwable {
       partialSpec.put("hr", "14");
       assertEquals(1, hm.getPartitions(tbl, partialSpec).size());
 
+      // Test get partitions with max_parts
+      assertEquals(1, hm.getPartitions(tbl, new HashMap(), (short) 1).size());
+
       hm.dropTable(Warehouse.DEFAULT_DATABASE_NAME, tableName);
     } catch (Throwable e) {
       System.err.println(StringUtils.stringifyException(e));

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -3959,7 +3959,7 @@ public List<Partition> listPartitionsPsWithAuth(String catName, String db_name,
         LOG.info(
             "Redirecting to directSQL enabled API: db: {} tbl: {} partVals: {}",
             db_name, tbl_name, Joiner.on(',').join(part_vals));
-        return getPartitions(catName, db_name, tbl_name, -1);
+        return getPartitions(catName, db_name, tbl_name, max_parts);
       }
       LOG.debug("executing listPartitionNamesPsWithAuth");
       Collection parts = getPartitionPsQueryResults(catName, db_name, tbl_name,

File: shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
Patch:
@@ -410,9 +410,9 @@ public class MiniTezShim extends Hadoop23Shims.MiniMrShim {
     public MiniTezShim(Configuration conf, int numberOfTaskTrackers, String nameNode,
                        boolean usingLlap) throws IOException {
       mr = new MiniTezCluster("hive", numberOfTaskTrackers);
-      conf.setInt(YarnConfiguration.YARN_MINICLUSTER_NM_PMEM_MB, 512);
+      conf.setInt(YarnConfiguration.YARN_MINICLUSTER_NM_PMEM_MB, 4096);
       conf.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, 128);
-      conf.setInt(YarnConfiguration.RM_SCHEDULER_MAXIMUM_ALLOCATION_MB, 512);
+      conf.setInt(YarnConfiguration.RM_SCHEDULER_MAXIMUM_ALLOCATION_MB, 4096);
 
       conf.set("fs.defaultFS", nameNode);
       conf.set("tez.am.log.level", "DEBUG");

File: shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
Patch:
@@ -1519,7 +1519,7 @@ private int compareKeyStrength(EncryptionZone zone1, EncryptionZone zone2) throw
   public HadoopShims.HdfsEncryptionShim createHdfsEncryptionShim(FileSystem fs, Configuration conf) throws IOException {
     if (isHdfsEncryptionSupported()) {
       URI uri = fs.getUri();
-      if ("hdfs".equals(uri.getScheme())) {
+      if ("hdfs".equals(uri.getScheme()) && fs instanceof DistributedFileSystem) {
         return new HdfsEncryptionShim(uri, conf);
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -621,6 +621,7 @@ public final class FunctionRegistry {
     system.registerGenericUDF("least", GenericUDFLeast.class);
     system.registerGenericUDF("cardinality_violation", GenericUDFCardinalityViolation.class);
     system.registerGenericUDF("width_bucket", GenericUDFWidthBucket.class);
+    system.registerGenericUDF("typeof", GenericUDFTypeOf.class);
 
     system.registerGenericUDF("from_utc_timestamp", GenericUDFFromUtcTimestamp.class);
     system.registerGenericUDF("to_utc_timestamp", GenericUDFToUtcTimestamp.class);

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampWithZoneObjectInspectorHive3.java
Patch:
@@ -20,12 +20,12 @@
 package org.apache.iceberg.mr.hive.serde.objectinspector;
 
 import java.time.OffsetDateTime;
-import java.time.ZoneOffset;
 import java.time.ZonedDateTime;
 import org.apache.hadoop.hive.common.type.TimestampTZ;
 import org.apache.hadoop.hive.serde2.io.TimestampLocalTZWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampLocalTZObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TimestampLocalTZTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 
@@ -58,7 +58,7 @@ public TimestampTZ getPrimitiveJavaObject(Object o) {
       return null;
     }
     OffsetDateTime odt = (OffsetDateTime) o;
-    ZonedDateTime zdt = odt.atZoneSameInstant(ZoneOffset.UTC);
+    ZonedDateTime zdt = odt.atZoneSameInstant(((TimestampLocalTZTypeInfo) typeInfo).getTimeZone());
     return new TimestampTZ(zdt);
   }
 

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimestampWithZoneObjectInspectorHive3.java
Patch:
@@ -23,6 +23,7 @@
 import java.time.OffsetDateTime;
 import java.time.ZoneId;
 import java.time.ZoneOffset;
+import java.time.ZonedDateTime;
 import org.apache.hadoop.hive.common.type.TimestampTZ;
 import org.apache.hadoop.hive.serde2.io.TimestampLocalTZWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -53,9 +54,11 @@ public void testIcebergTimestampLocalTZObjectInspector() {
 
     LocalDateTime dateTimeAtUTC = LocalDateTime.of(2020, 12, 10, 15, 55, 20, 30000);
     OffsetDateTime offsetDateTime = OffsetDateTime.of(dateTimeAtUTC.plusHours(4), ZoneOffset.ofHours(4));
+    ZonedDateTime zdt = offsetDateTime.atZoneSameInstant(TypeInfoFactory.timestampLocalTZTypeInfo.getTimeZone());
     TimestampTZ ts = new TimestampTZ(dateTimeAtUTC.atZone(ZoneId.of("UTC")));
 
     Assert.assertEquals(ts, oi.getPrimitiveJavaObject(offsetDateTime));
+    Assert.assertEquals(zdt, oi.getPrimitiveJavaObject(offsetDateTime).getZonedDateTime());
     Assert.assertEquals(new TimestampLocalTZWritable(ts), oi.getPrimitiveWritableObject(offsetDateTime));
 
     // try with another offset as well

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -5459,6 +5459,7 @@ private void copyMSD(MStorageDescriptor newSd, MStorageDescriptor oldSd) {
     oldSd.getSerDeInfo().setSerializationLib(
         newSd.getSerDeInfo().getSerializationLib());
     oldSd.getSerDeInfo().setParameters(newSd.getSerDeInfo().getParameters());
+    oldSd.getSerDeInfo().setDescription(newSd.getSerDeInfo().getDescription());
     oldSd.setSkewedColNames(newSd.getSkewedColNames());
     oldSd.setSkewedColValues(newSd.getSkewedColValues());
     oldSd.setSkewedColValueLocationMaps(newSd.getSkewedColValueLocationMaps());

File: kafka-handler/src/test/org/apache/hadoop/hive/kafka/KafkaRecordIteratorTest.java
Patch:
@@ -55,7 +55,6 @@
 /**
  * Kafka Iterator Tests.
  */
-@org.junit.Ignore("HIVE-23838: KafkaRecordIteratorTest is flaky")
 @RunWith(Parameterized.class) public class KafkaRecordIteratorTest {
   private static final Logger LOG = LoggerFactory.getLogger(KafkaRecordIteratorTest.class);
   private static final int RECORD_NUMBER = 19384;
@@ -92,7 +91,7 @@ public KafkaRecordIteratorTest(String currentTopic,
       boolean readUncommitted,
       List<ConsumerRecord<byte[], byte[]>> expectedRecords) {
     this.currentTopic = currentTopic;
-    // when true means the the topic is not Transactional topic
+    // when true means the topic is not Transactional topic
     this.readUncommitted = readUncommitted;
     this.expectedRecords = expectedRecords;
     this.topicPartition = new TopicPartition(currentTopic, 0);
@@ -279,7 +278,6 @@ private static void sendData(List<ConsumerRecord<byte[], byte[]>> recordList, @N
     producerProps.setProperty("bootstrap.servers", KafkaBrokerResource.BROKER_IP_PORT);
     producerProps.setProperty("key.serializer", "org.apache.kafka.common.serialization.ByteArraySerializer");
     producerProps.setProperty("value.serializer", "org.apache.kafka.common.serialization.ByteArraySerializer");
-    producerProps.setProperty("max.block.ms", "10000");
     if (txId != null) {
       producerProps.setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG, txId);
     }

File: service/src/java/org/apache/hive/service/cli/thrift/RetryingThriftCLIServiceClient.java
Patch:
@@ -310,8 +310,8 @@ protected synchronized TTransport connect(HiveConf conf) throws HiveSQLException
 
     String host = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST);
     int port = conf.getIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_PORT);
-    int maxThriftMessageSize = (int) Math.min(conf.getSizeVar(HiveConf.ConfVars.HIVE_THRIFT_CLIENT_MAX_MESSAGE_SIZE),Integer.MAX_VALUE);
-    LOG.info("Connecting to " + host + ":" + port+ " using a thrift max message of size: "+ maxThriftMessageSize);
+    int maxThriftMessageSize = (int) conf.getSizeVar(HiveConf.ConfVars.HIVE_THRIFT_CLIENT_MAX_MESSAGE_SIZE);
+    LOG.info("Connecting to " + host + ":" + port);
 
     transport = HiveAuthUtils.getSocketTransport(host, port, 0, maxThriftMessageSize);
     ((TSocket) transport).setTimeout((int) conf.getTimeVar(HiveConf.ConfVars.SERVER_READ_SOCKET_TIMEOUT,

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -610,7 +610,7 @@ public void renamePartition(String catName, String dbname, String tableName, Lis
   }
 
   private <T extends TTransport> T configureThriftMaxMessageSize(T transport) {
-    int maxThriftMessageSize = (int) Math.min(MetastoreConf.getSizeVar(conf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE),Integer.MAX_VALUE);
+    int maxThriftMessageSize = (int) MetastoreConf.getSizeVar(conf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE);
     if (maxThriftMessageSize > 0) {
       if (transport.getConfiguration() == null) {
         LOG.warn("TTransport {} is returning a null Configuration, Thrift max message size is not getting configured",

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
Patch:
@@ -1443,9 +1443,9 @@ public enum ConfVars {
                 " corresponding service discovery servers e.g. a zookeeper. Otherwise they are " +
                 "used as URIs for remote metastore."),
     THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE("metastore.thrift.client.max.message.size",
-            "hive.thrift.client.max.message.size", (2*1024*1024*1024L)-1L, new SizeValidator(-1L, true, (long) Integer.MAX_VALUE, true),
-            "Thrift client configuration for max message size. 0 or -1 will use the default defined in the Thrift " +
-                    "library. The upper limit is 2147483647 bytes"),
+        "hive.thrift.client.max.message.size", "1gb", new SizeValidator(-1L, true, (long) Integer.MAX_VALUE, true),
+        "Thrift client configuration for max message size. 0 or -1 will use the default defined in the Thrift " +
+        "library. The upper limit is 2147483648 bytes (or 2gb)."),
     THRIFT_SERVICE_DISCOVERY_MODE("metastore.service.discovery.mode",
             "hive.metastore.service.discovery.mode",
             "",

File: service/src/java/org/apache/hive/service/cli/thrift/RetryingThriftCLIServiceClient.java
Patch:
@@ -310,8 +310,8 @@ protected synchronized TTransport connect(HiveConf conf) throws HiveSQLException
 
     String host = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST);
     int port = conf.getIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_PORT);
-    int maxThriftMessageSize = (int) conf.getSizeVar(HiveConf.ConfVars.HIVE_THRIFT_CLIENT_MAX_MESSAGE_SIZE);
-    LOG.info("Connecting to " + host + ":" + port);
+    int maxThriftMessageSize = (int) Math.min(conf.getSizeVar(HiveConf.ConfVars.HIVE_THRIFT_CLIENT_MAX_MESSAGE_SIZE),Integer.MAX_VALUE);
+    LOG.info("Connecting to " + host + ":" + port+ " using a thrift max message of size: "+ maxThriftMessageSize);
 
     transport = HiveAuthUtils.getSocketTransport(host, port, 0, maxThriftMessageSize);
     ((TSocket) transport).setTimeout((int) conf.getTimeVar(HiveConf.ConfVars.SERVER_READ_SOCKET_TIMEOUT,

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -610,7 +610,7 @@ public void renamePartition(String catName, String dbname, String tableName, Lis
   }
 
   private <T extends TTransport> T configureThriftMaxMessageSize(T transport) {
-    int maxThriftMessageSize = (int) MetastoreConf.getSizeVar(conf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE);
+    int maxThriftMessageSize = (int) Math.min(MetastoreConf.getSizeVar(conf, ConfVars.THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE),Integer.MAX_VALUE);
     if (maxThriftMessageSize > 0) {
       if (transport.getConfiguration() == null) {
         LOG.warn("TTransport {} is returning a null Configuration, Thrift max message size is not getting configured",

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
Patch:
@@ -1443,9 +1443,9 @@ public enum ConfVars {
                 " corresponding service discovery servers e.g. a zookeeper. Otherwise they are " +
                 "used as URIs for remote metastore."),
     THRIFT_METASTORE_CLIENT_MAX_MESSAGE_SIZE("metastore.thrift.client.max.message.size",
-        "hive.thrift.client.max.message.size", "1gb", new SizeValidator(-1L, true, (long) Integer.MAX_VALUE, true),
-        "Thrift client configuration for max message size. 0 or -1 will use the default defined in the Thrift " +
-        "library. The upper limit is 2147483648 bytes (or 2gb)."),
+            "hive.thrift.client.max.message.size", (2*1024*1024*1024L)-1L, new SizeValidator(-1L, true, (long) Integer.MAX_VALUE, true),
+            "Thrift client configuration for max message size. 0 or -1 will use the default defined in the Thrift " +
+                    "library. The upper limit is 2147483647 bytes"),
     THRIFT_SERVICE_DISCOVERY_MODE("metastore.service.discovery.mode",
             "hive.metastore.service.discovery.mode",
             "",

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/Decimal64ColumnVector.java
Patch:
@@ -136,12 +136,12 @@ public void setElement(int outputElementNum, int inputElementNum, ColumnVector i
           decimal64ColVector.vector[inputElementNum], decimal64ColVector.scale);
       scratchHiveDecWritable.mutateEnforcePrecisionScale(precision, scale);
       if (scratchHiveDecWritable.isSet()) {
-        vector[inputElementNum] = scratchHiveDecWritable.serialize64(scale);
+        vector[outputElementNum] = scratchHiveDecWritable.serialize64(scale);
       } else {
 
         // In effect, the input is NULL because of out-of-range precision/scale.
         noNulls = false;
-        isNull[inputElementNum] = true;
+        isNull[outputElementNum] = true;
       }
     } else {
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java
Patch:
@@ -28,7 +28,6 @@
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.metastore.utils.MetaStoreUtils;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 import org.apache.hadoop.hive.ql.exec.FilterOperator;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
@@ -319,8 +318,9 @@ private boolean disableSemiJoinOptDueToExternalTable(HiveConf conf, TableScanOpe
           if (columnOrigin != null && columnOrigin.op instanceof TableScanOperator) {
             // Join key origin has been traced to a table column. Check if the table is external.
             TableScanOperator joinKeyTs = (TableScanOperator) columnOrigin.op;
-            if (MetaStoreUtils.isExternalTable(joinKeyTs.getConf().getTableMetadata().getTTable())) {
-              LOG.debug("Join key {} is from {} which is an external table. Disabling semijoin optimization.",
+            if (!StatsUtils.checkCanProvideStats(new Table(joinKeyTs.getConf().getTableMetadata().getTTable()))) {
+              LOG.debug("Join key {} is from {} which is an external table and also could not provide statistics. " +
+                      "Disabling semijoin optimization.",
                   columnOrigin.col,
                   joinKeyTs.getConf().getTableMetadata().getFullyQualifiedName());
               disableSemiJoin = true;

File: kafka-handler/src/java/org/apache/hadoop/hive/kafka/KafkaUtils.java
Patch:
@@ -75,6 +75,7 @@ final class KafkaUtils {
   private static final String JAAS_TEMPLATE_SCRAM =
       "org.apache.kafka.common.security.scram.ScramLoginModule required "
           + "username=\"%s\" password=\"%s\" serviceName=\"%s\" tokenauth=true;";
+  static final Text KAFKA_DELEGATION_TOKEN_KEY = new Text("KAFKA_DELEGATION_TOKEN");
 
   private KafkaUtils() {
   }
@@ -383,7 +384,7 @@ static void addKerberosJaasConf(Configuration configuration, Properties props) {
 
     if (configuration instanceof JobConf) {
       Credentials creds = ((JobConf) configuration).getCredentials();
-      Token<?> token = creds.getToken(new Text("KAFKA_DELEGATION_TOKEN"));
+      Token<?> token = creds.getToken(KAFKA_DELEGATION_TOKEN_KEY);
 
       if (token != null) {
         log.info("Kafka delegation token has been found: {}", token);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java
Patch:
@@ -73,6 +73,7 @@
 import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;
 import org.apache.hadoop.hive.ql.plan.SelectDesc;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
+import org.apache.hadoop.hive.ql.stats.StatsUtils;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.Mode;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFIn;
@@ -302,8 +303,8 @@ private boolean disableSemiJoinOptDueToExternalTable(HiveConf conf, TableScanOpe
     boolean disableSemiJoin = false;
     if (conf.getBoolVar(HiveConf.ConfVars.HIVE_DISABLE_UNSAFE_EXTERNALTABLE_OPERATIONS)) {
       // We already have the TableScan for one side of the join. Check this now.
-      if (MetaStoreUtils.isExternalTable(ts.getConf().getTableMetadata().getTTable())) {
-        LOG.debug("Disabling semijoin optimzation on {} since it is an external table.",
+      if (!StatsUtils.checkCanProvideStats(new Table(ts.getConf().getTableMetadata().getTTable()))) {
+        LOG.debug("Disabling semijoin optimzation on {} since it is an external table and also could not provide statistics.",
             ts.getConf().getTableMetadata().getFullyQualifiedName());
         disableSemiJoin = true;
       } else {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java
Patch:
@@ -295,8 +295,9 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
           tbl = hive.getTable(tbl.getDbName(), tbl.getTableName(), true, true);
         }
 
-        if (MetaStoreUtils.isExternalTable(tbl.getTTable())) {
-          Logger.info("Table " + tbl.getTableName() + " is external. Skip StatsOptimizer.");
+        if (!StatsUtils.checkCanProvideStats(tbl)) {
+          Logger.info("Table " + tbl.getTableName() + " is external and also could not provide statistics. " +
+              "Skip StatsOptimizer.");
           return null;
         }
         if (MetaStoreUtils.isNonNativeTable(tbl.getTTable())

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java
Patch:
@@ -245,10 +245,10 @@ public HiveRelOptMaterialization createMaterialization(HiveConf conf, Table mate
   private HiveRelOptMaterialization.IncrementalRebuildMode determineIncrementalRebuildMode(RelNode definitionPlan) {
     MaterializedViewIncrementalRewritingRelVisitor visitor = new MaterializedViewIncrementalRewritingRelVisitor();
     visitor.go(definitionPlan);
-    if (!visitor.isRewritingAllowed()) {
+    if (!visitor.hasAllowedOperatorsOnly()) {
       return HiveRelOptMaterialization.IncrementalRebuildMode.NOT_AVAILABLE;
     }
-    if (visitor.isContainsAggregate() && !visitor.hasCountStar()) {
+    if (visitor.isContainsAggregate() && !visitor.hasCountStar() || visitor.isInsertAllowedOnly()) {
       return HiveRelOptMaterialization.IncrementalRebuildMode.INSERT_ONLY;
     }
     return HiveRelOptMaterialization.IncrementalRebuildMode.AVAILABLE;

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecOrcFileDump.java
Patch:
@@ -82,13 +82,13 @@ public void run(HookContext hookContext) throws Exception {
       List<Path> directories;
       if (partitionedTable) {
         LOG.info("Printing orc file dump for files from partitioned directory..");
-        directories = fetchWork.getPartDir();
+        directories = Lists.newArrayList(fetchWork.getPartDir());
       } else {
         LOG.info("Printing orc file dump for files from table directory..");
-        directories = Lists.newArrayList();
-        directories.add(fetchWork.getTblDir());
+        directories = Lists.newArrayList(fetchWork.getTblDir());
       }
 
+      Collections.sort(directories);
       for (Path dir : directories) {
         printFileStatus(console, dir.getFileSystem(conf), dir);
       }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java
Patch:
@@ -659,15 +659,16 @@ private boolean convertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcCon
       // Prepare updated partition columns for small table(s).
       // Get the positions of bucketed columns
 
-      int i = 0;
+      int bigTableExprPos = 0;
       Map<String, ExprNodeDesc> colExprMap = bigTableRS.getColumnExprMap();
       for (ExprNodeDesc bigTableExpr : bigTablePartitionCols) {
         // It is guaranteed there is only 1 list within listBucketCols.
         for (String colName : listBucketCols.get(0)) {
           if (colExprMap.get(colName).isSame(bigTableExpr)) {
-            positions.add(i++);
+            positions.add(bigTableExprPos);
           }
         }
+        bigTableExprPos = bigTableExprPos + 1;
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/handler/CompactionCleaner.java
Patch:
@@ -86,7 +86,7 @@ public List<Runnable> getTasks() throws MetaException {
             : 0;
     List<CompactionInfo> readyToClean = txnHandler.findReadyToClean(minOpenTxnId, retentionTime);
     if (!readyToClean.isEmpty()) {
-      long minTxnIdSeenOpen = txnHandler.findMinTxnIdSeenOpen();
+      long minTxnIdSeenOpen = Math.min(minOpenTxnId, txnHandler.findMinTxnIdSeenOpen());
       // For checking which compaction can be cleaned we can use the minOpenTxnId
       // However findReadyToClean will return all records that were compacted with old version of HMS
       // where the CQ_NEXT_TXN_ID is not set. For these compactions we need to provide minTxnIdSeenOpen

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/CreateTableDesc.java
Patch:
@@ -26,7 +26,6 @@
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.stream.Collectors;
 
 import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.fs.Path;
@@ -937,7 +936,9 @@ public Table toTable(HiveConf conf) throws HiveException {
             tbl.getTTable().getDictionary() : new ObjectDictionary();
         List<ByteBuffer> buffers = new ArrayList<>();
         String statsSetup = StatsSetupConst.ColumnStatsSetup.getStatsSetupAsString(true,
-            storageHandler != null && storageHandler.isMetadataTableSupported() ? "metadata" : null, // Skip metadata directory for Iceberg table
+            // Ignore all Iceberg leftover files when storageHandler.isMetadataTableSupported() is true,
+            // as the method is only enabled in Iceberg currently.
+            storageHandler != null && storageHandler.isMetadataTableSupported(),
             MetaStoreUtils.getColumnNames(tbl.getCols()));
         buffers.add(ByteBuffer.wrap(statsSetup.getBytes(StandardCharsets.UTF_8)));
         dictionary.putToValues(StatsSetupConst.STATS_FOR_CREATE_TABLE, buffers);

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/common/StatsSetupConst.java
Patch:
@@ -236,7 +236,7 @@ public static class ColumnStatsSetup {
     @JsonInclude(JsonInclude.Include.NON_DEFAULT)
     public boolean enabled;
     @JsonInclude(JsonInclude.Include.NON_DEFAULT)
-    public String fileToEscape;
+    public boolean isIcebergTable;
     @JsonInclude(JsonInclude.Include.NON_EMPTY)
     public List<String> columnNames = new ArrayList<>();
 
@@ -255,13 +255,13 @@ public static ColumnStatsSetup parseStatsSetup(String statsSetup) {
      * Get json representation of the ColumnStatsSetup
      */
     public static String getStatsSetupAsString(boolean enabled,
-        String fileToEscape,
+        boolean isIcebergTable,
         List<String> columns) {
       try {
         ColumnStatsSetup statsSetup = new ColumnStatsSetup();
         statsSetup.enabled = enabled;
+        statsSetup.isIcebergTable = isIcebergTable;
         statsSetup.columnNames = new ArrayList<>(columns);
-        statsSetup.fileToEscape = fileToEscape;
         return objectWriter.writeValueAsString(statsSetup);
       } catch (Exception e) {
         // this should not happen

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/GenericColumnVectorProducer.java
Patch:
@@ -220,6 +220,9 @@ public static List<TypeDescription> setTypeBuilderFromSchema(
         case TIMESTAMP:
           type.setKind(OrcProto.Type.Kind.TIMESTAMP);
           break;
+        case TIMESTAMP_INSTANT:
+          type.setKind(OrcProto.Type.Kind.TIMESTAMP_INSTANT);
+          break;
         case DATE:
           type.setKind(OrcProto.Type.Kind.DATE);
           break;

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
Patch:
@@ -2540,6 +2540,8 @@ public static TypeDescription convertTypeInfo(TypeInfo info) {
             return TypeDescription.createDate();
           case TIMESTAMP:
             return TypeDescription.createTimestamp();
+          case TIMESTAMPLOCALTZ:
+            return TypeDescription.createTimestampInstant();
           case BINARY:
             return TypeDescription.createBinary();
           case DECIMAL: {

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/dataconnector/jdbc/AbstractJDBCConnectorProvider.java
Patch:
@@ -346,6 +346,8 @@ protected String getDataType(String mySqlType, int size) {
       case "longblob":
       case "bytea":
       case "binary":
+      case "varbinary":
+      case "binary varying":
         return ColumnType.BINARY_TYPE_NAME;
       case "tinyint":
         return ColumnType.TINYINT_TYPE_NAME;
@@ -380,6 +382,7 @@ protected String getDataType(String mySqlType, int size) {
       case "timestampz":
       case "timez":
         return ColumnType.TIMESTAMPTZ_TYPE_NAME;
+      case "bool":
       case "boolean":
         return ColumnType.BOOLEAN_TYPE_NAME;
       default:

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -1956,7 +1956,9 @@ protected RelNode applyPreJoinOrderingTransforms(RelNode basePlan, RelMetadataPr
               HiveRemoveEmptySingleRules.SORT_INSTANCE,
               HiveRemoveEmptySingleRules.SORT_FETCH_ZERO_INSTANCE,
               HiveRemoveEmptySingleRules.AGGREGATE_INSTANCE,
-              HiveRemoveEmptySingleRules.UNION_INSTANCE);
+              HiveRemoveEmptySingleRules.UNION_INSTANCE,
+              HiveRemoveEmptySingleRules.CORRELATE_LEFT_INSTANCE,
+              HiveRemoveEmptySingleRules.CORRELATE_RIGHT_INSTANCE);
 
       // Trigger program
       perfLogger.perfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);

File: itests/qtest-iceberg/src/test/java/org/apache/hadoop/hive/cli/TestIcebergCliDriver.java
Patch:
@@ -21,6 +21,8 @@
 import java.util.List;
 import org.apache.hadoop.hive.cli.control.CliAdapter;
 import org.apache.hadoop.hive.cli.control.CliConfigs;
+import org.apache.hadoop.hive.ql.exec.tez.ObjectCache;
+import org.apache.tez.runtime.common.objectregistry.ObjectRegistryImpl;
 import org.junit.ClassRule;
 import org.junit.Rule;
 import org.junit.Test;
@@ -55,6 +57,7 @@ public TestIcebergCliDriver(String name, File qfile) {
 
   @Test
   public void testCliDriver() throws Exception {
+    ObjectCache.setupObjectRegistry(new ObjectRegistryImpl());
     adapter.runTest(name, qfile);
   }
 }

File: itests/qtest-iceberg/src/test/java/org/apache/hadoop/hive/cli/TestIcebergNegativeCliDriver.java
Patch:
@@ -21,6 +21,8 @@
 import java.util.List;
 import org.apache.hadoop.hive.cli.control.CliAdapter;
 import org.apache.hadoop.hive.cli.control.CliConfigs;
+import org.apache.hadoop.hive.ql.exec.tez.ObjectCache;
+import org.apache.tez.runtime.common.objectregistry.ObjectRegistryImpl;
 import org.junit.ClassRule;
 import org.junit.Rule;
 import org.junit.Test;
@@ -55,6 +57,7 @@ public TestIcebergNegativeCliDriver(String name, File qfile) {
 
   @Test
   public void testCliDriver() throws Exception {
+    ObjectCache.setupObjectRegistry(new ObjectRegistryImpl());
     adapter.runTest(name, qfile);
   }
 }

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyPrimitiveObjectInspectorFactory.java
Patch:
@@ -149,6 +149,8 @@ public static AbstractPrimitiveLazyObjectInspector<?> getLazyObjectInspector(
       return getLazyBooleanObjectInspector(lazyParams.isExtendedBooleanLiteral());
     case TIMESTAMP:
       return getLazyTimestampObjectInspector(lazyParams.getTimestampFormats());
+    case TIMESTAMPLOCALTZ:
+      return new LazyTimestampLocalTZObjectInspector((TimestampLocalTZTypeInfo)typeInfo, lazyParams.getTimestampFormats());
     default:
      return getLazyObjectInspector(typeInfo);
     }
@@ -172,9 +174,6 @@ public static AbstractPrimitiveLazyObjectInspector<?> getLazyObjectInspector(
     case DECIMAL:
       poi = new LazyHiveDecimalObjectInspector((DecimalTypeInfo)typeInfo);
       break;
-    case TIMESTAMPLOCALTZ:
-      poi = new LazyTimestampLocalTZObjectInspector((TimestampLocalTZTypeInfo)typeInfo);
-      break;
     default:
       throw new RuntimeException(
           "Primitive type " + typeInfo.getPrimitiveCategory() + " should not take parameters");

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java
Patch:
@@ -277,7 +277,8 @@ public static Object processReduceSinkToHashJoin(ReduceSinkOperator parentRS, Ma
         edgeType = EdgeType.CUSTOM_SIMPLE_EDGE;
       }
     }
-    if (edgeType == EdgeType.CUSTOM_EDGE) {
+    if (edgeType == EdgeType.CUSTOM_EDGE || (edgeType == EdgeType.CUSTOM_SIMPLE_EDGE && !mapJoinOp.getConf()
+        .isDynamicPartitionHashJoin())) {
       // disable auto parallelism for bucket map joins
       parentRS.getConf().setReducerTraits(EnumSet.of(FIXED));
     }

File: ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/translator/TestASTConverter.java
Patch:
@@ -129,7 +129,9 @@ void testEmptyPlanWithComplexTypes() {
           "               farray\n" +
           "               TOK_FUNCTION\n" +
           "                  array\n" +
-          "                  TOK_NULL\n" +
+          "                  TOK_FUNCTION\n" +
+          "                     TOK_INT\n" +
+          "                     TOK_NULL\n" +
           "               fmap\n" +
           "               TOK_FUNCTION\n" +
           "                  map\n" +

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncLogWithBaseDoubleToDouble.java
Patch:
@@ -40,7 +40,7 @@ public FuncLogWithBaseDoubleToDouble() {
 
   @Override
   protected double func(double d) {
-    return Math.log(d) / Math.log(base);
+    return StrictMath.log(d) / StrictMath.log(base);
   }
 
   public double getBase() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncLogWithBaseLongToDouble.java
Patch:
@@ -40,7 +40,7 @@ public FuncLogWithBaseLongToDouble() {
 
   @Override
   protected double func(long l) {
-    return Math.log((double) l) / Math.log(base);
+    return StrictMath.log((double) l) / StrictMath.log(base);
   }
 
   public double getBase() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncPowerDoubleToDouble.java
Patch:
@@ -43,7 +43,7 @@ public FuncPowerDoubleToDouble() {
 
   @Override
   public double func(double d) {
-    return Math.pow(d, power);
+    return StrictMath.pow(d, power);
   }
 
   public double getPower() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncPowerLongToDouble.java
Patch:
@@ -43,7 +43,7 @@ public FuncPowerLongToDouble() {
 
   @Override
   public double func(long l) {
-    return Math.pow((double) l, power);
+    return StrictMath.pow((double) l, power);
   }
 
   public double getPower() {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFAcos.java
Patch:
@@ -48,7 +48,7 @@ protected DoubleWritable doEvaluate(DoubleWritable a) {
     if (d < -1 || d > 1) {
       return null;
     } else {
-      result.set(Math.acos(d));
+      result.set(StrictMath.acos(d));
       return result;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFAsin.java
Patch:
@@ -48,7 +48,7 @@ protected DoubleWritable doEvaluate(DoubleWritable a) {
     if (d < -1 || d > 1) {
       return null;
     } else {
-      result.set(Math.asin(d));
+      result.set(StrictMath.asin(d));
       return result;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFAtan.java
Patch:
@@ -37,7 +37,7 @@ public class UDFAtan extends UDFMath {
 
   @Override
   protected DoubleWritable doEvaluate(DoubleWritable a) {
-    result.set(Math.atan(a.get()));
+    result.set(StrictMath.atan(a.get()));
     return result;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFSin.java
Patch:
@@ -42,7 +42,7 @@ public class UDFSin extends UDFMath {
    */
   @Override
   protected DoubleWritable doEvaluate(DoubleWritable a) {
-    result.set(Math.sin(a.get()));
+    result.set(StrictMath.sin(a.get()));
     return result;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFSqrt.java
Patch:
@@ -45,7 +45,7 @@ protected DoubleWritable doEvaluate(DoubleWritable a) {
     if (a.get() < 0) {
       return null;
     } else {
-      result.set(Math.sqrt(a.get()));
+      result.set(StrictMath.sqrt(a.get()));
       return result;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFTan.java
Patch:
@@ -40,7 +40,7 @@ public class UDFTan extends UDFMath {
    */
   @Override
   protected DoubleWritable doEvaluate(DoubleWritable a) {
-    result.set(Math.tan(a.get()));
+    result.set(StrictMath.tan(a.get()));
     return result;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCbrt.java
Patch:
@@ -64,7 +64,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
       return null;
     }
 
-    double cbrt = Math.cbrt(val);
+    double cbrt = StrictMath.cbrt(val);
     output.set(cbrt);
     return output;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPower.java
Patch:
@@ -125,7 +125,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
       return null;
     }
 
-    doubleWritable.set(Math.pow(((DoubleWritable)base).get(), ((DoubleWritable)power).get()));
+    doubleWritable.set(StrictMath.pow(((DoubleWritable)base).get(), ((DoubleWritable)power).get()));
     return doubleWritable;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -4746,7 +4746,6 @@ private static boolean isSubDir(Path srcf, Path destf, FileSystem srcFs, FileSys
       return false;
     }
 
-    LOG.debug("The source path is " + fullF1 + " and the destination path is " + fullF2);
     return fullF1.startsWith(fullF2);
   }
 

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java
Patch:
@@ -1327,7 +1327,7 @@ public void visit(LeafNode node) throws MetaException {
         return;
       }
 
-      String colTypeStr = partitionKeys.get(partColIndex).getType();
+      String colTypeStr = ColumnType.getTypeName(partitionKeys.get(partColIndex).getType());
       FilterType colType = FilterType.fromType(colTypeStr);
       if (colType == FilterType.Invalid) {
         filterBuffer.setError("Filter pushdown not supported for type " + colTypeStr);

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java
Patch:
@@ -465,7 +465,7 @@ public int getPartColIndexForFilter(
     private String getJdoFilterPushdownParam(int partColIndex,
                                              FilterBuilder filterBuilder, boolean canPushDownIntegral, List<FieldSchema> partitionKeys) throws MetaException {
       boolean isIntegralSupported = canPushDownIntegral && canJdoUseStringsWithIntegral();
-      String colType = partitionKeys.get(partColIndex).getType();
+      String colType = ColumnType.getTypeName(partitionKeys.get(partColIndex).getType());
       // Can only support partitions whose types are string, or maybe integers
       // Date/Timestamp data type value is considered as string hence pushing down to JDO.
       if (!ColumnType.StringTypes.contains(colType) && !ColumnType.DATE_TYPE_NAME.equalsIgnoreCase(colType)

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
Patch:
@@ -2299,9 +2299,9 @@ public void testPartitionFilter() throws Exception {
         .addCol("c1", ColumnType.STRING_TYPE_NAME)
         .addCol("c2", ColumnType.INT_TYPE_NAME)
         .addPartCol("p1", ColumnType.STRING_TYPE_NAME)
-        .addPartCol("p2", ColumnType.VARCHAR_TYPE_NAME)
+        .addPartCol("p2", "varchar(20)")
         .addPartCol("p3", ColumnType.INT_TYPE_NAME)
-        .addPartCol("p4", ColumnType.CHAR_TYPE_NAME)
+        .addPartCol("p4", "char(20)")
         .create(client, conf);
 
     tbl = client.getTable(dbName, tblName);

File: common/src/java/org/apache/hadoop/hive/conf/Constants.java
Patch:
@@ -102,6 +102,7 @@ public class Constants {
 
   public static final Pattern COMPACTION_POOLS_PATTERN = Pattern.compile("hive\\.compactor\\.worker\\.(.*)\\.threads");
   public static final String HIVE_COMPACTOR_WORKER_POOL = "hive.compactor.worker.pool";
+  public static final String HIVE_COMPACTOR_REBALANCE_ORDERBY = "hive.compactor.rebalance.orderby";
 
   public static final String HTTP_HEADER_REQUEST_TRACK = "X-Request-ID";
   public static final String TIME_POSTFIX_REQUEST_TRACK = "_TIME";

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java
Patch:
@@ -19,6 +19,7 @@
 
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.Constants;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
@@ -286,7 +287,7 @@ void createDb(String dbName) throws Exception {
 
     void createDb(String dbName, String poolName) throws Exception {
       executeStatementOnDriver("drop database if exists " + dbName + " cascade", driver);
-      executeStatementOnDriver("create database " + dbName + " WITH DBPROPERTIES('hive.compactor.worker.pool'='" + poolName + "')", driver);
+      executeStatementOnDriver("create database " + dbName + " WITH DBPROPERTIES('" + Constants.HIVE_COMPACTOR_WORKER_POOL + "'='" + poolName + "')", driver);
     }
 
     /**

File: ql/src/java/org/apache/hadoop/hive/ql/Compiler.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Schema;
 import org.apache.hadoop.hive.metastore.api.TxnType;
+import org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils;
 import org.apache.hadoop.hive.ql.exec.ExplainTask;
 import org.apache.hadoop.hive.ql.exec.FetchTask;
 import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;
@@ -263,7 +264,7 @@ private void setLastReplIdForDump(HiveConf conf) throws HiveException, TExceptio
 
   private void openTransaction(TxnType txnType) throws LockException, CommandProcessorException {
     if (DriverUtils.checkConcurrency(driverContext) && startImplicitTxn(driverContext.getTxnManager()) &&
-        !driverContext.getTxnManager().isTxnOpen() && txnType != TxnType.COMPACTION) {
+        !driverContext.getTxnManager().isTxnOpen() && !MetaStoreServerUtils.isCompactionTxn(txnType)) {
       String userFromUGI = DriverUtils.getUserFromUGI(driverContext);
       if (HiveOperation.REPLDUMP.equals(driverContext.getQueryState().getHiveOperation())
          || HiveOperation.REPLLOAD.equals(driverContext.getQueryState().getHiveOperation())) {

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -289,6 +289,7 @@ private boolean validateTxnList() throws CommandProcessorException {
           }
           // Since we're reusing the compiled plan, we need to update its start time for current run
           driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());
+          driverContext.setRetrial(false);
         }
         // Re-check snapshot only in case we had to release locks and open a new transaction,
         // otherwise exclusive locks should protect output tables/partitions in snapshot from concurrent writes.

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/storage/compact/AlterTableCompactOperation.java
Patch:
@@ -103,6 +103,7 @@ private CompactionResponse compact(Table table, String partitionName) throws Hiv
     req.setProperties(desc.getProperties());
     req.setInitiatorId(JavaUtils.hostname() + "-" + HiveMetaStoreClient.MANUALLY_INITIATED_COMPACTION);
     req.setInitiatorVersion(HiveMetaStoreClient.class.getPackage().getImplementationVersion());
+    req.setOrderByClause(desc.getOrderByClause());
     if (desc.getNumberOfBuckets() > 0) {
       req.setNumberOfBuckets(desc.getNumberOfBuckets());
     }

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/storage/concatenate/AlterTableConcatenateAnalyzer.java
Patch:
@@ -104,7 +104,7 @@ private void compactAcidTable(TableName tableName, Map<String, String> partition
     boolean isBlocking = !HiveConf.getBoolVar(conf, ConfVars.TRANSACTIONAL_CONCATENATE_NOBLOCK, false);
 
     AlterTableCompactDesc desc = new AlterTableCompactDesc(tableName, partitionSpec, CompactionType.MAJOR.name(), isBlocking,
-        poolName, 0, null);
+        poolName, 0, null, null);
     addInputsOutputsAlterTable(tableName, partitionSpec, desc, desc.getType(), false);
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(), desc)));
     setAcidDdlDesc(getTable(tableName), desc);

File: ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java
Patch:
@@ -1180,19 +1180,19 @@ private void verifyTxn1IsAborted(int compactionNum, Table t, CompactionType type
   }
 
   // With high timeout, but fast run we should finish without a problem
-  @Test(timeout=1000)
+  @Test(timeout=2000)
   public void testNormalRun() throws Exception {
     runTimeoutTest(10000, false, true);
   }
 
   // With low timeout, but slow run we should finish without a problem
-  @Test(timeout=1000)
+  @Test(timeout=2000)
   public void testTimeoutWithInterrupt() throws Exception {
     runTimeoutTest(1, true, false);
   }
 
   // With low timeout, but slow run we should finish without a problem, even if the interrupt is swallowed
-  @Test(timeout=1000)
+  @Test(timeout=2000)
   public void testTimeoutWithoutInterrupt() throws Exception {
     runTimeoutTest(1, true, true);
   }

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HMSHandler.java
Patch:
@@ -577,7 +577,7 @@ public RawStore getMS() throws MetaException {
     return getMSForConf(conf);
   }
 
-  public static RawStore getMSForConf(Configuration conf) throws MetaException {
+  public static synchronized RawStore getMSForConf(Configuration conf) throws MetaException {
     RawStore ms = getRawStore();
     if (ms == null) {
       ms = newRawStoreForConf(conf);

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
Patch:
@@ -1155,7 +1155,7 @@ public enum ConfVars {
     // Partition management task params
     PARTITION_MANAGEMENT_TASK_FREQUENCY("metastore.partition.management.task.frequency",
       "metastore.partition.management.task.frequency",
-      300, TimeUnit.SECONDS, "Frequency at which timer task runs to do automatic partition management for tables\n" +
+      6, TimeUnit.HOURS, "Frequency at which timer task runs to do automatic partition management for tables\n" +
       "with table property 'discover.partitions'='true'. Partition management include 2 pieces. One is partition\n" +
       "discovery and other is partition retention period. When 'discover.partitions'='true' is set, partition\n" +
       "management will look for partitions in table location and add partitions objects for it in metastore.\n" +

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorFilterOperator.java
Patch:
@@ -113,7 +113,7 @@ public void process(Object row, int tag) throws HiveException {
 
     //The selected vector represents selected rows.
     //Clone the selected vector
-    System.arraycopy(vrg.selected, 0, temporarySelected, 0, vrg.size);
+    System.arraycopy(vrg.selected, 0, temporarySelected, 0, vrg.selected.length);
     int [] selectedBackup = vrg.selected;
     vrg.selected = temporarySelected;
     int sizeBackup = vrg.size;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorTopNKeyOperator.java
Patch:
@@ -118,7 +118,7 @@ public void process(Object data, int tag) throws HiveException {
     incomingBatches++;
     // The selected vector represents selected rows.
     // Clone the selected vector
-    System.arraycopy(batch.selected, 0, temporarySelected, 0, batch.size);
+    System.arraycopy(batch.selected, 0, temporarySelected, 0, batch.selected.length);
     int [] selectedBackup = batch.selected;
     batch.selected = temporarySelected;
     int sizeBackup = batch.size;

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/data/DeleteReadTests.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
 import org.apache.iceberg.types.Types;
 import org.apache.iceberg.util.ArrayUtil;
+import org.apache.iceberg.util.CharSequenceSet;
 import org.apache.iceberg.util.Pair;
 import org.apache.iceberg.util.StructLikeSet;
 import org.apache.iceberg.util.StructProjection;
@@ -193,7 +194,7 @@ public void testPositionDeletes() throws IOException {
         Pair.of(dataFile.path(), 6L) // id = 122
     );
 
-    Pair<DeleteFile, Set<CharSequence>> posDeletes = FileHelpers.writeDeleteFile(
+    Pair<DeleteFile, CharSequenceSet> posDeletes = FileHelpers.writeDeleteFile(
         table, Files.localOutput(temp.newFile()), Row.of(0), deletes);
 
     table.newRowDelta()
@@ -225,7 +226,7 @@ public void testMixedPositionAndEqualityDeletes() throws IOException {
         Pair.of(dataFile.path(), 5L) // id = 121
     );
 
-    Pair<DeleteFile, Set<CharSequence>> posDeletes = FileHelpers.writeDeleteFile(
+    Pair<DeleteFile, CharSequenceSet> posDeletes = FileHelpers.writeDeleteFile(
         table, Files.localOutput(temp.newFile()), Row.of(0), deletes);
 
     table.newRowDelta()

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AbstractAlterTableOperation.java
Patch:
@@ -66,7 +66,7 @@ public int execute() throws HiveException {
     Table oldTable = context.getDb().getTable(desc.getDbTableName());
     List<Partition> partitions = getPartitions(oldTable, desc.getPartitionSpec(), context);
 
-    // Don't change the table object returned by the metastore, as we'll mess with it's caches.
+    // Don't change the table object returned by the metastore, as we'll mess with its caches.
     Table table = oldTable.copy();
 
     environmentContext = initializeEnvironmentContext(oldTable, desc.getEnvironmentContext());
@@ -91,7 +91,7 @@ private List<Partition> getPartitions(Table tbl, Map<String, String> partSpec, D
         partitions = new ArrayList<Partition>();
         Partition part = context.getDb().getPartition(tbl, partSpec, false);
         if (part == null) {
-          // User provided a fully specified partition spec but it doesn't exist, fail.
+          // User provided a fully specified partition spec, but it doesn't exist, fail.
           throw new HiveException(ErrorMsg.INVALID_PARTITION,
                 StringUtils.join(partSpec.keySet(), ',') + " for table " + tbl.getTableName());
 
@@ -109,7 +109,7 @@ private List<Partition> getPartitions(Table tbl, Map<String, String> partSpec, D
 
   private EnvironmentContext initializeEnvironmentContext(Table table, EnvironmentContext environmentContext) {
     EnvironmentContext result = environmentContext == null ? new EnvironmentContext() : environmentContext;
-    // do not need update stats in alter table/partition operations
+    // do not need to update stats in alter table/partition operations
     if (result.getProperties() == null ||
         result.getProperties().get(StatsSetupConst.DO_NOT_UPDATE_STATS) == null) {
       result.putToProperties(StatsSetupConst.DO_NOT_UPDATE_STATS, StatsSetupConst.TRUE);

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/DropTableOperation.java
Patch:
@@ -76,7 +76,7 @@ public int execute() throws HiveException {
        * to repeats. What can happen, sometimes, is that a drone processing a replication task can
        * have been abandoned for not returning in time, but still execute its task after a while,
        * which should not result in it mucking up data that has been impressed later on. So, for eg.,
-       * if we create partition P1, followed by droppping it, followed by creating it yet again,
+       * if we create partition P1, followed by dropping it, followed by creating it yet again,
        * the replication of that drop should not drop the newer partition if it runs after the destination
        * object is already in the newer state.
        *
@@ -91,7 +91,7 @@ public int execute() throws HiveException {
        */
       Map<String, String> dbParams = context.getDb().getDatabase(table.getDbName()).getParameters();
       if (!replicationSpec.allowEventReplacementInto(dbParams)) {
-        // Drop occured as part of replicating a drop, but the destination
+        // Drop occurred as part of replicating a drop, but the destination
         // table was newer than the event being replicated. Ignore, but drop
         // any partitions inside that are older.
         if (table.isPartitioned()) {

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/misc/properties/AbstractAlterTablePropertiesAnalyzer.java
Patch:
@@ -120,11 +120,11 @@ private boolean validate(TableName tableName, Map<String, String> properties) th
   }
 
   private boolean hasConstraintsEnabled(String tableName) throws SemanticException{
-    NotNullConstraint notNullConstriant = null;
+    NotNullConstraint notNullConstraint = null;
     DefaultConstraint defaultConstraint = null;
     try {
       // retrieve enabled NOT NULL constraint from metastore
-      notNullConstriant = Hive.get().getEnabledNotNullConstraints(db.getDatabaseCurrent().getName(), tableName);
+      notNullConstraint = Hive.get().getEnabledNotNullConstraints(db.getDatabaseCurrent().getName(), tableName);
       defaultConstraint = Hive.get().getEnabledDefaultConstraints(db.getDatabaseCurrent().getName(), tableName);
     } catch (Exception e) {
       if (e instanceof SemanticException) {
@@ -134,7 +134,7 @@ private boolean hasConstraintsEnabled(String tableName) throws SemanticException
       }
     }
     return
-        (notNullConstriant != null && !notNullConstriant.getNotNullConstraints().isEmpty()) ||
+        (notNullConstraint != null && !notNullConstraint.getNotNullConstraints().isEmpty()) ||
         (defaultConstraint != null && !defaultConstraint.getDefaultConstraints().isEmpty());
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/storage/archive/AbstractAlterTableArchiveAnalyzer.java
Patch:
@@ -47,7 +47,7 @@ public AbstractAlterTableArchiveAnalyzer(QueryState queryState) throws SemanticE
   }
 
   @Override
-  // partSpec coming from the input is not applicable here as archiver gets it's partitions from a different part of
+  // partSpec coming from the input is not applicable here as archiver gets its partitions from a different part of
   // the AST tree
   protected void analyzeCommand(TableName tableName, Map<String, String> partSpec, ASTNode command)
       throws SemanticException {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/AddToClassPathAction.java
Patch:
@@ -31,7 +31,7 @@
  * To try to add to the class path of the existing class loader; call the above without forceNewClassLoader=true.
  * Note that a class loader might be still created as fallback method.
  * <p>
- * This is slightly inconvenient, but forces the caller code to make the doPriviliged call, rather than us making the
+ * This is slightly inconvenient, but forces the caller code to make the doPrivileged call, rather than us making the
  * call on the caller's behalf, in accordance with the security guidelines at:
  * https://docs.oracle.com/javase/8/docs/technotes/guides/security/doprivileged.html
  */
@@ -73,7 +73,7 @@ private boolean useExistingClassLoader() {
       // The classloader may have been closed, Cannot add to the same instance
       return !udfClassLoader.isClosed();
     }
-    // Cannot use the same classloader if it is not an instance of {@code UDFClassLoader}, or new loader was explicily
+    // Cannot use the same classloader if it is not an instance of {@code UDFClassLoader}, or new loader was explicitly
     // requested
     return false;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLPlanUtils.java
Patch:
@@ -611,7 +611,7 @@ public String paramToValues(Map<String, String> parameters) {
    * Parses the basic table statistics for the given table.
    *
    * @param pt
-   * @return Returns the alter table .... update statistics for partititon.
+   * @return Returns the alter table .... update statistics for partition.
    */
   public String getAlterTableStmtPartitionStatsBasic(Partition pt) {
     Map<String, String> parameters = pt.getParameters();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java
Patch:
@@ -455,7 +455,7 @@ private JSONObject getLocks(PrintStream out, ExplainWork work) {
   public void addCreateTableStatement(Table table, List<String> tableCreateStmt , DDLPlanUtils ddlPlanUtils) {
     tableCreateStmt.add(ddlPlanUtils.getCreateTableCommand(table, false) + ";");
   }
-  
+
   public void addPKandBasicStats(Table tbl, List<String> basicDef, DDLPlanUtils ddlPlanUtils){
     String primaryKeyStmt = ddlPlanUtils.getAlterTableStmtPrimaryKeyConstraint(tbl.getPrimaryKeyInfo());
     if (primaryKeyStmt != null) {
@@ -793,7 +793,7 @@ else if (ent.getValue() != null) {
   /**
    * Retruns a map which have either primitive or string keys.
    *
-   * This is neccessary to discard object level comparators which may sort the objects based on some non-trivial logic.
+   * This is necessary to discard object level comparators which may sort the objects based on some non-trivial logic.
    *
    * @param mp
    * @return

File: ql/src/java/org/apache/hadoop/hive/ql/exec/errors/DataConstraintViolationError.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hadoop.hive.ql.ErrorMsg;
 
 /**
- * Error class, thrown when arguments's constraints violate.
+ * Error class, thrown when argument's constraints violate.
  */
 public class DataConstraintViolationError extends Error {
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/errors/DataCorruptErrorHeuristic.java
Patch:
@@ -24,7 +24,7 @@
 import java.util.regex.Pattern;
 
 /**
- * Detects the condition where there is a error with one of the input files in
+ * Detects the condition where there is an error with one of the input files in
  * the query.
  *
  * Conditions to check:

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java
Patch:
@@ -69,7 +69,7 @@ public void clear() {
   }
 
   /**
-   * For CompbineFileInputFormat, the mapper's input file will be changed on the
+   * For CombineFileInputFormat, the mapper's input file will be changed on the
    * fly, and the input file name is passed to jobConf by shims/initNextRecordReader.
    * If the map local work has any mapping depending on the current
    * mapper's input file, the work need to clear context and re-initialization

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java
Patch:
@@ -168,7 +168,7 @@ public void onRootVertexInitialized(String inputName, InputDescriptor inputDescr
     LOG.info("On root vertex initialized " + inputName);
     try {
       // This is using the payload from the RootVertexInitializer corresponding
-      // to InputName. Ideally it should be using it's own configuration class -
+      // to InputName. Ideally it should be using its own configuration class -
       // but that
       // means serializing another instance.
       MRInputUserPayloadProto protoPayload =

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WmTezSession.java
Patch:
@@ -93,7 +93,7 @@ public ListenableFuture<WmTezSession> waitForAmRegistryAsync(
       }
       if (amRegistryFuture != null) {
         // We don't need this for now, so do not support it.
-        future.setException(new RuntimeException("Multiple waits are not suported"));
+        future.setException(new RuntimeException("Multiple waits are not supported"));
         return future;
       }
       amRegistryFuture = future;
@@ -140,7 +140,7 @@ public void updateAmEndpointInfo(TezAmInstance si, int ephSeqVersion) {
       }
     }
   }
-  
+
 
   private void handleGuaranteedTasksChange(int guaranteedCount) {
     boolean doNotify = false;
@@ -210,7 +210,7 @@ Integer setSendingGuaranteed(Integer intAlloc) {
       return intAlloc;
     }
   }
-  
+
   public String getAllocationState() {
     synchronized (actualState) {
       return "actual/target " + actualState.sent + "/" + actualState.target

File: ql/src/java/org/apache/hadoop/hive/ql/exec/util/DAGTraversal.java
Patch:
@@ -36,8 +36,8 @@ public static void traverse(List<Task<?>> tasks, Function function) {
     List<Task<?>> listOfTasks = new ArrayList<>(tasks);
     while (!listOfTasks.isEmpty()) {
       // HashSet will make sure that no duplicate children are added. If a task is added multiple
-      // time to the children list then it may cause the list to grow exponentially. Lets take an example of
-      // incremental load with 2 events. The DAG will look some thing similar as below.
+      // time to the children list then it may cause the list to grow exponentially. Let's take an example of
+      // incremental load with 2 events. The DAG will look something similar as below.
       //
       //                 --->ev1.task1--                          --->ev2.task1--
       //                /               \                        /               \
@@ -48,7 +48,7 @@ public static void traverse(List<Task<?>> tasks, Function function) {
       // While traversing the DAG, if the filter is not added then  ev1.barrierTask will be added 3 times in
       // the children list and in next iteration ev2.task1 will be added 3 times and ev2.task2 will be added
       // 3 times. So in next iteration ev2.barrierTask will be added 6 times. As it goes like this, the next barrier
-      // task will be added 12-15 times and may reaches millions with large number of events.
+      // task will be added 12-15 times and may reach millions with large number of events.
       Set<Task<?>> children = new HashSet<>();
       for (Task<?> task : listOfTasks) {
         // skip processing has to be done first before continuing

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDateToTimestamp.java
Patch:
@@ -101,7 +101,7 @@ public void evaluate(VectorizedRowBatch batch) throws HiveException {
       } else {
         if (!outputColVector.noNulls) {
 
-          // Assume it is almost always a performance win to fill all of isNull so we can
+          // Assume it is almost always a performance win to fill all of isNull, so we can
           // safely reset noNulls.
           Arrays.fill(outputIsNull, false);
           outputColVector.noNulls = true;
@@ -124,7 +124,7 @@ public void evaluate(VectorizedRowBatch batch) throws HiveException {
           setDays(outputColVector, vector, i);
         }
       } else {
-        // Set isNull before calls in case tney change their mind.
+        // Set isNull before calls in case they change their mind.
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
         for(int i = 0; i != n; i++) {
           setDays(outputColVector, vector, i);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToDecimal.java
Patch:
@@ -118,7 +118,7 @@ public void evaluate(VectorizedRowBatch batch) throws HiveException {
       } else {
         if (!outputColVector.noNulls) {
 
-          // Assume it is almost always a performance win to fill all of isNull so we can
+          // Assume it is almost always a performance win to fill all of isNull, so we can
           // safely reset noNulls.
           Arrays.fill(outputIsNull, false);
           outputColVector.noNulls = true;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToLong.java
Patch:
@@ -40,7 +40,7 @@ public CastDecimalToLong(int inputColumn, int outputColumnNum) {
   protected void func(LongColumnVector outV, DecimalColumnVector inV,  int i) {
     HiveDecimalWritable decWritable = inV.vector[i];
 
-    // Check based on the Hive integer type we need to test with isByte, isShort, isInt, isLong
+    // Check based on the Hive integer type we need to test with isByte, isShort, isInt, isLong,
     // so we do not use corrupted (truncated) values for the Hive integer type.
     boolean isInRange;
     switch (integerPrimitiveCategory) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java
Patch:
@@ -234,7 +234,7 @@ public void processBatch(VectorizedRowBatch batch) throws HiveException {
             // Common anti join result processing.
             switch (saveJoinResult) {
             case MATCH:
-              allMatchs[allMatchCount++] = batchIndex;
+              allMatches[allMatchCount++] = batchIndex;
               // VectorizedBatchUtil.debugDisplayOneRow(batch, batchIndex, CLASS_NAME + " MATCH isSingleValue " + equalKeySeriesIsSingleValue[equalKeySeriesCount] + " currentKey " + currentKey);
               break;
 
@@ -252,7 +252,7 @@ public void processBatch(VectorizedRowBatch batch) throws HiveException {
             // Series of equal keys.
             switch (saveJoinResult) {
             case MATCH:
-              allMatchs[allMatchCount++] = batchIndex;
+              allMatches[allMatchCount++] = batchIndex;
               // VectorizedBatchUtil.debugDisplayOneRow(batch, batchIndex, CLASS_NAME + " MATCH duplicate");
               break;
 
@@ -286,7 +286,7 @@ public void processBatch(VectorizedRowBatch batch) throws HiveException {
 
         if (LOG.isDebugEnabled()) {
           LOG.debug(CLASS_NAME +
-              " allMatchs " + intArrayToRangesString(allMatchs, allMatchCount) +
+              " allMatches " + intArrayToRangesString(allMatches, allMatchCount) +
               " spills " + intArrayToRangesString(spills, spillCount) +
               " spillHashMapResultIndices " + intArrayToRangesString(spillHashMapResultIndices, spillCount) +
               " hashMapResults " + Arrays.toString(Arrays.copyOfRange(hashSetResults, 0, hashSetResultCount)));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterStringOperator.java
Patch:
@@ -323,7 +323,7 @@ public void processBatch(VectorizedRowBatch batch) throws HiveException {
                 equalKeySeriesAllMatchIndices[equalKeySeriesCount] = allMatchCount;
                 equalKeySeriesIsSingleValue[equalKeySeriesCount] = hashMapResults[hashMapResultCount].isSingleRow();
                 equalKeySeriesDuplicateCounts[equalKeySeriesCount] = 1;
-                allMatchs[allMatchCount++] = batchIndex;
+                allMatches[allMatchCount++] = batchIndex;
                 break;
 
               case SPILL:
@@ -343,7 +343,7 @@ public void processBatch(VectorizedRowBatch batch) throws HiveException {
               switch (saveJoinResult) {
               case MATCH:
                 equalKeySeriesDuplicateCounts[equalKeySeriesCount]++;
-                allMatchs[allMatchCount++] = batchIndex;
+                allMatches[allMatchCount++] = batchIndex;
                 break;
 
               case SPILL:
@@ -376,7 +376,7 @@ public void processBatch(VectorizedRowBatch batch) throws HiveException {
 
         if (isDebugEnabled) {
           LOG.debug(CLASS_NAME + " batch #" + batchCounter +
-              " allMatchs " + intArrayToRangesString(allMatchs,allMatchCount) +
+              " allMatches " + intArrayToRangesString(allMatches,allMatchCount) +
               " equalKeySeriesHashMapResultIndices " + intArrayToRangesString(equalKeySeriesHashMapResultIndices, equalKeySeriesCount) +
               " equalKeySeriesAllMatchIndices " + intArrayToRangesString(equalKeySeriesAllMatchIndices, equalKeySeriesCount) +
               " equalKeySeriesIsSingleValue " + Arrays.toString(Arrays.copyOfRange(equalKeySeriesIsSingleValue, 0, equalKeySeriesCount)) +

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java
Patch:
@@ -279,7 +279,7 @@ public long getEstimatedMemorySize() {
     size += (2 * jdm.primitive2());
     size += (2 * jdm.primitive1());
     size += jdm.object();
-    // adding 16KB constant memory for keyBinarySortableDeserializeRead as the rabit hole is deep to implement
+    // adding 16KB constant memory for keyBinarySortableDeserializeRead as the rabbit hole is deep to implement
     // MemoryEstimate interface, also it is constant overhead
     size += (16 * 1024L);
     return size;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastStringHashMap.java
Patch:
@@ -57,7 +57,7 @@ public VectorMapJoinFastStringHashMap(
   @Override
   public long getEstimatedMemorySize() {
     long size = 0;
-    // adding 16KB constant memory for stringCommon as the rabit hole is deep to implement
+    // adding 16KB constant memory for stringCommon as the rabbit hole is deep to implement
     // MemoryEstimate interface, also it is constant overhead
     size += (16 * 1024L);
     return super.getEstimatedMemorySize() + size;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastStringHashMultiSet.java
Patch:
@@ -59,7 +59,7 @@ public VectorMapJoinFastStringHashMultiSet(
 
   @Override
   public long getEstimatedMemorySize() {
-    // adding 16KB constant memory for stringCommon as the rabit hole is deep to implement
+    // adding 16KB constant memory for stringCommon as the rabbit hole is deep to implement
     // MemoryEstimate interface, also it is constant overhead
     long size = (16 * 1024L);
     return super.getEstimatedMemorySize() + size;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastStringHashSet.java
Patch:
@@ -52,7 +52,7 @@ public VectorMapJoinFastStringHashSet(
 
   @Override
   public long getEstimatedMemorySize() {
-    // adding 16KB constant memory for stringCommon as the rabit hole is deep to implement
+    // adding 16KB constant memory for stringCommon as the rabbit hole is deep to implement
     // MemoryEstimate interface, also it is constant overhead
     long size = (16 * 1024L);
     return super.getEstimatedMemorySize() + size;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastValueStore.java
Patch:
@@ -343,7 +343,7 @@ public ByteSegmentRef internalRead() {
 
         if (isNextLast) {
           /*
-           * No realativeOffsetWord in last value.  (This was the first value written.)
+           * No relativeOffsetWord in last value.  (This was the first value written.)
            */
           isNextEof = true;
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringHashSet.java
Patch:
@@ -64,7 +64,7 @@ public VectorMapJoinOptimizedStringHashSet(boolean isOuterJoin, MapJoinTableCont
 
   @Override
   public long getEstimatedMemorySize() {
-    // adding 16KB constant memory for stringCommon as the rabit hole is deep to implement
+    // adding 16KB constant memory for stringCommon as the rabbit hole is deep to implement
     // MemoryEstimate interface, also it is constant overhead
     long size = (16 * 1024L);
     return super.getEstimatedMemorySize() + size;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/rowbytescontainer/VectorRowBytesContainer.java
Patch:
@@ -105,7 +105,7 @@ private void initFile() {
     }
   }
 
-  public Output getOuputForRowBytes() {
+  public Output getOutputForRowBytes() {
     if (!isOpen) {
       initFile();
       isOpen = true;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java
Patch:
@@ -350,7 +350,7 @@ private static ExprNodeDesc foldExprShortcut(ExprNodeDesc desc, Map<ColumnInfo,
       // Don't evaluate nondeterministic function since the value can only calculate during runtime.
       if (!isConstantFoldableUdf(udf, newExprs)) {
         if (LOG.isDebugEnabled()) {
-          LOG.debug("Function " + udf.getClass() + " is undeterministic. Don't evaluate immediately.");
+          LOG.debug("Function " + udf.getClass() + " is not deterministic. Don't evaluate immediately.");
         }
         ((ExprNodeGenericFuncDesc) desc).setChildren(newExprs);
         return desc;
@@ -407,7 +407,7 @@ private static ExprNodeDesc foldExprFull(ExprNodeDesc desc, Map<ColumnInfo, Expr
       // Don't evaluate nondeterministic function since the value can only calculate during runtime.
       if (!isConstantFoldableUdf(udf, newExprs)) {
         if (LOG.isDebugEnabled()) {
-          LOG.debug("Function " + udf.getClass() + " is undeterministic. Don't evaluate immediately.");
+          LOG.debug("Function " + udf.getClass() + " is not deterministic. Don't evaluate immediately.");
         }
         ((ExprNodeGenericFuncDesc) desc).setChildren(newExprs);
         return desc;
@@ -967,7 +967,7 @@ private static ExprNodeDesc evaluateFunction(GenericUDF udf, List<ExprNodeDesc>
             ObjectInspectorUtils.copyToStandardJavaObject(o, coi));
       } else if (!PrimitiveObjectInspectorUtils.isPrimitiveJavaClass(clz)) {
         if (LOG.isErrorEnabled()) {
-          LOG.error("Unable to evaluate {}({}). Return value unrecoginizable.",
+          LOG.error("Unable to evaluate {}({}). Return value unrecognizable.",
               udf.getClass().getName(), exprs);
         }
         return null;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java
Patch:
@@ -721,7 +721,7 @@ private boolean convertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcCon
 
 
   /**
-   * Preserves additional informations about the operator.
+   * Preserves additional information about the operator.
    *
    * When an operator is replaced by a new one; some of the information of the old have to be retained.
    */

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java
Patch:
@@ -35,7 +35,7 @@ public enum UnsupportedFeature {
     Less_than_equal_greater_than, Others, Same_name_in_multiple_expressions,
     Schema_less_table, Select_alias_in_having_clause, Select_transform, Subquery,
     Table_sample_clauses, UDTF, Union_type, Unique_join,
-    HighPrecissionTimestamp // CALCITE-1690
+    HighPrecisionTimestamp // CALCITE-1690
   };
 
   private UnsupportedFeature unsupportedFeature;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/AbstractCorrelationProcCtx.java
Patch:
@@ -37,7 +37,7 @@ abstract class AbstractCorrelationProcCtx implements NodeProcessorCtx {
 
   // This is min number of reducer for deduped RS to avoid query executed on
   // too small number of reducers. For example, queries GroupBy+OrderBy can be executed by
-  // only one reducer if this configuration does not prevents
+  // only one reducer if this configuration does not prevent
   private final int minReducer;
   private final Set<Operator<?>> removedOps;
   private final boolean isMapAggr;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -1895,7 +1895,7 @@ private ImmutablePair<Boolean, Boolean> validateInputFormatAndSchemaEvolution(Ma
         if (isPartitionRowConversion && isLlapIoEnabled) {
           enabledConditionsNotMetList.add(
               "Could not enable vectorization. " +
-              "LLAP I/O is enabled wbich automatically deserializes into " +
+              "LLAP I/O is enabled which automatically deserializes into " +
               "VECTORIZED_INPUT_FILE_FORMAT. " +
               "A partition requires data type conversion and that is not supported");
 
@@ -1990,7 +1990,7 @@ private void validateAndVectorizeMapWork(MapWork mapWork, VectorTaskColumnInfo v
         supportRemovedReasons.add(removeString);
       }
 
-      // Now rememember what is supported for this query and any support that was
+      // Now remember what is supported for this query and any support that was
       // removed.
       vectorTaskColumnInfo.setSupportSetInUse(supportSet);
       vectorTaskColumnInfo.setSupportRemovedReasons(supportRemovedReasons);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -5256,7 +5256,7 @@ private void validateNoHavingReferenceToAlias(QB qb, ASTNode havingExpr)
       QBParseInfo qbPI = qb.getParseInfo();
       Map<ASTNode, String> exprToAlias = qbPI.getAllExprToColumnAlias();
       /*
-       * a mouthful, but safe: - a QB is guaranteed to have atleast 1
+       * a mouthful, but safe: - a QB is guaranteed to have at least 1
        * destination - we don't support multi insert, so picking the first dest.
        */
       Set<String> aggExprs = qbPI.getDestToAggregationExprs().values().iterator().next().keySet();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/CommitTxnHandler.java
Patch:
@@ -77,7 +77,7 @@ private void createDumpFile(Context withinContext, org.apache.hadoop.hive.ql.met
     }
 
     Path metaDataPath = new Path(withinContext.eventRoot, EximUtil.METADATA_NAME);
-    // In case of ACID operations, same directory may have many other sub directory for different write id stmt id
+    // In case of ACID operations, same directory may have many other subdirectory for different write id stmt id
     // combination. So we can not set isreplace to true.
     withinContext.replicationSpec.setIsReplace(false);
     EximUtil.createExportDump(metaDataPath.getFileSystem(withinContext.hiveConf), metaDataPath,

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java
Patch:
@@ -208,7 +208,7 @@ public List<Task<?>> getTasks(HiveConf conf, Object objCtx) {
    * 3. merge task followed by a move task.
    * It used to be true for dynamic partition only since static partition doesn't have #3.
    * It changes w/ list bucketing. Static partition has #3 since it has sub-directories.
-   * For example, if a static partition is defined as skewed and stored-as-directores,
+   * For example, if a static partition is defined as skewed and stored-as-directories,
    * instead of all files in one directory, it will create a sub-dir per skewed value plus
    * default directory. So #3 is required for static partition.
    * So, we move it to a method so that it can be used by both SP and DP.

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/AlterPartitionEvent.java
Patch:
@@ -94,8 +94,8 @@ private String buildCommandString(String cmdStr, String tbl, Partition partition
 
     if (tbl != null) {
       String tblName    = (StringUtils.isNotEmpty(tbl) ? " " + tbl : "");
-      String partionStr = (partition != null) ? partition.toString() : "";
-      ret               = String.format(cmdStr, tblName, partionStr);
+      String partitionStr = (partition != null) ? partition.toString() : "";
+      ret               = String.format(cmdStr, tblName, partitionStr);
     }
 
     return ret;

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactionQueryBuilder.java
Patch:
@@ -207,7 +207,7 @@ CompactionQueryBuilder setIsDeleteDelta(boolean deleteDelta) {
    * @param compactionType major or minor; crud or insert-only, e.g. CompactionType.MAJOR_CRUD.
    *                       Cannot be null.
    * @param operation query's Operation e.g. Operation.CREATE.
-   * @param insertOnly Inidicated if the table is not full ACID but Insert-only (Micromanaged)
+   * @param insertOnly Indicates if the table is not full ACID but Insert-only (Micromanaged)
    * @throws IllegalArgumentException if compactionType is null
    */
   CompactionQueryBuilder(CompactionType compactionType, Operation operation, boolean insertOnly,

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/BaseMaskUDF.java
Patch:
@@ -98,8 +98,8 @@ public String getDisplayString(String[] children) {
  */
 abstract class AbstractTransformer {
   /**
-   * Initialzie the transformer object
-   * @param arguments arguments given to GenericUDF.initialzie()
+   * Initialize the transformer object
+   * @param arguments arguments given to GenericUDF.initialize()
    * @param startIdx index into array, from which the transformer should read values
    */
   abstract void init(ObjectInspector[] arguments, int startIdx);

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFLeadLag.java
Patch:
@@ -65,7 +65,7 @@ public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo parameters)
       Object o = ((ConstantObjectInspector) amtOI).getWritableConstantValue();
       amt = ((IntWritable) o).get();
       if (amt < 0) {
-        throw new UDFArgumentTypeException(1, fNm + " amount can not be nagative. Specified: " + amt );
+        throw new UDFArgumentTypeException(1, fNm + " amount can not be negative. Specified: " + amt );
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMax.java
Patch:
@@ -168,10 +168,10 @@ public GenericUDAFEvaluator getWindowingEvaluator(WindowFrameDef wFrmDef) {
    * the ith step o/p the front of the queue as the max for the ith entry.
    *
    * Here we modify the algorithm: 1. to handle window's that are of the form
-   * (i-p, i+f), where p is numPreceding,f = numFollowing - we start outputing
+   * (i-p, i+f), where p is numPreceding,f = numFollowing - we start outputting
    * rows only after receiving f rows. - the formula for 'influence range' of an
    * idx accounts for the following rows. 2. optimize for the case when
-   * numPreceding is Unbounded. In this case only 1 max needs to be tarcked at
+   * numPreceding is Unbounded. In this case only 1 max needs to be tracked at
    * any given time.
    */
   static class MaxStreamingFixedWindow extends

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFLeadLag.java
Patch:
@@ -91,7 +91,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen
       Object o = ((ConstantObjectInspector) amtOI).getWritableConstantValue();
       amt = ((IntWritable) o).get();
       if (amt < 0) {
-        throw new UDFArgumentTypeException(1,  " amount can not be nagative. Specified: " + amt);
+        throw new UDFArgumentTypeException(1,  " amount can not be negative. Specified: " + amt);
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/wm/WmContext.java
Patch:
@@ -64,7 +64,7 @@ public class WmContext implements PrintSummary {
   private Set<String> subscribedCounters = new HashSet<>();
   @JsonProperty("currentCounters")
   private Map<String, Long> currentCounters = new HashMap<>();
-  @JsonIgnore // explictly ignoring as Getter visibility is ANY for auto-json serialization of Trigger based on getters
+  @JsonIgnore // explicitly ignoring as Getter visibility is ANY for auto-json serialization of Trigger based on getters
   private Future<Boolean> returnEventFuture;
 
   public WmContext(final long queryStartTime, final String queryId) {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorRowBytesContainer.java
Patch:
@@ -38,7 +38,7 @@ public void doFillReplay(Random random, int maxCount) throws Exception {
     int count = Math.min(maxCount, random.nextInt(500));
     for (int i = 0; i < count; i++) {
       byte[] bytes = randomByteArrayStream.next();
-      Output output = vectorMapJoinRowBytesContainer.getOuputForRowBytes();
+      Output output = vectorMapJoinRowBytesContainer.getOutputForRowBytes();
       output.write(bytes);
       vectorMapJoinRowBytesContainer.finishRow();
     }

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOptimisedBootstrap.java
Patch:
@@ -547,7 +547,7 @@ public void testReverseReplicationFailureWhenSourceDbIsDropped() throws Throwabl
 
     // this load should throw exception
     List<String> finalWithClause = withClause;
-    assertThrows("Should fail with db doesn't exist exception", HiveException.class, () -> {
+    assertThrows("Should fail with db doesn't exist exception", SemanticException.class, () -> {
       primary.load(primaryDbName, replicatedDbName, finalWithClause);
     });
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/OptimisedBootstrapUtils.java
Patch:
@@ -88,7 +88,7 @@ public class OptimisedBootstrapUtils {
    * @return true, if the database has repl.target.for property set.
    * @throws HiveException
    */
-  public static boolean isFailover(String dbName, Hive hive) throws HiveException {
+  public static boolean isDbTargetOfFailover(String dbName, Hive hive) throws HiveException {
     Database database = hive.getDatabase(dbName);
     return database != null ? MetaStoreUtils.isTargetOfReplication(database) : false;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/metric/PreOptimizedBootstrapDumpMetricCollector.java
Patch:
@@ -23,7 +23,8 @@
 
 
 public class PreOptimizedBootstrapDumpMetricCollector extends ReplicationMetricCollector {
-  public PreOptimizedBootstrapDumpMetricCollector(String dbName, String stagingDir, HiveConf conf, Long executorId) {
-    super(dbName, Metadata.ReplicationType.PRE_OPTIMIZED_BOOTSTRAP, stagingDir, executorId, conf);
+  public PreOptimizedBootstrapDumpMetricCollector(String dbName, String stagingDir, HiveConf conf, Long executorId,
+                                                  String failoverEndpoint, String failOverType) {
+    super(dbName, Metadata.ReplicationType.PRE_OPTIMIZED_BOOTSTRAP, stagingDir, executorId, conf, failoverEndpoint, failOverType);
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/metric/OptimizedBootstrapLoadMetricCollector.java
Patch:
@@ -26,7 +26,8 @@
  * Bootstrap Load Metric Collector
  */
 public class OptimizedBootstrapLoadMetricCollector extends ReplicationMetricCollector {
-  public OptimizedBootstrapLoadMetricCollector(String dbName, String stagingDir, long dumpExecutionId, HiveConf conf) {
-    super(dbName, Metadata.ReplicationType.OPTIMIZED_BOOTSTRAP, stagingDir, dumpExecutionId, conf);
+  public OptimizedBootstrapLoadMetricCollector(String dbName, String stagingDir, long dumpExecutionId, HiveConf conf,
+                                               String failoverEndpoint, String failoverType) {
+    super(dbName, Metadata.ReplicationType.OPTIMIZED_BOOTSTRAP, stagingDir, dumpExecutionId, conf, failoverEndpoint, failoverType);
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/metric/PreOptimizedBootstrapLoadMetricCollector.java
Patch:
@@ -23,7 +23,8 @@
 
 
 public class PreOptimizedBootstrapLoadMetricCollector extends ReplicationMetricCollector {
-  public PreOptimizedBootstrapLoadMetricCollector(String dbName, String stagingDir, long dumpExecutionId, HiveConf conf) {
-    super(dbName, Metadata.ReplicationType.PRE_OPTIMIZED_BOOTSTRAP, stagingDir, dumpExecutionId, conf);
+  public PreOptimizedBootstrapLoadMetricCollector(String dbName, String stagingDir, long dumpExecutionId, HiveConf conf,
+                                                  String failoverEndpoint, String failoverType) {
+    super(dbName, Metadata.ReplicationType.PRE_OPTIMIZED_BOOTSTRAP, stagingDir, dumpExecutionId, conf, failoverEndpoint, failoverType);
   }
 }

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
Patch:
@@ -2098,7 +2098,7 @@ public void testMultiDBTxn() throws Throwable {
       replica.loadWithoutExplain("", "`*`");
       fail();
     } catch (HiveException e) {
-      assertEquals("MetaException(message:Database name cannot be null.)", e.getMessage());
+      assertEquals("REPL LOAD Target database name shouldn't be null", e.getMessage());
     }
   }
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
Patch:
@@ -701,7 +701,7 @@ public void testBootStrapDumpOfWarehouse() throws Throwable {
       replica.load("", "`*`");
       Assert.fail();
     } catch (HiveException e) {
-      assertEquals("MetaException(message:Database name cannot be null.)", e.getMessage());
+      assertEquals("REPL LOAD Target database name shouldn't be null", e.getMessage());
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java
Patch:
@@ -320,7 +320,8 @@ private void analyzeReplLoad(ASTNode ast) throws SemanticException {
     // looking at each db, and then each table, and then setting up the appropriate
     // import job in its place.
     try {
-      assert (sourceDbNameOrPattern != null);
+      Objects.requireNonNull(sourceDbNameOrPattern, "REPL LOAD Source database name shouldn't be null");
+      Objects.requireNonNull(replScope.getDbName(), "REPL LOAD Target database name shouldn't be null");
       Path loadPath = getCurrentLoadPath();
 
       // Now, the dumped path can be one of three things:
@@ -370,7 +371,7 @@ private void analyzeReplLoad(ASTNode ast) throws SemanticException {
         }
       } else {
         ReplUtils.reportStatusInReplicationMetrics("REPL_LOAD", Status.SKIPPED, null, conf,  sourceDbNameOrPattern, null);
-        LOG.warn("Previous Dump Already Loaded");
+        LOG.warn("No dump to load or the previous dump already loaded");
       }
     } catch (Exception e) {
       // TODO : simple wrap & rethrow for now, clean up with error codes

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java
Patch:
@@ -22,12 +22,14 @@
 import static org.apache.commons.lang3.StringUtils.normalizeSpace;
 import static org.apache.commons.lang3.StringUtils.repeat;
 import static org.apache.hadoop.hive.metastore.ColumnType.BIGINT_TYPE_NAME;
+import static org.apache.hadoop.hive.metastore.ColumnType.CHAR_TYPE_NAME;
 import static org.apache.hadoop.hive.metastore.ColumnType.DATE_TYPE_NAME;
 import static org.apache.hadoop.hive.metastore.ColumnType.INT_TYPE_NAME;
 import static org.apache.hadoop.hive.metastore.ColumnType.SMALLINT_TYPE_NAME;
 import static org.apache.hadoop.hive.metastore.ColumnType.STRING_TYPE_NAME;
 import static org.apache.hadoop.hive.metastore.ColumnType.TIMESTAMP_TYPE_NAME;
 import static org.apache.hadoop.hive.metastore.ColumnType.TINYINT_TYPE_NAME;
+import static org.apache.hadoop.hive.metastore.ColumnType.VARCHAR_TYPE_NAME;
 
 import java.sql.Connection;
 import java.sql.SQLException;
@@ -1261,7 +1263,7 @@ protected boolean shouldStop() {
 
     private static enum FilterType {
       Integral(ImmutableSet.of(TINYINT_TYPE_NAME, SMALLINT_TYPE_NAME, INT_TYPE_NAME, BIGINT_TYPE_NAME), Long.class),
-      String(ImmutableSet.of(STRING_TYPE_NAME), String.class),
+      String(ImmutableSet.of(STRING_TYPE_NAME, CHAR_TYPE_NAME, VARCHAR_TYPE_NAME), String.class),
       Date(ImmutableSet.of(DATE_TYPE_NAME), java.sql.Date.class),
       Timestamp(ImmutableSet.of(TIMESTAMP_TYPE_NAME), java.sql.Timestamp.class),
 

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java
Patch:
@@ -458,7 +458,7 @@ private String getJdoFilterPushdownParam(int partColIndex,
       String colType = partitionKeys.get(partColIndex).getType();
       // Can only support partitions whose types are string, or maybe integers
       // Date/Timestamp data type value is considered as string hence pushing down to JDO.
-      if (!ColumnType.STRING_TYPE_NAME.equalsIgnoreCase(colType) && !ColumnType.DATE_TYPE_NAME.equalsIgnoreCase(colType)
+      if (!ColumnType.StringTypes.contains(colType) && !ColumnType.DATE_TYPE_NAME.equalsIgnoreCase(colType)
           && !ColumnType.TIMESTAMP_TYPE_NAME.equalsIgnoreCase(colType)
           && (!isIntegralSupported || !ColumnType.IntegralTypes.contains(colType))) {
         filterBuilder.setError("Filtering is supported only on partition keys of type " +

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HMSHandler.java
Patch:
@@ -5987,7 +5987,7 @@ private void alter_partitions_with_environment_context(String catName, String db
           .defaultMetaException();
     } finally {
       tableLock.unlock();
-      endFunction("alter_partition", oldParts != null, ex, tbl_name);
+      endFunction("alter_partitions", oldParts != null, ex, tbl_name);
     }
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/optimizer/calcite/TestCBORuleFiredOnlyOnce.java
Patch:
@@ -61,8 +61,7 @@ public void testRuleFiredOnlyOnce() {
 
     // Create rules registry to not trigger a rule more than once
     HiveRulesRegistry registry = new HiveRulesRegistry();
-    HivePlannerContext context = new HivePlannerContext(null, registry, null,
-        null, null, null);
+    HivePlannerContext context = new HivePlannerContext(null, registry, null, null, null);
     HepPlanner planner = new HepPlanner(programBuilder.build(), context);
 
     // Cluster

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java
Patch:
@@ -456,7 +456,8 @@ private String getJdoFilterPushdownParam(int partColIndex,
       boolean isIntegralSupported = canPushDownIntegral && canJdoUseStringsWithIntegral();
       String colType = partitionKeys.get(partColIndex).getType();
       // Can only support partitions whose types are string, or maybe integers
-      if (!colType.equals(ColumnType.STRING_TYPE_NAME)
+      // Date data type value is considered as string hence pushing down to JDO.
+      if (!colType.equals(ColumnType.STRING_TYPE_NAME) && !colType.equals(ColumnType.DATE_TYPE_NAME)
           && (!isIntegralSupported || !ColumnType.IntegralTypes.contains(colType))) {
         filterBuilder.setError("Filtering is supported only on partition keys of type " +
             "string" + (isIntegralSupported ? ", or integral types" : ""));

File: common/src/java/org/apache/hive/common/util/DateUtils.java
Patch:
@@ -73,7 +73,7 @@ public static int parseNumericValueWithRange(String fieldName,
    * @param field the calendar field
    * @return the calendar field name
    * @exception IndexOutOfBoundsException if <code>field</code> is negative,
-   * equal to or greater then <code>FIELD_COUNT</code>.
+   * equal to or greater than <code>FIELD_COUNT</code>.
    */
   public static String getFieldName(int field) {
       return FIELD_NAME[field];

File: jdbc/src/java/org/apache/hive/jdbc/Utils.java
Patch:
@@ -99,6 +99,7 @@ public static class JdbcConnectionParams {
     public static final String AUTH_PASSWD = "password";
     public static final String AUTH_KERBEROS_AUTH_TYPE = "kerberosAuthType";
     public static final String AUTH_KERBEROS_AUTH_TYPE_FROM_SUBJECT = "fromSubject";
+    public static final String AUTH_KERBEROS_ENABLE_CANONICAL_HOSTNAME_CHECK = "kerberosEnableCanonicalHostnameCheck";
     public static final String AUTH_TYPE_JWT = "jwt";
     public static final String AUTH_TYPE_JWT_KEY = "jwt";
     public static final String AUTH_JWT_ENV = "JWT";

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -529,6 +529,8 @@ public static enum ConfVars {
             + "task increment that would cross the specified limit."),
     REPL_PARTITIONS_DUMP_PARALLELISM("hive.repl.partitions.dump.parallelism",100,
         "Number of threads that will be used to dump partition data information during repl dump."),
+    REPL_TABLE_DUMP_PARALLELISM("hive.repl.table.dump.parallelism", 15,
+            "Number of threads that will be used to dump table data information during repl dump."),
     REPL_RUN_DATA_COPY_TASKS_ON_TARGET("hive.repl.run.data.copy.tasks.on.target", true,
             "Indicates whether replication should run data copy tasks during repl load operation."),
     REPL_DUMP_METADATA_ONLY("hive.repl.dump.metadata.only", false,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExportTask.java
Patch:
@@ -50,7 +50,7 @@ public int execute() {
       work.acidPostProcess(db);
       TableExport tableExport = new TableExport(exportPaths, work.getTableSpec(),
           work.getReplicationSpec(), db, null, conf, work.getMmContext());
-      tableExport.write(true, null, false);
+      tableExport.serialWrite(true, null, false);
     } catch (Exception e) {
       LOG.error("failed", e);
       setException(e);

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -1684,7 +1684,7 @@ public Table getTable(final String dbName, final String tableName, String metaTa
       if (checkTransactional) {
         ValidWriteIdList validWriteIdList = null;
         long txnId = SessionState.get() != null && SessionState.get().getTxnMgr() != null ?
-             SessionState.get().getTxnMgr().getCurrentTxnId() : 0;
+            SessionState.get().getTxnMgr().getCurrentTxnId() : 0;
         if (txnId > 0) {
           validWriteIdList = AcidUtils.getTableValidWriteIdListWithTxnList(conf, dbName, tableName);
         }
@@ -1752,7 +1752,7 @@ public Table getTable(final String dbName, final String tableName, String metaTa
    */
   private ValidWriteIdList getValidWriteIdList(String dbName, String tableName) throws LockException {
     ValidWriteIdList validWriteIdList = null;
-    long txnId = SessionState.get().getTxnMgr() != null ? SessionState.get().getTxnMgr().getCurrentTxnId() : 0;
+    long txnId = SessionState.get() != null && SessionState.get().getTxnMgr() != null ? SessionState.get().getTxnMgr().getCurrentTxnId() : 0;
     if (txnId > 0) {
       validWriteIdList = AcidUtils.getTableValidWriteIdListWithTxnList(conf, dbName, tableName);
     }

File: ql/src/test/org/apache/hadoop/hive/ql/exec/repl/TestReplDumpTask.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.parse.EximUtil;
+import org.apache.hadoop.hive.ql.parse.repl.dump.ExportService;
 import org.apache.hadoop.hive.ql.parse.repl.dump.HiveWrapper;
 import org.apache.hadoop.hive.ql.parse.repl.dump.Utils;
 import org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData;
@@ -131,7 +132,7 @@ public void removeDBPropertyToPreventRenameWhenBootstrapDumpOfTableFails() throw
       private int tableDumpCount = 0;
 
       @Override
-      void dumpTable(String dbName, String tblName, String validTxnList,
+      void dumpTable(ExportService exportService, String dbName, String tblName, String validTxnList,
                      Path dbRootMetadata, Path dbRootData,
                      long lastReplId, Hive hiveDb,
                      HiveWrapper.Tuple<Table> tuple, FileList managedTableDirFileList, boolean dataCopyAtLoad)

File: service/src/java/org/apache/hive/service/server/HiveServer2.java
Patch:
@@ -1008,6 +1008,7 @@ public synchronized void stop() {
       // reinitialized after a HS2 is restarted.
       HiveSaml2Client.shutdown();
     }
+
   }
 
   private void shutdownExecutor(final ExecutorService leaderActionsExecutorService) {

File: kudu-handler/src/java/org/apache/hadoop/hive/kudu/KuduHiveUtils.java
Patch:
@@ -115,7 +115,7 @@ public static void importCredentialsFromCurrentSubject(KuduClient client) {
     }
   }
 
-  /* This method converts a Kudu type to to the corresponding Hive type */
+  /* This method converts a Kudu type to the corresponding Hive type */
   public static PrimitiveTypeInfo toHiveType(Type kuduType, ColumnTypeAttributes attributes)
       throws SerDeException {
     switch (kuduType) {

File: kudu-handler/src/test/org/apache/hadoop/hive/kudu/TestKuduInputFormat.java
Patch:
@@ -143,7 +143,7 @@ public void testAllColumns() throws Exception {
         (KuduRecordReader) input.getRecordReader(split, jobConf, null);
     assertTrue(reader.nextKeyValue());
     RowResult value = reader.getCurrentValue().getRowResult();
-    verfiyRow(value);
+    verifyRow(value);
     assertFalse(reader.nextKeyValue());
   }
 
@@ -313,11 +313,11 @@ public void testPredicate() throws Exception {
           (KuduRecordReader) input.getRecordReader(split, jobConf, null);
       assertTrue(reader.nextKeyValue());
       RowResult value = reader.getCurrentValue().getRowResult();
-      verfiyRow(value);
+      verifyRow(value);
       assertFalse("Extra row on column: " + col.getName(), reader.nextKeyValue());
     }
   }
-  private void verfiyRow(RowResult value) {
+  private void verifyRow(RowResult value) {
     assertEquals(SCHEMA.getColumnCount(), value.getSchema().getColumnCount());
     assertEquals(ROW.getByte(0), value.getByte(0));
     assertEquals(ROW.getShort(1), value.getShort(1));

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/drop/AbstractDropPartitionAnalyzer.java
Patch:
@@ -140,7 +140,8 @@ private void addTableDropPartsOutputs(Table table, Collection<List<ExprNodeGener
         try {
           hasUnknown = db.getPartitionsByExpr(table, partitionSpec, conf, parts);
         } catch (Exception e) {
-          throw new SemanticException(ErrorMsg.INVALID_PARTITION.getMsg(partitionSpec.getExprString()), e);
+          throw new SemanticException("Error fetching partitions for " + partitionSpec.getExprString() +
+             ", message: " + e.getMessage(), e);
         }
         if (hasUnknown) {
           throw new SemanticException("Unexpected unknown partitions for " + partitionSpec.getExprString());

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java
Patch:
@@ -855,9 +855,6 @@ private boolean doReadField(Field field) {
 
               decimalIsNull = !currentHiveDecimalWritable.mutateEnforcePrecisionScale(precision, scale);
               if (!decimalIsNull) {
-                if (HiveDecimalWritable.isPrecisionDecimal64(precision)) {
-                  currentDecimal64 = currentHiveDecimalWritable.serialize64(scale);
-                }
                 return true;
               }
             }

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/Decimal64ColumnVector.java
Patch:
@@ -23,7 +23,7 @@
 /**
 
  */
-public class Decimal64ColumnVector extends LongColumnVector {
+public class Decimal64ColumnVector extends LongColumnVector implements IDecimalColumnVector {
 
   public short scale;
   public short precision;

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 
-public class DecimalColumnVector extends ColumnVector {
+public class DecimalColumnVector extends ColumnVector implements IDecimalColumnVector {
 
   /**
    * A vector of HiveDecimalWritable objects.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java
Patch:
@@ -28,6 +28,7 @@
 import java.lang.annotation.Annotation;
 import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
+import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
@@ -549,7 +550,7 @@ public int execute() {
     try {
       Path resFile = work.getResFile();
       OutputStream outS = resFile.getFileSystem(conf).create(resFile);
-      out = new PrintStream(outS);
+      out = new PrintStream(outS, false, StandardCharsets.UTF_8.name());
 
       if(work.isDDL()){
         getDDLPlan(out);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java
Patch:
@@ -205,7 +205,7 @@ public static ConstantVectorExpression create(int outputColumnNum, Object consta
               outputColumnNum, (HiveDecimal) constantValue, outputTypeInfo);
         case STRING:
           return new ConstantVectorExpression(
-              outputColumnNum, ((String) constantValue).getBytes(), outputTypeInfo);
+              outputColumnNum, ((String) constantValue).getBytes(StandardCharsets.UTF_8), outputTypeInfo);
         case VARCHAR:
           return new ConstantVectorExpression(
               outputColumnNum, ((HiveVarchar) constantValue), outputTypeInfo);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -4158,7 +4158,7 @@ public static void addDependencyJars(Configuration conf, Class<?>... classes)
       if (!localFs.exists(new Path(path))) {
         throw new RuntimeException("Could not validate jar file " + path + " for class " + clazz);
       }
-      jars.add(path);
+      jars.add(localFs.makeQualified(new Path(path)).toString());
     }
     if (jars.isEmpty()) {
       return;

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java
Patch:
@@ -173,8 +173,6 @@ private static RecordReader<NullWritable, VectorizedRowBatch> orcRecordReader(Jo
     // If LLAP enabled, try to retrieve an LLAP record reader - this might yield to null in some special cases
     if (HiveConf.getBoolVar(job, HiveConf.ConfVars.LLAP_IO_ENABLED, LlapProxy.isDaemon()) &&
         LlapProxy.getIo() != null) {
-      // Required to prevent LLAP from dealing with decimal64, HiveIcebergInputFormat.getSupportedFeatures()
-      HiveConf.setVar(job, HiveConf.ConfVars.HIVE_VECTORIZED_INPUT_FORMAT_SUPPORTS_ENABLED, "");
       recordReader = LlapProxy.getIo().llapVectorizedOrcReaderForPath(fileId, path, null, readColumnIds,
           job, start, length, reporter);
     }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3068,7 +3068,7 @@ public static enum ConfVars {
         "Enables non-blocking DROP TABLE operation.\n" +
         "If enabled, every table directory would be suffixed with the corresponding table creation txnId."),
     
-    HIVE_ACID_TRUNCATE_USE_BASE("hive.acid.truncate.usebase", false,
+    HIVE_ACID_TRUNCATE_USE_BASE("hive.acid.truncate.usebase", true,
         "If enabled, truncate for transactional tables will not delete the data directories,\n" +
         "rather create a new base directory with no datafiles."),
     

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java
Patch:
@@ -149,6 +149,7 @@ void initHiveConf() {
     //of these tests.
     HiveConf.setBoolVar(hiveConf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, false);
     HiveConf.setBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTIMIZEMETADATAQUERIES, false);
+    HiveConf.setBoolVar(hiveConf, HiveConf.ConfVars.HIVE_ACID_TRUNCATE_USE_BASE, false);
   }
   
   @Override

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/BinaryColumnStatsAggregator.java
Patch:
@@ -32,6 +32,8 @@ public class BinaryColumnStatsAggregator extends ColumnStatsAggregator {
   @Override
   public ColumnStatisticsObj aggregate(List<ColStatsObjWithSourceInfo> colStatsWithSourceInfo,
       List<String> partNames, boolean areAllPartsFound) throws MetaException {
+    checkStatisticsList(colStatsWithSourceInfo);
+
     ColumnStatisticsObj statsObj = null;
     String colType = null;
     String colName = null;

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/BooleanColumnStatsAggregator.java
Patch:
@@ -32,6 +32,8 @@ public class BooleanColumnStatsAggregator extends ColumnStatsAggregator {
   @Override
   public ColumnStatisticsObj aggregate(List<ColStatsObjWithSourceInfo> colStatsWithSourceInfo,
       List<String> partNames, boolean areAllPartsFound) throws MetaException {
+    checkStatisticsList(colStatsWithSourceInfo);
+
     ColumnStatisticsObj statsObj = null;
     String colType = null;
     String colName = null;

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/DoubleColumnStatsAggregator.java
Patch:
@@ -48,6 +48,8 @@ public class DoubleColumnStatsAggregator extends ColumnStatsAggregator implement
   @Override
   public ColumnStatisticsObj aggregate(List<ColStatsObjWithSourceInfo> colStatsWithSourceInfo,
       List<String> partNames, boolean areAllPartsFound) throws MetaException {
+    checkStatisticsList(colStatsWithSourceInfo);
+
     ColumnStatisticsObj statsObj = null;
     String colType = null;
     String colName = null;

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java
Patch:
@@ -42,11 +42,13 @@
 public class StringColumnStatsAggregator extends ColumnStatsAggregator implements
     IExtrapolatePartStatus {
 
-  private static final Logger LOG = LoggerFactory.getLogger(LongColumnStatsAggregator.class);
+  private static final Logger LOG = LoggerFactory.getLogger(StringColumnStatsAggregator.class);
 
   @Override
   public ColumnStatisticsObj aggregate(List<ColStatsObjWithSourceInfo> colStatsWithSourceInfo,
       List<String> partNames, boolean areAllPartsFound) throws MetaException {
+    checkStatisticsList(colStatsWithSourceInfo);
+
     ColumnStatisticsObj statsObj = null;
     String colType = null;
     String colName = null;

File: iceberg/iceberg-catalog/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
Patch:
@@ -70,6 +70,7 @@
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 import org.apache.iceberg.util.Tasks;
+import org.apache.parquet.hadoop.ParquetOutputFormat;
 import org.apache.thrift.TException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -96,8 +97,8 @@ public class HiveTableOperations extends BaseMetastoreTableOperations {
 
   private static final BiMap<String, String> ICEBERG_TO_HMS_TRANSLATION = ImmutableBiMap.of(
       // gc.enabled in Iceberg and external.table.purge in Hive are meant to do the same things but with different names
-      GC_ENABLED, "external.table.purge"
-  );
+      GC_ENABLED, "external.table.purge",
+      TableProperties.PARQUET_COMPRESSION, ParquetOutputFormat.COMPRESSION);
 
   private static Cache<String, ReentrantLock> commitLockCache;
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -3359,7 +3359,7 @@ public Object post(Object t) {
         ASTNode root = (ASTNode) t;
         if (root.getType() == HiveParser.TOK_FUNCTION) {
           ASTNode func = (ASTNode) ParseDriver.adaptor.getChild(root, 0);
-          if (func.getText().equals("grouping") && func.getChildCount() == 0) {
+          if ("grouping".equalsIgnoreCase(func.getText()) && func.getChildCount() == 0) {
             int numberOperands = ParseDriver.adaptor.getChildCount(root);
             // We implement this logic using replaceChildren instead of replacing
             // the root node itself because windowing logic stores multiple

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationScenariosAcidTables.java
Patch:
@@ -117,7 +117,7 @@ static void internalBeforeClassSetup(Map<String, String> overrides, Class clazz)
     replicaNonAcid = new WarehouseInstance(LOG, miniDFSCluster, overridesForHiveConf1);
   }
 
-  private static void setReplicaExternalBase(FileSystem fs, Map<String, String> confMap) throws IOException {
+  protected static void setReplicaExternalBase(FileSystem fs, Map<String, String> confMap) throws IOException {
     fs.mkdirs(REPLICA_EXTERNAL_BASE);
     fullyQualifiedReplicaExternalBase =  fs.getFileStatus(REPLICA_EXTERNAL_BASE).getPath().toString();
     confMap.put(HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR.varname, fullyQualifiedReplicaExternalBase);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -66,7 +66,6 @@
 import org.apache.hadoop.hive.ql.udf.esri.ST_GeomFromText;
 import org.apache.hadoop.hive.ql.udf.esri.ST_GeomFromWKB;
 import org.apache.hadoop.hive.ql.udf.esri.ST_GeometryN;
-import org.apache.hadoop.hive.ql.udf.esri.ST_GeometryProcessing;
 import org.apache.hadoop.hive.ql.udf.esri.ST_GeometryType;
 import org.apache.hadoop.hive.ql.udf.esri.ST_InteriorRingN;
 import org.apache.hadoop.hive.ql.udf.esri.ST_Intersection;
@@ -703,7 +702,6 @@ public final class FunctionRegistry {
     system.registerFunction("ST_GeodesicLengthWGS84", ST_GeodesicLengthWGS84.class);
     system.registerFunction("ST_GeomCollection", ST_GeomCollection.class);
     system.registerFunction("ST_GeometryN", ST_GeometryN.class);
-    system.registerFunction("ST_GeometryProcessing", ST_GeometryProcessing.class);
     system.registerFunction("ST_GeomFromGeoJson", ST_GeomFromGeoJson.class);
     system.registerFunction("ST_GeomFromJson", ST_GeomFromJson.class);
     system.registerFunction("ST_GeomFromShape", ST_GeomFromShape.class);

File: beeline/src/test/org/apache/hive/beeline/TestBeelineArgParsing.java
Patch:
@@ -113,9 +113,9 @@ public static Collection<Object[]> data() throws IOException, InterruptedExcepti
         + File.separator + "org"
         + File.separator + "postgresql"
         + File.separator + "postgresql"
-        + File.separator + "42.2.14"
+        + File.separator + "42.4.1"
         + File.separator
-        + "postgresql-42.2.14.jar";
+        + "postgresql-42.4.1.jar";
     return Arrays.asList(new Object[][] {
         { "jdbc:postgresql://host:5432/testdb", "org.postgresql.Driver", pathToPostgresJar, true },
         { "jdbc:dummy://host:5432/testdb", dummyDriverClazzName, pathToDummyDriver, false } });

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniHS2.java
Patch:
@@ -1091,7 +1091,7 @@ public void testHttpRetryOnServerIdleTimeout() throws Exception {
     stopMiniHS2();
     HiveConf conf = new HiveConf();
     // Set server's idle timeout to a very low value
-    conf.setVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_MAX_IDLE_TIME, "5");
+    conf.setVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_MAX_IDLE_TIME, "5000");
     startMiniHS2(conf, true);
     String userName = System.getProperty("user.name");
     Connection conn = getConnection(miniHS2.getJdbcURL(testDbName)+";retries=3", userName, "password");

File: service/src/java/org/apache/hive/service/servlet/QueryProfileServlet.java
Patch:
@@ -53,7 +53,7 @@ public void doGet(HttpServletRequest request, HttpServletResponse response)
       LOG.debug("No display object found for operation {} ", opId);
       return;
     }
-
+    response.setContentType("text/html; charset=utf-8");
     new QueryProfileTmpl().render(response.getWriter(), queryInfo, hiveConf);
   }
 }

File: jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputFormat.java
Patch:
@@ -20,10 +20,8 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.Constants;
 import org.apache.hadoop.hive.ql.io.HiveInputFormat;
-import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.MapWritable;
 import org.apache.hadoop.mapred.FileInputFormat;
@@ -98,7 +96,7 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
         if (!columnNames.contains(partitionColumn)) {
           throw new IOException("Cannot find partitionColumn:" + partitionColumn + " in " + columnNames);
         }
-        List<TypeInfo> hiveColumnTypesList = TypeInfoUtils.getTypeInfosFromTypeString(job.get(serdeConstants.LIST_COLUMN_TYPES));
+        List<TypeInfo> hiveColumnTypesList = dbAccessor.getColumnTypes(job);
         TypeInfo typeInfo = hiveColumnTypesList.get(columnNames.indexOf(partitionColumn));
         if (!(typeInfo instanceof PrimitiveTypeInfo)) {
           throw new IOException(partitionColumn + " is a complex type, only primitive type can be a partition column");

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4640,6 +4640,9 @@ public static enum ConfVars {
             + "numRows,impala_.*chunk.*   comma separated and mixed (handles strings and regexes at the same time)"),
     HIVE_AM_SPLIT_GENERATION("hive.compute.splits.in.am", true,
         "Whether to generate the splits locally or in the AM (tez only)"),
+    HIVE_SPLITS_AVAILABLE_SLOTS_CALCULATOR_CLASS("hive.splits.available.slots.calculator.class.name",
+        "org.apache.hadoop.hive.ql.exec.tez.TezAvailableSlotsCalculator",
+        "Class to use for calculating available slots during split generation"),
     HIVE_TEZ_GENERATE_CONSISTENT_SPLITS("hive.tez.input.generate.consistent.splits", true,
         "Whether to generate consistent split locations when generating splits in the AM"),
     HIVE_PREWARM_ENABLED("hive.prewarm.enabled", false, "Enables container prewarm for Tez(Hadoop 2 only)"),

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4689,7 +4689,7 @@ public static enum ConfVars {
     HIVE_ACID_DIRECT_INSERT_ENABLED("hive.acid.direct.insert.enabled", true,
         "Enable writing the data files directly to the table's final destination instead of the staging directory."
         + "This optimization only applies on INSERT operations on ACID tables."),
-    TXN_CTAS_X_LOCK("hive.txn.xlock.ctas", false,
+    TXN_CTAS_X_LOCK("hive.txn.xlock.ctas", true,
         "Enables exclusive locking for CTAS operations."),
     // role names are case-insensitive
     USERS_IN_ADMIN_ROLE("hive.users.in.admin.role", "", false,

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java
Patch:
@@ -450,9 +450,10 @@ private Collection<LockComponent> getGlobalLocks(Configuration conf) {
       }
       LockComponentBuilder compBuilder = new LockComponentBuilder();
       compBuilder.setExclusive();
-      compBuilder.setOperationType(DataOperationType.UPDATE);
+      compBuilder.setOperationType(DataOperationType.NO_TXN);
       compBuilder.setDbName(GLOBAL_LOCKS);
       compBuilder.setTableName(lockName);
+
       globalLocks.add(compBuilder.build());
       LOG.debug("Adding global lock: " + lockName);
     }

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergInserts.java
Patch:
@@ -334,8 +334,6 @@ public void testMultiTableInsert() throws IOException {
 
   @Test
   public void testStructMapWithNull() throws IOException {
-    Assume.assumeTrue("Vectorized parquet read throws class cast exception",
-        !(fileFormat == FileFormat.PARQUET && isVectorized));
     Schema schema = new Schema(required(1, "id", Types.LongType.get()),
         required(2, "mapofstructs", Types.MapType.ofRequired(3, 4, Types.StringType.get(),
             Types.StructType.of(required(5, "something", Types.StringType.get()),

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2237,6 +2237,9 @@ public static enum ConfVars {
         "Whether to use former Java date/time APIs to convert between timezones when writing timestamps in " +
         "Parquet files. Once data are written to the file the effect is permanent (also reflected in the metadata)." +
         "Changing the value of this property affects only new data written to the file."),
+    HIVE_PARQUET_INFER_BINARY_AS("hive.parquet.infer.binary.as", "binary", new StringSet("binary", "string"),
+        "This setting controls what the parquet binary type gets inferred as by CREATE TABLE LIKE FILE. This is helpful " +
+        "since some systems specify the parquet schema for strings as binary."),
     HIVE_AVRO_TIMESTAMP_SKIP_CONVERSION("hive.avro.timestamp.skip.conversion", false,
         "Some older Hive implementations (pre-3.1) wrote Avro timestamps in a UTC-normalized" +
         "manner, while from version 3.1 until now Hive wrote time zone agnostic timestamps. " +

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -1682,7 +1682,7 @@ private List<String> getTxnDbsUpdated(long txnId, Connection dbConn) throws Meta
     try {
       try (Statement stmt = dbConn.createStatement()) {
 
-        String query = "SELECT DISTINCT T2W_DATABASE " +
+        String query = "SELECT DISTINCT \"T2W_DATABASE\" " +
                 " FROM \"TXN_TO_WRITE_ID\" \"COMMITTED\"" +
                 "   WHERE \"T2W_TXNID\" = " + txnId;
 

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyOutputTableLocationSchemeIsFileHook.java
Patch:
@@ -25,7 +25,7 @@ public class VerifyOutputTableLocationSchemeIsFileHook implements ExecuteWithHoo
   @Override
   public void run(HookContext hookContext) {
     for (WriteEntity output : hookContext.getOutputs()) {
-      if (output.getType() == WriteEntity.Type.TABLE) {
+      if (output.getType() == WriteEntity.Type.TABLE && hookContext.getInputs().iterator().next().getT().equals(output.getT())) {
         String scheme = output.getTable().getDataLocation().toUri().getScheme();
         Assert.assertTrue(output.getTable().getTableName() + " has a location which has a " +
               "scheme other than file: " + scheme, scheme.equals("file"));

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/GrantPrivAuthUtils.java
Patch:
@@ -62,7 +62,7 @@ private static void checkRequiredPrivileges(
 
     // get privileges for this user and its roles on this object
     RequiredPrivileges availPrivs = SQLAuthorizationUtils.getPrivilegesFromMetaStore(
-        metastoreClient, userName, hivePrivObject, curRoles, isAdmin);
+        metastoreClient, userName, hivePrivObject, curRoles, isAdmin, false);
 
     // check if required privileges is subset of available privileges
     List<String> deniedMessages = new ArrayList<String>();

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreUtils.java
Patch:
@@ -504,7 +504,7 @@ public static Map<String, String> getMetaStoreSaslProperties(Configuration conf,
 
   /**
    * Returns currently known class paths as best effort. For system class loader, this may return
-   * In such cases we will anyway create new child class loader in {@link #addToClassPath(ClassLo
+   * In such cases we will anyway create new child class loader in {@link #addToClassPath(ClassLoader cloader, String[] newPaths)
    * so all new class paths will be added and next time we will have a URLClassLoader to work wit
    */
   private static List<URL> getCurrentClassPaths(ClassLoader parentLoader) {

File: common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -384,6 +384,8 @@ public enum ErrorMsg {
   MASKING_FILTERING_ON_MATERIALIZED_VIEWS_SOURCES(10288,
       "Querying directly materialized view contents is not supported since we detected {0}.{1} " +
           "used by materialized view has row masking/column filtering enabled", true),
+  MASKING_COMPLEX_TYPE_NOT_SUPPORTED(10289,
+      "Masking complex types is not supported, found a masking expression {0} over column {1}:{2}", true),
 
   UPDATEDELETE_PARSE_ERROR(10290, "Encountered parse error while parsing rewritten merge/update or " +
       "delete query"),

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java
Patch:
@@ -666,7 +666,7 @@ public void onAbortTxn(AbortTxnEvent abortTxnEvent, Connection dbConn, SQLGenera
       return;
     }
     AbortTxnMessage msg =
-        MessageBuilder.getInstance().buildAbortTxnMessage(abortTxnEvent.getTxnId());
+        MessageBuilder.getInstance().buildAbortTxnMessage(abortTxnEvent.getTxnId(), abortTxnEvent.getDbsUpdated());
     NotificationEvent event =
         new NotificationEvent(0, now(), EventType.ABORT_TXN.toString(),
             msgEncoder.getSerializer().serialize(msg));

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/messaging/AbortTxnMessage.java
Patch:
@@ -17,6 +17,8 @@
 
 package org.apache.hadoop.hive.metastore.messaging;
 
+import java.util.List;
+
 /**
  * HCat message sent when an abort transaction is done.
  */
@@ -33,4 +35,5 @@ protected AbortTxnMessage() {
    */
   public abstract Long getTxnId();
 
+  public abstract List<String> getDbsUpdated();
 }

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/messaging/MessageBuilder.java
Patch:
@@ -296,8 +296,8 @@ public CommitTxnMessage buildCommitTxnMessage(Long txnId) {
     return new JSONCommitTxnMessage(MS_SERVER_URL, MS_SERVICE_PRINCIPAL, txnId, now());
   }
 
-  public AbortTxnMessage buildAbortTxnMessage(Long txnId) {
-    return new JSONAbortTxnMessage(MS_SERVER_URL, MS_SERVICE_PRINCIPAL, txnId, now());
+  public AbortTxnMessage buildAbortTxnMessage(Long txnId, List<String> dbsUpdated) {
+    return new JSONAbortTxnMessage(MS_SERVER_URL, MS_SERVICE_PRINCIPAL, txnId, now(), dbsUpdated);
   }
 
   public AllocWriteIdMessage buildAllocWriteIdMessage(List<TxnToWriteId> txnToWriteIdList,

File: ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java
Patch:
@@ -421,9 +421,9 @@ public static boolean isLocklessReadsEnabled(Table table, HiveConf conf) {
   }
 
   public static boolean isTableSoftDeleteEnabled(Table table, HiveConf conf) {
-    return (HiveConf.getBoolVar(conf, ConfVars.HIVE_ACID_CREATE_TABLE_USE_SUFFIX)
-        || HiveConf.getBoolVar(conf, ConfVars.HIVE_ACID_LOCKLESS_READS_ENABLED))
-      && AcidUtils.isTransactionalTable(table)
+    boolean isSoftDelete = HiveConf.getBoolVar(conf, ConfVars.HIVE_ACID_CREATE_TABLE_USE_SUFFIX)
+      || HiveConf.getBoolVar(conf, ConfVars.HIVE_ACID_LOCKLESS_READS_ENABLED);
+    return isSoftDelete && AcidUtils.isTransactionalTable(table)
       && Boolean.parseBoolean(table.getProperty(SOFT_DELETE_TABLE));
   }
 

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/AcidEventListener.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.metastore.api.EnvironmentContext;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.HiveObjectType;
+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.hive.metastore.api.Table;
@@ -239,7 +240,7 @@ private TxnStore getTxnHandler() {
   private long getTxnId(EnvironmentContext context) {
     return Optional.ofNullable(context)
       .map(EnvironmentContext::getProperties)
-      .map(prop -> prop.get("txnId"))
+      .map(prop -> prop.get(hive_metastoreConstants.TXN_ID))
       .map(Long::parseLong)
       .orElse(0L);
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
Patch:
@@ -1088,9 +1088,8 @@ private boolean needBootstrapAcidTablesDuringIncrementalDump() {
     // of the ACID tables might be included for bootstrap during incremental dump. For old policy, its because the table
     // may not satisfying the old policy but satisfying the new policy. For filter, it may happen that the table
     // is renamed and started satisfying the policy.
-    return ((!work.replScope.includeAllTables())
-            || (previousReplScopeModified())
-            || conf.getBoolVar(HiveConf.ConfVars.REPL_BOOTSTRAP_ACID_TABLES));
+    return !work.replScope.includeAllTables() || previousReplScopeModified() || !tablesForBootstrap.isEmpty()
+            || conf.getBoolVar(HiveConf.ConfVars.REPL_BOOTSTRAP_ACID_TABLES);
   }
 
   private void dumpEvent(NotificationEvent ev, Path evRoot, Path dumpRoot, Path cmRoot, Hive db) throws Exception {

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/misc/msck/MsckAnalyzer.java
Patch:
@@ -87,7 +87,8 @@ public void analyzeInternal(ASTNode root) throws SemanticException {
     }
 
     if (repair && AcidUtils.isTransactionalTable(table)) {
-      outputs.add(new WriteEntity(table, WriteType.DDL_EXCLUSIVE));
+      outputs.add(new WriteEntity(table, AcidUtils.isLocklessReadsEnabled(table, conf) ? 
+          WriteType.DDL_EXCL_WRITE : WriteType.DDL_EXCLUSIVE));
     } else {
       outputs.add(new WriteEntity(table, WriteEntity.WriteType.DDL_SHARED));
     }

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/DropTableAnalyzer.java
Patch:
@@ -35,8 +35,6 @@
 import org.apache.hadoop.hive.ql.parse.ReplicationSpec;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 
-import static org.apache.hadoop.hive.common.AcidConstants.SOFT_DELETE_TABLE;
-
 /**
  * Analyzer for table dropping commands.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/drop/DropMaterializedViewAnalyzer.java
Patch:
@@ -34,8 +34,6 @@
 import org.apache.hadoop.hive.ql.parse.HiveParser;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 
-import static org.apache.hadoop.hive.common.AcidConstants.SOFT_DELETE_TABLE;
-
 /**
  * Analyzer for drop materialized view commands.
  */

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/DbTxnManagerEndToEndTestBase.java
Patch:
@@ -75,13 +75,14 @@ public void setUp() throws Exception {
     }
     SessionState.start(conf);
     ctx = new Context(conf);
-    driver = new Driver(new QueryState.Builder().withHiveConf(conf).nonIsolated().build());
-    driver2 = new Driver(new QueryState.Builder().withHiveConf(conf).build());
 
     HiveConf.setIntVar(conf, HiveConf.ConfVars.HIVE_LOCKS_PARTITION_THRESHOLD, -1);
     HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_ACID_LOCKLESS_READS_ENABLED, false);
     HiveConf.setBoolVar(conf, HiveConf.ConfVars.TXN_WRITE_X_LOCK, false);
     MetastoreConf.setBoolVar(conf, MetastoreConf.ConfVars.TXN_USE_MIN_HISTORY_LEVEL, true);
+
+    driver = new Driver(new QueryState.Builder().withHiveConf(conf).nonIsolated().build());
+    driver2 = new Driver(new QueryState.Builder().withHiveConf(conf).build());
     
     TestTxnDbUtil.cleanDb(conf);
     SessionState ss = SessionState.get();

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -1616,7 +1616,7 @@ public void dropDatabase(DropDatabaseRequest req)
         // First we delete the materialized views
         Table mview = getTable(getDefaultCatalog(conf), req.getName(), table);
         boolean isSoftDelete = req.isSoftDelete() && Boolean.parseBoolean(
-          mview.getParameters().getOrDefault(SOFT_DELETE_TABLE, "false"));
+          mview.getParameters().get(SOFT_DELETE_TABLE));
         mview.setTxnId(req.getTxnId());
         dropTable(mview, req.isDeleteData() && !isSoftDelete, true, false);
       }
@@ -1675,7 +1675,7 @@ private void dropDatabaseCascadePerTable(DropDatabaseRequest req, List<String> t
           hook.preDropTable(table);
         }
         boolean isSoftDelete = req.isSoftDelete() && Boolean.parseBoolean(
-          table.getParameters().getOrDefault(SOFT_DELETE_TABLE, "false"));
+          table.getParameters().get(SOFT_DELETE_TABLE));
         EnvironmentContext context = null;
         if (req.isSetTxnId()) {
           context = new EnvironmentContext();

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
Patch:
@@ -252,9 +252,8 @@ private static class OrcRecordReader
 
     OrcRecordReader(Reader file, Configuration conf,
                     FileSplit split) throws IOException {
-      List<OrcProto.Type> types = file.getTypes();
       this.file = file;
-      numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();
+      numColumns = file.getSchema().getChildren().size();
       this.offset = split.getStart();
       this.length = split.getLength();
       this.reader = createReaderFromFile(file, conf, offset, length);

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewInputFormat.java
Patch:
@@ -66,8 +66,7 @@ private static class OrcRecordReader
 
     OrcRecordReader(Reader file, Configuration conf,
                     long offset, long length) throws IOException {
-      List<OrcProto.Type> types = file.getTypes();
-      numColumns = (types.size() == 0) ? 0 : types.get(0).getSubtypesCount();
+      numColumns = file.getSchema().getChildren().size();
       value = new OrcStruct(numColumns);
       this.reader = OrcInputFormat.createReaderFromFile(file, conf, offset,
           length);

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -477,7 +477,7 @@ public Thread newThread(@NotNull Runnable r) {
     IHMSHandler handler = newRetryingHMSHandler(baseHandler, conf);
     processor = new ThriftHiveMetastore.Processor<>(handler);
     LOG.info("Starting DB backed MetaStore Server with generic processor");
-    TServlet thriftHttpServlet = new HmsThriftHttpServlet(processor, protocolFactory);
+    TServlet thriftHttpServlet = new HmsThriftHttpServlet(processor, protocolFactory, conf);
 
     boolean directSqlEnabled = MetastoreConf.getBoolVar(conf, ConfVars.TRY_DIRECT_SQL);
     HMSHandler.LOG.info("Direct SQL optimization = {}",  directSqlEnabled);

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/GenericUDFIcebergBucket.java
Patch:
@@ -185,7 +185,7 @@ private static int getNumBuckets(ObjectInspector arg) throws UDFArgumentExceptio
   public Object evaluate(DeferredObject[] arguments) throws HiveException {
 
     DeferredObject argument = arguments[0];
-    if (argument == null) {
+    if (argument == null || argument.get() == null) {
       return null;
     } else {
       evaluator.apply(argument);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java
Patch:
@@ -608,12 +608,12 @@ public void addPartitionColsToBatch(ColumnVector col, Object value, int colIndex
     case CHAR:
     case VARCHAR: {
       BytesColumnVector bcv = (BytesColumnVector) col;
-      String sVal = value.toString();
-      if (sVal == null) {
+      if (value == null) {
         bcv.noNulls = false;
         bcv.isNull[0] = true;
         bcv.isRepeating = true;
       } else {
+        String sVal = value.toString();
         bcv.fill(sVal.getBytes());
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveCalciteUtil.java
Patch:
@@ -1253,8 +1253,8 @@ public Void visitCall(RexCall call) {
    * Returns whether the expression has disjunctions (OR) at any level of nesting.
    * <ul>
    * <li> Example 1: OR(=($0, $1), IS NOT NULL($2))):INTEGER (OR in the top-level expression) </li>
-   * <li> Example 2: NOT(AND(=($0, $1), IS NOT NULL($2)) </li>
-   *   this is equivalent to OR((&lt&gt($0, $1), IS NULL($2))
+   * <li> Example 2: NOT(AND(=($0, $1), IS NOT NULL($2))
+   *   this is equivalent to OR((&lt;&gt;($0, $1), IS NULL($2)) </li>
    * <li> Example 3: AND(OR(=($0, $1), IS NOT NULL($2)))) (OR in inner expression) </li>
    * </ul>
    * @param node the expression where to look for disjunctions.

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/dataconnector/jdbc/AbstractJDBCConnectorProvider.java
Patch:
@@ -354,7 +354,7 @@ protected String getDataType(String mySqlType, int size) {
   }
 
   @Override protected String getOutputClass() {
-    return JDBC_INPUTFORMAT_CLASS;
+    return JDBC_OUTPUTFORMAT_CLASS;
   }
   @Override protected String getTableLocation(String tableName) {
     if (warehouse != null) {

File: service/src/java/org/apache/hive/service/cli/operation/HiveCommandOperation.java
Patch:
@@ -107,7 +107,7 @@ public void runInternal() throws HiveSQLException {
     setState(OperationState.RUNNING);
     try {
       String command = getStatement().trim();
-      String[] tokens = statement.split("\\s");
+      String[] tokens = command.split("\\s");
       String commandArgs = command.substring(tokens[0].length()).trim();
 
       CommandProcessorResponse response = commandProcessor.run(commandArgs);

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerWithEngineBase.java
Patch:
@@ -112,8 +112,7 @@ public static Collection<Object[]> parameters() {
         if (javaVersion.equals("1.8")) {
           testParams.add(new Object[] {fileFormat, engine, TestTables.TestTableType.HIVE_CATALOG, false});
           // test for vectorization=ON in case of ORC and PARQUET format with Tez engine
-          if ((fileFormat == FileFormat.ORC || fileFormat == FileFormat.PARQUET) &&
-               "tez".equals(engine) && MetastoreUtil.hive3PresentOnClasspath()) {
+          if (fileFormat != FileFormat.METADATA && "tez".equals(engine) && MetastoreUtil.hive3PresentOnClasspath()) {
             testParams.add(new Object[] {fileFormat, engine, TestTables.TestTableType.HIVE_CATALOG, true});
           }
         }

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergSelects.java
Patch:
@@ -63,7 +63,6 @@ public void testScanTable() throws IOException {
   @Test
   public void testCBOWithSelectedColumnsNonOverlapJoin() throws IOException {
     shell.setHiveSessionValue("hive.cbo.enable", true);
-
     testTables.createTable(shell, "products", PRODUCT_SCHEMA, fileFormat, PRODUCT_RECORDS);
     testTables.createTable(shell, "orders", ORDER_SCHEMA, fileFormat, ORDER_RECORDS);
 
@@ -190,6 +189,7 @@ public void testSpecialCharacters() {
 
   @Test
   public void testScanTableCaseInsensitive() throws IOException {
+    shell.setHiveSessionValue(InputFormatConfig.CASE_SENSITIVE, false);
     testTables.createTable(shell, "customers",
         HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA_WITH_UPPERCASE, fileFormat,
         HiveIcebergStorageHandlerTestUtils.CUSTOMER_RECORDS);

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStore.java
Patch:
@@ -45,7 +45,10 @@ public void setUp() throws Exception {
       Assert.assertNotNull("Unable to connect to the MetaStore server", client);
       return;
     }
+    start();
+  }
 
+  protected void start() throws Exception {
     port = MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(),
         conf);
     System.out.println("Starting MetaStore Server on port " + port);

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java
Patch:
@@ -305,7 +305,7 @@ public RecordUpdater getRecordUpdater(Path path,
       }
     }
     final OrcRecordUpdater.KeyIndexBuilder watcher =
-        new OrcRecordUpdater.KeyIndexBuilder("compactor");
+        new OrcRecordUpdater.KeyIndexBuilder();
     opts.inspector(options.getInspector())
         .callback(watcher);
     final Writer writer = OrcFile.createWriter(filename, opts);

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
Patch:
@@ -4113,7 +4113,7 @@ public void testAcidReadPastLastStripeOffset() throws Exception {
             "currentTransaction:bigint," +
             "row:struct<a:int,b:struct<c:int>,d:string>>");
 
-    OrcRecordUpdater.KeyIndexBuilder indexBuilder = new OrcRecordUpdater.KeyIndexBuilder("test");
+    OrcRecordUpdater.KeyIndexBuilder indexBuilder = new OrcRecordUpdater.KeyIndexBuilder();
     OrcFile.WriterOptions options = OrcFile.writerOptions(conf)
         .fileSystem(fs)
         .setSchema(fileSchema)

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -2252,7 +2252,6 @@ public List<Table> getAllMaterializedViewObjectsForRewriting() throws HiveExcept
    * specified sql query text. It is guaranteed that it will always return an up-to-date version wrt metastore.
    * This method filters out outdated Materialized views. It compares the transaction ids of the passed usedTables and
    * the materialized view using the txnMgr.
-   * @param querySql extended query text (has fully qualified identifiers)
    * @param tablesUsed List of tables to verify whether materialized view is outdated
    * @param txnMgr Transaction manager to get open transactions affects used tables.
    * @return List of materialized views has matching query definition with querySql

File: common/src/java/org/apache/hadoop/hive/common/type/Date.java
Patch:
@@ -39,7 +39,7 @@
 /**
  * This is the internal type for Date. The full qualified input format of Date
  * is "uuuu-MM-dd". For example: "2021-02-11".
- * <table border="2">
+ * <table border="2" summary="">
  * <tr>
  * <th>Field</th>
  * <th>Format</th>
@@ -73,8 +73,8 @@
  * positive.
  * </p>
  *
- * @see {@link ChronoField#YEAR}
- * @see {@link ChronoField#YEAR_OF_ERA}
+ * @see java.time.temporal.ChronoField#YEAR
+ * @see java.time.temporal.ChronoField#YEAR_OF_ERA
  */
 public class Date implements Comparable<Date> {
 

File: common/src/java/org/apache/hadoop/hive/common/type/Timestamp.java
Patch:
@@ -44,7 +44,7 @@
  * Timestamp is "uuuu-MM-dd HH:mm:ss[.SSS...]", where the time part is optional.
  * If time part is absent, a default '00:00:00.0' will be used.
  *
- * <table border="2">
+ * <table border="2" summary="">
  * <tr>
  * <th>Field</th>
  * <th>Format</th>
@@ -78,8 +78,8 @@
  * positive.
  * </p>
  *
- * @see {@link ChronoField#YEAR}
- * @see {@link ChronoField#YEAR_OF_ERA}
+ * @see java.time.temporal.ChronoField#YEAR
+ * @see java.time.temporal.ChronoField#YEAR_OF_ERA
  */
 public class Timestamp implements Comparable<Timestamp> {
   

File: common/src/java/org/apache/hive/common/util/TimestampParser.java
Patch:
@@ -48,7 +48,7 @@
  * In addition to accepting format patterns, this parser provides support for
  * three pre-defined formats:
  *
- * <table border="1">
+ * <table border="1" summary="">
  * <thead>
  * <tr>
  * <th>Formatter</th>

File: itests/hive-jmh/src/main/java/org/apache/hive/benchmark/calcite/FieldTrimmerBench.java
Patch:
@@ -55,13 +55,13 @@
 
 /**
  * This test measures the performance for field trimmer.
- * <p/>
+ * <p>
  * This test uses JMH framework for benchmarking.
  * You may execute this benchmark tool using JMH command line in different ways:
- * <p/>
+ * <p>
  * To use the settings shown in the main() function, use:
  * $ java -cp target/benchmarks.jar org.apache.hive.benchmark.calcite.FieldTrimmerBench
- * <p/>
+ * <p>
  * To use the default settings used by JMH, use:
  * $ java -jar target/benchmarks.jar org.apache.hive.benchmark.calcite.FieldTrimmerBench
  */

File: itests/hive-jmh/src/main/java/org/apache/hive/benchmark/serde/LazySimpleSerDeBench.java
Patch:
@@ -50,13 +50,13 @@
 public class LazySimpleSerDeBench {
   /**
    * This test measures the performance for LazySimpleSerDe.
-   * <p/>
+   * <p>
    * This test uses JMH framework for benchmarking. You may execute this
    * benchmark tool using JMH command line in different ways:
-   * <p/>
+   * <p>
    * To run using default settings, use: 
    * $ java -cp target/benchmarks.jar org.apache.hive.benchmark.serde.LazySimpleSerDeBench
-   * <p/>
+   * <p>
    */
   public static final int DEFAULT_ITER_TIME = 1000000;
   public static final int DEFAULT_DATA_SIZE = 4096;

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/qoption/QTestOptionDispatcher.java
Patch:
@@ -32,7 +32,7 @@
  * Provides facilities to invoke {@link QTestOptionHandler}-s.
  *
  * Enables to dispatch option arguments to a specific option handler.
- * The option invocation format is '--! qt:<optionName>:<optionArgs>
+ * The option invocation format is '--! qt:&lt;optionName&gt;:&lt;optionArgs&gt;
  *
  * Please refer to specific implementations of {@link QTestOptionHandler} for more detailed information about them.
  */

File: jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java
Patch:
@@ -222,7 +222,7 @@ public Blob getBlob(String colName) throws SQLException {
    * @param columnIndex the first column is 1, the second is 2, ...
    * @return the column value; if the value is SQL NULL, the value returned is
    *         false
-   * @throws if the columnIndex is not valid; if a database access error occurs
+   * @throws SQLException if the columnIndex is not valid; if a database access error occurs
    *           or this method is called on a closed result set
    * @see ResultSet#getBoolean(int)
    */
@@ -261,7 +261,7 @@ public boolean getBoolean(int columnIndex) throws SQLException {
    *          the name of the column
    * @return the column value; if the value is SQL NULL, the value returned is
    *         false
-   * @throws if the columnIndex is not valid; if a database access error occurs
+   * @throws SQLException if the columnIndex is not valid; if a database access error occurs
    *           or this method is called on a closed result set
    * @see ResultSet#getBoolean(String)
    */

File: jdbc/src/java/org/apache/hive/jdbc/saml/IJdbcBrowserClient.java
Patch:
@@ -53,7 +53,7 @@ public interface IJdbcBrowserClient extends Closeable {
   /**
    * Initializes the browser client context. The client context contains a client
    * identifier which must be used to set the http header with key
-   * {@link HiveSamlUtils.SSO_CLIENT_IDENTIFIER}.
+   * {@link HiveSamlUtils#SSO_CLIENT_IDENTIFIER}.
    */
   void init(JdbcBrowserClientContext context);
 

File: llap-client/src/java/org/apache/hadoop/hive/llap/io/api/LlapIo.java
Patch:
@@ -56,7 +56,7 @@ InputFormat<NullWritable, T> getInputFormat(
    * @param tag a CacheTag instance must be provided as that's needed for cache insertion
    * @param fileKey fileId of the ORC file (either the Long fileId of HDFS or the SyntheticFileId).
    *                Optional, if it is not provided, it will be generated, see:
-   *                {@link org.apache.hadoop.hive.ql.io.HdfsUtils.getFileId()}
+   *                org.apache.hadoop.hive.ql.io.HdfsUtils#getFileId()
    * @return The tail of the ORC file
    * @throws IOException ex
    */

File: llap-common/src/java/org/apache/hadoop/hive/llap/security/DefaultJwtSharedSecretProvider.java
Patch:
@@ -43,7 +43,7 @@
  *
  * If secret is not found even after 1) and 2), {@link #init(Configuration)} methods throws {@link IllegalStateException}.
  *
- * Length of shared secret provided in 1) or 2) should be > 32 bytes.
+ * Length of shared secret provided in 1) or 2) should be &gt; 32 bytes.
  *
  * It uses the same encryption and decryption secret which can be used to sign and verify JWT.
  */

File: llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/metrics/LlapMetricsListener.java
Patch:
@@ -41,7 +41,7 @@ public interface LlapMetricsListener {
 
   /**
    * Handler will be called when new data is arrived for every active Llap Daemon in the cluster.
-   * @param newMetrics The map of the worker indentity -> metrics
+   * @param newMetrics The map of the worker indentity -&gt; metrics
    */
   void newClusterMetrics(Map<String, LlapMetrics> newMetrics);
 }

File: ql/src/java/org/apache/hadoop/hive/llap/LlapHiveUtils.java
Patch:
@@ -52,7 +52,7 @@ private LlapHiveUtils() {
   }
 
   /**
-   * Takes a Path and looks up the PartitionDesc instance associated with it in a map of Path->PartitionDesc entries.
+   * Takes a Path and looks up the PartitionDesc instance associated with it in a map of Path-&gt;PartitionDesc entries.
    * If it is not found (e.g. Path denotes a partition path, but map contains table level instances only) we will try
    * to do the same with the parent of this path, traversing up until there's a match, if any.
    * @param path the absolute path used for the look up
@@ -104,7 +104,6 @@ public static int getSchemaHash(PartitionDesc part) {
    * Returns MapWork based what is serialized in the JobConf instance provided.
    * @param job
    * @return the MapWork instance. Might be null if missing.
-   * @throws HiveException
    */
   public static MapWork findMapWork(JobConf job) {
     String inputName = job.get(Utilities.INPUT_NAME, null);

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/DescTableAnalyzer.java
Patch:
@@ -46,14 +46,14 @@
  * A query like this will generate a tree as follows
  *   "describe formatted default.maptable partition (b=100) id;"
  * TOK_TABTYPE
- *   TOK_TABNAME --> root for tablename, 2 child nodes mean DB specified
+ *   TOK_TABNAME --&gt; root for tablename, 2 child nodes mean DB specified
  *     default
  *     maptable
- *   TOK_PARTSPEC  --> root node for partition spec. else columnName
+ *   TOK_PARTSPEC  --&gt; root node for partition spec. else columnName
  *     TOK_PARTVAL
  *       b
  *       100
- *   id           --> root node for columnName
+ *   id           --&gt; root node for columnName
  * formatted
  */
 @DDLType(types = HiveParser.TOK_DESCTABLE)

File: ql/src/java/org/apache/hadoop/hive/ql/exec/AddToClassPathAction.java
Patch:
@@ -26,8 +26,8 @@
 
 /**
  * Helper class to create UDFClassLoader when running under a security manager. To create a class loader:
- * > AddToClassPathAction addAction = new AddToClassPathAction(parentLoader, newPaths, true);
- * > UDFClassLoader childClassLoader = AccessController.doPrivileged(addAction);
+ * &gt; AddToClassPathAction addAction = new AddToClassPathAction(parentLoader, newPaths, true);
+ * &gt; UDFClassLoader childClassLoader = AccessController.doPrivileged(addAction);
  * To try to add to the class path of the existing class loader; call the above without forceNewClassLoader=true.
  * Note that a class loader might be still created as fallback method.
  * <p>

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
Patch:
@@ -142,7 +142,7 @@ public enum TaskState {
   protected List<Task<?>> parentTasks;
   /**
    * this can be set by the Task, to provide more info about the failure in TaskResult
-   * where the Driver can find it.  This is checked if {@link Task#execute(org.apache.hadoop.hive.ql.TaskQueue)}
+   * where the Driver can find it.  This is checked if {@link Task#execute()}
    * returns non-0 code.
    */
   private Throwable exception;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -1127,7 +1127,8 @@ private static void moveFileOrDir(FileSystem fs, FileStatus file, Path dst) thro
    * @param destFileName
    *          the target filename
    * @return The final path the file was moved to.
-   * @throws IOException, HiveException
+   * @throws IOException
+   * @throws HiveException
    */
   public static Path moveFile(FileSystem fs, Path srcFile, Path destDir, String destFileName)
       throws IOException, HiveException {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/OptimisedBootstrapUtils.java
Patch:
@@ -74,7 +74,7 @@ public class OptimisedBootstrapUtils {
   public static final String BOOTSTRAP_TABLES_LIST = "_failover_bootstrap_table_list";
 
   /**
-   * Gets & checks whether the database is target of replication.
+   * Gets &amp; checks whether the database is target of replication.
    * @param dbName name of database
    * @param hive hive object
    * @return true, if the database has repl.target.for property set.
@@ -91,7 +91,7 @@ public static boolean checkFileExists(Path dumpPath, HiveConf conf, String fileN
   }
 
   /**
-   * Gets the source & target event id  from the event ack file
+   * Gets the source &amp; target event id  from the event ack file
    * @param dumpPath the dump path
    * @param conf the hive configuration
    * @return the event id from file.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplStatsTracker.java
Patch:
@@ -107,7 +107,7 @@ public synchronized void addEntry(String eventType, String eventId, long timeTak
 
   /**
    * Get the DescriptiveStatistics for each event type.
-   * @return A HashMap, with key as event type & value as the DescriptiveAnalytics of the entire run.
+   * @return A HashMap, with key as event type &amp; value as the DescriptiveAnalytics of the entire run.
    */
   public ConcurrentHashMap<String, DescriptiveStatistics> getDescMap() {
     return descMap;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
Patch:
@@ -1260,7 +1260,6 @@ private static String[] getTempArchivesFromConf(Configuration conf) {
    * @param inputOutputJars The file names to localize.
    * @return Map&lt;String, LocalResource&gt; (srcPath, local resources) to add to execution
    * @throws IOException when hdfs operation fails.
-   * @throws LoginException when getDefaultDestDir fails with the same exception
    */
   public Map<String, LocalResource> localizeTempFiles(String hdfsDirPathStr, Configuration conf,
       String[] inputOutputJars, String[] skipJars) throws IOException {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDateToCharWithFormat.java
Patch:
@@ -24,7 +24,7 @@
 import java.nio.charset.StandardCharsets;
 
 /**
- * Vectorized UDF for CAST (<DATE> TO CHAR(<LENGTH>) WITH FORMAT <STRING>).
+ * Vectorized UDF for CAST (&lt;DATE&gt; TO CHAR(&lt;LENGTH&gt;) WITH FORMAT &lt;STRING&gt;).
  */
 public class CastDateToCharWithFormat extends CastDateToChar {
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDateToStringWithFormat.java
Patch:
@@ -24,7 +24,7 @@
 import java.nio.charset.StandardCharsets;
 
 /**
- * Vectorized UDF for CAST (<DATE> TO STRING WITH FORMAT <STRING>).
+ * Vectorized UDF for CAST (&lt;DATE&gt; TO STRING WITH FORMAT &lt;STRING&gt;).
  */
 public class CastDateToStringWithFormat extends CastDateToString {
   private static final long serialVersionUID = 1L;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDateToVarCharWithFormat.java
Patch:
@@ -24,7 +24,7 @@
 import java.nio.charset.StandardCharsets;
 
 /**
- * Vectorized UDF for CAST (<DATE> TO VARCHAR(<LENGTH>) WITH FORMAT <STRING>).
+ * Vectorized UDF for CAST (&lt;DATE&gt; TO VARCHAR(&lt;LENGTH&gt;) WITH FORMAT &lt;STRING&gt;).
  */
 public class CastDateToVarCharWithFormat extends CastDateToVarChar {
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastStringToDateWithFormat.java
Patch:
@@ -27,7 +27,7 @@
 import java.nio.charset.StandardCharsets;
 
 /**
- * Vectorized UDF for CAST (<STRING> TO DATE WITH FORMAT <STRING>).
+ * Vectorized UDF for CAST (&lt;STRING&gt; TO DATE WITH FORMAT &lt;STRING&gt;).
  */
 public class CastStringToDateWithFormat extends CastStringToDate {
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastStringToTimestampWithFormat.java
Patch:
@@ -26,7 +26,7 @@
 import java.nio.charset.StandardCharsets;
 
 /**
- * Vectorized UDF for CAST (<STRING> TO TIMESTAMP WITH FORMAT <STRING>).
+ * Vectorized UDF for CAST (&lt;STRING&gt; TO TIMESTAMP WITH FORMAT &lt;STRING&gt;).
  */
 public class CastStringToTimestampWithFormat extends CastStringToTimestamp {
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToCharWithFormat.java
Patch:
@@ -25,7 +25,7 @@
 import java.nio.charset.StandardCharsets;
 
 /**
- * Vectorized UDF for CAST (<TIMESTAMP> TO CHAR(<LENGTH>) WITH FORMAT <STRING>).
+ * Vectorized UDF for CAST (&lt;TIMESTAMP&gt; TO CHAR(&lt;LENGTH&gt;) WITH FORMAT &lt;STRING&gt;).
  */
 public class CastTimestampToCharWithFormat extends CastTimestampToChar {
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToStringWithFormat.java
Patch:
@@ -25,7 +25,7 @@
 import java.nio.charset.StandardCharsets;
 
 /**
- * Vectorized UDF for CAST (<TIMESTAMP> TO STRING WITH FORMAT <STRING>).
+ * Vectorized UDF for CAST (&lt;TIMESTAMP&gt; TO STRING WITH FORMAT &lt;STRING&gt;).
  */
 public class CastTimestampToStringWithFormat extends CastTimestampToString {
   private static final long serialVersionUID = 1L;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToVarCharWithFormat.java
Patch:
@@ -25,7 +25,7 @@
 import java.nio.charset.StandardCharsets;
 
 /**
- * Vectorized UDF for CAST (<TIMESTAMP> TO VARCHAR(<LENGTH>) WITH FORMAT <STRING>).
+ * Vectorized UDF for CAST (&lt;TIMESTAMP&gt; TO VARCHAR(&lt;LENGTH&gt;) WITH FORMAT &lt;STRING&gt;).
  */
 public class CastTimestampToVarCharWithFormat extends CastTimestampToVarChar {
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java
Patch:
@@ -137,7 +137,7 @@ public DeltaMetaData() {
     /**
      * @param minWriteId min writeId of the delta directory
      * @param maxWriteId max writeId of the delta directory
-     * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition
+     * @param stmtIds delta dir suffixes when a single txn writes &gt; 1 delta in the same partition
      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible
      * @param deltaFiles bucketFiles in the directory
      */

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/StreamUtils.java
Patch:
@@ -37,7 +37,6 @@ public class StreamUtils {
    * @param streamName - stream name
    * @param streamBuffer - stream buffer
    * @return - SettableUncompressedStream
-   * @throws IOException
    */
   public static SettableUncompressedStream createSettableUncompressedStream(String streamName,
       ColumnStreamData streamBuffer) {

File: ql/src/java/org/apache/hadoop/hive/ql/log/syslog/SyslogInputFormat.java
Patch:
@@ -83,7 +83,7 @@
  *   If a filename is 2019-04-02-21-00_0.log.gz and timeslice is 300s then the file 2019-04-02-21-00_0.log.gz is
  *   expected to have log lines from timestamp 2019:04:02 21:00:00 to 2019:04:02 21:05:00 timestamp.
  * - Logs table should have 'ts' as timestamp column.
- * - Only simple BETWEEN filter predicate is supported for 'ts' column. There cannot be >1 predicates on 'ts' column.
+ * - Only simple BETWEEN filter predicate is supported for 'ts' column. There cannot be &gt;1 predicates on 'ts' column.
  */
 public class SyslogInputFormat extends TextInputFormat {
   private static final Logger LOG = LoggerFactory.getLogger(SyslogInputFormat.class);

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -5038,7 +5038,7 @@ static private boolean needToCopy(final HiveConf conf, Path srcf, Path destf, Fi
    * @param isSrcLocal true if source is on local file system
    * @param isAcidIUD true if this is an ACID based Insert/Update/Delete
    * @param isOverwrite if true, then overwrite if destination file exist, else add a duplicate copy
-   * @param newFiles if this is non-null, a list of files that were created as a result of this
+   * @param newFilesStatus if this is non-null, a list of files that were created as a result of this
    *                 move will be returned.
    * @param isManaged if table is managed.
    * @param isCompactionTable is table used in query-based compaction

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.java
Patch:
@@ -213,7 +213,7 @@ default boolean addDynamicSplitPruningEdge(org.apache.hadoop.hive.ql.metadata.Ta
    *
    * @param operatorDesc operatorDesc
    * @param initialProps Map containing initial operator properties
-   * @return Map<String, String> containing additional operator specific information from storage handler
+   * @return Map&lt;String, String&gt; containing additional operator specific information from storage handler
    * OR `initialProps` if the storage handler choose to not provide any such information.
    */
   default Map<String, String> getOperatorDescProperties(OperatorDesc operatorDesc, Map<String, String> initialProps) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ParallelEdgeFixer.java
Patch:
@@ -75,7 +75,7 @@
  *       |        |                       |        |
  *       |        |                       |   +--- | ---+
  *       |        |                       |   | +-----+ |
- *       |        |         >>>>          |   | |RS_T | |
+ *       |        |         $gt;          |   | |RS_T | |
  *       |        |                       |   | +-----+ |
  *       |        |                       |   +--- | ---+
  *       |        |                       |        |

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/functions/HiveMergeableAggregate.java
Patch:
@@ -28,7 +28,7 @@
  * Mergeable aggregate.
  *
  * A mergeable aggregate is:
- * - accepts the same kind as inputs as the output (an X^n -> X function)
+ * - accepts the same kind as inputs as the output (an X^n -&gt; X function)
  *
  * Example: the SUM function is a great example; since SUM of SUM -s is the overall sum.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateSortLimitRule.java
Patch:
@@ -30,7 +30,7 @@
  * Rule that adds sorting to GROUP BY col0 LIMIT n in presence of aggregate functions.
  * Ex.: SELECT id, count(1) FROM t_table GROUP BY id LIMIT 2
  *
- * Above query has a physical plan like Reducer 2 <- Map 1 (SIMPLE_EDGE)
+ * Above query has a physical plan like Reducer 2 &lt;- Map 1 (SIMPLE_EDGE)
  * Both mapper and reducer edges may have multiple Mapper and Reducer instances to enable parallel process of data.
  * Aggregate function results are calculated in two steps:
  * 1) first mappers calculate a partial result from the rows processed by each instance.

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveDruidRules.java
Patch:
@@ -62,7 +62,7 @@
  * Druid rules with Hive builder factory.
  *
  * Simplify this class when upgrading to Calcite 1.26 using
- * <a href="https://issues.apache.org/jira/browse/CALCITE-4200">
+ * <a href="https://issues.apache.org/jira/browse/CALCITE-4200">CALCITE-4200</a>
  */
 public class HiveDruidRules {
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveHepExtractRelNodeRule.java
Patch:
@@ -29,7 +29,8 @@
 /**
  * The goal of this rule is to extract the RelNode from the
  * HepRelVertex node so rules do tree traversal can be applied correctly.
- * {@see HiveFieldTrimmerRule, HiveAggregateInsertDeleteIncrementalRewritingRule}
+ * @see HiveFieldTrimmerRule
+ * @see org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveAggregateInsertDeleteIncrementalRewritingRule
  */
 public class HiveHepExtractRelNodeRule extends RelOptRule {
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveProjectSortExchangeTransposeRule.java
Patch:
@@ -48,7 +48,7 @@
  *   HiveSortExchange
  *     ...
  *
- * =>
+ * =&gt;
  *
  * HiveSortExchange
  *   HiveProject

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveRewriteToDataSketchesRules.java
Patch:
@@ -67,7 +67,7 @@
 /**
  * This rule could rewrite aggregate calls to be calculated using sketch based functions.
  *
- * <br/>
+ * <br>
  * Currently it can rewrite:
  * <ul>
  *  <li>{@code count(distinct(x))} using {@code CountDistinctRewrite}
@@ -652,7 +652,7 @@ protected RexNode evaluateRankValue(RexNode projRex, RexOver over, RexInputRef s
    *  <pre>
    *   SELECT id, NTILE(4) OVER (ORDER BY id) FROM sketch_input;
    *     ⇒ SELECT id, CASE
-   *                    WHEN CEIL(ds_kll_cdf(ds, CAST(id AS FLOAT) )[0]) < 1
+   *                    WHEN CEIL(ds_kll_cdf(ds, CAST(id AS FLOAT) )[0]) &lt; 1
    *                      THEN 1
    *                    ELSE CEIL(ds_kll_cdf(ds, CAST(id AS FLOAT) )[0])
    *                  END
@@ -708,7 +708,7 @@ private RexNode lt(RexNode op1, RexNode op2) {
    *  <pre>
    *   SELECT id, RANK() OVER (ORDER BY id) FROM sketch_input;
    *     ⇒ SELECT id, CASE
-   *                    WHEN ds_kll_n(ds) < (ceil(ds_kll_rank(ds, CAST(id AS FLOAT) )*ds_kll_n(ds))+1)
+   *                    WHEN ds_kll_n(ds) &lt; (ceil(ds_kll_rank(ds, CAST(id AS FLOAT) )*ds_kll_n(ds))+1)
    *                    THEN ds_kll_n(ds)
    *                    ELSE (ceil(ds_kll_rank(ds, CAST(id AS FLOAT) )*ds_kll_n(ds))+1)
    *                  END

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveAggregateInsertIncrementalRewritingRule.java
Patch:
@@ -57,15 +57,15 @@
  *   JOIN TAB_B ON (TAB_A.a = TAB_B.z)
  *   WHERE TAB_A.ROW_ID &gt; 5
  *   GROUP BY a, b) source
- * ON (mv.a <=> source.a AND mv.b <=> source.b)
+ * ON (mv.a &lt;=&gt; source.a AND mv.b &lt;=&gt; source.b)
  * WHEN MATCHED AND mv.c + source.c &lt;&gt; 0
  *   THEN UPDATE SET mv.s = mv.s + source.s, mv.c = mv.c + source.c
  * WHEN NOT MATCHED
  *   THEN INSERT VALUES (source.a, source.b, s, c);
  *
  * To be precise, we need to convert it into a MERGE rewritten as:
  * FROM (select *, true flag from mv) mv right outer join _source_ source
- * ON (mv.a <=> source.a AND mv.b <=> source.b)
+ * ON (mv.a &lt;=&gt; source.a AND mv.b &lt;=&gt; source.b)
  * INSERT INTO TABLE mv
  *   SELECT source.a, source.b, s, c
  *   WHERE mv.flag IS NULL
@@ -79,7 +79,7 @@
  *   WHERE mv.flag
  *   SORT BY mv.ROW__ID;
  *
- * {@see CalcitePlanner#fixUpASTAggregateInsertIncrementalRebuild}
+ * @see org.apache.hadoop.hive.ql.parse.CalcitePlanner
  */
 public class HiveAggregateInsertIncrementalRewritingRule extends HiveAggregateIncrementalRewritingRuleBase<
         HiveAggregateIncrementalRewritingRuleBase.IncrementalComputePlan> {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveAggregatePartitionIncrementalRewritingRule.java
Patch:
@@ -59,9 +59,9 @@
  *
  * SELECT a, b, sum(sumc) FROM (
  *     SELECT a, b, sumc FROM mat1
- *     LEFT SEMI JOIN (SELECT a, b, sum(c) FROM t1 WHERE ROW__ID.writeId > 1 GROUP BY b, a) q ON (mat1.a <=> q.a)
+ *     LEFT SEMI JOIN (SELECT a, b, sum(c) FROM t1 WHERE ROW__ID.writeId &gt; 1 GROUP BY b, a) q ON (mat1.a &lt;=&gt; q.a)
  *     UNION ALL
- *     SELECT a, b, sum(c) sumc FROM t1 WHERE ROW__ID.writeId > 1 GROUP BY b, a
+ *     SELECT a, b, sum(c) sumc FROM t1 WHERE ROW__ID.writeId &gt; 1 GROUP BY b, a
  * ) sub
  * GROUP BY b, a
  */

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveJoinInsertDeleteIncrementalRewritingRule.java
Patch:
@@ -44,11 +44,11 @@
  * MULTI INSERT statement instead: one insert branch for inserted rows
  * and another for inserting deleted rows to delete delta.
  * Since CBO plan does not contain the INSERT branches we focus on the SELECT part of the plan in this rule.
- * See also {@link CalcitePlanner#fixUpASTJoinInsertDeleteIncrementalRebuild(ASTNode)}
+ * See also {@link CalcitePlanner}
  *
  * FROM (select mv.ROW__ID, mv.a, mv.b from mv) mv
  * RIGHT OUTER JOIN (SELECT _source_.ROW__IS_DELETED,_source_.a, _source_.b FROM _source_) source
- * ON (mv.a <=> source.a AND mv.b <=> source.b)
+ * ON (mv.a &lt;=&gt; source.a AND mv.b &lt;=&gt; source.b)
  * INSERT INTO TABLE mv_delete_delta
  *   SELECT mv.ROW__ID
  *   WHERE source.ROW__IS__DELETED

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
Patch:
@@ -532,9 +532,9 @@ public static List<RexNode> rewriteToDateChildren(List<RexNode> childRexNodeLst,
    * </pre>
    * Or:
    * <pre>
-   * (c,d) IN ( (v1,v2), (v3,v4), ...) =&gt; (c=v1 && d=v2) || (c=v3 && d=v4) || ...
+   * (c,d) IN ( (v1,v2), (v3,v4), ...) =&gt; (c=v1 &amp;&amp; d=v2) || (c=v3 &amp;&amp; d=v4) || ...
    * Input: ((c,d), (v1,v2), (v3,v4), ...)
-   * Output: (c=v1 && d=v2, c=v3 && d=v4, ...)
+   * Output: (c=v1 &amp;&amp; d=v2, c=v3 &amp;&amp; d=v4, ...)
    * </pre>
    *
    * Returns null if the transformation fails, e.g., when non-deterministic

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/CommonKeyPrefix.java
Patch:
@@ -78,10 +78,10 @@ public static CommonKeyPrefix map(TopNKeyDesc topNKeyDesc, ReduceSinkDesc reduce
    *
    *      opKeys: Column[_col0], Column[_col1], Column[_col2], Column[_col3]
    *      parentKeys: Column[KEY._col0], Column[KEY._col1], Column[KEY._col4]
-   *      parentColExprMap: {_col0 -> Column[KEY._col0]}, {_col1 -> Column[KEY._col1]}, {_col4 -> Column[KEY._col4]}
+   *      parentColExprMap: {_col0 -&gt; Column[KEY._col0]}, {_col1 -&gt; Column[KEY._col1]}, {_col4 -&gt; Column[KEY._col4]}
    *
    * Column ordering and null ordering is given by a string where each character represents a column order/null order.
-   * Ex.: a ASC NULLS FIRST, b DESC NULLS LAST, c ASC NULLS LAST -> order="+-+", null order="azz"
+   * Ex.: a ASC NULLS FIRST, b DESC NULLS LAST, c ASC NULLS LAST -&gt; order="+-+", null order="azz"
    *
    * When <code>parentColExprMap</code> is null this method falls back to
    * {@link #map(List, String, String, List, String, String)}.
@@ -90,7 +90,7 @@ public static CommonKeyPrefix map(TopNKeyDesc topNKeyDesc, ReduceSinkDesc reduce
    * @param opOrder operator's key column ordering in {@link String} format
    * @param opNullOrder operator's key column null ordering in {@link String} format
    * @param parentKeys {@link List} of {@link ExprNodeDesc}. contains the parent operator's key columns
-   * @param parentColExprMap {@link Map} of {@link String} -> {@link ExprNodeDesc}.
+   * @param parentColExprMap {@link Map} of {@link String} -&gt; {@link ExprNodeDesc}.
    *                                    contains parent operator's key column name {@link ExprNodeDesc} mapping
    * @param parentOrder parent operator's key column ordering in {@link String} format
    * @param parentNullOrder parent operator's key column null ordering in {@link String} format

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/topnkey/TopNKeyProcessor.java
Patch:
@@ -39,7 +39,7 @@
 
 /**
  * TopNKeyProcessor is a processor for TopNKeyOperator.
- * A TopNKeyOperator will be placed before any ReduceSinkOperator which has a topN property >= 0.
+ * A TopNKeyOperator will be placed before any ReduceSinkOperator which has a topN property &gt;= 0.
  */
 public class TopNKeyProcessor implements SemanticNodeProcessor {
   private static final Logger LOG = LoggerFactory.getLogger(TopNKeyProcessor.class);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -2405,7 +2405,6 @@ private RelNode removeSubqueries(RelNode basePlan, RelMetadataProvider mdProvide
      * @param isCollection
      * @param order
      * @param rules
-     * @return HEP program
      */
     protected void generatePartialProgram(HepProgramBuilder programBuilder, boolean isCollection, HepMatchOrder order,
         RelOptRule... rules) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java
Patch:
@@ -137,7 +137,7 @@ void addTranslation(ASTNode node, String replacementText) {
   /**
    * Register a translation for an tabName.
    *
-   * @param node
+   * @param tableName
    *          source node (which must be an tabName) to be replaced
    */
   public void addTableNameTranslation(ASTNode tableName, String currentDatabaseName) {
@@ -175,7 +175,7 @@ public void addTableNameTranslation(ASTNode tableName, String currentDatabaseNam
   /**
    * Register a translation for an identifier.
    *
-   * @param node
+   * @param identifier
    *          source node (which must be an identifier) to be replaced
    */
   public void addIdentifierTranslation(ASTNode identifier) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/type/FunctionHelper.java
Patch:
@@ -98,7 +98,7 @@ default RexNode foldExpression(RexNode expr) {
   boolean isInFunction(FunctionInfo fi);
 
   /**
-   * returns true if FunctionInfo is a compare function (e.g. '<=')
+   * returns true if FunctionInfo is a compare function (e.g. '&lt;=')
    */
   boolean isCompareFunction(FunctionInfo fi);
 

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java
Patch:
@@ -106,7 +106,7 @@ public void init(AtomicBoolean stop) throws Exception {
    * Get the partition being compacted.
    * @param ci compaction info returned from the compaction queue
    * @return metastore partition, or null if there is not partition in this compaction info
-   * @throws Exception if underlying calls throw, or if the partition name resolves to more than
+   * @throws MetaException if underlying calls throw, or if the partition name resolves to more than
    * one partition.
    */
   protected Partition resolvePartition(CompactionInfo ci) throws MetaException {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFRank.java
Patch:
@@ -274,7 +274,7 @@ public static Object[] copyToStandardObject(Object[] o, ObjectInspector[] oi,
    *  Calculates the rank of a hypothetical row specified by the arguments of the
    *  function in a group of values specified by the order by clause.
    *  SELECT rank(expression1[, expressionn]*) WITHIN GROUP (ORDER BY col1[, coln]*)
-   *  (the number of rows where col1 < expression1 [and coln < expressionn]*) + 1
+   *  (the number of rows where col1 &lt; expression1 [and coln &lt; expressionn]*) + 1
    */
   public static class GenericUDAFHypotheticalSetRankEvaluator extends GenericUDAFEvaluator {
     public static final String RANK_FIELD = "rank";

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCastFormat.java
Patch:
@@ -53,7 +53,7 @@
 import java.util.Map;
 
 /**
- * CAST(<value> AS <type> FORMAT <STRING>).
+ * CAST(&lt;value&gt; AS &lt;type&gt; FORMAT &lt;STRING&gt;).
  *
  * Vector expressions: CastDateToCharWithFormat, CastDateToStringWithFormat,
  *     CastDateToVarCharWithFormat, CastTimestampToCharWithFormat,

File: serde/src/java/org/apache/hadoop/hive/serde2/JsonSerDe.java
Patch:
@@ -41,10 +41,10 @@
 /**
  * Hive SerDe for processing JSON formatted data. This is typically paired with
  * the TextInputFormat and therefore each line provided to this SerDe must be a
- * single, and complete JSON object.<br/>
+ * single, and complete JSON object.<br>
  * <h2>Example</h2>
  * <p>
- * {"name="john","age"=30}<br/>
+ * {"name="john","age"=30}<br>
  * {"name="sue","age"=32}
  * </p>
  */

File: serde/src/java/org/apache/hadoop/hive/serde2/json/BinaryEncoding.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.serde2.json;
 
 /**
- * Enums describing the available String->Bytes encoding available for JSON
+ * Enums describing the available String-&gt;Bytes encoding available for JSON
  * parsing. This base-64 variant is what most people would think of "the
  * standard" Base64 encoding for JSON: the specific MIME content transfer
  * encoding. The Raw String encoding produces an array of bytes by reading the

File: serde/src/java/org/apache/hadoop/hive/serde2/json/HiveJsonReader.java
Patch:
@@ -71,9 +71,9 @@
 /**
  * This class converts JSON strings into Java or Hive Primitive objects.
  *
- * Support types are:<br/>
- * <br/>
- * <table border="1">
+ * Support types are:<br>
+ * <br>
+ * <table border="1" summary="">
  * <tr>
  * <th>JSON Type</th>
  * <th>Java Type</th>

File: service/src/java/org/apache/hive/service/cli/operation/QueryInfoCache.java
Patch:
@@ -78,7 +78,7 @@ public List<QueryInfo> getLiveQueryInfos() {
   }
 
   /**
-   * Remove the live operation's query info from the {@liveQueryInfos},
+   * Remove the live operation's query info from the {@link #liveQueryInfos},
    * and push the query info to the historic query cache if enabled.
    * @param operation the to remove operation
    */

File: shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
Patch:
@@ -259,8 +259,6 @@ RecordReader getRecordReader(JobConf job, CombineFileSplit split, Reporter repor
    * @param filter A filter to use on the files in the directory
    * @return A list of file status with IDs
    * @throws IOException An I/O exception of some sort has occurred
-   * @throws FileNotFoundException If the path is not found in the
-   *           {@code FileSystem}
    * @throws UnsupportedOperationException the {@code FileSystem} is not a
    *           {@code DistributedFileSystem}
    */

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -4748,7 +4748,7 @@ public SerDeInfo getSerDe(String serDeName) throws TException {
    * if the validWriteIdList is not explicitly passed (as a method argument) to the HMS APIs.
    * This method returns the ValidWriteIdList based on the VALID_TABLES_WRITEIDS_KEY key.
    * Since, VALID_TABLES_WRITEIDS_KEY is set during the lock acquisition phase after query compilation
-   * ( DriverTxnHandler.acquireLocks -> recordValidWriteIds -> setValidWriteIds ),
+   * ( DriverTxnHandler.acquireLocks -&gt; recordValidWriteIds -&gt; setValidWriteIds ),
    * this only covers a subset of cases, where we invoke get_* APIs after query compilation,
    * if the validWriteIdList is not explicitly passed (as a method argument) to the HMS APIs.
    */

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/FileUtils.java
Patch:
@@ -353,7 +353,6 @@ public static String unescapePathName(String path) {
    * @param fs
    *          the file system
    * @return array of FileStatus
-   * @throws IOException
    */
   public static List<FileStatus> getFileStatusRecurse(Path base, FileSystem fs) {
     try {

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ExceptionHandler.java
Patch:
@@ -81,8 +81,8 @@ ExceptionHandler throwIfInstance(
   }
 
   /**
-   * Converts the input exception to the target instance of class {@param target},
-   * if the input exception is the instance of class {@param source}, throws the
+   * Converts the input exception to the target instance of class {@code target},
+   * if the input exception is the instance of class {@code source}, throws the
    * converted target exception.
    */
   public <S extends Exception, T extends TException> ExceptionHandler

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreEventListener.java
Patch:
@@ -343,7 +343,7 @@ public void onAcidWrite(AcidWriteEvent acidWriteEvent, Connection dbConn, SQLGen
 
   /**
    * This will be called to perform acid write operation in a batch.
-   * @param acidWriteEvent event to be processed
+   * @param batchAcidWriteEvent event to be processed
    * @param dbConn jdbc connection to remote meta store db.
    * @param sqlGenerator helper class to generate db specific sql string.
    * @throws MetaException

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/dataconnector/jdbc/DerbySQLConnectorProvider.java
Patch:
@@ -43,7 +43,7 @@ public DerbySQLConnectorProvider(String dbName, DataConnector connector) {
   /**
    * Returns a list of all table names from the remote database.
    * @return List A collection of all the table names, null if there are no tables.
-   * @throws IOException To indicate any failures with executing this API
+   * @throws MetaException To indicate any failures with executing this API
    */
   @Override
   protected ResultSet fetchTableNames() throws MetaException {

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnStore.java
Patch:
@@ -746,7 +746,7 @@ void removeCompactionMetricsData(String dbName, String tblName, String partition
 
   /**
    * Returns the top ACID metrics from each type {@link CompactionMetricsData.MetricType}
-   * @oaram limit number of returned records for each type
+   * @param limit number of returned records for each type
    * @return list of metrics, always non-null
    * @throws MetaException
    */

File: iceberg/iceberg-catalog/src/main/java/org/apache/iceberg/hive/HiveSchemaConverter.java
Patch:
@@ -19,7 +19,6 @@
 
 package org.apache.iceberg.hive;
 
-import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
 import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;
@@ -30,6 +29,7 @@
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
+import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.types.Type;
 import org.apache.iceberg.types.Types;
 import org.slf4j.Logger;
@@ -62,7 +62,7 @@ static Type convert(TypeInfo typeInfo, boolean autoConvert) {
   }
 
   List<Types.NestedField> convertInternal(List<String> names, List<TypeInfo> typeInfos, List<String> comments) {
-    List<Types.NestedField> result = new ArrayList<>(names.size());
+    List<Types.NestedField> result = Lists.newArrayListWithExpectedSize(names.size());
     for (int i = 0; i < names.size(); ++i) {
       result.add(Types.NestedField.optional(id++, names.get(i), convertType(typeInfos.get(i)),
           comments.isEmpty() || i >= comments.size() ? null : comments.get(i)));

File: iceberg/iceberg-catalog/src/test/java/org/apache/iceberg/hive/HiveMetastoreTest.java
Patch:
@@ -19,14 +19,14 @@
 
 package org.apache.iceberg.hive;
 
-import java.util.HashMap;
 import java.util.concurrent.TimeUnit;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.iceberg.CatalogProperties;
 import org.apache.iceberg.CatalogUtil;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
@@ -47,15 +47,15 @@ public static void startMetastore() throws Exception {
     HiveMetastoreTest.hiveConf = metastore.hiveConf();
     HiveMetastoreTest.metastoreClient = new HiveMetaStoreClient(hiveConf);
     String dbPath = metastore.getDatabasePath(DB_NAME);
-    Database db = new Database(DB_NAME, "description", dbPath, new HashMap<>());
+    Database db = new Database(DB_NAME, "description", dbPath, Maps.newHashMap());
     metastoreClient.createDatabase(db);
     HiveMetastoreTest.catalog = (HiveCatalog)
         CatalogUtil.loadCatalog(HiveCatalog.class.getName(), CatalogUtil.ICEBERG_CATALOG_TYPE_HIVE, ImmutableMap.of(
                 CatalogProperties.CLIENT_POOL_CACHE_EVICTION_INTERVAL_MS, String.valueOf(EVICTION_INTERVAL)), hiveConf);
   }
 
   @AfterClass
-  public static void stopMetastore() {
+  public static void stopMetastore() throws Exception {
     HiveMetastoreTest.catalog = null;
 
     metastoreClient.close();

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java
Patch:
@@ -24,7 +24,6 @@
 import java.net.URI;
 import java.net.URISyntaxException;
 import java.util.Collection;
-import java.util.HashMap;
 import java.util.List;
 import java.util.ListIterator;
 import java.util.Map;
@@ -271,7 +270,7 @@ public Map<String, String> getBasicStatistics(Partish partish) {
     org.apache.hadoop.hive.ql.metadata.Table hmsTable = partish.getTable();
     TableDesc tableDesc = Utilities.getTableDesc(hmsTable);
     Table table = Catalogs.loadTable(conf, tableDesc.getProperties());
-    Map<String, String> stats = new HashMap<>();
+    Map<String, String> stats = Maps.newHashMap();
     if (table.currentSnapshot() != null) {
       Map<String, String> summary = table.currentSnapshot().summary();
       if (summary != null) {

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveTableUtil.java
Patch:
@@ -20,7 +20,6 @@
 package org.apache.iceberg.mr.hive;
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
@@ -49,6 +48,7 @@
 import org.apache.iceberg.mapping.NameMapping;
 import org.apache.iceberg.mapping.NameMappingParser;
 import org.apache.iceberg.mr.Catalogs;
+import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;
 
 public class HiveTableUtil {
@@ -89,7 +89,7 @@ public static void importFiles(String sourceLocation,
         dataFiles.forEach(append::appendFile);
       } else {
         PartitionSpecProxy.PartitionIterator partitionIterator = partitionSpecProxy.getPartitionIterator();
-        List<Callable<Void>> tasks = new ArrayList<>();
+        List<Callable<Void>> tasks = Lists.newArrayList();
         while (partitionIterator.hasNext()) {
           Partition partition = partitionIterator.next();
           Callable<Void> task = () -> {
@@ -121,7 +121,7 @@ public static void importFiles(String sourceLocation,
   private static List<DataFile> getDataFiles(RemoteIterator<LocatedFileStatus> fileStatusIterator,
       Map<String, String> partitionKeys, String format, PartitionSpec spec, MetricsConfig metricsConfig,
       NameMapping nameMapping, Configuration conf) throws IOException {
-    List<DataFile> dataFiles = new ArrayList<>();
+    List<DataFile> dataFiles = Lists.newArrayList();
     while (fileStatusIterator.hasNext()) {
       LocatedFileStatus fileStatus = fileStatusIterator.next();
       String fileName = fileStatus.getPath().getName();

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInternalRecordWrapper.java
Patch:
@@ -21,13 +21,13 @@
 
 import java.lang.reflect.Array;
 import java.util.Arrays;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.function.Function;
 import java.util.stream.Collectors;
 import org.apache.iceberg.StructLike;
 import org.apache.iceberg.data.Record;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 import org.apache.iceberg.types.Type;
 import org.apache.iceberg.types.Types;
 import org.apache.iceberg.types.Types.StructType;
@@ -131,7 +131,7 @@ private IcebergInternalRecordWrapper(IcebergInternalRecordWrapper toCopy) {
   }
 
   private Map<String, Integer> buildFieldPositionMap(StructType schema) {
-    Map<String, Integer> nameToPosition = new HashMap<>();
+    Map<String, Integer> nameToPosition = Maps.newHashMap();
     List<Types.NestedField> fields = schema.fields();
     for (int i = 0; i < fields.size(); i += 1) {
       nameToPosition.put(fields.get(i).name(), i);

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergOutputCommitter.java
Patch:
@@ -20,7 +20,6 @@
 package org.apache.iceberg.mr.hive;
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
@@ -48,6 +47,7 @@
 import org.apache.iceberg.mr.TestHelper;
 import org.apache.iceberg.mr.mapred.Container;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
+import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 import org.apache.iceberg.types.Types;
 import org.apache.iceberg.util.SerializationUtil;
@@ -259,7 +259,7 @@ private JobConf jobConf(Table table, int taskNum) {
    */
   private List<Record> writeRecords(String name, int taskNum, int attemptNum, boolean commitTasks, boolean abortTasks,
                                     JobConf conf, OutputCommitter committer) throws IOException {
-    List<Record> expected = new ArrayList<>(RECORD_NUM * taskNum);
+    List<Record> expected = Lists.newArrayListWithExpectedSize(RECORD_NUM * taskNum);
 
     Table table = HiveIcebergStorageHandler.table(conf, name);
     FileIO io = table.io();

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergSchemaEvolution.java
Patch:
@@ -21,7 +21,6 @@
 
 import java.io.IOException;
 import java.math.BigDecimal;
-import java.util.ArrayList;
 import java.util.Comparator;
 import java.util.List;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
@@ -33,6 +32,7 @@
 import org.apache.iceberg.hive.HiveSchemaUtil;
 import org.apache.iceberg.mr.TestHelper;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
+import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.types.Types;
 import org.apache.thrift.TException;
 import org.junit.Assert;
@@ -376,7 +376,7 @@ public void testAddColumnIntoStructToIcebergTable() throws IOException {
     icebergTable = testTables.loadTable(TableIdentifier.of("default", "people"));
     testTables.appendIcebergTable(shell.getHiveConf(), icebergTable, fileFormat, null, newPeople);
 
-    List<Record> sortedExpected = new ArrayList<>(people);
+    List<Record> sortedExpected = Lists.newArrayList(people);
     sortedExpected.addAll(newPeople);
     sortedExpected.sort(Comparator.comparingLong(record -> (Long) record.get(0)));
     List<Object[]> rows = shell

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerTimezone.java
Patch:
@@ -92,7 +92,7 @@ public static void beforeClass() {
   }
 
   @AfterClass
-  public static void afterClass() {
+  public static void afterClass() throws Exception {
     shell.stop();
   }
 

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveShell.java
Patch:
@@ -19,7 +19,6 @@
 
 package org.apache.iceberg.mr.hive;
 
-import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
 import java.util.stream.Collectors;
@@ -38,6 +37,7 @@
 import org.apache.iceberg.hive.TestHiveMetastore;
 import org.apache.iceberg.relocated.com.google.common.base.Joiner;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
+import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 
 /**
  * Test class for running HiveQL queries, essentially acting like a Beeline shell in tests.
@@ -101,7 +101,7 @@ public void start() {
     started = true;
   }
 
-  public void stop() {
+  public void stop() throws Exception {
     if (client != null) {
       client.stop();
     }
@@ -140,7 +140,7 @@ public List<Object[]> executeStatement(String statement) {
             "You have to start TestHiveShell and open a session first, before running a query.");
     try {
       OperationHandle handle = client.executeStatement(session.getSessionHandle(), statement, Collections.emptyMap());
-      List<Object[]> resultSet = new ArrayList<>();
+      List<Object[]> resultSet = Lists.newArrayList();
       if (handle.hasResultSet()) {
         RowSet rowSet;
         // keep fetching results until we can

File: iceberg/iceberg-catalog/src/main/java/org/apache/iceberg/hive/HiveCatalog.java
Patch:
@@ -214,7 +214,7 @@ public void renameTable(TableIdentifier from, TableIdentifier originalTo) {
       table.setTableName(to.name());
 
       clients.run(client -> {
-        client.alter_table(fromDatabase, fromName, table);
+        MetastoreUtil.alterTable(client, fromDatabase, fromName, table);
         return null;
       });
 

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java
Patch:
@@ -96,8 +96,7 @@
 public class HiveIcebergMetaHook implements HiveMetaHook {
   private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);
   public static final Map<String, String> COMMON_HMS_PROPERTIES = ImmutableMap.of(
-      BaseMetastoreTableOperations.TABLE_TYPE_PROP, BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase(),
-      InputFormatConfig.EXTERNAL_TABLE_PURGE, "TRUE"
+      BaseMetastoreTableOperations.TABLE_TYPE_PROP, BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase()
   );
   private static final Set<String> PARAMETERS_TO_REMOVE = ImmutableSet
       .of(InputFormatConfig.TABLE_SCHEMA, Catalogs.LOCATION, Catalogs.NAME, InputFormatConfig.PARTITION_SPEC);

File: standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/hive_metastoreConstants.java
Patch:
@@ -81,4 +81,6 @@
 
   public static final java.lang.String CTAS_LEGACY_CONFIG = "create_table_as_external";
 
+  public static final java.lang.String DEFAULT_TABLE_TYPE = "defaultTableType";
+
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java
Patch:
@@ -406,9 +406,8 @@ a database ( directory )
     if (conf.getBoolVar(REPL_SNAPSHOT_DIFF_FOR_EXTERNAL_TABLE_COPY)) {
       Path snapPath = SnapshotUtils.getSnapshotFileListPath(new Path(work.dumpDirectory));
       try {
-        SnapshotUtils.getDFS(getExternalTableBaseDir(conf), conf)
-            .rename(new Path(snapPath, EximUtil.FILE_LIST_EXTERNAL_SNAPSHOT_CURRENT),
-                new Path(snapPath, EximUtil.FILE_LIST_EXTERNAL_SNAPSHOT_OLD), Options.Rename.OVERWRITE);
+        SnapshotUtils.getDFS(snapPath, conf).rename(new Path(snapPath, EximUtil.FILE_LIST_EXTERNAL_SNAPSHOT_CURRENT),
+            new Path(snapPath, EximUtil.FILE_LIST_EXTERNAL_SNAPSHOT_OLD), Options.Rename.OVERWRITE);
       } catch (FileNotFoundException fnf) {
         // Ignore if no file.
       }

File: streaming/src/test/org/apache/hive/streaming/TestStreaming.java
Patch:
@@ -72,6 +72,7 @@
 import org.apache.hadoop.hive.metastore.api.TxnAbortedException;
 import org.apache.hadoop.hive.metastore.api.TxnInfo;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
+import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.metastore.txn.AcidHouseKeeperService;
 import org.apache.hadoop.hive.metastore.txn.TxnCommonUtils;
 import org.apache.hadoop.hive.metastore.utils.TestTxnDbUtil;
@@ -216,7 +217,7 @@ public TestStreaming() throws Exception {
     conf.setBoolVar(HiveConf.ConfVars.METASTORE_EXECUTE_SET_UGI, true);
     conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, true);
     dbFolder.create();
-
+    MetastoreConf.setVar(conf, MetastoreConf.ConfVars.WAREHOUSE, "raw://" + dbFolder.newFolder("warehouse"));
 
     //1) Start from a clean slate (metastore)
     TestTxnDbUtil.cleanDb(conf);

File: streaming/src/test/org/apache/hive/streaming/TestStreamingDynamicPartitioning.java
Patch:
@@ -39,6 +39,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
+import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.metastore.utils.TestTxnDbUtil;
 import org.apache.hadoop.hive.ql.DriverFactory;
 import org.apache.hadoop.hive.ql.IDriver;
@@ -134,6 +135,7 @@ public TestStreamingDynamicPartitioning() throws Exception {
     conf.setBoolVar(HiveConf.ConfVars.METASTORE_EXECUTE_SET_UGI, true);
     conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, true);
     dbFolder.create();
+    MetastoreConf.setVar(conf, MetastoreConf.ConfVars.WAREHOUSE, "raw://" + dbFolder.newFolder("warehouse"));
     loc1 = dbFolder.newFolder(dbName + ".db").toString();
 
     //1) Start from a clean slate (metastore)

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/DirectSqlUpdateStat.java
Patch:
@@ -668,9 +668,9 @@ public long getNextCSIdForMPartitionColumnStatistics(long numStats) throws MetaE
       // the caller gets a reserved range for CSId not used by any other thread.
       boolean insertDone = false;
       while (maxCsId == 0) {
-        String query = "SELECT \"NEXT_VAL\" FROM \"SEQUENCE_TABLE\" WHERE \"SEQUENCE_NAME\"= "
-                + quoteString("org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics")
-                + " FOR UPDATE";
+        String query = sqlGenerator.addForUpdateClause("SELECT \"NEXT_VAL\" FROM \"SEQUENCE_TABLE\" "
+                + "WHERE \"SEQUENCE_NAME\"= "
+                + quoteString("org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics"));
         LOG.debug("Going to execute query " + query);
         statement = dbConn.createStatement();
         rs = statement.executeQuery(query);

File: hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/HCatClientHMSImpl.java
Patch:
@@ -586,7 +586,7 @@ private void dropPartitionsUsingExpressions(Table table, Map<String, String> par
     LOG.info("HCatClient: Dropping partitions using partition-predicate Expressions.");
     ExprNodeGenericFuncDesc partitionExpression = new ExpressionBuilder(table, partitionSpec).build();
     Pair<Integer, byte[]> serializedPartitionExpression = Pair.of(partitionSpec.size(),
-            SerializationUtilities.serializeExpressionToKryo(partitionExpression));
+            SerializationUtilities.serializeObjectWithTypeInformation(partitionExpression));
     hmsClient.dropPartitions(table.getDbName(), table.getTableName(), Arrays.asList(serializedPartitionExpression),
         deleteData && !isExternal(table),  // Delete data?
         ifExists,                          // Fail if table doesn't exist?

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/misc/msck/MsckAnalyzer.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.ql.ddl.misc.msck;
 
+import java.io.Serializable;
 import java.util.List;
 import java.util.Map;
 
@@ -81,8 +82,8 @@ public void analyzeInternal(ASTNode root) throws SemanticException {
             "to be set to org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore");
       }
       // fetch the first value of partitionSpecs map since it will always have one key, value pair
-      filterExp = SerializationUtilities.serializeExpressionToKryo(
-          (ExprNodeGenericFuncDesc) ((List) partitionSpecs.values().toArray()[0]).get(0));
+      filterExp = SerializationUtilities.serializeObjectWithTypeInformation(
+          (Serializable) ((List) partitionSpecs.values().toArray()[0]).get(0));
     }
 
     if (repair && AcidUtils.isTransactionalTable(table)) {

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/drop/AlterTableDropPartitionOperation.java
Patch:
@@ -123,7 +123,7 @@ private void dropPartitions(boolean isRepl) throws HiveException {
     List<Pair<Integer, byte[]>> partitionExpressions = new ArrayList<>(desc.getPartSpecs().size());
     for (AlterTableDropPartitionDesc.PartitionDesc partSpec : desc.getPartSpecs()) {
       partitionExpressions.add(Pair.of(partSpec.getPrefixLength(),
-          SerializationUtilities.serializeExpressionToKryo(partSpec.getPartSpec())));
+          SerializationUtilities.serializeObjectWithTypeInformation(partSpec.getPartSpec())));
     }
 
     PartitionDropOptions options =

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java
Patch:
@@ -103,7 +103,7 @@ static public Object evalExprWithPart(ExprNodeDesc expr,
   }
 
   public static Pair<PrimitiveObjectInspector, ExprNodeEvaluator> prepareExpr(
-      ExprNodeGenericFuncDesc expr, List<String> partColumnNames,
+      ExprNodeDesc expr, List<String> partColumnNames,
       List<PrimitiveTypeInfo> partColumnTypeInfos) throws HiveException {
     // Create the row object
     List<ObjectInspector> partObjectInspectors = new ArrayList<ObjectInspector>();

File: ql/src/test/org/apache/hadoop/hive/metastore/TestMetastoreExpr.java
Patch:
@@ -183,12 +183,12 @@ public void checkExpr(int numParts,
       String dbName, String tblName, ExprNodeGenericFuncDesc expr, Table t) throws Exception {
     List<Partition> parts = new ArrayList<Partition>();
     client.listPartitionsByExpr(dbName, tblName,
-        SerializationUtilities.serializeExpressionToKryo(expr), null, (short)-1, parts);
+        SerializationUtilities.serializeObjectWithTypeInformation(expr), null, (short)-1, parts);
     assertEquals("Partition check failed: " + expr.getExprString(), numParts, parts.size());
 
     // check with partition spec as well
     PartitionsByExprRequest req = new PartitionsByExprRequest(dbName, tblName,
-            ByteBuffer.wrap(SerializationUtilities.serializeExpressionToKryo(expr)));
+        ByteBuffer.wrap(SerializationUtilities.serializeObjectWithTypeInformation(expr)));
     req.setMaxParts((short)-1);
     req.setId(t.getId());
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -422,7 +422,7 @@ public int execute() {
           int statementId = tbd.getStmtId();
           if (tbd.isDirectInsert() || tbd.isMmTable()) {
             statementId = queryPlan.getStatementIdForAcidWriteType(work.getLoadTableWork().getWriteId(),
-                tbd.getMoveTaskId(), work.getLoadTableWork().getWriteType(), tbd.getSourcePath());
+                tbd.getMoveTaskId(), work.getLoadTableWork().getWriteType(), tbd.getSourcePath(), statementId);
             LOG.debug("The statementId used when loading the dynamic partitions is " + statementId);
           }
 
@@ -566,7 +566,7 @@ private DataContainer handleDynParts(Hive db, Table table, LoadTableDesc tbd,
     int statementId = tbd.getStmtId();
     if (tbd.isDirectInsert() || tbd.isMmTable()) {
       statementId = queryPlan.getStatementIdForAcidWriteType(work.getLoadTableWork().getWriteId(),
-          tbd.getMoveTaskId(), work.getLoadTableWork().getWriteType(), tbd.getSourcePath());
+          tbd.getMoveTaskId(), work.getLoadTableWork().getWriteType(), tbd.getSourcePath(), statementId);
       LOG.debug("The statementId used when loading the dynamic partitions is " + statementId);
     }
     Map<String, List<Path>> dynamicPartitionSpecs = null;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4398,7 +4398,7 @@ public static enum ConfVars {
     HIVE_LOAD_DYNAMIC_PARTITIONS_THREAD_COUNT("hive.load.dynamic.partitions.thread", 15,
         new  SizeValidator(1L, true, 1024L, true),
         "Number of threads used to load dynamic partitions."),
-    HIVE_LOAD_DYNAMIC_PARTITIONS_SCAN_SPECIFIC_PARTITIONS("hive.load.dynamic.partitions.scan.specific.partitions", false,
+    HIVE_LOAD_DYNAMIC_PARTITIONS_SCAN_SPECIFIC_PARTITIONS("hive.load.dynamic.partitions.scan.specific.partitions", true,
         "For the dynamic partitioned tables, scan only the specific partitions using the name from the list"),
     // If this is set all move tasks at the end of a multi-insert query will only begin once all
     // outputs are ready

File: beeline/src/java/org/apache/hive/beeline/BeeLine.java
Patch:
@@ -86,6 +86,7 @@
 import org.apache.commons.cli.OptionBuilder;
 import org.apache.commons.cli.Options;
 import org.apache.commons.cli.ParseException;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hive.conf.Constants;
@@ -1340,7 +1341,7 @@ private int executeFile(String fileName) {
       } else {
         org.apache.hadoop.fs.Path path = new org.apache.hadoop.fs.Path(fileName);
         FileSystem fs;
-        HiveConf conf = new HiveConf();
+        Configuration conf = new Configuration();
         if (!path.toUri().isAbsolute()) {
           fs = FileSystem.getLocal(conf);
           path = fs.makeQualified(path);

File: beeline/src/java/org/apache/hive/beeline/BeeLine.java
Patch:
@@ -86,7 +86,6 @@
 import org.apache.commons.cli.OptionBuilder;
 import org.apache.commons.cli.Options;
 import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hive.conf.Constants;
@@ -1341,7 +1340,7 @@ private int executeFile(String fileName) {
       } else {
         org.apache.hadoop.fs.Path path = new org.apache.hadoop.fs.Path(fileName);
         FileSystem fs;
-        Configuration conf = new Configuration();
+        HiveConf conf = new HiveConf();
         if (!path.toUri().isAbsolute()) {
           fs = FileSystem.getLocal(conf);
           path = fs.makeQualified(path);

File: beeline/src/java/org/apache/hive/beeline/BeeLine.java
Patch:
@@ -86,6 +86,7 @@
 import org.apache.commons.cli.OptionBuilder;
 import org.apache.commons.cli.Options;
 import org.apache.commons.cli.ParseException;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hive.conf.Constants;
@@ -1340,7 +1341,7 @@ private int executeFile(String fileName) {
       } else {
         org.apache.hadoop.fs.Path path = new org.apache.hadoop.fs.Path(fileName);
         FileSystem fs;
-        HiveConf conf = new HiveConf();
+        Configuration conf = new Configuration();
         if (!path.toUri().isAbsolute()) {
           fs = FileSystem.getLocal(conf);
           path = fs.makeQualified(path);

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.common.ZKDeRegisterWatcher;
 import org.apache.hadoop.hive.common.ZooKeeperHiveHelper;
-import org.apache.hadoop.hive.common.metrics.common.MetricsFactory;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore;
@@ -284,7 +283,6 @@ public static void main(String[] args) throws Throwable {
         if (MetastoreConf.getBoolVar(conf, ConfVars.METRICS_ENABLED)) {
           try {
             Metrics.shutdown();
-            MetricsFactory.close();
           } catch (Exception e) {
             LOG.error("error in Metrics deinit: " + e.getClass().getName() + " "
                 + e.getMessage(), e);
@@ -306,7 +304,6 @@ public static void main(String[] args) throws Throwable {
       if (MetastoreConf.getBoolVar(conf, ConfVars.METRICS_ENABLED)) {
         try {
           Metrics.initialize(conf);
-          MetricsFactory.init(conf);
         } catch (Exception e) {
           // log exception, but ignore inability to start
           LOG.error("error in Metrics init: " + e.getClass().getName() + " "

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4125,7 +4125,7 @@ public static enum ConfVars {
         "          (Use with property hive.server2.custom.authentication.class)\n" +
         "  PAM: Pluggable authentication module\n" +
         "  NOSASL:  Raw transport\n" +
-        "  SAML2: SAML 2.0 compliant authentication. This is only supported in http transport mode."),
+        "  SAML: SAML 2.0 compliant authentication. This is only supported in http transport mode."),
     HIVE_SERVER2_TRUSTED_DOMAIN("hive.server2.trusted.domain", "",
         "Specifies the host or a domain to trust connections from. Authentication is skipped " +
         "for any connection coming from a host whose hostname ends with the value of this" +

File: service/src/test/org/apache/hive/service/cli/thrift/ThriftHttpServletTest.java
Patch:
@@ -43,7 +43,7 @@ public class ThriftHttpServletTest {
   private ThriftHttpServlet thriftHttpServlet;
 
   @Before
-  public void setUp() {
+  public void setUp() throws Exception {
     String authType = HiveAuthConstants.AuthTypes.KERBEROS.toString();
     thriftHttpServlet = new ThriftHttpServlet(null, null, authType, null, null, null,
         new HiveConf());

File: service/src/java/org/apache/hive/service/auth/AuthenticationProviderFactory.java
Patch:
@@ -52,7 +52,7 @@ public HiveConf getConf() {
     public static AuthMethods getValidAuthMethod(String authMethodStr)
       throws AuthenticationException {
       for (AuthMethods auth : AuthMethods.values()) {
-        if (authMethodStr.equals(auth.getAuthMethod())) {
+        if (authMethodStr.toLowerCase().contains(auth.getAuthMethod().toLowerCase())) {
           return auth;
         }
       }

File: service/src/java/org/apache/hive/service/auth/saml/HiveSamlUtils.java
Patch:
@@ -36,7 +36,7 @@ public class HiveSamlUtils {
   public static final String MESSAGE_KEY = "message";
 
   public static boolean isSamlAuthMode(String authType) {
-    return authType.equalsIgnoreCase(HiveAuthConstants.AuthTypes.SAML.toString());
+    return authType.toLowerCase().contains(HiveAuthConstants.AuthTypes.SAML.toString().toLowerCase());
   }
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/MetaStoreCompactorThread.java
Patch:
@@ -128,7 +128,7 @@ protected static long updateCycleDurationMetric(String metric, long startedAt) {
       long elapsed = System.currentTimeMillis() - startedAt;
       LOG.debug("Updating {} metric to {}", metric, elapsed);
       Metrics.getOrCreateGauge(metric)
-          .set((int)elapsed);
+          .set((int) elapsed);
       return elapsed;
     }
     return 0;

File: ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java
Patch:
@@ -370,7 +370,7 @@ public void processCompactionCandidatesInParallel() throws Exception {
     Table t = newTable("default", "camipc", true);
     List<Partition> partitions = new ArrayList<>();
     Partition p;
-    for(int i = 0; i < 10; i++) {
+    for (int i = 0; i < 10; i++) {
       p = newPartition(t, "today" + i);
 
       addBaseFile(t, p, 20L, 20);
@@ -381,9 +381,9 @@ public void processCompactionCandidatesInParallel() throws Exception {
     }
 
     burnThroughTransactions("default", "camipc", 25);
-    for(int i = 0; i < 10; i++) {
+    for (int i = 0; i < 10; i++) {
       CompactionRequest rqst = new CompactionRequest("default", "camipc", CompactionType.MINOR);
-      rqst.setPartitionname("ds=today"+i);
+      rqst.setPartitionname("ds=today" + i);
       compactInTxn(rqst);
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java
Patch:
@@ -171,7 +171,7 @@ public void run() {
             handle.releaseLocks();
           }
           if (metricsEnabled) {
-            updateCycleDurationMetric(MetricsConstants.COMPACTION_INITIATOR_CYCLE_DURATION, startedAt);
+            updateCycleDurationMetric(MetricsConstants.COMPACTION_CLEANER_CYCLE_DURATION, startedAt);
           }
           stopCycleUpdater();
         }

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
Patch:
@@ -1348,7 +1348,7 @@ public enum ConfVars {
     TCP_KEEP_ALIVE("metastore.server.tcp.keepalive",
         "hive.metastore.server.tcp.keepalive", true,
         "Whether to enable TCP keepalive for the metastore server. Keepalive will prevent accumulation of half-open connections."),
-    THREAD_POOL_SIZE("metastore.thread.pool.size", "no.such", 10,
+    THREAD_POOL_SIZE("metastore.thread.pool.size", "no.such", 15,
         "Number of threads in the thread pool.  These will be used to execute all background " +
             "processes."),
     THRIFT_CONNECTION_RETRIES("metastore.connect.retries", "hive.metastore.connect.retries", 3,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLPlanUtils.java
Patch:
@@ -258,7 +258,7 @@ public List<String> getTableColumnNames(Table tbl) {
 
   public String getPartitionActualName(Partition pt) {
     Map<String, String> colTypeMap = getTableColumnsToType(pt.getTable());
-    String[] partColsDef = pt.getName().split(",");
+    String[] partColsDef = pt.getName().split("/");
     List<String> ptParam = new ArrayList<>();
     for (String partCol : partColsDef) {
       String[] colValue = partCol.split("=");

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/TestHelper.java
Patch:
@@ -125,7 +125,7 @@ public DataFile writeFile(StructLike partition, List<Record> records) throws IOE
   }
 
   private GenericAppenderHelper appender() {
-    return new GenericAppenderHelper(table, fileFormat, tmp);
+    return new GenericAppenderHelper(table, fileFormat, tmp, conf);
   }
 
   public static class RecordsBuilder {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDeserialize.java
Patch:
@@ -72,9 +72,6 @@ public ObjectInspector initialize(ObjectInspector[] arguments)
     public Object evaluate(DeferredObject[] arguments) throws HiveException {
         String value = PrimitiveObjectInspectorUtils.getString(arguments[0].get(), stringOI);
         String compressionFormat = PrimitiveObjectInspectorUtils.getString(arguments[1].get(), this.compressionFormat);
-        if (value == null || StringUtils.isEmpty(compressionFormat)) {
-            return value;
-        }
         MessageEncoder encoder;
         try {
             encoder = MessageFactory.getInstance(compressionFormat);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3342,7 +3342,7 @@ public static enum ConfVars {
     MERGE_CARDINALITY_VIOLATION_CHECK("hive.merge.cardinality.check", true,
       "Set to true to ensure that each SQL Merge statement ensures that for each row in the target\n" +
         "table there is at most 1 matching row in the source table per SQL Specification."),
-    MERGE_SPLIT_UPDATE("hive.merge.split.update", false,
+    MERGE_SPLIT_UPDATE("hive.merge.split.update", true,
         "If true, SQL Merge statement will handle WHEN MATCHED UPDATE by splitting it into 2\n" +
             "branches of a multi-insert, representing delete of existing row and an insert of\n" +
             "the new version of the row.  Updating bucketing and partitioning columns should\n" +

File: storage-api/src/java/org/apache/hive/common/util/TxnIdUtils.java
Patch:
@@ -67,7 +67,7 @@ public static int compare(ValidWriteIdList a, ValidWriteIdList b) {
       }
     } else {
       if (b.getHighWatermark() != a.getInvalidWriteIds()[minLen] -1) {
-        return Long.signum(b.getHighWatermark() - (a.getInvalidWriteIds()[minLen] -1));
+        return Long.signum((a.getInvalidWriteIds()[minLen] - 1) - b.getHighWatermark());
       }
       if (allInvalidFrom(a.getInvalidWriteIds(), minLen, a.getHighWatermark())) {
         return 0;

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -5286,8 +5286,8 @@ private boolean hasRemainingCDReference(MColumnDescriptor oldCD) {
      */
     try {
       // HIVE-21075: Fix Postgres performance regression caused by HIVE-9447
-      DatabaseProduct dbProduct = DatabaseProduct.determineDatabaseProduct(MetaStoreDirectSql.getProductName(pm), conf);
-      if (dbProduct.isPOSTGRES() || dbProduct.isMYSQL()) {
+      LOG.debug("The dbType is {} ", dbType.getHiveSchemaPostfix());
+      if (dbType.isPOSTGRES() || dbType.isMYSQL()) {
         query = pm.newQuery(MStorageDescriptor.class, "this.cd == inCD");
         query.declareParameters("MColumnDescriptor inCD");
         List<MStorageDescriptor> referencedSDs = null;

File: serde/src/java/org/apache/hadoop/hive/serde2/RandomTypeUtil.java
Patch:
@@ -131,7 +131,7 @@ public static Date getRandDate(Random r) {
 
   public static final long NANOSECONDS_PER_SECOND = TimeUnit.SECONDS.toNanos(1);
   public static final long MILLISECONDS_PER_SECOND = TimeUnit.SECONDS.toMillis(1);
-  public static final long NANOSECONDS_PER_MILLISSECOND = TimeUnit.MILLISECONDS.toNanos(1);
+  public static final long NANOSECONDS_PER_MILLISECOND = TimeUnit.MILLISECONDS.toNanos(1);
 
   private static final ThreadLocal<DateFormat> DATE_FORMAT =
       new ThreadLocal<DateFormat>() {
@@ -172,12 +172,12 @@ public static Timestamp getRandTimestamp(Random r, int minYear, int maxYear) {
     case 2:
       // Limit to milliseconds only...
       optionalNanos = String.format(".%09d",
-          Integer.valueOf(r.nextInt((int) MILLISECONDS_PER_SECOND)) * NANOSECONDS_PER_MILLISSECOND);
+          Integer.valueOf(r.nextInt((int) MILLISECONDS_PER_SECOND)) * NANOSECONDS_PER_MILLISECOND);
       break;
     case 3:
       // Limit to below milliseconds only...
       optionalNanos = String.format(".%09d",
-          Integer.valueOf(r.nextInt((int) NANOSECONDS_PER_MILLISSECOND)));
+          Integer.valueOf(r.nextInt((int) NANOSECONDS_PER_MILLISECOND)));
       break;
     }
     String timestampStr = String.format("%04d-%02d-%02d %02d:%02d:%02d%s",

File: serde/src/java/org/apache/hadoop/hive/serde2/SerDeStatsStruct.java
Patch:
@@ -21,7 +21,7 @@
 public interface SerDeStatsStruct {
 
   /**
-   * Rerurns the serialized size of the object.
+   * Returns the serialized size of the object.
    */
   public long getRawDataSerializedSize();
 

File: serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
Patch:
@@ -112,7 +112,7 @@ public static String escapeString(String str) {
         escape.append('t');
         break;
       default:
-        // Control characeters! According to JSON RFC u0020
+        // Control characters! According to JSON RFC u0020
         if (c < ' ') {
           String hex = Integer.toHexString(c);
           escape.append('\\');

File: serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java
Patch:
@@ -161,7 +161,7 @@ private Object serialize(TypeInfo typeInfo, ObjectInspector fieldOI, Object stru
 
   /** private cache to avoid lots of EnumSymbol creation while serializing.
    *  Two levels because the enum symbol is specific to a schema.
-   *  Object because we want to avoid the overhead of repeated toString calls while maintaining compatability.
+   *  Object because we want to avoid the overhead of repeated toString calls while maintaining compatibility.
    *  Provided there are few enum types per record, and few symbols per enum, memory use should be moderate.
    *  eg 20 types with 50 symbols each as length-10 Strings should be on the order of 100KB per AvroSerializer.
    */

File: serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarSerDe.java
Patch:
@@ -88,7 +88,7 @@ public void initialize(Configuration configuration, Properties tableProperties,
     serdeParams = new LazySerDeParameters(configuration, properties, getClass().getName());
 
     // Create the ObjectInspectors for the fields. Note: Currently
-    // ColumnarObject uses same ObjectInpector as LazyStruct
+    // ColumnarObject uses same ObjectInspector as LazyStruct
     cachedObjectInspector = LazyFactory.createColumnarStructInspector(
         serdeParams.getColumnNames(), serdeParams.getColumnTypes(), serdeParams);
 

File: serde/src/java/org/apache/hadoop/hive/serde2/io/HiveCharWritable.java
Patch:
@@ -26,7 +26,7 @@
 /**
  * HiveCharWritable.
  * String values will be padded to full char length.
- * Character legnth, comparison, hashCode should ignore trailing spaces.
+ * Character length, comparison, hashCode should ignore trailing spaces.
  */
 public class HiveCharWritable extends HiveBaseCharWritable 
     implements WritableComparable<HiveCharWritable> {

File: serde/src/java/org/apache/hadoop/hive/serde2/json/HiveJsonReader.java
Patch:
@@ -469,7 +469,7 @@ private byte[] getByteValue(final JsonNode binaryNode) throws SerDeException {
   /**
    * Matches the JSON object's field name with the Hive data type.
    *
-   * @param oi The ObjectInsepctor to lookup the matching in
+   * @param oi The ObjectInspector to lookup the matching in
    * @param fieldName The name of the field parsed from the JSON text
    * @return The meta data of regarding this field
    * @throws SerDeException The SerDe is not configured correctly

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
Patch:
@@ -84,7 +84,7 @@ public final class LazyFactory {
    * of the lazy object.
    *
    * @param poi PrimitiveObjectInspector
-   * @param typeBinary a switch to return either a LazyPrimtive class or it's binary
+   * @param typeBinary a switch to return either a LazyPrimitive class or it's binary
    *        companion
    * @return LazyPrimitive&lt;? extends ObjectInspector, ? extends Writable&gt;
    */

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java
Patch:
@@ -58,7 +58,7 @@ public LazyHiveDecimal(LazyHiveDecimal copy) {
   }
 
   /**
-   * Initilizes LazyHiveDecimal object by interpreting the input bytes
+   * Initializes LazyHiveDecimal object by interpreting the input bytes
    * as a numeric string
    *
    * @param bytes

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyTimestamp.java
Patch:
@@ -46,7 +46,7 @@ public LazyTimestamp(LazyTimestamp copy) {
   }
 
   /**
-   * Initilizes LazyTimestamp object by interpreting the input bytes
+   * Initializes LazyTimestamp object by interpreting the input bytes
    * as a JDBC timestamp string
    *
    * @param bytes

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/StringToDouble.java
Patch:
@@ -71,7 +71,7 @@ public static double strtod(byte[] utf8, int offset, int length)
     int c;
     int exp = 0;		/* Exponent read from "EX" field. */
     int fracExp = 0;		/* Exponent that derives from the fractional
-				 * part.  Under normal circumstatnces, it is
+				 * part.  Under normal circumstances, it is
 				 * the negative of the number of digits in F.
 				 * However, if I is very long, the last digits
 				 * of I get dropped (otherwise a long I with a

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryArray.java
Patch:
@@ -135,9 +135,9 @@ private void parse() {
     // adjust arrays
     adjustArraySize(arraySize);
     // find out the null-bytes
-    int arryByteStart = start + vInt.length;
-    int nullByteCur = arryByteStart;
-    int nullByteEnd = arryByteStart + (arraySize + 7) / 8;
+    int arrayByteStart = start + vInt.length;
+    int nullByteCur = arrayByteStart;
+    int nullByteEnd = arrayByteStart + (arraySize + 7) / 8;
     // the begin the real elements
     int lastElementByteEnd = nullByteEnd;
     // the list element object inspector

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe2.java
Patch:
@@ -65,7 +65,7 @@
 /**
  * Subclass of LazyBinarySerDe with faster serialization, initializing a serializer based on the
  * row columns rather than checking the ObjectInspector category/primitiveType for every value.
- * This appears to be around 3x faster than the LazyBinarSerDe serialization.
+ * This appears to be around 3x faster than the LazyBinarySerDe serialization.
  */
 @SerDeSpec(schemaProps = {serdeConstants.LIST_COLUMNS, serdeConstants.LIST_COLUMN_TYPES})
 public class LazyBinarySerDe2 extends LazyBinarySerDe {

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ListObjectsEqualComparer.java
Patch:
@@ -32,7 +32,7 @@
  * Compare two list of objects.
  * Two lists are expected to have same types. Type information for every object is
  * passed when calling Constructor to avoid the step of figuring out types from
- * ObjectInspetor and determine how to compare the types when comparing.
+ * ObjectInspector and determine how to compare the types when comparing.
  * Also, for string and text elements, it performs slightly better than
  * using ObjectInspectorUtils.compare() == 0, which instead of calling .compare()
  * calls .equalTo(), which compares size before byte by byte comparison.

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
Patch:
@@ -439,7 +439,7 @@ public static class UnionConverter implements Converter {
     UnionObjectInspector inputOI;
     SettableUnionObjectInspector outputOI;
 
-    // Object inspectors for the tags for the input and output unionss
+    // Object inspectors for the tags for the input and output unions
     List<? extends ObjectInspector> inputTagsOIs;
     List<? extends ObjectInspector> outputTagsOIs;
 

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
Patch:
@@ -631,7 +631,7 @@ public static String getObjectInspectorName(ObjectInspector oi) {
   /**
    * Computes the bucket number to which the bucketFields belong to
    * @param bucketFields  the bucketed fields of the row
-   * @param bucketFieldInspectors  the ObjectInpsectors for each of the bucketed fields
+   * @param bucketFieldInspectors  the ObjectInspectors for each of the bucketed fields
    * @param totalBuckets the number of buckets in the table
    * @return the bucket number using Murmur hash
    */
@@ -642,7 +642,7 @@ public static int getBucketNumber(Object[] bucketFields, ObjectInspector[] bucke
   /**
    * Computes the bucket number to which the bucketFields belong to
    * @param bucketFields  the bucketed fields of the row
-   * @param bucketFieldInspectors  the ObjectInpsectors for each of the bucketed fields
+   * @param bucketFieldInspectors  the ObjectInspectors for each of the bucketed fields
    * @param totalBuckets the number of buckets in the table
    * @return the bucket number
    */

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java
Patch:
@@ -1418,7 +1418,7 @@ public static enum PrimitiveGrouping {
    * Based on the PrimitiveCategory of a type, return the PrimitiveGrouping
    * that the PrimitiveCategory belongs to (numeric, string, date, etc).
    * @param primitiveCategory Primitive category of the type
-   * @return PrimitveGrouping corresponding to the PrimitiveCategory,
+   * @return PrimitiveGrouping corresponding to the PrimitiveCategory,
    *         or UNKNOWN_GROUP if the type does not match to a grouping.
    */
   public static PrimitiveGrouping getPrimitiveGrouping(PrimitiveCategory primitiveCategory) {

File: serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinaryDataInputStream.java
Patch:
@@ -136,7 +136,7 @@ public Date readDate() throws IOException, ParseException {
    * Read CHAR(N).
    * The representation of char in Teradata binary format is
    * the byte number to read is based on the [charLength] * [bytePerChar] &lt;- totalLength,
-   * bytePerChar is decided by the charset: LATAIN charset is 2 bytes per char and UNICODE charset is 3 bytes per char.
+   * bytePerChar is decided by the charset: LATIN charset is 2 bytes per char and UNICODE charset is 3 bytes per char.
    * the null char will use space to pad.
    *
    * @param totalLength the total length

File: serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinaryDataOutputStream.java
Patch:
@@ -165,7 +165,7 @@ public void writeLong(long l) throws IOException {
    * Write CHAR(N).
    * The representation of char in Teradata binary format is:
    * the byte number to read is based on the [charLength] * [bytePerChar] &lt;- totalLength,
-   * bytePerChar is decided by the charset: LATAIN charset is 2 bytes per char and UNICODE charset is 3 bytes per char.
+   * bytePerChar is decided by the charset: LATIN charset is 2 bytes per char and UNICODE charset is 3 bytes per char.
    * the null char will use space to pad.
    *
    * @param writable the writable

File: serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinarySerde.java
Patch:
@@ -412,8 +412,8 @@ private void serializeField(Object objectForField, ObjectInspector oi, TypeInfo
   private Object deserializeField(TeradataBinaryDataInputStream in, TypeInfo type, Object reuse, boolean isNull)
       throws IOException, ParseException, SerDeException {
     // isNull:
-    // In the Teradata Binary file, even the field is null (isNull=true),
-    // thd data still has some default values to pad the record.
+    // In the Teradata Binary file, even if the field is null (isNull=true),
+    // the data still has some default values to pad the record.
     // In this case, you cannot avoid reading the bytes even it is not used.
     switch (type.getCategory()) {
     case PRIMITIVE:

File: serde/src/java/org/apache/hadoop/hive/serde2/thrift/TCTLSeparatedProtocol.java
Patch:
@@ -182,7 +182,7 @@ public String getMapSeparator() {
   protected final TTransport innerTransport;
 
   /**
-   * Strings used to lookup the various configurable paramaters of this
+   * Strings used to lookup the various configurable parameters of this
    * protocol.
    */
   public static final String ReturnNullsKey = "separators.return_nulls";

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TimestampLocalTZTypeInfo.java
Patch:
@@ -45,7 +45,7 @@ public String getTypeName() {
 
   @Override
   public void setTypeName(String typeName) {
-    // No need to set type name, it should always be timestamplocaltz
+    // No need to set type name, it should always be {@link serdeConstants.TIMESTAMPLOCALTZ_TYPE_NAME}
     return;
   }
 

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
Patch:
@@ -712,9 +712,9 @@ public static ObjectInspector getStandardJavaObjectInspectorFromTypeInfo(
         break;
       }
       case STRUCT: {
-        StructTypeInfo strucTypeInfo = (StructTypeInfo) typeInfo;
-        List<String> fieldNames = strucTypeInfo.getAllStructFieldNames();
-        List<TypeInfo> fieldTypeInfos = strucTypeInfo
+        StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;
+        List<String> fieldNames = structTypeInfo.getAllStructFieldNames();
+        List<TypeInfo> fieldTypeInfos = structTypeInfo
             .getAllStructFieldTypeInfos();
         List<ObjectInspector> fieldObjectInspectors = new ArrayList<ObjectInspector>(
             fieldTypeInfos.size());

File: serde/src/test/org/apache/hadoop/hive/serde2/SerdeRandomRowSource.java
Patch:
@@ -457,7 +457,7 @@ private void chooseSchema(SupportedTypes supportedTypes, int maxComplexDepth) {
         primitiveObjectInspectorList.add(null);
         break;
       default:
-        throw new RuntimeException("Unexpected catagory " + category);
+        throw new RuntimeException("Unexpected category " + category);
       }
       objectInspectorList.add(objectInspector);
 

File: serde/src/test/org/apache/hadoop/hive/serde2/TestJsonSerDe.java
Patch:
@@ -49,7 +49,7 @@
 public class TestJsonSerDe {
 
   @Test
-  public void testPrimativeDataTypes() throws Exception {
+  public void testPrimitiveDataTypes() throws Exception {
     Properties props = new Properties();
     props.setProperty(serdeConstants.LIST_COLUMNS,
         "name,height,weight,endangered,born");
@@ -168,7 +168,7 @@ public void testArray() throws Exception {
   /**
    * Test when a map has a key defined as a numeric value. Technically, JSON
    * does not support this because each key in a map must be a quoted string.
-   * Unquoted strings (hence an int value) is allowed by Javascript, but not by
+   * Unquoted strings (hence an int value) is allowed by JavaScript, but not by
    * JSON specification. For Hive, the int map key type is stored as a string
    * and must be converted back into an int type.
    */

File: serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java
Patch:
@@ -510,7 +510,7 @@ public void canDeserializeEnums() throws SerDeException, IOException {
     assertEquals("DALEKS", finalValue);
   }
 
-  @Test // Fixed doesn't exist in Hive. Fixeds go in, lists of bytes go out.
+  @Test // Fixed doesn't exist in Hive. Fixed go in, lists of bytes go out.
   public void canDeserializeFixed() throws SerDeException, IOException {
     Schema s = AvroSerdeUtils.getSchemaFor(TestAvroObjectInspectorGenerator.FIXED_SCHEMA);
     GenericData.Record record = new GenericData.Record(s);

File: serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerdeUtils.java
Patch:
@@ -48,7 +48,7 @@ public class TestAvroSerdeUtils {
       "    {\"name\":\"mayBeNull\", \"type\":[\"string\", \"null\"]}\n" +
       "  ]\n" +
       "}";
-  // Same union, order reveresed
+  // Same union, order reversed
   private final String NULLABLE_UNION2 = "{\n" +
     "  \"type\": \"record\", \n" +
     "  \"name\": \"nullTest\",\n" +
@@ -151,7 +151,7 @@ public void determineSchemaFindsLiterals() throws Exception {
   }
 
   @Test
-  public void detemineSchemaTriesToOpenUrl() throws AvroSerdeException, IOException {
+  public void determineSchemaTriesToOpenUrl() throws AvroSerdeException, IOException {
     Configuration conf = new Configuration();
     Properties props = new Properties();
     props.put(AvroTableProperties.SCHEMA_URL.getPropName(), "not:///a.real.url");

File: serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java
Patch:
@@ -338,7 +338,7 @@ private void verifyRead(BinarySortableDeserializeRead binarySortableDeserializeR
         }
       }
       if (!VerifyLazy.lazyCompare(typeInfo, complexFieldObj, expectedObject)) {
-        fail("Comparision failed typeInfo " + typeInfo.toString());
+        fail("Comparison failed typeInfo " + typeInfo.toString());
       }
     }
   }

File: serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazySimpleFast.java
Patch:
@@ -244,7 +244,7 @@ private void verifyRead(LazySimpleDeserializeRead lazySimpleDeserializeRead,
         }
       }
       if (!VerifyLazy.lazyCompare(typeInfo, complexFieldObj, expectedObject)) {
-        fail("Comparision failed typeInfo " + typeInfo.toString());
+        fail("Comparison failed typeInfo " + typeInfo.toString());
       }
     }
   }

File: serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinaryFast.java
Patch:
@@ -240,7 +240,7 @@ private void verifyRead(LazyBinaryDeserializeRead lazyBinaryDeserializeRead,
         }
       }
       if (!VerifyLazy.lazyCompare(typeInfo, complexFieldObj, expectedObject)) {
-        fail("Comparision failed typeInfo " + typeInfo.toString());
+        fail("Comparison failed typeInfo " + typeInfo.toString());
       }
     }
   }

File: serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorConverters.java
Patch:
@@ -85,7 +85,7 @@ public void testObjectInspectorConverters() throws Throwable {
       convertText();
 
       // Binary
-      converBinary();
+      convertBinary();
 
       // Union
       convertUnion();
@@ -185,7 +185,7 @@ private void convertUnion() {
       assertEquals(expectedObjectExtra, convertedObjectExtra); // we should get back null
 }
 
-private void converBinary() {
+private void convertBinary() {
 	Converter baConverter = ObjectInspectorConverters.getConverter(
           PrimitiveObjectInspectorFactory.javaStringObjectInspector,
           PrimitiveObjectInspectorFactory.writableBinaryObjectInspector);

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloHiveConstants.java
Patch:
@@ -35,7 +35,7 @@ public class AccumuloHiveConstants {
       + Character.toString(ASTERISK);
 
   // Escape the escape, and escape the asterisk
-  public static final String ESCAPED_ASERTISK_REGEX = Character.toString(ESCAPE)
+  public static final String ESCAPED_ASTERISK_REGEX = Character.toString(ESCAPE)
       + Character.toString(ESCAPE) + Character.toString(ESCAPE) + Character.toString(ASTERISK);
 
   public static final Charset UTF_8 = Charset.forName("UTF-8");

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/HiveAccumuloHelper.java
Patch:
@@ -341,7 +341,7 @@ public Token<? extends TokenIdentifier> setConnectorInfoForInputAndOutput(Accumu
     AuthenticationToken token = getDelegationToken(conn);
 
     // Make sure the Accumulo token is set in the Configuration (only a stub of the Accumulo
-    // AuthentiationToken is serialized, not the entire token). configureJobConf may be
+    // AuthenticationToken is serialized, not the entire token). configureJobConf may be
     // called multiple times with the same JobConf which results in an error from Accumulo
     // MapReduce API. Catch the error, log a debug message and just keep going
     try {

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/columns/ColumnMappingFactory.java
Patch:
@@ -87,7 +87,7 @@ public static ColumnMapping get(String columnSpec, ColumnEncoding defaultEncodin
 
         // Replace any \* that appear in the prefix with a regular *
         if (-1 != cq.indexOf(AccumuloHiveConstants.ESCAPED_ASTERISK)) {
-          cq = cq.replaceAll(AccumuloHiveConstants.ESCAPED_ASERTISK_REGEX,
+          cq = cq.replaceAll(AccumuloHiveConstants.ESCAPED_ASTERISK_REGEX,
               Character.toString(AccumuloHiveConstants.ASTERISK));
         }
 

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/HiveAccumuloTableInputFormat.java
Patch:
@@ -121,7 +121,7 @@ public InputSplit[] getSplits(JobConf jobConf, int numSplits) throws IOException
           log.info("Job credential tokens: " + jobConf.getCredentials().getAllTokens());
           AuthenticationToken unwrappedToken = ConfiguratorBase.unwrapAuthenticationToken(jobConf, token);
           log.info("Converted authentication token from Configuration into: " + unwrappedToken);
-          // It's possible that the Job doesn't have the token in its credentials. In this case, unwrapAuthenticatinoToken
+          // It's possible that the Job doesn't have the token in its credentials. In this case, unwrapAuthenticationToken
           // will return back the original token (which we know is insufficient)
           if (unwrappedToken != token) {
             log.info("Creating Accumulo Connector with unwrapped delegation token");

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/AccumuloPredicateHandler.java
Patch:
@@ -242,7 +242,7 @@ public List<Range> getRanges(Configuration conf, ColumnMapper columnMapper)
   }
 
   /**
-   * Encapsulates the traversal over some {@link ExprNodeDesc} tree for the generation of Accumuluo.
+   * Encapsulates the traversal over some {@link ExprNodeDesc} tree for the generation of Accumulo.
    * Ranges using expressions involving the Accumulo rowid-mapped Hive column.
    *
    * @param conf

File: common/src/test/org/apache/hadoop/hive/common/metrics/TestLegacyMetrics.java
Patch:
@@ -82,10 +82,10 @@ public void testMetricsMBean() throws Exception {
     mbs.setAttribute(oname, attr);
 
     mBeanInfo = mbs.getMBeanInfo(oname);
-    MBeanAttributeInfo[] attrinuteInfos = mBeanInfo.getAttributes();
-    assertEquals(1, attrinuteInfos.length);
+    MBeanAttributeInfo[] attributeInfos = mBeanInfo.getAttributes();
+    assertEquals(1, attributeInfos.length);
     boolean attrFound = false;
-    for (MBeanAttributeInfo info : attrinuteInfos) {
+    for (MBeanAttributeInfo info : attributeInfos) {
       if ("fooMetric".equals(info.getName())) {
         assertEquals("java.lang.Long", info.getType());
         assertTrue(info.isReadable());

File: hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/repl/ReplicationTask.java
Patch:
@@ -64,7 +64,7 @@ private static Factory getFactoryInstance(HCatClient client) {
    * a) If a factory has already been instantiated, and is valid, use it.
    * b) If a factoryClassName has been provided, through .resetFactory(), attempt to instantiate that.
    * c) If a hive.repl.task.factory has been set in the default hive conf, use that.
-   * d) If none of the above methods work, instantiate an anoymous factory that will return an error
+   * d) If none of the above methods work, instantiate an anonymous factory that will return an error
    *    whenever called, till a user calls resetFactory.
    */
   private synchronized static void createFactoryInstance(HCatClient client) {

File: itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/JdbcWithMiniKdcSQLAuthTest.java
Patch:
@@ -90,7 +90,7 @@ public void testAuthorization1() throws Exception {
 
     String tableName1 = "test_jdbc_sql_auth1";
     String tableName2 = "test_jdbc_sql_auth2";
-    // using different code blocks so that jdbc variables are not accidently re-used
+    // using different code blocks so that jdbc variables are not accidentally re-used
     // between the actions. Different connection/statement object should be used for each action.
     {
       // create tables as user1

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationScenariosAcidTables.java
Patch:
@@ -347,7 +347,7 @@ List<Long> openTxns(int numTxns, TxnStore txnHandler, HiveConf primaryConf) thro
     return txns;
   }
 
-  List<Long> allocateWriteIdsForTablesAndAquireLocks(String primaryDbName, Map<String, Long> tables,
+  List<Long> allocateWriteIdsForTablesAndAcquireLocks(String primaryDbName, Map<String, Long> tables,
                                                      TxnStore txnHandler,
                                                      List<Long> txns, HiveConf primaryConf) throws Throwable {
     AllocateTableWriteIdsRequest rqst = new AllocateTableWriteIdsRequest();

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTablesBootstrap.java
Patch:
@@ -200,7 +200,7 @@ public void testAcidTablesBootstrapDuringIncrementalWithOpenTxnsTimeout() throws
     Map<String, Long> tables = new HashMap<>();
     tables.put("t1", numTxns+2L);
     tables.put("t2", numTxns+6L);
-    List<Long> lockIds = allocateWriteIdsForTablesAndAquireLocks(primaryDbName, tables, txnHandler, txns, primaryConf);
+    List<Long> lockIds = allocateWriteIdsForTablesAndAcquireLocks(primaryDbName, tables, txnHandler, txns, primaryConf);
 
     // Bootstrap dump with open txn timeout as 1s.
     List<String> withConfigs = new LinkedList<>(dumpWithAcidBootstrapClause);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestTableLevelReplicationScenarios.java
Patch:
@@ -383,14 +383,14 @@ public void testIncorrectTablePolicyInReplDump() throws Throwable {
     String[] originalTables = new String[] {"t1", "t11", "t2", "t3", "t111"};
     createTables(originalTables, CreateTableType.NON_ACID);
 
-    // Invalid repl policy where abrubtly placed DOT which causes ParseException during REPL dump.
+    // Invalid repl policy where abruptly placed DOT which causes ParseException during REPL dump.
     String[] replicatedTables = new String[] {};
     boolean failed;
     String[] invalidReplPolicies = new String[] {
         primaryDbName + ".t1.t2", // Didn't enclose table pattern within single quotes.
         primaryDbName + ".'t1'.t2", // Table name and include list not allowed.
         primaryDbName + ".t1.'t2'", // Table name and exclude list not allowed.
-        primaryDbName + ".'t1+'.", // Abrubtly ended dot.
+        primaryDbName + ".'t1+'.", // Abruptly ended dot.
         primaryDbName +  ".['t1+'].['t11']", // With square brackets
         primaryDbName + "..''", // Two dots with empty list
         primaryDbName + ".'t1'.'tt2'.'t3'" // More than two list

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestMultiAuthorizationPreEventListener.java
Patch:
@@ -51,7 +51,7 @@ public static void setUp() throws Exception {
     System.setProperty(HiveConf.ConfVars.METASTORE_PRE_EVENT_LISTENERS.varname,
         AuthorizationPreEventListener.class.getName());
 
-    // Set two dummy classes as authorizatin managers. Two instances should get created.
+    // Set two dummy classes as authorization managers. Two instances should get created.
     System.setProperty(HiveConf.ConfVars.HIVE_METASTORE_AUTHORIZATION_MANAGER.varname,
         DummyHiveMetastoreAuthorizationProvider.class.getName() + ","
             + DummyHiveMetastoreAuthorizationProvider.class.getName());

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/authorization/TestJdbcWithSQLAuthorization.java
Patch:
@@ -69,7 +69,7 @@ public void testAuthorization1() throws Exception {
 
     String tableName1 = "test_jdbc_sql_auth1";
     String tableName2 = "test_jdbc_sql_auth2";
-    // using different code blocks so that jdbc variables are not accidently re-used
+    // using different code blocks so that jdbc variables are not accidentally re-used
     // between the actions. Different connection/statement object should be used for each action.
     {
       // create tables as user1
@@ -133,7 +133,7 @@ private Connection getConnection(String userName) throws Exception {
   @Test
   public void testAllowedCommands() throws Exception {
 
-    // using different code blocks so that jdbc variables are not accidently re-used
+    // using different code blocks so that jdbc variables are not accidentally re-used
     // between the actions. Different connection/statement object should be used for each action.
     {
       // create tables as user1
@@ -160,7 +160,7 @@ public void testAllowedCommands() throws Exception {
 
   @Test
   public void testAuthZFailureLlapCachePurge() throws Exception {
-    // using different code blocks so that jdbc variables are not accidently re-used
+    // using different code blocks so that jdbc variables are not accidentally re-used
     // between the actions. Different connection/statement object should be used for each action.
     {
       Connection hs2Conn = getConnection("user1");

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/StartMiniHS2Cluster.java
Patch:
@@ -78,7 +78,7 @@ public void testRunCluster() throws Exception {
     miniHS2.start(confOverlay);
     miniHS2.getDFS().getFileSystem().mkdirs(new Path("/apps_staging_dir/anonymous"));
 
-    System.out.println("JDBC URL avaailable at " + miniHS2.getJdbcURL());
+    System.out.println("JDBC URL available at " + miniHS2.getJdbcURL());
 
     // MiniHS2 cluster is up .. let it run until someone kills the test
     while (true) {

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/AbstractCoreBlobstoreCliDriver.java
Patch:
@@ -96,7 +96,7 @@ public void tearDown() throws Exception {
   public void shutdown() throws Exception {
     qt.shutdown();
     if (System.getenv(QTestUtil.QTEST_LEAVE_FILES) == null) {
-      qt.executeAdhocCommand("dfs -rmdir " + testBlobstorePathUnique);
+      qt.executeAdHocCommand("dfs -rmdir " + testBlobstorePathUnique);
     }
   }
 

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -680,7 +680,7 @@ private void closeSession(SessionState oldSs) throws IOException {
     }
   }
 
-  public int executeAdhocCommand(String q) throws CommandProcessorException {
+  public int executeAdHocCommand(String q) throws CommandProcessorException {
     if (!q.contains(";")) {
       return -1;
     }

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java
Patch:
@@ -34,7 +34,7 @@
 /*
  * This hook is used for verifying the table access key information
  * that is generated and maintained in the QueryPlan object by the
- * TableAccessAnalyer. All the hook does is print out the table/keys
+ * TableAccessAnalyzer. All the hook does is print out the table/keys
  * per operator recorded in the TableAccessInfo in the QueryPlan.
  */
 public class CheckTableAccessHook implements ExecuteWithHookContext {

File: itests/util/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java
Patch:
@@ -533,7 +533,7 @@ public String getJdbcURL(String dbName) throws Exception {
   /**
    * return connection URL for this server instance
    * @param dbName - DB name to be included in the URL
-   * @param sessionConfExt - Addional string to be appended to sessionConf part of url
+   * @param sessionConfExt - Additional string to be appended to sessionConf part of url
    * @return
    * @throws Exception
    */
@@ -544,7 +544,7 @@ public String getJdbcURL(String dbName, String sessionConfExt) throws Exception
   /**
    * return connection URL for this server instance
    * @param dbName - DB name to be included in the URL
-   * @param sessionConfExt - Addional string to be appended to sessionConf part of url
+   * @param sessionConfExt - Additional string to be appended to sessionConf part of url
    * @param hiveConfExt - Additional string to be appended to HiveConf part of url (excluding the ?)
    * @return
    * @throws Exception

File: llap-common/src/test/org/apache/hadoop/hive/llap/metrics/TestReadWriteLockMetrics.java
Patch:
@@ -384,7 +384,7 @@ public void testWithoutContention() throws Exception {
                rec.getMetrics().get(ReadLockWaitTimeMax).longValue()
                                     < lhR.getLockMax());
 
-    assertTrue("Max greater or equal to avergae lock time",
+    assertTrue("Max greater or equal to average lock time",
                (rec.getMetrics().get(ReadLockWaitTimeTotal).longValue()
                 / rec.getMetrics().get(ReadLockCount).longValue())
                   <= rec.getMetrics().get(ReadLockWaitTimeMax).longValue());

File: llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/DirWatcher.java
Patch:
@@ -157,7 +157,7 @@ private void registerDir(Path path, WatchedPathInfo watchedPathInfo) {
 
   private void trackWatchForAttempt(WatchedPathInfo watchedPathInfo, WatchKey watchKey) {
     assert watchedPathInfo.pathIdentifier != null;
-    // TODO May be possible to do finer grained locks.
+    // TODO May be possible to do finer-grained locks.
     synchronized (watchesPerAttempt) {
       List<WatchKey> list = watchesPerAttempt.get(watchedPathInfo.pathIdentifier);
       if (list == null) {
@@ -169,7 +169,7 @@ private void trackWatchForAttempt(WatchedPathInfo watchedPathInfo, WatchKey watc
   }
 
   private void cancelWatchesForAttempt(AttemptPathIdentifier pathIdentifier) {
-    // TODO May be possible to do finer grained locks.
+    // TODO May be possible to do finer-grained locks.
     synchronized(watchesPerAttempt) {
       List<WatchKey> list = watchesPerAttempt.remove(pathIdentifier);
       if (list != null) {

File: llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java
Patch:
@@ -91,7 +91,7 @@ public long transferTo(WritableByteChannel target, long position)
   /**
    * Since Netty4, deallocate() is called automatically during cleanup, but before the
    * ChannelFutureListeners. Deallocate calls FileChannel.close() and makes the file descriptor
-   * invalid, so every OS cache operation (e.g. posix_fadvice) with the original file descriptor
+   * invalid, so every OS cache operation (e.g. posix_fadvise) with the original file descriptor
    * will fail after this operation, so we need to take care of cleanup operations here (before
    * deallocating) instead of listeners outside.
    */

File: llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java
Patch:
@@ -430,7 +430,7 @@ public static ShuffleHandler get() {
 
   /**
    * Serialize the shuffle port into a ByteBuffer for use later on.
-   * @param port the port to be sent to the ApplciationMaster
+   * @param port the port to be sent to the ApplicationMaster
    * @return the serialized form of the port.
    */
   public static ByteBuffer serializeMetaData(int port) throws IOException {

File: llap-server/src/test/org/apache/hadoop/hive/llap/daemon/impl/comparator/TestFirstInFirstOutComparator.java
Patch:
@@ -102,7 +102,7 @@ public void testWaitQueueComparator() throws InterruptedException, IOException {
     assertEquals(r3, queue.peek());
     assertNull(queue.offer(r4, 0));
     assertEquals(r4, queue.peek());
-    // this offer will be accpeted and r1 evicted
+    // this offer will be accepted and r1 evicted
     assertEquals(r1, queue.offer(r5, 0));
     assertEquals(r5, queue.take());
     assertEquals(r4, queue.take());

File: llap-server/src/test/org/apache/hadoop/hive/llap/shufflehandler/TestShuffleHandler.java
Patch:
@@ -58,7 +58,7 @@ void setAddress(SocketAddress lastAddress) {
       this.lastAddress = lastAddress;
     }
 
-    SocketAddress getSocketAddres() {
+    SocketAddress getSocketAddress() {
       return lastAddress;
     }
   }
@@ -192,7 +192,7 @@ protected void sendError(ChannelHandlerContext ctx, String message,
     byte[] buffer = new byte[1024];
     while (input.read(buffer) != -1) {
     }
-    SocketAddress firstAddress = lastSocketAddress.getSocketAddres();
+    SocketAddress firstAddress = lastSocketAddress.getSocketAddress();
     input.close();
 
     // For keepAlive via URL
@@ -211,7 +211,7 @@ protected void sendError(ChannelHandlerContext ctx, String message,
     header = new ShuffleHeader();
     header.readFields(input);
     input.close();
-    SocketAddress secondAddress = lastSocketAddress.getSocketAddres();
+    SocketAddress secondAddress = lastSocketAddress.getSocketAddress();
     Assert.assertNotNull("Initial shuffle address should not be null", firstAddress);
     Assert.assertNotNull("Keep-Alive shuffle address should not be null", secondAddress);
     Assert.assertEquals(

File: llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
Patch:
@@ -375,7 +375,7 @@ public interface OperationCallback<ResultType, CtxType> {
   /**
    * @param node
    * @param callback
-   * @return if it was possible to attemp the registration. Sometimes it's
+   * @return if it was possible to attempt the registration. Sometimes it's
    * not possible because is a dag is not running
    */
   public boolean registerDag(NodeInfo node, final OperationCallback<QueryIdentifierProto, Void> callback) {

File: llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
Patch:
@@ -2764,7 +2764,7 @@ boolean hadCommFailure() {
       return hadCommFailure;
     }
 
-    boolean _canAccepInternal() {
+    boolean _canAcceptInternal() {
       return !hadCommFailure && !disabled
           &&(numSchedulableTasks == -1 || ((numSchedulableTasks - numScheduledTasks) > 0));
     }
@@ -2773,7 +2773,7 @@ boolean _canAccepInternal() {
     may be running in the system. Also depends upon the capacity usage configuration
      */
     boolean canAcceptTask() {
-      boolean result = _canAccepInternal();
+      boolean result = _canAcceptInternal();
       if (LOG.isTraceEnabled()) {
         LOG.trace(constructCanAcceptLogResult(result));
       }
@@ -2828,7 +2828,7 @@ public String toString() {
 
     private String toShortString() {
       StringBuilder sb = new StringBuilder();
-      sb.append(", canAcceptTask=").append(_canAccepInternal());
+      sb.append(", canAcceptTask=").append(_canAcceptInternal());
       sb.append(", st=").append(numScheduledTasks);
       sb.append(", ac=").append((numSchedulableTasks - numScheduledTasks));
       sb.append(", commF=").append(hadCommFailure);

File: ql/src/java/org/apache/hadoop/hive/llap/WritableByteChannelAdapter.java
Patch:
@@ -54,7 +54,7 @@ public class WritableByteChannelAdapter implements WritableByteChannel {
   private ChannelFutureListener writeListener = new ChannelFutureListener() {
     @Override
     public void operationComplete(ChannelFuture future) {
-      //Asynch write completed
+      //Async write completed
       //Up the semaphore
       writeResources.release();
 

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AlterTableType.java
Patch:
@@ -60,8 +60,8 @@ public enum AlterTableType {
   TOUCH("touch"),
   RENAME("rename"),
   OWNER("set owner"),
-  ARCHIVE("archieve"),
-  UNARCHIVE("unarchieve"),
+  ARCHIVE("archive"),
+  UNARCHIVE("unarchive"),
   COMPACT("compact"),
   TRUNCATE("truncate"),
   MERGEFILES("merge files"),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
Patch:
@@ -122,7 +122,7 @@ public class ReduceSinkOperator extends TerminalOperator<ReduceSinkDesc>
    * If there is no distinct expression, cachedKeys is simply like this.
    * cachedKeys[0] = [col0][col1]
    *
-   * with two distict expression, union(tag:key) is attatched for each distinct expression
+   * with two distict expression, union(tag:key) is attached for each distinct expression
    * cachedKeys[0] = [col0][col1][0:dist1]
    * cachedKeys[1] = [col0][col1][1:dist2]
    *

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java
Patch:
@@ -356,8 +356,8 @@ private int insertKeyIntoHeap(HiveKey key) throws IOException, HiveException {
     hashes[index] = key.hashCode();
     if (null != indexes.store(index)) {
       // it's only for GBY which should forward all values associated with the key in the range
-      // of limit. new value should be attatched with the key but in current implementation,
-      // only one values is allowed. with map-aggreagtion which is true by default,
+      // of limit. new value should be attached with the key but in current implementation,
+      // only one values is allowed. with map-aggregation which is true by default,
       // this is not common case, so just forward new key/value and forget that (todo)
       return FORWARD;
     }
@@ -408,7 +408,7 @@ private interface IndexStore {
 
   /**
    * for order by, same keys are counted (For 1-2-2-3-4, limit 3 is 1-2-2)
-   * MinMaxPriorityQueue is used because it alows duplication and fast access to biggest one
+   * MinMaxPriorityQueue is used because it allows duplication and fast access to biggest one
    */
   private class HashForRow implements IndexStore {
     private final MinMaxPriorityQueue<Integer> indexes = MinMaxPriorityQueue.orderedBy(C).create();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
Patch:
@@ -489,7 +489,7 @@ public static String isEligibleForLocalMode(HiveConf conf,
 
     // ideally we would like to do this check based on the number of splits
     // in the absence of an easy way to get the number of splits - do this
-    // based on the total number of files (pessimistically assumming that
+    // based on the total number of files (pessimistically assuming that
     // splits are equal to number of files in worst case)
     if (inputFileCount > maxInputFiles) {
       return "Number of Input Files (= " + inputFileCount +

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableConf.java
Patch:
@@ -27,7 +27,7 @@
  * among them, which is used in n-way join (multiple small tables are involved).
  */
 public class HybridHashTableConf {
-  private List<HybridHashTableContainer> loadedContainerList; // A list of alrady loaded containers
+  private List<HybridHashTableContainer> loadedContainerList; // A list of already loaded containers
   private int numberOfPartitions = 0; // Number of partitions each table should have
   private int nextSpillPartition = -1;       // The partition to be spilled next
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java
Patch:
@@ -577,7 +577,7 @@ else if (filename.startsWith(BUCKET_PREFIX)) {
   }
 
   /**
-   * If the direct insert is on for ACID tables, the files will contain an "_attempID" postfix.
+   * If the direct insert is on for ACID tables, the files will contain an "_attemptID" postfix.
    * In order to be able to read the files from the delete deltas, we need to know which
    * attemptId belongs to which delta. To make this lookup easy, this method created a map
    * to link the deltas to the attemptId.

File: ql/src/java/org/apache/hadoop/hive/ql/io/FlatFileInputFormat.java
Patch:
@@ -106,7 +106,7 @@ public static class SerializationContextFromConf<S> implements
 
     /**
      * Implements configurable so it can use the configuration to find the right
-     * classes Note: ReflectionUtils will automatigically call setConf with the
+     * classes Note: ReflectionUtils will automagically call setConf with the
      * right configuration.
      */
     private Configuration conf;

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
Patch:
@@ -151,7 +151,7 @@ private List<String> getObjectNames(HiveLockObject key) {
    * @param  lockObjects  List of objects and the modes of the locks requested
    * @param  keepAlive    Whether the lock is to be persisted after the statement
    *
-   * Acuire all the locks. Release all the locks and return null if any lock
+   * Acquire all the locks. Release all the locks and return null if any lock
    * could not be acquired.
    **/
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java
Patch:
@@ -352,7 +352,7 @@ private boolean getColumnInfo(DynamicListContext ctx, StringBuilder internalColN
     }
     internalColName.append(colExpr.getColumn());
 
-    // fetch table ablias
+    // fetch table alias
     ExprNodeDescUtils.ColumnOrigin columnOrigin =
             ExprNodeDescUtils.findColumnOrigin(exprNodeDesc, ctx.generator);
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -1959,7 +1959,7 @@ private static boolean isMergeRequiredForMr(HiveConf hconf,
       // If the user has HIVEMERGEMAPREDFILES set to false, the idea was the
       // number of reducers are few, so the number of files anyway are small.
       // However, with this optimization, we are increasing the number of files
-      // possibly by a big margin. So, merge aggresively.
+      // possibly by a big margin. So, merge aggressively.
       return (hconf.getBoolVar(ConfVars.HIVEMERGEMAPFILES) ||
           hconf.getBoolVar(ConfVars.HIVEMERGEMAPREDFILES));
     }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
Patch:
@@ -194,7 +194,7 @@ private static void genMapJoinLocalWork(MapredWork newWork, MapJoinOperator mapJ
       // set alias to fetch work
       newLocalWork.getAliasToFetchWork().put(alias, fetchWork);
     }
-    // remove small table ailias from aliasToWork;Avoid concurrent modification
+    // remove small table alias from aliasToWork;Avoid concurrent modification
     for (String alias : smallTableAliasList) {
       newWork.getMapWork().getAliasToWork().remove(alias);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java
Patch:
@@ -30,7 +30,7 @@ public class CalciteSemanticException extends SemanticException {
   private static final long serialVersionUID = 1L;
 
   public enum UnsupportedFeature {
-    Distinct_without_an_aggreggation, Duplicates_in_RR, Filter_expression_with_non_boolean_return_type,
+    Distinct_without_an_aggregation, Duplicates_in_RR, Filter_expression_with_non_boolean_return_type,
     Having_clause_without_any_groupby, Invalid_column_reference, Invalid_decimal,
     Less_than_equal_greater_than, Others, Same_name_in_multiple_expressions,
     Schema_less_table, Select_alias_in_having_clause, Select_transform, Subquery,

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveRelColumnsAlignment.java
Patch:
@@ -82,7 +82,7 @@ public HiveRelColumnsAlignment(RelBuilder relBuilder) {
 
   /**
    * Execute the logic in this class. In particular, make a top-down traversal of the tree
-   * and annotate and recreate appropiate operators.
+   * and annotate and recreate appropriate operators.
    */
   public RelNode align(RelNode root) {
     final RelNode newRoot = dispatchAlign(root, ImmutableList.<RelFieldCollation>of());

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSubQueryRemoveRule.java
Patch:
@@ -147,7 +147,7 @@ private HiveSubQueryRemoveRule(RelOptRuleOperand operand, String description, Hi
     }
   }
 
-  // given a subquery it checks to see what is the aggegate function
+  // given a subquery it checks to see what is the aggregate function
   /// if COUNT returns true since COUNT produces 0 on empty result set
   private boolean isAggZeroOnEmpty(RexSubQuery e) {
     //as this is corr scalar subquery with agg we expect one aggregate

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTConverter.java
Patch:
@@ -956,7 +956,7 @@ static class Schema extends ArrayList<ColumnInfo> {
      * Assumption:<br>
      * 1. Project will always be child of Sort.<br>
      * 2. In Calcite every projection in Project is uniquely named
-     * (unambigous) without using table qualifier (table name).<br>
+     * (unambiguous) without using table qualifier (table name).<br>
      *
      * @param order
      *          Hive Sort Node

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
Patch:
@@ -244,7 +244,7 @@ private RexNode convert(ExprNodeGenericFuncDesc func) throws SemanticException {
       if (calciteOp.getKind() == SqlKind.CASE) {
         // If it is a case operator, we need to rewrite it
         childRexNodeLst = rewriteCaseChildren(func.getFuncText(), childRexNodeLst, rexBuilder);
-        // Adjust branch types by inserting explicit casts if the actual is ambigous
+        // Adjust branch types by inserting explicit casts if the actual is ambiguous
         childRexNodeLst = adjustCaseBranchTypes(childRexNodeLst, retType, rexBuilder);
       } else if (HiveExtractDate.ALL_FUNCTIONS.contains(calciteOp)) {
         // If it is a extract operator, we need to rewrite it
@@ -273,7 +273,7 @@ private RexNode convert(ExprNodeGenericFuncDesc func) throws SemanticException {
         // This allows to be further reduced to OR, if possible
         calciteOp = SqlStdOperatorTable.CASE;
         childRexNodeLst = rewriteCoalesceChildren(childRexNodeLst, rexBuilder);
-        // Adjust branch types by inserting explicit casts if the actual is ambigous
+        // Adjust branch types by inserting explicit casts if the actual is ambiguous
         childRexNodeLst = adjustCaseBranchTypes(childRexNodeLst, retType, rexBuilder);
       } else if (calciteOp == HiveToDateSqlOperator.INSTANCE) {
         childRexNodeLst = rewriteToDateChildren(childRexNodeLst, rexBuilder);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/BucketingSortingInferenceOptimizer.java
Patch:
@@ -51,7 +51,7 @@
  *
  * BucketingSortingInferenceOptimizer.
  *
- * For each map reduce task, attmepts to infer bucketing and sorting metadata for the outputs.
+ * For each map reduce task, attempts to infer bucketing and sorting metadata for the outputs.
  *
  * Currently only map reduce tasks which produce final output have there output metadata inferred,
  * but it can be extended to intermediate tasks as well.

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java
Patch:
@@ -99,7 +99,7 @@ private GenMRSkewJoinProcessor() {
    * </ul>
    * For each table, we launch one mapjoin job, taking the directory containing
    * big keys in this table and corresponding dirs in other tables as input.
-   * (Actally one job for one row in the above.)
+   * (Actually one job for one row in the above.)
    *
    * <p>
    * For more discussions, please check

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -672,7 +672,7 @@ Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticExcept
           }
           this.ctx.setCboInfo(cboMsg);
 
-          // Determine if we should re-throw the exception OR if we try to mark plan as reAnayzeAST to retry
+          // Determine if we should re-throw the exception OR if we try to mark plan as reAnalyzeAST to retry
           // planning as non-CBO.
           if (fallbackStrategy.isFatal(e)) {
             if (e instanceof RuntimeException || e instanceof SemanticException) {
@@ -3643,7 +3643,7 @@ private RelNode genGBRelNode(List<RexNode> gbExprs, List<AggregateInfo> aggInfoL
       }
 
       if (gbChildProjLst.isEmpty()) {
-        // This will happen for count(*), in such cases we arbitarily pick
+        // This will happen for count(*), in such cases we arbitrarily pick
         // first element from srcRel
         gbChildProjLst.add(this.cluster.getRexBuilder().makeInputRef(srcRel, 0));
       }
@@ -4738,7 +4738,7 @@ && isRegex(
               (srcRel.getInputs().size() == 1 && srcRel.getInput(0) instanceof HiveAggregate))) {
             // Likely a malformed query eg, select hash(distinct c1) from t1;
             throw new CalciteSemanticException("Distinct without an aggregation.",
-                    UnsupportedFeature.Distinct_without_an_aggreggation);
+                    UnsupportedFeature.Distinct_without_an_aggregation);
           } else {
             // Case when this is an expression
             TypeCheckCtx tcCtx = new TypeCheckCtx(inputRR, cluster.getRexBuilder());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
Patch:
@@ -505,7 +505,7 @@ private static Task<?> loadTable(URI fromURI, Table table, boolean replace, Path
     }
 
     //if Importing into existing table, FileFormat is checked by
-    // ImportSemanticAnalzyer.checked checkTable()
+    // ImportSemanticAnalyzer.checked checkTable()
     Task<?> loadTableTask = TaskFactory.get(moveWork, x.getConf());
     copyTask.addDependentTask(loadTableTask);
     x.getTasks().add(copyTask);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java
Patch:
@@ -554,7 +554,7 @@ void subqueryRestrictionsCheck(RowResolver parentQueryRR,
           subQueryAST, "SubQuery can contain only 1 item in Select List."));
     }
 
-    boolean hasAggreateExprs = false;
+    boolean hasAggregateExprs = false;
     boolean hasWindowing = false;
 
     // we need to know if aggregate is COUNT since IN corr subq with count aggregate
@@ -566,7 +566,7 @@ void subqueryRestrictionsCheck(RowResolver parentQueryRR,
       int r = SubQueryUtils.checkAggOrWindowing(selectItem);
 
       hasWindowing = hasWindowing | ( r == 3);
-      hasAggreateExprs = hasAggreateExprs | ( r == 1 | r== 2 );
+      hasAggregateExprs = hasAggregateExprs | ( r == 1 | r== 2 );
       hasCount = hasCount | ( r == 2 );
     }
 
@@ -624,7 +624,7 @@ void subqueryRestrictionsCheck(RowResolver parentQueryRR,
       // * IN - always allowed, BUT returns true for cases with aggregate other than COUNT since later in subquery remove
       //        rule we need to know about this case.
       // * NOT IN - always allow, but always return true because later subq remove rule will generate diff plan for this case
-      if (hasAggreateExprs &&
+      if (hasAggregateExprs &&
               !hasExplicitGby) {
 
         if(operator.getType() == SubQueryType.SCALAR) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/RowResolver.java
Patch:
@@ -518,8 +518,8 @@ private boolean isAmbiguousReference(String tableAlias, String colAlias) {
         return true;
       }
     } else {
-      for (Map.Entry<String, Map<String, String>> ambigousColsEntry: ambiguousColumns.entrySet()) {
-        Map<String, String> cmap = ambigousColsEntry.getValue();
+      for (Map.Entry<String, Map<String, String>> ambiguousColsEntry: ambiguousColumns.entrySet()) {
+        Map<String, String> cmap = ambiguousColsEntry.getValue();
         for (Map.Entry<String, String> cmapEnt : cmap.entrySet()) {
           if (colAlias.equalsIgnoreCase(cmapEnt.getKey())) {
             return true;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -15181,7 +15181,7 @@ private List<Table> getNonTransactionalTables() {
   /**
    * Check the query results cache to see if the query represented by the lookupInfo can be
    * answered using the results cache. If the cache contains a suitable entry, the semantic analyzer
-   * will be configured to use the found cache entry to anwer the query.
+   * will be configured to use the found cache entry to answer the query.
    */
   private boolean checkResultsCache(QueryResultsCache.LookupInfo lookupInfo, boolean needsReset) {
     if (lookupInfo == null) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/type/HiveFunctionHelper.java
Patch:
@@ -253,7 +253,7 @@ public RexNode getExpression(String functionText, FunctionInfo fi,
       if (calciteOp.getKind() == SqlKind.CASE) {
         // If it is a case operator, we need to rewrite it
         inputs = RexNodeConverter.rewriteCaseChildren(functionText, inputs, rexBuilder);
-        // Adjust branch types by inserting explicit casts if the actual is ambigous
+        // Adjust branch types by inserting explicit casts if the actual is ambiguous
         inputs = RexNodeConverter.adjustCaseBranchTypes(inputs, returnType, rexBuilder);
         checkForStatefulFunctions(inputs);
       } else if (HiveExtractDate.ALL_FUNCTIONS.contains(calciteOp)) {
@@ -292,7 +292,7 @@ public RexNode getExpression(String functionText, FunctionInfo fi,
         // This allows to be further reduced to OR, if possible
         calciteOp = SqlStdOperatorTable.CASE;
         inputs = RexNodeConverter.rewriteCoalesceChildren(inputs, rexBuilder);
-        // Adjust branch types by inserting explicit casts if the actual is ambigous
+        // Adjust branch types by inserting explicit casts if the actual is ambiguous
         inputs = RexNodeConverter.adjustCaseBranchTypes(inputs, returnType, rexBuilder);
         checkForStatefulFunctions(inputs);
       } else if (calciteOp == HiveToDateSqlOperator.INSTANCE) {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractOperatorDesc.java
Patch:
@@ -126,8 +126,8 @@ public long getMaxMemoryAvailable() {
   }
 
   @Override
-  public void setMaxMemoryAvailable(final long memoryAvailble) {
-    this.memAvailable = memoryAvailble;
+  public void setMaxMemoryAvailable(final long memoryAvailable) {
+    this.memAvailable = memoryAvailable;
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/plan/OperatorDesc.java
Patch:
@@ -32,7 +32,7 @@ public interface OperatorDesc extends Serializable, Cloneable {
   public long getMemoryNeeded();
   public void setMemoryNeeded(long memoryNeeded);
   public long getMaxMemoryAvailable();
-  public void setMaxMemoryAvailable(long memoryAvailble);
+  public void setMaxMemoryAvailable(long memoryAvailable);
   public String getRuntimeStatsTmpDir();
   public void setRuntimeStatsTmpDir(String runtimeStatsTmpDir);
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorPTFInfo.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 
 /**
- * VectorGroupByAggregrationInfo.
+ * VectorGroupByAggregationInfo.
  *
  * A convenience data structure that has information needed to vectorize reduce sink.
  *

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkInfo.java
Patch:
@@ -23,12 +23,12 @@
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 
 /**
- * VectorGroupByAggregrationInfo.
+ * VectorGroupByAggregationInfo.
  *
  * A convenience data structure that has information needed to vectorize reduce sink.
  *
  * It is created by the Vectorizer when it is determining whether it can specialize so the
- * information doesn't have to be recreated again and agains by the VectorReduceSinkOperator's
+ * information doesn't have to be recreated again and against by the VectorReduceSinkOperator's
  * constructors and later during execution.
  */
 public class VectorReduceSinkInfo {

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/HiveMetastoreAuthorizationProvider.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePolicyProvider;
 
 /**
- * HiveMetastoreAuthorizationProvider : An extension of HiveAuthorizaytionProvider
+ * HiveMetastoreAuthorizationProvider : An extension of HiveAuthorizationProvider
  * that is intended to be called from the metastore-side. It will be invoked
  * by AuthorizationPreEventListener.
  *

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java
Patch:
@@ -166,7 +166,7 @@ public static StatsUpdater init(CompactionInfo ci, List<String> columnListForSta
     private StatsUpdater(CompactionInfo ci, List<String> columnListForStats,
         HiveConf conf, String userName, String compactionQueueName) {
       this.conf = new HiveConf(conf);
-      //so that Driver doesn't think it's arleady in a transaction
+      //so that Driver doesn't think it's already in a transaction
       this.conf.unset(ValidTxnList.VALID_TXNS_KEY);
       this.userName = userName;
       this.ci = ci;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAssertTrueOOM.java
Patch:
@@ -66,7 +66,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen
   public Object evaluate(DeferredObject[] arguments) throws HiveException {
     BooleanWritable condition = (BooleanWritable) conditionConverter.convert(arguments[0].get());
     if (condition == null || !condition.get()) {
-      throw new MapJoinMemoryExhaustionError("assert_true_oom: assertation failed; Simulated OOM");
+      throw new MapJoinMemoryExhaustionError("assert_true_oom: assertion failed; Simulated OOM");
     }
     return null;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java
Patch:
@@ -172,7 +172,7 @@ public void merge(List other, DoubleObjectInspector doi) {
    */
   public void add(double v) {
     // Binary search to find the closest bucket that v should go into.
-    // 'bin' should be interpreted as the bin to shift right in order to accomodate
+    // 'bin' should be interpreted as the bin to shift right in order to accommodate
     // v. As a result, bin is in the range [0,N], where N means that the value v is
     // greater than all the N bins currently in the histogram. It is also possible that
     // a bucket centered at 'v' already exists, so this must be checked in the next step.

File: ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java
Patch:
@@ -1563,7 +1563,7 @@ public void run() {
   }
 
   /**
-   * This cannnot be run against Derby (thus in UT) but it can run againt MySQL.
+   * This cannnot be run against Derby (thus in UT) but it can run against MySQL.
    * 1. add to metastore/pom.xml
    *     <dependency>
    *      <groupId>mysql</groupId>

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CheckFastHashTable.java
Patch:
@@ -764,7 +764,7 @@ public long addRandomExisting(byte[] value, Random r) {
       Preconditions.checkState(count > 0);
       int index = r.nextInt(count);
 
-      // Exists aleady.
+      // Exists already.
 
       return array[index].getKey();
     }

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/DecimalTypeInfo.java
Patch:
@@ -107,7 +107,7 @@ public boolean accept(TypeInfo other) {
     }
 
     DecimalTypeInfo dti = (DecimalTypeInfo)other;
-    // Make sure "this" has enough integer room to accomodate other's integer digits.
+    // Make sure "this" has enough integer room to accommodate other's integer digits.
     return this.precision() - this.scale() >= dti.precision() - dti.scale();
   }
 

File: serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java
Patch:
@@ -416,7 +416,7 @@ private void testBinarySortableFastCase(
     Arrays.fill(columnNotNullMarker, BinarySortableSerDe.ONE);
 
     /*
-     * Acending.
+     * Ascending.
      */
     testBinarySortableFast(source, rows,
         columnSortOrderIsDesc, columnNullMarker, columnNotNullMarker,

File: serde/src/test/org/apache/hadoop/hive/serde2/io/TestTimestampWritableV2.java
Patch:
@@ -427,7 +427,7 @@ public void testMaxSize() {
     assertTrue((((double) MAX_ADDITIONAL_SECONDS_BITS + 1) * (1L << 31)) * 1000 >
       Long.MAX_VALUE);
 
-    // This is how many bytes we need to store those additonal bits as a VInt.
+    // This is how many bytes we need to store those additional bits as a VInt.
     assertEquals(4, WritableUtils.getVIntSize(MAX_ADDITIONAL_SECONDS_BITS));
 
     // Therefore, the maximum total size of a serialized timestamp is 4 + 5 + 4 = 13.

File: service/src/java/org/apache/hive/service/auth/saml/HiveSaml2Client.java
Patch:
@@ -173,7 +173,7 @@ public void setRedirect(HttpServletRequest request, HttpServletResponse response
    * @param request
    * @param response
    * @return the NameId as received in the assertion if the assertion was valid.
-   * @throws HttpSamlAuthenticationException In case the assertition is not present or is
+   * @throws HttpSamlAuthenticationException In case the assertion is not present or is
    *                                         invalid.
    */
   public String validate(HttpServletRequest request, HttpServletResponse response)

File: service/src/java/org/apache/hive/service/cli/ColumnDescriptor.java
Patch:
@@ -50,7 +50,7 @@ public ColumnDescriptor(TColumnDesc tColumnDesc) {
   public static ColumnDescriptor newPrimitiveColumnDescriptor(String name, String comment,
       Type type, int position) {
     // Current usage looks like it's only for metadata columns, but if that changes then
-    // this method may need to require a type qualifiers aruments.
+    // this method may need to require a type qualifiers arguments.
     return new ColumnDescriptor(name, comment, new TypeDescriptor(type), position);
   }
 

File: service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContext.java
Patch:
@@ -22,7 +22,7 @@
 /**
  * HiveSessionHookContext.
  * Interface passed to the HiveServer2 session hook execution. This enables
- * the hook implementation to accesss session config, user and session handle
+ * the hook implementation to access session config, user and session handle
  */
 public interface HiveSessionHookContext {
 

File: service/src/test/org/apache/hive/service/cli/thrift/ThriftHttpServletTest.java
Patch:
@@ -50,7 +50,7 @@ public void setUp() {
   }
 
   @Test
-  public void testMissingAuthorizatonHeader() throws Exception {
+  public void testMissingAuthorizationHeader() throws Exception {
     HttpServletRequest httpServletRequest = Mockito.mock(HttpServletRequest.class);
     Mockito.when(httpServletRequest.getHeader(HttpAuthUtils.AUTHORIZATION)).thenReturn(null);
 
@@ -61,7 +61,7 @@ public void testMissingAuthorizatonHeader() throws Exception {
   }
 
   @Test
-  public void testEmptyAuthorizatonHeader() throws Exception {
+  public void testEmptyAuthorizationHeader() throws Exception {
     HttpServletRequest httpServletRequest = Mockito.mock(HttpServletRequest.class);
     Mockito.when(httpServletRequest.getHeader(HttpAuthUtils.AUTHORIZATION)).thenReturn("");
 

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java
Patch:
@@ -3451,7 +3451,7 @@ LockResponse checkLock(long lockid)
 
   /**
    * Unlock a set of locks.  This can only be called when the locks are not
-   * assocaited with a transaction.
+   * associated with a transaction.
    * @param lockid lock id returned by
    * {@link #lock(org.apache.hadoop.hive.metastore.api.LockRequest)}
    * @throws NoSuchLockException if the requested lockid does not exist.

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/RawStore.java
Patch:
@@ -1560,7 +1560,7 @@ List<ColStatsObjWithSourceInfo> getPartitionColStatsForDatabase(String catName,
   /**
    * @param fileIds List of file IDs from the filesystem.
    * @param metadata Metadata buffers corresponding to fileIds in the list.
-   * @param type The type; determines the class that can do additiona processing for metadata.
+   * @param type The type; determines the class that can do additional processing for metadata.
    */
   void putFileMetadata(List<Long> fileIds, List<ByteBuffer> metadata,
       FileMetadataExprType type) throws MetaException;

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java
Patch:
@@ -983,7 +983,7 @@ private static void updateTableAggregatePartitionColStats(RawStore rawStore, Str
         List<String> partNames = rawStore.listPartitionNames(catName, dbName, tblName, (short) -1);
         List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);
         if ((partNames != null) && (partNames.size() > 0)) {
-          Deadline.startTimer("getAggregareStatsForAllPartitions");
+          Deadline.startTimer("getAggregateStatsForAllPartitions");
           AggrStats aggrStatsAllPartitions = rawStore.get_aggr_stats_for(catName, dbName, tblName, partNames, colNames, CacheUtils.HIVE_ENGINE);
           Deadline.stopTimer();
           // Remove default partition from partition names and get aggregate stats again
@@ -997,7 +997,7 @@ private static void updateTableAggregatePartitionColStats(RawStore rawStore, Str
           }
           String defaultPartitionName = FileUtils.makePartName(partCols, partVals);
           partNames.remove(defaultPartitionName);
-          Deadline.startTimer("getAggregareStatsForAllPartitionsExceptDefault");
+          Deadline.startTimer("getAggregateStatsForAllPartitionsExceptDefault");
           AggrStats aggrStatsAllButDefaultPartition =
               rawStore.get_aggr_stats_for(catName, dbName, tblName, partNames, colNames, CacheUtils.HIVE_ENGINE);
           Deadline.stopTimer();

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnStore.java
Patch:
@@ -539,7 +539,7 @@ Set<CompactionInfo> findPotentialCompactions(int abortedThreshold, long abortedT
   MutexAPI getMutexAPI();
 
   /**
-   * This is primarily designed to provide coarse grained mutex support to operations running
+   * This is primarily designed to provide coarse-grained mutex support to operations running
    * inside the Metastore (of which there could be several instances).  The initial goal is to
    * ensure that various sub-processes of the Compactor don't step on each other.
    *

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java
Patch:
@@ -264,7 +264,7 @@ public void setValPreallocated(int elementNum, int length) {
    * @param leftLen length of left argument
    * @param rightSourceBuf container of right argument
    * @param rightStart start of right argument
-   * @param rightLen length of right arugment
+   * @param rightLen length of right argument
    */
   public void setConcat(int elementNum, byte[] leftSourceBuf, int leftStart, int leftLen,
       byte[] rightSourceBuf, int rightStart, int rightLen) {

File: storage-api/src/test/org/apache/hadoop/hive/common/TestValidCompactorWriteIdList.java
Patch:
@@ -78,7 +78,7 @@ public void exceptionsInMidst() {
     Assert.assertEquals(ValidWriteIdList.RangeResponse.NONE, rsp);
   }
   @Test
-  public void exceptionsAbveHighWaterMark() {
+  public void exceptionsAboveHighWaterMark() {
     BitSet bitSet = new BitSet(4);
     bitSet.set(0, 4);
     ValidWriteIdList writeIds = new ValidCompactorWriteIdList(tableName, new long[]{8, 11, 17, 29}, bitSet, 15);

File: ql/src/java/org/apache/hadoop/hive/ql/io/arrow/ArrowColumnarBatchSerDe.java
Patch:
@@ -210,9 +210,9 @@ static ListTypeInfo toStructListTypeInfo(MapTypeInfo mapTypeInfo) {
   static ListColumnVector toStructListVector(MapColumnVector mapVector) {
     final StructColumnVector structVector;
     final ListColumnVector structListVector;
-    structVector = new StructColumnVector();
+    structVector = new StructColumnVector(mapVector.childCount);
     structVector.fields = new ColumnVector[] {mapVector.keys, mapVector.values};
-    structListVector = new ListColumnVector();
+    structListVector = new ListColumnVector(mapVector.childCount, null);
     structListVector.child = structVector;
     structListVector.childCount = mapVector.childCount;
     structListVector.isRepeating = mapVector.isRepeating;

File: ql/src/java/org/apache/hadoop/hive/ql/io/arrow/Deserializer.java
Patch:
@@ -391,6 +391,7 @@ private void readPrimitive(FieldVector arrowVector, ColumnVector hiveVector) {
 
   private void readList(FieldVector arrowVector, ListColumnVector hiveVector, ListTypeInfo typeInfo) {
     final int size = arrowVector.getValueCount();
+    hiveVector.ensureSize(size, false);
     final ArrowBuf offsets = arrowVector.getOffsetBuffer();
     final int OFFSET_WIDTH = 4;
 
@@ -412,6 +413,7 @@ private void readList(FieldVector arrowVector, ListColumnVector hiveVector, List
 
   private void readMap(FieldVector arrowVector, MapColumnVector hiveVector, MapTypeInfo typeInfo) {
     final int size = arrowVector.getValueCount();
+    hiveVector.ensureSize(size, false);
     final ListTypeInfo mapStructListTypeInfo = toStructListTypeInfo(typeInfo);
     final ListColumnVector mapStructListVector = toStructListVector(hiveVector);
     final StructColumnVector mapStructVector = (StructColumnVector) mapStructListVector.child;
@@ -430,6 +432,7 @@ private void readMap(FieldVector arrowVector, MapColumnVector hiveVector, MapTyp
 
   private void readStruct(FieldVector arrowVector, StructColumnVector hiveVector, StructTypeInfo typeInfo) {
     final int size = arrowVector.getValueCount();
+    hiveVector.ensureSize(size, false);
     final List<TypeInfo> fieldTypeInfos = typeInfo.getAllStructFieldTypeInfos();
     final int fieldSize = arrowVector.getChildrenFromFields().size();
     for (int i = 0; i < fieldSize; i++) {

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java
Patch:
@@ -574,7 +574,7 @@ private void handleReplaceColumns(org.apache.hadoop.hive.metastore.api.Table hms
       schemaDifference.getMissingFromFirst().forEach(icebergCols::remove);
     }
 
-    Pair<String, Optional<String>> outOfOrder = HiveSchemaUtil.getFirstOutOfOrderColumn(
+    Pair<String, Optional<String>> outOfOrder = HiveSchemaUtil.getReorderedColumn(
         hmsCols, icebergCols, ImmutableMap.of());
 
     // limit the scope of this operation to only dropping columns
@@ -612,8 +612,7 @@ private void handleChangeColumn(org.apache.hadoop.hive.metastore.api.Table hmsTa
           schemaDifference.getMissingFromSecond().get(0).getName(),
           schemaDifference.getMissingFromFirst().get(0).getName());
     }
-    Pair<String, Optional<String>> outOfOrder = HiveSchemaUtil.getFirstOutOfOrderColumn(hmsCols, icebergCols,
-        renameMapping);
+    Pair<String, Optional<String>> outOfOrder = HiveSchemaUtil.getReorderedColumn(hmsCols, icebergCols, renameMapping);
 
     if (!schemaDifference.isEmpty() || outOfOrder != null) {
       transaction = icebergTable.newTransaction();

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/ITestDbTxnManager.java
Patch:
@@ -34,7 +34,7 @@
 
 /**
  * Test class to run DbTxnManager tests against different dbms types.
- * Example: mvn test -Dtest=ITestDbTxnManager -Dtest.metastore.db=postgres -Ditest.jdbc.jars=yourPathtoJdbcDriver
+ * Example: mvn test -Dtest=ITestDbTxnManager -Dtest.metastore.db=postgres
  */
 public class ITestDbTxnManager extends TestDbTxnManager2 {
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFSpace.java
Patch:
@@ -44,7 +44,7 @@ public Text evaluate(IntWritable n) {
       len = 0;
     }
 
-    if (result.getBytes().length >= len) {
+    if (result.getLength() >= len) {
       result.set(result.getBytes(), 0, len);
     } else {
       byte[] spaces = new byte[len];

File: itests/hive-unit/src/test/java/org/apache/hive/service/cli/operation/TestOperationLoggingLayout.java
Patch:
@@ -152,6 +152,9 @@ private void executeWithOperationLog(String query, boolean queryLogEnabled) thro
     checkAppenderState("before operation close ", LogDivertAppender.QUERY_ROUTING_APPENDER, queryId, expectedStopped);
     checkAppenderState("before operation close ", LogDivertAppenderForTest.TEST_QUERY_ROUTING_APPENDER, queryId, expectedStopped);
     client.closeOperation(operationHandle);
+    checkAppenderState("after operation close ", LogDivertAppender.QUERY_ROUTING_APPENDER, queryId, expectedStopped);
+    checkAppenderState("after operation close ", LogDivertAppenderForTest.TEST_QUERY_ROUTING_APPENDER, queryId, expectedStopped);
+    Thread.sleep(8000);
     checkAppenderState("after operation close ", LogDivertAppender.QUERY_ROUTING_APPENDER, queryId, true);
     checkAppenderState("after operation close ", LogDivertAppenderForTest.TEST_QUERY_ROUTING_APPENDER, queryId, true);
   }

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestTables.java
Patch:
@@ -437,7 +437,7 @@ public String identifier(String tableIdentifier) {
       }
 
       Assert.assertTrue(location.delete());
-      return location.toString();
+      return "file://" + location;
     }
 
     @Override

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CoreCliDriver.java
Patch:
@@ -25,7 +25,6 @@
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.ql.QTestExternalDB;
 import org.apache.hadoop.hive.ql.QTestArguments;
 import org.apache.hadoop.hive.ql.QTestProcessExecResult;
 import org.apache.hadoop.hive.ql.QTestUtil;
@@ -63,7 +62,6 @@ public void beforeClass() throws Exception {
     String hiveConfDir = cliConfig.getHiveConfDir();
     String initScript = cliConfig.getInitScript();
     String cleanupScript = cliConfig.getCleanupScript();
-    Set<QTestExternalDB> externalDBs = cliConfig.getExternalDBs();
 
     qt = new ElapsedTimeLoggingWrapper<QTestUtil>() {
       @Override
@@ -76,7 +74,6 @@ public QTestUtil invokeInternal() throws Exception {
               .withConfDir(hiveConfDir)
               .withInitScript(initScript)
               .withCleanupScript(cleanupScript)
-              .withExternalDBs(externalDBs)
               .withLlapIo(true)
               .withFsType(cliConfig.getFsType())
               .build());

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTConverter.java
Patch:
@@ -803,7 +803,7 @@ public ASTNode visitCall(RexCall call) {
           astNodeLst.add(operand.accept(this));
         }
         return SqlFunctionConverter.buildAST(SqlStdOperatorTable.NOT,
-          Collections.singletonList(SqlFunctionConverter.buildAST(SqlStdOperatorTable.IS_NOT_DISTINCT_FROM, astNodeLst)));
+          Collections.singletonList(SqlFunctionConverter.buildAST(SqlStdOperatorTable.IS_NOT_DISTINCT_FROM, astNodeLst, call.getType())), call.getType());
       case CAST:
         assert(call.getOperands().size() == 1);
         if (call.getType().isStruct() ||
@@ -850,7 +850,7 @@ public ASTNode visitCall(RexCall call) {
       if (isFlat(call)) {
         return SqlFunctionConverter.buildAST(op, astNodeLst, 0);
       } else {
-        return SqlFunctionConverter.buildAST(op, astNodeLst);
+        return SqlFunctionConverter.buildAST(op, astNodeLst, call.getType());
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java
Patch:
@@ -236,7 +236,7 @@ private static FunctionInfo handleCastForParameterizedType(TypeInfo ti, Function
 
   // TODO: 1) handle Agg Func Name translation 2) is it correct to add func
   // args as child of func?
-  public static ASTNode buildAST(SqlOperator op, List<ASTNode> children) {
+  public static ASTNode buildAST(SqlOperator op, List<ASTNode> children, RelDataType type) {
     HiveToken hToken = calciteToHiveToken.get(op);
     ASTNode node;
     if (hToken != null) {
@@ -293,6 +293,8 @@ public static ASTNode buildAST(SqlOperator op, List<ASTNode> children) {
       }
     }
 
+    node.setTypeInfo(TypeConverter.convert(type));
+
     for (ASTNode c : children) {
       ParseDriver.adaptor.addChild(node, c);
     }

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreUtils.java
Patch:
@@ -1081,7 +1081,7 @@ public static boolean hasUnknownPartitions(PartitionsSpecByExprResult r) {
   }
 
   public static TableName getTableNameFor(Table table) {
-    return TableName.fromString(table.getTableName(), table.getCatName(), table.getDbName(), null);
+    return TableName.fromString(table.getTableName().toLowerCase(), table.getCatName().toLowerCase(), table.getDbName().toLowerCase(), null);
   }
 
   /**

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ExceptionHandler.java
Patch:
@@ -58,7 +58,7 @@ public static ExceptionHandler handleException(Exception e) {
   public <T extends Exception> ExceptionHandler
       throwIfInstance(Class ...te) throws T {
     if (te != null) {
-      for (Class t : te) {
+      for (Class<T> t : te) {
         throwIfInstance(t);
       }
     }

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/client/TestTablesCreateDropAlterTruncate.java
Patch:
@@ -411,6 +411,7 @@ public void testCreateTableEmptyName() throws Exception {
   @Test(expected = MetaException.class)
   public void testCreateTableNullStorageDescriptor() throws Exception {
     Table table = testTables[0];
+    table.setTableName("NullStorageT");
     table.setSd(null);
 
     client.createTable(table);

File: ql/src/java/org/apache/hadoop/hive/llap/WritableByteChannelAdapter.java
Patch:
@@ -93,7 +93,7 @@ public int write(ByteBuffer src) throws IOException {
     int size = src.remaining();
     //Down the semaphore or block until available
     takeWriteResources(1);
-    ByteBuf buf = allocator.buffer(size);
+    ByteBuf buf = allocator.getAsByteBufAllocator().buffer(size);
     buf.writeBytes(src);
     chc.writeAndFlush(buf).addListener(writeListener);
     return size;

File: ql/src/java/org/apache/hadoop/hive/llap/WritableByteChannelAdapter.java
Patch:
@@ -93,7 +93,7 @@ public int write(ByteBuffer src) throws IOException {
     int size = src.remaining();
     //Down the semaphore or block until available
     takeWriteResources(1);
-    ByteBuf buf = allocator.getAsByteBufAllocator().buffer(size);
+    ByteBuf buf = allocator.buffer(size);
     buf.writeBytes(src);
     chc.writeAndFlush(buf).addListener(writeListener);
     return size;

File: ql/src/java/org/apache/hadoop/hive/llap/WritableByteChannelAdapter.java
Patch:
@@ -93,7 +93,7 @@ public int write(ByteBuffer src) throws IOException {
     int size = src.remaining();
     //Down the semaphore or block until available
     takeWriteResources(1);
-    ByteBuf buf = allocator.buffer(size);
+    ByteBuf buf = allocator.getAsByteBufAllocator().buffer(size);
     buf.writeBytes(src);
     chc.writeAndFlush(buf).addListener(writeListener);
     return size;

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/Deserializer.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.iceberg.data.GenericRecord;
 import org.apache.iceberg.data.Record;
 import org.apache.iceberg.mr.hive.serde.objectinspector.WriteObjectInspector;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 import org.apache.iceberg.schema.SchemaWithPartnerVisitor;
 import org.apache.iceberg.types.Type.PrimitiveType;
 import org.apache.iceberg.types.Types.ListType;
@@ -232,7 +233,7 @@ private static class FixNameMappingObjectInspectorPair extends ObjectInspectorPa
     FixNameMappingObjectInspectorPair(Schema schema, ObjectInspectorPair pair) {
       super(pair.writerInspector(), pair.sourceInspector());
 
-      this.sourceNameMap = new HashMap<>(schema.columns().size());
+      this.sourceNameMap = Maps.newHashMapWithExpectedSize(schema.columns().size());
 
       List<? extends StructField> fields = ((StructObjectInspector) sourceInspector()).getAllStructFieldRefs();
       for (int i = 0; i < schema.columns().size(); ++i) {

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java
Patch:
@@ -116,7 +116,7 @@ public RecordReader<Void, Container<Record>> getRecordReader(InputSplit split, J
       IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();
       // bogus cast for favouring code reuse over syntax
       return (RecordReader) HIVE_VECTORIZED_RECORDREADER_CTOR.newInstance(
-          new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>(),
+          new IcebergInputFormat<>(),
           icebergSplit,
           job,
           reporter);

File: iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java
Patch:
@@ -21,7 +21,6 @@
 
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
@@ -56,6 +55,7 @@
 import org.apache.iceberg.mr.mapred.Container;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -70,7 +70,7 @@ public class HiveIcebergSerDe extends AbstractSerDe {
   private ObjectInspector inspector;
   private Schema tableSchema;
   private Collection<String> partitionColumns;
-  private Map<ObjectInspector, Deserializer> deserializers = new HashMap<>(1);
+  private Map<ObjectInspector, Deserializer> deserializers = Maps.newHashMapWithExpectedSize(1);
   private Container<Record> row = new Container<>();
 
   @Override

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/TestInputFormatReaderDeletes.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.iceberg.TableMetadata;
 import org.apache.iceberg.TableOperations;
 import org.apache.iceberg.data.DeleteReadTests;
+import org.apache.iceberg.data.InternalRecordWrapper;
 import org.apache.iceberg.hadoop.HadoopTables;
 import org.apache.iceberg.util.StructLikeSet;
 import org.junit.Assert;
@@ -104,6 +105,7 @@ public StructLikeSet rowSet(String name, Table table, String... columns) {
         .filter(recordFactory -> recordFactory.name().equals(inputFormat))
         .map(recordFactory -> recordFactory.create(builder.project(projected).conf()).getRecords())
         .flatMap(List::stream)
+        .map(record -> new InternalRecordWrapper(projected.asStruct()).wrap(record))
         .collect(Collectors.toList())
     );
 

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerLocalScan.java
Patch:
@@ -36,7 +36,6 @@
 import org.apache.iceberg.catalog.TableIdentifier;
 import org.apache.iceberg.data.GenericRecord;
 import org.apache.iceberg.data.Record;
-import org.apache.iceberg.mr.Catalogs;
 import org.apache.iceberg.mr.InputFormatConfig;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
@@ -233,7 +232,7 @@ public void testCreatePartitionedTableByProperty() throws IOException {
         "TBLPROPERTIES ('" + InputFormatConfig.PARTITION_SPEC + "'='" + PartitionSpecParser.toJson(spec) + "', " +
         "'" + InputFormatConfig.TABLE_SCHEMA + "'='" +
         SchemaParser.toJson(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA) + "', "  +
-        "'" + InputFormatConfig.CATALOG_NAME + "'='" + Catalogs.ICEBERG_DEFAULT_CATALOG_NAME + "')";
+        "'" + InputFormatConfig.CATALOG_NAME + "'='" + testTables.catalogName() + "')";
     runCreateAndReadTest(identifier, createSql, HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA, spec, data);
   }
 

File: iceberg/patched-iceberg-core/src/main/java/org/apache/iceberg/ClientPool.java
Patch:
@@ -17,12 +17,14 @@
  * under the License.
  */
 
-package org.apache.iceberg.hive;
+package org.apache.iceberg;
 
 public interface ClientPool<C, E extends Exception> {
   interface Action<R, C, E extends Exception> {
     R run(C client) throws E;
   }
 
   <R> R run(Action<R, C, E> action) throws E, InterruptedException;
+
+  <R> R run(Action<R, C, E> action, boolean retry) throws E, InterruptedException;
 }

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
Patch:
@@ -1442,7 +1442,7 @@ public void startReadSplitFromFile(
     Path path = split.getPath().getFileSystem(daemonConf).makeQualified(split.getPath());
     PartitionDesc partDesc = HiveFileFormatUtils.getFromPathRecursively(parts, path, null);
     try {
-      offsetReader = createOffsetReader(sourceReader, partDesc.getTableDesc());
+      offsetReader = createOffsetReader(sourceReader, partDesc.getTableDesc(), split);
       sourceReader = null;
     } finally {
       if (sourceReader != null) {
@@ -1650,7 +1650,7 @@ private ObjectInspector getOiFromSerDe() throws IOException {
     }
   }
 
-  private ReaderWithOffsets createOffsetReader(RecordReader<?, ?> sourceReader, TableDesc tableDesc)
+  private ReaderWithOffsets createOffsetReader(RecordReader<?, ?> sourceReader, TableDesc tableDesc, FileSplit split)
       throws IOException {
     int headerCount = Utilities.getHeaderCount(tableDesc);
     int footerCount = Utilities.getFooterCount(tableDesc, jobConf);
@@ -1661,7 +1661,7 @@ private ReaderWithOffsets createOffsetReader(RecordReader<?, ?> sourceReader, Ta
     // Handle the special cases here. Perhaps we could have a more general structure, or even
     // a configurable set (like storage handlers), but for now we only have one.
     if (isLrrEnabled && sourceReader instanceof LineRecordReader) {
-      return LineRrOffsetReader.create((LineRecordReader)sourceReader, jobConf, headerCount, footerCount);
+      return LineRrOffsetReader.create((LineRecordReader)sourceReader, jobConf, headerCount, footerCount, split);
     }
     return new PassThruOffsetReader(sourceReader, jobConf, headerCount, footerCount);
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java
Patch:
@@ -101,7 +101,7 @@ public IncrementalLoadTasksBuilder(String dbName, String loadPath, IncrementalLo
     this.shouldFailover = shouldFailover;
     if (shouldFailover) {
       this.metricCollector.reportFailoverStart("REPL_LOAD", metricMap,
-              new FailoverMetaData(new Path(dumpDirectory), conf));
+              new FailoverMetaData(new Path(dumpDirectory, ReplUtils.REPL_HIVE_BASE_DIR), conf));
     } else {
       this.metricCollector.reportStageStart("REPL_LOAD", metricMap);
     }

File: jdbc/src/java/org/apache/hive/jdbc/Utils.java
Patch:
@@ -85,7 +85,7 @@ public static class JdbcConnectionParams {
     // Client param names:
 
     // Retry setting
-    public static final String RETRIES = "retries";
+    static final String RETRIES = "retries";
     public static final String RETRY_INTERVAL = "retryInterval";
 
     public static final String AUTH_TYPE = "auth";

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
Patch:
@@ -698,7 +698,8 @@ public enum ConfVars {
             "\n" +
             "See HIVE-4409 for more details."),
     ALLOW_INCOMPATIBLE_COL_TYPE_CHANGES_TABLE_SERDES("metastore.allow.incompatible.col.type.changes.serdes",
-        "hive.metastore.allow.incompatible.col.type.changes.serdes", "org.apache.hadoop.hive.kudu.KuduSerDe",
+        "hive.metastore.allow.incompatible.col.type.changes.serdes",
+        "org.apache.hadoop.hive.kudu.KuduSerDe,org.apache.iceberg.mr.hive.HiveIcebergSerDe",
         "Comma-separated list of table serdes which are allowed to make incompatible column type\n" +
         "changes. This configuration is only applicable if metastore.disallow.incompatible.col.type.changes\n" +
         "is true."),

File: ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java
Patch:
@@ -195,8 +195,7 @@ public boolean accept(Path path) {
             HiveParser.TOK_DESCTABLE,
             HiveParser.TOK_SHOWTABLES,
             HiveParser.TOK_SHOW_TABLESTATUS,
-            HiveParser.TOK_SHOW_TBLPROPERTIES,
-            HiveParser.TOK_EXPLAIN
+            HiveParser.TOK_SHOW_TBLPROPERTIES
     ));
   }
 

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveShell.java
Patch:
@@ -214,9 +214,6 @@ private HiveConf initializeConf() {
     // set to true so that the Tez session will create an empty jar for localization
     hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_IN_TEST_IDE, true);
 
-    // enables vectorization on Tez
-    hiveConf.set("tez.mrreader.config.update.properties", "hive.io.file.readcolumn.names,hive.io.file.readcolumn.ids");
-
     // set lifecycle hooks
     hiveConf.setVar(HiveConf.ConfVars.HIVE_QUERY_LIFETIME_HOOKS, HiveIcebergQueryLifeTimeHook.class.getName());
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/ReplicationTestUtils.java
Patch:
@@ -320,7 +320,7 @@ public static void insertIntoDB(WarehouseInstance primary, String dbName, String
             .run(txnStrCommit);
   }
 
-  private static void insertIntoDB(WarehouseInstance primary, String dbName, String tableName,
+  public static void insertIntoDB(WarehouseInstance primary, String dbName, String tableName,
                                    String tableProperty, String storageType, String[] resultArray)
           throws Throwable {
     insertIntoDB(primary, dbName, tableName, tableProperty, storageType, resultArray, false);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -162,6 +162,7 @@
 import org.apache.hadoop.hive.ql.optimizer.calcite.HiveConfPlannerContext;
 import org.apache.hadoop.hive.ql.optimizer.calcite.HiveDefaultRelMetadataProvider;
 import org.apache.hadoop.hive.ql.optimizer.calcite.HiveTezModelRelMetadataProvider;
+import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateSortLimitRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinSwapConstraintsRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSemiJoinProjectTransposeRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveMaterializationRelMetadataProvider;
@@ -2214,7 +2215,7 @@ private RelNode applyPostJoinOrderingTransform(RelNode basePlan, RelMetadataProv
           ProjectRemoveRule.Config.DEFAULT.toRule(), HiveUnionMergeRule.INSTANCE,
           new HiveUnionSimpleSelectsToInlineTableRule(dummyTableScan),
           HiveAggregateProjectMergeRule.INSTANCE, HiveProjectMergeRule.INSTANCE_NO_FORCE,
-          HiveJoinCommuteRule.INSTANCE);
+          HiveJoinCommuteRule.INSTANCE, HiveAggregateSortLimitRule.getInstance(conf));
 
       // 2. Run aggregate-join transpose (cost based)
       //    If it failed because of missing stats, we continue with

File: ql/src/java/org/apache/hadoop/hive/ql/parse/MergeSemanticAnalyzer.java
Patch:
@@ -611,7 +611,6 @@ private void handleInsert(ASTNode whenNotMatchedClause, StringBuilder rewrittenQ
     if (columnListNode != null) {
       rewrittenQueryStr.append(' ').append(getMatchedText(columnListNode));
     }
-    addPartitionColsToInsert(targetTable.getPartCols(), rewrittenQueryStr);
 
     rewrittenQueryStr.append("    -- insert clause\n  SELECT ");
     if (hintStr != null) {

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/datasource/HikariCPDataSourceProvider.java
Patch:
@@ -41,6 +41,7 @@ public class HikariCPDataSourceProvider implements DataSourceProvider {
 
   static final String HIKARI = "hikaricp";
   private static final String CONNECTION_TIMEOUT_PROPERTY = HIKARI + ".connectionTimeout";
+  private static final String LEAK_DETECTION_THRESHOLD = HIKARI + ".leakDetectionThreshold";
 
   @Override
   public DataSource create(Configuration hdpConfig) throws SQLException {
@@ -56,6 +57,7 @@ public DataSource create(Configuration hdpConfig) throws SQLException {
     Properties properties = replacePrefix(
         DataSourceProvider.getPrefixedProperties(hdpConfig, HIKARI));
     long connectionTimeout = hdpConfig.getLong(CONNECTION_TIMEOUT_PROPERTY, 30000L);
+    long leakDetectionThreshold = hdpConfig.getLong(LEAK_DETECTION_THRESHOLD, 3600000L);
 
     HikariConfig config;
     try {
@@ -67,6 +69,7 @@ public DataSource create(Configuration hdpConfig) throws SQLException {
     config.setJdbcUrl(driverUrl);
     config.setUsername(user);
     config.setPassword(passwd);
+    config.setLeakDetectionThreshold(leakDetectionThreshold);
 
     //https://github.com/brettwooldridge/HikariCP
     config.setConnectionTimeout(connectionTimeout);

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveShell.java
Patch:
@@ -185,7 +185,7 @@ private HiveConf initializeConf() {
     hiveConf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_WEBUI_PORT, -1);
 
     // Switch off optimizers in order to contain the map reduction within this JVM
-    hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_CBO_ENABLED, false);
+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_CBO_ENABLED, true);
     hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_INFER_BUCKET_SORT, false);
     hiveConf.setBoolVar(HiveConf.ConfVars.HIVEMETADATAONLYQUERIES, false);
     hiveConf.setBoolVar(HiveConf.ConfVars.HIVEOPTINDEXFILTER, false);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -530,7 +530,7 @@ Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticExcept
       List<ASTNode> oldHints = new ArrayList<>();
       // Cache the hints before CBO runs and removes them.
       // Use the hints later in top level QB.
-        getHintsFromQB(getQB(), oldHints);
+      getHintsFromQB(getQB(), oldHints);
 
       // Note: for now, we don't actually pass the queryForCbo to CBO, because
       // it accepts qb, not AST, and can also access all the private stuff in
@@ -612,6 +612,7 @@ Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticExcept
             if (!doPhase1(newAST, getQB(), ctx_1, null)) {
               throw new RuntimeException("Couldn't do phase1 on CBO optimized query plan");
             }
+
             // unfortunately making prunedPartitions immutable is not possible
             // here with SemiJoins not all tables are costed in CBO, so their
             // PartitionList is not evaluated until the run phase.

File: ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestVectorizedListColumnReader.java
Patch:
@@ -280,7 +280,7 @@ private void testListRead(boolean isDictionaryEncoding, String type, int element
           long length = vector.lengths[i];
           boolean isNull = isNull(row);
           if (isNull) {
-            assertEquals(vector.isNull[i], true);
+            assertEquals("vector.isNull[" + i + "] is expected to be true", true, vector.isNull[i]);
           } else {
             for (long j = 0; j < length; j++) {
               assertValue(type, vector.child, isDictionaryEncoding, index, (int) (start + j));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/DirCopyTask.java
Patch:
@@ -226,14 +226,13 @@ public int execute() {
 
   private HiveConf getConf(HiveConf conf) {
     // if it is a db level path check for custom configurations.
+    HiveConf clonedConf = new HiveConf(conf);
     if (work.getTableName().startsWith("dbPath:")) {
-      HiveConf clonedConf = new HiveConf(conf);
       for (Map.Entry<String, String> entry : conf.getPropsWithPrefix(CUSTOM_PATH_CONFIG_PREFIX).entrySet()) {
         clonedConf.set(entry.getKey().replaceFirst(CUSTOM_PATH_CONFIG_PREFIX, ""), entry.getValue());
       }
-      return clonedConf;
     }
-    return conf;
+    return clonedConf;
   }
 
   private static Path reservedRawPath(URI uri) {

File: shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
Patch:
@@ -1203,8 +1203,8 @@ public boolean runDistCp(List<Path> srcPaths, Path dst, Configuration conf) thro
     List<String> params = constructDistCpParams(srcPaths, dst, conf);
     DistCp distcp = null;
     try {
-      conf.setBoolean("mapred.mapper.new-api", true);
       distcp = new DistCp(conf, options);
+      distcp.getConf().setBoolean("mapred.mapper.new-api", true);
 
       // HIVE-13704 states that we should use run() instead of execute() due to a hadoop known issue
       // added by HADOOP-10459
@@ -1223,7 +1223,6 @@ public boolean runDistCp(List<Path> srcPaths, Path dst, Configuration conf) thro
           conf.set(CONF_LABEL_DISTCP_JOB_ID, jobId);
         }
       }
-      conf.setBoolean("mapred.mapper.new-api", false);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -9021,7 +9021,7 @@ private Operator genReduceSinkPlan(Operator<?> input, List<ExprNodeDesc> partiti
 
     List<String> keyColNames = rsdesc.getOutputKeyColumnNames();
     for (int i = 0 ; i < keyColNames.size(); i++) {
-      colExprMap.put(Utilities.ReduceField.KEY + "." + keyColNames.get(i), sortCols.get(i));
+      colExprMap.put(Utilities.ReduceField.KEY + "." + keyColNames.get(i), newSortCols.get(i));
     }
     List<String> valueColNames = rsdesc.getOutputValueColumnNames();
     for (int i = 0 ; i < valueColNames.size(); i++) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/type/ExprNodeDescExprFactory.java
Patch:
@@ -809,7 +809,8 @@ protected boolean convertCASEIntoCOALESCEFuncCallExpr(FunctionInfo fi, List<Expr
       Object thenVal = constThen.getValue();
       Object elseVal = constElse.getValue();
       if (thenVal instanceof Boolean && elseVal instanceof Boolean) {
-        return true;
+        //only convert to COALESCE when both branches are valid
+        return !thenVal.equals(elseVal);
       }
     }
     return false;

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
Patch:
@@ -1854,7 +1854,6 @@ public void testDumpAcidTableWithTableDirMissing() throws Throwable {
     primary.run("drop database " + dbName);
   }
 
-  @org.junit.Ignore("HIVE-25367")
   @Test
   public void testMultiDBTxn() throws Throwable {
     String tableName = testName.getMethodName();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TaskRunner.java
Patch:
@@ -109,11 +109,11 @@ public void runSequential() {
       }
       LOG.error("Error in executeTask", t);
     }
-    result.setExitVal(exitVal);
-    taskQueue.releaseRunnable();
     if (tsk.getException() != null) {
       result.setTaskError(tsk.getException());
     }
+    result.setExitVal(exitVal);
+    taskQueue.releaseRunnable();
   }
 
   public static long getTaskRunnerID () {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3690,7 +3690,7 @@ public static enum ConfVars {
     HIVE_SSL_PROTOCOL_BLACKLIST("hive.ssl.protocol.blacklist", "SSLv2,SSLv3",
         "SSL Versions to disable for all Hive Servers"),
 
-    HIVE_PRIVILEGE_SYNCHRONIZER("hive.privilege.synchronizer", true,
+    HIVE_PRIVILEGE_SYNCHRONIZER("hive.privilege.synchronizer", false,
             "Whether to synchronize privileges from external authorizer periodically in HS2"),
     HIVE_PRIVILEGE_SYNCHRONIZER_INTERVAL("hive.privilege.synchronizer.interval",
         "1800s", new TimeValidator(TimeUnit.SECONDS),

File: itests/hive-unit/src/test/java/org/apache/hive/service/server/InformationSchemaWithPrivilegeTestBase.java
Patch:
@@ -210,6 +210,7 @@ public static void setupInternal(boolean zookeeperSSLEnabled) throws Exception {
     confOverlay.put(MetastoreConf.ConfVars.AUTO_CREATE_ALL.getVarname(), "true");
     confOverlay.put(ConfVars.HIVE_AUTHENTICATOR_MANAGER.varname, FakeGroupAuthenticator.class.getName());
     confOverlay.put(ConfVars.HIVE_AUTHORIZATION_ENABLED.varname, "true");
+    confOverlay.put(ConfVars.HIVE_PRIVILEGE_SYNCHRONIZER.varname, "true");
     confOverlay.put(ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST.varname, ".*");
 
     if(zookeeperSSLEnabled) {

File: ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/CompactorTest.java
Patch:
@@ -118,6 +118,7 @@ public abstract class CompactorTest {
   @Before
   public void setup() throws Exception {
     conf = new HiveConf();
+    MetastoreConf.setTimeVar(conf, MetastoreConf.ConfVars.TXN_OPENTXN_TIMEOUT, 2, TimeUnit.SECONDS);
     TestTxnDbUtil.setConfValues(conf);
     TestTxnDbUtil.cleanDb(conf);
     TestTxnDbUtil.prepDb(conf);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
Patch:
@@ -351,8 +351,8 @@ public boolean addDependentTask(Task<?> dependent) {
 
   @SuppressWarnings({"unchecked", "rawtypes"})
   public static List<Task<?>>
-      findLeafs(List<Task<? extends Serializable>> rootTasks) {
-    final List<Task<? extends Serializable>> leafTasks = new ArrayList<Task<?>>();
+      findLeafs(List<Task<?>> rootTasks) {
+    final List<Task<?>> leafTasks = new ArrayList<Task<?>>();
 
     NodeUtils.iterateTask(rootTasks, Task.class, new NodeUtils.Function<Task>() {
       @Override

File: common/src/java/org/apache/hadoop/hive/common/type/Date.java
Patch:
@@ -19,14 +19,14 @@
 
 import org.apache.hive.common.util.SuppressFBWarnings;
 
+import java.time.DateTimeException;
 import java.time.Instant;
 import java.time.LocalDate;
 import java.time.LocalDateTime;
 import java.time.ZoneId;
 import java.time.ZoneOffset;
 import java.time.format.DateTimeFormatter;
 import java.time.format.DateTimeFormatterBuilder;
-import java.time.format.DateTimeParseException;
 import java.time.format.ResolverStyle;
 import java.time.format.SignStyle;
 import java.time.temporal.ChronoField;
@@ -83,7 +83,7 @@ public class Date implements Comparable<Date> {
   private static final DateTimeFormatter PARSE_FORMATTER =
       new DateTimeFormatterBuilder().appendValue(YEAR, 1, 10, SignStyle.NORMAL).appendLiteral('-')
           .appendValue(MONTH_OF_YEAR, 1, 2, SignStyle.NORMAL).appendLiteral('-')
-          .appendValue(DAY_OF_MONTH, 1, 2, SignStyle.NORMAL).toFormatter().withResolverStyle(ResolverStyle.LENIENT);
+          .appendValue(DAY_OF_MONTH, 1, 2, SignStyle.NORMAL).toFormatter().withResolverStyle(ResolverStyle.STRICT);
 
   private static final DateTimeFormatter PRINT_FORMATTER =
       new DateTimeFormatterBuilder().append(DateTimeFormatter.ofPattern("uuuu-MM-dd")).toFormatter();
@@ -187,7 +187,7 @@ public static Date valueOf(final String text) {
     LocalDate localDate;
     try {
       localDate = LocalDate.parse(s, PARSE_FORMATTER);
-    } catch (DateTimeParseException e) {
+    } catch (DateTimeException e) {
       throw new IllegalArgumentException("Cannot create date, parsing error");
     }
     return new Date(localDate);

File: common/src/java/org/apache/hadoop/hive/common/type/TimestampUtils.java
Patch:
@@ -198,11 +198,11 @@ public static Timestamp stringToTimestamp(final String text) {
       try {
         return Timestamp.valueOf(
             TimestampTZUtil.parse(s).getZonedDateTime().toLocalDateTime().toString());
-      } catch (IllegalArgumentException | DateTimeParseException eTZ) {
+      } catch (IllegalArgumentException | DateTimeException eTZ) {
         try {
           // Try HH:mm:ss format (For Hour, Minute & Second UDF).
           return Timestamp.getTimestampFromTime(s);
-        } catch(DateTimeParseException e) {
+        } catch(DateTimeException e) {
           // Last attempt
           return Timestamp.ofEpochMilli(Date.valueOf(s).toEpochMilli());
         }

File: common/src/test/org/apache/hive/common/util/TestDateParser.java
Patch:
@@ -58,9 +58,6 @@ public void testValidCases() throws Exception {
     // Leading spaces
     checkValidCase(" 1946-01-01", Date.valueOf("1946-01-01"));
     checkValidCase(" 2001-11-12 01:02:03", Date.valueOf("2001-11-12"));
-
-    checkValidCase("2001-13-12", Date.valueOf("2002-01-12"));
-    checkValidCase("2001-11-31", Date.valueOf("2001-12-01"));
   }
 
   @Test
@@ -70,5 +67,8 @@ public void testInvalidCases() throws Exception {
     checkInvalidCase("abc");
     checkInvalidCase(" 2001 ");
     checkInvalidCase("a2001-01-01");
+    checkInvalidCase("0000-00-00");
+    checkInvalidCase("2001-13-12");
+    checkInvalidCase("2001-11-31");
   }
 }

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFQuarter.java
Patch:
@@ -97,8 +97,8 @@ public void testWrongDateStr() throws HiveException {
 
     udf.initialize(arguments);
 
-    runAndVerifyStr("2016-03-35", 2, udf);
-    runAndVerifyStr("2014-01-32", 1, udf);
+    runAndVerifyStr("2016-03-35", null, udf);
+    runAndVerifyStr("2014-01-32", null, udf);
     runAndVerifyStr("01/14/2014", null, udf);
     runAndVerifyStr(null, null, udf);
   }

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestReplChangeManager.java
Patch:
@@ -95,7 +95,7 @@ private static void internalSetUpProvidePerm() throws Exception {
     String noPermBaseDir = Files.createTempDirectory("noPerm").toFile().getAbsolutePath();
     configuration.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, noPermBaseDir);
     configuration.set("dfs.client.use.datanode.hostname", "true");
-    permDdfs = new MiniDFSCluster.Builder(configuration).numDataNodes(1).format(true).build();
+    permDdfs = new MiniDFSCluster.Builder(configuration).numDataNodes(2).format(true).build();
     permhiveConf = new HiveConf(TestReplChangeManager.class);
     permhiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname,
       "hdfs://" + permDdfs.getNameNode().getHostAndPort() + HiveConf.ConfVars.METASTOREWAREHOUSE.defaultStrVal);
@@ -107,7 +107,7 @@ private static void internalSetUpProvidePerm() throws Exception {
   }
 
   private static void internalSetUp() throws Exception {
-    m_dfs = new MiniDFSCluster.Builder(new Configuration()).numDataNodes(1).format(true).build();
+    m_dfs = new MiniDFSCluster.Builder(new Configuration()).numDataNodes(2).format(true).build();
     hiveConf = new HiveConf(TestReplChangeManager.class);
     hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname,
       "hdfs://" + m_dfs.getNameNode().getHostAndPort() + HiveConf.ConfVars.METASTOREWAREHOUSE.defaultStrVal);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationAcrossInstances.java
Patch:
@@ -59,7 +59,7 @@ static void internalBeforeClassSetup(Map<String, String> overrides, Class clazz)
     conf.set("hive.repl.cmrootdir", "/tmp/");
     conf.set("dfs.namenode.acls.enabled", "true");
     MiniDFSCluster miniDFSCluster =
-        new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
+        new MiniDFSCluster.Builder(conf).numDataNodes(2).format(true).build();
     Map<String, String> localOverrides = new HashMap<String, String>() {{
       put("fs.defaultFS", miniDFSCluster.getFileSystem().getUri().toString());
       put(HiveConf.ConfVars.HIVE_IN_TEST_REPL.varname, "true");
@@ -89,15 +89,15 @@ static void internalBeforeClassSetupExclusiveReplica(Map<String, String> primary
     replicaConf.set("dfs.client.use.datanode.hostname", "true");
     replicaConf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     MiniDFSCluster miniReplicaDFSCluster =
-            new MiniDFSCluster.Builder(replicaConf).numDataNodes(1).format(true).build();
+            new MiniDFSCluster.Builder(replicaConf).numDataNodes(2).format(true).build();
 
     // Setup primary HDFS.
     String primaryBaseDir = Files.createTempDirectory("base").toFile().getAbsolutePath();
     conf = new HiveConf(clazz);
     conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, primaryBaseDir);
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
-    MiniDFSCluster miniPrimaryDFSCluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
+    MiniDFSCluster miniPrimaryDFSCluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).format(true).build();
 
     // Setup primary warehouse.
     setFullyQualifiedReplicaExternalTableBase(miniReplicaDFSCluster.getFileSystem());

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationScenariosAcidTables.java
Patch:
@@ -85,7 +85,7 @@ static void internalBeforeClassSetup(Map<String, String> overrides, Class clazz)
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     MiniDFSCluster miniDFSCluster =
-        new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
+        new MiniDFSCluster.Builder(conf).numDataNodes(2).format(true).build();
     HashMap<String, String> acidEnableConf = new HashMap<String, String>() {{
       put("fs.defaultFS", miniDFSCluster.getFileSystem().getUri().toString());
       put("hive.support.concurrency", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestMetaStoreEventListenerInRepl.java
Patch:
@@ -66,7 +66,7 @@ public static void internalBeforeClassSetup() throws Exception {
     TestMetaStoreEventListenerInRepl.conf.set("dfs.client.use.datanode.hostname", "true");
     TestMetaStoreEventListenerInRepl.conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     MiniDFSCluster miniDFSCluster =
-        new MiniDFSCluster.Builder(TestMetaStoreEventListenerInRepl.conf).numDataNodes(1).format(true).build();
+        new MiniDFSCluster.Builder(TestMetaStoreEventListenerInRepl.conf).numDataNodes(2).format(true).build();
 
     Map<String, String> conf = new HashMap<String, String>() {{
 	      put("fs.defaultFS", miniDFSCluster.getFileSystem().getUri().toString());

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOfHiveStreaming.java
Patch:
@@ -73,7 +73,7 @@ static void internalBeforeClassSetup(Map<String, String> overrides,
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     MiniDFSCluster miniDFSCluster =
-        new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
+        new MiniDFSCluster.Builder(conf).numDataNodes(2).format(true).build();
     Map<String, String> acidEnableConf = new HashMap<String, String>() {{
         put("fs.defaultFS", miniDFSCluster.getFileSystem().getUri().toString());
         put("hive.support.concurrency", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOnHDFSEncryptedZones.java
Patch:
@@ -67,7 +67,7 @@ public static void beforeClassSetup() throws Exception {
     conf.setBoolean(METASTORE_AGGREGATE_STATS_CACHE_ENABLED.varname, false);
 
     miniDFSCluster =
-        new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
+        new MiniDFSCluster.Builder(conf).numDataNodes(2).format(true).build();
 
     DFSTestUtil.createKey("test_key", miniDFSCluster, conf);
     primary = new WarehouseInstance(LOG, miniDFSCluster, new HashMap<String, String>() {{

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
Patch:
@@ -98,7 +98,7 @@ static void internalBeforeClassSetup(Map<String, String> overrides,
     conf.set("metastore.warehouse.tenant.colocation", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     MiniDFSCluster miniDFSCluster =
-        new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
+        new MiniDFSCluster.Builder(conf).numDataNodes(2).format(true).build();
     Map<String, String> acidEnableConf = new HashMap<String, String>() {{
         put("fs.defaultFS", miniDFSCluster.getFileSystem().getUri().toString());
         put("hive.support.concurrency", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosIncrementalLoadAcidTables.java
Patch:
@@ -70,7 +70,7 @@ static void internalBeforeClassSetup(Map<String, String> overrides, Class clazz)
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     MiniDFSCluster miniDFSCluster =
-           new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
+           new MiniDFSCluster.Builder(conf).numDataNodes(2).format(true).build();
     HashMap<String, String> acidConfs = new HashMap<String, String>() {{
         put("fs.defaultFS", miniDFSCluster.getFileSystem().getUri().toString());
         put("hive.support.concurrency", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestScheduledReplicationScenarios.java
Patch:
@@ -86,7 +86,7 @@ static void internalBeforeClassSetup(Map<String, String> overrides,
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     MiniDFSCluster miniDFSCluster =
-        new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
+        new MiniDFSCluster.Builder(conf).numDataNodes(2).format(true).build();
     Map<String, String> acidEnableConf = new HashMap<String, String>() {{
         put("fs.defaultFS", miniDFSCluster.getFileSystem().getUri().toString());
         put("hive.support.concurrency", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenarios.java
Patch:
@@ -97,12 +97,13 @@ static void internalBeforeClassSetup(Map<String, String> primaryOverrides,
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     MiniDFSCluster miniDFSCluster =
-        new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
+        new MiniDFSCluster.Builder(conf).numDataNodes(2).format(true).build();
     Map<String, String> additionalOverrides = new HashMap<String, String>() {{
         put("fs.defaultFS", miniDFSCluster.getFileSystem().getUri().toString());
         put(HiveConf.ConfVars.HIVE_IN_TEST_REPL.varname, "true");
         put(HiveConf.ConfVars.REPL_RUN_DATA_COPY_TASKS_ON_TARGET.varname, "false");
         put(HiveConf.ConfVars.REPL_RETAIN_CUSTOM_LOCATIONS_FOR_DB_ON_TARGET.varname, "false");
+        put(MetastoreConf.ConfVars.TXN_OPENTXN_TIMEOUT.getVarname(), "2000");
       }};
     Map<String, String> replicatedOverrides = new HashMap<>();
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenariosACID.java
Patch:
@@ -30,7 +30,6 @@
 /**
  * Tests statistics replication for ACID tables.
  */
-@org.junit.Ignore("HIVE-23924")
 public class TestStatsReplicationScenariosACID extends TestStatsReplicationScenarios {
   @Rule
   public final TestName testName = new TestName();

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenariosACIDNoAutogather.java
Patch:
@@ -33,7 +33,6 @@
 /**
  * Tests statistics replication for ACID tables.
  */
-@Ignore("HIVE-22626 should fix this")
 public class TestStatsReplicationScenariosACIDNoAutogather extends TestStatsReplicationScenarios {
   @Rule
   public final TestName testName = new TestName();

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenariosMMNoAutogather.java
Patch:
@@ -30,7 +30,6 @@
 /**
  * Tests statistics replication for ACID tables.
  */
-@org.junit.Ignore("HIVE-23944")
 public class TestStatsReplicationScenariosMMNoAutogather extends TestStatsReplicationScenarios {
   @Rule
   public final TestName testName = new TestName();

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCleanerWithReplication.java
Patch:
@@ -82,7 +82,7 @@ public static void classLevelSetup() throws LoginException, IOException {
     hadoopConf.set("dfs.client.use.datanode.hostname", "true");
     hadoopConf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
     miniDFSCluster =
-        new MiniDFSCluster.Builder(hadoopConf).numDataNodes(1).format(true).build();
+        new MiniDFSCluster.Builder(hadoopConf).numDataNodes(2).format(true).build();
     fs = miniDFSCluster.getFileSystem();
   }
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestMmCompactorOnTez.java
Patch:
@@ -53,7 +53,6 @@
 /**
  * Test functionality of MmMinorQueryCompactor.
  */
-@org.junit.Ignore("HIVE-25288")
 public class TestMmCompactorOnTez extends CompactorOnTezTest {
 
   public TestMmCompactorOnTez() {

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java
Patch:
@@ -123,7 +123,7 @@ public void run() {
             long minTxnIdSeenOpen = txnHandler.findMinTxnIdSeenOpen();
             final long cleanerWaterMark = minTxnIdSeenOpen < 0 ? minOpenTxnId : Math.min(minOpenTxnId, minTxnIdSeenOpen);
 
-            LOG.info("Cleaning based on min open txn id: " + minOpenTxnId);
+            LOG.info("Cleaning based on min open txn id: " + cleanerWaterMark);
             List<CompletableFuture<Void>> cleanerList = new ArrayList<>();
             // For checking which compaction can be cleaned we can use the minOpenTxnId
             // However findReadyToClean will return all records that were compacted with old version of HMS

File: common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -629,7 +629,8 @@ public enum ErrorMsg {
   REPL_PERMISSION_DENIED(40014, "{0}org.apache.hadoop.security.AccessControlException{1}", true),
   REPL_DISTCP_SNAPSHOT_EXCEPTION(40015, "SNAPSHOT_ERROR", true),
   RANGER_AUTHORIZATION_FAILED(40016, "Authorization Failure while communicating to Ranger admin", true),
-  RANGER_AUTHENTICATION_FAILED(40017, "Authentication Failure while communicating to Ranger admin", true)
+  RANGER_AUTHENTICATION_FAILED(40017, "Authentication Failure while communicating to Ranger admin", true),
+  REPL_INCOMPATIBLE_EXCEPTION(40018, "Cannot load into database {0} as it is replication incompatible.", true)
   ;
 
   private int errorCode;

File: ql/src/test/org/apache/hadoop/hive/ql/parse/repl/metric/TestReplicationMetricUpdateOnFailure.java
Patch:
@@ -149,6 +149,7 @@ public void testReplLoadFailure() throws Exception {
     IncrementalLoadMetricCollector metricCollector =
             new IncrementalLoadMetricCollector(null, TEST_PATH, 0, conf);
     ReplLoadWork replLoadWork = Mockito.mock(ReplLoadWork.class);
+    Mockito.when(replLoadWork.getTargetDatabase()).thenReturn("dummy");
     Mockito.when(replLoadWork.getDumpDirectory()).thenReturn(
             new Path(dumpDir + Path.SEPARATOR + "test").toString());
     Mockito.when(replLoadWork.getMetricCollector()).thenReturn(metricCollector);
@@ -172,6 +173,7 @@ public void testReplLoadRecoverableMissingStage() throws Exception {
     BootstrapLoadMetricCollector metricCollector = 
             new BootstrapLoadMetricCollector(null, TEST_PATH, 0, conf);
     ReplLoadWork replLoadWork = Mockito.mock(ReplLoadWork.class);
+    Mockito.when(replLoadWork.getTargetDatabase()).thenReturn("dummy");
     Mockito.when(replLoadWork.getDumpDirectory()).thenReturn(
             new Path(dumpDir + Path.SEPARATOR + "test").toString());
     Mockito.when(replLoadWork.getMetricCollector()).thenReturn(metricCollector);
@@ -192,6 +194,7 @@ public void testReplLoadNonRecoverableMissingStage() throws Exception {
     IncrementalLoadMetricCollector metricCollector = 
             new IncrementalLoadMetricCollector(null, TEST_PATH, 0, conf);
     ReplLoadWork replLoadWork = Mockito.mock(ReplLoadWork.class);
+    Mockito.when(replLoadWork.getTargetDatabase()).thenReturn("dummy");
     Mockito.when(replLoadWork.getDumpDirectory()).thenReturn(
             new Path(dumpDir + Path.SEPARATOR + "test").toString());
     Mockito.when(replLoadWork.getMetricCollector()).thenReturn(metricCollector);

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/common/repl/ReplConst.java
Patch:
@@ -51,4 +51,6 @@ public class ReplConst {
   public static final String REPL_FAILOVER_ENABLED = "repl.failover.enabled";
 
   public static final String TARGET_OF_REPLICATION = "repl.target.for";
+
+  public static final String REPL_INCOMPATIBLE = "repl.incompatible";
 }

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
Patch:
@@ -74,7 +74,7 @@
 /**
  * TestReplicationScenariosAcidTables - test replication for ACID tables.
  */
-@org.junit.Ignore("HIVE-25267")
+
 public class TestReplicationScenariosAcidTables extends BaseReplicationScenariosAcidTables {
 
   @BeforeClass
@@ -108,6 +108,8 @@ static void internalBeforeClassSetup(Map<String, String> overrides,
         put("metastore.warehouse.tenant.colocation", "true");
         put("hive.in.repl.test", "true");
         put("hive.txn.readonly.enabled", "true");
+        //HIVE-25267
+        put(MetastoreConf.ConfVars.TXN_OPENTXN_TIMEOUT.getVarname(), "2000");
         put(HiveConf.ConfVars.REPL_RUN_DATA_COPY_TASKS_ON_TARGET.varname, "false");
         put(HiveConf.ConfVars.REPL_RETAIN_CUSTOM_LOCATIONS_FOR_DB_ON_TARGET.varname, "false");
       }};

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToBoolean.java
Patch:
@@ -20,6 +20,7 @@
 
 
 import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.common.type.TimestampTZ;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDF;
 import org.apache.hadoop.hive.ql.exec.UDFMethodResolver;
@@ -213,7 +214,8 @@ public BooleanWritable evaluate(TimestampWritableV2 i) {
     if (i == null) {
       return null;
     } else {
-      booleanWritable.set(i.getSeconds() != 0 || i.getNanos() != 0);
+      TimestampTZ timestamp = UDFUtils.getTimestampTZFromTimestamp(i.getTimestamp());
+      booleanWritable.set((timestamp.getEpochSecond() != 0) || (timestamp.getNanos() != 0));
       return booleanWritable;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToByte.java
Patch:
@@ -211,7 +211,7 @@ public ByteWritable evaluate(TimestampWritableV2 i) {
     if (i == null) {
       return null;
     } else {
-      final long longValue = i.getSeconds();
+      final long longValue = UDFUtils.getTimestampTZFromTimestamp(i.getTimestamp()).getEpochSecond();
       final byte byteValue = (byte) longValue;
       if (byteValue != longValue) {
         return null;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToInteger.java
Patch:
@@ -221,7 +221,7 @@ public IntWritable evaluate(TimestampWritableV2 i) {
     if (i == null) {
       return null;
     } else {
-      final long longValue = i.getSeconds();
+      final long longValue = UDFUtils.getTimestampTZFromTimestamp(i.getTimestamp()).getEpochSecond();
       final int intValue = (int) longValue;
       if (intValue != longValue) {
         return null;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToLong.java
Patch:
@@ -222,7 +222,7 @@ public LongWritable evaluate(TimestampWritableV2 i) {
     if (i == null) {
       return null;
     } else {
-      longWritable.set(i.getSeconds());
+      longWritable.set(UDFUtils.getTimestampTZFromTimestamp(i.getTimestamp()).getEpochSecond());
       return longWritable;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToShort.java
Patch:
@@ -212,7 +212,7 @@ public ShortWritable evaluate(TimestampWritableV2 i) {
     if (i == null) {
       return null;
     } else {
-      final long longValue = i.getSeconds();
+      final long longValue = UDFUtils.getTimestampTZFromTimestamp(i.getTimestamp()).getEpochSecond();
       final short shortValue = (short) longValue;
       if (shortValue != longValue) {
         return null;

File: itests/hive-unit/src/test/java/org/apache/hive/service/TestHS2ImpersonationWithRemoteMS.java
Patch:
@@ -39,7 +39,7 @@
 /**
  * Test HiveServer2 sends correct user name to remote MetaStore server for user impersonation.
  */
-@org.junit.Ignore("HIVE-25250")
+
 public class TestHS2ImpersonationWithRemoteMS {
 
   private static MiniHS2 miniHS2 = null;
@@ -75,7 +75,7 @@ public void testImpersonation() throws Exception {
     Class.forName(MiniHS2.getJdbcDriverName());
 
     // Create two tables one as user "foo" and other as user "bar"
-    Connection hs2Conn = DriverManager.getConnection(miniHS2.getJdbcURL(), "foo", null);
+    Connection hs2Conn = DriverManager.getConnection(miniHS2.getJdbcURL()+";retries=3", "foo", null);
     Statement stmt = hs2Conn.createStatement();
 
     String tableName = "foo_table";
@@ -85,7 +85,7 @@ public void testImpersonation() throws Exception {
     stmt.close();
     hs2Conn.close();
 
-    hs2Conn = DriverManager.getConnection(miniHS2.getJdbcURL(), "bar", null);
+    hs2Conn = DriverManager.getConnection(miniHS2.getJdbcURL()+";retries=3", "bar", null);
     stmt = hs2Conn.createStatement();
 
     tableName = "bar_table";

File: jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java
Patch:
@@ -186,7 +186,7 @@ public class HiveConnection implements java.sql.Connection {
   private String wmPool = null, wmApp = null;
   private Properties clientInfo;
   private Subject loggedInSubject;
-  private int maxRetries = 5;
+  private int maxRetries = 1;
   private final IJdbcBrowserClient browserClient;
 
   /**

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorLimitOperator.java
Patch:
@@ -136,6 +136,8 @@ private void validateVectorLimitOperator(int limit, int batchSize, int expectedB
     VectorLimitDesc vectorDesc = new VectorLimitDesc();
     VectorLimitOperator lo = new VectorLimitOperator(
         new CompilationOpContext(), ld, null, vectorDesc);
+    // make sure an object registry is present for the test
+    ObjectCache.setupObjectRegistry(new ObjectRegistryImpl());
     lo.initialize(new Configuration(), null);
 
     // Process the batch

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactionQueryBuilder.java
Patch:
@@ -472,7 +472,7 @@ private int getMinorCrudBucketing(StringBuilder query, int bucketingVersion) {
             sourceTab.getTableName());
       } finally {
         query.append(" clustered by (`bucket`)")
-            .append(" sorted by (`bucket`, `originalTransaction`, `rowId`)")
+            .append(" sorted by (`originalTransaction`, `bucket`, `rowId`)")
             .append(" into ").append(numBuckets).append(" buckets");
       }
     }

File: spark-client/src/test/java/org/apache/hive/spark/client/rpc/TestRpc.java
Patch:
@@ -131,6 +131,7 @@ public void testClientServer() throws Exception {
   }
 
   @Test
+  @org.junit.Ignore("HIVE-25271")
   public void testServerAddress() throws Exception {
     String hostAddress = InetAddress.getLocalHost().getHostName();
     Map<String, String> config = new HashMap<String, String>();
@@ -178,6 +179,7 @@ public void testBadHello() throws Exception {
   }
 
   @Test
+  @org.junit.Ignore("HIVE-25271")
   public void testServerPort() throws Exception {
     Map<String, String> config = new HashMap<String, String>();
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java
Patch:
@@ -58,6 +58,7 @@
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
 import org.apache.hadoop.hive.ql.exec.UDTFOperator;
 import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.exec.tez.DagUtils;
 import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.lib.SemanticNodeProcessor;
 import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
@@ -1741,8 +1742,8 @@ private boolean checkMapSideAggregation(GroupByOperator gop,
         float hashAggMem = conf.getFloatVar(HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);
         float hashAggMaxThreshold = conf.getFloatVar(HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);
 
-        // get available map memory
-        long totalMemory = StatsUtils.getAvailableMemory(conf) * 1000L * 1000L;
+        // get available map memory in bytes
+        long totalMemory = DagUtils.getContainerResource(conf).getMemorySize() * 1024L * 1024L;
         long maxMemHashAgg = Math.round(totalMemory * hashAggMem * hashAggMaxThreshold);
 
         // estimated number of rows will be product of NDVs

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -688,6 +688,8 @@ public static enum ConfVars {
         + "data copy, the target data is overwritten and the modifications are removed and the copy is again "
         + "attempted using the snapshot based approach. If disabled, the replication will fail in case the target is "
         + "modified."),
+    REPL_STATS_TOP_EVENTS_COUNTS("hive.repl.stats.events.count", 5,
+        "Number of top costliest events that needs to maintained per event type for the replication statistics."),
     LOCALSCRATCHDIR("hive.exec.local.scratchdir",
         "${system:java.io.tmpdir}" + File.separator + "${system:user.name}",
         "Local scratch space for Hive jobs"),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
Patch:
@@ -742,7 +742,7 @@ private Long incrementalDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive
         dumpTableListToDumpLocation(tableList, dumpRoot, dbName, conf);
       }
       setDataCopyIterators(extTableFileList, managedTblList);
-      work.getMetricCollector().reportStageEnd(getName(), Status.SUCCESS, lastReplId, snapshotCount);
+      work.getMetricCollector().reportStageEnd(getName(), Status.SUCCESS, lastReplId, snapshotCount, null);
       // Clean-up snapshots
       if (isSnapshotEnabled) {
         cleanupSnapshots(SnapshotUtils.getSnapshotFileListPath(dumpRoot), work.dbNameOrPattern.toLowerCase(), conf,
@@ -1071,7 +1071,8 @@ Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive hiveDb)
           }
         }
         replLogger.endLog(bootDumpBeginReplId.toString());
-        work.getMetricCollector().reportStageEnd(getName(), Status.SUCCESS, bootDumpBeginReplId, replSnapshotCount);
+        work.getMetricCollector().reportStageEnd(getName(), Status.SUCCESS, bootDumpBeginReplId, replSnapshotCount,
+            replLogger.getReplStatsTracker());
       }
       work.setFunctionCopyPathIterator(functionsBinaryCopyPaths.iterator());
       setDataCopyIterators(extTableFileList, managedTblList);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplStateLogWork.java
Patch:
@@ -162,7 +162,7 @@ public void replStateLog() throws SemanticException {
         metricCollector.reportStageEnd("REPL_LOAD", Status.SUCCESS);
       } else {
         metricCollector.reportStageEnd("REPL_LOAD", Status.SUCCESS,
-            Long.parseLong(lastReplId), new SnapshotUtils.ReplSnapshotCount());
+            Long.parseLong(lastReplId), new SnapshotUtils.ReplSnapshotCount(), replLogger.getReplStatsTracker());
       }
       metricCollector.reportEnd(Status.SUCCESS);
       break;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java
Patch:
@@ -145,7 +145,7 @@ void dumpNonTableLevelCopyPaths(HashMap<String, Boolean> singlePathLocations, Fi
       if (!StringUtils.isEmpty(location.getKey()) && location.getValue()) {
         Path fullyQualifiedDataLocation =
             PathBuilder.fullyQualifiedHDFSUri(new Path(location.getKey()), FileSystem.get(hiveConf));
-        dirLocationToCopy(fullyQualifiedDataLocation.getName(), fileList, fullyQualifiedDataLocation, conf,
+        dirLocationToCopy("dbPath:" + fullyQualifiedDataLocation.getName(), fileList, fullyQualifiedDataLocation, conf,
             isSnapshotEnabled, snapshotPrefix, replSnapshotCount, snapPathFileList, prevSnaps, isBootstrap);
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -12970,8 +12970,9 @@ Map<ASTNode, ExprNodeDesc> genAllExprNodeDesc(ASTNode expr, RowResolver input,
       }
       String[] tmp = input.reverseLookup(columnDesc.getColumn());
       // in subquery case, tmp may be from outside.
-      if (tmp[0] != null && columnDesc.getTabAlias() != null
-          && !tmp[0].equals(columnDesc.getTabAlias()) && tcCtx.getOuterRR() != null) {
+      // check if outer present && (tmp is null || tmp not null - contains tbl info)
+      if (tcCtx.getOuterRR() != null && (tmp == null || (tmp[0] != null && columnDesc.getTabAlias() != null
+          && !tmp[0].equals(columnDesc.getTabAlias())))) {
         tmp = tcCtx.getOuterRR().reverseLookup(columnDesc.getColumn());
       }
       StringBuilder replacementText = new StringBuilder();

File: llap-server/src/java/org/apache/hadoop/hive/llap/cli/service/LlapServiceDriver.java
Patch:
@@ -44,7 +44,7 @@
 import java.io.IOException;
 import java.net.URL;
 import java.nio.file.Paths;
-import java.time.Instant;
+import java.time.LocalDateTime;
 import java.time.format.DateTimeFormatter;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -298,7 +298,7 @@ private int startLlap(Path tmpDir, Path scriptParent) throws IOException, Interr
     int rc;
     String version = System.getenv("HIVE_VERSION");
     if (StringUtils.isEmpty(version)) {
-      version = DateTimeFormatter.BASIC_ISO_DATE.format(Instant.now());
+      version = DateTimeFormatter.BASIC_ISO_DATE.format(LocalDateTime.now());
     }
 
     String outputDir = cl.getOutput();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java
Patch:
@@ -103,6 +103,7 @@ public IncrementalLoadTasksBuilder(String dbName, String loadPath,
 
   public Task<?> build(Context context, Hive hive, Logger log,
                                             TaskTracker tracker) throws Exception {
+    long builderStartTime = System.currentTimeMillis();
     Task<?> evTaskRoot = TaskFactory.get(new DependencyCollectionWork());
     Task<?> taskChainTail = evTaskRoot;
     Long lastReplayedEvent = null;
@@ -178,6 +179,8 @@ public Task<?> build(Context context, Hive hive, Logger log,
               taskChainTail.getClass(), taskChainTail.getId(),
               barrierTask.getClass(), barrierTask.getId());
     }
+    this.log.info("REPL_INCREMENTAL_LOAD task-builder iteration #{}, duration : {} ms",
+            numIteration, System.currentTimeMillis() - builderStartTime);
     return evTaskRoot;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/events/ReadDatabaseEvent.java
Patch:
@@ -42,7 +42,7 @@ public ReadDatabaseEvent(PreEventContext preEventContext) {
 
   @Override public HiveMetaStoreAuthzInfo getAuthzContext() {
     HiveMetaStoreAuthzInfo ret =
-        new HiveMetaStoreAuthzInfo(preEventContext, HiveOperationType.QUERY, getInputHObjs(), getOutputHObjs(),
+        new HiveMetaStoreAuthzInfo(preEventContext, HiveOperationType.SHOWDATABASES, getInputHObjs(), getOutputHObjs(),
             COMMAND_STR);
     return ret;
   }

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/dataconnector/AbstractDataConnectorProvider.java
Patch:
@@ -20,10 +20,12 @@ public abstract class AbstractDataConnectorProvider implements IDataConnectorPro
   protected Object  handle = null;
   protected boolean isOpen = false;
   protected DataConnector connector = null;
+  protected String driverClassName = null;
 
-  public AbstractDataConnectorProvider(String dbName, DataConnector connector) {
+  public AbstractDataConnectorProvider(String dbName, DataConnector connector, String driverClassName) {
     this.scoped_db = dbName;
     this.connector = connector;
+    this.driverClassName = driverClassName;
   }
 
   @Override public final void setScope(String scoped_db) {

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/dataconnector/jdbc/DerbySQLConnectorProvider.java
Patch:
@@ -19,8 +19,7 @@ public class DerbySQLConnectorProvider extends AbstractJDBCConnectorProvider {
   private static final String DRIVER_CLASS = "org.apache.derby.jdbc.AutoloadedDriver".intern();
 
   public DerbySQLConnectorProvider(String dbName, DataConnector connector) {
-    super(dbName, connector);
-    driverClassName = DRIVER_CLASS;
+    super(dbName, connector, DRIVER_CLASS);
   }
 
   /**

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/dataconnector/jdbc/MySQLConnectorProvider.java
Patch:
@@ -18,8 +18,7 @@ public class MySQLConnectorProvider extends AbstractJDBCConnectorProvider {
   private static final String DRIVER_CLASS = "com.mysql.jdbc.Driver".intern();
 
   public MySQLConnectorProvider(String dbName, DataConnector dataConn) {
-    super(dbName, dataConn);
-    driverClassName = DRIVER_CLASS;
+    super(dbName, dataConn, DRIVER_CLASS);
   }
 
   /**

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/dataconnector/jdbc/PostgreSQLConnectorProvider.java
Patch:
@@ -16,8 +16,7 @@ public class PostgreSQLConnectorProvider extends AbstractJDBCConnectorProvider {
   private static final String DRIVER_CLASS = "org.postgresql.Driver".intern();
 
   public PostgreSQLConnectorProvider(String dbName, DataConnector dataConn) {
-    super(dbName, dataConn);
-    driverClassName = DRIVER_CLASS;
+    super(dbName, dataConn, DRIVER_CLASS);
   }
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactionQueryBuilder.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
 import org.apache.hadoop.hive.ql.ddl.table.create.show.ShowCreateTableOperation;
+import org.apache.hadoop.hive.ql.exec.DDLPlanUtils;
 import org.apache.hadoop.hive.ql.io.AcidDirectory;
 import org.apache.hadoop.hive.ql.io.AcidUtils;
 import org.apache.hadoop.hive.ql.metadata.Hive;
@@ -518,7 +519,7 @@ private void copySerdeFromSourceTable(StringBuilder query) {
         .append(HiveStringUtils.escapeHiveCommand(serdeInfo.getSerializationLib())).append("'");
     // WITH SERDEPROPERTIES
     if (!serdeParams.isEmpty()) {
-      ShowCreateTableOperation.appendSerdeParams(query, serdeParams);
+      DDLPlanUtils.appendSerdeParams(query, serdeParams);
     }
     query.append("STORED AS INPUTFORMAT '")
         .append(HiveStringUtils.escapeHiveCommand(storageDescriptor.getInputFormat())).append("'")

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -2594,7 +2594,9 @@ private boolean validateMapJoinDesc(MapJoinDesc desc) {
       return false;
     }
     List<ExprNodeDesc> keyExprs = desc.getKeys().get(posBigTable);
-    if (!validateExprNodeDesc(keyExprs, "Key")) {
+    if (!validateExprNodeDescNoComplex(keyExprs, "Key")) {
+      // Vectorization for join keys of complex type is not supported.
+      // https://issues.apache.org/jira/browse/HIVE-24989
       return false;
     }
     List<ExprNodeDesc> valueExprs = desc.getExprs().get(posBigTable);

File: ql/src/test/org/apache/hadoop/hive/metastore/txn/TestCompactionTxnHandler.java
Patch:
@@ -285,6 +285,8 @@ public void testGetLatestCommittedCompaction() throws Exception {
     assertEquals("Expecting a single compaction record", 1, response.getCompactionsSize());
     lci = response.getCompactions().get(0);
     assertEquals("Expecting the first succeeded compaction record", 1, lci.getId());
+    assertEquals(dbName, lci.getDbname());
+    assertEquals(tableName, lci.getTablename());
     assertNull("Expecting null partitionname for a non-partitioned table", lci.getPartitionname());
     assertEquals(CompactionType.MINOR, lci.getType());
   }

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -3710,6 +3710,8 @@ public GetLatestCommittedCompactionInfoResponse getLatestCommittedCompactionInfo
         while (rs.next()) {
           CompactionInfoStruct lci = new CompactionInfoStruct();
           lci.setId(rs.getLong(1));
+          lci.setDbname(rs.getString(2));
+          lci.setTablename(rs.getString(3));
           String partition = rs.getString(4);
           if (!rs.wasNull()) {
             lci.setPartitionname(partition);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java
Patch:
@@ -387,7 +387,7 @@ private static void updateIfCustomDbLocations(Database database, Configuration c
               MetastoreConf.getVar(conf, MetastoreConf.ConfVars.WAREHOUSE));
       Path dbDerivedLoc = new Path(whLocatoion, database.getName().toLowerCase() + DATABASE_PATH_SUFFIX);
       String defaultDbLoc = Utilities.getQualifiedPath((HiveConf) conf, dbDerivedLoc);
-      database.getParameters().put(ReplUtils.REPL_IS_CUSTOM_DB_LOC,
+      database.putToParameters(ReplUtils.REPL_IS_CUSTOM_DB_LOC,
               Boolean.toString(!defaultDbLoc.equals(database.getLocationUri())));
       String whManagedLocatoion = MetastoreConf.getVar(conf, MetastoreConf.ConfVars.WAREHOUSE);
       Path dbDerivedManagedLoc = new Path(whManagedLocatoion, database.getName().toLowerCase()

File: iceberg/iceberg-catalog/src/test/java/org/apache/iceberg/hive/TestClientPoolImpl.java
Patch:
@@ -36,7 +36,7 @@
 import org.junit.Test;
 import org.mockito.Mockito;
 
-public class TestClientPool {
+public class TestClientPoolImpl {
 
   HiveClientPool clients;
 

File: iceberg/iceberg-catalog/src/test/java/org/apache/iceberg/hive/TestHiveClientPool.java
Patch:
@@ -42,14 +42,12 @@ public class TestHiveClientPool {
   public void testConf() {
     HiveConf conf = createHiveConf();
     conf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, "file:/mywarehouse/");
-    conf.setInt("iceberg.hive.client-pool-size", 10);
 
-    HiveClientPool clientPool = new HiveClientPool(conf);
+    HiveClientPool clientPool = new HiveClientPool(10, conf);
     HiveConf clientConf = clientPool.hiveConf();
 
     Assert.assertEquals(conf.get(HiveConf.ConfVars.METASTOREWAREHOUSE.varname),
             clientConf.get(HiveConf.ConfVars.METASTOREWAREHOUSE.varname));
-    Assert.assertEquals(conf.get("iceberg.hive.client-pool-size"), clientConf.get("iceberg.hive.client-pool-size"));
     Assert.assertEquals(10, clientPool.poolSize());
 
     // 'hive.metastore.sasl.enabled' should be 'true' as defined in xml

File: iceberg/iceberg-catalog/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java
Patch:
@@ -51,7 +51,7 @@
 public class TestHiveMetastore {
 
   private static final String DEFAULT_DATABASE_NAME = "default";
-  private static final int DEFAULT_POOL_SIZE = 10;
+  private static final int DEFAULT_POOL_SIZE = 5;
 
   // create the metastore handlers based on whether we're working with Hive2 or Hive3 dependencies
   // we need to do this because there is a breaking API change between Hive2 and Hive3

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java
Patch:
@@ -263,6 +263,7 @@ public static void validateFiles(Table table, Configuration conf, JobID jobId, i
         .collect(Collectors.toList());
 
     Assert.assertEquals(dataFileNum, dataFiles.size());
-    Assert.assertFalse(new File(HiveIcebergOutputCommitter.generateJobLocation(conf, jobId)).exists());
+    Assert.assertFalse(
+        new File(HiveIcebergOutputCommitter.generateJobLocation(table.location(), conf, jobId)).exists());
   }
 }

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergSerDe.java
Patch:
@@ -28,6 +28,8 @@
 import org.apache.iceberg.data.RandomGenericData;
 import org.apache.iceberg.data.Record;
 import org.apache.iceberg.hadoop.HadoopTables;
+import org.apache.iceberg.mr.Catalogs;
+import org.apache.iceberg.mr.InputFormatConfig;
 import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;
 import org.apache.iceberg.mr.mapred.Container;
 import org.apache.iceberg.types.Types;
@@ -54,6 +56,7 @@ public void testInitialize() throws IOException, SerDeException {
 
     Properties properties = new Properties();
     properties.setProperty("location", location.toString());
+    properties.setProperty(InputFormatConfig.CATALOG_NAME, Catalogs.ICEBERG_HADOOP_TABLE_NAME);
 
     HadoopTables tables = new HadoopTables(conf);
     tables.create(schema, location.toString());

File: iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveShell.java
Patch:
@@ -81,7 +81,7 @@ public void setHiveSessionValue(String key, boolean value) {
 
   public void start() {
     // Create a copy of the HiveConf for the metastore
-    metastore.start(new HiveConf(hs2Conf));
+    metastore.start(new HiveConf(hs2Conf), 10);
     hs2Conf.setVar(HiveConf.ConfVars.METASTOREURIS, metastore.hiveConf().getVar(HiveConf.ConfVars.METASTOREURIS));
     hs2Conf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE,
         metastore.hiveConf().getVar(HiveConf.ConfVars.METASTOREWAREHOUSE));

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
Patch:
@@ -1228,8 +1228,8 @@ private void processColumnCacheData(LlapSerDeDataBuffer[][][] cacheBuffers,
   }
 
   private void logProcessOneSlice(int stripeIx, Object diskData, StripeData cacheData) {
-    String sliceStr = cacheData == null ? "null" : cacheData.toCoordinateString();
     if (LlapIoImpl.LOG.isDebugEnabled()) {
+      String sliceStr = cacheData == null ? "null" : cacheData.toCoordinateString();
       LlapIoImpl.LOG.debug("Processing slice #" + stripeIx + " " + sliceStr + "; has"
         + ((cacheData == null) ? " no" : "") + " cache data; has"
         + ((diskData == null) ? " no" : "") + " disk data");

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -678,9 +678,8 @@ public static void setMapWork(MapWork plan, ParseContext parseCtx, Set<ReadEntit
         if (p == null) {
           continue;
         }
-        String path = p.toString();
         if (LOG.isDebugEnabled()) {
-          LOG.debug("Adding " + path + " of table " + alias_id);
+          LOG.debug("Adding " + p.toString() + " of table " + alias_id);
         }
 
         partDir.add(p);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -5678,7 +5678,7 @@ Map<ASTNode, RexNode> genAllRexNode(ASTNode expr, RowResolver input,
     for (ASTNode node : fieldDescList) {
       Map<ASTNode, String> map = translateFieldDesc(node);
       for (Entry<ASTNode, String> entry : map.entrySet()) {
-        unparseTranslator.addTranslation(entry.getKey(), entry.getValue());
+        unparseTranslator.addTranslation(entry.getKey(), entry.getValue().toLowerCase());
       }
     }
 

File: hplsql/src/main/java/org/apache/hive/hplsql/Arguments.java
Patch:
@@ -39,7 +39,7 @@ public class Arguments {
   Map<String, String> vars = new HashMap<String, String>();
   
   @SuppressWarnings("static-access")
-  Arguments() {
+  public Arguments() {
     // -e 'query'
     options.addOption(OptionBuilder
         .hasArg()

File: hplsql/src/main/java/org/apache/hive/hplsql/Copy.java
Patch:
@@ -400,12 +400,12 @@ void initOptions(HplsqlParser.Copy_stmtContext ctx) {
       else if (option.T_SQLINSERT() != null) {
         sqlInsert = true;
         delimiter = ", ";
-        if (option.ident() != null) {
-          sqlInsertName = option.ident().getText();
+        if (option.qident() != null) {
+          sqlInsertName = option.qident().getText();
         }
       }
       else if (option.T_AT() != null) {
-        targetConn = option.ident().getText();
+        targetConn = option.qident().getText();
         if (ctx.copy_target().expr() != null) {
           sqlInsertName = evalPop(ctx.copy_target().expr()).toString();
         }

File: hplsql/src/main/java/org/apache/hive/hplsql/Expression.java
Patch:
@@ -68,7 +68,7 @@ else if (ctx.interval_item() != null) {
       exec.signal(e);
     }
   }
-  
+
   /**
    * Evaluate an expression in executable SQL statement
    */

File: hplsql/src/main/java/org/apache/hive/hplsql/Package.java
Patch:
@@ -26,10 +26,10 @@
 import java.util.Map;
 
 import org.antlr.v4.runtime.ParserRuleContext;
-import org.apache.hive.hplsql.HplsqlParser.Package_spec_itemContext;
-import org.apache.hive.hplsql.HplsqlParser.Package_body_itemContext;
 import org.apache.hive.hplsql.HplsqlParser.Create_function_stmtContext;
 import org.apache.hive.hplsql.HplsqlParser.Create_procedure_stmtContext;
+import org.apache.hive.hplsql.HplsqlParser.Package_body_itemContext;
+import org.apache.hive.hplsql.HplsqlParser.Package_spec_itemContext;
 import org.apache.hive.hplsql.functions.BuiltinFunctions;
 import org.apache.hive.hplsql.functions.InMemoryFunctionRegistry;
 

File: hplsql/src/main/java/org/apache/hive/hplsql/Signal.java
Patch:
@@ -22,7 +22,7 @@
  * Signals and exceptions
  */
 public class Signal {
-  public enum Type { LEAVE_LOOP, LEAVE_ROUTINE, LEAVE_PROGRAM, SQLEXCEPTION, NOTFOUND, UNSUPPORTED_OPERATION, USERDEFINED, VALIDATION}
+  public enum Type { LEAVE_LOOP, LEAVE_ROUTINE, LEAVE_PROGRAM, SQLEXCEPTION, NOTFOUND, TOO_MANY_ROWS, UNSUPPORTED_OPERATION, USERDEFINED, VALIDATION}
   Type type;
   String value = "";
   Exception exception = null;

File: service/src/java/org/apache/hive/service/cli/operation/ExecuteStatementOperation.java
Patch:
@@ -24,17 +24,16 @@
 import java.sql.SQLException;
 import java.util.Map;
 
-import org.apache.hive.service.cli.operation.hplsql.BeelineConsole;
 import org.apache.hadoop.hive.ql.processors.CommandProcessor;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hive.common.util.HiveStringUtils;
 import org.apache.hive.hplsql.Conf;
 import org.apache.hive.hplsql.Exec;
 import org.apache.hive.hplsql.HplSqlSessionState;
-import org.apache.hive.hplsql.ResultListener;
 import org.apache.hive.service.cli.HiveSQLException;
 import org.apache.hive.service.cli.OperationType;
+import org.apache.hive.service.cli.operation.hplsql.BeelineConsole;
 import org.apache.hive.service.cli.operation.hplsql.HplSqlOperation;
 import org.apache.hive.service.cli.operation.hplsql.HplSqlQueryExecutor;
 import org.apache.hive.service.cli.session.HiveSession;
@@ -67,11 +66,11 @@ public static ExecuteStatementOperation newExecuteStatementOperation(HiveSession
         Exec interpreter = new Exec(
                 new Conf(),
                 new BeelineConsole(),
-                ResultListener.NONE,
                 new HplSqlQueryExecutor(parentSession),
                 parentSession.getMetaStoreClient(),
                 new HiveHplSqlSessionState(SessionState.get())
         );
+        interpreter.init();
         SessionState.get().addDynamicVar(interpreter);
       }
       return new HplSqlOperation(parentSession, statement, confOverlay, runAsync, SessionState.get().getDynamicVar(Exec.class));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
Patch:
@@ -174,7 +174,7 @@ public int execute() {
           Path currentDumpPath = getCurrentDumpPath(dumpRoot, isBootstrap);
           Path hiveDumpRoot = new Path(currentDumpPath, ReplUtils.REPL_HIVE_BASE_DIR);
           // Set distCp custom name corresponding to the replication policy.
-          String mapRedCustomName = ReplUtils.getDistCpCustomName(conf);
+          String mapRedCustomName = ReplUtils.getDistCpCustomName(conf, work.dbNameOrPattern);
           conf.set(JobContext.JOB_NAME, mapRedCustomName);
           work.setCurrentDumpPath(currentDumpPath);
           work.setMetricCollector(initMetricCollection(isBootstrap, hiveDumpRoot));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java
Patch:
@@ -121,7 +121,7 @@ public int execute() {
       work.setRootTask(this);
       this.parentTasks = null;
       // Set distCp custom name corresponding to the replication policy.
-      String mapRedCustomName = ReplUtils.getDistCpCustomName(conf);
+      String mapRedCustomName = ReplUtils.getDistCpCustomName(conf, work.dbNameToLoadIn);
       conf.set(JobContext.JOB_NAME, mapRedCustomName);
       if (shouldLoadAtlasMetadata()) {
         addAtlasLoadTask();

File: ql/src/java/org/apache/hadoop/hive/ql/log/syslog/SyslogParser.java
Patch:
@@ -112,6 +112,7 @@ public class SyslogParser implements Closeable {
 
   private InputStream in;
   private boolean parseTag;
+  private static final TimeZone UTC = TimeZone.getTimeZone("UTC");
   private static final Charset UTF8 = StandardCharsets.UTF_8;
   private Charset charset;
 
@@ -252,7 +253,7 @@ public List<Object> readEvent() throws IOException {
         }
       }
 
-      cal = new GregorianCalendar(TimeZone.getTimeZone("UTC"), Locale.getDefault());
+      cal = new GregorianCalendar(UTC, Locale.getDefault());
 
       cal.set(y, m - 1, d, hh, mm, ss);
       cal.set(Calendar.MILLISECOND, (int) (subss * 1000));

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIn.java
Patch:
@@ -88,7 +88,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments)
     for (ObjectInspector oi : arguments) {
       if(!conversionHelper.updateForComparison(oi)) {
         StringBuilder sb = new StringBuilder();
-        sb.append("The arguments for IN should be the same type! Types are: {");
+        sb.append("Type mismatch: {");
         sb.append(arguments[0].getTypeName());
         sb.append(" IN (");
         for(int i=1; i<arguments.length; i++) {

File: hplsql/src/main/java/org/apache/hive/hplsql/Package.java
Patch:
@@ -129,7 +129,7 @@ public boolean execFunc(String name, HplsqlParser.Expr_func_paramsContext ctx) {
     }
     ArrayList<Var> actualParams = function.getActualCallParameters(ctx);
     exec.enterScope(Scope.Type.ROUTINE, this);
-    setCallParameters(ctx, actualParams, f.create_routine_params(), null, exec);
+    setCallParameters(name, ctx, actualParams, f.create_routine_params(), null, exec);
     visit(f.single_block_stmt());
     exec.leaveScope(); 
     return true;
@@ -157,7 +157,7 @@ public boolean execProc(String name, HplsqlParser.Expr_func_paramsContext ctx, b
       visit(p.declare_block_inplace());
     }
     if (p.create_routine_params() != null) {
-      setCallParameters(ctx, actualParams, p.create_routine_params(), out, exec);
+      setCallParameters(name, ctx, actualParams, p.create_routine_params(), out, exec);
     }
     visit(p.proc_block());
     exec.callStackPop();

File: hplsql/src/main/java/org/apache/hive/hplsql/Signal.java
Patch:
@@ -22,7 +22,7 @@
  * Signals and exceptions
  */
 public class Signal {
-  public enum Type { LEAVE_LOOP, LEAVE_ROUTINE, LEAVE_PROGRAM, SQLEXCEPTION, NOTFOUND, UNSUPPORTED_OPERATION, USERDEFINED };
+  public enum Type { LEAVE_LOOP, LEAVE_ROUTINE, LEAVE_PROGRAM, SQLEXCEPTION, NOTFOUND, UNSUPPORTED_OPERATION, USERDEFINED, VALIDATION}
   Type type;
   String value = "";
   Exception exception = null;

File: hplsql/src/main/java/org/apache/hive/hplsql/functions/HmsFunctionRegistry.java
Patch:
@@ -128,13 +128,13 @@ private void execProcOrFunc(HplsqlParser.Expr_func_paramsContext ctx, ParserRule
   private void callWithParameters(HplsqlParser.Expr_func_paramsContext ctx, ParserRuleContext procCtx, HashMap<String, Var> out, ArrayList<Var> actualParams) {
     if (procCtx instanceof HplsqlParser.Create_function_stmtContext) {
       HplsqlParser.Create_function_stmtContext func = (HplsqlParser.Create_function_stmtContext) procCtx;
-      setCallParameters(ctx, actualParams, func.create_routine_params(), null, exec);
+      setCallParameters(func.ident().getText(), ctx, actualParams, func.create_routine_params(), null, exec);
       if (func.declare_block_inplace() != null)
         exec.visit(func.declare_block_inplace());
       exec.visit(func.single_block_stmt());
     } else {
       HplsqlParser.Create_procedure_stmtContext proc = (HplsqlParser.Create_procedure_stmtContext) procCtx;
-      setCallParameters(ctx, actualParams, proc.create_routine_params(), out, exec);
+      setCallParameters(proc.ident(0).getText(), ctx, actualParams, proc.create_routine_params(), out, exec);
       exec.visit(proc.proc_block());
     }
   }
@@ -152,7 +152,7 @@ private Optional<StoredProcedure> getProcFromHMS(String name) {
     try {
       StoredProcedureRequest request = new StoredProcedureRequest(
               hplSqlSession.currentCatalog(), hplSqlSession.currentDatabase(), name);
-      return Optional.of(msc.getStoredProcedure(request));
+      return Optional.ofNullable(msc.getStoredProcedure(request));
     } catch (NoSuchObjectException e) {
       return Optional.empty();
     } catch (TException e) {

File: itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/DummyRawStoreFailEvent.java
Patch:
@@ -1436,12 +1436,12 @@ public void createOrUpdateStoredProcedure(StoredProcedure proc) throws NoSuchObj
   }
 
   @Override
-  public StoredProcedure getStoredProcedure(String catName, String db, String name) throws MetaException, NoSuchObjectException {
+  public StoredProcedure getStoredProcedure(String catName, String db, String name) throws MetaException {
     return objectStore.getStoredProcedure(catName, db, name);
   }
 
   @Override
-  public void dropStoredProcedure(String catName, String dbName, String funcName) throws MetaException, NoSuchObjectException {
+  public void dropStoredProcedure(String catName, String dbName, String funcName) throws MetaException {
     objectStore.dropStoredProcedure(catName, dbName, funcName);
   }
 

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -10560,7 +10560,7 @@ public void createOrUpdateStoredProcedure(StoredProcedure proc) throws NoSuchObj
   }
 
   @Override
-  public StoredProcedure getStoredProcedure(String catName, String db, String name) throws MetaException, NoSuchObjectException {
+  public StoredProcedure getStoredProcedure(String catName, String db, String name) throws MetaException {
     MStoredProc proc = getMStoredProcedure(catName, db, name);
     return proc == null ? null : convertToStoredProc(catName, proc);
   }
@@ -10609,7 +10609,7 @@ private StoredProcedure convertToStoredProc(String catName, MStoredProc proc) {
   }
 
   @Override
-  public void dropStoredProcedure(String catName, String dbName, String funcName) throws MetaException, NoSuchObjectException {
+  public void dropStoredProcedure(String catName, String dbName, String funcName) throws MetaException {
     boolean success = false;
     try {
       openTransaction();

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/RawStore.java
Patch:
@@ -1961,9 +1961,9 @@ void scheduledQueryProgress(ScheduledQueryProgressInfo info)
 
   void createOrUpdateStoredProcedure(StoredProcedure proc) throws NoSuchObjectException, MetaException;
 
-  StoredProcedure getStoredProcedure(String catName, String db, String name) throws MetaException, NoSuchObjectException;
+  StoredProcedure getStoredProcedure(String catName, String db, String name) throws MetaException;
 
-  void dropStoredProcedure(String catName, String dbName, String funcName) throws MetaException, NoSuchObjectException;
+  void dropStoredProcedure(String catName, String dbName, String funcName) throws MetaException;
 
   List<String> getAllStoredProcedures(ListStoredProcedureRequest request);
 

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java
Patch:
@@ -3133,12 +3133,12 @@ public void createOrUpdateStoredProcedure(StoredProcedure proc) throws NoSuchObj
   }
 
   @Override
-  public StoredProcedure getStoredProcedure(String catName, String db, String name) throws MetaException, NoSuchObjectException {
+  public StoredProcedure getStoredProcedure(String catName, String db, String name) throws MetaException {
     return rawStore.getStoredProcedure(catName, db, name);
   }
 
   @Override
-  public void dropStoredProcedure(String catName, String dbName, String funcName) throws MetaException, NoSuchObjectException {
+  public void dropStoredProcedure(String catName, String dbName, String funcName) throws MetaException {
     rawStore.dropStoredProcedure(catName, dbName, funcName);
   }
 

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/DummyRawStoreControlledCommit.java
Patch:
@@ -1383,12 +1383,12 @@ public void createOrUpdateStoredProcedure(StoredProcedure proc) throws NoSuchObj
   }
 
   @Override
-  public StoredProcedure getStoredProcedure(String catName, String db, String name) throws MetaException, NoSuchObjectException {
+  public StoredProcedure getStoredProcedure(String catName, String db, String name) throws MetaException {
     return objectStore.getStoredProcedure(catName, db, name);
   }
 
   @Override
-  public void dropStoredProcedure(String catName, String dbName, String funcName) throws MetaException, NoSuchObjectException {
+  public void dropStoredProcedure(String catName, String dbName, String funcName) throws MetaException {
     objectStore.dropStoredProcedure(catName, dbName, funcName);
   }
 

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/DummyRawStoreForJdoConnection.java
Patch:
@@ -1368,12 +1368,12 @@ public void createOrUpdateStoredProcedure(StoredProcedure proc) throws NoSuchObj
   }
 
   @Override
-  public StoredProcedure getStoredProcedure(String catName, String db, String name) throws MetaException, NoSuchObjectException {
+  public StoredProcedure getStoredProcedure(String catName, String db, String name) throws MetaException {
     return null;
   }
 
   @Override
-  public void dropStoredProcedure(String catName, String dbName, String funcName) throws MetaException, NoSuchObjectException {
+  public void dropStoredProcedure(String catName, String dbName, String funcName) throws MetaException {
 
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -2868,7 +2868,7 @@ public Map<Map<String, String>, Partition> loadDynamicPartitions(final LoadTable
             newFiles = partitionDetails.newFiles;
           } else if (conf.getBoolVar(ConfVars.FIRE_EVENTS_FOR_DML) && !tbl.isTemporary() && oldPartition == null) {
             // Otherwise only collect them, if we are going to fire write notifications
-            newFiles = new ArrayList<>();
+            newFiles = Collections.synchronizedList(new ArrayList<>());
           }
           // load the partition
           Partition partition = loadPartitionInternal(entry.getKey(), tbl,

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2616,7 +2616,7 @@ public static enum ConfVars {
         "Whether to enable shared work extended optimizer. The optimizer tries to merge equal operators\n" +
         "after a work boundary after shared work optimizer has been executed. Requires hive.optimize.shared.work\n" +
         "to be set to true. Tez only."),
-    HIVE_SHARED_WORK_SEMIJOIN_OPTIMIZATION("hive.optimize.shared.work.semijoin", true,
+    HIVE_SHARED_WORK_SEMIJOIN_OPTIMIZATION("hive.optimize.shared.work.semijoin", false,
         "Whether to enable shared work extended optimizer for semijoins. The optimizer tries to merge\n" +
         "scan operators if one of them reads the full table, even if the other one is the target for\n" +
         "one or more semijoin edges. Tez only."),

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/show/ShowCreateTableOperation.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.ddl.table.create.show;
 
 import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE;
+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.TABLE_IS_CTAS;
 
 import java.io.DataOutputStream;
 import java.io.IOException;
@@ -347,7 +348,7 @@ private String getLocationBlock(Table table) {
   }
 
   private static final Set<String> PROPERTIES_TO_IGNORE_AT_TBLPROPERTIES = Sets.union(
-      ImmutableSet.<String>of("TEMPORARY", "EXTERNAL", "comment", "SORTBUCKETCOLSPREFIX", META_TABLE_STORAGE),
+      ImmutableSet.<String>of("TEMPORARY", "EXTERNAL", "comment", "SORTBUCKETCOLSPREFIX", META_TABLE_STORAGE, TABLE_IS_CTAS),
       new HashSet<String>(StatsSetupConst.TABLE_PARAMS_STATS_KEYS));
 
   private String getProperties(Table table) {

File: standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/hive_metastoreConstants.java
Patch:
@@ -61,6 +61,8 @@
 
   public static final java.lang.String TABLE_IS_TRANSACTIONAL = "transactional";
 
+  public static final java.lang.String TABLE_IS_CTAS = "created_with_ctas";
+
   public static final java.lang.String TABLE_NO_AUTO_COMPACT = "no_auto_compaction";
 
   public static final java.lang.String TABLE_TRANSACTIONAL_PROPERTIES = "transactional_properties";

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java
Patch:
@@ -166,7 +166,7 @@ public enum EventType {
 
   public enum OtherInfoType {
     QUERY, STATUS, TEZ, MAPRED, INVOKER_INFO, SESSION_ID, THREAD_NAME, VERSION, CLIENT_IP_ADDRESS,
-    HIVE_ADDRESS, HIVE_INSTANCE_TYPE, CONF, PERF, LLAP_APP_ID
+    HIVE_ADDRESS, HIVE_INSTANCE_TYPE, CONF, PERF, LLAP_APP_ID, ERROR_MESSAGE
   }
 
   public enum ExecutionMode {
@@ -441,6 +441,7 @@ private HiveHookEventProtoPartialBuilder getPostHookEvent(HookContext hookContex
         builder.setOperationId(hookContext.getOperationId());
       }
       addMapEntry(builder, OtherInfoType.STATUS, Boolean.toString(success));
+      addMapEntry(builder, OtherInfoType.ERROR_MESSAGE, hookContext.getErrorMessage());
       JSONObject perfObj = new JSONObject();
       for (String key : hookContext.getPerfLogger().getEndTimes().keySet()) {
         perfObj.put(key, hookContext.getPerfLogger().getDuration(key));

File: ql/src/test/org/apache/hadoop/hive/ql/hooks/TestHiveProtoLoggingHook.java
Patch:
@@ -234,6 +234,7 @@ public void testPostEventLog() throws Exception {
   @Test
   public void testFailureEventLog() throws Exception {
     context.setHookType(HookType.ON_FAILURE_HOOK);
+    context.setErrorMessage("test_errormessage");
 
     EventLogger evtLogger = new EventLogger(conf, SystemClock.getInstance());
     evtLogger.handle(context);
@@ -247,6 +248,7 @@ public void testFailureEventLog() throws Exception {
     Assert.assertEquals("test_op_id", event.getOperationId());
 
     assertOtherInfo(event, OtherInfoType.STATUS, Boolean.FALSE.toString());
+    assertOtherInfo(event, OtherInfoType.ERROR_MESSAGE, "test_errormessage");
     assertOtherInfo(event, OtherInfoType.PERF, null);
   }
 

File: common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -608,8 +608,6 @@ public enum ErrorMsg {
   SPARK_JOB_RUNTIME_ERROR(40001, "Spark job failed due to: {0}", true),
   SPARK_TASK_RUNTIME_ERROR(40002, "Spark job failed due to task failures: {0}", true),
   REPL_DATABASE_IS_TARGET_OF_REPLICATION(40003, "Cannot dump database as it is a Target of replication."),
-  REPL_DATABASE_IS_NOT_SOURCE_OF_REPLICATION(40004,
-                                               "Source of replication (repl.source.for) is not set in the database properties."),
   REPL_INVALID_DB_OR_TABLE_PATTERN(40005,
                                      "Invalid pattern for the DB or table name in the replication policy. "
                                      + "It should be a valid regex enclosed within single or double quotes."),

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java
Patch:
@@ -94,7 +94,7 @@ public static String replaceNameserviceInEncodedURI(String cmEncodedURI, HiveCon
     return modifiedURI;
   }
 
-  private static String replaceHost(String originalURIStr, String newHost) throws SemanticException {
+  public static String replaceHost(String originalURIStr, String newHost) throws SemanticException {
     if (StringUtils.isEmpty(originalURIStr)) {
       return originalURIStr;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveHooks.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hadoop.hive.ql.hooks;
 
-import com.cronutils.utils.VisibleForTesting;
+import com.google.common.annotations.VisibleForTesting;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.session.SessionState;

File: service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
Patch:
@@ -395,8 +395,8 @@ private synchronized void cleanup(OperationState state) throws HiveSQLException
         String queryId = queryState.getQueryId();
         if (success) {
           log.info("The running operation has been successfully interrupted: {}", queryId);
-        } else {
-          log.info("The running operation could not be cancelled, typically because it has already completed normally: {}", queryId);
+        } else if (log.isDebugEnabled()) {
+          log.debug("The running operation could not be cancelled, typically because it has already completed normally: {}", queryId);
         }
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java
Patch:
@@ -247,6 +247,8 @@ public static ASTNode literal(RexLiteral literal) {
     case INTERVAL_SECOND:
     case INTERVAL_YEAR:
     case INTERVAL_YEAR_MONTH:
+    case MAP:
+    case ARRAY:
     case ROW:
       if (literal.getValue() == null) {
         return ASTBuilder.construct(HiveParser.TOK_NULL, "TOK_NULL").node();

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeBitVectorHLL.java
Patch:
@@ -23,9 +23,9 @@
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedUDAFs;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFComputeBitVectorDecimal;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFComputeBitVectorDouble;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFComputeBitVectorFinal;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFComputeBitVectorLong;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFComputeBitVectorDouble;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFComputeBitVectorFinal;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFComputeBitVectorLong;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFComputeBitVectorString;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFComputeBitVectorTimestamp;
 import org.apache.hadoop.hive.ql.metadata.HiveException;

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExclusiveReplica.java
Patch:
@@ -138,7 +138,7 @@ public void testTableLevelReplicationWithRemoteStaging() throws Throwable {
             .run("insert into table t1 values (100)")
             .run("create table t2 (id int)")
             .run("insert into table t2 values (200)")
-            .dump(primaryDbName +".'t[0-9]+'", null, withClauseOptions);
+            .dump(primaryDbName +".'t[0-9]+'", withClauseOptions);
 
     // verify that the external table info is written correctly for bootstrap
     assertExternalFileInfo(Arrays.asList("t1"), tuple.dumpLocation, false, replica);
@@ -166,7 +166,7 @@ public void testTableLevelReplicationWithRemoteStaging() throws Throwable {
             .run("create table t5 (id int) partitioned by (p int)")
             .run("insert into t5 partition(p=1) values(10)")
             .run("insert into t5 partition(p=2) values(20)")
-            .dump(primaryDbName + ".'t[0-9]+'", null, withClauseOptions);
+            .dump(primaryDbName + ".'t[0-9]+'", withClauseOptions);
 
     // verify that the external table info is written correctly for incremental
     assertExternalFileInfo(Arrays.asList("t1", "t3"), tuple.dumpLocation, true, replica);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AlterTableHandler.java
Patch:
@@ -210,7 +210,7 @@ public void handle(Context withinContext) throws Exception {
       String oldName = before.getTableName();
       String newName = after.getTableName();
       boolean needDump = true;
-      if (withinContext.oldReplScope != null) {
+      if (withinContext.oldReplScope != null && !withinContext.oldReplScope.equals(withinContext.replScope)) {
         needDump = handleRenameForReplacePolicy(withinContext, oldName, newName);
       } else if (!withinContext.replScope.includeAllTables()) {
         needDump = handleRenameForTableLevelReplication(withinContext, oldName, newName);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
Patch:
@@ -809,7 +809,7 @@ private void dumpTableListToDumpLocation(List<String> tableList, Path dbRoot, St
       retryable.executeCallable((Callable<Void>) () -> {
         Path tableListFile = new Path(dbRoot, ReplUtils.REPL_TABLE_LIST_DIR_NAME);
         tableListFile = new Path(tableListFile, dbName.toLowerCase());
-        FSDataOutputStream writer = FileSystem.get(hiveConf).create(tableListFile);
+        FSDataOutputStream writer = tableListFile.getFileSystem(hiveConf).create(tableListFile);
         for (String tableName : tableList) {
           String line = tableName.toLowerCase().concat("\n");
           writer.write(line.getBytes(StandardCharsets.UTF_8));

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java
Patch:
@@ -98,12 +98,12 @@ private static String replaceHost(String originalURIStr, String newHost) throws
     if (StringUtils.isEmpty(originalURIStr)) {
       return originalURIStr;
     }
-    URI origUri = URI.create(originalURIStr);
     try {
-      return new URI(origUri.getScheme(),
+      URI origUri = new Path(originalURIStr).toUri();
+      return new Path(new URI(origUri.getScheme(),
               origUri.getUserInfo(), newHost, origUri.getPort(),
               origUri.getPath(), origUri.getQuery(),
-              origUri.getFragment()).toString();
+              origUri.getFragment())).toString();
     } catch (URISyntaxException ex) {
       throw new SemanticException(ex);
     }

File: common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -446,8 +446,8 @@ public enum ErrorMsg {
   MATERIALIZED_VIEW_DEF_EMPTY(10403, "Query for the materialized view rebuild could not be retrieved"),
   MERGE_PREDIACTE_REQUIRED(10404, "MERGE statement with both UPDATE and DELETE clauses " +
     "requires \"AND <boolean>\" on the 1st WHEN MATCHED clause of <{0}>", true),
-  MERGE_TOO_MANY_DELETE(10405, "MERGE statment can have at most 1 WHEN MATCHED ... DELETE clause: <{0}>", true),
-  MERGE_TOO_MANY_UPDATE(10406, "MERGE statment can have at most 1 WHEN MATCHED ... UPDATE clause: <{0}>", true),
+  MERGE_TOO_MANY_DELETE(10405, "MERGE statement can have at most 1 WHEN MATCHED ... DELETE clause: <{0}>", true),
+  MERGE_TOO_MANY_UPDATE(10406, "MERGE statement can have at most 1 WHEN MATCHED ... UPDATE clause: <{0}>", true),
   INVALID_JOIN_CONDITION(10407, "Error parsing condition in join"),
   INVALID_TARGET_COLUMN_IN_SET_CLAUSE(10408, "Target column \"{0}\" of set clause is not found in table \"{1}\".", true),
   HIVE_GROUPING_FUNCTION_EXPR_NOT_IN_GROUPBY(10409, "Expression in GROUPING function not present in GROUP BY"),

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -1253,7 +1253,7 @@ private void updateReplId(Connection dbConn, ReplLastIdInfo replLastIdInfo) thro
    * as a logical time counter. If S.commitTime < T.startTime, T and S do NOT overlap.
    *
    * Motivating example:
-   * Suppose we have multi-statment transactions T and S both of which are attempting x = x + 1
+   * Suppose we have multi-statement transactions T and S both of which are attempting x = x + 1
    * In order to prevent lost update problem, then the non-overlapping txns must lock in the snapshot
    * that they read appropriately. In particular, if txns do not overlap, then one follows the other
    * (assuming they write the same entity), and thus the 2nd must see changes of the 1st.  We ensure

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -1022,8 +1022,8 @@ public static TypeInfo getCommonClassForStruct(StructTypeInfo a, StructTypeInfo
     }
 
     // Compare the field types
-    ArrayList<TypeInfo> fromTypes = a.getAllStructFieldTypeInfos();
-    ArrayList<TypeInfo> toTypes = b.getAllStructFieldTypeInfos();
+    List<TypeInfo> fromTypes = a.getAllStructFieldTypeInfos();
+    List<TypeInfo> toTypes = b.getAllStructFieldTypeInfos();
     for (int i = 0; i < fromTypes.size(); i++) {
       TypeInfo commonType = commonClassFunction.apply(fromTypes.get(i), toTypes.get(i));
       if (commonType == null) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java
Patch:
@@ -344,7 +344,7 @@ private Field allocateComplexField(TypeInfo sourceTypeInfo) {
     case STRUCT:
       {
         final StructTypeInfo structTypeInfo = (StructTypeInfo) sourceTypeInfo;
-        final ArrayList<TypeInfo> fieldTypeInfoList = structTypeInfo.getAllStructFieldTypeInfos();
+        final List<TypeInfo> fieldTypeInfoList = structTypeInfo.getAllStructFieldTypeInfos();
         final int count = fieldTypeInfoList.size();
         final Field[] fields = new Field[count];
         for (int i = 0; i < count; i++) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
Patch:
@@ -2680,7 +2680,7 @@ private VectorExpression getStructInExpression(List<ExprNodeDesc> childExpr, Exp
 
     StructTypeInfo structTypeInfo = (StructTypeInfo) colTypeInfo;
 
-    ArrayList<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
+    List<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
     final int fieldCount = fieldTypeInfos.size();
     ColumnVector.Type[] fieldVectorColumnTypes = new ColumnVector.Type[fieldCount];
     InConstantType[] fieldInConstantTypes = new InConstantType[fieldCount];

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java
Patch:
@@ -474,7 +474,7 @@ public Object getValue() {
 
   public void setStructValue(Object structValue) throws HiveException {
     StructTypeInfo structTypeInfo = (StructTypeInfo) outputTypeInfo;
-    ArrayList<TypeInfo> fieldTypeInfoList = structTypeInfo.getAllStructFieldTypeInfos();
+    List<TypeInfo> fieldTypeInfoList = structTypeInfo.getAllStructFieldTypeInfos();
     final int size = fieldTypeInfoList.size();
     this.structValue = new ConstantVectorExpression[size];
     List<Object> fieldValueList = (List<Object>) structValue;

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java
Patch:
@@ -189,8 +189,8 @@ protected OrcStructInspector() {
     }
 
     OrcStructInspector(StructTypeInfo info) {
-      ArrayList<String> fieldNames = info.getAllStructFieldNames();
-      ArrayList<TypeInfo> fieldTypes = info.getAllStructFieldTypeInfos();
+      List<String> fieldNames = info.getAllStructFieldNames();
+      List<TypeInfo> fieldTypes = info.getAllStructFieldTypeInfos();
       fields = new ArrayList<StructField>(fieldNames.size());
       for(int i=0; i < fieldNames.size(); ++i) {
         fields.add(new Field(fieldNames.get(i),

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -3120,9 +3120,8 @@ private boolean validateStructInExpression(ExprNodeDesc desc,
       }
       StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;
 
-      ArrayList<TypeInfo> fieldTypeInfos = structTypeInfo
-          .getAllStructFieldTypeInfos();
-      ArrayList<String> fieldNames = structTypeInfo.getAllStructFieldNames();
+      List<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
+      List<String> fieldNames = structTypeInfo.getAllStructFieldNames();
       final int fieldCount = fieldTypeInfos.size();
       for (int f = 0; f < fieldCount; f++) {
         TypeInfo fieldTypeInfo = fieldTypeInfos.get(f);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/VectorVerifyFast.java
Patch:
@@ -479,8 +479,8 @@ public static void serializeWrite(SerializeWrite serializeWrite,
     case STRUCT:
     {
       StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;
-      ArrayList<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
-      ArrayList<Object> fieldValues = (ArrayList<Object>) object;
+      List<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
+      List<Object> fieldValues = (List<Object>) object;
       final int size = fieldValues.size();
       serializeWrite.beginStruct(fieldValues);
       boolean isFirst = true;
@@ -647,7 +647,7 @@ private static Object getComplexField(DeserializeRead deserializeRead,
     case STRUCT:
     {
       StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;
-      ArrayList<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
+      List<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
       final int size = fieldTypeInfos.size();
       ArrayList<Object> fieldValues = new ArrayList<Object>();
       Object fieldObj;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VerifyFastRow.java
Patch:
@@ -483,8 +483,8 @@ public static void serializeWrite(SerializeWrite serializeWrite,
     case STRUCT:
       {
         StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;
-        ArrayList<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
-        ArrayList<Object> fieldValues = (ArrayList<Object>) object;
+        List<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
+        List<Object> fieldValues = (List<Object>) object;
         final int size = fieldValues.size();
         serializeWrite.beginStruct(fieldValues);
         boolean isFirst = true;
@@ -651,7 +651,7 @@ private static Object getComplexField(DeserializeRead deserializeRead,
     case STRUCT:
       {
         StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;
-        ArrayList<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
+        List<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
         final int size = fieldTypeInfos.size();
         ArrayList<Object> fieldValues = new ArrayList<Object>();
         Object fieldObj;

File: serde/src/java/org/apache/hadoop/hive/serde2/MetadataTypedColumnsetSerDe.java
Patch:
@@ -66,7 +66,7 @@ public class MetadataTypedColumnsetSerDe extends AbstractSerDe {
 
   @Override
   public String toString() {
-    return "MetaDataTypedColumnsetSerDe[" + separator + "," + columnNames + "]";
+    return "MetaDataTypedColumnsetSerDe[" + separator + "," + columnNames + "," + super.toString() + "]";
   }
 
   public MetadataTypedColumnsetSerDe() throws SerDeException {

File: serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java
Patch:
@@ -394,8 +394,8 @@ private Object deserializePrimitive(Object datum, Schema fileSchema, Schema reco
   private Object deserializeStruct(GenericData.Record datum, Schema fileSchema, StructTypeInfo columnType)
           throws AvroSerdeException {
     // No equivalent Java type for the backing structure, need to recurse and build a list
-    ArrayList<TypeInfo> innerFieldTypes = columnType.getAllStructFieldTypeInfos();
-    ArrayList<String> innerFieldNames = columnType.getAllStructFieldNames();
+    List<TypeInfo> innerFieldTypes = columnType.getAllStructFieldTypeInfos();
+    List<String> innerFieldNames = columnType.getAllStructFieldNames();
     List<Object> innerObjectRow = new ArrayList<Object>(innerFieldTypes.size());
 
     return workerBase(innerObjectRow, fileSchema, innerFieldNames, innerFieldTypes, datum);

File: serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java
Patch:
@@ -18,7 +18,6 @@
 package org.apache.hadoop.hive.serde2.avro;
 
 import java.time.ZoneOffset;
-import java.util.ArrayList;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
@@ -185,7 +184,7 @@ private Object serializeStruct(StructTypeInfo typeInfo, StructObjectInspector ss
     List<? extends StructField> allStructFieldRefs = ssoi.getAllStructFieldRefs();
     List<Object> structFieldsDataAsList = ssoi.getStructFieldsDataAsList(o);
     GenericData.Record record = new GenericData.Record(schema);
-    ArrayList<TypeInfo> allStructFieldTypeInfos = typeInfo.getAllStructFieldTypeInfos();
+    List<TypeInfo> allStructFieldTypeInfos = typeInfo.getAllStructFieldTypeInfos();
 
     for(int i  = 0; i < size; i++) {
       Field field = schema.getFields().get(i);

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/VerifyLazy.java
Patch:
@@ -91,7 +91,7 @@ public static boolean lazyCompareMap(MapTypeInfo mapTypeInfo, Map<Object, Object
   }
 
   public static boolean lazyCompareStruct(StructTypeInfo structTypeInfo, List<Object> fields, List<Object> expectedFields) {
-    ArrayList<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
+    List<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
     final int size = fieldTypeInfos.size();
     for (int i = 0; i < size; i++) {
       Object lazyEleObj = fields.get(i);

File: serde/src/test/org/apache/hadoop/hive/serde2/VerifyFast.java
Patch:
@@ -483,8 +483,8 @@ public static void serializeWrite(SerializeWrite serializeWrite,
     case STRUCT:
       {
         StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;
-        ArrayList<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
-        ArrayList<Object> fieldValues = (ArrayList<Object>) object;
+        List<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
+        List<Object> fieldValues = (List<Object>) object;
         final int size = fieldValues.size();
         serializeWrite.beginStruct(fieldValues);
         boolean isFirst = true;
@@ -651,7 +651,7 @@ private static Object getComplexField(DeserializeRead deserializeRead,
     case STRUCT:
       {
         StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;
-        ArrayList<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
+        List<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
         final int size = fieldTypeInfos.size();
         ArrayList<Object> fieldValues = new ArrayList<Object>();
         Object fieldObj;

File: serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java
Patch:
@@ -21,7 +21,6 @@
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
-import java.util.ArrayList;
 import java.util.List;
 
 import org.apache.avro.Schema;
@@ -468,8 +467,8 @@ public void canHandleRecords() throws SerDeException {
     StructTypeInfo structTypeInfo = (StructTypeInfo)typeInfo;
 
     // Check individual elements of subrecord
-    ArrayList<String> allStructFieldNames = structTypeInfo.getAllStructFieldNames();
-    ArrayList<TypeInfo> allStructFieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
+    List<String> allStructFieldNames = structTypeInfo.getAllStructFieldNames();
+    List<TypeInfo> allStructFieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
     assertEquals(allStructFieldNames.size(), 3);
     String[] names = new String[]{"int1", "boolean1", "long1"};
     String [] typeInfoStrings = new String [] {"int", "boolean", "bigint"};

File: ql/src/test/org/apache/hadoop/hive/ql/security/authorization/plugin/TestHivePrivilegeObjectOwnerNameAndType.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.commons.lang3.tuple.Pair;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.metastore.utils.TestTxnDbUtil;
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
 import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;
@@ -77,6 +78,7 @@ public static void beforeTest() throws Exception {
     conf.setVar(ConfVars.HIVE_TXN_MANAGER, DbTxnManager.class.getName());
     conf.setVar(ConfVars.HIVEMAPREDMODE, "nonstrict");
 
+    TestTxnDbUtil.prepDb(conf);
     SessionState.start(conf);
     driver = new Driver(conf);
     runCmd("create table " + TABLE_NAME + " (i int, j int, k string) partitioned by (city string, `date` string) ");

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -13070,7 +13070,7 @@ Map<ASTNode, ExprNodeDesc> genAllExprNodeDesc(ASTNode expr, RowResolver input,
     for (ASTNode node : fieldDescList) {
       Map<ASTNode, String> map = translateFieldDesc(node);
       for (Entry<ASTNode, String> entry : map.entrySet()) {
-        unparseTranslator.addTranslation(entry.getKey(), entry.getValue());
+        unparseTranslator.addTranslation(entry.getKey(), entry.getValue().toLowerCase());
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/RuntimeStatsPersistenceCheckerHook.java
Patch:
@@ -59,7 +59,7 @@ public void run(HookContext hookContext) throws Exception {
         throw new RuntimeException("while checking the signature of: " + sig.getSig(), e);
       }
     }
-    LOG.info("signature checked: " + sigs.size());
+    LOG.debug("signature checked: " + sigs.size());
   }
 
   private <T> T persistenceLoop(T sig, Class<T> clazz) throws IOException {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketVersionPopulator.java
Patch:
@@ -181,7 +181,7 @@ List<OperatorBucketingVersionInfo> getBucketingVersions() {
           if (numBuckets > 1) {
             ret.add(new OperatorBucketingVersionInfo(operator, bucketingVersion));
           } else {
-            LOG.info("not considering bucketingVersion for: %s because it has %d<2 buckets ", tso, numBuckets);
+            LOG.info("not considering bucketingVersion for: {} because it has {}<2 buckets ", tso, numBuckets);
           }
         }
         if (operator instanceof FileSinkOperator) {

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -7054,7 +7054,7 @@ private int get_num_partitions_by_expr(final String catName, final String dbName
     public List<Partition> get_partitions_by_names(final String dbName, final String tblName,
                                                    final List<String> partNames)
             throws TException {
-      return get_partitions_by_names(dbName, tblName, partNames);
+      return get_partitions_by_names(dbName, tblName, partNames, false, null, null);
     }
 
     @Override

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliAdapter.java
Patch:
@@ -42,7 +42,7 @@ public abstract class CliAdapter {
 
   public CliAdapter(AbstractCliConfig cliConfig) {
     this.cliConfig = cliConfig;
-    metaStoreHandler = new QTestMetaStoreHandler();
+    metaStoreHandler = new QTestMetaStoreHandler(cliConfig.getMetastoreType());
   }
 
   public final List<Object[]> getParameters() throws Exception {

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java
Patch:
@@ -425,7 +425,9 @@ public Database getDatabase(String catName, String dbName) throws MetaException{
       db.setOwnerType(
           (null == type || type.trim().isEmpty()) ? null : PrincipalType.valueOf(type));
       db.setCatalogName(MetastoreDirectSqlUtils.extractSqlString(dbline[6]));
-      db.setCreateTime(MetastoreDirectSqlUtils.extractSqlInt(dbline[7]));
+      if (dbline[7] != null) {
+        db.setCreateTime(MetastoreDirectSqlUtils.extractSqlInt(dbline[7]));
+      }
       db.setManagedLocationUri(MetastoreDirectSqlUtils.extractSqlString(dbline[8]));
       db.setParameters(MetaStoreServerUtils.trimMapNulls(dbParams,convertMapNullsToEmptyStrings));
       if (LOG.isDebugEnabled()){

File: serde/src/java/org/apache/hadoop/hive/serde2/avro/TypeInfoToSchema.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.avro;
 
+import org.apache.avro.JsonProperties;
 import org.apache.avro.Schema;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;
@@ -73,7 +74,7 @@ public Schema convert(List<String> columnNames, List<TypeInfo> columnTypes,
   }
 
   private Schema.Field createAvroField(String name, TypeInfo typeInfo, String comment) {
-    return new Schema.Field(name, createAvroSchema(typeInfo), comment, null);
+    return new Schema.Field(name, createAvroSchema(typeInfo), comment, JsonProperties.NULL_VALUE);
   }
 
   private Schema createAvroSchema(TypeInfo typeInfo) {
@@ -235,7 +236,7 @@ private Schema createAvroArray(TypeInfo typeInfo) {
   private List<Schema.Field> getFields(Schema.Field schemaField) {
     List<Schema.Field> fields = new ArrayList<Schema.Field>();
 
-    JsonNode nullDefault = JsonNodeFactory.instance.nullNode();
+    JsonProperties.Null nullDefault = JsonProperties.NULL_VALUE;
     if (schemaField.schema().getType() == Schema.Type.RECORD) {
       for (Schema.Field field : schemaField.schema().getFields()) {
         fields.add(new Schema.Field(field.name(), field.schema(), field.doc(), nullDefault));

File: serde/src/java/org/apache/hadoop/hive/serde2/avro/TypeInfoToSchema.java
Patch:
@@ -17,7 +17,6 @@
  */
 package org.apache.hadoop.hive.serde2.avro;
 
-import org.apache.avro.JsonProperties;
 import org.apache.avro.Schema;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;
@@ -74,7 +73,7 @@ public Schema convert(List<String> columnNames, List<TypeInfo> columnTypes,
   }
 
   private Schema.Field createAvroField(String name, TypeInfo typeInfo, String comment) {
-    return new Schema.Field(name, createAvroSchema(typeInfo), comment, JsonProperties.NULL_VALUE);
+    return new Schema.Field(name, createAvroSchema(typeInfo), comment, null);
   }
 
   private Schema createAvroSchema(TypeInfo typeInfo) {
@@ -236,7 +235,7 @@ private Schema createAvroArray(TypeInfo typeInfo) {
   private List<Schema.Field> getFields(Schema.Field schemaField) {
     List<Schema.Field> fields = new ArrayList<Schema.Field>();
 
-    JsonProperties.Null nullDefault = JsonProperties.NULL_VALUE;
+    JsonNode nullDefault = JsonNodeFactory.instance.nullNode();
     if (schemaField.schema().getType() == Schema.Type.RECORD) {
       for (Schema.Field field : schemaField.schema().getFields()) {
         fields.add(new Schema.Field(field.name(), field.schema(), field.doc(), nullDefault));

File: serde/src/java/org/apache/hadoop/hive/serde2/avro/TypeInfoToSchema.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.avro;
 
+import org.apache.avro.JsonProperties;
 import org.apache.avro.Schema;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;
@@ -73,7 +74,7 @@ public Schema convert(List<String> columnNames, List<TypeInfo> columnTypes,
   }
 
   private Schema.Field createAvroField(String name, TypeInfo typeInfo, String comment) {
-    return new Schema.Field(name, createAvroSchema(typeInfo), comment, null);
+    return new Schema.Field(name, createAvroSchema(typeInfo), comment, JsonProperties.NULL_VALUE);
   }
 
   private Schema createAvroSchema(TypeInfo typeInfo) {
@@ -235,7 +236,7 @@ private Schema createAvroArray(TypeInfo typeInfo) {
   private List<Schema.Field> getFields(Schema.Field schemaField) {
     List<Schema.Field> fields = new ArrayList<Schema.Field>();
 
-    JsonNode nullDefault = JsonNodeFactory.instance.nullNode();
+    JsonProperties.Null nullDefault = JsonProperties.NULL_VALUE;
     if (schemaField.schema().getType() == Schema.Type.RECORD) {
       for (Schema.Field field : schemaField.schema().getFields()) {
         fields.add(new Schema.Field(field.name(), field.schema(), field.doc(), nullDefault));

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4917,8 +4917,9 @@ public static enum ConfVars {
       "Sleep duration (in milliseconds) to wait before retrying on error when obtaining a\n" +
       "connection to LLAP daemon from Tez AM.",
       "llap.task.communicator.connection.sleep-between-retries-millis"),
-    LLAP_TASK_UMBILICAL_SERVER_PORT("hive.llap.daemon.umbilical.port", 0,
-      "LLAP task umbilical server RPC port"),
+    LLAP_TASK_UMBILICAL_SERVER_PORT("hive.llap.daemon.umbilical.port", "0",
+      "LLAP task umbilical server RPC port or range of ports to try in case "
+          + "the first port is occupied"),
     LLAP_DAEMON_WEB_PORT("hive.llap.daemon.web.port", 15002, "LLAP daemon web UI port.",
       "llap.daemon.service.port"),
     LLAP_DAEMON_WEB_SSL("hive.llap.daemon.web.ssl", false,

File: jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java
Patch:
@@ -412,7 +412,7 @@ public ResultSet getFunctions(String catalogName, String schemaPattern, String f
   }
 
   public String getIdentifierQuoteString() throws SQLException {
-    return " ";
+    return "`";
   }
 
   public ResultSet getImportedKeys(String catalog, String schema, String table)

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliAdapter.java
Patch:
@@ -42,7 +42,7 @@ public abstract class CliAdapter {
 
   public CliAdapter(AbstractCliConfig cliConfig) {
     this.cliConfig = cliConfig;
-    metaStoreHandler = new QTestMetaStoreHandler();
+    metaStoreHandler = new QTestMetaStoreHandler(cliConfig.getMetastoreType());
   }
 
   public final List<Object[]> getParameters() throws Exception {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3113,8 +3113,8 @@ public static enum ConfVars {
     HIVE_COMPACTOR_COMPACT_MM("hive.compactor.compact.insert.only", true,
         "Whether the compactor should compact insert-only tables. A safety switch."),
     COMPACTOR_CRUD_QUERY_BASED("hive.compactor.crud.query.based", false,
-        "Means Major compaction on full CRUD tables is done as a query, "
-        + "and minor compaction will be disabled."),
+        "Means compaction on full CRUD tables is done via queries. "
+        + "Compactions on insert-only tables will always run via queries regardless of the value of this configuration."),
     SPLIT_GROUPING_MODE("hive.split.grouping.mode", "query", new StringSet("query", "compactor"),
         "This is set to compactor from within the query based compactor. This enables the Tez SplitGrouper "
         + "to group splits based on their bucket number, so that all rows from different bucket files "

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java
Patch:
@@ -495,12 +495,11 @@ public TezSessionState reopen(TezSessionState sessionState) throws Exception {
 
   static void reopenInternal(
       TezSessionState sessionState) throws Exception {
-    HiveResources resources = sessionState.extractHiveResources();
     // TODO: close basically resets the object to a bunch of nulls.
     //       We should ideally not reuse the object because it's pointless and error-prone.
-    sessionState.close(false);
+    sessionState.close(true);
     // Note: scratchdir is reused implicitly because the sessionId is the same.
-    sessionState.open(resources);
+    sessionState.open(sessionState.extractHiveResources());
   }
 
 

File: ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionPool.java
Patch:
@@ -191,7 +191,7 @@ public void testSessionReopen() {
 
       poolManager.reopen(session);
 
-      Mockito.verify(session).close(false);
+      Mockito.verify(session).close(true);
       Mockito.verify(session).open(Mockito.<TezSessionState.HiveResources>any());
 
       // mocked session starts with default queue
@@ -329,7 +329,7 @@ public void testCloseAndOpenDefault() throws Exception {
 
     poolManager.reopen(session);
 
-    Mockito.verify(session).close(false);
+    Mockito.verify(session).close(true);
     Mockito.verify(session).open(Mockito.<TezSessionState.HiveResources>any());
   }
 

File: service/src/java/org/apache/hive/service/server/HiveServer2.java
Patch:
@@ -360,6 +360,9 @@ public synchronized void init(HiveConf hiveConf) {
             builder.setKeyStorePassword(ShimLoader.getHadoopShims().getPassword(
               hiveConf, ConfVars.HIVE_SERVER2_WEBUI_SSL_KEYSTORE_PASSWORD.varname));
             builder.setKeyStorePath(keyStorePath);
+            builder.setKeyStoreType(hiveConf.getVar(ConfVars.HIVE_SERVER2_WEBUI_SSL_KEYSTORE_TYPE));
+            builder.setKeyManagerFactoryAlgorithm(
+                hiveConf.getVar(ConfVars.HIVE_SERVER2_WEBUI_SSL_KEYMANAGERFACTORY_ALGORITHM));
             builder.setUseSSL(true);
           }
           if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_WEBUI_USE_SPNEGO)) {

File: service/src/test/org/apache/hive/service/server/TestHS2HttpServerPamConfiguration.java
Patch:
@@ -48,6 +48,7 @@ public class TestHS2HttpServerPamConfiguration {
   private static HiveConf hiveConf = null;
   private static String keyStorePassword = "123456";
   private static String keyFileName = "myKeyStore";
+  private static String keyStoreType = KeyStore.getDefaultType();
   private static String testDataDir = new File(
       System.getProperty("java.io.tmpdir") + File.separator + TestHS2HttpServerPam.class.getCanonicalName() + "-"
           + System.currentTimeMillis()).getPath().replaceAll("\\\\", "/");
@@ -99,6 +100,7 @@ public void testPamCorrectConfiguration() {
     hiveConf.setBoolVar(ConfVars.HIVE_SERVER2_WEBUI_USE_SSL, true);
     hiveConf.setVar(ConfVars.HIVE_SERVER2_WEBUI_SSL_KEYSTORE_PATH, sslKeyStorePath);
     hiveConf.setVar(ConfVars.HIVE_SERVER2_WEBUI_SSL_KEYSTORE_PASSWORD, keyStorePassword);
+    hiveConf.setVar(ConfVars.HIVE_SERVER2_WEBUI_SSL_KEYSTORE_TYPE, keyStoreType);
     hiveServer2 = new HiveServer2();
     hiveServer2.init(hiveConf);
   }

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
Patch:
@@ -1117,8 +1117,8 @@ public void testErrorMessages() throws SQLException {
     // codes and messages. This should be fixed.
     doTestErrorCase(
         "create table " + tableName + " (key int, value string)",
-        "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.ddl.DDLTask",
-        "08S01", 1);
+        "FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask",
+        "08S01", 40000);
   }
 
   private void doTestErrorCase(String sql, String expectedMessage,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java
Patch:
@@ -355,8 +355,9 @@ public int execute() {
     } catch (Exception e) {
       setException(e);
       LOG.info("Failed to persist stats in metastore", e);
+      return ReplUtils.handleException(work.isReplication(), e, work.getDumpDirectory(), work.getMetricCollector(),
+                                       getName(), conf);
     }
-    return 1;
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ReplTxnTask.java
Patch:
@@ -132,7 +132,9 @@ public int execute() {
       console.printError("Failed with exception " + e.getMessage(), "\n"
           + StringUtils.stringifyException(e));
       setException(e);
-      return 1;
+      LOG.error("ReplTxnTask failed", e);
+      return ReplUtils.handleException(true, e, work.getDumpDirectory(), work.getMetricCollector(),
+              getName(), conf);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadWork.java
Patch:
@@ -163,6 +163,8 @@ public Task<?> getRootTask() {
     return rootTask;
   }
 
+  public String getDumpDirectory() {return dumpDirectory;}
+  
   public void setRootTask(Task<?> rootTask) {
     this.rootTask = rootTask;
   }
@@ -193,7 +195,7 @@ public List<Task<?>> externalTableCopyTasks(TaskTracker tracker, HiveConf conf)
     }
     List<Task<?>> tasks = new ArrayList<>();
     while (externalTableDataCopyItr.hasNext() && tracker.canAddMoreTasks()) {
-      DirCopyWork dirCopyWork = new DirCopyWork();
+      DirCopyWork dirCopyWork = new DirCopyWork(metricCollector, (new Path(dumpDirectory).getParent()).toString());
       dirCopyWork.loadFromString(externalTableDataCopyItr.next());
       Task<DirCopyWork> task = TaskFactory.get(dirCopyWork, conf);
       tasks.add(task);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java
Patch:
@@ -168,7 +168,7 @@ public static long writeFile(FileSystem fs, Path exportFilePath, InputStream is,
     }
   }
 
-  public static void writeStackTrace(Exception e, Path outputFile, HiveConf conf) throws SemanticException {
+  public static void writeStackTrace(Throwable e, Path outputFile, HiveConf conf) throws SemanticException {
     Retryable retryable = Retryable.builder()
       .withHiveConf(conf)
       .withRetryOnException(IOException.class).withFailOnException(FileNotFoundException.class).build();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AbortTxnHandler.java
Patch:
@@ -45,7 +45,8 @@ public List<Task<?>> handle(Context context)
 
     Task<ReplTxnWork> abortTxnTask = TaskFactory.get(
         new ReplTxnWork(HiveUtils.getReplPolicy(context.dbName), context.dbName, null,
-                msg.getTxnId(), ReplTxnWork.OperationType.REPL_ABORT_TXN, context.eventOnlyReplicationSpec()),
+                msg.getTxnId(), ReplTxnWork.OperationType.REPL_ABORT_TXN, context.eventOnlyReplicationSpec(),
+                context.getDumpDirectory(), context.getMetricCollector()),
         context.hiveConf
     );
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AddCheckConstraintHandler.java
Patch:
@@ -70,7 +70,8 @@ public List<Task<?>> handle(Context context)
     AlterTableAddConstraintDesc addConstraintsDesc = new AlterTableAddConstraintDesc(tName,
       context.eventOnlyReplicationSpec(), constraints);
     Task<DDLWork> addConstraintsTask = TaskFactory.get(
-      new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc), context.hiveConf);
+      new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc, true,
+              context.getDumpDirectory(), context.getMetricCollector()), context.hiveConf);
     tasks.add(addConstraintsTask);
     context.log.debug("Added add constrains task : {}:{}", addConstraintsTask.getId(), actualTblName);
     updatedMetadata.set(context.dmd.getEventTo().toString(), actualDbName, actualTblName, null);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AddDefaultConstraintHandler.java
Patch:
@@ -69,7 +69,8 @@ public List<Task<?>> handle(Context context)
     AlterTableAddConstraintDesc addConstraintsDesc = new AlterTableAddConstraintDesc(tName,
       context.eventOnlyReplicationSpec(), constraints);
     Task<DDLWork> addConstraintsTask = TaskFactory.get(
-      new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc), context.hiveConf);
+      new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc, true,
+              context.getDumpDirectory(), context.getMetricCollector()), context.hiveConf);
     tasks.add(addConstraintsTask);
     context.log.debug("Added add constrains task : {}:{}", addConstraintsTask.getId(), actualTblName);
     updatedMetadata.set(context.dmd.getEventTo().toString(), actualDbName, actualTblName, null);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AddNotNullConstraintHandler.java
Patch:
@@ -66,8 +66,8 @@ public List<Task<?>> handle(Context context)
     Constraints constraints = new Constraints(null, null, nns, null, null, null);
     AlterTableAddConstraintDesc addConstraintsDesc = new AlterTableAddConstraintDesc(tName,
         context.eventOnlyReplicationSpec(), constraints);
-    Task<DDLWork> addConstraintsTask = TaskFactory.get(
-            new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc), context.hiveConf);
+    Task<DDLWork> addConstraintsTask = TaskFactory.get( new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc,
+            true, context.getDumpDirectory(), context.getMetricCollector()), context.hiveConf);
     tasks.add(addConstraintsTask);
     context.log.debug("Added add constrains task : {}:{}", addConstraintsTask.getId(), actualTblName);
     updatedMetadata.set(context.dmd.getEventTo().toString(), actualDbName, actualTblName, null);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AddPrimaryKeyHandler.java
Patch:
@@ -66,8 +66,9 @@ public List<Task<?>> handle(Context context)
     Constraints constraints = new Constraints(pks, null, null, null, null, null);
     AlterTableAddConstraintDesc addConstraintsDesc = new AlterTableAddConstraintDesc(tName,
         context.eventOnlyReplicationSpec(), constraints);
-    Task<DDLWork> addConstraintsTask = TaskFactory.get(
-            new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc), context.hiveConf);
+    Task<DDLWork> addConstraintsTask = TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet,
+            addConstraintsDesc, true, context.getDumpDirectory(),
+            context.getMetricCollector()), context.hiveConf);
     tasks.add(addConstraintsTask);
     context.log.debug("Added add constrains task : {}:{}", addConstraintsTask.getId(), actualTblName);
     updatedMetadata.set(context.dmd.getEventTo().toString(), actualDbName, actualTblName, null);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AddUniqueConstraintHandler.java
Patch:
@@ -67,7 +67,8 @@ public List<Task<?>> handle(Context context)
     AlterTableAddConstraintDesc addConstraintsDesc = new AlterTableAddConstraintDesc(tName,
         context.eventOnlyReplicationSpec(), constraints);
     Task<DDLWork> addConstraintsTask = TaskFactory.get(
-            new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc), context.hiveConf);
+            new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc,
+                    true, context.getDumpDirectory(), context.getMetricCollector()), context.hiveConf);
     tasks.add(addConstraintsTask);
     context.log.debug("Added add constrains task : {}:{}", addConstraintsTask.getId(), actualTblName);
     updatedMetadata.set(context.dmd.getEventTo().toString(), actualDbName, actualTblName, null);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AllocWriteIdHandler.java
Patch:
@@ -51,7 +51,8 @@ public List<Task<?>> handle(Context context)
 
     // Repl policy should be created based on the table name in context.
     ReplTxnWork work = new ReplTxnWork(HiveUtils.getReplPolicy(context.dbName), dbName, tableName,
-        ReplTxnWork.OperationType.REPL_ALLOC_WRITE_ID, msg.getTxnToWriteIdList(), context.eventOnlyReplicationSpec());
+        ReplTxnWork.OperationType.REPL_ALLOC_WRITE_ID, msg.getTxnToWriteIdList(), context.eventOnlyReplicationSpec(),
+            context.getDumpDirectory(), context.getMetricCollector());
 
     Task<?> allocWriteIdTask = TaskFactory.get(work, context.hiveConf);
     context.log.info("Added alloc write id task : {}", allocWriteIdTask.getId());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDatabaseHandler.java
Patch:
@@ -78,8 +78,8 @@ public List<Task<?>> handle(Context context)
             newDb.getOwnerType()), context.eventOnlyReplicationSpec());
       }
 
-      Task<DDLWork> alterDbTask = TaskFactory.get(
-          new DDLWork(readEntitySet, writeEntitySet, alterDbDesc), context.hiveConf);
+      Task<DDLWork> alterDbTask = TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet,
+                       alterDbDesc, true, context.getDumpDirectory(), context.getMetricCollector()), context.hiveConf);
       context.log.debug("Added alter database task : {}:{}",
               alterDbTask.getId(), actualDbName);
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropConstraintHandler.java
Patch:
@@ -43,7 +43,8 @@ public List<Task<?>> handle(Context context)
     AlterTableDropConstraintDesc dropConstraintsDesc =
         new AlterTableDropConstraintDesc(tName, context.eventOnlyReplicationSpec(), constraintName);
     Task<DDLWork> dropConstraintsTask = TaskFactory.get(
-            new DDLWork(readEntitySet, writeEntitySet, dropConstraintsDesc), context.hiveConf);
+            new DDLWork(readEntitySet, writeEntitySet, dropConstraintsDesc, true,
+                    context.getDumpDirectory(), context.getMetricCollector()), context.hiveConf);
     context.log.debug("Added drop constrain task : {}:{}", dropConstraintsTask.getId(), actualTblName);
     updatedMetadata.set(context.dmd.getEventTo().toString(), actualDbName, actualTblName, null);
     return Collections.singletonList(dropConstraintsTask);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java
Patch:
@@ -37,8 +37,8 @@ public List<Task<?>> handle(Context context)
         deserializer.getDropDatabaseMessage(context.dmd.getPayload());
     String actualDbName = context.isDbNameEmpty() ? msg.getDB() : context.dbName;
     DropDatabaseDesc desc = new DropDatabaseDesc(actualDbName, true, context.eventOnlyReplicationSpec());
-    Task<?> dropDBTask =
-        TaskFactory.get(new DDLWork(new HashSet<>(), new HashSet<>(), desc), context.hiveConf);
+    Task<?> dropDBTask = TaskFactory.get(new DDLWork(new HashSet<>(), new HashSet<>(), desc,
+                true, context.getDumpDirectory(), context.getMetricCollector()), context.hiveConf);
     context.log.info(
         "Added drop database task : {}:{}", dropDBTask.getId(), desc.getDatabaseName());
     updatedMetadata.set(context.dmd.getEventTo().toString(), actualDbName, null, null);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropFunctionHandler.java
Patch:
@@ -47,7 +47,8 @@ public List<Task<?>> handle(Context context)
     DropFunctionDesc desc = new DropFunctionDesc(
             qualifiedFunctionName, false, context.eventOnlyReplicationSpec());
     Task<DDLWork> dropFunctionTask =
-        TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet, desc), context.hiveConf);
+        TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet, desc, true,
+                context.getDumpDirectory(), context.getMetricCollector()), context.hiveConf);
     context.log.debug(
         "Added drop function task : {}:{}", dropFunctionTask.getId(), desc.getName()
     );

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropPartitionHandler.java
Patch:
@@ -47,8 +47,8 @@ public List<Task<?>> handle(Context context)
         AlterTableDropPartitionDesc dropPtnDesc =
             new AlterTableDropPartitionDesc(HiveTableName.ofNullable(actualTblName, actualDbName), partSpecs, true,
                 context.eventOnlyReplicationSpec());
-        Task<DDLWork> dropPtnTask = TaskFactory.get(
-            new DDLWork(readEntitySet, writeEntitySet, dropPtnDesc), context.hiveConf
+        Task<DDLWork> dropPtnTask = TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet, dropPtnDesc,
+                    true, context.getDumpDirectory(), context.getMetricCollector()), context.hiveConf
         );
         context.log.debug("Added drop ptn task : {}:{},{}", dropPtnTask.getId(),
             dropPtnDesc.getTableName(), msg.getPartitions());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropTableHandler.java
Patch:
@@ -49,7 +49,8 @@ public List<Task<?>> handle(Context context)
     DropTableDesc dropTableDesc = new DropTableDesc(actualDbName + "." + actualTblName, true, true,
         context.eventOnlyReplicationSpec(), false);
     Task<DDLWork> dropTableTask = TaskFactory.get(
-        new DDLWork(readEntitySet, writeEntitySet, dropTableDesc), context.hiveConf
+        new DDLWork(readEntitySet, writeEntitySet, dropTableDesc, true,
+                context.getDumpDirectory(), context.getMetricCollector()), context.hiveConf
     );
     context.log.debug(
         "Added drop tbl task : {}:{}", dropTableTask.getId(), dropTableDesc.getTableName()

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/InsertHandler.java
Patch:
@@ -55,7 +55,8 @@ public List<Task<?>> handle(Context withinContext)
     InsertMessage insertMessage = deserializer.getInsertMessage(withinContext.dmd.getPayload());
     String actualDbName =
         withinContext.isDbNameEmpty() ? insertMessage.getDB() : withinContext.dbName;
-    Context currentContext = new Context(withinContext, actualDbName);
+    Context currentContext = new Context(withinContext, actualDbName,
+                                         withinContext.getDumpDirectory(), withinContext.getMetricCollector());
 
     // Piggybacking in Import logic for now
     TableHandler tableHandler = new TableHandler();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/OpenTxnHandler.java
Patch:
@@ -44,7 +44,8 @@ public List<Task<?>> handle(Context context)
 
     Task<ReplTxnWork> openTxnTask = TaskFactory.get(
         new ReplTxnWork(HiveUtils.getReplPolicy(context.dbName), context.dbName, null,
-                msg.getTxnIds(), ReplTxnWork.OperationType.REPL_OPEN_TXN, context.eventOnlyReplicationSpec()),
+                msg.getTxnIds(), ReplTxnWork.OperationType.REPL_OPEN_TXN, context.eventOnlyReplicationSpec(),
+                context.getDumpDirectory(), context.getMetricCollector()),
         context.hiveConf
     );
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/RenamePartitionHandler.java
Patch:
@@ -61,7 +61,8 @@ public List<Task<?>> handle(Context context)
               tableName, oldPartSpec, newPartSpec, replicationSpec, null);
       renamePtnDesc.setWriteId(msg.getWriteId());
       Task<DDLWork> renamePtnTask = TaskFactory.get(
-          new DDLWork(readEntitySet, writeEntitySet, renamePtnDesc), context.hiveConf);
+          new DDLWork(readEntitySet, writeEntitySet, renamePtnDesc, true,
+                  context.getDumpDirectory(), context.getMetricCollector()), context.hiveConf);
       context.log.debug("Added rename ptn task : {}:{}->{}",
                         renamePtnTask.getId(), oldPartSpec, newPartSpec);
       updatedMetadata.set(context.dmd.getEventTo().toString(), actualDbName, actualTblName, newPartSpec);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/RenameTableHandler.java
Patch:
@@ -61,8 +61,9 @@ public List<Task<?>> handle(Context context)
       AlterTableRenameDesc renameTableDesc =
           new AlterTableRenameDesc(oldName, replicationSpec, false, newName.getNotEmptyDbTable());
       renameTableDesc.setWriteId(msg.getWriteId());
-      Task<DDLWork> renameTableTask = TaskFactory.get(
-          new DDLWork(readEntitySet, writeEntitySet, renameTableDesc), context.hiveConf);
+      Task<DDLWork> renameTableTask = TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet,
+              renameTableDesc, true, context.getDumpDirectory(),
+              context.getMetricCollector()), context.hiveConf);
       context.log.debug("Added rename table task : {}:{}->{}",
                         renameTableTask.getId(), oldName.getNotEmptyDbTable(), newName.getNotEmptyDbTable());
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/TruncatePartitionHandler.java
Patch:
@@ -60,7 +60,8 @@ public List<Task<?>> handle(Context context) throws SemanticException {
             context.eventOnlyReplicationSpec());
     truncateTableDesc.setWriteId(msg.getWriteId());
     Task<DDLWork> truncatePtnTask = TaskFactory.get(
-        new DDLWork(readEntitySet, writeEntitySet, truncateTableDesc), context.hiveConf);
+        new DDLWork(readEntitySet, writeEntitySet, truncateTableDesc, true,
+                context.getDumpDirectory(), context.getMetricCollector()), context.hiveConf);
     context.log.debug("Added truncate ptn task : {}:{}:{}", truncatePtnTask.getId(),
         truncateTableDesc.getTableName(), truncateTableDesc.getWriteId());
     updatedMetadata.set(context.dmd.getEventTo().toString(), tName.getDb(), tName.getTable(), partSpec);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/TruncateTableHandler.java
Patch:
@@ -37,8 +37,9 @@ public List<Task<?>> handle(Context context) throws SemanticException {
 
     TruncateTableDesc truncateTableDesc = new TruncateTableDesc(tName, null, context.eventOnlyReplicationSpec());
     truncateTableDesc.setWriteId(msg.getWriteId());
-    Task<DDLWork> truncateTableTask = TaskFactory.get(
-        new DDLWork(readEntitySet, writeEntitySet, truncateTableDesc), context.hiveConf);
+    Task<DDLWork> truncateTableTask = TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet,
+                truncateTableDesc, true, context.getDumpDirectory(),
+                context.getMetricCollector()), context.hiveConf);
 
     context.log.debug("Added truncate tbl task : {}:{}:{}", truncateTableTask.getId(),
         truncateTableDesc.getTableName(), truncateTableDesc.getWriteId());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/UpdatePartColStatHandler.java
Patch:
@@ -53,7 +53,9 @@ public List<Task<?>> handle(Context context)
 
     try {
       return ReplUtils.addTasksForLoadingColStats(colStats, context.hiveConf, updatedMetadata,
-                                                  upcsm.getTableObject(), upcsm.getWriteId());
+                                                  upcsm.getTableObject(), upcsm.getWriteId(),
+                                                  context.getDumpDirectory(),
+                                                  context.getMetricCollector());
     } catch(Exception e) {
       throw new SemanticException(e);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/UpdateTableColStatHandler.java
Patch:
@@ -53,7 +53,8 @@ public List<Task<?>> handle(Context context)
 
         try {
             return ReplUtils.addTasksForLoadingColStats(colStats, context.hiveConf, updatedMetadata,
-                    utcsm.getTableObject(), utcsm.getWriteId());
+                    utcsm.getTableObject(), utcsm.getWriteId(), context.getDumpDirectory(),
+                    context.getMetricCollector());
         } catch(Exception e) {
             throw new SemanticException(e);
         }

File: service/src/test/org/apache/hive/service/cli/CLIServiceTest.java
Patch:
@@ -245,7 +245,7 @@ public void testExecuteStatementAsync() throws Exception {
     opStatus = runAsyncAndWait(sessionHandle, queryString, confOverlay, OperationState.ERROR, longPollingTimeout);
     // sqlState, errorCode should be set
     assertEquals(opStatus.getOperationException().getSQLState(), "08S01");
-    assertEquals(opStatus.getOperationException().getErrorCode(), 1);
+    assertEquals(opStatus.getOperationException().getErrorCode(), 40000);
     /**
      * Execute an async query with default config
      */

File: service/src/test/org/apache/hive/service/cli/thrift/ThriftCLIServiceTest.java
Patch:
@@ -290,7 +290,7 @@ public void testExecuteStatementAsync() throws Exception {
         OperationState.ERROR, state);
     // sqlState, errorCode should be set to appropriate values
     assertEquals(opStatus.getOperationException().getSQLState(), "08S01");
-    assertEquals(opStatus.getOperationException().getErrorCode(), 1);
+    assertEquals(opStatus.getOperationException().getErrorCode(), 40000);
 
     // Cleanup
     queryString = "DROP TABLE TEST_EXEC_ASYNC_THRIFT";

File: service/src/java/org/apache/hive/service/cli/thrift/ThriftBinaryCLIService.java
Patch:
@@ -92,8 +92,10 @@ protected void initServer() {
         }
         String keyStorePassword = ShimLoader.getHadoopShims().getPassword(hiveConf,
             HiveConf.ConfVars.HIVE_SERVER2_SSL_KEYSTORE_PASSWORD.varname);
+        String keyStoreType = hiveConf.getVar(ConfVars.HIVE_SERVER2_SSL_KEYSTORE_TYPE).trim();
+        String keyStoreAlgorithm = hiveConf.getVar(ConfVars.HIVE_SERVER2_SSL_KEYMANAGERFACTORY_ALGORITHM).trim();
         serverSocket = HiveAuthUtils.getServerSSLSocket(hiveHost, portNum, keyStorePath, keyStorePassword,
-            sslVersionBlacklist);
+            keyStoreType, keyStoreAlgorithm, sslVersionBlacklist);
       }
 
       // Server args

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3565,6 +3565,9 @@ public static enum ConfVars {
         "The parent node in ZooKeeper used by HiveServer2 when supporting dynamic service discovery."),
     HIVE_SERVER2_ZOOKEEPER_PUBLISH_CONFIGS("hive.server2.zookeeper.publish.configs", true,
         "Whether we should publish HiveServer2's configs to ZooKeeper."),
+    HIVE_SERVER2_OOM_HOOKS("hive.server2.oom.hooks", null,
+        "A comma separated list of hooks which implement Runnable. Will be run in the order specified \n" +
+        "before HiveServer2 stops due to OutOfMemoryError."),
 
     // HiveServer2 global init file location
     HIVE_SERVER2_GLOBAL_INIT_FILE_LOCATION("hive.server2.global.init.file.location", "${env:HIVE_CONF_DIR}",

File: service/src/java/org/apache/hive/service/cli/session/SessionManager.java
Patch:
@@ -45,6 +45,7 @@
 import org.apache.hadoop.hive.common.metrics.common.MetricsVariable;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.ql.hooks.HookContext;
 import org.apache.hadoop.hive.ql.hooks.HookUtils;
 import org.apache.hive.service.CompositeService;
 import org.apache.hive.service.cli.HiveSQLException;
@@ -710,7 +711,7 @@ public static void clearProxyUserName() {
   // execute session hooks
   private void executeSessionHooks(HiveSession session) throws Exception {
     List<HiveSessionHook> sessionHooks =
-        HookUtils.readHooksFromConf(hiveConf, HiveConf.ConfVars.HIVE_SERVER2_SESSION_HOOK);
+        HookUtils.readHooksFromConf(hiveConf, HookContext.HookType.HIVE_SERVER2_SESSION_HOOK);
     for (HiveSessionHook sessionHook : sessionHooks) {
       sessionHook.run(new HiveSessionHookContextImpl(session));
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java
Patch:
@@ -162,7 +162,8 @@ private void initiateAuthorizationLoadTask() throws SemanticException {
     if (RANGER_AUTHORIZER.equalsIgnoreCase(conf.getVar(HiveConf.ConfVars.REPL_AUTHORIZATION_PROVIDER_SERVICE))) {
       Path rangerLoadRoot = new Path(new Path(work.dumpDirectory).getParent(), ReplUtils.REPL_RANGER_BASE_DIR);
       LOG.info("Adding Import Ranger Metadata Task from {} ", rangerLoadRoot);
-      RangerLoadWork rangerLoadWork = new RangerLoadWork(rangerLoadRoot, work.getSourceDbName(), work.dbNameToLoadIn,
+      String targetDbName = StringUtils.isEmpty(work.dbNameToLoadIn) ? work.getSourceDbName() : work.dbNameToLoadIn;
+      RangerLoadWork rangerLoadWork = new RangerLoadWork(rangerLoadRoot, work.getSourceDbName(), targetDbName,
           work.getMetricCollector());
       Task<RangerLoadWork> rangerLoadTask = TaskFactory.get(rangerLoadWork, conf);
       if (childTasks == null) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ranger/RangerRestClientImpl.java
Patch:
@@ -260,7 +260,7 @@ private RangerExportPolicyList importRangerPoliciesPlain(String jsonRangerExport
   public String getRangerImportUrl(String rangerUrl, String dbName) throws URISyntaxException {
     URIBuilder uriBuilder = new URIBuilder(rangerUrl);
     uriBuilder.setPath(RANGER_REST_URL_IMPORTJSONFILE);
-    uriBuilder.addParameter("updateIfExists", "true");
+    uriBuilder.addParameter("mergeIfExists", "true");
     uriBuilder.addParameter("polResource", dbName);
     return uriBuilder.build().toString();
   }
@@ -277,7 +277,7 @@ private synchronized Client getRangerClient() {
   @Override
   public List<RangerPolicy> changeDataSet(List<RangerPolicy> rangerPolicies, String sourceDbName,
                                           String targetDbName) {
-    if (targetDbName.equals(sourceDbName)) {
+    if (StringUtils.isEmpty(sourceDbName) || StringUtils.isEmpty(targetDbName) || targetDbName.equals(sourceDbName)) {
       return rangerPolicies;
     }
     if (CollectionUtils.isNotEmpty(rangerPolicies)) {

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/function/reload/ReloadFunctionsOperation.java
Patch:
@@ -34,7 +34,7 @@ public ReloadFunctionsOperation(DDLOperationContext context, ReloadFunctionsDesc
   @Override
   public int execute() throws HiveException {
     try {
-      Hive.get().reloadFunctions();
+      Hive.get(false).reloadFunctions();
       return 0;
     } catch (Exception e) {
       context.getTask().setException(e);

File: common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -472,6 +472,7 @@ public enum ErrorMsg {
   WITHIN_GROUP_PARAMETER_MISMATCH(10422,
           "The number of hypothetical direct arguments ({0}) must match the number of ordering columns ({1})", true),
   AMBIGUOUS_STRUCT_ATTRIBUTE(10423, "Attribute \"{0}\" specified more than once in structured type.", true),
+  OFFSET_NOT_SUPPORTED_IN_SUBQUERY(10424, "OFFSET is not supported in subquery of exists", true),
 
   //========================== 20000 range starts here ========================//
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptMaterializationValidator.java
Patch:
@@ -233,8 +233,8 @@ private RelNode visit(HiveUnion union) {
     return visitChildren(union);
   }
 
-  // Note: Not currently part of the HiveRelNode interface
-  private RelNode visit(HiveSortLimit sort) {
+  @Override
+  public RelNode visit(HiveSortLimit sort) {
     setAutomaticRewritingInvalidReason("Statement has unsupported clause: order by.");
     checkExpr(sort.getFetchExpr());
     checkExpr(sort.getOffsetExpr());

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelShuttle.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter;
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin;
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject;
+import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSortLimit;
 
 /**
  * Visitor that has methods for the common logical relational expressions.
@@ -35,6 +36,7 @@ public interface HiveRelShuttle extends RelShuttle {
     RelNode visit(HiveFilter filter);
     RelNode visit(HiveJoin join);
     RelNode visit(HiveAggregate aggregate);
+    RelNode visit(HiveSortLimit hiveSortLimit);
 }
 
 // End RelShuttle.java

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
Patch:
@@ -859,7 +859,7 @@ Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive hiveDb)
                   " with first incremental dump pending : " + dbName);
         }
         int estimatedNumTables = Utils.getAllTables(hiveDb, dbName, work.replScope).size();
-        int estimatedNumFunctions = hiveDb.getAllFunctions().size();
+        int estimatedNumFunctions = hiveDb.getFunctions(dbName, "*").size();
         replLogger = new BootstrapDumpLogger(dbName, dumpRoot.toString(),
                 estimatedNumTables,
                 estimatedNumFunctions);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/keyseries/VectorKeySeriesLongSerialized.java
Patch:
@@ -222,7 +222,6 @@ public void processBatch(VectorizedRowBatch batch) throws IOException {
   private void serialize(int pos, long value) throws IOException {
     serializeWrite.setAppend(output);
 
-    // UNDONE: Add support for DATE, TIMESTAMP, INTERVAL_YEAR_MONTH, INTERVAL_DAY_TIME...
     switch (primitiveCategory) {
     case BOOLEAN:
       serializeWrite.writeBoolean(value != 0);
@@ -236,6 +235,9 @@ private void serialize(int pos, long value) throws IOException {
     case INT:
       serializeWrite.writeInt((int) value);
       break;
+    case DATE:
+      serializeWrite.writeDate((int) value);
+      break;
     case LONG:
       serializeWrite.writeLong(value);
       break;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java
Patch:
@@ -117,6 +117,7 @@ private VectorMapJoinFastHashTable createHashTable(int newThreshold) {
     case BYTE:
     case SHORT:
     case INT:
+    case DATE:
     case LONG:
       switch (hashTableKind) {
       case HASH_MAP:

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java
Patch:
@@ -82,6 +82,9 @@ public SerializedBytes serialize(long key) throws IOException {
     case INT:
       keyBinarySortableSerializeWrite.writeInt((int) key);
       break;
+    case DATE:
+      keyBinarySortableSerializeWrite.writeDate((int) key);
+        break;
     case LONG:
       keyBinarySortableSerializeWrite.writeLong(key);
       break;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/EventHandlerFactory.java
Patch:
@@ -47,6 +47,8 @@ private EventHandlerFactory() {
     register(MessageBuilder.ADD_FOREIGNKEY_EVENT, AddForeignKeyHandler.class);
     register(MessageBuilder.ADD_UNIQUECONSTRAINT_EVENT, AddUniqueConstraintHandler.class);
     register(MessageBuilder.ADD_NOTNULLCONSTRAINT_EVENT, AddNotNullConstraintHandler.class);
+    register(MessageBuilder.ADD_DEFAULTCONSTRAINT_EVENT, AddDefaultConstraintHandler.class);
+    register(MessageBuilder.ADD_CHECKCONSTRAINT_EVENT, AddCheckConstraintHandler.class);
     register(MessageBuilder.DROP_CONSTRAINT_EVENT, DropConstraintHandler.class);
     register(MessageBuilder.CREATE_DATABASE_EVENT, CreateDatabaseHandler.class);
     register(MessageBuilder.DROP_DATABASE_EVENT, DropDatabaseHandler.class);

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/messaging/EventMessage.java
Patch:
@@ -47,6 +47,8 @@ public enum EventType {
     ADD_FOREIGNKEY(MessageBuilder.ADD_FOREIGNKEY_EVENT),
     ADD_UNIQUECONSTRAINT(MessageBuilder.ADD_UNIQUECONSTRAINT_EVENT),
     ADD_NOTNULLCONSTRAINT(MessageBuilder.ADD_NOTNULLCONSTRAINT_EVENT),
+    ADD_DEFAULTCONSTRAINT(MessageBuilder.ADD_DEFAULTCONSTRAINT_EVENT),
+    ADD_CHECKCONSTRAINT(MessageBuilder.ADD_CHECKCONSTRAINT_EVENT),
     DROP_CONSTRAINT(MessageBuilder.DROP_CONSTRAINT_EVENT),
     CREATE_ISCHEMA(MessageBuilder.CREATE_ISCHEMA_EVENT),
     ALTER_ISCHEMA(MessageBuilder.ALTER_ISCHEMA_EVENT),

File: beeline/src/java/org/apache/hive/beeline/Rows.java
Patch:
@@ -166,7 +166,7 @@ public String toString(){
         } else if (o instanceof byte[]) {
           value = convertBinaryArrayToString ? new String((byte[])o, StandardCharsets.UTF_8) : Base64.getEncoder().withoutPadding().encodeToString((byte[])o);
         } else {
-          value = o.toString();
+          value = rs.getString(i + 1);
         }
 
         if (beeLine.getOpts().getEscapeCRLF()) {

File: beeline/src/test/org/apache/hive/beeline/TestIncrementalRows.java
Patch:
@@ -115,6 +115,7 @@ public void testIncrementalRowsWithNormalization() throws SQLException {
     initNrOfResultSetCalls(10);
 
     when(mockResultSet.getObject(1)).thenReturn("Hello World");
+    when(mockResultSet.getString(1)).thenReturn("Hello World");
 
     // IncrementalRows constructor should buffer the first "incrementalBufferRows" rows
     IncrementalRowsWithNormalization incrementalRowsWithNormalization = new IncrementalRowsWithNormalization(

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestMmCompactorOnMr.java
Patch:
@@ -20,10 +20,10 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.junit.Before;
 
-@org.junit.Ignore("HIVE-24172")
 public class TestMmCompactorOnMr extends TestMmCompactorOnTez {
   @Before
   public void setMr() {
-    driver.getConf().setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
+    // NOTE: only compaction will run with MR as execution engine; setup and teardown queries will run with Tez.
+    conf.setVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE, "mr");
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java
Patch:
@@ -73,7 +73,7 @@ public class Registry {
   private static final Logger LOG = LoggerFactory.getLogger(FunctionRegistry.class);
 
   // prefix for window functions, to discern LEAD/LAG UDFs from window functions with the same name
-  private static final String WINDOW_FUNC_PREFIX = "@_";
+  public static final String WINDOW_FUNC_PREFIX = "@_";
 
   /**
    * The mapping from expression function names to expression classes.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
Patch:
@@ -41,8 +41,10 @@
 import org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement;
 import org.apache.hadoop.hive.metastore.api.TxnType;
 import org.apache.hadoop.hive.metastore.messaging.event.filters.AndFilter;
+import org.apache.hadoop.hive.metastore.messaging.event.filters.CatalogFilter;
 import org.apache.hadoop.hive.metastore.messaging.event.filters.EventBoundaryFilter;
 import org.apache.hadoop.hive.metastore.messaging.event.filters.ReplEventFilter;
+import org.apache.hadoop.hive.metastore.utils.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.utils.SecurityUtils;
 import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Task;
@@ -536,6 +538,7 @@ private Long incrementalDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive
     work.overrideLastEventToDump(hiveDb, bootDumpBeginReplId);
     IMetaStoreClient.NotificationFilter evFilter = new AndFilter(
         new ReplEventFilter(work.replScope),
+        new CatalogFilter(MetaStoreUtils.getDefaultCatalog(conf)),
         new EventBoundaryFilter(work.eventFrom, work.eventTo));
     EventUtils.MSClientNotificationFetcher evFetcher
         = new EventUtils.MSClientNotificationFetcher(hiveDb);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestDDLWithRemoteMetastoreSecondNamenode.java
Patch:
@@ -222,7 +222,7 @@ private Table createTableAndCheck(String tableName, String tableLocation)
   }
 
   private Table createTableAndCheck(Table baseTable, String tableName, String tableLocation) throws Exception {
-    executeQuery("CREATE TABLE " + tableName + (baseTable == null ?
+    executeQuery("CREATE EXTERNAL TABLE " + tableName + (baseTable == null ?
             " (col1 string, col2 string) PARTITIONED BY (p string) " :
             " LIKE " + baseTable.getTableName())
             + buildLocationClause(tableLocation));

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestWarehouseExternalDir.java
Patch:
@@ -166,7 +166,7 @@ public void testManagedPaths() throws Exception {
 
       stmt.execute("create table twed_db1.tab1(c1 string, c2 string)");
       tab = db.getTable("twed_db1", "tab1");
-      checkTableLocation(tab, new Path(new Path(whRootExternalPath, "twed_db1.db"), "tab1"));
+      checkTableLocation(tab, new Path(new Path(whRootManagedPath, "twed_db1.db"), "tab1"));
     }
   }
 

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/NonCatCallsWithCatalog.java
Patch:
@@ -93,6 +93,7 @@ public abstract class NonCatCallsWithCatalog {
   protected abstract IMetaStoreClient getClient() throws Exception;
   protected abstract String expectedCatalog();
   protected abstract String expectedBaseDir() throws MetaException;
+  protected abstract String expectedExtBaseDir() throws MetaException;
 
   @Before
   public void setUp() throws Exception {
@@ -218,7 +219,7 @@ public void databases() throws TException, URISyntaxException {
     }
 
     Database fetched = client.getDatabase(dbNames[0]);
-    String expectedLocation = new File(expectedBaseDir(), dbNames[0] + ".db").toURI().toString();
+    String expectedLocation = new File(expectedExtBaseDir(), dbNames[0] + ".db").toURI().toString();
     Assert.assertEquals(expectedCatalog(), fetched.getCatalogName());
     Assert.assertEquals(expectedLocation, fetched.getLocationUri() + "/");
     String db0Location = new URI(fetched.getLocationUri()).getPath();

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/client/TestTablesCreateDropAlterTruncate.java
Patch:
@@ -348,7 +348,7 @@ public void testCreateTableDefaultLocationInSpecificDatabase() throws Exception
 
     client.createTable(table);
     Table createdTable = client.getTable(table.getDbName(), table.getTableName());
-    Assert.assertEquals("Storage descriptor location", metaStore.getExternalWarehouseRoot()
+    Assert.assertEquals("Storage descriptor location", metaStore.getWarehouseRoot()
         + "/" + table.getDbName() + ".db/" + table.getTableName(),
         createdTable.getSd().getLocation());
   }

File: ql/src/java/org/apache/hadoop/hive/ql/Compiler.java
Patch:
@@ -189,6 +189,7 @@ private BaseSemanticAnalyzer analyze() throws Exception {
     // because at that point we need access to the objects.
     Hive.get().getMSC().flushCache();
 
+    driverContext.setBackupContext(new Context(context));
     boolean executeHooks = driverContext.getHookRunner().hasPreAnalyzeHooks();
 
     HiveSemanticAnalyzerHookContext hookCtx = new HiveSemanticAnalyzerHookContextImpl();

File: ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.reexec.IReExecutionPlugin;
 import org.apache.hadoop.hive.ql.reexec.ReExecDriver;
-import org.apache.hadoop.hive.ql.reexec.ReExecutionRetryLockPlugin;
 import org.apache.hadoop.hive.ql.reexec.ReExecuteLostAMQueryPlugin;
 import org.apache.hadoop.hive.ql.reexec.ReExecutionOverlayPlugin;
 import org.apache.hadoop.hive.ql.reexec.ReExecutionDagSubmitPlugin;
@@ -60,8 +59,6 @@ public static IDriver newDriver(QueryState queryState, QueryInfo queryInfo) {
       }
       plugins.add(buildReExecPlugin(string));
     }
-    // The retrylock plugin is always enabled
-    plugins.add(new ReExecutionRetryLockPlugin());
 
     return new ReExecDriver(queryState, queryInfo, plugins);
   }

File: ql/src/java/org/apache/hadoop/hive/ql/reexec/IReExecutionPlugin.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hadoop.hive.common.classification.InterfaceStability;
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;
-import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
 
 /**
  * Defines an interface for re-execution logics.
@@ -48,7 +47,7 @@ public interface IReExecutionPlugin {
   /**
    * The query have failed, does this plugin advises to re-execute it again?
    */
-  boolean shouldReExecute(int executionNum, CommandProcessorException ex);
+  boolean shouldReExecute(int executionNum);
 
   /**
    * The plugin should prepare for the re-compilaton of the query.

File: ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionOverlayPlugin.java
Patch:
@@ -26,7 +26,6 @@
 import org.apache.hadoop.hive.ql.hooks.HookContext;
 import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
 import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;
-import org.apache.hadoop.hive.ql.processors.CommandProcessorException;
 import org.apache.tez.dag.api.TezConfiguration;
 
 /**
@@ -75,8 +74,7 @@ public void prepareToReExecute() {
   }
 
   @Override
-  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {
-
+  public boolean shouldReExecute(int executionNum) {
     return executionNum == 1 && !subtree.isEmpty() && retryPossible;
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java
Patch:
@@ -275,7 +275,8 @@ public void run() {
       } catch (HiveException e) {
         throw new RuntimeException(e);
       }
-      try (IDriver d = DriverFactory.newDriver(hiveConf)) {
+      QueryState qs = new QueryState.Builder().withHiveConf(hiveConf).nonIsolated().build();
+      try (Driver d = new Driver(qs)) {
         LOG.info("Ready to run the query: " + query);
         syncThreadStart(cdlIn, cdlOut);
         try {

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands3.java
Patch:
@@ -60,6 +60,8 @@ protected String getTestDataDir() {
   @Test
   public void testRenameTable() throws Exception {
     MetastoreConf.setBoolVar(hiveConf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID, true);
+    hiveConf.setBoolVar(HiveConf.ConfVars.TXN_WRITE_X_LOCK, false);
+
     runStatementOnDriver("drop database if exists mydb1 cascade");
     runStatementOnDriver("drop database if exists mydb2 cascade");
     runStatementOnDriver("create database mydb1");

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -3558,7 +3558,7 @@ public void commitTxnWithKeyValue(long txnid, long tableId, String key,
   }
 
   @Override
-  public void replCommitTxn(CommitTxnRequest rqst)
+  public void commitTxn(CommitTxnRequest rqst)
           throws NoSuchTxnException, TxnAbortedException, TException {
     client.commit_txn(rqst);
   }

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java
Patch:
@@ -3119,7 +3119,7 @@ void commitTxnWithKeyValue(long txnid, long tableId,
    * aborted.  This can result from the transaction timing out.
    * @throws TException
    */
-  void replCommitTxn(CommitTxnRequest rqst)
+  void commitTxn(CommitTxnRequest rqst)
           throws NoSuchTxnException, TxnAbortedException, TException;
 
   /**

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientPreCatalog.java
Patch:
@@ -2429,7 +2429,7 @@ public void commitTxnWithKeyValue(long txnid, long tableId, String key,
   }
 
   @Override
-  public void replCommitTxn(CommitTxnRequest rqst)
+  public void commitTxn(CommitTxnRequest rqst)
           throws NoSuchTxnException, TxnAbortedException, TException {
     client.commit_txn(rqst);
   }

File: common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -622,7 +622,8 @@ public enum ErrorMsg {
   REPL_INVALID_CONFIG_FOR_SERVICE(40008, "Invalid config error : {0} for {1} service.", true),
   REPL_INVALID_INTERNAL_CONFIG_FOR_SERVICE(40009, "Invalid internal config error : {0} for {1} service.", true),
   REPL_RETRY_EXHAUSTED(40010, "Retry exhausted for retryable error code {0}.", true),
-  REPL_FAILED_WITH_NON_RECOVERABLE_ERROR(40011, "Replication failed with non recoverable error. Needs manual intervention")
+  REPL_FAILED_WITH_NON_RECOVERABLE_ERROR(40011, "Replication failed with non recoverable error. Needs manual intervention"),
+  REPL_INVALID_ARGUMENTS(40012, "Invalid arguments error : {0}.", true)
   ;
 
   private int errorCode;

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestCopyUtils.java
Patch:
@@ -90,7 +90,7 @@ public static void classLevelSetup() throws Exception {
     MiniDFSCluster miniDFSCluster =
         new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
     HashMap<String, String> overridesForHiveConf = new HashMap<String, String>() {{
-      put(ConfVars.HIVE_IN_TEST.varname, "true");
+      put(ConfVars.HIVE_IN_TEST_REPL.varname, "true");
       put(ConfVars.HIVE_EXEC_COPYFILE_MAXSIZE.varname, "1");
       put(ConfVars.HIVE_SERVER2_ENABLE_DOAS.varname, "false");
       put(ConfVars.HIVE_DISTCP_DOAS_USER.varname, currentUser);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
Patch:
@@ -315,7 +315,9 @@ private void deleteAllPreviousDumpMeta(Path currentDumpPath) {
   }
 
   private Path getDumpRoot(Path currentDumpPath) {
-    if (ReplDumpWork.testDeletePreviousDumpMetaPath && conf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST_REPL)) {
+    if (ReplDumpWork.testDeletePreviousDumpMetaPath
+            && (conf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)
+                || conf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST_REPL))) {
       //testDeleteDumpMetaDumpPath to be used only for test.
       return null;
     } else {

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -881,6 +881,7 @@ private void createDefaultDB_core(RawStore ms) throws MetaException, InvalidObje
       try {
         ms.getDatabase(DEFAULT_CATALOG_NAME, DEFAULT_DATABASE_NAME);
       } catch (NoSuchObjectException e) {
+        LOG.info("Started creating a default database with name: "+DEFAULT_DATABASE_NAME);
         Database db = new Database(DEFAULT_DATABASE_NAME, DEFAULT_DATABASE_COMMENT,
             wh.getDefaultDatabasePath(DEFAULT_DATABASE_NAME, true).toString(), null);
         db.setOwnerName(PUBLIC);
@@ -889,6 +890,7 @@ private void createDefaultDB_core(RawStore ms) throws MetaException, InvalidObje
         long time = System.currentTimeMillis() / 1000;
         db.setCreateTime((int) time);
         ms.createDatabase(db);
+        LOG.info("Successfully created a default database with name: "+DEFAULT_DATABASE_NAME);
       }
     }
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2325,8 +2325,6 @@ public static enum ConfVars {
         "the local task. If GC time percentage exceeds this number, the local task will abort by " +
         "itself. Applies to Hive-on-Spark only"),
 
-    HIVEDEBUGLOCALTASK("hive.debug.localtask",false, ""),
-
     HIVEINPUTFORMAT("hive.input.format", "org.apache.hadoop.hive.ql.io.CombineHiveInputFormat",
         "The default input format. Set this to HiveInputFormat if you encounter problems with CombineHiveInputFormat."),
     HIVETEZINPUTFORMAT("hive.tez.input.format", "org.apache.hadoop.hive.ql.io.HiveInputFormat",

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -82,6 +82,7 @@
 import org.apache.hadoop.hive.ql.qoption.QTestOptionDispatcher;
 import org.apache.hadoop.hive.ql.qoption.QTestReplaceHandler;
 import org.apache.hadoop.hive.ql.qoption.QTestSysDbHandler;
+import org.apache.hadoop.hive.ql.qoption.QTestTimezoneHandler;
 import org.apache.hadoop.hive.ql.qoption.QTestTransactional;
 import org.apache.hadoop.hive.ql.scheduled.QTestScheduledQueryCleaner;
 import org.apache.hadoop.hive.ql.scheduled.QTestScheduledQueryServiceProvider;
@@ -226,6 +227,7 @@ public QTestUtil(QTestArguments testArgs) throws Exception {
     dispatcher.register("transactional", new QTestTransactional());
     dispatcher.register("scheduledqueryservice", new QTestScheduledQueryServiceProvider(conf));
     dispatcher.register("scheduledquerycleaner", new QTestScheduledQueryCleaner());
+    dispatcher.register("timezone", new QTestTimezoneHandler());
     dispatcher.register("authorizer", new QTestAuthorizerHandler());
     dispatcher.register("disabled", new QTestDisabledHandler());
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedListColumnReader.java
Patch:
@@ -50,8 +50,9 @@ public class VectorizedListColumnReader extends BaseVectorizedColumnReader {
 
   public VectorizedListColumnReader(ColumnDescriptor descriptor, PageReader pageReader,
       boolean skipTimestampConversion, ZoneId writerTimezone, boolean skipProlepticConversion,
-      Type type, TypeInfo hiveType) throws IOException {
-    super(descriptor, pageReader, skipTimestampConversion, writerTimezone, skipProlepticConversion, type, hiveType);
+      boolean legacyConversionEnabled, Type type, TypeInfo hiveType) throws IOException {
+    super(descriptor, pageReader, skipTimestampConversion, writerTimezone, skipProlepticConversion,
+        legacyConversionEnabled, type, hiveType);
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedPrimitiveColumnReader.java
Patch:
@@ -51,10 +51,12 @@ public VectorizedPrimitiveColumnReader(
       boolean skipTimestampConversion,
       ZoneId writerTimezone,
       boolean skipProlepticConversion,
+      boolean legacyConversionEnabled,
       Type type,
       TypeInfo hiveType)
       throws IOException {
-    super(descriptor, pageReader, skipTimestampConversion, writerTimezone, skipProlepticConversion, type, hiveType);
+    super(descriptor, pageReader, skipTimestampConversion, writerTimezone, skipProlepticConversion,
+        legacyConversionEnabled, type, hiveType);
   }
 
   @Override

File: ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetTimestampUtils.java
Patch:
@@ -187,9 +187,9 @@ public void testNanos() {
     Assert.assertEquals(n2.getTimeOfDayNanos() - n1.getTimeOfDayNanos(), 600000000009L);
 
     NanoTime n3 = new NanoTime(n1.getJulianDay() - 1, n1.getTimeOfDayNanos() + TimeUnit.DAYS.toNanos(1));
-    Assert.assertEquals(ts1, NanoTimeUtils.getTimestamp(n3, false, GMT));
+    Assert.assertEquals(ts1, NanoTimeUtils.getTimestamp(n3, false, GMT, false));
     n3 = new NanoTime(n1.getJulianDay() + 3, n1.getTimeOfDayNanos() - TimeUnit.DAYS.toNanos(3));
-    Assert.assertEquals(ts1, NanoTimeUtils.getTimestamp(n3, false, GMT));
+    Assert.assertEquals(ts1, NanoTimeUtils.getTimestamp(n3, false, GMT, false));
   }
 
   @Test

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -5119,7 +5119,7 @@ public static enum ConfVars {
 
     HIVE_QUERY_REEXECUTION_ENABLED("hive.query.reexecution.enabled", true,
         "Enable query reexecutions"),
-    HIVE_QUERY_REEXECUTION_STRATEGIES("hive.query.reexecution.strategies", "overlay,reoptimize,reexecute_lost_am",
+    HIVE_QUERY_REEXECUTION_STRATEGIES("hive.query.reexecution.strategies", "overlay,reoptimize,reexecute_lost_am,dagsubmit",
         "comma separated list of plugin can be used:\n"
             + "  overlay: hiveconf subtree 'reexec.overlay' is used as an overlay in case of an execution errors out\n"
             + "  reoptimize: collects operator statistics during execution and recompile the query after a failure\n"

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -394,12 +394,12 @@ private void compileInternal(String command, boolean deferClose) throws CommandP
     }
 
     PerfLogger perfLogger = SessionState.getPerfLogger(true);
-    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.WAIT_COMPILE);
+    perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.WAIT_COMPILE);
 
     try (CompileLock compileLock = CompileLockFactory.newInstance(driverContext.getConf(), command)) {
       boolean success = compileLock.tryAcquire();
 
-      perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.WAIT_COMPILE);
+      perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.WAIT_COMPILE);
 
       if (metrics != null) {
         metrics.decrementCounter(MetricsConstant.WAITING_COMPILE_OPS, 1);

File: ql/src/java/org/apache/hadoop/hive/ql/HookRunner.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql;
 
-import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.List;
 
@@ -292,9 +291,9 @@ private static void invokeGeneralHook(List<ExecuteWithHookContext> hooks, String
       PerfLogger perfLogger = SessionState.getPerfLogger();
 
       for (ExecuteWithHookContext hook : hooks) {
-        perfLogger.PerfLogBegin(CLASS_NAME, prefix + hook.getClass().getName());
+        perfLogger.perfLogBegin(CLASS_NAME, prefix + hook.getClass().getName());
         hook.run(hookContext);
-        perfLogger.PerfLogEnd(CLASS_NAME, prefix + hook.getClass().getName());
+        perfLogger.perfLogEnd(CLASS_NAME, prefix + hook.getClass().getName());
       }
     } catch (HiveException e) {
       throw e;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
Patch:
@@ -380,7 +380,7 @@ public void generateMapMetaData() throws HiveException {
   // Core logic to load hash table using HashTableLoader
   private Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]> loadHashTableInternal(
           ExecMapperContext mapContext, MapredContext mrContext) throws HiveException {
-    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.LOAD_HASHTABLE);
+    perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.LOAD_HASHTABLE);
     loader.init(mapContext, mrContext, hconf, this);
     try {
       loader.load(mapJoinTables, mapJoinTableSerdes);
@@ -399,7 +399,7 @@ private Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]> loadHashTabl
     Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]> pair =
             new ImmutablePair<> (mapJoinTables, mapJoinTableSerdes);
 
-    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.LOAD_HASHTABLE);
+    perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.LOAD_HASHTABLE);
 
     if (canSkipJoinProcessing(mapContext)) {
       LOG.info("Skipping big table join processing for " + this.toString());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -42,7 +42,6 @@
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockManager;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockObj;
 import org.apache.hadoop.hive.ql.lockmgr.LockException;
-import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;
 import org.apache.hadoop.hive.ql.log.PerfLogger;
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -91,7 +90,7 @@ private void moveFile(Path sourcePath, Path targetPath, boolean isDfsDir)
       throws HiveException {
     try {
       PerfLogger perfLogger = SessionState.getPerfLogger();
-      perfLogger.PerfLogBegin("MoveTask", PerfLogger.FILE_MOVES);
+      perfLogger.perfLogBegin("MoveTask", PerfLogger.FILE_MOVES);
 
       String mesg = "Moving data to " + (isDfsDir ? "" : "local ") + "directory "
           + targetPath.toString();
@@ -107,7 +106,7 @@ private void moveFile(Path sourcePath, Path targetPath, boolean isDfsDir)
         moveFileFromDfsToLocal(sourcePath, targetPath, fs, dstFs);
       }
 
-      perfLogger.PerfLogEnd("MoveTask", PerfLogger.FILE_MOVES);
+      perfLogger.perfLogEnd("MoveTask", PerfLogger.FILE_MOVES);
     } catch (Exception e) {
       throw new HiveException("Unable to move source " + sourcePath + " to destination "
           + targetPath, e);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java
Patch:
@@ -100,11 +100,11 @@ public void closeOp(boolean abort) throws HiveException {
         }
       } else {
         String method = PerfLogger.SPARK_FLUSH_HASHTABLE + getName();
-        perfLogger.PerfLogBegin(CLASS_NAME, method);
+        perfLogger.perfLogBegin(CLASS_NAME, method);
         try {
           flushToFile(mapJoinTables[tag], tag);
         } finally {
-          perfLogger.PerfLogEnd(CLASS_NAME, method);
+          perfLogger.perfLogEnd(CLASS_NAME, method);
         }
       }
       super.closeOp(abort);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkDynamicPartitionPruner.java
Patch:
@@ -79,11 +79,11 @@ public void prune(MapWork work, JobConf jobConf) throws HiveException, SerDeExce
       // Nothing to prune for this MapWork
       return;
     }
-    perfLogger.PerfLogBegin(CLASS_NAME,
+    perfLogger.perfLogBegin(CLASS_NAME,
             PerfLogger.SPARK_DYNAMICALLY_PRUNE_PARTITIONS + work.getName());
     processFiles(work, jobConf);
     prunePartitions(work);
-    perfLogger.PerfLogBegin(CLASS_NAME,
+    perfLogger.perfLogBegin(CLASS_NAME,
             PerfLogger.SPARK_DYNAMICALLY_PRUNE_PARTITIONS + work.getName());
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkMapRecordHandler.java
Patch:
@@ -64,7 +64,7 @@ public class SparkMapRecordHandler extends SparkRecordHandler {
 
   @Override
   public <K, V> void init(JobConf job, OutputCollector<K, V> output, Reporter reporter) throws Exception {
-    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.SPARK_INIT_OPERATORS);
+    perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.SPARK_INIT_OPERATORS);
     super.init(job, output, reporter);
 
     try {
@@ -124,7 +124,7 @@ public <K, V> void init(JobConf job, OutputCollector<K, V> output, Reporter repo
         throw new RuntimeException("Map operator initialization failed: " + e, e);
       }
     }
-    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.SPARK_INIT_OPERATORS);
+    perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.SPARK_INIT_OPERATORS);
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkReduceRecordHandler.java
Patch:
@@ -115,7 +115,7 @@ public class SparkReduceRecordHandler extends SparkRecordHandler {
   @Override
   @SuppressWarnings("unchecked")
   public void init(JobConf job, OutputCollector output, Reporter reporter) throws Exception {
-    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.SPARK_INIT_OPERATORS);
+    perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.SPARK_INIT_OPERATORS);
     super.init(job, output, reporter);
 
     rowObjectInspector = new ObjectInspector[Byte.MAX_VALUE];
@@ -258,7 +258,7 @@ public void init(JobConf job, OutputCollector output, Reporter reporter) throws
         throw new RuntimeException("Reduce operator initialization failed", e);
       }
     }
-    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.SPARK_INIT_OPERATORS);
+    perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.SPARK_INIT_OPERATORS);
   }
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java
Patch:
@@ -115,10 +115,10 @@ public int execute() {
       sparkWork.setRequiredCounterPrefix(getOperatorCounters());
 
       // Submit the Spark job
-      perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.SPARK_SUBMIT_JOB);
+      perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.SPARK_SUBMIT_JOB);
       submitTime = perfLogger.getStartTime(PerfLogger.SPARK_SUBMIT_JOB);
       jobRef = sparkSession.submit(taskQueue, context, sparkWork);
-      perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.SPARK_SUBMIT_JOB);
+      perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.SPARK_SUBMIT_JOB);
 
       // If the driver context has been shutdown (due to query cancellation) kill the Spark job
       if (taskQueue.isShutdown()) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/RenderStrategy.java
Patch:
@@ -88,14 +88,14 @@ private String getReport(Map<SparkStage, SparkStageProgress> progressMap) {
             completed.add(s);
 
             if (!perfLogger.startTimeHasMethod(PerfLogger.SPARK_RUN_STAGE + s)) {
-              perfLogger.PerfLogBegin(SparkJobMonitor.CLASS_NAME, PerfLogger.SPARK_RUN_STAGE + s);
+              perfLogger.perfLogBegin(SparkJobMonitor.CLASS_NAME, PerfLogger.SPARK_RUN_STAGE + s);
             }
-            perfLogger.PerfLogEnd(SparkJobMonitor.CLASS_NAME, PerfLogger.SPARK_RUN_STAGE + s);
+            perfLogger.perfLogEnd(SparkJobMonitor.CLASS_NAME, PerfLogger.SPARK_RUN_STAGE + s);
           }
           if (complete < total && (complete > 0 || running > 0 || failed > 0)) {
             /* stage is started, but not complete */
             if (!perfLogger.startTimeHasMethod(PerfLogger.SPARK_RUN_STAGE + s)) {
-              perfLogger.PerfLogBegin(SparkJobMonitor.CLASS_NAME, PerfLogger.SPARK_RUN_STAGE + s);
+              perfLogger.perfLogBegin(SparkJobMonitor.CLASS_NAME, PerfLogger.SPARK_RUN_STAGE + s);
             }
             if (failed > 0) {
               reportBuffer.append(

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java
Patch:
@@ -121,7 +121,7 @@ private void setLlapOfFragmentId(final ProcessorContext context) {
   @Override
   void init(MRTaskReporter mrReporter,
       Map<String, LogicalInput> inputs, Map<String, LogicalOutput> outputs) throws Exception {
-    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
+    perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
     super.init(mrReporter, inputs, outputs);
     checkAbortCondition();
 
@@ -351,7 +351,7 @@ void init(MRTaskReporter mrReporter,
         throw new RuntimeException("Map operator initialization failed", e);
       }
     }
-    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
+    perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
   }
 
   private void initializeMapRecordSources() throws Exception {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java
Patch:
@@ -74,7 +74,7 @@ void init(
       MRTaskReporter mrReporter, Map<String, LogicalInput> inputs,
       Map<String, LogicalOutput> outputs) throws Exception {
     // TODO HIVE-14042. Abort handling.
-    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
+    perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
     super.init(mrReporter, inputs, outputs);
     execContext = new ExecMapperContext(jconf);
 
@@ -142,7 +142,7 @@ public Object call() {
         throw new RuntimeException("Map operator initialization failed", e);
       }
     }
-    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
+    perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java
Patch:
@@ -101,7 +101,7 @@ public ReduceRecordProcessor(final JobConf jconf, final ProcessorContext context
   @Override
   void init(MRTaskReporter mrReporter, Map<String, LogicalInput> inputs, Map<String, LogicalOutput> outputs)
       throws Exception {
-    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
+    perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
     super.init(mrReporter, inputs, outputs);
 
     MapredContext.init(false, new JobConf(jconf));
@@ -241,7 +241,7 @@ void init(MRTaskReporter mrReporter, Map<String, LogicalInput> inputs, Map<Strin
       }
     }
 
-    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
+    perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
   }
 
   private void initializeMultipleSources(ReduceWork redWork, int numTags, ObjectInspector[] ois,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java
Patch:
@@ -34,8 +34,6 @@
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory;
 import org.apache.hadoop.hive.ql.log.PerfLogger;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
@@ -242,7 +240,7 @@ void init(JobConf jconf, Operator<?> reducer, boolean vectorized, TableDesc keyT
         throw new RuntimeException("Reduce operator initialization failed", e);
       }
     }
-    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
+    perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
   }
 
   public TableDesc getKeyTableDesc() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/monitoring/RenderStrategy.java
Patch:
@@ -93,17 +93,17 @@ private String getReport(Map<String, Progress> progressMap) {
            * We may have missed the start of the vertex due to the 3 seconds interval
            */
             if (!perfLogger.startTimeHasMethod(PerfLogger.TEZ_RUN_VERTEX + s)) {
-              perfLogger.PerfLogBegin(TezJobMonitor.CLASS_NAME, PerfLogger.TEZ_RUN_VERTEX + s);
+              perfLogger.perfLogBegin(TezJobMonitor.CLASS_NAME, PerfLogger.TEZ_RUN_VERTEX + s);
             }
 
             if (!perfLogger.endTimeHasMethod(PerfLogger.TEZ_RUN_VERTEX + s)) {
-              perfLogger.PerfLogEnd(TezJobMonitor.CLASS_NAME, PerfLogger.TEZ_RUN_VERTEX + s);
+              perfLogger.perfLogEnd(TezJobMonitor.CLASS_NAME, PerfLogger.TEZ_RUN_VERTEX + s);
             }
           }
           if (complete < total && (complete > 0 || running > 0 || failed > 0)) {
 
             if (!perfLogger.startTimeHasMethod(PerfLogger.TEZ_RUN_VERTEX + s)) {
-              perfLogger.PerfLogBegin(TezJobMonitor.CLASS_NAME, PerfLogger.TEZ_RUN_VERTEX + s);
+              perfLogger.perfLogBegin(TezJobMonitor.CLASS_NAME, PerfLogger.TEZ_RUN_VERTEX + s);
             }
 
           /* vertex is started, but not complete */

File: ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java
Patch:
@@ -504,7 +504,7 @@ public Set<Integer> getNonCombinablePathIndices(JobConf job, Path[] paths, int n
   @Override
   public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
     PerfLogger perfLogger = SessionState.getPerfLogger();
-    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.GET_SPLITS);
+    perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.GET_SPLITS);
     init(job);
 
     ArrayList<InputSplit> result = new ArrayList<InputSplit>();
@@ -532,7 +532,7 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
         }
       } catch (Exception e) {
         LOG.error("Error checking non-combinable path", e);
-        perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.GET_SPLITS);
+        perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.GET_SPLITS);
         throw new IOException(e);
       }
     }
@@ -585,7 +585,7 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
     }
 
     LOG.info("Number of all splits " + result.size());
-    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.GET_SPLITS);
+    perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.GET_SPLITS);
     return result.toArray(new InputSplit[result.size()]);
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
Patch:
@@ -756,7 +756,7 @@ Path[] getInputPaths(JobConf job) throws IOException {
   @Override
   public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
     PerfLogger perfLogger = SessionState.getPerfLogger();
-    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.GET_SPLITS);
+    perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.GET_SPLITS);
     init(job);
     Path[] dirs = getInputPaths(job);
     JobConf newjob = new JobConf(job);
@@ -853,7 +853,7 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
     if (LOG.isInfoEnabled()) {
       LOG.info("number of splits " + result.size());
     }
-    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.GET_SPLITS);
+    perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.GET_SPLITS);
     return result.toArray(new HiveInputSplit[result.size()]);
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java
Patch:
@@ -167,7 +167,7 @@ public void run() {
       ss.setIsHiveServerQuery(true); // All is served from HS2, we do not need e.g. Tez sessions
       SessionState.start(ss);
       PerfLogger perfLogger = SessionState.getPerfLogger();
-      perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.MATERIALIZED_VIEWS_REGISTRY_REFRESH);
+      perfLogger.perfLogBegin(CLASS_NAME, PerfLogger.MATERIALIZED_VIEWS_REGISTRY_REFRESH);
       try {
         if (initialized.get()) {
           for (Table mvTable : db.getAllMaterializedViewObjectsForRewriting()) {
@@ -200,7 +200,7 @@ public void run() {
           LOG.error("Problem connecting to the metastore when initializing the view registry", e);
         }
       }
-      perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.MATERIALIZED_VIEWS_REGISTRY_REFRESH);
+      perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.MATERIALIZED_VIEWS_REGISTRY_REFRESH);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/Transform.java
Patch:
@@ -42,15 +42,15 @@ public abstract class Transform {
   
   public void beginPerfLogging() {
     PerfLogger perfLogger = SessionState.getPerfLogger();
-    perfLogger.PerfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);
+    perfLogger.perfLogBegin(this.getClass().getName(), PerfLogger.OPTIMIZER);
   }
 
   public void endPerfLogging() {
     PerfLogger perfLogger = SessionState.getPerfLogger();
-    perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER);
+    perfLogger.perfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER);
   }
   public void endPerfLogging(String additionalInfo) {
     PerfLogger perfLogger = SessionState.getPerfLogger();
-	perfLogger.PerfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, additionalInfo);
+	perfLogger.perfLogEnd(this.getClass().getName(), PerfLogger.OPTIMIZER, additionalInfo);
   }  
 }

File: ql/src/test/org/apache/hadoop/hive/ql/hooks/TestHiveProtoLoggingHook.java
Patch:
@@ -209,8 +209,8 @@ public void testPreAndPostEventBoth() throws Exception {
   @Test
   public void testPostEventLog() throws Exception {
     context.setHookType(HookType.POST_EXEC_HOOK);
-    context.getPerfLogger().PerfLogBegin("test", "LogTest");
-    context.getPerfLogger().PerfLogEnd("test", "LogTest");
+    context.getPerfLogger().perfLogBegin("test", "LogTest");
+    context.getPerfLogger().perfLogEnd("test", "LogTest");
 
     EventLogger evtLogger = new EventLogger(conf, SystemClock.getInstance());
     evtLogger.handle(context);

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java
Patch:
@@ -103,7 +103,7 @@ public Object invoke(final Object proxy, final Method method, final Object[] arg
     int threadId = baseHandler.getThreadId();
     boolean error = true;
     PerfLogger perfLogger = PerfLogger.getPerfLogger(false);
-    perfLogger.PerfLogBegin(CLASS_NAME, method.getName());
+    perfLogger.perfLogBegin(CLASS_NAME, method.getName());
     try {
       Result result = invokeInternal(proxy, method, args);
       retryCount = result.numRetries;
@@ -113,7 +113,7 @@ public Object invoke(final Object proxy, final Method method, final Object[] arg
       StringBuilder additionalInfo = new StringBuilder();
       additionalInfo.append("threadId=").append(threadId).append(" retryCount=").append(retryCount)
         .append(" error=").append(error);
-      perfLogger.PerfLogEnd(CLASS_NAME, method.getName(), additionalInfo.toString());
+      perfLogger.perfLogEnd(CLASS_NAME, method.getName(), additionalInfo.toString());
     }
   }
 

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/metrics/JsonReporter.java
Patch:
@@ -37,6 +37,7 @@
 import java.io.BufferedWriter;
 import java.io.FileWriter;
 import java.io.IOException;
+import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.Paths;
@@ -156,7 +157,7 @@ public void report(SortedMap<String, Gauge> sortedMap, SortedMap<String, Counter
     // Use try .. finally to cleanup temp file if something goes wrong
     try {
       // Write json to the temp file
-      try (BufferedWriter bw = new BufferedWriter(new FileWriter(tmpFile.toFile()))) {
+      try (BufferedWriter bw = Files.newBufferedWriter(tmpFile, StandardCharsets.UTF_8)) {
         bw.write(json);
       } catch (IOException e) {
         LOG.error("Unable to write to temp file {}" + tmpFile, e);

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/metrics/Metrics.java
Patch:
@@ -75,7 +75,7 @@ public static MetricRegistry getRegistry() {
     return self.registry;
   }
 
-  public static void shutdown() {
+  public static synchronized void shutdown() {
     if (self != null) {
       for (ScheduledReporter reporter : self.scheduledReporters) {
         reporter.stop();

File: common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -620,7 +620,8 @@ public enum ErrorMsg {
     "from some other path : {1}.", true),
   REPL_INVALID_CONFIG_FOR_SERVICE(40008, "Invalid config error : {0} for {1} service.", true),
   REPL_INVALID_INTERNAL_CONFIG_FOR_SERVICE(40009, "Invalid internal config error : {0} for {1} service.", true),
-  REPL_RETRY_EXHAUSTED(40010, "Retry exhausted for retryable error code {0}.", true)
+  REPL_RETRY_EXHAUSTED(40010, "Retry exhausted for retryable error code {0}.", true),
+  REPL_FAILED_WITH_NON_RECOVERABLE_ERROR(40011, "Replication failed with non recoverable error. Needs manual intervention")
   ;
 
   private int errorCode;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplAck.java
Patch:
@@ -23,7 +23,8 @@
 public enum ReplAck {
     DUMP_ACKNOWLEDGEMENT("_finished_dump"),
     EVENTS_DUMP("_events_dump"),
-    LOAD_ACKNOWLEDGEMENT("_finished_load");
+    LOAD_ACKNOWLEDGEMENT("_finished_load"),
+    NON_RECOVERABLE_MARKER("_non_recoverable");
   private String ack;
   ReplAck(String ack) {
     this.ack = ack;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/event/Status.java
Patch:
@@ -24,5 +24,6 @@
 public enum Status {
   SUCCESS,
   FAILED,
-  IN_PROGRESS
+  IN_PROGRESS,
+  FAILED_ADMIN
 }

File: ql/src/test/org/apache/hadoop/hive/ql/exec/repl/TestRangerLoadTask.java
Patch:
@@ -79,6 +79,7 @@ public void setup() throws Exception {
 
   @Test
   public void testFailureInvalidAuthProviderEndpoint() {
+    Mockito.when(work.getCurrentDumpPath()).thenReturn(new Path("dumppath"));
     int status = task.execute();
     Assert.assertEquals(ErrorMsg.REPL_INVALID_CONFIG_FOR_SERVICE.getErrorCode(), status);
   }

File: common/src/java/org/apache/hive/http/HttpServer.java
Patch:
@@ -24,6 +24,7 @@
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.Paths;
+import java.security.KeyStore;
 import java.util.Collections;
 import java.util.Enumeration;
 import java.util.HashMap;
@@ -518,6 +519,7 @@ Connector createChannelConnector(int queueSize, Builder b) {
     } else {
       SslContextFactory sslContextFactory = new SslContextFactory();
       sslContextFactory.setKeyStorePath(b.keyStorePath);
+      sslContextFactory.setKeyStoreType(KeyStore.getDefaultType());
       Set<String> excludedSSLProtocols = Sets.newHashSet(
         Splitter.on(",").trimResults().omitEmptyStrings().split(
           Strings.nullToEmpty(b.conf.getVar(ConfVars.HIVE_SSL_PROTOCOL_BLACKLIST))));

File: service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpCLIService.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hive.service.cli.thrift;
 
+import java.security.KeyStore;
 import java.util.Arrays;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.SynchronousQueue;
@@ -143,6 +144,7 @@ public void onClosed(Connection connection) {
             + Arrays.toString(sslContextFactory.getExcludeProtocols()));
         sslContextFactory.setKeyStorePath(keyStorePath);
         sslContextFactory.setKeyStorePassword(keyStorePassword);
+        sslContextFactory.setKeyStoreType(KeyStore.getDefaultType());
         connector = new ServerConnector(server, sslContextFactory, http);
       } else {
         connector = new ServerConnector(server, http);

File: cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java
Patch:
@@ -331,6 +331,7 @@ public void testprocessInitFiles() throws Exception {
 
     sessionState.err = new SessionStream(data);
     sessionState.out = new SessionStream(System.out);
+    sessionState.setIsQtestLogging(true);
     try {
       CliSessionState.start(sessionState);
       CliDriver cliDriver = new CliDriver();

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -623,6 +623,7 @@ public String cliInit(File file) throws Exception {
     }
     File outf = new File(logDir, stdoutName);
     setSessionOutputs(fileName, ss, outf);
+    ss.setIsQtestLogging(true);
 
     if (fileName.equals("init_file.q")) {
       ss.initFiles.add(AbstractCliConfig.HIVE_ROOT + "/data/scripts/test_init_file.sql");
@@ -638,13 +639,14 @@ private void setSessionOutputs(String fileName, CliSessionState ss, File outf) t
       ss.out.flush();
     }
     if (ss.err != null) {
-      ss.out.flush();
+      ss.err.flush();
     }
 
     qTestResultProcessor.setOutputs(ss, fo, fileName);
 
     ss.err = new CachingPrintStream(fo, true, "UTF-8");
     ss.setIsSilent(true);
+    ss.setIsQtestLogging(true);
   }
 
   public CliSessionState startSessionState(boolean canReuseSession) throws IOException {

File: common/src/java/org/apache/hadoop/hive/common/FileUtils.java
Patch:
@@ -71,7 +71,6 @@
 public final class FileUtils {
   private static final Logger LOG = LoggerFactory.getLogger(FileUtils.class.getName());
   private static final Random random = new Random();
-  public static final int MAX_IO_ERROR_RETRY = 5;
   public static final int IO_ERROR_SLEEP_TIME = 100;
 
   public static final PathFilter HIDDEN_FILES_PATH_FILTER = new PathFilter() {

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
Patch:
@@ -1957,6 +1957,8 @@ public void testReplTargetOfReplication() throws Throwable {
       replica.dump(replicatedDbName);
     } catch (Exception e) {
       Assert.assertEquals("Cannot dump database as it is a Target of replication.", e.getMessage());
+      Assert.assertEquals(ErrorMsg.REPL_DATABASE_IS_TARGET_OF_REPLICATION.getErrorCode(),
+        ErrorMsg.getErrorMsg(e.getMessage()).getErrorCode());
     }
     replica.run("alter database " + replicatedDbName + " set dbproperties ('repl.source.for'='')");
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestTableLevelReplicationScenarios.java
Patch:
@@ -415,6 +415,8 @@ public void testIncorrectTablePolicyInReplDump() throws Throwable {
         LOG.info("Got exception: {}", ex.getMessage());
         Assert.assertTrue(ex instanceof SemanticException);
         Assert.assertTrue(ex.getMessage().equals(ErrorMsg.REPL_INVALID_DB_OR_TABLE_PATTERN.getMsg()));
+        Assert.assertEquals(ErrorMsg.REPL_INVALID_DB_OR_TABLE_PATTERN.getErrorCode(),
+          ErrorMsg.getErrorMsg(ex.getMessage()).getErrorCode());
         failed = true;
       }
       Assert.assertTrue(failed);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java
Patch:
@@ -161,8 +161,9 @@ private void initiateAuthorizationLoadTask() throws SemanticException {
       }
       childTasks.add(rangerLoadTask);
     } else {
-      throw new SemanticException("Authorizer " + conf.getVar(HiveConf.ConfVars.REPL_AUTHORIZATION_PROVIDER_SERVICE)
-              + " not supported for replication ");
+      throw new SemanticException(ErrorMsg.REPL_INVALID_CONFIG_FOR_SERVICE.format("Authorizer " +
+        conf.getVar(HiveConf.ConfVars.REPL_AUTHORIZATION_PROVIDER_SERVICE)
+              + " not supported for replication ", ReplUtils.REPL_RANGER_SERVICE));
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/atlas/NoOpAtlasRestClient.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.atlas.model.impexp.AtlasImportRequest;
 import org.apache.atlas.model.impexp.AtlasImportResult;
 import org.apache.atlas.model.impexp.AtlasServer;
+import org.apache.hadoop.hive.conf.HiveConf;
 
 import java.io.ByteArrayInputStream;
 import java.io.InputStream;
@@ -42,7 +43,7 @@ public AtlasImportResult importData(AtlasImportRequest request, AtlasReplInfo at
     return new AtlasImportResult(request, "", "", "", 0L);
   }
 
-  public AtlasServer getServer(String endpoint) {
+  public AtlasServer getServer(String endpoint, HiveConf conf) {
     return new AtlasServer();
   }
 
@@ -51,7 +52,7 @@ public String getEntityGuid(final String entityType,
     return UUID.randomUUID().toString();
   }
 
-  public boolean getStatus() {
+  public boolean getStatus(HiveConf conf) {
     return true;
   }
 }

File: ql/src/test/org/apache/hadoop/hive/ql/exec/repl/TestRangerLoadTask.java
Patch:
@@ -20,6 +20,7 @@
 import com.google.gson.Gson;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.repl.ranger.RangerExportPolicyList;
 import org.apache.hadoop.hive.ql.exec.repl.ranger.RangerPolicy;
 import org.apache.hadoop.hive.ql.exec.repl.ranger.RangerRestClientImpl;
@@ -79,7 +80,7 @@ public void setup() throws Exception {
   @Test
   public void testFailureInvalidAuthProviderEndpoint() {
     int status = task.execute();
-    Assert.assertEquals(40000, status);
+    Assert.assertEquals(ErrorMsg.REPL_INVALID_CONFIG_FOR_SERVICE.getErrorCode(), status);
   }
 
   @Test

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -511,8 +511,8 @@ public static enum ConfVars {
         "Turn on ChangeManager, so delete files will go to cmrootdir."),
     REPLCMDIR("hive.repl.cmrootdir","/user/${system:user.name}/cmroot/",
         "Root dir for ChangeManager, used for deleted files."),
-    REPLCMRETIAN("hive.repl.cm.retain","24h",
-        new TimeValidator(TimeUnit.HOURS),
+    REPLCMRETIAN("hive.repl.cm.retain","7d",
+        new TimeValidator(TimeUnit.DAYS),
         "Time to retain removed files in cmrootdir."),
     REPLCMENCRYPTEDDIR("hive.repl.cm.encryptionzone.rootdir", ".cmroot",
             "Root dir for ChangeManager if encryption zones are enabled, used for deleted files."),
@@ -571,7 +571,7 @@ public static enum ConfVars {
     REPL_ADD_RAW_RESERVED_NAMESPACE("hive.repl.add.raw.reserved.namespace", false,
         "For TDE with same encryption keys on source and target, allow Distcp super user to access \n"
             + "the raw bytes from filesystem without decrypting on source and then encrypting on target."),
-    REPL_INCLUDE_EXTERNAL_TABLES("hive.repl.include.external.tables", false,
+    REPL_INCLUDE_EXTERNAL_TABLES("hive.repl.include.external.tables", true,
         "Indicates if repl dump should include information about external tables. It should be \n"
           + "used in conjunction with 'hive.repl.dump.metadata.only' set to false. if 'hive.repl.dump.metadata.only' \n"
           + " is set to true then this config parameter has no effect as external table meta data is flushed \n"

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestExportImport.java
Patch:
@@ -54,6 +54,7 @@ public static void classLevelSetup() throws Exception {
     Configuration conf = new Configuration();
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
+    conf.set("hive.repl.include.external.tables", "false");
     MiniDFSCluster miniDFSCluster =
         new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
     HashMap<String, String> overridesForHiveConf = new HashMap<String, String>() {{

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
Patch:
@@ -1000,7 +1000,7 @@ public void testIncrementalDumpMultiIteration() throws Throwable {
                     "clustered by(key) into 2 buckets stored as orc")
             .run("create table table4 (i int, j int)")
             .run("insert into table4 values (1,2)")
-            .dump(primaryDbName, Collections.emptyList());
+            .dump(primaryDbName, Collections.singletonList("'hive.repl.include.external.tables'='false'"));
 
     String hiveDumpDir = incremental.dumpLocation + File.separator + ReplUtils.REPL_HIVE_BASE_DIR;
     Path path = new Path(hiveDumpDir);
@@ -1009,7 +1009,7 @@ public void testIncrementalDumpMultiIteration() throws Throwable {
     int numEvents = fileStatus.length - 3; //for _metadata, _finished_dump and _events_dump
 
     replica.load(replicatedDbName, primaryDbName,
-        Collections.singletonList("'hive.repl.approx.max.load.tasks'='1'"))
+        Arrays.asList("'hive.repl.approx.max.load.tasks'='1','hive.repl.include.external.tables'='false'"))
             .run("use " + replicatedDbName)
             .run("show tables")
             .verifyResults(new String[] {"table1", "table2", "table3", "table4", "table5" })

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
Patch:
@@ -597,7 +597,7 @@ public enum ConfVars {
             + "present in HMS Notification. Any key-value pair whose key is matched with any regex will"
             +" be removed from Parameters map during Serialization of Table/Partition object."),
     EVENT_DB_LISTENER_TTL("metastore.event.db.listener.timetolive",
-        "hive.metastore.event.db.listener.timetolive", 86400, TimeUnit.SECONDS,
+        "hive.metastore.event.db.listener.timetolive", 7, TimeUnit.DAYS,
         "time after which events will be removed from the database listener queue"),
     EVENT_CLEAN_MAX_EVENTS("metastore.event.db.clean.maxevents",
             "hive.metastore.event.db.clean.maxevents", 10000,

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveOpConverterUtils.java
Patch:
@@ -189,7 +189,7 @@ static ReduceSinkOperator genReduceSink(Operator<?> input, String tableAlias, Ex
           reduceKeys.size(), numReducers, acidOperation, NullOrdering.defaultNullOrder(hiveConf));
     } else {
       rsDesc = PlanUtils.getReduceSinkDesc(reduceKeys, reduceValues, outputColumnNames, false, tag,
-          partitionCols, order, nullOrder, NullOrdering.defaultNullOrder(hiveConf), numReducers, acidOperation);
+          partitionCols, order, nullOrder, NullOrdering.defaultNullOrder(hiveConf), numReducers, acidOperation, false);
     }
 
     ReduceSinkOperator rsOp = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
Patch:
@@ -711,7 +711,7 @@ public static ReduceSinkDesc getReduceSinkDesc(
       List<ExprNodeDesc> keyCols, List<ExprNodeDesc> valueCols,
       List<String> outputColumnNames, boolean includeKeyCols, int tag,
       List<ExprNodeDesc> partitionCols, String order, String nullOrder, NullOrdering defaultNullOrder,
-      int numReducers, AcidUtils.Operation writeType) {
+      int numReducers, AcidUtils.Operation writeType, boolean isCompaction) {
     ReduceSinkDesc reduceSinkDesc = getReduceSinkDesc(keyCols, keyCols.size(), valueCols,
             new ArrayList<List<Integer>>(),
             includeKeyCols ? outputColumnNames.subList(0, keyCols.size()) :
@@ -723,6 +723,7 @@ public static ReduceSinkDesc getReduceSinkDesc(
       reduceSinkDesc.setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.FIXED));
       reduceSinkDesc.setNumReducers(1);
     }
+    reduceSinkDesc.setIsCompaction(isCompaction);
     return reduceSinkDesc;
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/parse/type/TestBigIntCompareValidation.java
Patch:
@@ -70,7 +70,7 @@ private void testValidateUDFOnComparingBigInt(ExprNodeDesc nodeDesc) {
     try {
       TypeCheckCtx ctx = new TypeCheckCtx(null);
       processor.validateUDF(null, false, ctx, functionInfo,
-          Lists.newArrayList(constant, nodeDesc), functionInfo.getGenericUDF());
+          Lists.newArrayList(constant, nodeDesc));
       Assert.fail("Should throw exception as comparing a bigint and a " + nodeDesc.getTypeString());
     } catch (Exception e) {
       Assert.assertEquals(errorMsg, e.getMessage());

File: jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java
Patch:
@@ -332,7 +332,7 @@ public boolean next() throws SQLException {
     try {
       TFetchOrientation orientation = TFetchOrientation.FETCH_NEXT;
       if (fetchFirst) {
-        // If we are asked to start from begining, clear the current fetched resultset
+        // If we are asked to start from beginning, clear the current fetched resultset
         orientation = TFetchOrientation.FETCH_FIRST;
         fetchedRows = null;
         fetchedRowsItr = null;
@@ -341,6 +341,7 @@ public boolean next() throws SQLException {
       if (fetchedRows == null || !fetchedRowsItr.hasNext()) {
         TFetchResultsReq fetchReq = new TFetchResultsReq(stmtHandle,
             orientation, fetchSize);
+        LOG.debug("HiveQueryResultsFetchReq: {}", fetchReq);
         TFetchResultsResp fetchResp;
         fetchResp = client.FetchResults(fetchReq);
         Utils.verifySuccessWithInfo(fetchResp.getStatus());

File: jdbc/src/java/org/apache/hive/jdbc/Utils.java
Patch:
@@ -142,6 +142,8 @@ public static class JdbcConnectionParams {
     static final String WM_POOL = "wmPool";
     // Cookie prefix
     static final String HTTP_COOKIE_PREFIX = "http.cookie.";
+    // Create external purge table by default
+    static final String CREATE_TABLE_AS_EXTERNAL = "hiveCreateAsExternalLegacy";
 
     // We support ways to specify application name modeled after some existing DBs, since
     // there's no standard approach.

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzerBase.java
Patch:
@@ -19,7 +19,6 @@
 
 package org.apache.hive.hcatalog.cli.SemanticAnalysis;
 
-import java.io.Serializable;
 import java.util.List;
 
 import org.apache.hadoop.hive.metastore.api.Database;
@@ -123,7 +122,8 @@ protected void authorizeDDLWork(HiveSemanticAnalyzerHookContext context,
   protected void authorize(Privilege[] inputPrivs, Privilege[] outputPrivs)
     throws AuthorizationException, SemanticException {
     try {
-      getAuthProvider().authorize(inputPrivs, outputPrivs);
+      getAuthProvider().authorizeDbLevelOperations(inputPrivs, outputPrivs,
+          null, null);
     } catch (HiveException ex) {
       throw new SemanticException(ex);
     }

File: common/src/java/org/apache/hadoop/hive/common/cli/CommonCliOptions.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.commons.cli.OptionBuilder;
 import org.apache.commons.cli.Options;
 import org.apache.commons.cli.ParseException;
+import org.apache.hive.common.util.SuppressFBWarnings;
 import org.apache.logging.log4j.Level;
 
 /**
@@ -131,6 +132,7 @@ public void printUsage() {
    * Parse the arguments.
    * @param args
    */
+  @SuppressFBWarnings(value = "DM_EXIT", justification = "Expected")
   public void parse(String[] args) {
     try {
       commandLine = new GnuParser().parse(OPTIONS, args);

File: common/src/java/org/apache/hadoop/hive/common/io/DigestPrintStream.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.common.io;
 
 import java.io.OutputStream;
+import java.nio.charset.StandardCharsets;
 import java.security.MessageDigest;
 import java.util.Base64;
 
@@ -33,7 +34,7 @@ public DigestPrintStream(OutputStream out, String encoding) throws Exception {
 
   @Override
   protected void process(String out) {
-    digest.update(out.getBytes());
+    digest.update(out.getBytes(StandardCharsets.UTF_8));
   }
 
   @Override

File: common/src/java/org/apache/hadoop/hive/common/io/NonSyncByteArrayInputStream.java
Patch:
@@ -17,6 +17,8 @@
  */
 package org.apache.hadoop.hive.common.io;
 
+import org.apache.hive.common.util.SuppressFBWarnings;
+
 import java.io.ByteArrayInputStream;
 
 /**
@@ -36,6 +38,7 @@ public NonSyncByteArrayInputStream(byte[] buf, int offset, int length) {
     super(buf, offset, length);
   }
 
+  @SuppressFBWarnings(value = "EI_EXPOSE_REP2", justification = "Intended")
   public void reset(byte[] input, int start, int length) {
     buf = input;
     count = start + length;

File: common/src/java/org/apache/hadoop/hive/common/io/NonSyncByteArrayOutputStream.java
Patch:
@@ -17,6 +17,8 @@
  */
 package org.apache.hadoop.hive.common.io;
 
+import org.apache.hive.common.util.SuppressFBWarnings;
+
 import java.io.ByteArrayOutputStream;
 import java.io.DataInput;
 import java.io.IOException;
@@ -45,6 +47,7 @@ public NonSyncByteArrayOutputStream() {
     super();
   }
 
+  @SuppressFBWarnings(value = "EI_EXPOSE_REP", justification = "Ref external obj for efficiency")
   public byte[] getData() {
     return buf;
   }

File: common/src/java/org/apache/hadoop/hive/common/io/SortAndDigestPrintStream.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.common.io;
 
 import java.io.OutputStream;
+import java.nio.charset.StandardCharsets;
 import java.security.MessageDigest;
 import java.util.Base64;
 
@@ -35,7 +36,7 @@ public SortAndDigestPrintStream(OutputStream out, String encoding) throws Except
   public void processFinal() {
     while (!outputs.isEmpty()) {
       String row = outputs.removeFirst();
-      digest.update(row.getBytes());
+      digest.update(row.getBytes(StandardCharsets.UTF_8));
       printDirect(row);
     }
     printDirect(Base64.getEncoder().encodeToString(digest.digest()));

File: common/src/java/org/apache/hadoop/hive/common/log/InPlaceUpdate.java
Patch:
@@ -159,9 +159,8 @@ public void render(ProgressMonitor monitor) {
 
     // Map 1 .......... container  SUCCEEDED      7          7        0        0       0       0
     List<String> printReady = Lists.transform(monitor.rows(), new Function<List<String>, String>() {
-      @Nullable
       @Override
-      public String apply(@Nullable List<String> row) {
+      public String apply(List<String> row) {
         return String.format(VERTEX_FORMAT, row.toArray());
       }
     });

File: common/src/java/org/apache/hadoop/hive/common/log/LogRedirector.java
Patch:
@@ -24,6 +24,7 @@
 import java.io.InputStream;
 import java.io.InputStreamReader;
 
+import java.nio.charset.StandardCharsets;
 import java.util.List;
 
 /**
@@ -44,14 +45,14 @@ public interface LogSourceCallback {
   private int numErrLogLines = 0;
 
   public LogRedirector(InputStream in, Logger logger, LogSourceCallback callback) {
-    this.in = new BufferedReader(new InputStreamReader(in));
+    this.in = new BufferedReader(new InputStreamReader(in, StandardCharsets.UTF_8));
     this.callback = callback;
     this.logger = logger;
   }
 
   public LogRedirector(InputStream in, Logger logger, List<String> errLogs,
                        LogSourceCallback callback) {
-    this.in = new BufferedReader(new InputStreamReader(in));
+    this.in = new BufferedReader(new InputStreamReader(in, StandardCharsets.UTF_8));
     this.errLogs = errLogs;
     this.callback = callback;
     this.logger = logger;

File: common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/JsonFileMetricsReporter.java
Patch:
@@ -31,6 +31,7 @@
 import java.io.BufferedWriter;
 import java.io.FileWriter;
 import java.io.IOException;
+import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.Paths;
@@ -163,7 +164,7 @@ public void run() {
       }
 
       // Write json to the temp file.
-      try (BufferedWriter bw = new BufferedWriter(new FileWriter(tmpFile.toFile()))) {
+      try (BufferedWriter bw = Files.newBufferedWriter(tmpFile, StandardCharsets.UTF_8)) {
         bw.write(json);
       } catch (IOException e) {
         LOGGER.error("Unable to write to temp file " + tmpFile, e);

File: common/src/java/org/apache/hadoop/hive/conf/LoopingByteArrayInputStream.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.conf;
 
+import org.apache.hive.common.util.SuppressFBWarnings;
+
 import java.io.ByteArrayInputStream;
 import java.io.IOException;
 import java.io.InputStream;
@@ -40,6 +42,7 @@ public class LoopingByteArrayInputStream extends InputStream {
 
   private final byte[] buf;
 
+  @SuppressFBWarnings(value = "EI_EXPOSE_REP2", justification = "Intended")
   public LoopingByteArrayInputStream(byte[] buf) {
     this.buf = buf;
   }

File: common/src/java/org/apache/hadoop/hive/conf/SystemVariables.java
Patch:
@@ -31,7 +31,7 @@
 public class SystemVariables {
 
   private static final Logger l4j = LoggerFactory.getLogger(SystemVariables.class);
-  protected static Pattern varPat = Pattern.compile("\\$\\{[^\\}\\$\u0020]+\\}");
+  protected static final Pattern varPat = Pattern.compile("\\$\\{[^\\}\\$\u0020]+\\}");
   private static final SystemVariables INSTANCE = new SystemVariables();
   private static final Map<String, VariableCoercion> COERCIONS =
       ImmutableMap.<String, VariableCoercion>builder()

File: common/src/java/org/apache/hive/common/util/FixedSizedObjectPool.java
Patch:
@@ -310,7 +310,7 @@ public void log(long oldVal, long newVal) {
     public synchronized void dumpLog(boolean doSleep) {
       if (doSleep) {
         try {
-          Thread.sleep(100);
+          this.wait(100);
         } catch (InterruptedException e) {
         }
       }

File: common/src/java/org/apache/hive/common/util/Ref.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hive.common.util;
 
 /** Reference to T. */
+@SuppressFBWarnings(value = "URF_UNREAD_PUBLIC_OR_PROTECTED_FIELD", justification = "Intentional exposure")
 public final class Ref<T> {
   public T value;
 

File: common/src/java/org/apache/hive/common/util/ShutdownHookManager.java
Patch:
@@ -122,6 +122,7 @@ private static class DeleteOnExitHook implements Runnable {
     private final Set<File> deleteTargets = Collections.synchronizedSet(new HashSet<File>());
 
     @Override
+    @SuppressFBWarnings(value = "RV_RETURN_VALUE_IGNORED_BAD_PRACTICE", justification = "Intended")
     public void run() {
       for (File deleteTarget : deleteTargets) {
         deleteTarget.delete();

File: common/src/java/org/apache/hive/common/util/StreamPrinter.java
Patch:
@@ -23,6 +23,7 @@
 import java.io.InputStream;
 import java.io.InputStreamReader;
 import java.io.PrintStream;
+import java.nio.charset.StandardCharsets;
 
 import org.apache.hadoop.io.IOUtils;
 
@@ -45,7 +46,7 @@ public StreamPrinter(InputStream is, String type, PrintStream... outputStreams)
   public void run() {
     BufferedReader br = null;
     try {
-      InputStreamReader isr = new InputStreamReader(is);
+      InputStreamReader isr = new InputStreamReader(is, StandardCharsets.UTF_8);
       br = new BufferedReader(isr);
       String line = null;
       if (type != null) {

File: common/src/java/org/apache/hive/http/ProfileServlet.java
Patch:
@@ -146,7 +146,7 @@ enum Output {
   private Lock profilerLock = new ReentrantLock();
   private Integer pid;
   private String asyncProfilerHome;
-  private Process process;
+  private transient Process process;
 
   public ProfileServlet() {
     this.asyncProfilerHome = getAsyncProfilerHome();

File: common/src/java/org/apache/hive/http/security/PamConstraint.java
Patch:
@@ -16,6 +16,7 @@
  */
 package org.apache.hive.http.security;
 
+import org.apache.hive.common.util.SuppressFBWarnings;
 import org.eclipse.jetty.util.security.Constraint;
 
 public class PamConstraint extends Constraint {
@@ -27,6 +28,7 @@ public boolean getAuthenticate() {
   }
 
   @Override
+  @SuppressFBWarnings(value = "EI_EXPOSE_REP", justification = "Ref external obj for efficiency")
   public String[] getRoles() {
     return roles;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/log/state/AtlasDumpBegin.java
Patch:
@@ -17,8 +17,8 @@
  */
 package org.apache.hadoop.hive.ql.parse.repl.dump.log.state;
 
-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 import org.apache.hadoop.hive.ql.parse.repl.ReplState;
+import org.apache.hive.common.util.SuppressFBWarnings;
 import org.codehaus.jackson.annotate.JsonProperty;
 
 /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/log/state/AtlasDumpEnd.java
Patch:
@@ -17,8 +17,8 @@
  */
 package org.apache.hadoop.hive.ql.parse.repl.dump.log.state;
 
-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 import org.apache.hadoop.hive.ql.parse.repl.ReplState;
+import org.apache.hive.common.util.SuppressFBWarnings;
 import org.codehaus.jackson.annotate.JsonProperty;
 
 /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/log/state/RangerDumpBegin.java
Patch:
@@ -17,8 +17,8 @@
  */
 package org.apache.hadoop.hive.ql.parse.repl.dump.log.state;
 
-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 import org.apache.hadoop.hive.ql.parse.repl.ReplState;
+import org.apache.hive.common.util.SuppressFBWarnings;
 import org.codehaus.jackson.annotate.JsonProperty;
 
 /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/log/state/RangerDumpEnd.java
Patch:
@@ -17,8 +17,8 @@
  */
 package org.apache.hadoop.hive.ql.parse.repl.dump.log.state;
 
-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 import org.apache.hadoop.hive.ql.parse.repl.ReplState;
+import org.apache.hive.common.util.SuppressFBWarnings;
 import org.codehaus.jackson.annotate.JsonProperty;
 
 /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/log/state/AtlasLoadBegin.java
Patch:
@@ -17,8 +17,8 @@
  */
 package org.apache.hadoop.hive.ql.parse.repl.load.log.state;
 
-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 import org.apache.hadoop.hive.ql.parse.repl.ReplState;
+import org.apache.hive.common.util.SuppressFBWarnings;
 import org.codehaus.jackson.annotate.JsonProperty;
 
 /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/log/state/AtlasLoadEnd.java
Patch:
@@ -17,8 +17,8 @@
  */
 package org.apache.hadoop.hive.ql.parse.repl.load.log.state;
 
-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 import org.apache.hadoop.hive.ql.parse.repl.ReplState;
+import org.apache.hive.common.util.SuppressFBWarnings;
 import org.codehaus.jackson.annotate.JsonProperty;
 
 /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/log/state/RangerLoadBegin.java
Patch:
@@ -17,8 +17,8 @@
  */
 package org.apache.hadoop.hive.ql.parse.repl.load.log.state;
 
-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 import org.apache.hadoop.hive.ql.parse.repl.ReplState;
+import org.apache.hive.common.util.SuppressFBWarnings;
 import org.codehaus.jackson.annotate.JsonProperty;
 
 /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/log/state/RangerLoadEnd.java
Patch:
@@ -17,8 +17,8 @@
  */
 package org.apache.hadoop.hive.ql.parse.repl.load.log.state;
 
-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 import org.apache.hadoop.hive.ql.parse.repl.ReplState;
+import org.apache.hive.common.util.SuppressFBWarnings;
 import org.codehaus.jackson.annotate.JsonProperty;
 
 /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/event/StageMapper.java
Patch:
@@ -17,7 +17,7 @@
  */
 package org.apache.hadoop.hive.ql.parse.repl.metric.event;
 
-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
+import org.apache.hive.common.util.SuppressFBWarnings;
 
 import java.util.ArrayList;
 import java.util.List;

File: shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
Patch:
@@ -39,7 +39,6 @@
 import java.util.TreeMap;
 import javax.security.auth.Subject;
 
-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.crypto.CipherSuite;
@@ -100,6 +99,7 @@
 import org.apache.hadoop.tools.DistCpOptions;
 import org.apache.hadoop.tools.DistCpOptions.FileAttribute;
 import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.util.SuppressFBWarnings;
 import org.apache.hadoop.yarn.conf.YarnConfiguration;
 import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration;
 import org.apache.tez.dag.api.TezConfiguration;

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
Patch:
@@ -33,11 +33,11 @@
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
-import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.common.ZooKeeperHiveHelper;
 import org.apache.hadoop.hive.metastore.utils.StringUtils;
 import org.apache.hadoop.security.alias.CredentialProviderFactory;
+import org.apache.hive.common.util.SuppressFBWarnings;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java
Patch:
@@ -608,15 +608,15 @@ public void expandAndRehashToTarget(int estimateNewRowCount) {
     int newRefsCount = oldRefsCount + estimateNewRowCount;
     if (resizeThreshold <= newRefsCount) {
       newRefsCount =
-          (Long.bitCount(newRefsCount) == 1) ? estimateNewRowCount : nextHighestPowerOfTwo(newRefsCount);
+          (Long.bitCount(newRefsCount) == 1) ? newRefsCount : nextHighestPowerOfTwo(newRefsCount);
       expandAndRehashImpl(newRefsCount);
       LOG.info("Expand and rehash to " + newRefsCount + " from " + oldRefsCount);
     }
   }
 
   private static void validateCapacity(long capacity) {
     if (Long.bitCount(capacity) != 1) {
-      throw new AssertionError("Capacity must be a power of two");
+      throw new AssertionError("Capacity must be a power of two but got " + capacity);
     }
     if (capacity <= 0) {
       throw new AssertionError("Invalid capacity " + capacity);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/MetricSink.java
Patch:
@@ -112,15 +112,16 @@ public void run() {
           LOG.debug("Converting metrics to thrift metrics {} ", metrics.size());
           int totalMetricsSize = metrics.size();
           List<ReplicationMetrics> replicationMetricsList = new ArrayList<>(totalMetricsSize);
+          ObjectMapper mapper = new ObjectMapper();
           for (int index = 0; index < totalMetricsSize; index++) {
             ReplicationMetric metric = metrics.removeFirst();
             ReplicationMetrics persistentMetric = new ReplicationMetrics();
             persistentMetric.setDumpExecutionId(metric.getDumpExecutionId());
             persistentMetric.setScheduledExecutionId(metric.getScheduledExecutionId());
             persistentMetric.setPolicy(metric.getPolicy());
-            ObjectMapper mapper = new ObjectMapper();
             persistentMetric.setProgress(mapper.writeValueAsString(metric.getProgress()));
             persistentMetric.setMetadata(mapper.writeValueAsString(metric.getMetadata()));
+            LOG.debug("Metric to be persisted {} ", persistentMetric);
             replicationMetricsList.add(persistentMetric);
           }
           metricList.setReplicationMetricList(replicationMetricsList);

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
Patch:
@@ -2143,9 +2143,6 @@ public RowReader<OrcStruct> getReader(InputSplit inputSplit,
         LOG.warn("Can't determine bucket ID for " + split.getPath() + "; ignoring");
       }
       bucket = acidIOOptions.getBucketId();
-      if(split.isOriginal()) {
-        mergerOptions.copyIndex(acidIOOptions.getCopyNumber()).bucketPath(split.getPath());
-      }
     } else {
       bucket = (int) split.getStart();
       assert false : "We should never have a split w/o base in acid 2.0 for full acid: " + split.getPath();

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
Patch:
@@ -3692,7 +3692,7 @@ public void testACIDReaderNoFooterSerializeWithDeltas() throws Exception {
         readOpsDelta = statistics.getReadOps() - readOpsBefore;
       }
     }
-    assertEquals(5, readOpsDelta);
+    assertEquals(4, readOpsDelta);
 
     // revert back to local fs
     conf.set("fs.defaultFS", "file:///");
@@ -3766,7 +3766,7 @@ public void testACIDReaderFooterSerializeWithDeltas() throws Exception {
         readOpsDelta = statistics.getReadOps() - readOpsBefore;
       }
     }
-    assertEquals(5, readOpsDelta);
+    assertEquals(4, readOpsDelta);
 
     // revert back to local fs
     conf.set("fs.defaultFS", "file:///");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
Patch:
@@ -865,6 +865,7 @@ Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive hiveDb)
         }
       }
       replLogger.endLog(bootDumpBeginReplId.toString());
+      work.getMetricCollector().reportStageEnd(getName(), Status.SUCCESS, bootDumpBeginReplId);
     }
     Long bootDumpEndReplId = currentNotificationId(hiveDb);
     LOG.info("Preparing to return {},{}->{}",
@@ -875,7 +876,6 @@ Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive hiveDb)
 
     work.setDirCopyIterator(extTableCopyWorks.iterator());
     work.setManagedTableCopyPathIterator(managedTableCopyPaths.iterator());
-    work.getMetricCollector().reportStageEnd(getName(), Status.SUCCESS, bootDumpBeginReplId);
     return bootDumpBeginReplId;
   }
 

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ReplicationMetricsMaintTask.java
Patch:
@@ -63,10 +63,11 @@ public void run() {
       if (!MetastoreConf.getBoolVar(conf, ConfVars.SCHEDULED_QUERIES_ENABLED)) {
         return;
       }
+      LOG.debug("Cleaning up older Metrics");
       RawStore ms = HiveMetaStore.HMSHandler.getMSForConf(conf);
       int maxRetainSecs = (int) TimeUnit.DAYS.toSeconds(MetastoreConf.getTimeVar(conf,
         ConfVars.REPL_METRICS_MAX_AGE, TimeUnit.DAYS));
-      int deleteCnt = ms.deleteScheduledExecutions(maxRetainSecs);
+      int deleteCnt = ms.deleteReplicationMetrics(maxRetainSecs);
       if (deleteCnt > 0L){
         LOG.info("Number of deleted entries: " + deleteCnt);
       }

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java
Patch:
@@ -99,7 +99,8 @@ LockState lock(LockRequest lock, String queryId, boolean isBlocking, List<HiveLo
     MAX_SLEEP = Math.max(15000, conf.getTimeVar(HiveConf.ConfVars.HIVE_LOCK_SLEEP_BETWEEN_RETRIES, TimeUnit.MILLISECONDS));
     int maxNumWaits = Math.max(0, conf.getIntVar(HiveConf.ConfVars.HIVE_LOCK_NUMRETRIES));
     try {
-      LOG.info("Requesting: queryId=" + queryId + " " + lock);
+      LOG.info("Requesting lock for queryId=" + queryId);
+      LOG.debug("Requested lock= " + lock);
       LockResponse res = txnManager.getMS().lock(lock);
       //link lockId to queryId
       LOG.info("Response to queryId=" + queryId + " " + res);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1831,7 +1831,8 @@ public static enum ConfVars {
         "the group by in the mapper by using BucketizedHiveInputFormat. The only downside to this\n" +
         "is that it limits the number of mappers to the number of files."),
     HIVE_DEFAULT_NULLS_LAST("hive.default.nulls.last", true,
-        "Whether to set NULLS LAST as the default null ordering"),
+        "Whether to set NULLS LAST as the default null ordering for ASC order and " +
+            "NULLS FIRST for DESC order."),
     HIVE_GROUPBY_POSITION_ALIAS("hive.groupby.position.alias", false,
         "Whether to enable using Column Position Alias in Group By"),
     HIVE_ORDERBY_POSITION_ALIAS("hive.orderby.position.alias", true,

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestScheduledReplicationScenarios.java
Patch:
@@ -180,6 +180,7 @@ public void testAcidTablesReplLoadBootstrapIncr() throws Throwable {
   }
 
   @Test
+  @Ignore("HIVE-23395")
   public void testExternalTablesReplLoadBootstrapIncr() throws Throwable {
     // Bootstrap
     String withClause = " WITH('" + HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR.varname

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/dbinstall/rules/DatabaseRule.java
Patch:
@@ -134,10 +134,10 @@ public void after() { // stopAndRmDockerContainer
       return;
     }
     try {
-      if (runCmdAndPrintStreams(buildStopCmd(), 60) != 0) {
+      if (runCmdAndPrintStreams(buildStopCmd(), 600) != 0) {
         throw new RuntimeException("Unable to stop docker container");
       }
-      if (runCmdAndPrintStreams(buildRmCmd(), 15) != 0) {
+      if (runCmdAndPrintStreams(buildRmCmd(), 600) != 0) {
         throw new RuntimeException("Unable to remove docker container");
       }
     } catch (InterruptedException | IOException e) {

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/dbinstall/rules/Mysql.java
Patch:
@@ -54,7 +54,7 @@ public String getJdbcDriver() {
 
   @Override
   public String getJdbcUrl() {
-    return "jdbc:mysql://localhost:3306/" + HIVE_DB + "?sessionVariables=sql_mode=ANSI_QUOTES";
+    return "jdbc:mysql://localhost:3306/" + HIVE_DB;
   }
 
   @Override

File: itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestRemoteHiveMetaStoreDualAuthKerb.java
Patch:
@@ -70,7 +70,7 @@ private void init(){
     }
 
     @Override
-    public void Authenticate(String user, String password) throws AuthenticationException {
+    public void authenticate(String user, String password) throws AuthenticationException {
 
       if(!userMap.containsKey(user)) {
         throw new AuthenticationException("Invalid user : "+user);

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreAnonymousAuthenticationProviderImpl.java
Patch:
@@ -26,7 +26,7 @@
 public class MetaStoreAnonymousAuthenticationProviderImpl implements MetaStorePasswdAuthenticationProvider {
 
   @Override
-  public void Authenticate(String user, String password) throws AuthenticationException {
+  public void authenticate(String user, String password) throws AuthenticationException {
     // no-op authentication
   }
 

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreConfigAuthenticationProviderImpl.java
Patch:
@@ -53,7 +53,7 @@ public class MetaStoreConfigAuthenticationProviderImpl implements MetaStorePassw
   }
 
   @Override
-  public void Authenticate(String authUser, String authPassword) throws AuthenticationException {
+  public void authenticate(String authUser, String authPassword) throws AuthenticationException {
     if (!userName.equals(authUser)) {
       LOG.debug("Invalid user " + authUser);
       throw new AuthenticationException("Invalid credentials");

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/MetaStoreCustomAuthenticationProviderImpl.java
Patch:
@@ -63,8 +63,8 @@ public class MetaStoreCustomAuthenticationProviderImpl implements MetaStorePassw
   }
 
   @Override
-  public void Authenticate(String user, String password) throws AuthenticationException {
-    customProvider.Authenticate(user, password);
+  public void authenticate(String user, String password) throws AuthenticationException {
+    customProvider.authenticate(user, password);
   }
 
 }

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/MetaStorePasswdAuthenticationProvider.java
Patch:
@@ -35,5 +35,5 @@ public interface MetaStorePasswdAuthenticationProvider {
    * @throws AuthenticationException When a user is found to be
    *                                 invalid by the implementation
    */
-  void Authenticate(String user, String password) throws AuthenticationException;
+  void authenticate(String user, String password) throws AuthenticationException;
 }

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/MetaStorePlainSaslHelper.java
Patch:
@@ -115,7 +115,7 @@ public void handle(Callback[] callbacks) throws IOException, UnsupportedCallback
       }
       MetaStorePasswdAuthenticationProvider provider =
         MetaStoreAuthenticationProviderFactory.getAuthenticationProvider(conf, authMethod);
-      provider.Authenticate(username, password);
+      provider.authenticate(username, password);
       if (ac != null) {
         ac.setAuthorized(true);
       }

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.metastore;
 
 import java.io.IOException;
+import java.nio.charset.StandardCharsets;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.Executors;
@@ -293,7 +294,7 @@ public Boolean execute() throws IOException {
         // xattr has limited capacity. We shall revisit and store all original
         // locations if orig-loc becomes important
         try {
-          fs.setXAttr(cmPath, ORIG_LOC_TAG, path.toString().getBytes());
+          fs.setXAttr(cmPath, ORIG_LOC_TAG, path.toString().getBytes(StandardCharsets.UTF_8));
         } catch (UnsupportedOperationException e) {
           LOG.warn("Error setting xattr for {}", path.toString());
         }

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecProxy.java
Patch:
@@ -24,6 +24,7 @@
 
 import java.util.List;
 import java.util.Map;
+import java.util.NoSuchElementException;
 
 /**
  * Polymorphic proxy class, equivalent to org.apache.hadoop.hive.metastore.api.PartitionSpec.
@@ -213,7 +214,7 @@ public static class SimplePartitionWrapperIterator implements PartitionIterator
     @Override public String getLocation() { return partition.getSd().getLocation(); }
     @Override public void setCreateTime(long time) { partition.setCreateTime((int)time);}
     @Override public boolean hasNext() { return false; } // No next partition.
-    @Override public Partition next() { return null; } // No next partition.
+    @Override public Partition next() { throw new NoSuchElementException(); } // No next partition.
     @Override public void remove() {} // Do nothing.
   } // P
 

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/security/HadoopThriftAuthBridge23.java
Patch:
@@ -52,8 +52,10 @@ public class HadoopThriftAuthBridge23 extends HadoopThriftAuthBridge {
       // HADOOP-10221, HADOOP-10451)
       try {
         RES_GET_INSTANCE_METHOD = SASL_PROPERTIES_RESOLVER_CLASS.getMethod("getInstance",
-            Configuration.class);
+                Configuration.class);
         GET_DEFAULT_PROP_METHOD = SASL_PROPERTIES_RESOLVER_CLASS.getMethod("getDefaultProperties");
+      } catch (RuntimeException e) {
+        throw e;
       } catch (Exception e) {
         // this must be hadoop 2.4 , where getDefaultProperties was protected
       }

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/FileUtils.java
Patch:
@@ -243,7 +243,7 @@ public static boolean rename(FileSystem srcFs, FileSystem destFs, Path srcPath,
   }
 
   private static boolean needsEscaping(char c) {
-    return c >= 0 && c < charToEscape.size() && charToEscape.get(c);
+    return c < charToEscape.size() && charToEscape.get(c);
   }
 
   public static String escapePathName(String path) {

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/Retry.java
Patch:
@@ -60,7 +60,7 @@ public T runWithDelay() throws Exception {
         if (MAX_RETRIES == tries) {
           throw e;
         } else {
-          Thread.sleep(DELAY * tries);
+          Thread.sleep((long) DELAY * tries);
           return runWithDelay();
         }
       } else {

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreCustomAuth.java
Patch:
@@ -153,7 +153,7 @@ private void init(){
     }
 
     @Override
-    public void Authenticate(String user, String password) throws AuthenticationException {
+    public void authenticate(String user, String password) throws AuthenticationException {
 
       if(!userMap.containsKey(user)) {
         throw new AuthenticationException("Invalid user : " + user);

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/conf/TestMetastoreConf.java
Patch:
@@ -131,7 +131,7 @@ public void defaults() {
     conf = MetastoreConf.newMetastoreConf();
     Assert.assertEquals("defaultval", MetastoreConf.getVar(conf, ConfVars.STR_TEST_ENTRY));
     Assert.assertEquals(42, MetastoreConf.getLongVar(conf, ConfVars.LONG_TEST_ENTRY));
-    Assert.assertEquals(3.141592654, MetastoreConf.getDoubleVar(conf, ConfVars.DOUBLE_TEST_ENTRY),
+    Assert.assertEquals(Math.PI, MetastoreConf.getDoubleVar(conf, ConfVars.DOUBLE_TEST_ENTRY),
         0.0000001);
     Assert.assertTrue(MetastoreConf.getBoolVar(conf, ConfVars.BOOLEAN_TEST_ENTRY));
     Assert.assertEquals(1, MetastoreConf.getTimeVar(conf, ConfVars.TIME_TEST_ENTRY, TimeUnit.SECONDS));
@@ -148,7 +148,7 @@ public void defaults() {
     Assert.assertEquals("defaultval", MetastoreConf.get(conf, ConfVars.STR_TEST_ENTRY.getHiveName()));
     Assert.assertEquals("defaultval", MetastoreConf.getAsString(conf, ConfVars.STR_TEST_ENTRY));
     Assert.assertEquals("42", MetastoreConf.getAsString(conf, ConfVars.LONG_TEST_ENTRY));
-    Assert.assertEquals("3.141592654", MetastoreConf.getAsString(conf, ConfVars.DOUBLE_TEST_ENTRY));
+    Assert.assertEquals("" + Math.PI, MetastoreConf.getAsString(conf, ConfVars.DOUBLE_TEST_ENTRY));
     Assert.assertEquals("true", MetastoreConf.getAsString(conf, ConfVars.BOOLEAN_TEST_ENTRY));
   }
 

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/ldap/LdapAuthenticationTestCase.java
Patch:
@@ -43,7 +43,7 @@ private LdapAuthenticationTestCase(Builder builder) {
 
   public void assertAuthenticatePasses(Credentials credentials) {
     try {
-      ldapProvider.Authenticate(credentials.getUser(), credentials.getPassword());
+      ldapProvider.authenticate(credentials.getUser(), credentials.getPassword());
     } catch (AuthenticationException e) {
       String message = String.format("Authentication failed for user '%s' with password '%s'",
           credentials.getUser(), credentials.getPassword());
@@ -61,7 +61,7 @@ public void assertAuthenticateFailsUsingWrongPassword(Credentials credentials) {
 
   public void assertAuthenticateFails(String user, String password) {
     try {
-      ldapProvider.Authenticate(user, password);
+      ldapProvider.authenticate(user, password);
       Assert.fail(String.format("Expected authentication to fail for %s", user));
     } catch (AuthenticationException expected) {
       Assert.assertNotNull("Expected authentication exception", expected);

File: storage-api/src/java/org/apache/hadoop/hive/common/io/DataCache.java
Patch:
@@ -19,10 +19,12 @@
 package org.apache.hadoop.hive.common.io;
 
 import org.apache.hadoop.hive.common.io.encoded.MemoryBuffer;
+import org.apache.hive.common.util.SuppressFBWarnings;
 
 /** An abstract data cache that IO formats can use to retrieve and cache data. */
 public interface DataCache {
-  public static final class BooleanRef {
+  @SuppressFBWarnings(value = "UUF_UNUSED_PUBLIC_OR_PROTECTED_FIELD", justification = "Used by interface consumers")
+  final class BooleanRef {
     public boolean value;
   }
 

File: storage-api/src/java/org/apache/hadoop/hive/common/type/FastHiveDecimal.java
Patch:
@@ -23,6 +23,7 @@
 import java.io.OutputStream;
 import java.math.BigDecimal;
 import java.math.BigInteger;
+import java.nio.charset.StandardCharsets;
 
 /**
  *    FastHiveDecimal is a mutable fast decimal object.  It is the base class for both the
@@ -188,7 +189,7 @@ protected boolean fastSetFromBigIntegerAndScale(
   }
 
   protected boolean fastSetFromString(String string, boolean trimBlanks) {
-    byte[] bytes = string.getBytes();
+    byte[] bytes = string.getBytes(StandardCharsets.UTF_8);
     return
         fastSetFromBytes(
             bytes, 0, bytes.length, trimBlanks);

File: storage-api/src/java/org/apache/hadoop/hive/common/type/HiveIntervalDayTime.java
Patch:
@@ -24,6 +24,7 @@
 
 import org.apache.commons.lang3.builder.HashCodeBuilder;
 import org.apache.hive.common.util.IntervalDayTimeUtils;
+import org.apache.hive.common.util.SuppressFBWarnings;
 
 
 /**
@@ -168,6 +169,7 @@ public boolean equals(Object obj) {
    * Return a copy of this object.
    */
   @Override
+  @SuppressFBWarnings(value = "CN_IMPLEMENTS_CLONE_BUT_NOT_CLONEABLE", justification = "Intended")
   public Object clone() {
       return new HiveIntervalDayTime(totalSeconds, nanos);
   }

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java
Patch:
@@ -25,6 +25,7 @@
 
 import org.apache.hadoop.hive.common.type.CalendarUtils;
 import org.apache.hadoop.io.Writable;
+import org.apache.hive.common.util.SuppressFBWarnings;
 
 /**
  * This class represents a nullable timestamp column vector capable of handing a wide range of
@@ -132,6 +133,7 @@ public void timestampUpdate(Timestamp timestamp, int elementNum) {
    * @param elementNum
    * @return
    */
+  @SuppressFBWarnings(value = "EI_EXPOSE_REP", justification = "Expose internal rep for efficiency")
   public Timestamp asScratchTimestamp(int elementNum) {
     scratchTimestamp.setTime(time[elementNum]);
     scratchTimestamp.setNanos(nanos[elementNum]);
@@ -142,6 +144,7 @@ public Timestamp asScratchTimestamp(int elementNum) {
    * Return the scratch timestamp (contents undefined).
    * @return
    */
+  @SuppressFBWarnings(value = "EI_EXPOSE_REP", justification = "Expose internal rep for efficiency")
   public Timestamp getScratchTimestamp() {
     return scratchTimestamp;
   }

File: storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java
Patch:
@@ -82,7 +82,6 @@ public PredicateLeafImpl(Operator operator,
       checkLiteralType(literal, type, conf);
       this.literalList = literalList;
       if (literalList != null) {
-        Class valueCls = type.getValueClass();
         for(Object lit: literalList) {
           checkLiteralType(lit, type, conf);
         }

File: storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java
Patch:
@@ -23,14 +23,13 @@
 import java.io.InputStream;
 import java.io.OutputStream;
 import java.io.IOException;
-import java.math.BigInteger;
 
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.common.type.FastHiveDecimal;
 import org.apache.hadoop.hive.common.type.FastHiveDecimalImpl;
-import org.apache.hadoop.hive.common.type.HiveDecimalVersionV2;
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.io.WritableUtils;
+import org.apache.hive.common.util.SuppressFBWarnings;
 
 /**
  * A mutable decimal.
@@ -577,6 +576,7 @@ public int bigIntegerBytesInternalScratch() {
    *
    */
   @HiveDecimalWritableVersionV2
+  @SuppressFBWarnings(value = "EI_EXPOSE_REP", justification = "Expose internal rep for efficiency")
   public byte[] bigIntegerBytesInternalScratchBuffer() {
     return internalScratchBuffer;
   }

File: storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritableV1.java
Patch:
@@ -26,6 +26,7 @@
 
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.io.WritableUtils;
+import org.apache.hive.common.util.SuppressFBWarnings;
 
 public class HiveDecimalWritableV1 implements WritableComparable<HiveDecimalWritableV1> {
 
@@ -77,6 +78,7 @@ public void set(HiveDecimalWritableV1 writable) {
   }
 
   @HiveDecimalWritableVersionV1
+  @SuppressFBWarnings(value = "EI_EXPOSE_REP2", justification = "Ref external obj for efficiency")
   public void set(byte[] bytes, int scale) {
     this.internalStorage = bytes;
     this.scale = scale;
@@ -162,6 +164,7 @@ public int hashCode() {
    * @return
    */
   @HiveDecimalWritableVersionV1
+  @SuppressFBWarnings(value = "EI_EXPOSE_REP", justification = "Expose internal rep for efficiency")
   public byte[] getInternalStorage() {
     return internalStorage;
   }

File: shims/common/src/main/java/org/apache/hadoop/fs/ProxyFileSystem.java
Patch:
@@ -42,7 +42,6 @@ public class ProxyFileSystem extends FilterFileSystem {
 
   protected String realScheme;
   protected String realAuthority;
-  protected URI realUri;
 
 
 
@@ -103,8 +102,7 @@ public ProxyFileSystem(FileSystem fs, URI myUri) {
 
     URI realUri = fs.getUri();
     this.realScheme = realUri.getScheme();
-    this.realAuthority=realUri.getAuthority();
-    this.realUri = realUri;
+    this.realAuthority = realUri.getAuthority();
 
     this.myScheme = myUri.getScheme();
     this.myAuthority=myUri.getAuthority();

File: shims/common/src/main/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java
Patch:
@@ -58,7 +58,6 @@ public void initialize(URI name, Configuration conf) throws IOException {
     // the scheme/authority serving as the proxy is derived
     // from the supplied URI
     this.scheme = name.getScheme();
-    String nameUriString = name.toString();
 
     String authority = name.getAuthority() != null ? name.getAuthority() : "";
     String proxyUriString = scheme + "://" + authority + "/";

File: shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java
Patch:
@@ -176,7 +176,7 @@ private static void removeBaseAclEntries(List<AclEntry> entries) {
     Iterables.removeIf(entries, new Predicate<AclEntry>() {
       @Override
       public boolean apply(AclEntry input) {
-        if (input.getName() == null) {
+        if (input != null && input.getName() == null) {
           return true;
         }
         return false;

File: shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
Patch:
@@ -125,9 +125,7 @@ public static class CombineFileRecordReader<K, V> implements RecordReader<K, V>
     protected CombineFileSplit split;
     protected JobConf jc;
     protected Reporter reporter;
-    protected Class<RecordReader<K, V>> rrClass;
     protected Constructor<RecordReader<K, V>> rrConstructor;
-    protected FileSystem fs;
 
     protected int idx;
     protected long progress;
@@ -193,7 +191,6 @@ public CombineFileRecordReader(JobConf job, CombineFileSplit split,
         throws IOException {
       this.split = split;
       this.jc = job;
-      this.rrClass = rrClass;
       this.reporter = reporter;
       this.idx = 0;
       this.curReader = null;

File: ql/src/java/org/apache/hadoop/hive/ql/io/arrow/ArrowColumnarBatchSerDe.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.ql.io.arrow;
 
+import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.Lists;
 import org.apache.arrow.memory.BufferAllocator;
 import org.apache.arrow.vector.complex.impl.UnionListWriter;
@@ -97,7 +98,8 @@ public class ArrowColumnarBatchSerDe extends AbstractSerDe {
   StructObjectInspector rowObjectInspector;
   Configuration conf;
 
-  private Serializer serializer;
+  @VisibleForTesting
+  Serializer serializer;
   private Deserializer deserializer;
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDF.java
Patch:
@@ -66,7 +66,7 @@
  * variable length of arguments. 3. It can accept an infinite number of function
  * signature - for example, it's easy to write a GenericUDF that accepts
  * array&lt;int&gt;, array&lt;array&lt;int&gt;&gt; and so on (arbitrary levels of nesting). 4. It
- * can do short-circuit evaluations using DeferedObject.
+ * can do short-circuit evaluations using {@link DeferredObject}.
  */
 @InterfaceAudience.Public
 @InterfaceStability.Stable
@@ -77,8 +77,8 @@ public abstract class GenericUDF implements Closeable {
       "th", "th", "th", "th", "th" };
 
   /**
-   * A Defered Object allows us to do lazy-evaluation and short-circuiting.
-   * GenericUDF use DeferedObject to pass arguments.
+   * A Deferred Object allows us to do lazy-evaluation and short-circuiting.
+   * GenericUDF use {@link DeferredObject} to pass arguments.
    */
   @InterfaceAudience.Public
   @InterfaceStability.Stable

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java
Patch:
@@ -110,7 +110,7 @@ public static class Writer implements Closeable {
       dumpMetadataOnly = hiveConf.getBoolVar(HiveConf.ConfVars.REPL_DUMP_METADATA_ONLY) ||
               hiveConf.getBoolVar(HiveConf.ConfVars.REPL_DUMP_METADATA_ONLY_FOR_EXTERNAL_TABLE);
       if (shouldWrite()) {
-        this.writer = FileSystem.get(hiveConf).create(writePath);
+        this.writer = writePath.getFileSystem(hiveConf).create(writePath);
       }
     }
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/MetastoreTaskThreadAlwaysTestImpl.java
Patch:
@@ -52,7 +52,7 @@ public void run() {
     LOG.info("Name of thread " + Thread.currentThread().getName() + " changed to " + TASK_NAME);
     Thread.currentThread().setName(TASK_NAME);
     try {
-      Thread.sleep(runFrequency(TimeUnit.MILLISECONDS));
+      Thread.sleep(10000);
     } catch (InterruptedException ie) {
       LOG.error("Task " + TASK_NAME + " interrupted: " + ie.getMessage(), ie);
     }

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/RemoteMetastoreTaskThreadTestImpl1.java
Patch:
@@ -52,7 +52,7 @@ public void run() {
     LOG.info("Name of thread " + Thread.currentThread().getName() + " changed to " + TASK_NAME);
     Thread.currentThread().setName(TASK_NAME);
     try {
-      Thread.sleep(runFrequency(TimeUnit.MILLISECONDS));
+      Thread.sleep(10000);
     } catch (InterruptedException ie) {
       LOG.error("Task " + TASK_NAME + " interrupted: " + ie.getMessage(), ie);
     }

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/RemoteMetastoreTaskThreadTestImpl2.java
Patch:
@@ -52,7 +52,7 @@ public void run() {
     LOG.info("Name of thread " + Thread.currentThread().getName() + " changed to " + TASK_NAME);
     Thread.currentThread().setName(TASK_NAME);
     try {
-      Thread.sleep(runFrequency(TimeUnit.MILLISECONDS));
+      Thread.sleep(10000);
     } catch (InterruptedException ie) {
       LOG.error("Task " + TASK_NAME + " interrupted: " + ie.getMessage(), ie);
     }

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreHousekeepingNonLeader.java
Patch:
@@ -52,7 +52,7 @@ public void testHouseKeepingThreadExistence() throws Exception {
       Assert.assertFalse("Thread with name " + entry.getKey() + " found.", entry.getValue());
     }
 
-    for (Map.Entry<Class, Boolean> entry : threadClasses.entrySet()) {
+    for (Map.Entry<Class<? extends Thread>, Boolean> entry : threadClasses.entrySet()) {
       // A non-leader HMS will still run the configured number of Compaction worker threads.
       if (entry.getKey() == Worker.class) {
         if (entry.getValue()) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -2238,7 +2238,7 @@ private void getMetaData(QB qb, ReadEntity parentInput)
       // Temporary tables created during the execution are not the input sources
       if (!PlanUtils.isValuesTempTable(alias)) {
         PlanUtils.addInput(inputs,
-            new ReadEntity(tab, parentViewInfo, parentViewInfo == null),mergeIsDirect);
+            new ReadEntity(tab, parentViewInfo, parentViewInfo == null), mergeIsDirect);
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
Patch:
@@ -1172,7 +1172,7 @@ public static void addInputsForView(ParseContext parseCtx) throws HiveException
 
       // Adds tables only for create view (PPD filter can be appended by outer query)
       Table table = topOp.getConf().getTableMetadata();
-      PlanUtils.addInput(inputs, new ReadEntity(table, parentViewInfo));
+      PlanUtils.addInput(inputs, new ReadEntity(table, parentViewInfo, parentViewInfo == null));
     }
   }
 

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
Patch:
@@ -485,11 +485,12 @@ public static Object copyToStandardObject(
     case UNION: {
       UnionObjectInspector uoi = (UnionObjectInspector)oi;
       List<ObjectInspector> objectInspectors = uoi.getObjectInspectors();
+      byte tag = uoi.getTag(o);
       Object object = copyToStandardObject(
               uoi.getField(o),
-              objectInspectors.get(uoi.getTag(o)),
+              objectInspectors.get(tag),
               objectInspectorOption);
-      result = object;
+      result = new StandardUnionObjectInspector.StandardUnion(tag, object);
       break;
     }
     default: {

File: common/src/test/org/apache/hadoop/hive/common/metrics/metrics2/TestCodahaleMetrics.java
Patch:
@@ -55,7 +55,7 @@ public class TestCodahaleMetrics {
   private static final Path tmpDir = Paths.get(System.getProperty("java.io.tmpdir"));
   private static File jsonReportFile;
   private static MetricRegistry metricRegistry;
-  private static final long REPORT_INTERVAL_MS = 100;
+  private static final long REPORT_INTERVAL_MS = 2000;
 
   @BeforeClass
   public static void setUp() throws Exception {

File: ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java
Patch:
@@ -1203,7 +1203,7 @@ public void testLockTimeout() throws Exception {
       LockRequest req = new LockRequest(components, "me", "localhost");
       LockResponse res = txnHandler.lock(req);
       assertTrue(res.getState() == LockState.ACQUIRED);
-      Thread.sleep(10);
+      Thread.sleep(1000);
       txnHandler.performTimeOuts();
       txnHandler.checkLock(new CheckLockRequest(res.getLockid()));
       fail("Told there was a lock, when it should have timed out.");
@@ -1218,7 +1218,7 @@ public void testRecoverManyTimeouts() throws Exception {
     long timeout = txnHandler.setTimeout(1);
     try {
       txnHandler.openTxns(new OpenTxnRequest(503, "me", "localhost"));
-      Thread.sleep(10);
+      Thread.sleep(1000);
       txnHandler.performTimeOuts();
       GetOpenTxnsInfoResponse rsp = txnHandler.getOpenTxnsInfo();
       int numAborted = 0;
@@ -1241,7 +1241,7 @@ public void testReplTimeouts() throws Exception {
       request.setReplPolicy("default.*");
       request.setReplSrcTxnIds(response.getTxn_ids());
       OpenTxnsResponse responseRepl = txnHandler.openTxns(request);
-      Thread.sleep(10);
+      Thread.sleep(1000);
       txnHandler.performTimeOuts();
       GetOpenTxnsInfoResponse rsp = txnHandler.getOpenTxnsInfo();
       int numAborted = 0;

File: service/src/test/org/apache/hive/service/cli/session/TestSessionManagerMetrics.java
Patch:
@@ -382,7 +382,7 @@ public void testAbandonedSessionMetrics() throws Exception {
 
     // We're going to wait for the session to be abandoned.
     String currentValue;
-    int count = 5; // how many times we'll sleep before giving up
+    int count = 10; // how many times we'll sleep before giving up
     String expectedValue = "1";
     do {
       // HIVE_SERVER2_SESSION_CHECK_INTERVAL is set to 3 seconds, so we have to wait for at least

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/MetaStoreTestUtils.java
Patch:
@@ -176,7 +176,7 @@ public static int startMetaStoreWithRetry(HadoopThriftAuthBridge bridge,
         String jdbcUrl = MetastoreConf.getVar(conf, ConfVars.CONNECT_URL_KEY);
         if (!keepJdbcUri) {
           // Setting metastore instance specific jdbc url postfixed with port
-          jdbcUrl = "jdbc:derby:;databaseName=" + TMP_DIR + File.separator
+          jdbcUrl = "jdbc:derby:memory:" + TMP_DIR + File.separator
               + MetaStoreServerUtils.JUNIT_DATABASE_PREFIX + "_" + metaStorePort + ";create=true";
           MetastoreConf.setVar(conf, ConfVars.CONNECT_URL_KEY, jdbcUrl);
         }

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestMarkPartition.java
Patch:
@@ -83,7 +83,7 @@ public void testMarkingPartitionSet() throws TException, InterruptedException {
     kvs.put("b", "'2011'");
     msc.markPartitionForEvent(dbName, tableName, kvs, PartitionEventType.LOAD_DONE);
     Assert.assertTrue(msc.isPartitionMarkedForEvent(dbName, tableName, kvs, PartitionEventType.LOAD_DONE));
-    Thread.sleep(3000);
+    Thread.sleep(10000);
     Assert.assertFalse(msc.isPartitionMarkedForEvent(dbName, tableName, kvs, PartitionEventType.LOAD_DONE));
 
     kvs.put("b", "'2012'");

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -207,6 +207,7 @@
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFieldTrimmerRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterAggregateTransposeRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule;
+import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterMergeRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTSTransposeRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTransposeRule;
 import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterSetOpTransposeRule;
@@ -2011,7 +2012,7 @@ private RelNode applyPreJoinOrderingTransforms(RelNode basePlan, RelMetadataProv
       rules.add(HiveFilterJoinRule.FILTER_ON_JOIN);
       rules.add(new HiveFilterAggregateTransposeRule(Filter.class, HiveRelFactories.HIVE_BUILDER,
           Aggregate.class));
-      rules.add(new FilterMergeRule(HiveRelFactories.HIVE_BUILDER));
+      rules.add(HiveFilterMergeRule.INSTANCE);
       if (conf.getBoolVar(HiveConf.ConfVars.HIVE_OPTIMIZE_REDUCE_WITH_STATS)) {
         rules.add(HiveReduceExpressionsWithStatsRule.INSTANCE);
       }

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/AbstractCliConfig.java
Patch:
@@ -368,6 +368,7 @@ protected void setClusterType(MiniClusterType type) {
     if (clusterType == null) {
       throw new RuntimeException("clustertype cant be null");
     }
+    this.setFsType(clusterType.getDefaultFsType());
   }
 
   protected FsType getFsType() {

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/tables/ShowTablesOperation.java
Patch:
@@ -72,7 +72,7 @@ private void showTables() throws HiveException {
     try (DataOutputStream os = DDLUtils.getOutputStream(new Path(desc.getResFile()), context)) {
       context.getFormatter().showTables(os, tableNames);
     } catch (Exception e) {
-      throw new HiveException(e, ErrorMsg.GENERIC_ERROR, "in database" + desc.getDbName());
+      throw new HiveException(e, ErrorMsg.GENERIC_ERROR, "in database " + desc.getDbName());
     }
   }
 
@@ -91,7 +91,7 @@ private void showTablesExtended() throws HiveException {
     try (DataOutputStream os = DDLUtils.getOutputStream(new Path(desc.getResFile()), context)) {
       context.getFormatter().showTablesExtended(os, tableObjects);
     } catch (Exception e) {
-      throw new HiveException(e, ErrorMsg.GENERIC_ERROR, "in database" + desc.getDbName());
+      throw new HiveException(e, ErrorMsg.GENERIC_ERROR, "in database " + desc.getDbName());
     }
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java
Patch:
@@ -1222,7 +1222,7 @@ public Partition getPartitionWithAuthInfo(String catName, String dbName, String
     if (partition == null) {
       throw new NoSuchObjectException("Partition with partition values " +
               (pvals != null ? Arrays.toString(pvals.toArray()) : "null") +
-              " for table " + tableName + " in database " + dbName + "and for user " +
+              " for table " + tableName + " in database " + dbName + " and for user " +
               userName + " and group names " + (groupNames != null ? Arrays.toString(groupNames.toArray()) : "null") +
               " is not found.");
     }

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java
Patch:
@@ -341,6 +341,8 @@ public boolean doNext(K key, V value) throws IOException {
           part = null;
         }
         TableDesc table = (part == null) ? null : part.getTableDesc();
+        // In TextFormat, skipping is already taken care of as part of SkippingTextInputFormat.
+        // This code will be also called from LLAP when pipeline is non-vectorized and cannot create wrapper.
         if (table != null && !TextInputFormat.class.isAssignableFrom(part.getInputFileFormatClass())) {
           headerCount = Utilities.getHeaderCount(table);
           footerCount = Utilities.getFooterCount(table, jobConf);

File: ql/src/test/org/apache/hadoop/hive/ql/io/TestLineBuffer.java
Patch:
@@ -25,7 +25,7 @@
 /**
  * LineEndBuffer simple unit test.
  */
-public class LineBufferTest {
+public class TestLineBuffer {
 
   @Test
   public void testLineEndBuffer() {

File: ql/src/java/org/apache/hadoop/hive/ql/ValidTxnManager.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.ql;
 
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -80,7 +81,7 @@ boolean isValidTxnListState() throws LockException {
     // - Exclusive for INSERT OVERWRITE, when shared write is disabled (HiveConf.TXN_WRITE_X_LOCK=false).
     // - Excl-write for UPDATE/DELETE, when shared write is disabled, INSERT OVERWRITE - when enabled.
     Set<String> nonSharedLockedTables = getNonSharedLockedTables();
-    if (nonSharedLockedTables == null) {
+    if (nonSharedLockedTables.isEmpty()) {
       return true; // Nothing to check
     }
 
@@ -100,7 +101,7 @@ boolean isValidTxnListState() throws LockException {
 
   private Set<String> getNonSharedLockedTables() {
     if (CollectionUtils.isEmpty(driver.getContext().getHiveLocks())) {
-      return null; // Nothing to check
+      return Collections.emptySet(); // Nothing to check
     }
 
     Set<String> nonSharedLockedTables = new HashSet<>();

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -40,7 +40,6 @@
 import java.net.InetAddress;
 import java.net.UnknownHostException;
 import java.nio.ByteBuffer;
-import java.sql.SQLIntegrityConstraintViolationException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.BitSet;
@@ -80,7 +79,6 @@
 import org.apache.calcite.rex.RexBuilder;
 import org.apache.commons.io.FilenameUtils;
 import org.apache.commons.lang3.ObjectUtils;
-import org.apache.commons.lang3.exception.ExceptionUtils;
 import org.apache.commons.lang3.tuple.Pair;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileChecksum;
@@ -3220,7 +3218,7 @@ public Partition createPartition(Table tbl, Map<String, String> partSpec) throws
     }
   }
 
-  public List<org.apache.hadoop.hive.metastore.api.Partition> addPartition(
+  public List<org.apache.hadoop.hive.metastore.api.Partition> addPartitions(
       List<org.apache.hadoop.hive.metastore.api.Partition> partitions, boolean ifNotExists, boolean needResults)
           throws HiveException {
     try {

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/database/alter/location/AlterDatabaseSetManagedLocationAnalyzer.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 
 /**
- * Analyzer for database set location commands.
+ * Analyzer for database set managed location commands.
  */
 @DDLType(types = HiveParser.TOK_ALTERDATABASE_MANAGEDLOCATION)
 public class AlterDatabaseSetManagedLocationAnalyzer extends AbstractAlterDatabaseAnalyzer {
@@ -41,7 +41,7 @@ public void analyzeInternal(ASTNode root) throws SemanticException {
 
     outputs.add(toWriteEntity(newLocation));
 
-    AlterDatabaseSetLocationDesc desc = new AlterDatabaseSetLocationDesc(databaseName, null, newLocation);
+    AlterDatabaseSetManagedLocationDesc desc = new AlterDatabaseSetManagedLocationDesc(databaseName, newLocation);
     addAlterDatabaseDesc(desc);
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/database/desc/DescDatabaseDesc.java
Patch:
@@ -33,16 +33,17 @@ public class DescDatabaseDesc implements DDLDesc, Serializable {
   private static final long serialVersionUID = 1L;
 
   public static final String DESC_DATABASE_SCHEMA =
-      "db_name,comment,location,managedLocation,owner_name,owner_type,parameters#string:string:string:string:string:string:string";
+      "db_name,comment,location,managedLocation,owner_name,owner_type,parameters#" +
+      "string:string:string:string:string:string:string";
 
   private final String resFile;
   private final String dbName;
   private final boolean isExt;
 
   public DescDatabaseDesc(Path resFile, String dbName, boolean isExt) {
-    this.isExt = isExt;
     this.resFile = resFile.toString();
     this.dbName = dbName;
+    this.isExt = isExt;
   }
 
   public boolean isExt() {

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/database/drop/DropDatabaseOperation.java
Patch:
@@ -52,8 +52,7 @@ public int execute() throws HiveException {
       context.getDb().dropDatabase(dbName, true, desc.getIfExists(), desc.isCasdade());
 
       if (LlapHiveUtils.isLlapMode(context.getConf())) {
-        ProactiveEviction.Request.Builder llapEvictRequestBuilder =
-            ProactiveEviction.Request.Builder.create();
+        ProactiveEviction.Request.Builder llapEvictRequestBuilder = ProactiveEviction.Request.Builder.create();
         llapEvictRequestBuilder.addDb(dbName);
         ProactiveEviction.evict(context.getConf(), llapEvictRequestBuilder.build());
       }

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/show/ShowCreateTableAnalyzer.java
Patch:
@@ -54,12 +54,10 @@ public void analyzeInternal(ASTNode root) throws SemanticException {
     // If no DB was specified in statement, do not include it in the final output
     ShowCreateTableDesc desc = new ShowCreateTableDesc(table.getDbName(), table.getTableName(),
         ctx.getResFile().toString(), StringUtils.isBlank(tableIdentifier.getKey()));
-
     Task<DDLWork> task = TaskFactory.get(new DDLWork(getInputs(), getOutputs(), desc));
-    task.setFetchSource(true);
-
     rootTasks.add(task);
 
+    task.setFetchSource(true);
     setFetchTask(createFetchTask(ShowCreateTableDesc.SCHEMA));
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/create/show/ShowCreateTableOperation.java
Patch:
@@ -103,7 +103,7 @@ public int execute() throws HiveException {
   }
 
   private static final String CREATE_VIEW_TEMPLATE =
-      "CREATE VIEW <if(" + DATABASE_NAME + ")>`<" + DATABASE_NAME + ">`.<endif>" + "`<" + TABLE_NAME + ">`" + " AS <SQL>";
+      "CREATE VIEW <if(" + DATABASE_NAME + ")>`<" + DATABASE_NAME + ">`.<endif>`<" + TABLE_NAME + ">` AS <SQL>";
 
   private String getCreateViewCommand(Table table, boolean isRelative) {
     ST command = new ST(CREATE_VIEW_TEMPLATE);

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/DropTableOperation.java
Patch:
@@ -108,8 +108,7 @@ public int execute() throws HiveException {
 
     if (LlapHiveUtils.isLlapMode(context.getConf())) {
       TableName tableName = HiveTableName.of(table);
-      ProactiveEviction.Request.Builder llapEvictRequestBuilder =
-              ProactiveEviction.Request.Builder.create();
+      ProactiveEviction.Request.Builder llapEvictRequestBuilder = ProactiveEviction.Request.Builder.create();
       llapEvictRequestBuilder.addTable(tableName.getDb(), tableName.getTable());
       ProactiveEviction.evict(context.getConf(), llapEvictRequestBuilder.build());
     }

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/drop/AlterTableDropPartitionOperation.java
Patch:
@@ -120,9 +120,8 @@ private void dropPartitions() throws HiveException {
     List<Partition> droppedPartitions = context.getDb().dropPartitions(tablenName.getDb(), tablenName.getTable(),
         partitionExpressions, options);
 
-    ProactiveEviction.Request.Builder llapEvictRequestBuilder =
-        LlapHiveUtils.isLlapMode(context.getConf()) ?
-            ProactiveEviction.Request.Builder.create() : null;
+    ProactiveEviction.Request.Builder llapEvictRequestBuilder = LlapHiveUtils.isLlapMode(context.getConf()) ?
+        ProactiveEviction.Request.Builder.create() : null;
 
     for (Partition partition : droppedPartitions) {
       context.getConsole().printInfo("Dropped the partition " + partition.getName());
@@ -133,6 +132,7 @@ private void dropPartitions() throws HiveException {
         llapEvictRequestBuilder.addPartitionOfATable(tablenName.getDb(), tablenName.getTable(), partition.getSpec());
       }
     }
+
     if (llapEvictRequestBuilder != null) {
       ProactiveEviction.evict(context.getConf(), llapEvictRequestBuilder.build());
     }

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/view/materialized/alter/rebuild/AlterMaterializedViewRebuildAnalyzer.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild;
 
-import org.antlr.runtime.tree.Tree;
 import org.apache.hadoop.hive.common.TableName;
 import org.apache.hadoop.hive.metastore.api.LockState;
 import org.apache.hadoop.hive.ql.Context;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java
Patch:
@@ -117,7 +117,7 @@ private boolean isDbEmpty(String dbName) throws HiveException {
 
   private Task<?> createDbTask(Database dbObj) {
     // note that we do not set location - for repl load, we want that auto-created.
-    CreateDatabaseDesc createDbDesc = new CreateDatabaseDesc(dbObj.getName(), dbObj.getDescription(), null, false,
+    CreateDatabaseDesc createDbDesc = new CreateDatabaseDesc(dbObj.getName(), dbObj.getDescription(), null, null, false,
         updateDbProps(dbObj, context.dumpDirectory));
     // If it exists, we want this to be an error condition. Repl Load is not intended to replace a
     // db.

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateDatabaseHandler.java
Patch:
@@ -58,7 +58,7 @@ public List<Task<?>> handle(Context context)
         context.dbName == null ? db.getName() : context.dbName;
 
     CreateDatabaseDesc createDatabaseDesc =
-        new CreateDatabaseDesc(destinationDBName, db.getDescription(), null, true, db.getParameters());
+        new CreateDatabaseDesc(destinationDBName, db.getDescription(), null, null, true, db.getParameters());
     Task<DDLWork> createDBTask = TaskFactory.get(
         new DDLWork(new HashSet<>(), new HashSet<>(), createDatabaseDesc), context.hiveConf);
     if (!db.getParameters().isEmpty()) {

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -884,7 +884,7 @@ private void createDefaultDB_core(RawStore ms) throws MetaException, InvalidObje
         ms.getDatabase(DEFAULT_CATALOG_NAME, DEFAULT_DATABASE_NAME);
       } catch (NoSuchObjectException e) {
         Database db = new Database(DEFAULT_DATABASE_NAME, DEFAULT_DATABASE_COMMENT,
-            wh.getDefaultDatabasePath(DEFAULT_DATABASE_NAME).toString(), null);
+            wh.getDefaultDatabasePath(DEFAULT_DATABASE_NAME, true).toString(), null);
         db.setOwnerName(PUBLIC);
         db.setOwnerType(PrincipalType.ROLE);
         db.setCatalogName(DEFAULT_CATALOG_NAME);

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/client/TestDatabases.java
Patch:
@@ -215,7 +215,7 @@ public void testDefaultDatabaseData() throws Exception {
     Assert.assertEquals("Default database name", "default", database.getName());
     Assert.assertEquals("Default database description", "Default Hive database",
         database.getDescription());
-    Assert.assertEquals("Default database location", metaStore.getWarehouseRoot(),
+    Assert.assertEquals("Default database location", metaStore.getExternalWarehouseRoot(),
         new Path(database.getLocationUri()));
     Assert.assertEquals("Default database parameters", new HashMap<String, String>(),
         database.getParameters());

File: jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/conf/DatabaseType.java
Patch:
@@ -23,5 +23,6 @@ public enum DatabaseType {
   POSTGRES,
   MSSQL,
   METASTORE,
-  JETHRO_DATA
+  JETHRO_DATA,
+  HIVE
 }

File: llap-server/src/test/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorTestHelpers.java
Patch:
@@ -131,7 +131,7 @@ public static SubmitWorkRequestProto createSubmitWorkRequestProto(
       int fragmentNumber, int selfAndUpstreamParallelism,
       int selfAndUpstreamComplete, long firstAttemptStartTime,
       long currentAttemptStartTime, int withinDagPriority) {
-    return createSubmitWorkRequestProto(fragmentNumber, selfAndUpstreamParallelism, 0, firstAttemptStartTime,
+    return createSubmitWorkRequestProto(fragmentNumber, selfAndUpstreamParallelism, selfAndUpstreamComplete, firstAttemptStartTime,
         currentAttemptStartTime, withinDagPriority, "MockDag", false);
   }
 

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java
Patch:
@@ -1014,7 +1014,7 @@ private static LockRequest createLockRequest(final HiveEndPoint hiveEndPoint,
       LockComponentBuilder lockCompBuilder = new LockComponentBuilder()
               .setDbName(hiveEndPoint.database)
               .setTableName(hiveEndPoint.table)
-              .setShared()
+              .setSharedRead()
               .setOperationType(DataOperationType.INSERT);
       if (partNameForLock!=null && !partNameForLock.isEmpty() ) {
           lockCompBuilder.setPartitionName(partNameForLock);

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/mutate/client/lock/Lock.java
Patch:
@@ -184,9 +184,9 @@ private LockRequest buildLockRequest(Long transactionId) {
       //todo: DataOperationType is set conservatively here, we'd really want to distinguish update/delete
       //and insert/select and if resource (that is written to) is ACID or not
       if (sinks.contains(table)) {
-        componentBuilder.setSemiShared().setOperationType(DataOperationType.UPDATE).setIsTransactional(true);
+        componentBuilder.setSharedWrite().setOperationType(DataOperationType.UPDATE).setIsTransactional(true);
       } else {
-        componentBuilder.setShared().setOperationType(DataOperationType.INSERT).setIsTransactional(true);
+        componentBuilder.setSharedRead().setOperationType(DataOperationType.INSERT).setIsTransactional(true);
       }
       LockComponent component = componentBuilder.build();
       requestBuilder.addLockComponent(component);

File: ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java
Patch:
@@ -88,6 +88,7 @@
 import static junit.framework.Assert.assertNull;
 import static junit.framework.Assert.assertTrue;
 import static junit.framework.Assert.fail;
+import static org.apache.hadoop.hive.metastore.utils.LockTypeUtil.getEncoding;
 
 /**
  * Tests for TxnHandler.
@@ -1394,7 +1395,7 @@ public void deadlockDetected() throws Exception {
       stmt.executeUpdate("INSERT INTO \"HIVE_LOCKS\" (\"HL_LOCK_EXT_ID\", \"HL_LOCK_INT_ID\", \"HL_TXNID\", " +
           "\"HL_DB\", \"HL_TABLE\", \"HL_PARTITION\", \"HL_LOCK_STATE\", \"HL_LOCK_TYPE\", \"HL_LAST_HEARTBEAT\", " +
           "\"HL_USER\", \"HL_HOST\") VALUES (1, 1, 1, 'MYDB', 'MYTABLE', 'MYPARTITION', '" +
-          tHndlr.LOCK_WAITING + "', '" + tHndlr.LOCK_EXCLUSIVE + "', " + now + ", 'fred', " +
+          tHndlr.LOCK_WAITING + "', '" + getEncoding(LockType.EXCLUSIVE) + "', " + now + ", 'fred', " +
           "'scooby.com')");
       conn.commit();
       tHndlr.closeDbConn(conn);

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java
Patch:
@@ -126,15 +126,15 @@ public void tearDown() throws Exception {
   @Test
   public void testMetadataOperationLocks() throws Exception {
     boolean isStrict = conf.getBoolVar(HiveConf.ConfVars.HIVE_TXN_STRICT_LOCKING_MODE);
-    //to make insert into non-acid take shared lock
+    //to make insert into non-acid take shared_read lock
     conf.setBoolVar(HiveConf.ConfVars.HIVE_TXN_STRICT_LOCKING_MODE, false);
     dropTable(new String[] {"T"});
     driver.run("create table if not exists T (a int, b int)");
     driver.compileAndRespond("insert into T values (1,2)", true);
     txnMgr.acquireLocks(driver.getPlan(), ctx, "Fifer");
     List<ShowLocksResponseElement> locks = getLocks();
     Assert.assertEquals("Unexpected lock count", 1, locks.size());
-    //since LM is using non strict mode we get shared lock
+    //since LM is using non strict mode we get shared_read lock
     checkLock(LockType.SHARED_READ, LockState.ACQUIRED, "default", "T", null, locks);
 
     //simulate concurrent session

File: streaming/src/java/org/apache/hive/streaming/TransactionBatch.java
Patch:
@@ -439,7 +439,7 @@ private static LockRequest createLockRequest(final HiveStreamingConnection conne
     LockComponentBuilder lockCompBuilder = new LockComponentBuilder()
         .setDbName(connection.getDatabase())
         .setTableName(connection.getTable().getTableName())
-        .setShared()
+        .setSharedRead()
         .setOperationType(DataOperationType.INSERT);
     if (connection.isDynamicPartitioning()) {
       lockCompBuilder.setIsDynamicPartitionWrite(true);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4464,7 +4464,7 @@ public static enum ConfVars {
       "llap.daemon.vcpus.per.instance"),
     LLAP_DAEMON_NUM_FILE_CLEANER_THREADS("hive.llap.daemon.num.file.cleaner.threads", 1,
       "Number of file cleaner threads in LLAP.", "llap.daemon.num.file.cleaner.threads"),
-    LLAP_FILE_CLEANUP_DELAY_SECONDS("hive.llap.file.cleanup.delay.seconds", "300s",
+    LLAP_FILE_CLEANUP_DELAY_SECONDS("hive.llap.file.cleanup.delay.seconds", "0s",
        new TimeValidator(TimeUnit.SECONDS),
       "How long to delay before cleaning up query files in LLAP (in seconds, for debugging).",
       "llap.file.cleanup.delay-seconds"),

File: llap-server/src/java/org/apache/hadoop/hive/llap/cli/service/LlapServiceDriver.java
Patch:
@@ -311,7 +311,7 @@ private int startLlap(Path tmpDir, Path scriptParent) throws IOException, Interr
 
     rc = runPackagePy(tmpDir, scriptParent, version, outputDir);
     if (rc == 0) {
-      String tarballName = "llap-" + version + ".tar.gz";
+      String tarballName = cl.getName() + "-" + version + ".tar.gz";
       startCluster(conf, cl.getName(), tarballName, packageDir, conf.getVar(ConfVars.LLAP_DAEMON_QUEUE_NAME));
     }
     return rc;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagate.java
Patch:
@@ -57,7 +57,7 @@
  * some constants of its parameters.
  *
  * 3. Propagate expression: if the expression is an assignment like column=constant, the expression
- * will be propagate to parents to see if further folding operation is possible.
+ * will be propagate to children to see if further folding operation is possible.
  */
 public class ConstantPropagate extends Transform {
 
@@ -147,7 +147,7 @@ protected void walk(Node nd) throws SemanticException {
           || getDispatchedList().containsAll(parents)) {
         opStack.push(nd);
 
-        // all children are done or no need to walk the children
+        // all parents are done or no need to walk the parents
         dispatch(nd, opStack);
         opStack.pop();
       } else {
@@ -157,7 +157,7 @@ protected void walk(Node nd) throws SemanticException {
         return;
       }
 
-      // move all the children to the front of queue
+      // move all the children to the end of queue
       List<? extends Node> children = nd.getChildren();
       if (children != null) {
         toWalk.removeAll(children);

File: service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
Patch:
@@ -461,7 +461,7 @@ public RowSet getNextRowSet(FetchOrientation orientation, long maxRows)
       maxRows = 1;
       isBlobBased = true;
     }
-    driver.setMaxRows((int) maxRows);
+    driver.setMaxRows(Math.toIntExact(maxRows));
     RowSet rowSet = RowSetFactory.create(getResultSetSchema(), getProtocolVersion(), isBlobBased);
     try {
       /* if client is requesting fetch-from-start and its not the first time reading from this operation
@@ -471,7 +471,7 @@ public RowSet getNextRowSet(FetchOrientation orientation, long maxRows)
         driver.resetFetch();
       }
       fetchStarted = true;
-      driver.setMaxRows((int) maxRows);
+      driver.setMaxRows(Math.toIntExact(maxRows));
       if (driver.getResults(convey)) {
         return decode(convey, rowSet);
       }

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommandsForMmTable.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hadoop.hive.ql;
 
 import java.io.File;
-import java.util.Arrays;
 import java.util.List;
 
 import org.apache.hadoop.fs.FileStatus;
@@ -211,7 +210,6 @@ public void testInsertOverwriteForPartitionedMmTable() throws Exception {
     for(int h=0; h < pStrings.length; h++) {
       status = fs.listStatus(new Path(TEST_WAREHOUSE_DIR + "/" +
           (TableExtended.MMTBLPART).toString().toLowerCase() + pStrings[h]), FileUtils.STAGING_DIR_PATH_FILTER);
-      Arrays.sort(status);
       // There should be 1 delta dir, plus a base dir in the location
       Assert.assertEquals(2, status.length);
       for (int i = 0; i < status.length; i++) {
@@ -221,7 +219,7 @@ public void testInsertOverwriteForPartitionedMmTable() throws Exception {
         } else {
           sawBase = true;
           baseDirs[h] = dirName;
-          Assert.assertTrue(baseDirs[i].matches("base_.*"));
+          Assert.assertTrue(baseDirs[h].matches("base_.*"));
         }
       }
       Assert.assertEquals(1, deltaCount);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3927,7 +3927,7 @@ public static enum ConfVars {
         "The default value is true."),
     HIVE_VECTORIZATION_GROUPBY_COMPLEX_TYPES_ENABLED("hive.vectorized.groupby.complex.types.enabled", true,
         "This flag should be set to true to enable group by vectorization\n" +
-        "of aggregations that use complex types.\n",
+        "of aggregations that use complex types.\n" +
         "For example, AVG uses a complex type (STRUCT) for partial aggregation results" +
         "The default value is true."),
     HIVE_VECTORIZATION_ROW_IDENTIFIER_ENABLED("hive.vectorized.row.identifier.enabled", true,

File: testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/conf/UnitTestPropertiesParser.java
Patch:
@@ -90,7 +90,7 @@ class UnitTestPropertiesParser {
                            File sourceDirectory, Logger logger,
                            FileListProvider fileListProvider,
                            Set<String> excludedProvided, boolean inTest) {
-    logger.info("{} created with sourceDirectory={}, testCasePropertyName={}, excludedProvide={}",
+    logger.info("{} created with sourceDirectory={}, testCasePropertyName={}, excludedProvide={}" +
         "fileListProvider={}, inTest={}",
         UnitTestPropertiesParser.class.getSimpleName(), sourceDirectory, testCasePropertyName,
         excludedProvided,

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java
Patch:
@@ -192,9 +192,9 @@ private static ExprNodeConstantDesc typeCast(ExprNodeDesc desc, TypeInfo ti, boo
     // We shouldn't cast strings to other types because that can break original data in cases of
     // leading zeros or zeros trailing after decimal point.
     // Example: "000126" => 126 => "126"
-    boolean brokenDataTypesCombination = unsafeConversionTypes.contains(
-        priti.getPrimitiveCategory()) && !unsafeConversionTypes.contains(
-            descti.getPrimitiveCategory());
+    boolean brokenDataTypesCombination = unsafeConversionTypes.contains(priti.getPrimitiveCategory())
+        && !unsafeConversionTypes.contains(descti.getPrimitiveCategory())
+        || priti.getPrimitiveCategory() == PrimitiveCategory.TIMESTAMP && descti.getPrimitiveCategory() == PrimitiveCategory.DATE;
     if (performSafeTypeCast && brokenDataTypesCombination) {
       if (LOG.isDebugEnabled()) {
         LOG.debug("Unsupported cast " + priti + "; " + descti);

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcGenericUDTFGetSplits.java
Patch:
@@ -71,8 +71,8 @@ public void testDecimalPrecisionAndScale() throws Exception {
       assertNotNull(schemaSplit);
       FieldDesc fieldDesc = schemaSplit.getSchema().getColumns().get(0);
       DecimalTypeInfo type = (DecimalTypeInfo) fieldDesc.getTypeInfo();
-      assertEquals(38, type.getPrecision());
-      assertEquals(24, type.scale());
+      assertEquals(12, type.getPrecision());
+      assertEquals(8, type.scale());
 
       LlapBaseInputFormat.close(handleId);
     }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2036,7 +2036,7 @@ public static enum ConfVars {
     HIVE_PARQUET_WRITE_INT64_TIMESTAMP("hive.parquet.write.int64.timestamp", false,
         "Write parquet timestamps as int64/LogicalTypes instead of int96/OriginalTypes. Note:" +
         "Timestamps will be time zone agnostic (NEVER converted to a different time zone)."),
-    HIVE_PARQUET_TIMESTAMP_TIME_UNIT("hive.parquet.timestamp.time.unit", "millis",
+    HIVE_PARQUET_TIMESTAMP_TIME_UNIT("hive.parquet.timestamp.time.unit", "micros",
         new StringSet("nanos", "micros", "millis"),
         "Store parquet int64/LogicalTypes timestamps in this time unit."),
 

File: ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java
Patch:
@@ -413,7 +413,7 @@ public void testLogicalTypes() throws Exception {
     testLogicalTypeAnnotation("boolean", "a", null, conf);
     testLogicalTypeAnnotation("binary", "a", null, conf);
     testLogicalTypeAnnotation("timestamp", "a",
-        LogicalTypeAnnotation.timestampType(false, LogicalTypeAnnotation.TimeUnit.MILLIS), conf);
+        LogicalTypeAnnotation.timestampType(false, LogicalTypeAnnotation.TimeUnit.MICROS), conf);
     testLogicalTypeAnnotation("char(3)", "a", LogicalTypeAnnotation.stringType(), conf);
     testLogicalTypeAnnotation("varchar(30)", "a", LogicalTypeAnnotation.stringType(), conf);
     testLogicalTypeAnnotation("decimal(7,2)", "a", LogicalTypeAnnotation.decimalType(2, 7), conf);

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatOutputFormat.java
Patch:
@@ -20,6 +20,7 @@
 package org.apache.hive.hcatalog.mapreduce;
 
 import java.io.IOException;
+import java.text.DecimalFormat;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
@@ -112,8 +113,9 @@ public static void setOutput(Configuration conf, Credentials credentials,
       // Set up a common id hash for this job, so that when we create any temporary directory
       // later on, it is guaranteed to be unique.
       String idHash;
+      DecimalFormat df = new DecimalFormat("#.####################");
       if ((idHash = conf.get(HCatConstants.HCAT_OUTPUT_ID_HASH)) == null) {
-        idHash = String.valueOf(Math.random());
+        idHash = String.valueOf(df.format(Math.random()));
       }
       conf.set(HCatConstants.HCAT_OUTPUT_ID_HASH,idHash);
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
Patch:
@@ -3212,7 +3212,7 @@ private VectorExpression getCastToDecimal(List<ExprNodeDesc> childExpr, TypeInfo
       try {
         Object constantValue = ((ExprNodeConstantDesc) child).getValue();
         if (tryDecimal64Cast) {
-          if (((DecimalTypeInfo)returnType).precision() + ((DecimalTypeInfo)returnType).scale() <= 18) {
+          if (((DecimalTypeInfo)returnType).precision() <= 18) {
             Long longValue = castConstantToLong(constantValue, child.getTypeInfo(), PrimitiveCategory.LONG);
             return getConstantVectorExpression(longValue, TypeInfoFactory.longTypeInfo,
                 VectorExpressionDescriptor.Mode.PROJECTION);
@@ -3229,7 +3229,7 @@ private VectorExpression getCastToDecimal(List<ExprNodeDesc> childExpr, TypeInfo
     }
     if (isIntFamily(inputType)) {
       if (tryDecimal64Cast) {
-        if (((DecimalTypeInfo)returnType).precision() + ((DecimalTypeInfo)returnType).scale() <= 18) {
+        if (((DecimalTypeInfo)returnType).precision() <= 18) {
           return createVectorExpression(CastLongToDecimal64.class, childExpr,
               VectorExpressionDescriptor.Mode.PROJECTION, returnType, DataTypePhysicalVariation.DECIMAL_64);
         }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java
Patch:
@@ -697,7 +697,7 @@ protected VectorizedRowBatch setupOverflowBatch() throws HiveException {
     // Now, add any scratch columns needed for children operators.
     int outputColumn = initialColumnCount;
     for (String typeName : vOutContext.getScratchColumnTypeNames()) {
-      allocateOverflowBatchColumnVector(overflowBatch, outputColumn++, typeName, vOutContext.getDataTypePhysicalVariation(outputColumn));
+      allocateOverflowBatchColumnVector(overflowBatch, outputColumn, typeName, vOutContext.getDataTypePhysicalVariation(outputColumn++));
     }
 
     overflowBatch.projectedColumns = outputProjection;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3379,7 +3379,7 @@ public static enum ConfVars {
         "launched on each of the queues specified by \"hive.server2.tez.default.queues\".\n" +
         "Determines the parallelism on each queue."),
     HIVE_SERVER2_TEZ_INITIALIZE_DEFAULT_SESSIONS("hive.server2.tez.initialize.default.sessions",
-        false,
+        true,
         "This flag is used in HiveServer2 to enable a user to use HiveServer2 without\n" +
         "turning on Tez for HiveServer2. The user could potentially want to run queries\n" +
         "over Tez without the pool of sessions."),

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3674,15 +3674,15 @@ public static enum ConfVars {
          "Whether enable loading UDFs from metastore on demand; this is mostly relevant for\n" +
          "HS2 and was the default behavior before Hive 1.2. Off by default."),
 
-    HIVE_SERVER2_SESSION_CHECK_INTERVAL("hive.server2.session.check.interval", "6h",
+    HIVE_SERVER2_SESSION_CHECK_INTERVAL("hive.server2.session.check.interval", "15m",
         new TimeValidator(TimeUnit.MILLISECONDS, 3000l, true, null, false),
         "The check interval for session/operation timeout, which can be disabled by setting to zero or negative value."),
     HIVE_SERVER2_CLOSE_SESSION_ON_DISCONNECT("hive.server2.close.session.on.disconnect", true,
       "Session will be closed when connection is closed. Set this to false to have session outlive its parent connection."),
-    HIVE_SERVER2_IDLE_SESSION_TIMEOUT("hive.server2.idle.session.timeout", "7d",
+    HIVE_SERVER2_IDLE_SESSION_TIMEOUT("hive.server2.idle.session.timeout", "4h",
         new TimeValidator(TimeUnit.MILLISECONDS),
         "Session will be closed when it's not accessed for this duration, which can be disabled by setting to zero or negative value."),
-    HIVE_SERVER2_IDLE_OPERATION_TIMEOUT("hive.server2.idle.operation.timeout", "5d",
+    HIVE_SERVER2_IDLE_OPERATION_TIMEOUT("hive.server2.idle.operation.timeout", "2h",
         new TimeValidator(TimeUnit.MILLISECONDS),
         "Operation will be closed when it's not accessed for this duration of time, which can be disabled by setting to zero value.\n" +
         "  With positive value, it's checked for operations in terminal state only (FINISHED, CANCELED, CLOSED, ERROR).\n" +

File: standalone-metastore/metastore-tools/tools-common/src/main/java/org/apache/hadoop/hive/metastore/tools/Constants.java
Patch:
@@ -26,7 +26,7 @@ public final class Constants {
   static final String OPT_DATABASE = "database";
   static final String OPT_CONF = "conf";
   static final String OPT_VERBOSE = "verbose";
-  static final int HMS_DEFAULT_PORT = 8093;
+  static final int HMS_DEFAULT_PORT = 9083;
 
   // Disable object construction
   private Constants() {}

File: ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/drop/AbstractDropPartitionAnalyzer.java
Patch:
@@ -49,8 +49,8 @@
 import org.apache.hadoop.hive.ql.parse.HiveParser;
 import org.apache.hadoop.hive.ql.parse.ReplicationSpec;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
-import org.apache.hadoop.hive.ql.parse.TypeCheckCtx;
-import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
+import org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck;
+import org.apache.hadoop.hive.ql.parse.type.TypeCheckCtx;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
@@ -158,7 +158,7 @@ private Map<Integer, List<ExprNodeGenericFuncDesc>> getFullPartitionSpecs(
         ASTNode partValNode = (ASTNode)partSpecSingleKey.getChild(2);
         TypeCheckCtx typeCheckCtx = new TypeCheckCtx(null);
         ExprNodeConstantDesc valExpr =
-            (ExprNodeConstantDesc)TypeCheckProcFactory.genExprNode(partValNode, typeCheckCtx).get(partValNode);
+            (ExprNodeConstantDesc) ExprNodeTypeCheck.genExprNode(partValNode, typeCheckCtx).get(partValNode);
         Object val = valExpr.getValue();
 
         boolean isDefaultPartitionName = val.equals(defaultPartitionName);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java
Patch:
@@ -30,7 +30,6 @@
 import java.util.stream.Collectors;
 import org.apache.hadoop.hive.conf.Constants;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
@@ -59,8 +58,8 @@
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.metadata.VirtualColumn;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
-import org.apache.hadoop.hive.ql.parse.ParseUtils;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck;
 import org.apache.hadoop.hive.ql.plan.ColStatistics;
 import org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
@@ -234,7 +233,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
         if (numBuckets > 0) {
           bucketColumns = new ArrayList<>();
           //add a cast(ROW__ID as int) to wrap in UDFToInteger()
-          bucketColumns.add(ParseUtils.createConversionCast(new ExprNodeColumnDesc(ci), TypeInfoFactory.intTypeInfo));
+          bucketColumns.add(ExprNodeTypeCheck.getExprNodeDefaultExprProcessor()
+              .createConversionCast(new ExprNodeColumnDesc(ci), TypeInfoFactory.intTypeInfo));
         }
       } else {
         if (!destTable.getSortCols().isEmpty()) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveUnionVisitor.java
Patch:
@@ -31,8 +31,8 @@
 import org.apache.hadoop.hive.ql.exec.RowSchema;
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveUnion;
 import org.apache.hadoop.hive.ql.optimizer.calcite.translator.opconventer.HiveOpConverter.OpAttr;
-import org.apache.hadoop.hive.ql.parse.ParseUtils;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
@@ -128,7 +128,8 @@ private Operator<? extends OperatorDesc> genInputSelectForUnion(Operator<? exten
       ExprNodeDesc column = new ExprNodeColumnDesc(oInfo.getType(), oInfo.getInternalName(),
           oInfo.getTabAlias(), oInfo.getIsVirtualCol(), oInfo.isSkewedCol());
       if (!oInfo.getType().equals(uInfo.getType())) {
-        column = ParseUtils.createConversionCast(column, (PrimitiveTypeInfo) uInfo.getType());
+        column = ExprNodeTypeCheck.getExprNodeDefaultExprProcessor()
+            .createConversionCast(column, (PrimitiveTypeInfo) uInfo.getType());
       }
       columns.add(column);
       colName.add(uInfo.getInternalName());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsAutoGatherContext.java
Patch:
@@ -38,6 +38,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.AnalyzeRewriteContext;
+import org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
@@ -276,7 +277,7 @@ private void replaceSelectOperatorProcess(SelectOperator operator, Operator<? ex
         TypeInfo destType = selRS.getSignature().get(this.columns.size() + i).getType();
         if (!srcType.equals(destType)) {
           // This may be possible when srcType is string but destType is integer
-          exprNodeDesc = ParseUtils
+          exprNodeDesc = ExprNodeTypeCheck.getExprNodeDefaultExprProcessor()
               .createConversionCast(exprNodeDesc, (PrimitiveTypeInfo) destType);
         }
       }
@@ -288,7 +289,7 @@ private void replaceSelectOperatorProcess(SelectOperator operator, Operator<? ex
         TypeInfo destType = selRS.getSignature().get(this.columns.size() + i).getType();
         exprNodeDesc = new ExprNodeColumnDesc(col);
         if (!srcType.equals(destType)) {
-          exprNodeDesc = ParseUtils
+          exprNodeDesc = ExprNodeTypeCheck.getExprNodeDefaultExprProcessor()
               .createConversionCast(exprNodeDesc, (PrimitiveTypeInfo) destType);
         }
       }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -107,6 +107,9 @@
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.parse.authorization.AuthorizationParseUtils;
+import org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck;
+import org.apache.hadoop.hive.ql.parse.type.TypeCheckCtx;
+import org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory;
 import org.apache.hadoop.hive.ql.plan.BasicStatsWork;
 import org.apache.hadoop.hive.ql.plan.ColumnStatsUpdateWork;
 import org.apache.hadoop.hive.ql.plan.HiveOperation;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/PTFTranslator.java
Patch:
@@ -55,6 +55,7 @@
 import org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec;
 import org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowSpec;
 import org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowType;
+import org.apache.hadoop.hive.ql.parse.type.TypeCheckCtx;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
 import org.apache.hadoop.hive.ql.plan.PTFDesc;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java
Patch:
@@ -32,7 +32,8 @@
 import org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSubquerySemanticException;
 import org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite;
 import org.apache.hadoop.hive.ql.parse.SubQueryUtils.ISubQueryJoinInfo;
-import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.DefaultExprProcessor;
+import org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck;
+import org.apache.hadoop.hive.ql.parse.type.TypeCheckCtx;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
@@ -264,7 +265,7 @@ class ConjunctAnalyzer {
         boolean forHavingClause,
         String parentQueryNewAlias) {
       this.parentQueryRR = parentQueryRR;
-      defaultExprProcessor = new DefaultExprProcessor();
+      defaultExprProcessor = ExprNodeTypeCheck.getExprNodeDefaultExprProcessor();
       this.forHavingClause = forHavingClause;
       this.parentQueryNewAlias = parentQueryNewAlias;
       stack = new Stack<Node>();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/UnparseTranslator.java
Patch:
@@ -175,7 +175,7 @@ void addTableNameTranslation(ASTNode tableName, String currentDatabaseName) {
    * @param node
    *          source node (which must be an identifier) to be replaced
    */
-  void addIdentifierTranslation(ASTNode identifier) {
+  public void addIdentifierTranslation(ASTNode identifier) {
     if (!enabled) {
       return;
     }
@@ -198,7 +198,7 @@ void addIdentifierTranslation(ASTNode identifier) {
    * @param sourceNode the node providing the replacement text
    *
    */
-  void addCopyTranslation(ASTNode targetNode, ASTNode sourceNode) {
+  public void addCopyTranslation(ASTNode targetNode, ASTNode sourceNode) {
     if (!enabled) {
       return;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/type/JoinTypeCheckCtx.java
Patch:
@@ -15,14 +15,14 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hadoop.hive.ql.optimizer.calcite.translator;
+package org.apache.hadoop.hive.ql.parse.type;
 
 import java.util.List;
 
 import org.apache.hadoop.hive.ql.parse.JoinType;
 import org.apache.hadoop.hive.ql.parse.RowResolver;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
-import org.apache.hadoop.hive.ql.parse.TypeCheckCtx;
+import org.apache.hadoop.hive.ql.parse.type.TypeCheckCtx;
 
 import com.google.common.collect.ImmutableList;
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/type/TypeCheckProcFactoryUtils.java
Patch:
@@ -16,13 +16,14 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.parse;
+package org.apache.hadoop.hive.ql.parse.type;
 
 import java.util.ArrayList;
 import java.util.List;
 
 import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
@@ -39,7 +40,7 @@
 
 public class TypeCheckProcFactoryUtils {
 
-  static ArrayList<ExprNodeDesc> rewriteInToOR(ArrayList<ExprNodeDesc> inOperands) throws SemanticException {
+  static List<ExprNodeDesc> rewriteInToOR(List<ExprNodeDesc> inOperands) throws SemanticException {
     ExprNodeDesc columnDesc = inOperands.get(0);
 
     ArrayList<ExprNodeDesc> orOperands = new ArrayList<>();

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java
Patch:
@@ -226,8 +226,7 @@ public ExprNodeDesc clone() {
    * @throws UDFArgumentException
    */
   public static ExprNodeGenericFuncDesc newInstance(GenericUDF genericUDF,
-      String funcText,
-      List<ExprNodeDesc> children) throws UDFArgumentException {
+      String funcText, List<ExprNodeDesc> children) throws UDFArgumentException {
     ObjectInspector[] childrenOIs = new ObjectInspector[children.size()];
     for (int i = 0; i < childrenOIs.length; i++) {
       childrenOIs[i] = children.get(i).getWritableObjectInspector();

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
Patch:
@@ -63,7 +63,7 @@
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
-import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
+import org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.util.NullOrdering;
 import org.apache.hadoop.hive.serde.serdeConstants;
@@ -866,7 +866,8 @@ public static ReduceSinkDesc getReduceSinkDesc(
       partitionCols.addAll(keyCols.subList(0, numPartitionFields));
     } else {
       // numPartitionFields = -1 means random partitioning
-      partitionCols.add(TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc("rand"));
+      partitionCols.add(ExprNodeTypeCheck.getExprNodeDefaultExprProcessor().
+          getFuncExprNodeDesc("rand"));
     }
 
     StringBuilder order = new StringBuilder();

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ptf/ShapeDetails.java
Patch:
@@ -21,9 +21,8 @@
 import java.util.List;
 import java.util.Map;
 
-import org.apache.hadoop.hive.ql.exec.PTFUtils;
 import org.apache.hadoop.hive.ql.parse.RowResolver;
-import org.apache.hadoop.hive.ql.parse.TypeCheckCtx;
+import org.apache.hadoop.hive.ql.parse.type.TypeCheckCtx;
 import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/MatchPath.java
Patch:
@@ -35,8 +35,8 @@
 import org.apache.hadoop.hive.ql.parse.RowResolver;
 import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
-import org.apache.hadoop.hive.ql.parse.TypeCheckCtx;
-import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
+import org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck;
+import org.apache.hadoop.hive.ql.parse.type.TypeCheckCtx;
 import org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowExpressionSpec;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
@@ -815,7 +815,7 @@ public static ExprNodeDesc buildExprNode(ASTNode expr,
     {
       // todo: use SemanticAnalyzer::genExprNodeDesc
       // currently SA not available to PTFTranslator.
-      Map<ASTNode, ExprNodeDesc> map = TypeCheckProcFactory
+      Map<ASTNode, ExprNodeDesc> map = ExprNodeTypeCheck
           .genExprNode(expr, typeCheckCtx);
       ExprNodeDesc desc = map.get(expr);
       if (desc == null) {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
Patch:
@@ -34,7 +34,8 @@
 import org.apache.hadoop.hive.ql.io.IOContextMap;
 import org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin;
 import org.apache.hadoop.hive.ql.optimizer.physical.LlapClusterStateForCompile;
-import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
+import org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck;
+import org.apache.hadoop.hive.ql.parse.type.TypeCheckProcFactory;
 import org.apache.hadoop.hive.ql.plan.CollectDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
@@ -232,7 +233,7 @@ public void testScriptOperator() throws Throwable {
       ExprNodeDesc expr1 = new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, "col0", "",
           false);
       ExprNodeDesc expr2 = new ExprNodeConstantDesc("1");
-      ExprNodeDesc exprDesc2 = TypeCheckProcFactory.DefaultExprProcessor
+      ExprNodeDesc exprDesc2 = ExprNodeTypeCheck.getExprNodeDefaultExprProcessor()
           .getFuncExprNodeDesc("concat", expr1, expr2);
 
       // select operator to project these two columns

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestPlan.java
Patch:
@@ -27,7 +27,7 @@
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.ql.CompilationOpContext;
-import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
+import org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.FilterDesc;
@@ -60,7 +60,7 @@ public void testPlan() throws Exception {
           TypeInfoFactory.stringTypeInfo, f1, "", false);
       ExprNodeDesc expr2 = new ExprNodeColumnDesc(
           TypeInfoFactory.stringTypeInfo, f2, "", false);
-      ExprNodeDesc filterExpr = TypeCheckProcFactory.DefaultExprProcessor
+      ExprNodeDesc filterExpr = ExprNodeTypeCheck.getExprNodeDefaultExprProcessor()
           .getFuncExprNodeDesc("==", expr1, expr2);
 
       FilterDesc filterCtx = new FilterDesc(filterExpr, false);

File: ql/src/java/org/apache/hadoop/hive/ql/scheduled/ScheduledQueryExecutionService.java
Patch:
@@ -106,16 +106,16 @@ public synchronized void reportQueryProgress() {
 
     private void processQuery(ScheduledQueryPollResponse q) {
       SessionState state = null;
+      info = new ScheduledQueryProgressInfo();
+      info.setScheduledExecutionId(q.getExecutionId());
+      info.setState(QueryState.EXECUTING);
       try {
         HiveConf conf = new HiveConf(context.conf);
         conf.set(Constants.HIVE_QUERY_EXCLUSIVE_LOCK, lockNameFor(q.getScheduleKey()));
         conf.setVar(HiveConf.ConfVars.HIVE_AUTHENTICATOR_MANAGER, SessionStateUserAuthenticator.class.getName());
         conf.unset(HiveConf.ConfVars.HIVESESSIONID.varname);
         state = new SessionState(conf, q.getUser());
         SessionState.start(state);
-        info = new ScheduledQueryProgressInfo();
-        info.setScheduledExecutionId(q.getExecutionId());
-        info.setState(QueryState.EXECUTING);
         reportQueryProgress();
         try (
           IDriver driver = DriverFactory.newDriver(DriverFactory.getNewQueryState(conf), null)) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
Patch:
@@ -46,8 +46,8 @@
 import org.apache.parquet.hadoop.api.ReadSupport;
 import org.apache.parquet.io.api.RecordMaterializer;
 import org.apache.parquet.schema.GroupType;
+import org.apache.parquet.schema.LogicalTypeAnnotation;
 import org.apache.parquet.schema.MessageType;
-import org.apache.parquet.schema.OriginalType;
 import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName;
 import org.apache.parquet.schema.Type;
 import org.apache.parquet.schema.Type.Repetition;
@@ -160,8 +160,8 @@ private static Type getProjectedType(TypeInfo colType, Type fieldType) {
             } else {
               subFieldType = getProjectedType(elemType, subFieldType);
             }
-            return Types.buildGroup(Repetition.OPTIONAL).as(OriginalType.LIST).addFields(
-              subFieldType).named(fieldType.getName());
+          return Types.buildGroup(Repetition.OPTIONAL).as(LogicalTypeAnnotation.listType())
+              .addFields(subFieldType).named(fieldType.getName());
           }
         }
         break;

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/BaseVectorizedColumnReader.java
Patch:
@@ -32,8 +32,8 @@
 import org.apache.parquet.column.values.ValuesReader;
 import org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder;
 import org.apache.parquet.io.ParquetDecodingException;
-import org.apache.parquet.schema.DecimalMetadata;
 import org.apache.parquet.schema.Type;
+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -254,8 +254,7 @@ private IntIterator newRLEIterator(int maxLevel, BytesInput bytes) {
    * @param type
    */
   protected void decimalTypeCheck(Type type) {
-    DecimalMetadata decimalMetadata = type.asPrimitiveType().getDecimalMetadata();
-    if (decimalMetadata == null) {
+    if (!(type.getLogicalTypeAnnotation() instanceof DecimalLogicalTypeAnnotation)) {
       throw new UnsupportedOperationException("The underlying Parquet type cannot be able to " +
           "converted to Hive Decimal type: " + type);
     }

File: service/src/java/org/apache/hive/service/CookieSigner.java
Patch:
@@ -81,7 +81,7 @@ public String verifyAndExtract(String signedStr) {
     if (LOG.isDebugEnabled()) {
       LOG.debug("Signature generated for " + rawValue + " inside verify is " + currentSignature);
     }
-    if (!originalSignature.equals(currentSignature)) {
+    if (!MessageDigest.isEqual(originalSignature.getBytes(), currentSignature.getBytes())) {
       throw new IllegalArgumentException("Invalid sign, original = " + originalSignature +
         " current = " + currentSignature);
     }

File: shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
Patch:
@@ -63,7 +63,7 @@ public abstract class ShimLoader {
 
   static {
     HADOOP_THRIFT_AUTH_BRIDGE_CLASSES.put(HADOOP23VERSIONNAME,
-        "org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge23");
+        "org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge23");
   }
 
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/opconventer/HiveTableScanVisitor.java
Patch:
@@ -108,6 +108,7 @@ OpAttr visit(HiveTableScan scanRel) {
     // 2. Setup TableScan
     TableScanOperator ts = (TableScanOperator) OperatorFactory.get(
         hiveOpConverter.getSemanticAnalyzer().getOpContext(), tsd, new RowSchema(colInfos));
+    ts.setBucketingVersion(tsd.getTableMetadata().getBucketingVersion());
 
     //now that we let Calcite process subqueries we might have more than one
     // tablescan with same alias.

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -691,7 +691,7 @@ private Map<String, ASTNode> doPhase1GetAggregationsFromSelect(
    * @param dest destination clause
    * @return true or false
    */
-  private boolean isInsertInto(QBParseInfo qbp, String dest) {
+  protected boolean isInsertInto(QBParseInfo qbp, String dest) {
     // get the destination and check if it is TABLE
     if(qbp == null || dest == null ) {
       return false;

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -3925,13 +3925,13 @@ protected String isWithinCheckInterval(String expr, long interval) throws MetaEx
         break;
       case MYSQL:
       case POSTGRES:
-        condition = expr + " => current_timestamp - interval '" + interval + "' second";
+        condition = expr + " >= current_timestamp - interval '" + interval + "' second";
         break;
       case SQLSERVER:
         condition = "DATEDIFF(second, " + expr + ", current_timestamp) <= " + interval;
         break;
       case ORACLE:
-        condition = expr + " => current_timestamp - numtodsinterval(" + interval + " , 'second')";
+        condition = expr + " >= current_timestamp - numtodsinterval(" + interval + " , 'second')";
         break;
       default:
         String msg = "Unknown database product: " + dbProduct.toString();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveDefaultRelMetadataProvider.java
Patch:
@@ -61,7 +61,6 @@
 import org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdMemory;
 import org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdParallelism;
 import org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates;
-import org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdMaxRowCount;
 import org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount;
 import org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRuntimeRowCount;
 import org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity;
@@ -84,7 +83,6 @@ public class HiveDefaultRelMetadataProvider {
                   new HiveRelMdCost(HiveDefaultCostModel.getCostModel()).getMetadataProvider(),
                   HiveRelMdSelectivity.SOURCE,
                   HiveRelMdRuntimeRowCount.SOURCE,
-                  HiveRelMdMaxRowCount.SOURCE,
                   HiveRelMdUniqueKeys.SOURCE,
                   HiveRelMdColumnUniqueness.SOURCE,
                   HiveRelMdSize.SOURCE,
@@ -156,7 +154,6 @@ private RelMetadataProvider init(HiveConf hiveConf) {
                   new HiveRelMdCost(HiveOnTezCostModel.getCostModel(hiveConf)).getMetadataProvider(),
                   HiveRelMdSelectivity.SOURCE,
                   HiveRelMdRowCount.SOURCE,
-                  HiveRelMdMaxRowCount.SOURCE,
                   HiveRelMdUniqueKeys.SOURCE,
                   HiveRelMdColumnUniqueness.SOURCE,
                   HiveRelMdSize.SOURCE,

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliConfigs.java
Patch:
@@ -244,8 +244,6 @@ public MiniLlapLocalCliConfig() {
 
         includesFrom(testConfigProps, "minillaplocal.query.files");
         includesFrom(testConfigProps, "minillaplocal.shared.query.files");
-        excludeQuery("results_cache_invalidation.q");// unstable; will be enabled by HIVE-22624
-        excludeQuery("results_cache_lifetime.q"); // unstable; will be enabled by HIVE-22624
         excludeQuery("bucket_map_join_tez1.q"); // Disabled in HIVE-19509
         excludeQuery("special_character_in_tabnames_1.q"); // Disabled in HIVE-19509
         excludeQuery("tez_smb_1.q"); // Disabled in HIVE-19509

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -27,6 +27,7 @@
 import java.io.FileWriter;
 import java.io.IOException;
 import java.io.OutputStream;
+import java.lang.management.ManagementFactory;
 import java.net.URL;
 import java.sql.SQLException;
 import java.util.ArrayList;
@@ -81,6 +82,7 @@
 import org.apache.hadoop.hive.ql.scheduled.QTestScheduledQueryCleaner;
 import org.apache.hadoop.hive.ql.scheduled.QTestScheduledQueryServiceProvider;
 import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hive.common.util.ProcessUtils;
 import org.junit.Assert;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -208,6 +210,7 @@ public QTestUtil(QTestArguments testArgs) throws Exception {
     datasetHandler = new QTestDatasetHandler(conf);
     testFiles = datasetHandler.getDataDir(conf);
     conf.set("test.data.dir", datasetHandler.getDataDir(conf));
+    conf.setVar(ConfVars.HIVE_QUERY_RESULTS_CACHE_DIRECTORY, "/tmp/hive/_resultscache_" + ProcessUtils.getPid());
     dispatcher.register("dataset", datasetHandler);
     dispatcher.register("replace", replaceHandler);
     dispatcher.register("sysdb", new QTestSysDbHandler());

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java
Patch:
@@ -1323,7 +1323,8 @@ public void testCompactWithDelete() throws Exception {
     ShowCompactResponse resp = txnHandler.showCompact(new ShowCompactRequest());
     Assert.assertEquals("Unexpected number of compactions in history", 2, resp.getCompactsSize());
     Assert.assertEquals("Unexpected 0 compaction state", TxnStore.CLEANING_RESPONSE, resp.getCompacts().get(0).getState());
-    Assert.assertEquals("Unexpected 1 compaction state", TxnStore.CLEANING_RESPONSE, resp.getCompacts().get(1).getState());
+    Assert.assertEquals("Unexpected 1 compaction state", TxnStore.SUCCEEDED_RESPONSE,
+        resp.getCompacts().get(1).getState());
   }
 
   /**

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -9679,7 +9679,7 @@ public boolean doesPartitionExist(String catName, String dbName, String tableNam
 
   private void debugLog(final String message) {
     if (LOG.isDebugEnabled()) {
-      LOG.debug("{}", message, new Exception());
+      LOG.debug("{}", message, new Exception("Debug Dump Stack Trace (Not an Exception)"));
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
Patch:
@@ -1505,7 +1505,7 @@ public List<String> add_resources(ResourceType t, Collection<String> values, boo
         Set<String> downloadedValues = new HashSet<String>();
 
         for (URI uri : downloadedURLs) {
-          String resourceValue = uri.toString();
+          String resourceValue = uri.getPath();
           downloadedValues.add(resourceValue);
           localized.add(resourceValue);
           if (reverseResourcePathMap.containsKey(resourceValue)) {

File: ql/src/java/org/apache/hadoop/hive/ql/util/ResourceDownloader.java
Patch:
@@ -98,13 +98,13 @@ private String downloadResource(URI srcUri, String subDir, boolean convertToUnix
     LOG.debug("Converting to local {}", srcUri);
     File destinationDir = (subDir == null) ? resourceDir : new File(resourceDir, subDir);
     ensureDirectory(destinationDir);
-    File destinationFile = new File(destinationDir, new Path(srcUri.toString()).getName());
+    File destinationFile = new File(destinationDir, new Path(srcUri).getName());
     String dest = destinationFile.getCanonicalPath();
     if (destinationFile.exists()) {
       return dest;
     }
     FileSystem fs = FileSystem.get(srcUri, conf);
-    fs.copyToLocalFile(new Path(srcUri.toString()), new Path(dest));
+    fs.copyToLocalFile(new Path(srcUri), new Path(dest));
     // add "execute" permission to downloaded resource file (needed when loading dll file)
     FileUtil.chmod(dest, "ugo+rx", true);
     return dest;

File: ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java
Patch:
@@ -217,6 +217,7 @@ private void pushRankLimit(PTFOperator ptfOp, OpWalkerInfo owi) throws SemanticE
       }
 
       WindowTableFunctionDef wTFn = (WindowTableFunctionDef) conf.getFuncDef();
+
       List<Integer> rFnIdxs = rankingFunctions(wTFn);
 
       if ( rFnIdxs.size() == 0 ) {
@@ -325,7 +326,6 @@ private int[] getLimit(WindowTableFunctionDef wTFn, List<Integer> rFnIdxs, ExprN
      * reference rows past the Current Row.
      */
     private boolean canPushLimitToReduceSink(WindowTableFunctionDef wTFn) {
-
       for(WindowFunctionDef wFnDef : wTFn.getWindowFunctions() ) {
         if ( (wFnDef.getWFnEval() instanceof GenericUDAFRankEvaluator) ||
             (wFnDef.getWFnEval() instanceof GenericUDAFDenseRankEvaluator )  ||

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
Patch:
@@ -93,7 +93,8 @@ class LlapRecordReader implements RecordReader<NullWritable, VectorizedRowBatch>
   private boolean isFirst = true;
   private int maxQueueSize = 0;
 
-  private boolean isClosed = false, isInterrupted = false;
+  private volatile boolean isClosed = false;
+  private volatile boolean isInterrupted = false;
   private final ConsumerFeedback<ColumnVectorBatch> feedback;
   private final QueryFragmentCounters counters;
   private long firstReturnTime;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java
Patch:
@@ -381,7 +381,7 @@ public void setValueSerializeInfo(TableDesc valueSerializeInfo) {
    *         (ascending order) and "-" (descending order).
    */
   @Signature
-  @Explain(displayName = "sort order")
+  @Explain(displayName = "sort order", explainLevels = { Level.DEFAULT, Level.EXTENDED, Level.USER })
   public String getOrder() {
     return keySerializeInfo.getProperties().getProperty(
         org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_SORT_ORDER);
@@ -407,7 +407,7 @@ public boolean isOrdering() {
    *         of the same length as key columns, that consists of only "a"
    *         (null first) and "z" (null last).
    */
-  @Explain(displayName = "null sort order", explainLevels = { Level.EXTENDED })
+  @Explain(displayName = "null sort order", explainLevels = { Level.DEFAULT, Level.EXTENDED, Level.USER })
   public String getNullOrder() {
     return keySerializeInfo.getProperties().getProperty(
         org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_NULL_SORT_ORDER);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/TopNKeyDesc.java
Patch:
@@ -69,7 +69,7 @@ public void setColumnSortOrder(String columnSortOrder) {
     this.columnSortOrder = columnSortOrder;
   }
 
-  @Explain(displayName = "null sort order", explainLevels = { Level.EXTENDED })
+  @Explain(displayName = "null sort order", explainLevels = { Level.DEFAULT, Level.EXTENDED, Level.USER })
   public String getNullOrder() {
     return nullOrder;
   }

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/ReplChangeManager.java
Patch:
@@ -481,7 +481,7 @@ public static String getReplPolicyIdString(Database db) {
         LOG.debug("repl policy for database {} is {}", db.getName(), replPolicyId);
         return replPolicyId;
       }
-      LOG.debug("Repl policy is not set for database ", db.getName());
+      LOG.debug("Repl policy is not set for database: {}", db.getName());
     }
     return null;
   }

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -572,7 +572,7 @@ private List<Long> openTxns(Connection dbConn, Statement stmt, OpenTxnRequest rq
     int numTxns = rqst.getNum_txns();
     ResultSet rs = null;
     List<PreparedStatement> insertPreparedStmts = null;
-    TxnType txnType = TxnType.DEFAULT;
+    TxnType txnType = rqst.isSetTxn_type() ? rqst.getTxn_type() : TxnType.DEFAULT;
     try {
       if (rqst.isSetReplPolicy()) {
         List<Long> targetTxnIdList = getTargetTxnIdList(rqst.getReplPolicy(), rqst.getReplSrcTxnIds(), dbConn);

File: shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
Patch:
@@ -262,9 +262,7 @@ public int compare(LongWritable o1, LongWritable o2) {
    */
   @Override
   public void refreshDefaultQueue(Configuration conf, String userName) throws IOException {
-    if (StringUtils.isNotBlank(userName) && isFairScheduler(conf)) {
-      ShimLoader.getSchedulerShims().refreshDefaultQueue(conf, userName);
-    }
+    //no op
   }
 
   private boolean isFairScheduler (Configuration conf) {

File: llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestBuddyAllocator.java
Patch:
@@ -105,7 +105,7 @@ public void testSameSizes() throws Exception {
     allocSameSize(a, arenaCount * 2, allocLog2);
   }
 
-  /*  HIVE-22286: Disabled test until HIVE-22175 is resolved @Test */
+  @Test
   public void testMTT() {
     final int min = 3, max = 8, maxAlloc = 1 << max, allocsPerSize = 3;
     final BuddyAllocator a = new BuddyAllocator(isDirect, isMapped, 1 << min, maxAlloc,

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestObjectStore.java
Patch:
@@ -454,9 +454,11 @@ public void testConcurrentDropPartitions() throws MetaException, InvalidObjectEx
     for (int i = 0; i < numThreads; i++) {
       executorService.execute(
         () -> {
+          ObjectStore threadObjectStore = new ObjectStore();
+          threadObjectStore.setConf(conf);
           for (List<String> p : partNames) {
             try {
-              objectStore.dropPartition(DEFAULT_CATALOG_NAME, DB1, TABLE1, p);
+              threadObjectStore.dropPartition(DEFAULT_CATALOG_NAME, DB1, TABLE1, p);
               System.out.println("Dropping partition: " + p.get(0));
             } catch (Exception e) {
               throw new RuntimeException(e);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3255,7 +3255,7 @@ public static enum ConfVars {
         "The implementation that we should use for the materialized views registry. \n" +
         "  DEFAULT: Default cache for materialized views\n" +
         "  DUMMY: Do not cache materialized views and hence forward requests to metastore"),
-    HIVE_SERVER2_MATERIALIZED_VIEWS_REGISTRY_REFRESH("hive.server2.materializedviews.registry.refresh.period", "60s",
+    HIVE_SERVER2_MATERIALIZED_VIEWS_REGISTRY_REFRESH("hive.server2.materializedviews.registry.refresh.period", "1500s",
         new TimeValidator(TimeUnit.SECONDS),
         "Period, specified in seconds, between successive refreshes of the registry to pull new materializations " +
         "from the metastore that may have been created by other HS2 instances."),

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java
Patch:
@@ -90,7 +90,7 @@ public final class MetaDataFormatUtils {
   private MetaDataFormatUtils() {
   }
 
-  private static String convertToString(Decimal val) {
+  public static String convertToString(Decimal val) {
     if (val == null) {
       return "";
     }
@@ -103,7 +103,7 @@ private static String convertToString(Decimal val) {
     }
   }
 
-  private static String convertToString(org.apache.hadoop.hive.metastore.api.Date val) {
+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Date val) {
     if (val == null) {
       return "";
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -1633,7 +1633,7 @@ private void analyzeDescribeTable(ASTNode ast) throws SemanticException {
     DescTableDesc descTblDesc = new DescTableDesc(ctx.getResFile(), tableName, partSpec, colPath, isExt, isFormatted);
     Task<?> ddlTask = TaskFactory.get(new DDLWork(getInputs(), getOutputs(), descTblDesc));
     rootTasks.add(ddlTask);
-    String schema = DescTableDesc.getSchema(showColStats);
+    String schema = showColStats ? DescTableDesc.COLUMN_STATISTICS_SCHEMA : DescTableDesc.SCHEMA;
     setFetchTask(createFetchTask(schema));
     LOG.info("analyzeDescribeTable done");
   }

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
Patch:
@@ -83,7 +83,7 @@ public ParquetRecordReaderWrapper(
     Configuration conf = jobConf;
     if (skipTimestampConversion ^ HiveConf.getBoolVar(
         conf, HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION)) {
-      conf = new JobConf(oldJobConf);
+      conf = new JobConf(jobConf);
       HiveConf.setBoolVar(conf,
         HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION, skipTimestampConversion);
     }

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DateColumnStatsMerger.java
Patch:
@@ -57,7 +57,7 @@ public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj new
     aggregateColStats.getStatsData().setDateStats(aggregateData);
   }
 
-  private void setLowValue(DateColumnStatsDataInspector aggregateData, DateColumnStatsDataInspector newData) {
+  public void setLowValue(DateColumnStatsDataInspector aggregateData, DateColumnStatsDataInspector newData) {
     if (!aggregateData.isSetLowValue() && !newData.isSetLowValue()) {
       return;
     }
@@ -75,7 +75,7 @@ private void setLowValue(DateColumnStatsDataInspector aggregateData, DateColumnS
     aggregateData.setLowValue(mergedLowValue);
   }
 
-  private void setHighValue(DateColumnStatsDataInspector aggregateData, DateColumnStatsDataInspector newData) {
+  public void setHighValue(DateColumnStatsDataInspector aggregateData, DateColumnStatsDataInspector newData) {
     if (!aggregateData.isSetHighValue() && !newData.isSetHighValue()) {
       return;
     }

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DoubleColumnStatsMerger.java
Patch:
@@ -54,7 +54,7 @@ public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj new
     aggregateColStats.getStatsData().setDoubleStats(aggregateData);
   }
 
-  private void setLowValue(DoubleColumnStatsDataInspector aggregateData, DoubleColumnStatsDataInspector newData) {
+  public void setLowValue(DoubleColumnStatsDataInspector aggregateData, DoubleColumnStatsDataInspector newData) {
     if (!aggregateData.isSetLowValue() && !newData.isSetLowValue()) {
       return;
     }
@@ -64,13 +64,13 @@ private void setLowValue(DoubleColumnStatsDataInspector aggregateData, DoubleCol
     aggregateData.setLowValue(lowValue);
   }
 
-  private void setHighValue(DoubleColumnStatsDataInspector aggregateData, DoubleColumnStatsDataInspector newData) {
+  public void setHighValue(DoubleColumnStatsDataInspector aggregateData, DoubleColumnStatsDataInspector newData) {
     if (!aggregateData.isSetHighValue() && !newData.isSetHighValue()) {
       return;
     }
     double highValue = Math.max(
         aggregateData.isSetHighValue() ? aggregateData.getHighValue() : Double.MIN_VALUE,
         newData.isSetHighValue() ? newData.getHighValue() : Double.MIN_VALUE);
-    aggregateData.setLowValue(highValue);
+    aggregateData.setHighValue(highValue);
   }
 }

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/LongColumnStatsMerger.java
Patch:
@@ -54,7 +54,7 @@ public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj new
     aggregateColStats.getStatsData().setLongStats(aggregateData);
   }
 
-  private void setLowValue(LongColumnStatsDataInspector aggregateData, LongColumnStatsDataInspector newData) {
+  public void setLowValue(LongColumnStatsDataInspector aggregateData, LongColumnStatsDataInspector newData) {
     if (!aggregateData.isSetLowValue() && !newData.isSetLowValue()) {
       return;
     }
@@ -64,7 +64,7 @@ private void setLowValue(LongColumnStatsDataInspector aggregateData, LongColumnS
     aggregateData.setLowValue(lowValue);
   }
 
-  private void setHighValue(LongColumnStatsDataInspector aggregateData, LongColumnStatsDataInspector newData) {
+  public void setHighValue(LongColumnStatsDataInspector aggregateData, LongColumnStatsDataInspector newData) {
     if (!aggregateData.isSetHighValue() && !newData.isSetHighValue()) {
       return;
     }

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/StringColumnStatsMerger.java
Patch:
@@ -50,5 +50,7 @@ public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj new
           + aggregateData.getNumDVs() + " and " + newData.getNumDVs() + " to be " + ndv);
       aggregateData.setNumDVs(ndv);
     }
+
+    aggregateColStats.getStatsData().setStringStats(aggregateData);
   }
 }

File: itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java
Patch:
@@ -272,7 +272,6 @@ public static void connectToMetastore() throws Exception {
     conf.setVar(HiveConf.ConfVars.METASTORE_EVENT_DB_LISTENER_TTL, String.valueOf(EVENTS_TTL) + "s");
     conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     conf.setBoolVar(HiveConf.ConfVars.FIRE_EVENTS_FOR_DML, true);
-    conf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
     conf.setVar(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL, DummyRawStoreFailEvent.class.getName());
     MetastoreConf.setTimeVar(conf, MetastoreConf.ConfVars.EVENT_DB_LISTENER_CLEAN_INTERVAL, CLEANUP_SLEEP_TIME, TimeUnit.SECONDS);
     MetastoreConf.setVar(conf, MetastoreConf.ConfVars.EVENT_MESSAGE_FACTORY, JSONMessageEncoder.class.getName());

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestMetaStoreLimitPartitionRequest.java
Patch:
@@ -73,7 +73,6 @@ public static void beforeTest() throws Exception {
     conf.setBoolVar(HiveConf.ConfVars.METASTORE_INTEGER_JDO_PUSHDOWN, true);
     conf.setBoolVar(HiveConf.ConfVars.METASTORE_TRY_DIRECT_SQL, true);
     conf.setBoolVar(HiveConf.ConfVars.DYNAMICPARTITIONING, true);
-    conf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
     conf.setBoolVar(HiveConf.ConfVars.HIVE_CBO_ENABLED, false);
 
     miniHS2 = new MiniHS2.Builder().withConf(conf).build();

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationScenariosAcidTables.java
Patch:
@@ -78,7 +78,6 @@ static void internalBeforeClassSetup(Map<String, String> overrides, Class clazz)
       put("hive.txn.manager", "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
       put("hive.metastore.client.capability.check", "false");
       put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-      put("hive.exec.dynamic.partition.mode", "nonstrict");
       put("hive.strict.checks.bucketing", "false");
       put("hive.mapred.mode", "nonstrict");
       put("mapred.input.dir.recursive", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestMetaStoreEventListenerInRepl.java
Patch:
@@ -74,7 +74,6 @@ public static void internalBeforeClassSetup() throws Exception {
 	      put("hive.txn.manager", "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
 	      put("hive.metastore.client.capability.check", "false");
 	      put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-	      put("hive.exec.dynamic.partition.mode", "nonstrict");
 	      put("hive.strict.checks.bucketing", "false");
 	      put("hive.mapred.mode", "nonstrict");
 	      put("mapred.input.dir.recursive", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOfHiveStreaming.java
Patch:
@@ -80,7 +80,6 @@ static void internalBeforeClassSetup(Map<String, String> overrides,
         put("hive.txn.manager", "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
         put("hive.metastore.client.capability.check", "false");
         put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-        put("hive.exec.dynamic.partition.mode", "nonstrict");
         put("hive.strict.checks.bucketing", "false");
         put("hive.mapred.mode", "nonstrict");
         put("mapred.input.dir.recursive", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
Patch:
@@ -80,7 +80,6 @@ static void internalBeforeClassSetup(Map<String, String> overrides,
         put("hive.txn.manager", "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
         put("hive.metastore.client.capability.check", "false");
         put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-        put("hive.exec.dynamic.partition.mode", "nonstrict");
         put("hive.strict.checks.bucketing", "false");
         put("hive.mapred.mode", "nonstrict");
         put("mapred.input.dir.recursive", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosIncrementalLoadAcidTables.java
Patch:
@@ -76,7 +76,6 @@ static void internalBeforeClassSetup(Map<String, String> overrides, Class clazz)
         put("hive.txn.manager", "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
         put("hive.metastore.client.capability.check", "false");
         put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-        put("hive.exec.dynamic.partition.mode", "nonstrict");
         put("hive.strict.checks.bucketing", "false");
         put("hive.mapred.mode", "nonstrict");
         put("mapred.input.dir.recursive", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationWithTableMigration.java
Patch:
@@ -94,7 +94,6 @@ static void internalBeforeClassSetup(Map<String, String> overrideConfigs) throws
       put("hive.txn.manager", "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
       put("hive.metastore.client.capability.check", "false");
       put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-      put("hive.exec.dynamic.partition.mode", "nonstrict");
       put("hive.strict.checks.bucketing", "false");
       put("hive.mapred.mode", "nonstrict");
       put("mapred.input.dir.recursive", "true");
@@ -107,7 +106,6 @@ static void internalBeforeClassSetup(Map<String, String> overrideConfigs) throws
       put("fs.defaultFS", fs.getUri().toString());
       put("hive.metastore.client.capability.check", "false");
       put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-      put("hive.exec.dynamic.partition.mode", "nonstrict");
       put("hive.strict.checks.bucketing", "false");
       put("hive.mapred.mode", "nonstrict");
       put("mapred.input.dir.recursive", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationWithTableMigrationEx.java
Patch:
@@ -73,7 +73,6 @@ static void internalBeforeClassSetup(Map<String, String> overrideConfigs) throws
       put("hive.txn.manager", "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
       put("hive.metastore.client.capability.check", "false");
       put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-      put("hive.exec.dynamic.partition.mode", "nonstrict");
       put("hive.strict.checks.bucketing", "false");
       put("hive.mapred.mode", "nonstrict");
       put("mapred.input.dir.recursive", "true");
@@ -87,7 +86,6 @@ static void internalBeforeClassSetup(Map<String, String> overrideConfigs) throws
       put("fs.defaultFS", fs.getUri().toString());
       put("hive.metastore.client.capability.check", "false");
       put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-      put("hive.exec.dynamic.partition.mode", "nonstrict");
       put("hive.strict.checks.bucketing", "false");
       put("hive.mapred.mode", "nonstrict");
       put("mapred.input.dir.recursive", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenariosACID.java
Patch:
@@ -44,8 +44,6 @@ public static void classLevelSetup() throws Exception {
               "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
     overrides.put(MetastoreConf.ConfVars.CAPABILITY_CHECK.getHiveName(), "false");
     overrides.put(HiveConf.ConfVars.REPL_BOOTSTRAP_DUMP_OPEN_TXN_TIMEOUT.varname, "1s");
-    overrides.put(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE.varname, "nonstrict");
-
 
     internalBeforeClassSetup(overrides, overrides, TestStatsReplicationScenariosACID.class, true,
             AcidTableKind.FULL_ACID);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenariosACIDNoAutogather.java
Patch:
@@ -46,8 +46,6 @@ public static void classLevelSetup() throws Exception {
               "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
     overrides.put(MetastoreConf.ConfVars.CAPABILITY_CHECK.getHiveName(),"false");
     overrides.put(HiveConf.ConfVars.REPL_BOOTSTRAP_DUMP_OPEN_TXN_TIMEOUT.varname,"1s");
-    overrides.put(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE.varname, "nonstrict");
-
 
     internalBeforeClassSetup(overrides, overrides,
             TestStatsReplicationScenariosACIDNoAutogather.class, false, AcidTableKind.FULL_ACID);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenariosMM.java
Patch:
@@ -46,8 +46,6 @@ public static void classLevelSetup() throws Exception {
               "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
     overrides.put(MetastoreConf.ConfVars.CAPABILITY_CHECK.getHiveName(),"false");
     overrides.put(HiveConf.ConfVars.REPL_BOOTSTRAP_DUMP_OPEN_TXN_TIMEOUT.varname,"1s");
-    overrides.put(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE.varname, "nonstrict");
-
 
     internalBeforeClassSetup(overrides, overrides, TestStatsReplicationScenariosMM.class, true,
             AcidTableKind.INSERT_ONLY);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenariosMMNoAutogather.java
Patch:
@@ -44,7 +44,6 @@ public static void classLevelSetup() throws Exception {
               "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
     overrides.put(MetastoreConf.ConfVars.CAPABILITY_CHECK.getHiveName(), "false");
     overrides.put(HiveConf.ConfVars.REPL_BOOTSTRAP_DUMP_OPEN_TXN_TIMEOUT.varname, "1s");
-    overrides.put(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE.varname, "nonstrict");
     overrides.put("mapred.input.dir.recursive", "true");
 
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenariosMigration.java
Patch:
@@ -44,7 +44,6 @@ public static void classLevelSetup() throws Exception {
       put("hive.txn.manager", "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
       put("hive.metastore.client.capability.check", "false");
       put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-      put("hive.exec.dynamic.partition.mode", "nonstrict");
       put("hive.strict.checks.bucketing", "false");
       put("hive.mapred.mode", "nonstrict");
       put("mapred.input.dir.recursive", "true");
@@ -56,7 +55,6 @@ public static void classLevelSetup() throws Exception {
     Map<String, String> primaryConfigs = new HashMap<String, String>() {{
       put("hive.metastore.client.capability.check", "false");
       put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-      put("hive.exec.dynamic.partition.mode", "nonstrict");
       put("hive.strict.checks.bucketing", "false");
       put("hive.mapred.mode", "nonstrict");
       put("mapred.input.dir.recursive", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestStatsReplicationScenariosMigrationNoAutogather.java
Patch:
@@ -44,7 +44,6 @@ public static void classLevelSetup() throws Exception {
       put("hive.txn.manager", "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
       put("hive.metastore.client.capability.check", "false");
       put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-      put("hive.exec.dynamic.partition.mode", "nonstrict");
       put("hive.strict.checks.bucketing", "false");
       put("hive.mapred.mode", "nonstrict");
       put("mapred.input.dir.recursive", "true");
@@ -56,7 +55,6 @@ public static void classLevelSetup() throws Exception {
     Map<String, String> primaryConfigs = new HashMap<String, String>() {{
       put("hive.metastore.client.capability.check", "false");
       put("hive.repl.bootstrap.dump.open.txn.timeout", "1s");
-      put("hive.exec.dynamic.partition.mode", "nonstrict");
       put("hive.strict.checks.bucketing", "false");
       put("hive.mapred.mode", "nonstrict");
       put("mapred.input.dir.recursive", "true");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java
Patch:
@@ -141,7 +141,6 @@ public void setup() throws Exception {
     hiveConf.setVar(HiveConf.ConfVars.POSTEXECHOOKS, "");
     hiveConf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE, TEST_WAREHOUSE_DIR);
     hiveConf.setVar(HiveConf.ConfVars.HIVEINPUTFORMAT, HiveInputFormat.class.getName());
-    hiveConf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
 
     TxnDbUtil.setConfValues(hiveConf);
     TxnDbUtil.cleanDb(hiveConf);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCrudCompactorOnTez.java
Patch:
@@ -88,7 +88,6 @@ public void setup() throws Exception {
     hiveConf.setVar(HiveConf.ConfVars.POSTEXECHOOKS, "");
     hiveConf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE, TEST_WAREHOUSE_DIR);
     hiveConf.setVar(HiveConf.ConfVars.HIVEINPUTFORMAT, HiveInputFormat.class.getName());
-    hiveConf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
     hiveConf.setVar(HiveConf.ConfVars.HIVEFETCHTASKCONVERSION, "none");
     TxnDbUtil.setConfValues(hiveConf);
     TxnDbUtil.cleanDb(hiveConf);

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/AbstractJdbcTriggersTest.java
Patch:
@@ -212,7 +212,6 @@ void runQueryWithTrigger(final String query, final List<String> setCmds,
 
   List<String> getConfigs(String... more) {
     List<String> setCmds = new ArrayList<>();
-    setCmds.add("set hive.exec.dynamic.partition.mode=nonstrict");
     setCmds.add("set mapred.min.split.size=200");
     setCmds.add("set mapred.max.split.size=200");
     setCmds.add("set tez.grouping.min-size=200");
@@ -229,4 +228,4 @@ WMTrigger wmTriggerFromTrigger(Trigger trigger) {
     result.setActionExpression(trigger.getAction().toString());
     return result;
   }
-}
\ No newline at end of file
+}

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/AbstractTestJdbcGenericUDTFGetSplits.java
Patch:
@@ -142,7 +142,6 @@ protected void runQuery(final String query, final List<String> setCmds,
 
   protected List<String> getConfigs(String... more) {
     List<String> setCmds = new ArrayList<>();
-    setCmds.add("set hive.exec.dynamic.partition.mode=nonstrict");
     setCmds.add("set mapred.min.split.size=10");
     setCmds.add("set mapred.max.split.size=10");
     setCmds.add("set tez.grouping.min-size=10");

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcGenericUDTFGetSplits.java
Patch:
@@ -78,4 +78,4 @@ public void testDecimalPrecisionAndScale() throws Exception {
     }
   }
 
-}
\ No newline at end of file
+}

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommandsForMmTable.java
Patch:
@@ -119,7 +119,6 @@ public void setUp() throws Exception {
   
   void setUpInternalExtended(boolean isOrcFormat) throws Exception {
     hiveConf.setBoolVar(HiveConf.ConfVars.DYNAMICPARTITIONING, true);
-    hiveConf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
     hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     hiveConf.setVar(HiveConf.ConfVars.HIVEFETCHTASKCONVERSION, "none");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "true");

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java
Patch:
@@ -199,7 +199,6 @@ public void testNoBuckets() throws Exception {
 
   @Test
   public void testNoBucketsDP() throws Exception {
-    hiveConf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
     int[][] sourceVals1 = {{0,0,0},{3,3,3}};
     int[][] sourceVals2 = {{1,1,1},{2,2,2}};
     int[][] sourceVals3 = {{3,3,3},{4,4,4}};
@@ -768,7 +767,6 @@ private void checkExpected(List<String> rs, String[][] expected, String msg) {
    */
   @Test
   public void testCompactStatsGather() throws Exception {
-    hiveConf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
     hiveConf.setIntVar(HiveConf.ConfVars.HIVEOPTSORTDYNAMICPARTITIONTHRESHOLD, -1);
     runStatementOnDriver("drop table if exists T");
     runStatementOnDriver("create table T(a int, b int) partitioned by (p int, q int) " +

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java
Patch:
@@ -98,7 +98,6 @@ public TestDbTxnManager2() throws Exception {
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     conf.setBoolVar(HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, false);
     TxnDbUtil.setConfValues(conf);
-    conf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
   }
   @Before
   public void setUp() throws Exception {

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestUpdateDeleteSemanticAnalyzer.java
Patch:
@@ -228,7 +228,6 @@ public void setup() {
     conf
     .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
-    conf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
     conf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
     conf.setBoolVar(HiveConf.ConfVars.HIVE_IN_TEST, true);

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateDatabaseHook.java
Patch:
@@ -79,7 +79,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)
 
   @Override
   public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-              List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+              List<Task<?>> rootTasks) throws SemanticException {
     context.getConf().set(HCatConstants.HCAT_CREATE_DB_NAME, databaseName);
     super.postAnalyze(context, rootTasks);
   }

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java
Patch:
@@ -132,7 +132,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,
 
   @Override
   public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-              List<Task<? extends Serializable>> rootTasks)
+              List<Task<?>> rootTasks)
     throws SemanticException {
 
     if (rootTasks.size() == 0) {

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java
Patch:
@@ -152,7 +152,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)
 
   @Override
   public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-              List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+              List<Task<?>> rootTasks) throws SemanticException {
 
     try {
 

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzerBase.java
Patch:
@@ -55,7 +55,7 @@ public HiveAuthorizationProvider getAuthProvider() {
 
   @Override
   public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-              List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+              List<Task<?>> rootTasks) throws SemanticException {
     super.postAnalyze(context, rootTasks);
 
     //Authorize the operation.
@@ -86,7 +86,7 @@ public void postAnalyze(HiveSemanticAnalyzerHookContext context,
   * @see https://issues.apache.org/jira/browse/HCATALOG-245
   */
   protected void authorizeDDL(HiveSemanticAnalyzerHookContext context,
-                List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+                List<Task<?>> rootTasks) throws SemanticException {
 
     if (!HCatAuthUtil.isAuthorizationEnabled(context.getConf())) {
       return;
@@ -96,7 +96,7 @@ protected void authorizeDDL(HiveSemanticAnalyzerHookContext context,
     try {
       hive = context.getHive();
 
-      for (Task<? extends Serializable> task : rootTasks) {
+      for (Task<?> task : rootTasks) {
         if (task.getWork() instanceof DDLWork) {
           DDLWork work = (DDLWork) task.getWork();
           if (work != null) {

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/hooks/TestHs2Hooks.java
Patch:
@@ -121,7 +121,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,
 
     @Override
     public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-        List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+        List<Task<?>> rootTasks) throws SemanticException {
       try {
         userName = context.getUserName();
         ipAddress = context.getIpAddress();

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
Patch:
@@ -356,11 +356,11 @@ public boolean hasTask(Task rootTask) {
       if (validate(rootTask)) {
         return true;
       }
-      List<Task<? extends Serializable>> childTasks = rootTask.getChildTasks();
+      List<Task<?>> childTasks = rootTask.getChildTasks();
       if (childTasks == null) {
         return false;
       }
-      for (Task<? extends Serializable> childTask : childTasks) {
+      for (Task<?> childTask : childTasks) {
         if (hasTask(childTask)) {
           return true;
         }

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/TestHs2Metrics.java
Patch:
@@ -67,7 +67,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,
 
     @Override
     public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-      List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+      List<Task<?>> rootTasks) throws SemanticException {
     }
   }
 

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyHiveSortedInputFormatUsedHook.java
Patch:
@@ -33,8 +33,8 @@ public void run(HookContext hookContext) {
 
       // Go through the root tasks, and verify the input format of the map reduce task(s) is
       // HiveSortedInputFormat
-      List<Task<? extends Serializable>> rootTasks = hookContext.getQueryPlan().getRootTasks();
-      for (Task<? extends Serializable> rootTask : rootTasks) {
+      List<Task<?>> rootTasks = hookContext.getQueryPlan().getRootTasks();
+      for (Task<?> rootTask : rootTasks) {
         if (rootTask.getWork() instanceof MapredWork) {
           Assert.assertTrue("The root map reduce task's input was not marked as sorted.",
               ((MapredWork)rootTask.getWork()).getMapWork().isInputFormatSorted());

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyHooksRunInOrder.java
Patch:
@@ -110,7 +110,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,ASTNode ast)
 
     @Override
     public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-        List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+        List<Task<?>> rootTasks) throws SemanticException {
       LogHelper console = SessionState.getConsole();
 
       if (console == null) {
@@ -145,7 +145,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,ASTNode ast)
 
     @Override
     public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-        List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+        List<Task<?>> rootTasks) throws SemanticException {
       LogHelper console = SessionState.getConsole();
 
       if (console == null) {

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook.java
Patch:
@@ -61,7 +61,7 @@ public DummySemanticAnalyzerHook() {
 
   @Override
   public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-      List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+      List<Task<?>> rootTasks) throws SemanticException {
 
     if(hook != null) {
       hook.postAnalyze(context, rootTasks);
@@ -91,7 +91,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)
 
   @Override
   public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-      List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+      List<Task<?>> rootTasks) throws SemanticException {
     CreateTableDesc desc = (CreateTableDesc) ((DDLTask)rootTasks.get(rootTasks.size()-1)).getWork().getDDLDesc();
     Map<String,String> tblProps = desc.getTblProps();
     if(tblProps == null) {

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/metadata/DummySemanticAnalyzerHook1.java
Patch:
@@ -56,7 +56,7 @@ public DummySemanticAnalyzerHook1() {
 
   @Override
   public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-      List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+      List<Task<?>> rootTasks) throws SemanticException {
     count = 0;
     if (!isCreateTable) {
       return;

File: ql/src/java/org/apache/hadoop/hive/ql/HookRunner.java
Patch:
@@ -228,7 +228,7 @@ public boolean hasPreAnalyzeHooks() {
   }
 
   public void runPostAnalyzeHooks(HiveSemanticAnalyzerHookContext hookCtx,
-      List<Task<? extends Serializable>> allRootTasks) throws HiveException {
+      List<Task<?>> allRootTasks) throws HiveException {
     initialize();
     try {
       for (HiveSemanticAnalyzerHook hook : saHooks) {

File: ql/src/java/org/apache/hadoop/hive/ql/QueryDisplay.java
Patch:
@@ -55,14 +55,14 @@ public class QueryDisplay {
 
   private final LinkedHashMap<String, TaskDisplay> tasks = new LinkedHashMap<String, TaskDisplay>();
 
-  public synchronized <T extends Serializable> void updateTaskStatus(Task<T> tTask) {
+  public void updateTaskStatus(Task<?> tTask) {
     if (!tasks.containsKey(tTask.getId())) {
       tasks.put(tTask.getId(), new TaskDisplay(tTask));
     }
     tasks.get(tTask.getId()).updateStatus(tTask);
   }
 
-  public synchronized <T extends Serializable> void updateTaskStatistics(MapRedStats mapRedStats,
+  public synchronized void updateTaskStatistics(MapRedStats mapRedStats,
       RunningJob rj, String taskId) throws IOException, JSONException {
     if (tasks.containsKey(taskId)) {
       tasks.get(taskId).updateMapRedStatsJson(mapRedStats, rj);
@@ -232,7 +232,7 @@ public synchronized String getExternalHandle() {
       return externalHandle;
     }
 
-    public synchronized <T extends Serializable> void updateStatus(Task<T> tTask) {
+    public void updateStatus(Task<?> tTask) {
       this.taskState = tTask.getTaskState();
       if (externalHandle == null && tTask.getExternalHandle() != null) {
         this.externalHandle = tTask.getExternalHandle();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/NodeUtils.java
Patch:
@@ -34,7 +34,7 @@
 public class NodeUtils {
 
 
-  public static <T> void iterateTask(Collection<Task<? extends Serializable>> tasks, Class<T> clazz, Function<T> function) {
+  public static <T> void iterateTask(Collection<Task<?>> tasks, Class<T> clazz, Function<T> function) {
     // Does a breadth first traversal of the tasks
     Set<Task> visited = new HashSet<Task>();
     while (!tasks.isEmpty()) {
@@ -43,7 +43,7 @@ public static <T> void iterateTask(Collection<Task<? extends Serializable>> task
     return;
   }
 
-  private static <T> Collection<Task<? extends Serializable>> iterateTask(Collection<Task<?>> tasks,
+  private static <T> Collection<Task<?>> iterateTask(Collection<Task<?>> tasks,
                                                      Class<T> clazz,
                                                      Function<T> function,
                                                      Set<Task> visited) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TaskRunner.java
Patch:
@@ -33,7 +33,7 @@
  **/
 
 public class TaskRunner extends Thread {
-  protected Task<? extends Serializable> tsk;
+  protected Task<?> tsk;
   protected TaskResult result;
   protected SessionState ss;
   private static AtomicLong taskCounter = new AtomicLong(0);
@@ -50,14 +50,14 @@ protected Long initialValue() {
 
   private final DriverContext driverCtx;
 
-  public TaskRunner(Task<? extends Serializable> tsk, DriverContext ctx) {
+  public TaskRunner(Task<?> tsk, DriverContext ctx) {
     this.tsk = tsk;
     this.result = new TaskResult();
     ss = SessionState.get();
     driverCtx = ctx;
   }
 
-  public Task<? extends Serializable> getTask() {
+  public Task<?> getTask() {
     return tsk;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java
Patch:
@@ -64,7 +64,7 @@ public class HadoopJobExecHelper {
   static final private org.slf4j.Logger LOG = LoggerFactory.getLogger(HadoopJobExecHelper.class.getName());
 
   protected transient JobConf job;
-  protected Task<? extends Serializable> task;
+  protected Task<?> task;
 
   protected transient int mapProgress = -1;
   protected transient int reduceProgress = -1;
@@ -142,7 +142,7 @@ public void setJobId(JobID jobId) {
   }
 
   public HadoopJobExecHelper(JobConf job, LogHelper console,
-      Task<? extends Serializable> task, HadoopJobExecHook hookCallBack) {
+      Task<?> task, HadoopJobExecHook hookCallBack) {
     this.queryId = HiveConf.getVar(job, HiveConf.ConfVars.HIVEQUERYID, "unknown-" + System.currentTimeMillis());
     this.job = job;
     this.console = console;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ExternalTableCopyTaskBuilder.java
Patch:
@@ -56,8 +56,8 @@ public class ExternalTableCopyTaskBuilder {
     this.conf = conf;
   }
 
-  List<Task<? extends Serializable>> tasks(TaskTracker tracker) {
-    List<Task<? extends Serializable>> tasks = new ArrayList<>();
+  List<Task<?>> tasks(TaskTracker tracker) {
+    List<Task<?>> tasks = new ArrayList<>();
     Iterator<DirCopyWork> itr = work.getPathsToCopyIterator();
     while (tracker.canAddMoreTasks() && itr.hasNext()) {
       DirCopyWork dirCopyWork = itr.next();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadWork.java
Patch:
@@ -53,7 +53,7 @@ public class ReplLoadWork implements Serializable {
   private DatabaseEvent.State state = null;
   private final transient BootstrapEventsIterator bootstrapIterator;
   private transient IncrementalLoadTasksBuilder incrementalLoadTasksBuilder;
-  private transient Task<? extends Serializable> rootTask;
+  private transient Task<?> rootTask;
   private final transient Iterator<DirCopyWork> pathsToCopyIterator;
 
   /*
@@ -143,11 +143,11 @@ IncrementalLoadTasksBuilder incrementalLoadTasksBuilder() {
     return incrementalLoadTasksBuilder;
   }
 
-  public Task<? extends Serializable> getRootTask() {
+  public Task<?> getRootTask() {
     return rootTask;
   }
 
-  public void setRootTask(Task<? extends Serializable> rootTask) {
+  public void setRootTask(Task<?> rootTask) {
     this.rootTask = rootTask;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadConstraint.java
Patch:
@@ -85,7 +85,7 @@ public TaskTracker tasks() throws IOException, SemanticException {
       String fksString = json.getString("fks");
       String uksString = json.getString("uks");
       String nnsString = json.getString("nns");
-      List<Task<? extends Serializable>> tasks = new ArrayList<Task<? extends Serializable>>();
+      List<Task<?>> tasks = new ArrayList<Task<?>>();
 
       if (pksString != null && !pksString.isEmpty() && !isPrimaryKeysAlreadyLoaded(pksString)) {
         AddPrimaryKeyHandler pkHandler = new AddPrimaryKeyHandler();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadFunction.java
Patch:
@@ -65,7 +65,7 @@ public LoadFunction(Context context, ReplLogger replLogger, FunctionEvent event,
     this.tracker = new TaskTracker(existingTracker);
   }
 
-  private void createFunctionReplLogTask(List<Task<? extends Serializable>> functionTasks,
+  private void createFunctionReplLogTask(List<Task<?>> functionTasks,
                                          String functionName) {
     ReplStateLogWork replLogWork = new ReplStateLogWork(replLogger, functionName);
     Task<ReplStateLogWork> replLogTask = TaskFactory.get(replLogWork, context.hiveConf);
@@ -82,7 +82,7 @@ public TaskTracker tasks() throws IOException, SemanticException {
         return tracker;
       }
       CreateFunctionHandler handler = new CreateFunctionHandler();
-      List<Task<? extends Serializable>> tasks = handler.handle(
+      List<Task<?>> tasks = handler.handle(
           new MessageHandler.Context(
               dbNameToLoadIn, fromPath.toString(), null, null, context.hiveConf,
               context.hiveDb, context.nestedContext, LOG)

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/table/LoadPartitions.java
Patch:
@@ -117,7 +117,7 @@ public TaskTracker tasks() throws Exception {
         updateReplicationState(initialReplicationState());
         if (!forNewTable().hasReplicationState()) {
           // Add ReplStateLogTask only if no pending table load tasks left for next cycle
-          Task<? extends Serializable> replLogTask
+          Task<?> replLogTask
                   = ReplUtils.getTableReplLogTask(tableDesc, replLogger, context.hiveConf);
           tracker.addDependentTask(replLogTask);
         }
@@ -131,7 +131,7 @@ public TaskTracker tasks() throws Exception {
           updateReplicationState(initialReplicationState());
           if (!forExistingTable(lastReplicatedPartition).hasReplicationState()) {
             // Add ReplStateLogTask only if no pending table load tasks left for next cycle
-            Task<? extends Serializable> replLogTask
+            Task<?> replLogTask
                     = ReplUtils.getTableReplLogTask(tableDesc, replLogger, context.hiveConf);
             tracker.addDependentTask(replLogTask);
           }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/table/LoadTable.java
Patch:
@@ -151,7 +151,7 @@ public TaskTracker tasks(boolean isBootstrapDuringInc) throws Exception {
             context.hiveConf
     );
     if (!isPartitioned(tableDesc)) {
-      Task<? extends Serializable> replLogTask
+      Task<?> replLogTask
               = ReplUtils.getTableReplLogTask(tableDesc, replLogger, context.hiveConf);
       ckptTask.addDependentTask(replLogTask);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java
Patch:
@@ -172,23 +172,23 @@ public void setTaskProperty(String queryId, String taskId, Keys propName,
    *
    * @param task
    */
-  public void startTask(String queryId, Task<? extends Serializable> task,
+  public void startTask(String queryId, Task<?> task,
       String taskName);
 
   /**
    * Called at the end of a task.
    *
    * @param task
    */
-  public void endTask(String queryId, Task<? extends Serializable> task);
+  public void endTask(String queryId, Task<?> task);
 
   /**
    * Logs progress of a task if ConfVars.HIVE_LOG_INCREMENTAL_PLAN_PROGRESS is
    * set to true
    *
    * @param task
    */
-  public void progressTask(String queryId, Task<? extends Serializable> task);
+  public void progressTask(String queryId, Task<?> task);
 
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryImpl.java
Patch:
@@ -263,7 +263,7 @@ public void endQuery(String queryId) {
   }
 
   @Override
-  public void startTask(String queryId, Task<? extends Serializable> task,
+  public void startTask(String queryId, Task<?> task,
       String taskName) {
     TaskInfo ti = new TaskInfo();
 
@@ -279,7 +279,7 @@ public void startTask(String queryId, Task<? extends Serializable> task,
   }
 
   @Override
-  public void endTask(String queryId, Task<? extends Serializable> task) {
+  public void endTask(String queryId, Task<?> task) {
     String id = queryId + ":" + task.getId();
     TaskInfo ti = taskInfoMap.get(id);
 
@@ -291,7 +291,7 @@ public void endTask(String queryId, Task<? extends Serializable> task) {
   }
 
   @Override
-  public void progressTask(String queryId, Task<? extends Serializable> task) {
+  public void progressTask(String queryId, Task<?> task) {
     String id = queryId + ":" + task.getId();
     TaskInfo ti = taskInfoMap.get(id);
     if (ti == null) {

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/NoOperatorReuseCheckerHook.java
Patch:
@@ -69,8 +69,8 @@ public void run(HookContext hookContext) throws Exception {
 
     List<Node> rootOps = Lists.newArrayList();
 
-    List<Task<? extends Serializable>> roots = hookContext.getQueryPlan().getRootTasks();
-    for (Task<? extends Serializable> task : roots) {
+    List<Task<?>> roots = hookContext.getQueryPlan().getRootTasks();
+    for (Task<?> task : roots) {
 
       Object work = task.getWork();
       if (work instanceof MapredWork) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java
Patch:
@@ -61,7 +61,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
     Map<Operator<? extends OperatorDesc>, GenMapRedCtx> mapCurrCtx = ctx
         .getMapCurrCtx();
     GenMapRedCtx mapredCtx = mapCurrCtx.get(stack.get(stack.size() - 2));
-    Task<? extends Serializable> currTask = mapredCtx.getCurrTask();
+    Task<?> currTask = mapredCtx.getCurrTask();
     MapredWork currPlan = (MapredWork) currTask.getWork();
     String currAliasId = mapredCtx.getCurrAliasId();
 
@@ -70,7 +70,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
           "But found multiple children : " + op.getChildOperators());
     }
     Operator<? extends OperatorDesc> reducer = op.getChildOperators().get(0);
-    Task<? extends Serializable> oldTask = ctx.getOpTaskMap().get(reducer);
+    Task<?> oldTask = ctx.getOpTaskMap().get(reducer);
 
     ctx.setCurrAliasId(currAliasId);
     ctx.setCurrTask(currTask);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java
Patch:
@@ -56,12 +56,12 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
     Map<Operator<? extends OperatorDesc>, GenMapRedCtx> mapCurrCtx = ctx
         .getMapCurrCtx();
     GenMapRedCtx mapredCtx = mapCurrCtx.get(op.getParentOperators().get(0));
-    Task<? extends Serializable> currTask = mapredCtx.getCurrTask();
+    Task<?> currTask = mapredCtx.getCurrTask();
     String currAliasId = mapredCtx.getCurrAliasId();
     Operator<? extends OperatorDesc> reducer = op.getChildOperators().get(0);
-    Map<Operator<? extends OperatorDesc>, Task<? extends Serializable>> opTaskMap = ctx
+    Map<Operator<? extends OperatorDesc>, Task<?>> opTaskMap = ctx
         .getOpTaskMap();
-    Task<? extends Serializable> oldTask = opTaskMap.get(reducer);
+    Task<?> oldTask = opTaskMap.get(reducer);
 
     ctx.setCurrAliasId(currAliasId);
     ctx.setCurrTask(currTask);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java
Patch:
@@ -67,7 +67,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
         .getMapCurrCtx();
     GenMapRedCtx mapredCtx = mapCurrCtx.get(union);
 
-    Task<? extends Serializable> unionTask = null;
+    Task<?> unionTask = null;
     if(mapredCtx != null) {
       unionTask = mapredCtx.getCurrTask();
     } else {
@@ -76,9 +76,9 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
 
 
     MapredWork plan = (MapredWork) unionTask.getWork();
-    HashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>> opTaskMap = ctx
+    HashMap<Operator<? extends OperatorDesc>, Task<?>> opTaskMap = ctx
         .getOpTaskMap();
-    Task<? extends Serializable> reducerTask = opTaskMap.get(reducer);
+    Task<?> reducerTask = opTaskMap.get(reducer);
 
     ctx.setCurrTask(unionTask);
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/AnnotateRunTimeStatsOptimizer.java
Patch:
@@ -68,7 +68,7 @@ public AnnotateRunTimeStatsDispatcher(PhysicalContext context, Map<Rule, NodePro
     @Override
     public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
         throws SemanticException {
-      Task<? extends Serializable> currTask = (Task<? extends Serializable>) nd;
+      Task<?> currTask = (Task<?>) nd;
       Set<Operator<? extends OperatorDesc>> ops = new HashSet<>();
 
       if (currTask instanceof MapRedTask) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CrossProductHandler.java
Patch:
@@ -101,16 +101,16 @@ public PhysicalContext resolve(PhysicalContext pctx) throws SemanticException {
   public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
       throws SemanticException {
     @SuppressWarnings("unchecked")
-    Task<? extends Serializable> currTask = (Task<? extends Serializable>) nd;
+    Task<?> currTask = (Task<?>) nd;
     if (currTask instanceof MapRedTask) {
       MapRedTask mrTsk = (MapRedTask)currTask;
       MapredWork mrWrk = mrTsk.getWork();
       checkMapJoins(mrTsk);
       checkMRReducer(currTask.toString(), mrWrk);
     } else if (currTask instanceof ConditionalTask ) {
-      List<Task<? extends Serializable>> taskListInConditionalTask =
+      List<Task<?>> taskListInConditionalTask =
           ((ConditionalTask) currTask).getListTasks();
-      for(Task<? extends Serializable> tsk: taskListInConditionalTask){
+      for(Task<?> tsk: taskListInConditionalTask){
         dispatch(tsk, stack, nodeOutputs);
       }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LlapDecider.java
Patch:
@@ -139,7 +139,7 @@ public LlapDecisionDispatcher(PhysicalContext pctx, LlapMode mode) {
     public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
       throws SemanticException {
       @SuppressWarnings("unchecked")
-      Task<? extends Serializable> currTask = (Task<? extends Serializable>) nd;
+      Task<?> currTask = (Task<?>) nd;
       if (currTask instanceof TezTask) {
         TezWork work = ((TezTask) currTask).getWork();
         for (BaseWork w: work.getAllWork()) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LlapPreVectorizationPass.java
Patch:
@@ -82,7 +82,7 @@ class LlapPreVectorizationPassDispatcher implements Dispatcher {
     public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
         throws SemanticException {
       @SuppressWarnings("unchecked")
-      Task<? extends Serializable> currTask = (Task<? extends Serializable>) nd;
+      Task<?> currTask = (Task<?>) nd;
       if (currTask instanceof TezTask) {
         TezWork work = ((TezTask) currTask).getWork();
         for (BaseWork w: work.getAllWork()) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/MemoryDecider.java
Patch:
@@ -84,7 +84,7 @@ public MemoryCalculator(PhysicalContext pctx) {
     @Override
     public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
       throws SemanticException {
-      Task<? extends Serializable> currTask = (Task<? extends Serializable>) nd;
+      Task<?> currTask = (Task<?>) nd;
       if (currTask instanceof StatsTask) {
         currTask = ((StatsTask) currTask).getWork().getSourceTask();
       }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java
Patch:
@@ -168,7 +168,7 @@ private String encode(Map<String, String> partSpec) {
   @Override
   public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
       throws SemanticException {
-    Task<? extends Serializable> task = (Task<? extends Serializable>) nd;
+    Task<?> task = (Task<?>) nd;
 
     // create a the context for walking operators
     ParseContext parseContext = physicalContext.getParseContext();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SerializeFilter.java
Patch:
@@ -68,7 +68,7 @@ public Serializer(PhysicalContext pctx) {
     @Override
     public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
       throws SemanticException {
-      Task<? extends Serializable> currTask = (Task<? extends Serializable>) nd;
+      Task<?> currTask = (Task<?>) nd;
       if (currTask instanceof StatsTask) {
         currTask = ((StatsTask) currTask).getWork().getSourceTask();
       }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SkewJoinProcFactory.java
Patch:
@@ -56,7 +56,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,
         return null;
       }
       ParseContext parseContext = context.getParseCtx();
-      Task<? extends Serializable> currentTsk = context.getCurrentTask();
+      Task<?> currentTsk = context.getCurrentTask();
       GenMRSkewJoinProcessor.processSkewJoin(op, currentTsk, parseContext);
       return null;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SparkCrossProductCheck.java
Patch:
@@ -59,14 +59,14 @@ public class SparkCrossProductCheck implements PhysicalPlanResolver, Dispatcher
   public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
       throws SemanticException {
     @SuppressWarnings("unchecked")
-    Task<? extends Serializable> currTask = (Task<? extends Serializable>) nd;
+    Task<?> currTask = (Task<?>) nd;
     if (currTask instanceof SparkTask) {
       SparkWork sparkWork = ((SparkTask) currTask).getWork();
       checkShuffleJoin(sparkWork);
       checkMapJoin((SparkTask) currTask);
     } else if (currTask instanceof ConditionalTask) {
-      List<Task<? extends Serializable>> taskList = ((ConditionalTask) currTask).getListTasks();
-      for (Task<? extends Serializable> task : taskList) {
+      List<Task<?>> taskList = ((ConditionalTask) currTask).getListTasks();
+      for (Task<?> task : taskList) {
         dispatch(task, stack, nodeOutputs);
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SparkDynamicPartitionPruningResolver.java
Patch:
@@ -81,7 +81,7 @@ private class SparkDynamicPartitionPruningDispatcher implements Dispatcher {
 
     @Override
     public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs) throws SemanticException {
-      Task<? extends Serializable> task = (Task<? extends Serializable>) nd;
+      Task<?> task = (Task<?>) nd;
 
       // If the given Task is a SparkTask then search its Work DAG for SparkPartitionPruningSinkOperator
       if (task instanceof SparkTask) {
@@ -124,12 +124,12 @@ public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs) throws
    * Recursively go through the children of the given {@link Task} and check if any child {@link SparkTask} contains
    * the specified {@link MapWork} object.
    */
-  private boolean taskContainsDependentMapWork(Task<? extends Serializable> task,
+  private boolean taskContainsDependentMapWork(Task<?> task,
                                                MapWork work) throws SemanticException {
     if (task == null || task.getChildTasks() == null) {
       return false;
     }
-    for (Task<? extends Serializable> childTask : task.getChildTasks()) {
+    for (Task<?> childTask : task.getChildTasks()) {
       if (childTask != null && childTask instanceof SparkTask && childTask.getMapWork().contains(work)) {
         return true;
       } else if (taskContainsDependentMapWork(childTask, work)) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/StageIDsRearranger.java
Patch:
@@ -70,14 +70,14 @@ protected void accepted(Task<?> task) {
         }
       }
     };
-    for (Task<? extends Serializable> task : tasks) {
+    for (Task<?> task : tasks) {
       traverse.traverse(task);
     }
     return sources;
   }
 
   public static List<Task> getExplainOrder(HiveConf conf, List<Task<?>> tasks) {
-    for (Task<? extends Serializable> task : tasks) {
+    for (Task<?> task : tasks) {
       task.setRootTask(true);
     }
     String var = conf.getVar(HiveConf.ConfVars.HIVESTAGEIDREARRANGE);
@@ -122,7 +122,7 @@ protected boolean isReady(Task<?> task) {
         return type == ArrangeType.NONE || type == ArrangeType.IDONLY || super.isReady(task);
       }
     };
-    for (Task<? extends Serializable> task : tasks) {
+    for (Task<?> task : tasks) {
       traverse.traverse(task);
     }
     return new ArrayList<Task>(traverse.traversed);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -989,7 +989,7 @@ class VectorizationDispatcher implements Dispatcher {
     @Override
     public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
         throws SemanticException {
-      Task<? extends Serializable> currTask = (Task<? extends Serializable>) nd;
+      Task<?> currTask = (Task<?>) nd;
       if (currTask instanceof MapRedTask) {
         MapredWork mapredWork = ((MapRedTask) currTask).getWork();
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkSkewJoinResolver.java
Patch:
@@ -77,7 +77,7 @@ public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
         throws SemanticException {
 
       @SuppressWarnings("unchecked")
-      Task<? extends Serializable> task = (Task<? extends Serializable>) nd;
+      Task<?> task = (Task<?>) nd;
       if (task instanceof SparkTask) {
         SparkWork sparkWork = ((SparkTask) task).getWork();
         SparkSkewJoinProcCtx skewJoinProcCtx =
@@ -114,7 +114,7 @@ public static class SparkSkewJoinProcCtx extends SkewJoinResolver.SkewJoinProcCt
     // need a map from the reducer to the corresponding ReduceWork
     private Map<Operator<?>, ReduceWork> reducerToReduceWork;
 
-    public SparkSkewJoinProcCtx(Task<? extends Serializable> task,
+    public SparkSkewJoinProcCtx(Task<?> task,
                                 ParseContext parseCtx) {
       super(task, parseCtx);
       reducerToReduceWork = new HashMap<Operator<?>, ReduceWork>();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SplitSparkWorkResolver.java
Patch:
@@ -54,7 +54,7 @@
 public class SplitSparkWorkResolver implements PhysicalPlanResolver {
   @Override
   public PhysicalContext resolve(PhysicalContext pctx) throws SemanticException {
-    for (Task<? extends Serializable> task : pctx.getRootTasks()) {
+    for (Task<?> task : pctx.getRootTasks()) {
       if (task instanceof SparkTask) {
         splitSparkWork(((SparkTask) task).getWork());
       }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/AbstractSemanticAnalyzerHook.java
Patch:
@@ -32,6 +32,6 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,ASTNode ast)
   }
 
   public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-      List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+      List<Task<?>> rootTasks) throws SemanticException {
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -120,7 +120,7 @@ public abstract class BaseSemanticAnalyzer {
   protected final Hive db;
   protected final HiveConf conf;
   protected final QueryState queryState;
-  protected List<Task<? extends Serializable>> rootTasks;
+  protected List<Task<?>> rootTasks;
   protected FetchTask fetchTask;
   protected final Logger LOG;
   protected final LogHelper console;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java
Patch:
@@ -68,7 +68,7 @@ public class GenTezProcContext implements NodeProcessorCtx{
   public final List<Task<MoveWork>> moveTask;
 
   // rootTasks is the entry point for all generated tasks
-  public final List<Task<? extends Serializable>> rootTasks;
+  public final List<Task<?>> rootTasks;
 
   public final Set<ReadEntity> inputs;
   public final Set<WriteEntity> outputs;
@@ -164,7 +164,7 @@ public class GenTezProcContext implements NodeProcessorCtx{
 
   @SuppressWarnings("unchecked")
   public GenTezProcContext(HiveConf conf, ParseContext parseContext,
-      List<Task<MoveWork>> moveTask, List<Task<? extends Serializable>> rootTasks,
+      List<Task<MoveWork>> moveTask, List<Task<?>> rootTasks,
       Set<ReadEntity> inputs, Set<WriteEntity> outputs) {
 
     this.conf = conf;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/HiveSemanticAnalyzerHook.java
Patch:
@@ -72,5 +72,5 @@ public ASTNode preAnalyze(
    */
   public void postAnalyze(
     HiveSemanticAnalyzerHookContext context,
-    List<Task<? extends Serializable>> rootTasks) throws SemanticException;
+    List<Task<?>> rootTasks) throws SemanticException;
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
Patch:
@@ -548,7 +548,7 @@ private static Task<?> dropTableTask(Table table, EximUtil.SemanticAnalyzerWrapp
     return TaskFactory.get(new DDLWork(x.getInputs(), x.getOutputs(), dropTblDesc), x.getConf());
   }
 
-  private static Task<? extends Serializable> alterTableTask(ImportTableDesc tableDesc,
+  private static Task<?> alterTableTask(ImportTableDesc tableDesc,
                                                              EximUtil.SemanticAnalyzerWrapperContext x,
                                                              ReplicationSpec replicationSpec) {
     tableDesc.setReplaceMode(true);
@@ -558,7 +558,7 @@ private static Task<? extends Serializable> alterTableTask(ImportTableDesc table
     return tableDesc.getCreateTableTask(x.getInputs(), x.getOutputs(), x.getConf());
   }
 
-  private static Task<? extends Serializable> alterSinglePartition(
+  private static Task<?> alterSinglePartition(
           ImportTableDesc tblDesc, Table table, Warehouse wh, AlterTableAddPartitionDesc addPartitionDesc,
           ReplicationSpec replicationSpec, org.apache.hadoop.hive.ql.metadata.Partition ptn,
           EximUtil.SemanticAnalyzerWrapperContext x) throws MetaException, IOException, HiveException {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AbortTxnHandler.java
Patch:
@@ -34,7 +34,7 @@
  */
 public class AbortTxnHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     if (!AcidUtils.isAcidEnabled(context.hiveConf)) {
       context.log.error("Cannot load transaction events as acid is not enabled");

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AddForeignKeyHandler.java
Patch:
@@ -33,7 +33,7 @@
 
 public class AddForeignKeyHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     AddForeignKeyMessage msg = deserializer.getAddForeignKeyMessage(context.dmd.getPayload());
 
@@ -48,7 +48,7 @@ public List<Task<? extends Serializable>> handle(Context context)
       }
     }
 
-    List<Task<? extends Serializable>> tasks = new ArrayList<Task<? extends Serializable>>();
+    List<Task<?>> tasks = new ArrayList<Task<?>>();
     if (fks.isEmpty()) {
       return tasks;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AddNotNullConstraintHandler.java
Patch:
@@ -33,7 +33,7 @@
 
 public class AddNotNullConstraintHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     AddNotNullConstraintMessage msg = deserializer.getAddNotNullConstraintMessage(context.dmd.getPayload());
 
@@ -48,7 +48,7 @@ public List<Task<? extends Serializable>> handle(Context context)
       }
     }
 
-    List<Task<? extends Serializable>> tasks = new ArrayList<Task<? extends Serializable>>();
+    List<Task<?>> tasks = new ArrayList<Task<?>>();
     if (nns.isEmpty()) {
       return tasks;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AddPrimaryKeyHandler.java
Patch:
@@ -33,7 +33,7 @@
 
 public class AddPrimaryKeyHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     AddPrimaryKeyMessage msg = deserializer.getAddPrimaryKeyMessage(context.dmd.getPayload());
 
@@ -48,7 +48,7 @@ public List<Task<? extends Serializable>> handle(Context context)
       }
     }
 
-    List<Task<? extends Serializable>> tasks = new ArrayList<Task<? extends Serializable>>();
+    List<Task<?>> tasks = new ArrayList<Task<?>>();
     if (pks.isEmpty()) {
       return tasks;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AddUniqueConstraintHandler.java
Patch:
@@ -33,7 +33,7 @@
 
 public class AddUniqueConstraintHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     AddUniqueConstraintMessage msg = deserializer.getAddUniqueConstraintMessage(context.dmd.getPayload());
 
@@ -48,7 +48,7 @@ public List<Task<? extends Serializable>> handle(Context context)
       }
     }
 
-    List<Task<? extends Serializable>> tasks = new ArrayList<Task<? extends Serializable>>();
+    List<Task<?>> tasks = new ArrayList<Task<?>>();
     if (uks.isEmpty()) {
       return tasks;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AllocWriteIdHandler.java
Patch:
@@ -34,7 +34,7 @@
  */
 public class AllocWriteIdHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     if (!AcidUtils.isAcidEnabled(context.hiveConf)) {
       context.log.error("Cannot load alloc write id event as acid is not enabled");
@@ -53,7 +53,7 @@ public List<Task<? extends Serializable>> handle(Context context)
     ReplTxnWork work = new ReplTxnWork(HiveUtils.getReplPolicy(context.dbName), dbName, tableName,
         ReplTxnWork.OperationType.REPL_ALLOC_WRITE_ID, msg.getTxnToWriteIdList(), context.eventOnlyReplicationSpec());
 
-    Task<? extends Serializable> allocWriteIdTask = TaskFactory.get(work, context.hiveConf);
+    Task<?> allocWriteIdTask = TaskFactory.get(work, context.hiveConf);
     context.log.info("Added alloc write id task : {}", allocWriteIdTask.getId());
     updatedMetadata.set(context.dmd.getEventTo().toString(), dbName, tableName, null);
     return Collections.singletonList(allocWriteIdTask);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDatabaseHandler.java
Patch:
@@ -44,7 +44,7 @@
  */
 public class AlterDatabaseHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     AlterDatabaseMessage msg = deserializer.getAlterDatabaseMessage(context.dmd.getPayload());
     String actualDbName = context.isDbNameEmpty() ? msg.getDB() : context.dbName;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CommitTxnHandler.java
Patch:
@@ -40,7 +40,7 @@
  */
 public class CommitTxnHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
           throws SemanticException {
     if (!AcidUtils.isAcidEnabled(context.hiveConf)) {
       context.log.error("Cannot load transaction events as acid is not enabled");
@@ -49,7 +49,7 @@ public List<Task<? extends Serializable>> handle(Context context)
 
     CommitTxnMessage msg = deserializer.getCommitTxnMessage(context.dmd.getPayload());
     int numEntry = (msg.getTables() == null ? 0 : msg.getTables().size());
-    List<Task<? extends Serializable>> tasks = new ArrayList<>();
+    List<Task<?>> tasks = new ArrayList<>();
     String dbName = context.dbName;
     String tableNamePrev = null;
     String tblName = null;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateDatabaseHandler.java
Patch:
@@ -43,7 +43,7 @@
 public class CreateDatabaseHandler extends AbstractMessageHandler {
 
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     MetaData metaData;
     try {
@@ -80,4 +80,4 @@ public List<Task<? extends Serializable>> handle(Context context)
         .set(context.dmd.getEventTo().toString(), destinationDBName, null, null);
     return Collections.singletonList(createDBTask);
   }
-}
\ No newline at end of file
+}

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java
Patch:
@@ -56,7 +56,7 @@ public String getFunctionName() {
   }
 
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     try {
       FunctionDescBuilder builder = new FunctionDescBuilder(context);
@@ -91,7 +91,7 @@ public List<Task<? extends Serializable>> handle(Context context)
          *  add the 'many' to parent/root tasks. The execution environment will make sure that the child barrier task will not get executed unless all parents of the barrier task are complete,
          *  which should only happen when the last task is finished, at which point the child of the barrier task is picked up.
          */
-        Task<? extends Serializable> barrierTask =
+        Task<?> barrierTask =
             TaskFactory.get(new DependencyCollectionWork(), context.hiveConf);
         builder.replCopyTasks.forEach(t -> t.addDependentTask(barrierTask));
         barrierTask.addDependentTask(createTask);
@@ -204,4 +204,4 @@ ResourceUri destinationResourceUri(ResourceUri resourceUri)
       return destinationUri;
     }
   }
-}
\ No newline at end of file
+}

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DefaultHandler.java
Patch:
@@ -26,7 +26,7 @@
 
 public class DefaultHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context withinContext)
+  public List<Task<?>> handle(Context withinContext)
       throws SemanticException {
     return new ArrayList<>();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DeletePartColStatHandler.java
Patch:
@@ -32,7 +32,7 @@
  */
 public class DeletePartColStatHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     context.log.info("Replication of partition stat delete event is not supported yet");
     if (!context.isDbNameEmpty()) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DeleteTableColStatHandler.java
Patch:
@@ -32,7 +32,7 @@
  */
 public class DeleteTableColStatHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     context.log.info("Replication of table stat delete event is not supported yet");
     if (!context.isDbNameEmpty()) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropConstraintHandler.java
Patch:
@@ -30,7 +30,7 @@
 
 public class DropConstraintHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     DropConstraintMessage msg = deserializer.getDropConstraintMessage(context.dmd.getPayload());
     String actualDbName = context.isDbNameEmpty() ? msg.getDB() : context.dbName;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropDatabaseHandler.java
Patch:
@@ -31,13 +31,13 @@
 
 public class DropDatabaseHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     DropDatabaseMessage msg =
         deserializer.getDropDatabaseMessage(context.dmd.getPayload());
     String actualDbName = context.isDbNameEmpty() ? msg.getDB() : context.dbName;
     DropDatabaseDesc desc = new DropDatabaseDesc(actualDbName, true, context.eventOnlyReplicationSpec());
-    Task<? extends Serializable> dropDBTask =
+    Task<?> dropDBTask =
         TaskFactory.get(new DDLWork(new HashSet<>(), new HashSet<>(), desc), context.hiveConf);
     context.log.info(
         "Added drop database task : {}:{}", dropDBTask.getId(), desc.getDatabaseName());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropFunctionHandler.java
Patch:
@@ -31,7 +31,7 @@
 
 public class DropFunctionHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     DropFunctionMessage msg = deserializer.getDropFunctionMessage(context.dmd.getPayload());
     String actualDbName = context.isDbNameEmpty() ? msg.getDB() : context.dbName;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropPartitionHandler.java
Patch:
@@ -34,7 +34,7 @@
 
 public class DropPartitionHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     try {
       DropPartitionMessage msg = deserializer.getDropPartitionMessage(context.dmd.getPayload());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropTableHandler.java
Patch:
@@ -32,7 +32,7 @@
 
 public class DropTableHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     String actualDbName;
     String actualTblName;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/InsertHandler.java
Patch:
@@ -36,7 +36,7 @@ public class InsertHandler extends AbstractMessageHandler {
   private static final Logger LOG = LoggerFactory.getLogger(InsertHandler.class);
 
   @Override
-  public List<Task<? extends Serializable>> handle(Context withinContext)
+  public List<Task<?>> handle(Context withinContext)
       throws SemanticException {
     try {
       FileSystem fs =
@@ -59,7 +59,7 @@ public List<Task<? extends Serializable>> handle(Context withinContext)
 
     // Piggybacking in Import logic for now
     TableHandler tableHandler = new TableHandler();
-    List<Task<? extends Serializable>> tasks = tableHandler.handle(currentContext);
+    List<Task<?>> tasks = tableHandler.handle(currentContext);
     readEntitySet.addAll(tableHandler.readEntities());
     writeEntitySet.addAll(tableHandler.writeEntities());
     getUpdatedMetadata().copyUpdatedMetadata(tableHandler.getUpdatedMetadata());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/MessageHandler.java
Patch:
@@ -37,7 +37,7 @@
 
 public interface MessageHandler {
 
-  List<Task<? extends Serializable>> handle(Context withinContext) throws SemanticException;
+  List<Task<?>> handle(Context withinContext) throws SemanticException;
 
   Set<ReadEntity> readEntities();
 
@@ -48,15 +48,15 @@ public interface MessageHandler {
   class Context {
     public String location;
     public final String dbName;
-    public final Task<? extends Serializable> precursor;
+    public final Task<?> precursor;
     public DumpMetaData dmd;
     final HiveConf hiveConf;
     final Hive db;
     final org.apache.hadoop.hive.ql.Context nestedContext;
     final Logger log;
 
     public Context(String dbName, String location,
-        Task<? extends Serializable> precursor, DumpMetaData dmd, HiveConf hiveConf,
+        Task<?> precursor, DumpMetaData dmd, HiveConf hiveConf,
         Hive db, org.apache.hadoop.hive.ql.Context nestedContext, Logger log) {
       this.dbName = dbName;
       this.location = location;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/OpenTxnHandler.java
Patch:
@@ -34,7 +34,7 @@
  */
 public class OpenTxnHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     if (!AcidUtils.isAcidEnabled(context.hiveConf)) {
       context.log.error("Cannot load transaction events as acid is not enabled");

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/RenamePartitionHandler.java
Patch:
@@ -36,7 +36,7 @@
 
 public class RenamePartitionHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
 
     AlterPartitionMessage msg = deserializer.getAlterPartitionMessage(context.dmd.getPayload());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/RenameTableHandler.java
Patch:
@@ -33,7 +33,7 @@
 
 public class RenameTableHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     AlterTableMessage msg = deserializer.getAlterTableMessage(context.dmd.getPayload());
     try {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/TruncatePartitionHandler.java
Patch:
@@ -34,7 +34,7 @@
 
 public class TruncatePartitionHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context) throws SemanticException {
+  public List<Task<?>> handle(Context context) throws SemanticException {
     AlterPartitionMessage msg = deserializer.getAlterPartitionMessage(context.dmd.getPayload());
     String actualDbName = context.isDbNameEmpty() ? msg.getDB() : context.dbName;
     String actualTblName = msg.getTable();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/TruncateTableHandler.java
Patch:
@@ -30,7 +30,7 @@
 
 public class TruncateTableHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context) throws SemanticException {
+  public List<Task<?>> handle(Context context) throws SemanticException {
     AlterTableMessage msg = deserializer.getAlterTableMessage(context.dmd.getPayload());
     String actualDbName = context.isDbNameEmpty() ? msg.getDB() : context.dbName;
     String actualTblName = msg.getTable();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/UpdatePartColStatHandler.java
Patch:
@@ -33,7 +33,7 @@
  */
 public class UpdatePartColStatHandler extends AbstractMessageHandler {
   @Override
-  public List<Task<? extends Serializable>> handle(Context context)
+  public List<Task<?>> handle(Context context)
       throws SemanticException {
     UpdatePartitionColumnStatMessage upcsm =
             deserializer.getUpdatePartitionColumnStatMessage(context.dmd.getPayload());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/UpdateTableColStatHandler.java
Patch:
@@ -33,7 +33,7 @@
  */
 public class UpdateTableColStatHandler extends AbstractMessageHandler {
     @Override
-    public List<Task<? extends Serializable>> handle(Context context)
+    public List<Task<?>> handle(Context context)
             throws SemanticException {
         UpdateTableColumnStatMessage utcsm =
                 deserializer.getUpdateTableColumnStatMessage(context.dmd.getPayload());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkProcContext.java
Patch:
@@ -67,7 +67,7 @@ public class GenSparkProcContext implements NodeProcessorCtx {
   public final List<Task<MoveWork>> moveTask;
 
   // rootTasks is the entry point for all generated tasks
-  public final List<Task<? extends Serializable>> rootTasks;
+  public final List<Task<?>> rootTasks;
 
   public final Set<ReadEntity> inputs;
   public final Set<WriteEntity> outputs;
@@ -151,7 +151,7 @@ public class GenSparkProcContext implements NodeProcessorCtx {
   public GenSparkProcContext(HiveConf conf,
       ParseContext parseContext,
       List<Task<MoveWork>> moveTask,
-      List<Task<? extends Serializable>> rootTasks,
+      List<Task<?>> rootTasks,
       Set<ReadEntity> inputs,
       Set<WriteEntity> outputs,
       Map<String, TableScanOperator> topOps) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java
Patch:
@@ -404,7 +404,7 @@ public void processFileSink(GenSparkProcContext context, FileSinkOperator fileSi
    * This is forked from {@link GenMapRedUtils}. The difference is that it doesn't check
    * 'isLinkedFileSink' and does not set parent dir for the linked file sinks.
    */
-  public static Path createMoveTask(Task<? extends Serializable> currTask, boolean chDir,
+  public static Path createMoveTask(Task<?> currTask, boolean chDir,
       FileSinkOperator fsOp, ParseContext parseCtx, List<Task<MoveWork>> mvTasks,
       HiveConf hconf, DependencyCollectionTask dependencyTask) {
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolver.java
Patch:
@@ -38,6 +38,6 @@ public interface ConditionalResolver {
    *          opaque context
    * @return position of the task
    */
-  List<Task<? extends Serializable>> getTasks(HiveConf conf, Object ctx);
+  List<Task<?>> getTasks(HiveConf conf, Object ctx);
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ImportTableDesc.java
Patch:
@@ -327,7 +327,7 @@ public String getDatabaseName() {
     return dbName;
   }
 
-  public Task<? extends Serializable> getCreateTableTask(Set<ReadEntity> inputs, Set<WriteEntity> outputs,
+  public Task<?> getCreateTableTask(Set<ReadEntity> inputs, Set<WriteEntity> outputs,
       HiveConf conf) {
     switch (getDescType()) {
     case TABLE:

File: ql/src/test/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/TestTaskTracker.java
Patch:
@@ -32,7 +32,7 @@
 @RunWith(PowerMockRunner.class)
   public class TestTaskTracker {
   @Mock
-  private Task<? extends Serializable> task;
+  private Task<?> task;
 
   @Test
   public void taskTrackerCompositionInitializesTheMaxTasksCorrectly() {
@@ -44,4 +44,4 @@ public void taskTrackerCompositionInitializesTheMaxTasksCorrectly() {
     TaskTracker taskTracker2 = new TaskTracker(taskTracker);
     assertFalse(taskTracker2.canAddMoreTasks());
   }
-}
\ No newline at end of file
+}

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestGenTezWork.java
Patch:
@@ -80,7 +80,7 @@ public void setUp() throws Exception {
         conf,
         pctx,
         Collections.EMPTY_LIST,
-        new ArrayList<Task<? extends Serializable>>(),
+        new ArrayList<Task<?>>(),
         Collections.EMPTY_SET,
         Collections.EMPTY_SET);
 

File: ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java
Patch:
@@ -58,8 +58,8 @@ public void testResolvingDriverAlias() throws Exception {
 
     // joins alias1, alias2, alias3 (alias1 was not eligible for big pos)
     // Must be deterministic order map for consistent q-test output across Java versions
-    HashMap<Task<? extends Serializable>, Set<String>> taskToAliases =
-        new LinkedHashMap<Task<? extends Serializable>, Set<String>>();
+    HashMap<Task<?>, Set<String>> taskToAliases =
+        new LinkedHashMap<Task<?>, Set<String>>();
     taskToAliases.put(task1, new HashSet<String>(Arrays.asList("alias2")));
     taskToAliases.put(task2, new HashSet<String>(Arrays.asList("alias3")));
 
@@ -88,4 +88,4 @@ public void testResolvingDriverAlias() throws Exception {
     resolved = resolver.resolveMapJoinTask(ctx, conf);
     Assert.assertNull(resolved);
   }
-}
\ No newline at end of file
+}

File: ql/src/test/org/apache/hadoop/hive/ql/plan/TestReadEntityDirect.java
Patch:
@@ -196,7 +196,7 @@ public static class CheckInputReadEntityDirect extends AbstractSemanticAnalyzerH
 
     @Override
     public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-        List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+        List<Task<?>> rootTasks) throws SemanticException {
       readEntities = context.getInputs();
     }
 

File: ql/src/test/org/apache/hadoop/hive/ql/plan/TestViewEntity.java
Patch:
@@ -45,7 +45,7 @@ public static class CheckInputReadEntity extends
 
     @Override
     public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-        List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+        List<Task<?>> rootTasks) throws SemanticException {
       readEntities = context.getInputs().toArray(new ReadEntity[0]);
     }
 

File: service/src/test/org/apache/hive/service/cli/CLIServiceTest.java
Patch:
@@ -366,7 +366,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context,
 
     @Override
     public void postAnalyze(HiveSemanticAnalyzerHookContext context,
-      List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+      List<Task<?>> rootTasks) throws SemanticException {
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/events/filesystem/FSTableEvent.java
Patch:
@@ -193,7 +193,8 @@ private AlterTableAddPartitionDesc addPartitionDesc(Path fromPath, ImportTableDe
         ColumnStatisticsDesc colStatsDesc = new ColumnStatisticsDesc(colStats.getStatsDesc());
         colStatsDesc.setTableName(tblDesc.getTableName());
         colStatsDesc.setDbName(tblDesc.getDatabaseName());
-        columnStatistics = new ColumnStatistics(colStatsDesc, colStats.getStatsObj(), colStats.getEngine());
+        columnStatistics = new ColumnStatistics(colStatsDesc, colStats.getStatsObj());
+        columnStatistics.setEngine(colStats.getEngine());
         writeId = replicationSpec().isMigratingToTxnTable() ?
                 ReplUtils.REPL_BOOTSTRAP_MIGRATION_BASE_WRITE_ID : partition.getWriteId();
       }

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestStats.java
Patch:
@@ -195,7 +195,9 @@ private ColumnStatistics buildStatsForOneTableOrPartition(String catName, String
 
     for (Column col : cols) objs.add(col.generate());
 
-    return new ColumnStatistics(desc, objs, ENGINE);
+    ColumnStatistics columnStatistics = new ColumnStatistics(desc, objs);
+    columnStatistics.setEngine(ENGINE);
+    return columnStatistics;
   }
 
   private void dropStats(String catName, String dbName, String tableName, String partName,

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyHiveSortedInputFormatUsedHook.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.hadoop.hive.ql.hooks;
 
 import java.io.Serializable;
-import java.util.ArrayList;
+import java.util.List;
 
 import org.junit.Assert;
 
@@ -33,8 +33,7 @@ public void run(HookContext hookContext) {
 
       // Go through the root tasks, and verify the input format of the map reduce task(s) is
       // HiveSortedInputFormat
-      ArrayList<Task<? extends Serializable>> rootTasks =
-          hookContext.getQueryPlan().getRootTasks();
+      List<Task<? extends Serializable>> rootTasks = hookContext.getQueryPlan().getRootTasks();
       for (Task<? extends Serializable> rootTask : rootTasks) {
         if (rootTask.getWork() instanceof MapredWork) {
           Assert.assertTrue("The root map reduce task's input was not marked as sorted.",

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcCtx.java
Patch:
@@ -36,10 +36,8 @@
 import org.apache.hadoop.hive.ql.exec.JoinOperator;
 import org.apache.hadoop.hive.ql.exec.LimitOperator;
 import org.apache.hadoop.hive.ql.exec.Operator;
-import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
 import org.apache.hadoop.hive.ql.exec.RowSchema;
 import org.apache.hadoop.hive.ql.exec.UnionOperator;
-import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
@@ -136,7 +134,7 @@ public Map<ColumnInfo, ExprNodeDesc> getPropagatedConstants(Operator<? extends S
       return constants;
     }
 
-    ArrayList<ColumnInfo> signature = op.getSchema().getSignature();
+    List<ColumnInfo> signature = op.getSchema().getSignature();
     if (op instanceof LimitOperator || op instanceof FilterOperator) {
       // there should be only one parent.
       if (op.getParentOperators().size() == 1) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java
Patch:
@@ -1120,7 +1120,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object..
       }
 
       GroupByDesc conf = op.getConf();
-      ArrayList<ExprNodeDesc> keys = conf.getKeys();
+      List<ExprNodeDesc> keys = conf.getKeys();
       for (int i = 0; i < keys.size(); i++) {
         ExprNodeDesc key = keys.get(i);
         ExprNodeDesc newkey = foldExpr(key, colToConstants, cppCtx, op, 0, false);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/CountDistinctRewriteProc.java
Patch:
@@ -142,7 +142,7 @@ protected int checkCountDistinct(GroupByOperator mGby, ReduceSinkOperator rs,
         GroupByOperator rGby) {
       // Position of distinct column in aggregator list of map Gby before rewrite.
       int indexOfDist = -1;
-      ArrayList<ExprNodeDesc> keys = mGby.getConf().getKeys();
+      List<ExprNodeDesc> keys = mGby.getConf().getKeys();
       if (!(mGby.getConf().getMode() == GroupByDesc.Mode.HASH
           && !mGby.getConf().isGroupingSetsPresent() && rs.getConf().getKeyCols().size() == 1
           && rs.getConf().getPartitionCols().size() == 0

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -1305,7 +1305,7 @@ public static void createMRWorkForMergingFiles(FileSinkOperator fsInput,
     DynamicPartitionCtx dpCtx = fsInputDesc.getDynPartCtx();
     if (dpCtx != null && dpCtx.getNumDPCols() > 0) {
       // adding DP ColumnInfo to the RowSchema signature
-      ArrayList<ColumnInfo> signature = inputRS.getSignature();
+      List<ColumnInfo> signature = inputRS.getSignature();
       String tblAlias = fsInputDesc.getTableInfo().getTableName();
       for (String dpCol : dpCtx.getDPColNames()) {
         ColumnInfo colInfo = new ColumnInfo(dpCol,

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java
Patch:
@@ -498,7 +498,7 @@ OpAttr visit(HiveSortLimit sortRel) throws SemanticException {
       List<String> keepColumns = new ArrayList<String>();
       final ImmutableBitSet sortColsPos = sortColsPosBuilder.build();
       final ImmutableBitSet sortOutputColsPos = sortOutputColsPosBuilder.build();
-      final ArrayList<ColumnInfo> inputSchema = inputOp.getSchema().getSignature();
+      final List<ColumnInfo> inputSchema = inputOp.getSchema().getSignature();
       for (int pos=0; pos<inputSchema.size(); pos++) {
         if ((sortColsPos.get(pos) && sortOutputColsPos.get(pos)) ||
                 (!sortColsPos.get(pos) && !sortOutputColsPos.get(pos))) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -3177,7 +3177,7 @@ private boolean validateAggregationDesc(AggregationDesc aggDesc, GroupByDesc.Mod
     }
 
 
-    ArrayList<ExprNodeDesc> parameters = aggDesc.getParameters();
+    List<ExprNodeDesc> parameters = aggDesc.getParameters();
 
     if (parameters != null && !validateExprNodeDesc(parameters, "Aggregation Function UDF " + udfName + " parameter")) {
       return false;
@@ -4603,7 +4603,7 @@ private static ImmutablePair<Operator<? extends OperatorDesc>,String> doVectoriz
     // For now, we don't support group by on DECIMAL_64 keys.
     VectorExpression[] vecKeyExpressions =
         vContext.getVectorExpressionsUpConvertDecimal64(keysDesc);
-    ArrayList<AggregationDesc> aggrDesc = groupByDesc.getAggregators();
+    List<AggregationDesc> aggrDesc = groupByDesc.getAggregators();
     final int size = aggrDesc.size();
 
     VectorAggregationDesc[] vecAggrDescs = new VectorAggregationDesc[size];
@@ -4820,7 +4820,7 @@ private static void createVectorPTFDesc(Operator<? extends OperatorDesc> ptfOp,
     List<WindowFunctionDef> windowsFunctions = windowTableFunctionDef.getWindowFunctions();
     final int functionCount = windowsFunctions.size();
 
-    ArrayList<ColumnInfo> outputSignature = ptfOp.getSchema().getSignature();
+    List<ColumnInfo> outputSignature = ptfOp.getSchema().getSignature();
     final int outputSize = outputSignature.size();
 
     /*

File: ql/src/java/org/apache/hadoop/hive/ql/parse/PTFTranslator.java
Patch:
@@ -841,9 +841,9 @@ private static ArrayList<? extends Object>[] getTypeMap(
    */
   public static StructObjectInspector getStandardStructOI(RowResolver rr) {
     StructObjectInspector oi;
-    ArrayList<ColumnInfo> colLists = rr.getColumnInfos();
-    ArrayList<String> structFieldNames = new ArrayList<String>();
-    ArrayList<ObjectInspector> structFieldObjectInspectors = new ArrayList<ObjectInspector>();
+    List<ColumnInfo> colLists = rr.getColumnInfos();
+    List<String> structFieldNames = new ArrayList<String>();
+    List<ObjectInspector> structFieldObjectInspectors = new ArrayList<ObjectInspector>();
     for (ColumnInfo columnInfo : colLists) {
       String colName = columnInfo.getInternalName();
       ObjectInspector colOI = columnInfo.getObjectInspector();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java
Patch:
@@ -122,7 +122,7 @@ public void init(QueryState queryState, LogHelper console, Hive db) {
   @SuppressWarnings("nls")
   public void compile(final ParseContext pCtx,
       final List<Task<? extends Serializable>> rootTasks,
-      final HashSet<ReadEntity> inputs, final HashSet<WriteEntity> outputs) throws SemanticException {
+      final Set<ReadEntity> inputs, final Set<WriteEntity> outputs) throws SemanticException {
 
     Context ctx = pCtx.getContext();
     GlobalLimitCtx globalLimitCtx = pCtx.getGlobalLimitCtx();
@@ -408,7 +408,7 @@ private String extractTableFullName(StatsTask tsk) throws SemanticException {
     return tsk.getWork().getFullTableName();
   }
 
-  private Task<?> genTableStats(ParseContext parseContext, TableScanOperator tableScan, Task currentTask, final HashSet<WriteEntity> outputs)
+  private Task<?> genTableStats(ParseContext parseContext, TableScanOperator tableScan, Task currentTask, final Set<WriteEntity> outputs)
       throws HiveException {
     Class<? extends InputFormat> inputFormat = tableScan.getConf().getTableMetadata()
         .getInputFormatClass();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -1617,7 +1617,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
           assert child.getType() == HiveParser.TOK_TABNAME;
           assert child.getChildCount() == 1;
           String tableAlias = BaseSemanticAnalyzer.unescapeIdentifier(child.getChild(0).getText());
-          HashMap<String, ColumnInfo> columns = input.getFieldMap(tableAlias);
+          Map<String, ColumnInfo> columns = input.getFieldMap(tableAlias);
           if (columns == null) {
             throw new SemanticException(ErrorMsg.INVALID_TABLE_ALIAS.getMsg(child));
           }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ImportTableDesc.java
Patch:
@@ -22,6 +22,7 @@
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 
 import com.google.common.collect.ImmutableSet;
 import org.apache.hadoop.hive.conf.Constants;
@@ -326,7 +327,7 @@ public String getDatabaseName() {
     return dbName;
   }
 
-  public Task<? extends Serializable> getCreateTableTask(HashSet<ReadEntity> inputs, HashSet<WriteEntity> outputs,
+  public Task<? extends Serializable> getCreateTableTask(Set<ReadEntity> inputs, Set<WriteEntity> outputs,
       HiveConf conf) {
     switch (getDescType()) {
     case TABLE:

File: ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
Patch:
@@ -399,7 +399,7 @@ public boolean getCacheAffinity() {
     return llapIoDesc.cached;
   }
 
- public void setNameToSplitSample(HashMap<String, SplitSample> nameToSplitSample) {
+ public void setNameToSplitSample(Map<String, SplitSample> nameToSplitSample) {
     this.nameToSplitSample = nameToSplitSample;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
Patch:
@@ -665,15 +665,15 @@ public static List<FieldSchema> getFieldSchemasFromColumnList(
    */
   public static List<FieldSchema> getFieldSchemasFromRowSchema(RowSchema row,
       String fieldPrefix) {
-    ArrayList<ColumnInfo> c = row.getSignature();
+    List<ColumnInfo> c = row.getSignature();
     return getFieldSchemasFromColumnInfo(c, fieldPrefix);
   }
 
   /**
    * Convert the ColumnInfo to FieldSchema.
    */
   public static List<FieldSchema> getFieldSchemasFromColumnInfo(
-      ArrayList<ColumnInfo> cols, String fieldPrefix) {
+      List<ColumnInfo> cols, String fieldPrefix) {
     if ((cols == null) || (cols.size() == 0)) {
       return new ArrayList<FieldSchema>();
     }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/MatchPath.java
Patch:
@@ -705,7 +705,7 @@ public static class ResultExpressionParser {
     TypeCheckCtx selectListInputTypeCheckCtx;
     StructObjectInspector selectListInputOI;
 
-    ArrayList<WindowExpressionSpec> selectSpec;
+    List<WindowExpressionSpec> selectSpec;
 
     ResultExprInfo resultExprInfo;
 

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestColumnAccess.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.hadoop.hive.ql.parse;
 
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 
 import org.junit.Assert;
 
@@ -159,7 +159,7 @@ public void testJoinView1AndTable2() throws ParseException {
     Assert.assertNotNull(cols.contains("name1"));
   }
 
-  private Map<String, List<String>> getColsFromReadEntity(HashSet<ReadEntity> inputs) {
+  private Map<String, List<String>> getColsFromReadEntity(Set<ReadEntity> inputs) {
     Map<String, List<String>> tableColsMap = new HashMap<String, List<String>>();
     for(ReadEntity entity: inputs) {
       switch (entity.getType()) {

File: service/src/java/org/apache/hive/service/server/HS2ActivePassiveHARegistryClient.java
Patch:
@@ -40,7 +40,7 @@ public static synchronized HS2ActivePassiveHARegistry getClient(Configuration co
     String namespace = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_SERVER2_ACTIVE_PASSIVE_HA_REGISTRY_NAMESPACE);
     Preconditions.checkArgument(!StringUtils.isBlank(namespace),
       HiveConf.ConfVars.HIVE_SERVER2_ACTIVE_PASSIVE_HA_REGISTRY_NAMESPACE.varname + " cannot be null or empty");
-    String nsKey = ZkRegistryBase.getRootNamespace(null, namespace + "-");
+    String nsKey = ZkRegistryBase.getRootNamespace(conf, null, namespace + "-");
     HS2ActivePassiveHARegistry registry = hs2Registries.get(nsKey);
     if (registry == null) {
       registry = HS2ActivePassiveHARegistry.create(conf, true);

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/security/ZooKeeperTokenStore.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.metastore.utils.SecurityUtils;
 import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.UserGroupInformation.AuthenticationMethod;
 import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;
 import org.apache.hadoop.security.token.delegation.MetastoreDelegationTokenSupport;
 import org.apache.zookeeper.CreateMode;
@@ -111,7 +112,8 @@ private CuratorFramework getSession() {
   }
 
   private void setupJAASConfig(Configuration conf) throws IOException {
-    if (!UserGroupInformation.getLoginUser().isFromKeytab()) {
+    if (!UserGroupInformation.getLoginUser().isFromKeytab() || AuthenticationMethod.SIMPLE.name().equalsIgnoreCase(
+        getNonEmptyConfVar(conf, "hive.security.zookeeper.authentication"))) {
       // The process has not logged in using keytab
       // this should be a test mode, can't use keytab to authenticate
       // with zookeeper.

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1736,7 +1736,7 @@ public static enum ConfVars {
         "Only works on Tez and Spark, because memory-optimized hashtable cannot be serialized."),
     HIVEMAPJOINOPTIMIZEDTABLEPROBEPERCENT("hive.mapjoin.optimized.hashtable.probe.percent",
         (float) 0.5, "Probing space percentage of the optimized hashtable"),
-    HIVEUSEHYBRIDGRACEHASHJOIN("hive.mapjoin.hybridgrace.hashtable", true, "Whether to use hybrid" +
+    HIVEUSEHYBRIDGRACEHASHJOIN("hive.mapjoin.hybridgrace.hashtable", false, "Whether to use hybrid" +
         "grace hash join as the join method for mapjoin. Tez only."),
     HIVEHYBRIDGRACEHASHJOINMEMCHECKFREQ("hive.mapjoin.hybridgrace.memcheckfrequency", 1024, "For " +
         "hybrid grace hash join, how often (how many rows apart) we check if memory is full. " +

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
Patch:
@@ -296,9 +296,10 @@ private Long incrementalDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive
     String dbName = (null != work.dbNameOrPattern && !work.dbNameOrPattern.isEmpty())
         ? work.dbNameOrPattern
         : "?";
+    int maxEventLimit = work.maxEventLimit();
     replLogger = new IncrementalDumpLogger(dbName, dumpRoot.toString(),
-            evFetcher.getDbNotificationEventsCount(work.eventFrom, dbName, work.eventTo,
-                    work.maxEventLimit()));
+            evFetcher.getDbNotificationEventsCount(work.eventFrom, dbName, work.eventTo, maxEventLimit),
+            work.eventFrom, work.eventTo, maxEventLimit);
     replLogger.startLog();
     while (evIter.hasNext()) {
       NotificationEvent ev = evIter.next();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorLimitOperator.java
Patch:
@@ -79,7 +79,6 @@ public void process(Object row, int tag) throws HiveException {
       batch.size = Math.min(batch.size, offset + limit - currCount);
       if (batch.selectedInUse == false) {
         batch.selectedInUse = true;
-        batch.selected = new int[batch.size];
         for (int i = 0; i < batch.size - skipSize; i++) {
           batch.selected[i] = skipSize + i;
         }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinOuterGenerateResultOperator.java
Patch:
@@ -714,6 +714,7 @@ public void finishOuterRepeated(VectorizedRowBatch batch, JoinUtil.JoinResult jo
         if (inputSelectedInUse) {
           System.arraycopy(inputSelected, 0, batch.selected, 0, inputLogicalSize);
         }
+        batch.selectedInUse = inputSelectedInUse;
         batch.size = inputLogicalSize;
       }
 

File: ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java
Patch:
@@ -1345,11 +1345,11 @@ public static long getWritableSize(ObjectInspector oi, Object value) {
     if (oi instanceof WritableStringObjectInspector) {
       WritableStringObjectInspector woi = (WritableStringObjectInspector) oi;
       return JavaDataModel.get().lengthForStringOfLength(
-          woi.getPrimitiveWritableObject(value).getLength());
+        value == null ? 0 : woi.getPrimitiveWritableObject(value).getLength());
     } else if (oi instanceof WritableBinaryObjectInspector) {
       WritableBinaryObjectInspector woi = (WritableBinaryObjectInspector) oi;
       return JavaDataModel.get().lengthForByteArrayOfSize(
-          woi.getPrimitiveWritableObject(value).getLength());
+        value == null ? 0 : woi.getPrimitiveWritableObject(value).getLength());
     } else if (oi instanceof WritableBooleanObjectInspector) {
       return JavaDataModel.get().primitive1();
     } else if (oi instanceof WritableByteObjectInspector) {

File: llap-common/src/java/org/apache/hadoop/hive/llap/LlapUtil.java
Patch:
@@ -47,6 +47,8 @@
 
 import com.google.protobuf.BlockingService;
 
+import javax.annotation.Nullable;
+
 public class LlapUtil {
   private static final Logger LOG = LoggerFactory.getLogger(LlapUtil.class);
 
@@ -372,7 +374,7 @@ private static boolean isSomeHiveDir(String p) {
   }
 
 
-  public static ThreadMXBean initThreadMxBean() {
+  @Nullable public static ThreadMXBean initThreadMxBean() {
     ThreadMXBean mxBean = ManagementFactory.getThreadMXBean();
     if (mxBean != null) {
       if (!mxBean.isCurrentThreadCpuTimeSupported()) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java
Patch:
@@ -702,7 +702,7 @@ private void processCurrentEvents(EventState e, WmThreadSyncWork syncWork) throw
     // 11. Finally, for all the pools that have changes, promote queued queries and rebalance.
     for (String poolName : poolsToRedistribute) {
       if (LOG.isDebugEnabled()) {
-        LOG.info("Processing changes for pool " + poolName + ": " + pools.get(poolName));
+        LOG.debug("Processing changes for pool " + poolName + ": " + pools.get(poolName));
       }
       processPoolChangesOnMasterThread(poolName, hasRequeues, syncWork);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadTasksBuilder.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hadoop.hive.metastore.api.ReplLastIdInfo;
 import org.apache.hadoop.hive.ql.DriverContext;
 import org.apache.hadoop.hive.ql.ddl.DDLWork;
-import org.apache.hadoop.hive.ql.ddl.database.AlterDatabaseDesc;
+import org.apache.hadoop.hive.ql.ddl.database.AlterDatabaseSetPropertiesDesc;
 import org.apache.hadoop.hive.ql.ddl.misc.ReplRemoveFirstIncLoadPendFlagDesc;
 import org.apache.hadoop.hive.ql.ddl.table.misc.AlterTableSetPropertiesDesc;
 import org.apache.hadoop.hive.ql.exec.Task;
@@ -278,7 +278,8 @@ private Task<? extends Serializable> dbUpdateReplStateTask(String dbName, String
     HashMap<String, String> mapProp = new HashMap<>();
     mapProp.put(ReplicationSpec.KEY.CURR_STATE_ID.toString(), replState);
 
-    AlterDatabaseDesc alterDbDesc = new AlterDatabaseDesc(dbName, mapProp, new ReplicationSpec(replState, replState));
+    AlterDatabaseSetPropertiesDesc alterDbDesc = new AlterDatabaseSetPropertiesDesc(dbName, mapProp,
+        new ReplicationSpec(replState, replState));
     Task<? extends Serializable> updateReplIdTask = TaskFactory.get(new DDLWork(inputs, outputs, alterDbDesc), conf);
 
     // Link the update repl state task with dependency collection task

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1989,7 +1989,7 @@ public static enum ConfVars {
         "Maximum fraction of heap that can be used by Parquet file writers in one task.\n" +
         "It is for avoiding OutOfMemory error in tasks. Work with Parquet 1.6.0 and above.\n" +
         "This config parameter is defined in Parquet, so that it does not start with 'hive.'."),
-    HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION("hive.parquet.timestamp.skip.conversion", false,
+    HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION("hive.parquet.timestamp.skip.conversion", true,
       "Current Hive implementation of parquet stores timestamps to UTC, this flag allows skipping of the conversion" +
       "on reading parquet files from other tools"),
     HIVE_AVRO_TIMESTAMP_SKIP_CONVERSION("hive.avro.timestamp.skip.conversion", false,

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2587,7 +2587,7 @@ public static enum ConfVars {
         "The port of ZooKeeper servers to talk to.\n" +
         "If the list of Zookeeper servers specified in hive.zookeeper.quorum\n" +
         "does not contain port numbers, this value is used."),
-    HIVE_ZOOKEEPER_SESSION_TIMEOUT("hive.zookeeper.session.timeout", "1200000ms",
+    HIVE_ZOOKEEPER_SESSION_TIMEOUT("hive.zookeeper.session.timeout", "120000ms",
         new TimeValidator(TimeUnit.MILLISECONDS),
         "ZooKeeper client's session timeout (in milliseconds). The client is disconnected, and as a result, all locks released, \n" +
         "if a heartbeat is not sent in the timeout."),

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4669,7 +4669,7 @@ public static enum ConfVars {
         "comma separated list of plugin can be used:\n"
             + "  overlay: hiveconf subtree 'reexec.overlay' is used as an overlay in case of an execution errors out\n"
             + "  reoptimize: collects operator statistics during execution and recompile the query after a failure"),
-    HIVE_QUERY_REEXECUTION_STATS_PERSISTENCE("hive.query.reexecution.stats.persist.scope", "query",
+    HIVE_QUERY_REEXECUTION_STATS_PERSISTENCE("hive.query.reexecution.stats.persist.scope", "metastore",
         new StringSet("query", "hiveserver", "metastore"),
         "Sets the persistence scope of runtime statistics\n"
             + "  query: runtime statistics are only used during re-execution\n"

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExpandDistinctAggregatesRule.java
Patch:
@@ -250,8 +250,10 @@ public RexNode apply(RelDataTypeField input) {
             originalInputRefs.get(pos));
         condition = rexBuilder.makeCall(SqlStdOperatorTable.AND, condition, notNull);
       }
+      RexNode caseExpr1 = rexBuilder.makeExactLiteral(BigDecimal.ONE);
+      RexNode caseExpr2 = rexBuilder.makeNullLiteral(caseExpr1.getType());
       RexNode when = rexBuilder.makeCall(SqlStdOperatorTable.CASE, condition,
-          rexBuilder.makeExactLiteral(BigDecimal.ONE), rexBuilder.constantNull());
+          caseExpr1, caseExpr2);
       gbChildProjLst.add(when);
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java
Patch:
@@ -145,13 +145,13 @@ HiveMetaStoreAuthzInfo buildAuthzContext(PreEventContext preEventContext) throws
           }
           break;
         case ALTER_TABLE:
-          authzEvent = new CreateTableEvent(preEventContext);
+          authzEvent = new AlterTableEvent(preEventContext);
           if (isViewOperation(preEventContext) && (!isSuperUser(getCurrentUser(authzEvent)))) {
             throw new MetaException(getErrorMessage("ALTER_VIEW", getCurrentUser(authzEvent)));
           }
           break;
         case DROP_TABLE:
-          authzEvent = new CreateTableEvent(preEventContext);
+          authzEvent = new DropTableEvent(preEventContext);
           if (isViewOperation(preEventContext) && (!isSuperUser(getCurrentUser(authzEvent)))) {
             throw new MetaException(getErrorMessage("DROP_VIEW", getCurrentUser(authzEvent)));
           }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -12825,7 +12825,8 @@ static List<FieldSchema> convertRowSchemaToResultSetSchema(RowResolver rr,
       }
 
       qualifiedColName = rr.reverseLookup(colInfo.getInternalName());
-      if (useTabAliasIfAvailable && qualifiedColName[0] != null && !qualifiedColName[0].isEmpty()) {
+      // __u<n> is a UNION ALL placeholder name
+      if (useTabAliasIfAvailable && qualifiedColName[0] != null && (!qualifiedColName[0].isEmpty()) && (!qualifiedColName[0].startsWith("__u"))) {
         colName = qualifiedColName[0] + "." + qualifiedColName[1];
       } else {
         colName = qualifiedColName[1];

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java
Patch:
@@ -293,7 +293,7 @@ WarehouseInstance load(String replicatedDbName, String dumpLocation) throws Thro
   }
 
   WarehouseInstance loadWithoutExplain(String replicatedDbName, String dumpLocation) throws Throwable {
-    run("REPL LOAD " + replicatedDbName + " FROM '" + dumpLocation + "'");
+    run("REPL LOAD " + replicatedDbName + " FROM '" + dumpLocation + "' with ('hive.exec.parallel'='true')");
     return this;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -2692,7 +2692,7 @@ private TaskRunner launchTask(Task<? extends Serializable> tsk, String queryId,
       console.printInfo("Launching Job " + cxt.getCurJobNo() + " out of " + jobs);
     }
     tsk.initialize(queryState, plan, cxt, ctx.getOpContext());
-    TaskRunner tskRun = new TaskRunner(tsk);
+    TaskRunner tskRun = new TaskRunner(tsk, cxt);
 
     cxt.launching(tskRun);
     // Launch Task

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
Patch:
@@ -1538,6 +1538,9 @@ private VectorExpression getConstantVectorExpression(Object constantValue, TypeI
       outCol = ocm.allocateOutputColumn(typeInfo);
     }
     if (constantValue == null) {
+      if (typeInfo.getCategory() != Category.PRIMITIVE) {
+        throw new HiveException("Complex type constants (" + typeInfo.getCategory() + ") not supported for type name " + typeName);
+      }
       return new ConstantVectorExpression(outCol, typeInfo, true);
     }
 

File: cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java
Patch:
@@ -124,12 +124,12 @@ public void testThatCliDriverDoesNotStripComments() throws Exception {
     // Save output as yo cannot print it while System.out and System.err are weird
     String message;
     String errors;
-    int ret;
+    CommandProcessorResponse response;
     try {
       CliSessionState.start(ss);
       CliDriver cliDriver = new CliDriver();
       // issue a command with bad options
-      ret = cliDriver.processCmd("!ls --abcdefghijklmnopqrstuvwxyz123456789");
+      response = cliDriver.processCmd("!ls --abcdefghijklmnopqrstuvwxyz123456789");
     } finally {
       // restore System.out and System.err
       System.setOut(oldOut);
@@ -138,7 +138,7 @@ public void testThatCliDriverDoesNotStripComments() throws Exception {
     message = dataOut.toString("UTF-8");
     errors = dataErr.toString("UTF-8");
     assertTrue("Comments with '--; should not have been stripped,"
-        + " so command should fail", ret != 0);
+        + " so command should fail", response.getResponseCode() != 0);
     assertTrue("Comments with '--; should not have been stripped,"
         + " so we should have got an error in the output: '" + errors + "'.",
         errors.contains("option"));

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CoreHBaseNegativeCliDriver.java
Patch:
@@ -104,7 +104,7 @@ public void runTest(String tname, String fname, String fpath) {
       System.err.println("Begin query: " + fname);
       qt.addFile(fpath);
       qt.cliInit(new File(fpath));
-      int ecode = qt.executeClient(fname);
+      int ecode = qt.executeClient(fname).getResponseCode();
       if (ecode == 0) {
         qt.failed(fname, null);
       }
@@ -115,7 +115,7 @@ public void runTest(String tname, String fname, String fpath) {
       }
 
     } catch (Exception e) {
-      qt.failed(e, fname, null);
+      qt.failedWithException(e, fname, null);
     }
 
     long elapsedTime = System.currentTimeMillis() - startTime;

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CoreNegativeCliDriver.java
Patch:
@@ -125,7 +125,7 @@ public void runTest(String tname, String fname, String fpath) throws Exception {
       qt.addFile(fpath);
       qt.cliInit(new File(fpath));
 
-      int ecode = qt.executeClient(fname);
+      int ecode = qt.executeClient(fname).getResponseCode();
       if (ecode == 0) {
         qt.failed(fname, debugHint);
       }
@@ -145,7 +145,7 @@ public void runTest(String tname, String fname, String fpath) throws Exception {
       }
     }
     catch (Exception e) {
-      qt.failed(e, fname, debugHint);
+      qt.failedWithException(e, fname, debugHint);
     }
 
     long elapsedTime = System.currentTimeMillis() - startTime;

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/parse/CoreParseNegative.java
Patch:
@@ -126,7 +126,7 @@ public void runTest(String tname, String fname, String fpath) throws Exception {
     catch (ParseException pe) {
       QTestProcessExecResult result = qt.checkNegativeResults(fname, pe);
       if (result.getReturnCode() != 0) {
-        qt.failed(result.getReturnCode(), fname, result.getCapturedOutput() + "\r\n" + debugHint);
+        qt.failedQuery(null, result.getReturnCode(), fname, result.getCapturedOutput() + "\r\n" + debugHint);
       }
     }
     catch (SemanticException se) {
@@ -138,7 +138,7 @@ public void runTest(String tname, String fname, String fpath) throws Exception {
       }
     }
     catch (Exception e) {
-      qt.failed(e, fname, debugHint);
+      qt.failedWithException(e, fname, debugHint);
     }
 
     long elapsedTime = System.currentTimeMillis() - startTime;

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java
Patch:
@@ -724,7 +724,6 @@ public void run() {
       } else {
         try {
           triggerPreWarm(rawStore);
-          shouldRunPrewarm = false;
         } catch (Exception e) {
           LOG.error("Prewarm failure", e);
           return;
@@ -816,6 +815,7 @@ private void updateTableColStats(RawStore rawStore, String catName, String dbNam
         if (table != null && !table.isSetPartitionKeys()) {
           List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);
           Deadline.startTimer("getTableColumnStatistics");
+
           ColumnStatistics tableColStats =
               rawStore.getTableColumnStatistics(catName, dbName, tblName, colNames);
           Deadline.stopTimer();
@@ -865,9 +865,7 @@ private void updateTablePartitionColStats(RawStore rawStore, String catName, Str
                   rawStore.getPartitionColumnStatistics(catName, dbName, tblName, partNames, colNames);
           Deadline.stopTimer();
           sharedCache.refreshPartitionColStatsInCache(catName, dbName, tblName, partitionColStats);
-          Deadline.startTimer("getPartitionsByNames");
           List<Partition> parts = rawStore.getPartitionsByNames(catName, dbName, tblName, partNames);
-          Deadline.stopTimer();
           // Also save partitions for consistency as they have the stats state.
           for (Partition part : parts) {
             sharedCache.alterPartitionInCache(catName, dbName, tblName, part.getValues(), part);

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java
Patch:
@@ -724,6 +724,7 @@ public void run() {
       } else {
         try {
           triggerPreWarm(rawStore);
+          shouldRunPrewarm = false;
         } catch (Exception e) {
           LOG.error("Prewarm failure", e);
           return;
@@ -815,7 +816,6 @@ private void updateTableColStats(RawStore rawStore, String catName, String dbNam
         if (table != null && !table.isSetPartitionKeys()) {
           List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);
           Deadline.startTimer("getTableColumnStatistics");
-
           ColumnStatistics tableColStats =
               rawStore.getTableColumnStatistics(catName, dbName, tblName, colNames);
           Deadline.stopTimer();
@@ -865,7 +865,9 @@ private void updateTablePartitionColStats(RawStore rawStore, String catName, Str
                   rawStore.getPartitionColumnStatistics(catName, dbName, tblName, partNames, colNames);
           Deadline.stopTimer();
           sharedCache.refreshPartitionColStatsInCache(catName, dbName, tblName, partitionColStats);
+          Deadline.startTimer("getPartitionsByNames");
           List<Partition> parts = rawStore.getPartitionsByNames(catName, dbName, tblName, partNames);
+          Deadline.stopTimer();
           // Also save partitions for consistency as they have the stats state.
           for (Partition part : parts) {
             sharedCache.alterPartitionInCache(catName, dbName, tblName, part.getValues(), part);

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/AccumuloPredicateHandler.java
Patch:
@@ -134,7 +134,7 @@ public Set<String> pComparisonKeyset() {
    *
    * @param udfType
    *          GenericUDF classname to lookup matching CompareOpt
-   * @return Class<? extends CompareOpt/>
+   * @return Class&lt;? extends CompareOpt/&gt;
    */
   public Class<? extends CompareOp> getCompareOpClass(String udfType)
       throws NoSuchCompareOpException {
@@ -166,7 +166,7 @@ public CompareOp getCompareOp(String udfType, IndexSearchCondition sc)
    *
    * @param type
    *          String hive column lookup matching PrimitiveCompare
-   * @return Class<? extends ></?>
+   * @return Class&lt;? extends &gt;&lt;/?&gt;
    */
   public Class<? extends PrimitiveComparison> getPrimitiveComparisonClass(String type)
       throws NoSuchPrimitiveComparisonException {

File: common/src/java/org/apache/hive/http/ProfileServlet.java
Patch:
@@ -38,7 +38,7 @@
  * Servlet that runs async-profiler as web-endpoint.
  * Following options from async-profiler can be specified as query paramater.
  * //  -e event          profiling event: cpu|alloc|lock|cache-misses etc.
- * //  -d duration       run profiling for <duration> seconds (integer)
+ * //  -d duration       run profiling for &lt;duration&gt; seconds (integer)
  * //  -i interval       sampling interval in nanoseconds (long)
  * //  -j jstackdepth    maximum Java stack depth (integer)
  * //  -b bufsize        frame buffer size (long)
@@ -53,7 +53,7 @@
  * - To collect 30 second CPU profile of current process (returns FlameGraph svg)
  * curl "http://localhost:10002/prof"
  * - To collect 1 minute CPU profile of current process and output in tree format (html)
- * curl "http://localhost:10002/prof?output=tree&duration=60"
+ * curl "http://localhost:10002/prof?output=tree&amp;duration=60"
  * - To collect 30 second heap allocation profile of current process (returns FlameGraph svg)
  * curl "http://localhost:10002/prof?event=alloc"
  * - To collect lock contention profile of current process (returns FlameGraph svg)

File: contrib/src/java/org/apache/hadoop/hive/contrib/mr/GenericMR.java
Patch:
@@ -41,7 +41,7 @@
  * As an example, here's the wordcount reduce:
  * 
  * new GenericMR().reduce(System.in, System.out, new Reducer() { public void
- * reduce(String key, Iterator<String[]> records, Output output) throws
+ * reduce(String key, Iterator&lt;String[]&gt; records, Output output) throws
  * Exception { int count = 0;
  * 
  * while (records.hasNext()) { count += Integer.parseInt(records.next()[1]); }

File: contrib/src/java/org/apache/hadoop/hive/contrib/serde2/RegexSerDe.java
Patch:
@@ -61,8 +61,8 @@
  * into a row. If the output type of the column in a query is not a string, it
  * will be automatically converted to String by Hive.
  *
- * For the format of the format String, please refer to {@link http
- * ://java.sun.com/j2se/1.5.0/docs/api/java/util/Formatter.html#syntax}
+ * For the format of the format String, please refer to link: http
+ * ://java.sun.com/j2se/1.5.0/docs/api/java/util/Formatter.html#syntax
  *
  * NOTE: Obviously, all columns have to be strings. Users can use
  * "CAST(a AS INT)" to convert columns to other types.

File: contrib/src/java/org/apache/hadoop/hive/contrib/udaf/example/UDAFExampleAvg.java
Patch:
@@ -44,7 +44,7 @@ public final class UDAFExampleAvg extends UDAF {
    * by a primitive.
    * 
    * The internal state can also contains fields with types like
-   * ArrayList<String> and HashMap<String,Double> if needed.
+   * ArrayList&lt;String&gt; and HashMap&lt;String,Double&gt; if needed.
    */
   public static class UDAFAvgState {
     private long mCount;

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDe.java
Patch:
@@ -154,7 +154,7 @@ public static ColumnMappings parseColumnsMapping(
    * @param columnsMappingSpec string hbase.columns.mapping specified when creating table
    * @param doColumnRegexMatching whether to do a regex matching on the columns or not
    * @param hideColumnPrefix whether to hide a prefix of column mapping in key name in a map (works only if @doColumnRegexMatching is true)
-   * @return List<ColumnMapping> which contains the column mapping information by position
+   * @return List&lt;ColumnMapping&gt; which contains the column mapping information by position
    * @throws org.apache.hadoop.hive.serde2.SerDeException
    */
   public static ColumnMappings parseColumnsMapping(

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/struct/HBaseValueFactory.java
Patch:
@@ -38,7 +38,7 @@ public interface HBaseValueFactory {
   /**
    * Initialize factory with properties
    * 
-   * @param hbaseParam the {@link HBaseParameters hbase parameters}
+   * @param hbaseParam the HBaseParameters hbase parameters
    * @param conf the hadoop {@link Configuration configuration}
    * @param properties the custom {@link Properties}
    * @throws SerDeException if there was an issue initializing the factory
@@ -67,7 +67,7 @@ void init(HBaseSerDeParameters hbaseParam, Configuration conf, Properties proper
    * @param object the object to be serialized
    * @param field the {@link StructField}
    * @return the serialized value
-   * @throws {@link IOException} if there was an issue serializing the value
+   * @throws IOException if there was an issue serializing the value
    */
   byte[] serializeValue(Object object, StructField field) throws IOException;
 }
\ No newline at end of file

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatConstants.java
Patch:
@@ -186,7 +186,7 @@ private HCatConstants() { // restrict instantiation
 
   /**
    * {@value} (default: {@value #HCAT_DATA_CONVERT_BOOLEAN_TO_INTEGER_DEFAULT}).
-   * Pig < 0.10.0 does not have boolean support, and scripts written for pre-boolean Pig versions
+   * Pig &lt; 0.10.0 does not have boolean support, and scripts written for pre-boolean Pig versions
    * will not expect boolean values when upgrading Pig. For integration the option is offered to
    * convert boolean fields to integers by setting this Hadoop configuration key.
    */

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/transfer/HCatWriter.java
Patch:
@@ -58,7 +58,7 @@ public abstract class HCatWriter {
    *
    * @param recordItr
    *          {@link Iterator} records to be written into HCatalog.
-   * @throws {@link HCatException}
+   * @throws  HCatException
    */
   public abstract void write(final Iterator<HCatRecord> recordItr)
     throws HCatException;
@@ -67,15 +67,15 @@ public abstract void write(final Iterator<HCatRecord> recordItr)
    * This method should be called at master node. Primary purpose of this is to
    * do metadata commit.
    *
-   * @throws {@link HCatException}
+   * @throws HCatException
    */
   public abstract void commit(final WriterContext context) throws HCatException;
 
   /**
    * This method should be called at master node. Primary purpose of this is to
    * do cleanups in case of failures.
    *
-   * @throws {@link HCatException} *
+   * @throws HCatException
    */
   public abstract void abort(final WriterContext context) throws HCatException;
 

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java
Patch:
@@ -150,7 +150,7 @@ public void onAlterPartition(AlterPartitionEvent ape) throws MetaException {
    * particular table by listening on a topic named "dbName.tableName" with message selector
    * string {@value org.apache.hive.hcatalog.common.HCatConstants#HCAT_EVENT} =
    * {@value org.apache.hive.hcatalog.common.HCatConstants#HCAT_DROP_PARTITION_EVENT}.
-   * </br>
+   * <br>
    * TODO: DataNucleus 2.0.3, currently used by the HiveMetaStore for persistence, has been
    * found to throw NPE when serializing objects that contain null. For this reason we override
    * some fields in the StorageDescriptor of this notification. This should be fixed after
@@ -264,7 +264,7 @@ public void onAlterTable(AlterTableEvent tableEvent) throws MetaException {
    * dropped tables by listening on topic "HCAT" with message selector string
    * {@value org.apache.hive.hcatalog.common.HCatConstants#HCAT_EVENT} =
    * {@value org.apache.hive.hcatalog.common.HCatConstants#HCAT_DROP_TABLE_EVENT}
-   * </br>
+   * <br>
    * TODO: DataNucleus 2.0.3, currently used by the HiveMetaStore for persistence, has been
    * found to throw NPE when serializing objects that contain null. For this reason we override
    * some fields in the StorageDescriptor of this notification. This should be fixed after

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java
Patch:
@@ -54,7 +54,7 @@
 import java.util.Properties;
 
 /**
- * @deprecated as of Hive 3.0.0, replaced by {@link org.apache.hive.streaming.AbstractRecordWriter}
+ * @deprecated as of Hive 3.0.0, replaced by org.apache.hive.streaming.AbstractRecordWriter
  */
 @Deprecated
 public abstract class AbstractRecordWriter implements RecordWriter {

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java
Patch:
@@ -44,9 +44,9 @@
 
 /**
  * Streaming Writer handles delimited input (eg. CSV).
- * Delimited input is parsed & reordered to match column order in table
+ * Delimited input is parsed &amp; reordered to match column order in table
  * Uses Lazy Simple Serde to process delimited input
- * @deprecated as of Hive 3.0.0, replaced by {@link org.apache.hive.streaming.StrictDelimitedInputWriter}
+ * @deprecated as of Hive 3.0.0, replaced by org.apache.hive.streaming.StrictDelimitedInputWriter
  */
 @Deprecated
 public class DelimitedInputWriter extends AbstractRecordWriter {

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java
Patch:
@@ -67,7 +67,7 @@
  * Information about the hive end point (i.e. table or partition) to write to.
  * A light weight object that does NOT internally hold on to resources such as
  * network connections. It can be stored in Hashed containers such as sets and hash tables.
- * @deprecated as of Hive 3.0.0, replaced by {@link org.apache.hive.streaming.HiveStreamingConnection}
+ * @deprecated as of Hive 3.0.0, replaced by org.apache.hive.streaming.HiveStreamingConnection
  */
 @Deprecated
 public class HiveEndPoint {

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/RecordWriter.java
Patch:
@@ -20,7 +20,7 @@
 
 
 /**
- * @deprecated as of Hive 3.0.0, replaced by {@link org.apache.hive.streaming.RecordWriter}
+ * @deprecated as of Hive 3.0.0, replaced by org.apache.hive.streaming.RecordWriter
  */
 @Deprecated
 public interface RecordWriter {

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/StreamingConnection.java
Patch:
@@ -24,7 +24,7 @@
  * Represents a connection to a HiveEndPoint. Used to acquire transaction batches.
  * Note: the expectation is that there is at most 1 TransactionBatch outstanding for any given
  * StreamingConnection.  Violating this may result in "out of sequence response".
- * @deprecated as of Hive 3.0.0, replaced by {@link org.apache.hive.streaming.HiveStreamingConnection}
+ * @deprecated as of Hive 3.0.0, replaced by org.apache.hive.streaming.HiveStreamingConnection
  */
 @Deprecated
 public interface StreamingConnection {

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/StrictJsonWriter.java
Patch:
@@ -37,7 +37,7 @@
 /**
  * Streaming Writer handles utf8 encoded Json (Strict syntax).
  * Uses org.apache.hive.hcatalog.data.JsonSerDe to process Json input
- * @deprecated as of Hive 3.0.0, replaced by {@link org.apache.hive.streaming.StrictJsonWriter}
+ * @deprecated as of Hive 3.0.0, replaced by org.apache.hive.streaming.StrictJsonWriter
  */
 @Deprecated
 public class StrictJsonWriter extends AbstractRecordWriter {

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/StrictRegexWriter.java
Patch:
@@ -41,7 +41,7 @@
 /**
  * Streaming Writer handles text input data with regex. Uses
  * org.apache.hadoop.hive.serde2.RegexSerDe
- * @deprecated as of Hive 3.0.0, replaced by {@link org.apache.hive.streaming.StrictRegexWriter}
+ * @deprecated as of Hive 3.0.0, replaced by org.apache.hive.streaming.StrictRegexWriter
  */
 @Deprecated
 public class StrictRegexWriter extends AbstractRecordWriter {

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/TransactionBatch.java
Patch:
@@ -30,7 +30,7 @@
  * Note on thread safety: At most 2 threads can run through a given TransactionBatch at the same
  * time.  One thread may call {@link #heartbeat()} and the other all other methods.
  * Violating this may result in "out of sequence response".
- * @deprecated as of Hive 3.0.0, replaced by {@link org.apache.hive.streaming.HiveStreamingConnection}
+ * @deprecated as of Hive 3.0.0, replaced by org.apache.hive.streaming.HiveStreamingConnection
  */
 @Deprecated
 public interface TransactionBatch  {

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/mutate/worker/MutatorCoordinator.java
Patch:
@@ -47,7 +47,7 @@
  * previously closed. The {@link MutatorCoordinator} will seamlessly handle transitions between groups, creating and
  * closing {@link Mutator Mutators} as needed to write to the appropriate partition and bucket. New partitions will be
  * created in the meta store if {@link AcidTable#createPartitions()} is set.
- * <p/>
+ * <p>
  * {@link #insert(List, Object) Insert} events must be artificially assigned appropriate bucket ids in the preceding
  * grouping phase so that they are grouped correctly. Note that any write id or row id assigned to the
  * {@link RecordIdentifier RecordIdentifier} of such events will be ignored by both the coordinator and the underlying

File: hplsql/src/main/java/org/apache/hive/hplsql/Meta.java
Patch:
@@ -259,7 +259,7 @@ public String normalizeIdentifierPart(String name) {
   }
   
   /**
-   * Split qualified object to 2 parts: schema.tab.col -> schema.tab|col; tab.col -> tab|col 
+   * Split qualified object to 2 parts: schema.tab.col -&gt; schema.tab|col; tab.col -&gt; tab|col
    */
   public ArrayList<String> splitIdentifierToTwoParts(String name) {
     ArrayList<String> parts = splitIdentifier(name);    

File: hplsql/src/main/java/org/apache/hive/hplsql/Utils.java
Patch:
@@ -52,7 +52,7 @@ public static String unquoteString(String s) {
   }
 
   /**
-   * Quote string and escape characters - ab'c -> 'ab''c'
+   * Quote string and escape characters - ab'c -&gt; 'ab''c'
    */
   public static String quoteString(String s) {
     if(s == null) {
@@ -73,7 +73,7 @@ public static String quoteString(String s) {
   }
   
   /**
-   * Merge quoted strings: 'a' 'b' -> 'ab'; 'a''b' 'c' -> 'a''bc'
+   * Merge quoted strings: 'a' 'b' -&gt; 'ab'; 'a''b' 'c' -&gt; 'a''bc'
    */
   public static String mergeQuotedStrings(String s1, String s2) {
 	  if(s1 == null || s2 == null) {

File: hplsql/src/main/java/org/apache/hive/hplsql/Var.java
Patch:
@@ -624,7 +624,7 @@ else if (type == Type.TIMESTAMP) {
 	}
 
   /**
-   * Convert value to SQL string - string literals are quoted and escaped, ab'c -> 'ab''c'
+   * Convert value to SQL string - string literals are quoted and escaped, ab'c -&gt; 'ab''c'
    */
   public String toSqlString() {
     if (value == null) {

File: jdbc/src/java/org/apache/hive/jdbc/Utils.java
Patch:
@@ -307,8 +307,8 @@ public static JdbcConnectionParams parseURL(String uri) throws JdbcUriParseExcep
   /**
    * Parse JDBC connection URL
    * The new format of the URL is:
-   * jdbc:hive2://<host1>:<port1>,<host2>:<port2>/dbName;sess_var_list?hive_conf_list#hive_var_list
-   * where the optional sess, conf and var lists are semicolon separated <key>=<val> pairs.
+   * jdbc:hive2://&lt;host1&gt;:&lt;port1&gt;,&lt;host2&gt;:&lt;port2&gt;/dbName;sess_var_list?hive_conf_list#hive_var_list
+   * where the optional sess, conf and var lists are semicolon separated &lt;key&gt;=&lt;val&gt; pairs.
    * For utilizing dynamic service discovery with HiveServer2 multiple comma separated host:port pairs can
    * be specified as shown above.
    * The JDBC driver resolves the list of uris and picks a specific server instance to connect to.

File: llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapDump.java
Patch:
@@ -36,7 +36,8 @@
 
 /**
  * Utility to test query and data retrieval via the LLAP input format.
- * llapdump --hiveconf hive.zookeeper.quorum=localhost --hiveconf hive.zookeeper.client.port=2181 --hiveconf hive.llap.daemon.service.hosts=@llap_MiniLlapCluster 'select * from employee where employee_id < 10'
+ * llapdump --hiveconf hive.zookeeper.quorum=localhost --hiveconf hive.zookeeper.client.port=2181\
+ *   --hiveconf hive.llap.daemon.service.hosts=@llap_MiniLlapCluster 'select * from employee where employee_id &lt; 10'
  *
  */
 public class LlapDump {

File: llap-server/src/java/org/apache/hadoop/hive/llap/ConsumerFeedback.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.hadoop.hive.llap;
 
 /**
- * Consumer feedback typically used by Consumer<T>;
+ * Consumer feedback typically used by Consumer&lt;T&gt;;
  * allows consumer to influence production of data.
  */
 public interface ConsumerFeedback<T> {

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCache.java
Patch:
@@ -47,7 +47,7 @@ public enum Priority {
    *    can be thrown away, the reader will never touch it; but we need code in the reader to
    *    handle such cases to avoid disk reads for these "tails" vs real unmatched ranges.
    *    Some sort of InvalidCacheChunk could be placed to avoid them. TODO
-   * @param base base offset for the ranges (stripe/stream offset in case of ORC).
+   * @param baseOffset base offset for the ranges (stripe/stream offset in case of ORC).
    */
   DiskRangeList getFileData(Object fileKey, DiskRangeList range, long baseOffset,
       DiskRangeListFactory factory, LowLevelCacheCounters qfCounters, BooleanRef gotAllData);

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java
Patch:
@@ -76,15 +76,15 @@
  * are available or when a higher priority task arrives and will schedule it for execution.
  * When pre-emption is enabled, the tasks from wait queue can replace(pre-empt) a running task.
  * The pre-empted task is reported back to the Application Master(AM) for it to be rescheduled.
- * <p/>
+ * <br>
  * Because of the concurrent nature of task submission, the position of the task in wait queue is
  * held as long the scheduling of the task from wait queue (with or without pre-emption) is complete.
  * The order of pre-emption is based on the ordering in the pre-emption queue. All tasks that cannot
  * run to completion immediately (canFinish = false) are added to pre-emption queue.
- * <p/>
+ * <br>
  * When all the executor threads are occupied and wait queue is full, the task scheduler will
  * return SubmissionState.REJECTED response
- * <p/>
+ * <br>
  * Task executor service can be shut down which will terminated all running tasks and reject all
  * new tasks. Shutting down of the task executor service can be done gracefully or immediately.
  */

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java
Patch:
@@ -344,7 +344,7 @@ private String constructThreadNameSuffix(TezTaskAttemptID taskAttemptId) {
   /**
    * Attempt to kill a running task. If the task has not started running, it will not start.
    * If it's already running, a kill request will be sent to it.
-   * <p/>
+   * <br>
    * The AM will be informed about the task kill.
    */
   public void killTask() {

File: llap-server/src/java/org/apache/hadoop/hive/llap/metrics/LlapDaemonJvmInfo.java
Patch:
@@ -24,7 +24,7 @@
 
 /**
  * Llap daemon JVM info. These are some additional metrics that are not exposed via
- * {@link org.apache.hadoop.metrics.jvm.JvmMetrics}
+ * {@link org.apache.hadoop.hive.common.JvmMetrics}
  *
  * NOTE: These metrics are for sinks supported by hadoop-metrics2. There is already a /jmx endpoint
  * that gives all these info.

File: serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinaryDataInputStream.java
Patch:
@@ -108,8 +108,8 @@ public Timestamp readTimestamp(Integer byteNum) throws IOException {
    * Read DATE.
    * The representation of date in Teradata binary format is:
    * The Date D is a int with 4 bytes using little endian,
-   * The representation is (D+19000000).ToString -> YYYYMMDD,
-   * eg: Date 07 b2 01 00 -> 111111 in little endian -> 19111111 - > 1911.11.11.
+   * The representation is (D+19000000).ToString -&gt; YYYYMMDD,
+   * eg: Date 07 b2 01 00 -&gt; 111111 in little endian -&gt; 19111111 - &gt; 1911.11.11.
    * the null date will use 0 to pad.
    *
    * @return the date
@@ -135,7 +135,7 @@ public Date readDate() throws IOException, ParseException {
   /**
    * Read CHAR(N).
    * The representation of char in Teradata binary format is
-   * the byte number to read is based on the [charLength] * [bytePerChar] <- totalLength,
+   * the byte number to read is based on the [charLength] * [bytePerChar] &lt;- totalLength,
    * bytePerChar is decided by the charset: LATAIN charset is 2 bytes per char and UNICODE charset is 3 bytes per char.
    * the null char will use space to pad.
    *

File: serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinaryDataOutputStream.java
Patch:
@@ -138,8 +138,8 @@ public void writeDouble(double d) throws IOException {
    * Write DATE.
    * The representation of date in Teradata binary format is:
    * The Date D is a int with 4 bytes using little endian.
-   * The representation is (YYYYMMDD - 19000000).toInt -> D
-   * eg. 1911.11.11 -> 19111111 -> 111111 -> 07 b2 01 00 in little endian.
+   * The representation is (YYYYMMDD - 19000000).toInt -&gt; D
+   * eg. 1911.11.11 -&gt; 19111111 -&gt; 111111 -&gt; 07 b2 01 00 in little endian.
    * the null date will use 0 to pad.
    *
    * @param date the date
@@ -168,7 +168,7 @@ public void writeLong(long l) throws IOException {
   /**
    * Write CHAR(N).
    * The representation of char in Teradata binary format is:
-   * the byte number to read is based on the [charLength] * [bytePerChar] <- totalLength,
+   * the byte number to read is based on the [charLength] * [bytePerChar] &lt;- totalLength,
    * bytePerChar is decided by the charset: LATAIN charset is 2 bytes per char and UNICODE charset is 3 bytes per char.
    * the null char will use space to pad.
    *

File: service/src/java/org/apache/hive/service/Service.java
Patch:
@@ -49,7 +49,7 @@ public enum STATE {
    * The transition must be from {@link STATE#NOTINITED} to {@link STATE#INITED} unless the
    * operation failed and an exception was raised.
    *
-   * @param config
+   * @param conf
    *          the configuration of the service
    */
   void init(HiveConf conf);

File: service/src/java/org/apache/hive/service/auth/HttpAuthUtils.java
Patch:
@@ -78,7 +78,7 @@ public static String getKerberosServiceTicket(String principal, String host, Str
    * @param clientUserName Client User name.
    * @return An unsigned cookie token generated from input parameters.
    * The final cookie generated is of the following format :
-   * cu=<username>&rn=<randomNumber>&s=<cookieSignature>
+   * cu=&lt;username&gt;&amp;rn=&lt;randomNumber&gt;&amp;s=&lt;cookieSignature&gt;
    */
   public static String createCookieToken(String clientUserName) {
     StringBuilder sb = new StringBuilder();

File: service/src/java/org/apache/hive/service/auth/PasswdAuthenticationProvider.java
Patch:
@@ -26,7 +26,7 @@ public interface PasswdAuthenticationProvider {
    * to authenticate users for their requests.
    * If a user is to be granted, return nothing/throw nothing.
    * When a user is to be disallowed, throw an appropriate {@link AuthenticationException}.
-   * <p/>
+   * <br>
    * For an example implementation, see {@link LdapAuthenticationProviderImpl}.
    *
    * @param user     The username received over the connection request

File: service/src/java/org/apache/hive/service/auth/TSetIpAddressProcessor.java
Patch:
@@ -31,12 +31,12 @@
 
 /**
  * This class is responsible for setting the ipAddress for operations executed via HiveServer2.
- * <p>
+ * <br>
  * <ul>
  * <li>IP address is only set for operations that calls listeners with hookContext</li>
  * <li>IP address is only set if the underlying transport mechanism is socket</li>
  * </ul>
- * </p>
+ * <br>
  *
  * @see org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext
  */

File: service/src/java/org/apache/hive/service/auth/ldap/CustomQueryFilterFactory.java
Patch:
@@ -30,7 +30,7 @@
  * <br>
  * The produced filter object filters out all users that are not found in the search result
  * of the query provided in Hive configuration.
- * @see HiveConf.ConfVars.HIVE_SERVER2_PLAIN_LDAP_CUSTOMLDAPQUERY
+ * @see org.apache.hadoop.hive.conf.HiveConf.ConfVars#HIVE_SERVER2_PLAIN_LDAP_CUSTOMLDAPQUERY
  */
 public class CustomQueryFilterFactory implements FilterFactory {
 

File: service/src/java/org/apache/hive/service/auth/ldap/GroupFilterFactory.java
Patch:
@@ -35,7 +35,7 @@
  * <br>
  * The produced filter object filters out all users that are not members of at least one of
  * the groups provided in Hive configuration.
- * @see HiveConf.ConfVars.HIVE_SERVER2_PLAIN_LDAP_GROUPFILTER
+ * @see HiveConf.ConfVars#HIVE_SERVER2_PLAIN_LDAP_GROUPFILTER
  */
 public final class GroupFilterFactory implements FilterFactory {
 

File: service/src/java/org/apache/hive/service/auth/ldap/SearchResultHandler.java
Patch:
@@ -147,7 +147,7 @@ public void handle(RecordProcessor processor) throws NamingException {
    * Implementations of this interface perform the actual work of processing each record,
    * but don't need to worry about exception handling, closing underlying data structures,
    * and combining results from several search requests.
-   * {@see SearchResultHandler}
+   * @see SearchResultHandler
    */
   public interface RecordProcessor {
 

File: service/src/java/org/apache/hive/service/auth/ldap/UserFilterFactory.java
Patch:
@@ -30,7 +30,7 @@
  * <br>
  * The produced filter object filters out all users that are not on the provided in
  * Hive configuration list.
- * @see HiveConf.ConfVars.HIVE_SERVER2_PLAIN_LDAP_USERFILTER
+ * @see HiveConf.ConfVars#HIVE_SERVER2_PLAIN_LDAP_USERFILTER
  */
 public final class UserFilterFactory implements FilterFactory {
 

File: service/src/java/org/apache/hive/service/cli/CLIServiceUtils.java
Patch:
@@ -31,7 +31,7 @@ public class CLIServiceUtils {
    * Convert a SQL search pattern into an equivalent Java Regex.
    *
    * @param pattern input which may contain '%' or '_' wildcard characters, or
-   * these characters escaped using {@link #getSearchStringEscape()}.
+   * these characters escaped using getSearchStringEscape().
    * @return replace %/_ with regex search characters, also handle escaped
    * characters.
    */

File: service/src/java/org/apache/hive/service/cli/operation/ClassicTableTypeMapping.java
Patch:
@@ -36,9 +36,9 @@
 /**
  * ClassicTableTypeMapping.
  * Classic table type mapping :
- *  Managed Table ==> Table
- *  External Table ==> Table
- *  Virtual View ==> View
+ *  Managed Table ==&gt; Table
+ *  External Table ==&gt; Table
+ *  Virtual View ==&gt; View
  */
 public class ClassicTableTypeMapping implements TableTypeMapping {
 

File: service/src/java/org/apache/hive/service/cli/operation/TableTypeMapping.java
Patch:
@@ -35,7 +35,7 @@ public interface TableTypeMapping {
 
   /**
    * Map hive's table type name to client's table type
-   * @param clientTypeName
+   * @param hiveTypeName
    * @return
    */
   public String mapToClientType (String hiveTypeName);

File: service/src/java/org/apache/hive/service/server/ThreadFactoryWithGarbageCleanup.java
Patch:
@@ -30,12 +30,12 @@
  * in custom cleanup code to be called before this thread is GC-ed.
  * Currently cleans up the following:
  * 1. ThreadLocal RawStore object:
- * In case of an embedded metastore, HiveServer2 threads (foreground & background)
+ * In case of an embedded metastore, HiveServer2 threads (foreground &amp; background)
  * end up caching a ThreadLocal RawStore object. The ThreadLocal RawStore object has
- * an instance of PersistenceManagerFactory & PersistenceManager.
+ * an instance of PersistenceManagerFactory &amp; PersistenceManager.
  * The PersistenceManagerFactory keeps a cache of PersistenceManager objects,
  * which are only removed when PersistenceManager#close method is called.
- * HiveServer2 uses ExecutorService for managing thread pools for foreground & background threads.
+ * HiveServer2 uses ExecutorService for managing thread pools for foreground &amp; background threads.
  * ExecutorService unfortunately does not provide any hooks to be called,
  * when a thread from the pool is terminated.
  * As a solution, we're using this ThreadFactory to keep a cache of RawStore objects per thread.

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaHook.java
Patch:
@@ -35,7 +35,7 @@
  *
  *<p>
  *
- * Implementations can use {@link MetaStoreUtils#isExternalTable} to
+ * Implementations can use {@link org.apache.hadoop.hive.metastore.utils.MetaStoreUtils#isExternalTable} to
  * distinguish external tables from managed tables.
  */
 @InterfaceAudience.Public

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/Warehouse.java
Patch:
@@ -590,7 +590,7 @@ public static Map<String, String> makeEscSpecFromName(String name) throws MetaEx
    * pairs to create the Path for the partition directory
    *
    * @param db - parent database which is used to get the base location of the partition directory
-   * @param tableName - table name for the partitions
+   * @param table - table for the partitions
    * @param pm - Partition key value pairs
    * @return
    * @throws MetaException

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/FilterUtils.java
Patch:
@@ -325,7 +325,6 @@ public static List<String> filterCatalogNamesIfEnabled(
    * could improve performance when filtering partitions.
    * @param dbName the database name
    * @param tblName the table name contained in the database
-   * @return if the
    * @throws NoSuchObjectException if the database or table is filtered out
    */
   public static void checkDbAndTableFilters(boolean isFilterEnabled,

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/PartitionIterable.java
Patch:
@@ -29,10 +29,10 @@
 
 
 /**
- * PartitionIterable - effectively a lazy Iterable<Partition>
+ * PartitionIterable - effectively a lazy Iterable&lt;Partition&gt;
  * Sometimes, we have a need for iterating through a list of partitions,
  * but the list of partitions can be too big to fetch as a single object.
- * Thus, the goal of PartitionIterable is to act as an Iterable<Partition>
+ * Thus, the goal of PartitionIterable is to act as an Iterable&lt;Partition&gt;
  * while lazily fetching each relevant partition, one after the other as
  * independent metadata calls.
  * It is very likely that any calls to PartitionIterable are going to result
@@ -133,7 +133,7 @@ enum Type {
   /**
    * Dummy constructor, which simply acts as an iterator on an already-present
    * list of partitions, allows for easy drop-in replacement for other methods
-   * that already have a List<Partition>
+   * that already have a List&lt;Partition&gt;
    */
   public PartitionIterable(Collection<Partition> ptnsProvided) {
     this.currType = Type.LIST_PROVIDED;

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/events/OpenTxnEvent.java
Patch:
@@ -43,7 +43,7 @@ public OpenTxnEvent(List<Long> txnIds, IHMSHandler handler) {
   }
 
   /**
-   * @return List<Long> txnIds
+   * @return List&lt;Long&gt; txnIds
    */
   public List<Long> getTxnIds() {
     return txnIds;

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionInfo.java
Patch:
@@ -51,8 +51,8 @@ public class CompactionInfo implements Comparable<CompactionInfo> {
   public String properties;
   public boolean tooManyAborts = false;
   /**
+   * The highest write id that the compaction job will pay attention to.
    * {@code 0} means it wasn't set (e.g. in case of upgrades, since ResultSet.getLong() will return 0 if field is NULL) 
-   * See {@link TxnStore#setCompactionHighestWriteId(CompactionInfo, long)} for precise definition.
    * See also {@link TxnUtils#createValidCompactWriteIdList(TableValidWriteIds)} and
    * {@link ValidCompactorWriteIdList#highWatermark}.
    */

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java
Patch:
@@ -124,7 +124,7 @@ public static TxnStore getTxnStore(Configuration conf) {
    * Note, users are responsible for using the correct TxnManager. We do not look at
    * SessionState.get().getTxnMgr().supportsAcid() here
    * Should produce the same result as
-   * {@link org.apache.hadoop.hive.ql.io.AcidUtils#isTransactionalTable(org.apache.hadoop.hive.ql.metadata.Table)}.
+   * org.apache.hadoop.hive.ql.io.AcidUtils#isTransactionalTable.
    * @return true if table is a transactional table, false otherwise
    */
   public static boolean isTransactionalTable(Table table) {
@@ -147,7 +147,7 @@ public static boolean isTransactionalTable(Map<String, String> parameters) {
 
   /**
    * Should produce the same result as
-   * {@link org.apache.hadoop.hive.ql.io.AcidUtils#isAcidTable(org.apache.hadoop.hive.ql.metadata.Table)}.
+   * org.apache.hadoop.hive.ql.io.AcidUtils#isAcidTable.
    */
   public static boolean isAcidTable(Table table) {
     return TxnUtils.isTransactionalTable(table) &&
@@ -156,7 +156,7 @@ public static boolean isAcidTable(Table table) {
   }
 
   /**
-   * Should produce the result as <dbName>.<tableName>.
+   * Should produce the result as &lt;dbName&gt;.&lt;tableName&gt;.
    */
   public static String getFullTableName(String dbName, String tableName) {
     return dbName.toLowerCase() + "." + tableName.toLowerCase();

File: storage-api/src/java/org/apache/hadoop/hive/common/io/FileMetadataCache.java
Patch:
@@ -57,8 +57,6 @@ MemoryBufferOrBuffers putFileMetadata(
   /**
    * Puts the metadata for a given file (e.g. a footer buffer into cache).
    * @param fileKey The file key.
-   * @param length The footer length.
-   * @param is The stream to read the footer from.
    * @return The buffer or buffers representing the cached footer.
    *         The caller must decref this buffer when done.
    */

File: streaming/src/java/org/apache/hive/streaming/HiveStreamingConnection.java
Patch:
@@ -63,12 +63,12 @@
  * To bind to the correct metastore, HiveConf object has to be created from hive-site.xml or HIVE_CONF_DIR.
  * If hive conf is manually created, metastore uri has to be set correctly. If hive conf object is not specified,
  * "thrift://localhost:9083" will be used as default.
- * <br/><br/>
+ * <br><br>
  * NOTE: The streaming connection APIs and record writer APIs are not thread-safe. Streaming connection creation,
  * begin/commit/abort transactions, write and close has to be called in the same thread. If close() or
  * abortTransaction() has to be triggered from a separate thread it has to be co-ordinated via external variables or
  * synchronization mechanism
- * <br/><br/>
+ * <br><br>
  * Example usage:
  * <pre>{@code
  * // create delimited record writer whose schema exactly matches table schema

File: streaming/src/java/org/apache/hive/streaming/StreamingTransaction.java
Patch:
@@ -119,7 +119,7 @@ void commit(@Nullable Set<String> partitions, @Nullable String key,
   Set<String> getPartitions();
 
   /**
-   * @return get the paris for transaction ids <--> write ids
+   * @return get the paris for transaction ids &lt;--&gt; write ids
    */
   List<TxnToWriteId> getTxnToWriteIds();
 }

File: testutils/src/java/org/apache/hive/testutils/jdbc/HiveBurnInClient.java
Patch:
@@ -34,7 +34,7 @@ public class HiveBurnInClient {
   /**
    * Creates 2 tables to query from
    *
-   * @param num
+   * @param con
    */
   public static void createTables(Connection con) throws SQLException {
     Statement stmt = con.createStatement();

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliConfigs.java
Patch:
@@ -65,6 +65,7 @@ public CliConfig() {
         excludeQuery("udaf_corr.q"); // disabled in HIVE-20741
         excludeQuery("udaf_histogram_numeric.q"); // disabled in HIVE-20715
         excludeQuery("stat_estimate_related_col.q"); // disabled in HIVE-20727
+        excludeQuery("vector_groupby_reduce.q"); // Disabled in HIVE-21396
 
         setResultsDir("ql/src/test/results/clientpositive");
         setLogDir("itests/qtest/target/qfile-results/clientpositive");

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/AggregateStatsCache.java
Patch:
@@ -253,6 +253,7 @@ private AggrColStats findBestMatch(List<String> partNames, List<AggrColStats> ca
         // Check if this is the best match so far
         if (matchStats.hits > bestMatchHits) {
           bestMatch = candidate;
+          bestMatchHits = matchStats.hits;
         }
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/TruncDateFromDate.java
Patch:
@@ -1,5 +1,4 @@
 /*
- * Licensed to the Apache Software Foundation (ASF) under one
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/TruncDateFromString.java
Patch:
@@ -1,5 +1,4 @@
 /*
- * Licensed to the Apache Software Foundation (ASF) under one
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/TruncDateFromTimestamp.java
Patch:
@@ -1,5 +1,4 @@
 /*
- * Licensed to the Apache Software Foundation (ASF) under one
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/TruncDecimal.java
Patch:
@@ -1,5 +1,4 @@
 /*
- * Licensed to the Apache Software Foundation (ASF) under one
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/TruncDecimalNoScale.java
Patch:
@@ -1,5 +1,4 @@
 /*
- * Licensed to the Apache Software Foundation (ASF) under one
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/TruncFloat.java
Patch:
@@ -1,5 +1,4 @@
 /*
- * Licensed to the Apache Software Foundation (ASF) under one
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/TruncFloatNoScale.java
Patch:
@@ -1,5 +1,4 @@
 /*
- * Licensed to the Apache Software Foundation (ASF) under one
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java
Patch:
@@ -741,10 +741,10 @@ private KeyInterval discoverOriginalKeyBounds(Reader reader, int bucket,
       }
     }
     if (rowOffset > 0) {
-      minKey = new RecordIdentifier(0, bucketProperty, rowOffset - 1);
+      minKey = new RecordIdentifier(tfp.syntheticWriteId, bucketProperty, rowOffset - 1);
     }
     if (!isTail) {
-      maxKey = new RecordIdentifier(0, bucketProperty, rowOffset + rowLength - 1);
+      maxKey = new RecordIdentifier(tfp.syntheticWriteId, bucketProperty, rowOffset + rowLength - 1);
     }
     return new KeyInterval(minKey, maxKey);
   }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1390,7 +1390,7 @@ public static enum ConfVars {
      * @deprecated Use MetastoreConf.AGGREGATE_STATS_CACHE_ENABLED
      */
     @Deprecated
-    METASTORE_AGGREGATE_STATS_CACHE_ENABLED("hive.metastore.aggregate.stats.cache.enabled", true,
+    METASTORE_AGGREGATE_STATS_CACHE_ENABLED("hive.metastore.aggregate.stats.cache.enabled", false,
         "Whether aggregate stats caching is enabled or not."),
     /**
      * @deprecated Use MetastoreConf.AGGREGATE_STATS_CACHE_SIZE

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
Patch:
@@ -266,7 +266,7 @@ public enum ConfVars {
         "hive.metastore.aggregate.stats.cache.clean.until", 0.8,
         "The cleaner thread cleans until cache reaches this % full size."),
     AGGREGATE_STATS_CACHE_ENABLED("metastore.aggregate.stats.cache.enabled",
-        "hive.metastore.aggregate.stats.cache.enabled", true,
+        "hive.metastore.aggregate.stats.cache.enabled", false,
         "Whether aggregate stats caching is enabled or not."),
     AGGREGATE_STATS_CACHE_FPP("metastore.aggregate.stats.cache.fpp",
         "hive.metastore.aggregate.stats.cache.fpp", 0.01,

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -358,7 +358,7 @@ private static void logAuditEvent(String cmd) {
       auditLog.info("ugi={}	ip={}	cmd={}	", ugi.getUserName(), address, cmd);
     }
 
-    private static String getIPAddress() {
+    public static String getIPAddress() {
       if (useSasl) {
         if (saslServer != null && saslServer.getRemoteAddress() != null) {
           return saslServer.getRemoteAddress().getHostAddress();

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/BaseCharUtils.java
Patch:
@@ -22,8 +22,6 @@
 import org.apache.hadoop.hive.common.type.HiveChar;
 import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.serde2.io.HiveBaseCharWritable;
-import org.apache.hadoop.hive.serde2.io.HiveCharWritable;
-import org.apache.hadoop.hive.serde2.io.HiveVarcharWritable;
 
 public class BaseCharUtils {
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1741,14 +1741,14 @@ public static enum ConfVars {
         "How many rows with the same key value should be cached in memory per smb joined table."),
     HIVEGROUPBYMAPINTERVAL("hive.groupby.mapaggr.checkinterval", 100000,
         "Number of rows after which size of the grouping keys/aggregation classes is performed"),
-    HIVEMAPAGGRHASHMEMORY("hive.map.aggr.hash.percentmemory", (float) 0.99,
+    HIVEMAPAGGRHASHMEMORY("hive.map.aggr.hash.percentmemory", (float) 0.5,
         "Portion of total memory to be used by map-side group aggregation hash table"),
     HIVEMAPJOINFOLLOWEDBYMAPAGGRHASHMEMORY("hive.mapjoin.followby.map.aggr.hash.percentmemory", (float) 0.3,
         "Portion of total memory to be used by map-side group aggregation hash table, when this group by is followed by map join"),
     HIVEMAPAGGRMEMORYTHRESHOLD("hive.map.aggr.hash.force.flush.memory.threshold", (float) 0.9,
         "The max memory to be used by map-side group aggregation hash table.\n" +
         "If the memory usage is higher than this number, force to flush data"),
-    HIVEMAPAGGRHASHMINREDUCTION("hive.map.aggr.hash.min.reduction", (float) 0.5,
+    HIVEMAPAGGRHASHMINREDUCTION("hive.map.aggr.hash.min.reduction", (float) 0.99,
         "Hash aggregation will be turned off if the ratio between hash  table size and input rows is bigger than this number. \n" +
         "Set to 1 to make sure hash aggregation is never turned off."),
     HIVEMULTIGROUPBYSINGLEREDUCER("hive.multigroupby.singlereducer", true,

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java
Patch:
@@ -1237,9 +1237,8 @@ public Boolean apply(CallerArguments args) {
     assertEquals(0, replica.getForeignKeyList(replicatedDbName, "t2").size());
 
     // Retry with different dump should fail.
-    CommandProcessorResponse ret = replica.runCommand("REPL LOAD " + replicatedDbName +
-            " FROM '" + tuple2.dumpLocation + "'");
-    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID.getErrorCode());
+    replica.loadFailure(replicatedDbName, tuple2.dumpLocation, null,
+            ErrorMsg.REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID.getErrorCode());
 
     // Verify if create table is not called on table t1 but called for t2 and t3.
     // Also, allow constraint creation only on t1 and t3. Foreign key creation on t2 fails.

File: ql/src/java/org/apache/hadoop/hive/llap/LlapArrowRecordWriter.java
Patch:
@@ -39,7 +39,7 @@
  * calls to the {@link #write(Writable, Writable)} method only serve as a signal that
  * a new batch has been loaded to the associated VectorSchemaRoot.
  * Payload data for writing is indirectly made available by reference:
- * ArrowStreamWriter -> VectorSchemaRoot -> List<FieldVector>
+ * ArrowStreamWriter -&gt; VectorSchemaRoot -&gt; List&lt;FieldVector&gt;
  * i.e. both they key and value are ignored once a reference to the VectorSchemaRoot
  * is obtained.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ComparisonOpMethodResolver.java
Patch:
@@ -28,8 +28,8 @@
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 
 /**
- * The class implements the method resolution for operators like (> < <= >= =
- * <>). The resolution logic is as follows: 1. If one of the parameters is null,
+ * The class implements the method resolution for operators like (&gt; &lt; &lt;= &gt;= =
+ * &lt;&gt;). The resolution logic is as follows: 1. If one of the parameters is null,
  * then it resolves to evaluate(Double, Double) 2. If both of the parameters are
  * of type T, then it resolves to evaluate(T, T) 3. If 1 and 2 fails then it
  * resolves to evaluate(Double, Double).

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -383,8 +383,8 @@ public Collection<String> getStoredStats() {
      * FileSink, in ways similar to the multi file spray, but without knowing the total number of
      * buckets ahead of time.
      *
-     * ROW__ID (1,2[0],3) => bucket_00002
-     * ROW__ID (1,3[0],4) => bucket_00003 etc
+     * ROW__ID (1,2[0],3) =&gt; bucket_00002
+     * ROW__ID (1,3[0],4) =&gt; bucket_00003 etc
      *
      * A new FSP is created for each partition, so this only requires the bucket numbering and that
      * is mapped in directly as an index.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/LateralViewJoinOperator.java
Patch:
@@ -64,7 +64,7 @@
  *
  * The output of select in the left branch and output of the UDTF in the right
  * branch are then sent to the lateral view join (LVJ). In most cases, the UDTF
- * will generate > 1 row for every row received from the TS, while the left
+ * will generate &gt; 1 row for every row received from the TS, while the left
  * select operator will generate only one. For each row output from the TS, the
  * LVJ outputs all possible rows that can be created by joining the row from the
  * left select and one of the rows output from the UDTF.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java
Patch:
@@ -264,7 +264,7 @@ public void tryStoreVectorizedKey(HiveKey key, boolean partColsIsNull, int batch
   /**
    * Get vectorized batch result for particular index.
    * @param batchIndex index of the key in the batch.
-   * @return the result, same as from {@link #tryStoreKey(HiveKey)}
+   * @return the result, same as from {@link TopNHash#tryStoreKey(HiveKey,boolean)}
    */
   public int getVectorizedBatchResult(int batchIndex) {
     int result = batchIndexToResult[batchIndex];
@@ -309,9 +309,8 @@ public int getVectorizedKeyHashCode(int batchIndex) {
   /**
    * Stores the value for the key in the heap.
    * @param index The index, either from tryStoreKey or from tryStoreVectorizedKey result.
-   * @param hasCode hashCode of key, used by ptfTopNHash.
+   * @param hashCode hashCode of key, used by ptfTopNHash.
    * @param value The value to store.
-   * @param keyHash The key hash to store.
    * @param vectorized Whether the result is coming from a vectorized batch.
    */
   public void storeValue(int index, int hashCode, BytesWritable value, boolean vectorized) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -2228,7 +2228,7 @@ public static List<String> getColumnTypes(Properties props) {
    * If there is no db name part, set the current sessions default db
    * @param dbtable
    * @return String array with two elements, first is db name, second is table name
-   * @throws HiveException
+   * @throws SemanticException
    */
   public static String[] getDbTableName(String dbtable) throws SemanticException {
     return getDbTableName(SessionState.get().getCurrentDatabase(), dbtable);
@@ -3941,9 +3941,9 @@ public static String getQualifiedPath(HiveConf conf, Path path) throws HiveExcep
   }
 
   /**
-   * Checks if the current HiveServer2 logging operation level is >= PERFORMANCE.
+   * Checks if the current HiveServer2 logging operation level is &gt;= PERFORMANCE.
    * @param conf Hive configuration.
-   * @return true if current HiveServer2 logging operation level is >= PERFORMANCE.
+   * @return true if current HiveServer2 logging operation level is &gt;= PERFORMANCE.
    * Else, false.
    */
   public static boolean isPerfOrAboveLogging(HiveConf conf) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/errors/ScriptErrorHeuristic.java
Patch:
@@ -28,7 +28,7 @@
  *
  * Conditions to check:
  *
- * 1. "Script failed with code <some number>" is in the log
+ * 1. "Script failed with code &lt;some number&gt;" is in the log
  *
  */
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java
Patch:
@@ -493,7 +493,7 @@ public byte getValueResult(byte[] key, int offset, int length, Result hashMapRes
   }
 
   /**
-   * Take the segment reference from {@link #getValueRefs(byte[], int, List)}
+   * Take the segment reference from getValueRefs(byte[],int,List)
    * result and makes it self-contained - adds byte array where the value is stored, and
    * updates the offset from "global" write buffers offset to offset within that array.
    */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/events/FunctionEvent.java
Patch:
@@ -24,8 +24,8 @@
  * <p>
  * Since the bootstrap and incremental for functions is handled similarly. There
  * is additional work to make sure we pass the event object from both places.
- *
- * @see org.apache.hadoop.hive.ql.parse.repl.load.message.CreateFunctionHandler.FunctionDescBuilder
+ * <p>
+ * FunctionDescBuilder in {@link org.apache.hadoop.hive.ql.parse.repl.load.message.CreateFunctionHandler}
  * would be merged here mostly.
  */
 public interface FunctionEvent extends BootstrapEvent {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/events/filesystem/BootstrapEventsIterator.java
Patch:
@@ -61,7 +61,7 @@
  * 2. Table before partition is not explicitly required as table and partition metadata are in the same file.
  *
  *
- * For future integrations other sources of events like kafka, would require to implement an Iterator<BootstrapEvent>
+ * For future integrations other sources of events like kafka, would require to implement an Iterator&lt;BootstrapEvent&gt;
  *
  */
 public class BootstrapEventsIterator implements Iterator<BootstrapEvent> {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/TaskTracker.java
Patch:
@@ -30,7 +30,7 @@
 /**
  * This class will be responsible to track how many tasks have been created,
  * organization of tasks such that after the number of tasks for next execution are created
- * we create a dependency collection task(DCT) -> another bootstrap task,
+ * we create a dependency collection task(DCT) -&gt; another bootstrap task,
  * and then add DCT as dependent to all existing tasks that are created so the cycle can continue.
  */
 public class TaskTracker {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
Patch:
@@ -1016,7 +1016,7 @@ public Path getDefaultDestDir(Configuration conf) throws LoginException, IOExcep
    * to provide on the cluster as resources for execution.
    *
    * @param conf
-   * @return List<LocalResource> local resources to add to execution
+   * @return List&lt;LocalResource&gt; local resources to add to execution
    * @throws IOException when hdfs operation fails
    * @throws LoginException when getDefaultDestDir fails with the same exception
    */
@@ -1101,7 +1101,7 @@ private static String[] getTempArchivesFromConf(Configuration conf) {
    * @param hdfsDirPathStr Destination directory in HDFS.
    * @param conf Configuration.
    * @param inputOutputJars The file names to localize.
-   * @return List<LocalResource> local resources to add to execution
+   * @return List&lt;LocalResource&gt; local resources to add to execution
    * @throws IOException when hdfs operation fails.
    * @throws LoginException when getDefaultDestDir fails with the same exception
    */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HivePreWarmProcessor.java
Patch:
@@ -47,7 +47,7 @@
  * A simple sleep processor implementation that sleeps for the configured
  * time in milliseconds.
  *
- * @see Config for configuring the HivePreWarmProcessor
+ * @see Configuration for configuring the HivePreWarmProcessor
  */
 public class HivePreWarmProcessor extends AbstractLogicalIOProcessor {
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java
Patch:
@@ -323,7 +323,6 @@ private ColumnVector createColumnVectorFromRowColumnTypeInfos(int columnNum) {
    * Creates a Vectorized row batch and the column vectors.
    *
    * @return VectorizedRowBatch
-   * @throws HiveException
    */
   public VectorizedRowBatch createVectorizedRowBatch()
   {
@@ -381,7 +380,6 @@ public VectorizedRowBatch createVectorizedRowBatch()
    *
    * @param batch
    * @param partitionValues
-   * @throws HiveException
    */
   public void addPartitionColsToBatch(VectorizedRowBatch batch, Object[] partitionValues)
   {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CuckooSetBytes.java
Patch:
@@ -24,7 +24,7 @@
  * A high-performance set implementation used to support fast set membership testing,
  * using Cuckoo hashing. This is used to support fast tests of the form
  *
- *       column IN ( <list-of-values )
+ *       column IN ( list-of-values )
  *
  * For details on the algorithm, see R. Pagh and F. F. Rodler, "Cuckoo Hashing,"
  * Elsevier Science preprint, Dec. 2003. http://www.itu.dk/people/pagh/papers/cuckoo-jour.pdf.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CuckooSetDouble.java
Patch:
@@ -23,7 +23,7 @@
  * A high-performance set implementation used to support fast set membership testing,
  * using Cuckoo hashing. This is used to support fast tests of the form
  *
- *       column IN ( <list-of-values )
+ *       column IN ( list-of-values )
  *
  * For double, we simply layer over the implementation for long. Double.doubleToRawLongBits
  * is used to convert a 64-bit double to a 64-bit long with bit-for-bit fidelity.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CuckooSetLong.java
Patch:
@@ -25,7 +25,7 @@
  * A high-performance set implementation used to support fast set membership testing,
  * using Cuckoo hashing. This is used to support fast tests of the form
  *
- *       column IN ( <list-of-values )
+ *       column IN ( list-of-values )
  *
  * For details on the algorithm, see R. Pagh and F. F. Rodler, "Cuckoo Hashing,"
  * Elsevier Science preprint, Dec. 2003. http://www.itu.dk/people/pagh/papers/cuckoo-jour.pdf.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashKeyRef.java
Patch:
@@ -169,7 +169,6 @@ public static long extractPartialHashCode(long hashCode) {
 
   /**
    * Get partial hash code from the reference word.
-   * @param hashCode
    * @return
    */
   public static long getPartialHashCodeFromRefWord(long refWord) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashMultiSetStore.java
Patch:
@@ -58,7 +58,7 @@ public class VectorMapJoinFastBytesHashMultiSetStore implements MemoryEstimate {
    *      --------------------------------------
    *                                           |
    *                                           v
-   *       <4 bytes's for set membership count> <Key Bytes>
+   *       &lt;4 bytes's for set membership count&gt; &lt;Key Bytes&gt;
    *            COUNT                              KEY
    *
    * NOTE: MultiSetCount.byteLength = 4
@@ -72,7 +72,7 @@ public class VectorMapJoinFastBytesHashMultiSetStore implements MemoryEstimate {
    *      -------------------------------------
    *                                          |
    *                                          v
-   *      <4 byte's for set membership count> [Big Key Length] <Key Bytes>
+   *      &lt;4 byte's for set membership count&gt; [Big Key Length] &lt;Key Bytes&gt;
    *                NEXT (NONE)                optional           KEY
    */
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashSetStore.java
Patch:
@@ -57,7 +57,7 @@ public class VectorMapJoinFastBytesHashSetStore implements MemoryEstimate {
    *      |
    *      |
    *      v
-   *      <Key Bytes>
+   *      &lt;Key Bytes&gt;
    *        KEY
    *
    *  2) One element, general: shows optional big key length.
@@ -68,7 +68,7 @@ public class VectorMapJoinFastBytesHashSetStore implements MemoryEstimate {
    *      |
    *      |
    *      v
-   *      [Big Key Length] <Key Bytes>
+   *      [Big Key Length] &lt;Key Bytes&gt;
    *        optional           KEY
    */
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBase.java
Patch:
@@ -35,7 +35,7 @@
  * A hash map key wrapper for vectorized processing.
  * It stores the key values as primitives in arrays for each supported primitive type.
  * This works in conjunction with
- * {@link org.apache.hadoop.hive.ql.exec.VectorHashKeyWrapperBatch VectorHashKeyWrapperBatch}
+ * {@link VectorHashKeyWrapperBatch}
  * to hash vectorized processing units (batches).
  */
 public abstract class VectorHashKeyWrapperBase extends KeyWrapper {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperGeneral.java
Patch:
@@ -44,7 +44,7 @@
  * A hash map key wrapper for vectorized processing.
  * It stores the key values as primitives in arrays for each supported primitive type.
  * This works in conjunction with
- * {@link org.apache.hadoop.hive.ql.exec.VectorHashKeyWrapperBatch VectorHashKeyWrapperBatch}
+ * {@link VectorHashKeyWrapperBatch}
  * to hash vectorized processing units (batches).
  */
 public class VectorHashKeyWrapperGeneral extends VectorHashKeyWrapperBase {

File: ql/src/java/org/apache/hadoop/hive/ql/io/AcidOutputFormat.java
Patch:
@@ -199,7 +199,7 @@ public Options bucket(int bucket) {
      * Multiple inserts into legacy (pre-acid) tables can generate multiple copies of each bucket
      * file.
      * @see org.apache.hadoop.hive.ql.exec.Utilities#COPY_KEYWORD
-     * @param copyNumber the number of the copy ( > 0)
+     * @param copyNumber the number of the copy ( &gt; 0)
      * @return this
      */
     public Options copyNumber(int copyNumber) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/BucketCodec.java
Patch:
@@ -72,7 +72,7 @@ public int encode(AcidOutputFormat.Options options) {
    * by {@link RecordIdentifier} which includes the {@link RecordIdentifier#getBucketProperty()}
    * which has the actual bucket ID in the high order bits.  This scheme also ensures that 
    * {@link org.apache.hadoop.hive.ql.exec.FileSinkOperator#process(Object, int)} works in case
-   * there numBuckets > numReducers.  (The later could be fixed by changing how writers are
+   * there numBuckets &gt; numReducers.  (The later could be fixed by changing how writers are
    * initialized in "if (fpaths.acidLastBucket != bucketNum) {")
    */
   V1(1) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java
Patch:
@@ -50,7 +50,7 @@
   * data.  The binary search can be used by setting the value of inputFormatSorted in the
   * MapreduceWork to true, but it should only be used if the data is going to a FilterOperator,
   * which filters by comparing a value in the data with a constant, using one of the comparisons
-  * =, <, >, <=, >=.  If the RecordReader's underlying format is an RCFile, this object can perform
+  * =, &lt;, &gt;, &lt;=, &gt;=.  If the RecordReader's underlying format is an RCFile, this object can perform
   * a binary search to find the block to begin reading from, and stop reading once it can be
   * determined no other entries will match the filter.
   */

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveIgnoreKeyTextOutputFormat.java
Patch:
@@ -37,8 +37,8 @@
 import org.apache.hadoop.util.Progressable;
 
 /**
- * HiveIgnoreKeyTextOutputFormat replaces key with null before feeding the <key,
- * value> to TextOutputFormat.RecordWriter.
+ * HiveIgnoreKeyTextOutputFormat replaces key with null before feeding the &lt;key,
+ * value&gt; to TextOutputFormat.RecordWriter.
  *
  */
 public class HiveIgnoreKeyTextOutputFormat<K extends WritableComparable, V extends Writable>

File: ql/src/java/org/apache/hadoop/hive/ql/io/IgnoreKeyTextOutputFormat.java
Patch:
@@ -30,7 +30,7 @@
 import org.apache.hadoop.util.Progressable;
 
 /**
- * This class replaces key with null before feeding the <key, value> to
+ * This class replaces key with null before feeding the &lt;key, value&gt; to
  * TextOutputFormat.RecordWriter.
  * 
  * @deprecated use {@link HiveIgnoreKeyTextOutputFormat} instead}

File: ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java
Patch:
@@ -144,7 +144,7 @@ public void resolveDynamicPartitionStoredAsSubDirsMerge(HiveConf conf,
 
   /**
    * alter table ... concatenate
-   * <p/>
+   * <br>
    * If it is skewed table, use subdirectories in inputpaths.
    */
   public void resolveConcatenateMerge(HiveConf conf) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java
Patch:
@@ -63,8 +63,8 @@
 /**
  * A RecordUpdater where the files are stored as ORC.
  * A note on various record structures: the {@code row} coming in (as in {@link #insert(long, Object)}
- * for example), is a struct like <RecordIdentifier, f1, ... fn> but what is written to the file
- * * is <op, owid, writerId, rowid, cwid, <f1, ... fn>> (see {@link #createEventSchema(ObjectInspector)})
+ * for example), is a struct like &lt;RecordIdentifier, f1, ... fn&gt; but what is written to the file
+ * * is &lt;op, owid, writerId, rowid, cwid, &lt;f1, ... fn&gt;&gt; (see {@link #createEventObjectInspector(ObjectInspector)})
  * So there are OIs here to make the translation.
  */
 public class OrcRecordUpdater implements RecordUpdater {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java
Patch:
@@ -349,7 +349,7 @@ public int getStatementId() {
   /**
    * Note: this is the bucket number as seen in the file name that contains this split.
    * Hive 3.0 encodes a bunch of info in the Acid schema's bucketId attribute.
-   * See: {@link org.apache.hadoop.hive.ql.io.BucketCodec.V1} for details.
+   * See: {@link org.apache.hadoop.hive.ql.io.BucketCodec#V1} for details.
    * @return
    */
   public int getBucketId() {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java
Patch:
@@ -798,10 +798,10 @@ static Path[] getDeleteDeltaDirsFromSplit(OrcSplit orcSplit) throws IOException
   /**
    * There are 2 types of schema from the {@link #baseReader} that this handles.  In the case
    * the data was written to a transactional table from the start, every row is decorated with
-   * transaction related info and looks like <op, owid, writerId, rowid, cwid, <f1, ... fn>>.
+   * transaction related info and looks like &lt;op, owid, writerId, rowid, cwid, &lt;f1, ... fn&gt;&gt;.
    *
    * The other case is when data was written to non-transactional table and thus only has the user
-   * data: <f1, ... fn>.  Then this table was then converted to a transactional table but the data
+   * data: &lt;f1, ... fn&gt;.  Then this table was then converted to a transactional table but the data
    * files are not changed until major compaction.  These are the "original" files.
    *
    * In this case we may need to decorate the outgoing data with transactional column values at

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReader.java
Patch:
@@ -36,7 +36,7 @@ public interface EncodedReader {
    * @param encodings Externally provided metadata (from metadata reader or external cache).
    * @param streams Externally provided metadata (from metadata reader or external cache).
    * @param physicalFileIncludes The array of booleans indicating whether each column should be read.
-   * @param colRgs Arrays of rgs, per column set to true in included, that are to be read.
+   * @param rgs Arrays of rgs, per column set to true in included, that are to be read.
    *               null in each respective position means all rgs for this column need to be read.
    * @param consumer The sink for data that has been read.
    */

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java
Patch:
@@ -36,7 +36,7 @@
 
 /**
  *
- * The ArrayWritableObjectInspector will inspect an ArrayWritable, considering it as a Hive struct.<br />
+ * The ArrayWritableObjectInspector will inspect an ArrayWritable, considering it as a Hive struct.<br>
  * It can also inspect a List if Hive decides to inspect the result of an inspection.
  *
  */

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java
Patch:
@@ -21,8 +21,8 @@
 import org.apache.hadoop.io.Writable;
 
 /**
- * The DeepParquetHiveMapInspector will inspect an ArrayWritable, considering it as a Hive map.<br />
- * It can also inspect a Map if Hive decides to inspect the result of an inspection.<br />
+ * The DeepParquetHiveMapInspector will inspect an ArrayWritable, considering it as a Hive map.<br>
+ * It can also inspect a Map if Hive decides to inspect the result of an inspection.<br>
  * When trying to access elements from the map it will iterate over all keys, inspecting them and comparing them to the
  * desired key.
  *

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.io.Writable;
 
 /**
- * The ParquetHiveArrayInspector will inspect an ArrayWritable, considering it as an Hive array.<br />
+ * The ParquetHiveArrayInspector will inspect an ArrayWritable, considering it as an Hive array.<br>
  * It can also inspect a List if Hive decides to inspect the result of an inspection.
  *
  */

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java
Patch:
@@ -20,7 +20,7 @@
 import org.apache.hadoop.io.Writable;
 
 /**
- * The StandardParquetHiveMapInspector will inspect an ArrayWritable, considering it as a Hive map.<br />
+ * The StandardParquetHiveMapInspector will inspect an ArrayWritable, considering it as a Hive map.<br>
  * It can also inspect a Map if Hive decides to inspect the result of an inspection.
  *
  */

File: ql/src/java/org/apache/hadoop/hive/ql/lib/Node.java
Patch:
@@ -30,7 +30,7 @@ public interface Node {
    * Gets the vector of children nodes. This is used in the graph walker
    * algorithms.
    * 
-   * @return List<? extends Node>
+   * @return List&lt;? extends Node&gt;
    */
   List<? extends Node> getChildren();
 

File: ql/src/java/org/apache/hadoop/hive/ql/lib/RuleExactMatch.java
Patch:
@@ -36,13 +36,13 @@ public class RuleExactMatch implements Rule {
    * The rule specified as operator names separated by % symbols, the left side represents the
    * bottom of the stack.
    *
-   * E.g. TS%FIL%RS -> means
+   * E.g. TS%FIL%RS -&gt; means
    * TableScan Node followed by Filter followed by ReduceSink in the tree, or, in terms of the
    * stack, ReduceSink on top followed by Filter followed by TableScan
    *
    * @param ruleName
    *          name of the rule
-   * @param regExp
+   * @param pattern
    *          string specification of the rule
    **/
   public RuleExactMatch(String ruleName, String[] pattern) {

File: ql/src/java/org/apache/hadoop/hive/ql/lib/RuleRegExp.java
Patch:
@@ -90,7 +90,7 @@ private static boolean patternHasOnlyWildCardChar(String pattern, char wcc) {
 
   /**
    * The rule specified by the regular expression. Note that, the regular
-   * expression is specified in terms of Node name. For eg: TS.*RS -> means
+   * expression is specified in terms of Node name. For eg: TS.*RS -&gt; means
    * TableScan Node followed by anything any number of times followed by
    * ReduceSink
    * 

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/HiveTxnManager.java
Patch:
@@ -177,7 +177,7 @@ void replTableWriteIdState(String validWriteIdList, String dbName, String tableN
    * {@link ValidTxnWriteIdList} object can be passed as string to the processing
    * tasks for use in the reading the data.  This call will return same results as long as validTxnString
    * passed is same.
-   * @param tableList list of tables (<db_name>.<table_name>) read/written by current transaction.
+   * @param tableList list of tables (&lt;db_name&gt;.&lt;table_name&gt;) read/written by current transaction.
    * @param validTxnList snapshot of valid txns for the current txn
    * @return list of valid table write Ids.
    * @throws LockException

File: ql/src/java/org/apache/hadoop/hive/ql/log/PidFilePatternConverter.java
Patch:
@@ -26,13 +26,13 @@
 import org.apache.logging.log4j.core.pattern.ConverterKeys;
 
 /**
- * FilePattern converter that converts %pid pattern to <process-id>@<hostname> information
+ * FilePattern converter that converts %pid pattern to &lt;process-id&gt;@&lt;hostname&gt; information
  * obtained at runtime.
  *
  * Example usage:
- * <RollingFile name="Rolling-default" fileName="test.log" filePattern="test.log.%pid.gz">
+ * &lt;RollingFile name="Rolling-default" fileName="test.log" filePattern="test.log.%pid.gz"&gt;
  *
- * Will generate output file with name containing <process-id>@<hostname> like below
+ * Will generate output file with name containing &lt;process-id&gt;@&lt;hostname&gt; like below
  * test.log.95232@localhost.gz
  */
 @Plugin(name = "PidFilePatternConverter", category = "FileConverter")

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/PartitionIterable.java
Patch:
@@ -25,11 +25,11 @@
 import java.util.Map;
 
 /**
- * PartitionIterable - effectively a lazy Iterable<Partition>
+ * PartitionIterable - effectively a lazy Iterable&lt;Partition&gt;
  *
  * Sometimes, we have a need for iterating through a list of partitions,
  * but the list of partitions can be too big to fetch as a single object.
- * Thus, the goal of PartitionIterable is to act as an Iterable<Partition>
+ * Thus, the goal of PartitionIterable is to act as an Iterable&lt;Partition&gt;
  * while lazily fetching each relevant partition, one after the other as
  * independent metadata calls.
  *
@@ -134,7 +134,7 @@ enum Type {
   /**
    * Dummy constructor, which simply acts as an iterator on an already-present
    * list of partitions, allows for easy drop-in replacement for other methods
-   * that already have a List<Partition>
+   * that already have a List&lt;Partition&gt;
    */
   public PartitionIterable(Collection<Partition> ptnsProvided){
     this.currType = Type.LIST_PROVIDED;

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
Patch:
@@ -681,7 +681,7 @@ private List<FieldSchema> getColsInternal(boolean forMs) {
    * Returns a list of all the columns of the table (data columns + partition
    * columns in that order.
    *
-   * @return List<FieldSchema>
+   * @return List&lt;FieldSchema&gt;
    */
   public List<FieldSchema> getAllCols() {
     ArrayList<FieldSchema> f_list = new ArrayList<FieldSchema>();
@@ -919,7 +919,7 @@ public boolean isMaterializedView() {
   }
 
   /**
-   * Creates a partition name -> value spec map object
+   * Creates a partition name -&gt; value spec map object
    *
    * @param tp
    *          Use the information from this partition.

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatter.java
Patch:
@@ -92,7 +92,6 @@ public void showMaterializedViews(DataOutputStream out, List<Table> materialized
    * @param cols
    * @param isFormatted - describe with formatted keyword
    * @param isExt
-   * @param isPretty
    * @param isOutputPadded - if true, add spacing and indentation
    * @param colStats
    * @param fkInfo  foreign keys information

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java
Patch:
@@ -303,7 +303,7 @@ public static ColumnPrunerScriptProc getScriptProc() {
    * - add column names referenced in WindowFn args and in WindowFn expressions
    *   to the pruned list of the child Select Op.
    * - finally we set the prunedColList on the ColumnPrunerContx;
-   *   and update the RR & signature on the PTFOp.
+   *   and update the RR &amp; signature on the PTFOp.
    */
   public static class ColumnPrunerPTFProc extends ColumnPrunerScriptProc {
     @Override

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcCtx.java
Patch:
@@ -47,7 +47,7 @@
 /**
  * This class implements the processor context for Constant Propagate.
  *
- * ConstantPropagateProcCtx keeps track of propagated constants in a column->const map for each
+ * ConstantPropagateProcCtx keeps track of propagated constants in a column-&gt;const map for each
  * operator, enabling constants to be revolved across operators.
  */
 public class ConstantPropagateProcCtx implements NodeProcessorCtx {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java
Patch:
@@ -120,7 +120,7 @@ private ConstantPropagateProcFactory() {
   /**
    * Get ColumnInfo from column expression.
    *
-   * @param rr
+   * @param rs
    * @param desc
    * @return
    */

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/CountDistinctRewriteProc.java
Patch:
@@ -62,14 +62,14 @@
 
 /**
  * Queries of form : select max(c), count(distinct c) from T; generates a plan
- * of form TS->mGBy->RS->rGBy->FS This plan suffers from a problem that vertex
- * containing rGBy->FS necessarily need to have 1 task. This limitation results
+ * of form TS-&gt;mGBy-&gt;RS-&gt;rGBy-&gt;FS This plan suffers from a problem that vertex
+ * containing rGBy-&gt;FS necessarily need to have 1 task. This limitation results
  * in slow execution because that task gets all the data. This optimization if
  * successful will rewrite above plan to mGby1-rs1-mGby2-mGby3-rs2-rGby1 This
  * introduces extra vertex of mGby2-mGby3-rs2. Note this vertex can have
  * multiple tasks and since we are doing aggregation, output of this must
  * necessarily be smaller than its input, which results in much less data going
- * in to original rGby->FS vertex, which continues to have single task. Also
+ * in to original rGby-&gt;FS vertex, which continues to have single task. Also
  * note on calcite tree we have HiveExpandDistinctAggregatesRule rule which does
  * similar plan transformation but has different conditions which needs to be
  * satisfied. Additionally, we don't do any costing here but this is possibly

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java
Patch:
@@ -176,8 +176,6 @@ public GenMRProcContext() {
    *          hive configuration
    * @param opTaskMap
    *          reducer to task mapping
-   * @param seenOps
-   *          operator already visited
    * @param parseCtx
    *          current parse context
    * @param rootTasks

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/LimitPushdownOptimizer.java
Patch:
@@ -54,7 +54,7 @@
  * If RS is only for limiting rows, RSHash counts row with same key separately.
  * But if RS is for GBY, RSHash should forward all the rows with the same key.
  *
- * Legend : A(a) --> key A, value a, row A(a)
+ * Legend : A(a) --&gt; key A, value a, row A(a)
  *
  * If each RS in mapper tasks is forwarded rows like this
  *

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/PrunerOperatorFactory.java
Patch:
@@ -93,7 +93,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
     /**
      * Generate predicate.
      *
-     * Subclass should implement the function. Please refer to {@link OpProcFactory.FilterPPR}
+     * Subclass should implement the function. Please refer to {@link org.apache.hadoop.hive.ql.optimizer.ppr.OpProcFactory.FilterPPR}
      *
      * @param procCtx
      * @param fop

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java
Patch:
@@ -96,7 +96,7 @@
  * in the query plan and merge them if they met some preconditions.
  *
  *  TS   TS             TS
- *  |    |     ->      /  \
+ *  |    |     -&gt;      /  \
  *  Op   Op           Op  Op
  *
  * <p>Now the rule has been extended to find opportunities to other operators
@@ -105,7 +105,7 @@
  *  TS1   TS2    TS1   TS2            TS1   TS2
  *   |     |      |     |              |     |
  *   |    RS      |    RS              |    RS
- *    \   /        \   /       ->       \   /
+ *    \   /        \   /       -&gt;       \   /
  *   MapJoin      MapJoin              MapJoin
  *      |            |                  /   \
  *      Op           Op                Op   Op

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/SparkMapJoinProcessor.java
Patch:
@@ -37,9 +37,7 @@ public class SparkMapJoinProcessor extends MapJoinProcessor {
    * convert a regular join to a a map-side join.
    *
    * @param conf
-   * @param opParseCtxMap
    * @param op join operator
-   * @param joinTree qb join tree
    * @param bigTablePos position of the source to be read as part of
    *                   map-reduce framework. All other sources are cached in memory
    * @param noCheckOuterJoin

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveCalciteUtil.java
Patch:
@@ -613,7 +613,7 @@ public static boolean orderRelNode(RelNode rel) {
 
   /**
    * Get top level select starting from root. Assumption here is root can only
-   * be Sort & Project. Also the top project should be at most 2 levels below
+   * be Sort &amp; Project. Also the top project should be at most 2 levels below
    * Sort; i.e Sort(Limit)-Sort(OB)-Select
    *
    * @param rootRel

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveProject.java
Patch:
@@ -120,7 +120,7 @@ public static HiveProject create(RelOptCluster cluster, RelNode child, List<? ex
    * are projected multiple times.
    *
    * <p>
-   * This method could optimize the result as {@link #permute} does, but does
+   * This method could optimize the result as permute does, but does
    * not at present.
    *
    * @param rel

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExceptRewriteRule.java
Patch:
@@ -68,8 +68,8 @@
  * have m+n=a, 2m+n=b where m is the #row in R1 and n is the #row in R2 then
  * m=b-a, n=2a-b, m-n=2b-3a
  * if it is except (distinct)
- * then R5 = Fil (b-a>0 && 2a-b=0) R6 = select only keys from R5
- * else R5 = Fil (2b-3a>0) R6 = UDTF (R5) which will explode the tuples based on 2b-3a.
+ * then R5 = Fil (b-a&gt;0 &amp;&amp; 2a-b=0) R6 = select only keys from R5
+ * else R5 = Fil (2b-3a&gt; 0) R6 = UDTF (R5) which will explode the tuples based on 2b-3a.
  * Note that NULLs are handled the same as other values. Please refer to the test cases.
  */
 public class HiveExceptRewriteRule extends RelOptRule {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveInsertExchange4JoinRule.java
Patch:
@@ -46,10 +46,10 @@
 import com.google.common.collect.Sets;
 
 /** Not an optimization rule.
- * Rule to aid in translation from Calcite tree -> Hive tree.
+ * Rule to aid in translation from Calcite tree -&gt; Hive tree.
  * Transforms :
  *   Left     Right                  Left                    Right
- *       \   /           ->             \                   /
+ *       \   /           -&gt;             \                   /
  *       Join                          HashExchange       HashExchange
  *                                             \         /
  *                                                 Join

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HivePointLookupOptimizerRule.java
Patch:
@@ -74,8 +74,8 @@
  *
  * Similarily
  * <pre>
- * v1 <= c1 and c1 <= v2
- * <pre>
+ * v1 &lt;= c1 and c1 &lt;= v2
+ * </pre>
  * is rewritten to <p>c1 between v1 and v2</p>
  */
 public abstract class HivePointLookupOptimizerRule extends RelOptRule {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveReduceExpressionsWithStatsRule.java
Patch:
@@ -52,12 +52,12 @@
  * column statistics (if available).
  *
  * For instance, given the following predicate:
- *   a > 5
+ *   a &gt; 5
  * we can infer that the predicate will evaluate to false if the max
  * value for column a is 4.
  *
  * Currently we support the simplification of:
- *  - =, >=, <=, >, <
+ *  - =, &gt;=, &lt;=, &gt;, &lt;
  *  - IN
  *  - IS_NULL / IS_NOT_NULL
  */

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSubQueryRemoveRule.java
Patch:
@@ -69,9 +69,9 @@
  * <p>Sub-queries are represented by {@link RexSubQuery} expressions.
  *
  * <p>A sub-query may or may not be correlated. If a sub-query is correlated,
- * the wrapped {@link RelNode} will contain a {@link RexCorrelVariable} before
- * the rewrite, and the product of the rewrite will be a {@link Correlate}.
- * The Correlate can be removed using {@link RelDecorrelator}.
+ * the wrapped {@link RelNode} will contain a {@link org.apache.calcite.rex.RexCorrelVariable} before
+ * the rewrite, and the product of the rewrite will be a {@link org.apache.calcite.rel.core.Correlate}.
+ * The Correlate can be removed using {@link org.apache.calcite.sql2rel.RelDecorrelator}.
  */
 public class HiveSubQueryRemoveRule extends RelOptRule {
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCAggregationPushDownRule.java
Patch:
@@ -36,7 +36,7 @@
 
 /**
  * JDBCAggregationPushDownRule convert a {@link org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate}
- * into a {@link org.apache.calcite.adapter.jdbc.JdbcRules.JdbcAggregateRule.JdbcAggregate}
+ * into a {@link JdbcAggregate}
  * and pushes it down below the {@link org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.jdbc.HiveJdbcConverter}
  * operator so it will be sent to the external table.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/jdbc/JDBCProjectPushDownRule.java
Patch:
@@ -33,7 +33,7 @@
 
 /**
  * JDBCProjectPushDownRule convert a {@link org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject}
- * into a {@link org.apache.calcite.adapter.jdbc.JdbcRules.JdbcAggregateRule.JdbcProject}
+ * into a {@link JdbcProject}
  * and pushes it down below the {@link org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.jdbc.HiveJdbcConverter}}
  * operator so it will be sent to the external table.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/views/HiveAggregateIncrementalRewritingRule.java
Patch:
@@ -51,7 +51,7 @@
  *   SELECT a, b, SUM(x) AS s, COUNT(*) AS c --NEW DATA
  *   FROM TAB_A
  *   JOIN TAB_B ON (TAB_A.a = TAB_B.z)
- *   WHERE TAB_A.ROW_ID > 5
+ *   WHERE TAB_A.ROW_ID &gt; 5
  *   GROUP BY a, b) inner_subq
  * GROUP BY a, b;
  *
@@ -61,10 +61,10 @@
  *   SELECT a, b, SUM(x) AS s, COUNT(*) AS c --NEW DATA
  *   FROM TAB_A
  *   JOIN TAB_B ON (TAB_A.a = TAB_B.z)
- *   WHERE TAB_A.ROW_ID > 5
+ *   WHERE TAB_A.ROW_ID &gt; 5
  *   GROUP BY a, b) source
  * ON (mv.a = source.a AND mv.b = source.b)
- * WHEN MATCHED AND mv.c + source.c <> 0
+ * WHEN MATCHED AND mv.c + source.c &lt;&gt; 0
  *   THEN UPDATE SET mv.s = mv.s + source.s, mv.c = mv.c + source.c
  * WHEN NOT MATCHED
  *   THEN INSERT VALUES (source.a, source.b, s, c);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveGBOpConvUtil.java
Patch:
@@ -69,7 +69,7 @@
  * 1. Change the output col/ExprNodeColumn names to external names.<br>
  * 2. Verify if we need to use the "KEY."/"VALUE." in RS cols; switch to
  * external names if possible.<br>
- * 3. In ExprNode & in ColumnInfo the tableAlias/VirtualColumn is specified
+ * 3. In ExprNode &amp; in ColumnInfo the tableAlias/VirtualColumn is specified
  * differently for different GB/RS in pipeline. Remove the different treatments.
  * 4. VirtualColMap needs to be maintained
  *

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationOptimizer.java
Patch:
@@ -203,7 +203,7 @@ private void findPossibleAutoConvertedJoinOperators() throws SemanticException {
   /**
    * Detect correlations and transform the query tree.
    *
-   * @param pactx
+   * @param pctx
    *          current parse context
    * @throws SemanticException
    */

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/PhysicalOptimizer.java
Patch:
@@ -104,7 +104,7 @@ private void initialize(HiveConf hiveConf) {
    * invoke all the resolvers one-by-one, and alter the physical plan.
    *
    * @return PhysicalContext
-   * @throws HiveException
+   * @throws SemanticException
    */
   public PhysicalContext optimize() throws SemanticException {
     for (PhysicalPlanResolver r : resolvers) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java
Patch:
@@ -47,7 +47,6 @@ public class PartExprEvalUtils {
    * Evaluate expression with partition columns
    *
    * @param expr
-   * @param partSpec
    * @param rowObjectInspector
    * @return value returned by the expression
    * @throws HiveException

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java
Patch:
@@ -100,7 +100,7 @@ public String getName() {
   /**
    * For every node in this subtree, make sure it's start/stop token's
    * are set.  Walk depth first, visit bottom up.  Only updates nodes
-   * with at least one token index < 0.
+   * with at least one token index &lt; 0.
    *
    * In contrast to the method in the parent class, this method is
    * iterative.

File: ql/src/java/org/apache/hadoop/hive/ql/parse/AcidExportSemanticAnalyzer.java
Patch:
@@ -77,7 +77,7 @@ protected void analyze(ASTNode tree) throws SemanticException {
    * were generated.  It may also contain insert events that belong to transactions that aborted
    * where the same constraints apply.
    * In order to make the export artifact free of these constraints, the export does a
-   * insert into tmpTable select * from <export table> to filter/apply the events in current
+   * insert into tmpTable select * from &lt;export table&gt; to filter/apply the events in current
    * context and then export the tmpTable.  This export artifact can now be imported into any
    * table on any cluster (subject to schema checks etc).
    * See {@link #analyzeAcidExport(ASTNode)}

File: ql/src/java/org/apache/hadoop/hive/ql/parse/PTFInvocationSpec.java
Patch:
@@ -413,7 +413,7 @@ public void setExpressions(ArrayList<OrderExpression> columns)
 
     /**
      * Add order expressions from the list of expressions in the format of ASTNode
-     * @param args
+     * @param nodes
      */
     public void addExpressions(ArrayList<ASTNode> nodes) {
       for (int i = 0; i < nodes.size(); i++) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSpec.java
Patch:
@@ -210,7 +210,7 @@ public boolean allowEventReplacementInto(Map<String, String> params){
   }
 
   /**
-   * Returns a predicate filter to filter an Iterable<Partition> to return all partitions
+   * Returns a predicate filter to filter an Iterable&lt;Partition&gt; to return all partitions
    * that the current replication event specification is allowed to replicate-replace-into
    */
   public Predicate<Partition> allowEventReplacementInto() {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TableSample.java
Patch:
@@ -120,7 +120,7 @@ public void setDenominator(int den) {
   /**
    * Gets the ON part's expression list.
    * 
-   * @return ArrayList<ASTNode>
+   * @return ArrayList&lt;ASTNode&gt;
    */
   public ArrayList<ASTNode> getExprs() {
     return exprs;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableDesc.java
Patch:
@@ -929,7 +929,7 @@ public boolean getIsCascade() {
   }
 
   /**
-   * @param cascade the isCascade to set
+   * @param isCascade the isCascade to set
    */
   public void setIsCascade(boolean isCascade) {
     this.isCascade = isCascade;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/CreateViewDesc.java
Patch:
@@ -86,12 +86,11 @@ public CreateViewDesc() {
    * @param tblProps
    * @param partColNames
    * @param ifNotExists
-   * @param orReplace
+   * @param replace
    * @param isAlterViewAs
    * @param inputFormat
    * @param outputFormat
    * @param location
-   * @param serName
    * @param serde
    * @param storageHandler
    * @param serdeProps

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ExportWork.java
Patch:
@@ -102,7 +102,7 @@ public MmContext getMmContext() {
    * For exporting Acid table, change the "pointer" to the temp table.
    * This has to be done after the temp table is populated and all necessary Partition objects
    * exist in the metastore.
-   * See {@link org.apache.hadoop.hive.ql.parse.AcidExportAnalyzer#isAcidExport(ASTNode)}
+   * See {@link org.apache.hadoop.hive.ql.parse.AcidExportSemanticAnalyzer#isAcidExport(ASTNode)}
    * for more info.
    */
   public void acidPostProcess(Hive db) throws HiveException {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java
Patch:
@@ -416,7 +416,7 @@ public static List<ExprNodeDesc> resolveJoinKeysAsRSColumns(List<ExprNodeDesc> s
   /**
    * Join keys are expressions based on the select operator. Resolve the expressions so they
    * are based on the ReduceSink operator
-   *   SEL -> RS -> JOIN
+   *   SEL -&gt; RS -&gt; JOIN
    * @param source
    * @param reduceSinkOp
    * @return
@@ -666,10 +666,10 @@ public static PrimitiveTypeInfo deriveMinArgumentCast(
    * @param inputOp
    *          Input Hive Operator
    * @param startPos
-   *          starting position in the input operator schema; must be >=0 and <=
+   *          starting position in the input operator schema; must be &gt;=0 and &lt;=
    *          endPos
    * @param endPos
-   *          end position in the input operator schema; must be >=0.
+   *          end position in the input operator schema; must be &gt;=0.
    * @return List of ExprNodeDesc
    */
   public static ArrayList<ExprNodeDesc> genExprNodeDesc(Operator inputOp, int startPos, int endPos,

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ListBucketingCtx.java
Patch:
@@ -184,7 +184,6 @@ public void setDefaultDirName(String defaultDirName) {
   /**
    * check if list bucketing is enabled.
    *
-   * @param ctx
    * @return
    */
   public  boolean isSkewedStoredAsDir() {
@@ -201,7 +200,6 @@ public  boolean isSkewedStoredAsDir() {
    * 0: not list bucketing
    * int: no. of skewed columns
    *
-   * @param ctx
    * @return
    */
   public  int calculateListBucketingLevel() {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
Patch:
@@ -214,10 +214,10 @@ public void removePathToAlias(Path path){
   }
 
   /**
-   * This is used to display and verify output of "Path -> Alias" in test framework.
+   * This is used to display and verify output of "Path -&gt; Alias" in test framework.
    *
-   * QTestUtil masks "Path -> Alias" and makes verification impossible.
-   * By keeping "Path -> Alias" intact and adding a new display name which is not
+   * QTestUtil masks "Path -&gt; Alias" and makes verification impossible.
+   * By keeping "Path -&gt; Alias" intact and adding a new display name which is not
    * masked by QTestUtil by removing prefix.
    *
    * Notes: we would still be masking for intermediate directories.

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
Patch:
@@ -1008,7 +1008,7 @@ public static String stripQuotes(String val) {
   }
 
   /**
-   * Remove prefix from "Path -> Alias"
+   * Remove prefix from "Path -&gt; Alias"
    * This is required for testing.
    * In order to verify that path is right, we need to display it in expected test result.
    * But, mask pattern masks path with some patterns.

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ShowCreateDatabaseDesc.java
Patch:
@@ -85,7 +85,7 @@ public String getDatabaseName() {
   }
 
   /**
-   * @param databaseName
+   * @param dbName
    *          the dbName to set
    */
   public void setDatabaseName(String dbName) {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ShowFunctionsDesc.java
Patch:
@@ -77,7 +77,7 @@ public ShowFunctionsDesc(Path resFile, String pattern) {
   /**
    * @param pattern
    *          names of tables to show
-   * @param like
+   * @param isLikePattern
    *          is like keyword used
    */
   public ShowFunctionsDesc(Path resFile, String pattern, boolean isLikePattern) {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/SkewedColumnPositionPair.java
Patch:
@@ -32,7 +32,7 @@
  * 1. It's position in table column is 1.
  * 2. It's position in skewed column list is 0.
  *
- * This information will be used in {@FileSinkOperator} generateListBucketingDirName
+ * This information will be used in {@link org.apache.hadoop.hive.ql.exec.FileSinkOperator} generateListBucketingDirName
  */
 public class SkewedColumnPositionPair {
   private int tblColPosition;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/SparkWork.java
Patch:
@@ -279,7 +279,6 @@ public SparkEdgeProperty getEdgeProperty(BaseWork a, BaseWork b) {
   /**
    * connect adds an edge between a and b. Both nodes have
    * to be added prior to calling connect.
-   * @param
    */
   public void connect(BaseWork a, BaseWork b, SparkEdgeProperty edgeProp) {
     workGraph.get(a).add(b);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java
Patch:
@@ -370,7 +370,6 @@ public String[] configureJobConfAndExtractJars(JobConf jobConf) {
   /**
    * connect adds an edge between a and b. Both nodes have
    * to be added prior to calling connect.
-   * @param
    */
   public void connect(BaseWork a, BaseWork b,
       TezEdgeProperty edgeProp) {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/UDTFDesc.java
Patch:
@@ -27,8 +27,8 @@
 
 
 /**
- * All member variables should have a setters and getters of the form get<member
- * name> and set<member name> or else they won't be recreated properly at run
+ * All member variables should have a setters and getters of the form get&lt;member
+ * name&gt; and set&lt;member name&gt; or else they won't be recreated properly at run
  * time.
  *
  */

File: ql/src/java/org/apache/hadoop/hive/ql/ppd/PredicatePushDown.java
Patch:
@@ -53,9 +53,9 @@
  * plan generation adds filters where they are seen but in some instances some
  * of the filter expressions can be pushed nearer to the operator that sees this
  * particular data for the first time. e.g. select a.*, b.* from a join b on
- * (a.col1 = b.col1) where a.col1 > 20 and b.col2 > 40
+ * (a.col1 = b.col1) where a.col1 &gt; 20 and b.col2 &gt; 40
  *
- * For the above query, the predicates (a.col1 > 20) and (b.col2 > 40), without
+ * For the above query, the predicates (a.col1 &gt; 20) and (b.col2 &gt; 40), without
  * predicate pushdown, would be evaluated after the join processing has been
  * done. Suppose the two predicates filter out most of the rows from a and b,
  * the join is unnecessarily processing these rows. With predicate pushdown,

File: ql/src/java/org/apache/hadoop/hive/ql/processors/CommandProcessorResponse.java
Patch:
@@ -28,7 +28,7 @@
  * <code>CommandProcessor</code> interface. Typically <code>errorMessage</code>
  * and <code>SQLState</code> will only be set if the <code>responseCode</code>
  * is not 0.  Note that often {@code responseCode} ends up the exit value of
- * command shell process so should keep it to < 127.
+ * command shell process so should keep it to &lt; 127.
  */
 public class CommandProcessorResponse extends Exception {
   private final int responseCode;

File: ql/src/java/org/apache/hadoop/hive/ql/processors/CryptoProcessor.java
Patch:
@@ -36,7 +36,7 @@
 
 /**
  * This class processes HADOOP commands used for HDFS encryption. It is meant to be run
- * only by Hive unit & queries tests.
+ * only by Hive unit &amp; queries tests.
  */
 public class CryptoProcessor implements CommandProcessor {
   public static final Logger LOG = LoggerFactory.getLogger(CryptoProcessor.class.getName());

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java
Patch:
@@ -169,7 +169,6 @@ public static HivePrivilegeObject getHiveObjectRef(HiveObjectRef privObj) throws
    * Convert authorization plugin principal type to thrift principal type
    * @param type
    * @return
-   * @throws HiveException
    */
   public static PrincipalType getThriftPrincipalType(HivePrincipalType type) {
     if(type == null){

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/PrivilegeType.java
Patch:
@@ -62,7 +62,7 @@ public Integer getToken() {
 
   /**
    * Do case lookup of PrivilegeType associated with this antlr token
-   * @param privilegeName
+   * @param token
    * @return corresponding PrivilegeType
    */
   public static PrivilegeType getPrivTypeByToken(int token) {

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java
Patch:
@@ -146,7 +146,7 @@ public HivePrivilegeObject(HivePrivilegeObjectType type, String dbname, String o
   }
 
   /**
-   * Create HivePrivilegeObject of type {@link HivePrivilegeObjectType.COMMAND_PARAMS}
+   * Create HivePrivilegeObject of type {@link HivePrivilegeObjectType#COMMAND_PARAMS}
    * @param cmdParams
    * @return
    */
@@ -215,7 +215,7 @@ public List<String> getPartKeys() {
   }
 
   /**
-   * Applicable columns in this object, when the type is {@link HivePrivilegeObjectType.TABLE}
+   * Applicable columns in this object, when the type is {@link HivePrivilegeObjectType#TABLE_OR_VIEW}
    * In case of DML read operations, this is the set of columns being used.
    * Column information is not set for DDL operations and for tables being written into
    * @return list of applicable columns
@@ -225,7 +225,7 @@ public List<String> getColumns() {
   }
 
   /**
-   * The class name when the type is {@link HivePrivilegeObjectType.FUNCTION}
+   * The class name when the type is {@link HivePrivilegeObjectType#FUNCTION}
    * @return the class name
    */
   public String getClassName() {

File: ql/src/java/org/apache/hadoop/hive/ql/stats/StatsAggregator.java
Patch:
@@ -31,9 +31,6 @@ public interface StatsAggregator {
   /**
    * This method connects to the temporary storage.
    *
-   * @param hconf
-   *          HiveConf that contains the connection parameters.
-   * @param sourceTask
    * @return true if connection is successful, false otherwise.
    */
   public boolean connect(StatsCollectionContext scc);

File: ql/src/java/org/apache/hadoop/hive/ql/stats/StatsPublisher.java
Patch:
@@ -35,15 +35,12 @@ public interface StatsPublisher {
    * database (if not exist).
    * This method is usually called in the Hive client side rather than by the mappers/reducers
    * so that it is initialized only once.
-   * @param hconf HiveConf that contains the configurations parameters used to connect to
-   * intermediate stats database.
    * @return true if initialization is successful, false otherwise.
    */
   public boolean init(StatsCollectionContext context);
 
   /**
    * This method connects to the intermediate statistics database.
-   * @param hconf HiveConf that contains the connection parameters.
    * @return true if connection is successful, false otherwise.
    */
   public boolean connect(StatsCollectionContext context);

File: ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java
Patch:
@@ -787,7 +787,7 @@ public static boolean containsNonPositives(List<Long> vals) {
   }
 
   /**
-   * Get sum of all values in the list that are >0
+   * Get sum of all values in the list that are &gt;0
    * @param vals
    *          - list of values
    * @return sum

File: ql/src/java/org/apache/hadoop/hive/ql/udf/SettableUDF.java
Patch:
@@ -30,7 +30,6 @@ public interface SettableUDF {
   /**
    * Add data to UDF prior to initialization.
    * An exception may be thrown if the UDF doesn't know what to do with this data.
-   * @param params UDF-specific data to add to the UDF
    */
   void setTypeInfo(TypeInfo typeInfo) throws UDFArgumentException;
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFConv.java
Patch:
@@ -139,7 +139,7 @@ private void char2byte(int radix, int fromPos) {
   }
 
   /**
-   * Convert numbers between different number bases. If toBase>0 the result is
+   * Convert numbers between different number bases. If toBase&gt;0 the result is
    * unsigned, otherwise it is signed.
    *
    */

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFParseUrl.java
Patch:
@@ -36,7 +36,7 @@
  * 'Ref' parse_url('http://facebook.com/path/p1.php?query=1#Ref', 'PROTOCOL')
  * will return 'http' Possible values are
  * HOST,PATH,QUERY,REF,PROTOCOL,AUTHORITY,FILE,USERINFO Also you can get a value
- * of particular key in QUERY, using syntax QUERY:<KEY_NAME> eg: QUERY:k1.
+ * of particular key in QUERY, using syntax QUERY:&lt;KEY_NAME&gt; eg: QUERY:k1.
  */
 @Description(name = "parse_url",
     value = "_FUNC_(url, partToExtract[, key]) - extracts a part from a URL",

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFSign.java
Patch:
@@ -75,7 +75,7 @@ public DoubleWritable evaluate(LongWritable a) {
   /**
    * Get the sign of the decimal input
    *
-   * @param dec decimal input
+   * @param decWritable decimal input
    *
    * @return -1, 0, or 1 representing the sign of the input decimal
    */

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEvaluator.java
Patch:
@@ -46,7 +46,7 @@
  * accept arguments of complex types, and return complex types. 2. It can accept
  * variable length of arguments. 3. It can accept an infinite number of function
  * signature - for example, it's easy to write a GenericUDAF that accepts
- * array<int>, array<array<int>> and so on (arbitrary levels of nesting).
+ * array&lt;int&gt;, array&lt;array&lt;int&gt;&gt; and so on (arbitrary levels of nesting).
  */
 @InterfaceAudience.Public
 @InterfaceStability.Stable

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFLeadLag.java
Patch:
@@ -33,7 +33,7 @@
 import org.apache.hadoop.io.IntWritable;
 
 /**
- * abstract class for Lead & lag UDAFs GenericUDAFLeadLag.
+ * abstract class for Lead &amp; lag UDAFs GenericUDAFLeadLag.
  *
  */
 public abstract class GenericUDAFLeadLag extends AbstractGenericUDAFResolver {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDF.java
Patch:
@@ -64,7 +64,7 @@
  * accept arguments of complex types, and return complex types. 2. It can accept
  * variable length of arguments. 3. It can accept an infinite number of function
  * signature - for example, it's easy to write a GenericUDF that accepts
- * array<int>, array<array<int>> and so on (arbitrary levels of nesting). 4. It
+ * array&lt;int&gt;, array&lt;array&lt;int&gt;&gt; and so on (arbitrary levels of nesting). 4. It
  * can do short-circuit evaluations using DeferedObject.
  */
 @InterfaceAudience.Public
@@ -222,7 +222,7 @@ public void close() throws IOException {
 
   /**
    * Some functions like comparisons may be affected by appearing order of arguments.
-   * This is to convert a function, such as 3 > x to x < 3. The flip function of
+   * This is to convert a function, such as 3 &gt; x to x &lt; 3. The flip function of
    * GenericUDFOPGreaterThan is GenericUDFOPLessThan.
    */
   public GenericUDF flip() {
@@ -233,7 +233,6 @@ public GenericUDF flip() {
    * Gets the negative function of the current one. E.g., GenericUDFOPNotEqual for
    * GenericUDFOPEqual, or GenericUDFOPNull for GenericUDFOPNotNull.
    * @return Negative function
-   * @throws UDFArgumentException
    */
   public GenericUDF negative() {
     throw new UnsupportedOperationException("Negative function doesn't exist for " + getFuncName());

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFConcatWS.java
Patch:
@@ -37,7 +37,7 @@
 
 /**
  * Generic UDF for string function
- * <code>CONCAT_WS(sep, [string | array(string)]+)<code>.
+ * <code>CONCAT_WS(sep, [string | array(string)]+)</code>.
  * This mimics the function from
  * MySQL http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#
  * function_concat-ws

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIf.java
Patch:
@@ -76,7 +76,7 @@
 
 /**
  * IF(expr1,expr2,expr3) <br>
- * If expr1 is TRUE (expr1 <> 0 and expr1 <> NULL) then IF() returns expr2;
+ * If expr1 is TRUE (expr1 &lt;&gt; 0 and expr1 &lt;&gt; NULL) then IF() returns expr2;
  * otherwise it returns expr3. IF() returns a numeric or string value, depending
  * on the context in which it is used.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTimestamp.java
Patch:
@@ -42,7 +42,7 @@
  * GenericUDFTimestamp
  *
  * Example usage:
- * ... CAST(<Timestamp string> as TIMESTAMP) ...
+ * ... CAST(&lt;Timestamp string&gt; as TIMESTAMP) ...
  *
  * Creates a TimestampWritableV2 object using PrimitiveObjectInspectorConverter
  *

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToIntervalDayTime.java
Patch:
@@ -34,7 +34,7 @@
 * GenericUDFIntervalDayTime
 *
 * Example usage:
-* ... CAST(<Interval string> as INTERVAL DAY TO SECOND) ...
+* ... CAST(&lt;Interval string&gt; as INTERVAL DAY TO SECOND) ...
 *
 * Creates a HiveIntervalDayTimeWritable object using PrimitiveObjectInspectorConverter
 *

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToIntervalYearMonth.java
Patch:
@@ -34,7 +34,7 @@
 * GenericUDFIntervalYearMonth
 *
 * Example usage:
-* ... CAST(<Interval string> as INTERVAL YEAR TO MONTH) ...
+* ... CAST(&lt;Interval string&gt; as INTERVAL YEAR TO MONTH) ...
 *
 * Creates a HiveIntervalYearMonthWritable object using PrimitiveObjectInspectorConverter
 *

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTF.java
Patch:
@@ -44,7 +44,7 @@ public abstract class GenericUDTF {
    * Additionally setup GenericUDTF with MapredContext before initializing.
    * This is only called in runtime of MapRedTask.
    *
-   * @param context context
+   * @param mapredContext context
    */
   public void configure(MapredContext mapredContext) {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/MatchPath.java
Patch:
@@ -70,7 +70,7 @@
  * where the first occurrence was LATE, followed by zero or more EARLY flights,
  * followed by a ONTIME or EARLY flight.
  * <li><b>symbols</b> specify a list of name, expression pairs. For e.g.
- * 'LATE', arrival_delay > 0, 'EARLY', arrival_delay < 0 , 'ONTIME', arrival_delay == 0.
+ * 'LATE', arrival_delay &gt; 0, 'EARLY', arrival_delay &lt; 0 , 'ONTIME', arrival_delay == 0.
  * These symbols can be used in the Pattern defined above.
  * <li><b>resultSelectList</b> specified as a select list.
  * The expressions in the selectList are evaluated in the context where all the

File: ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/TableFunctionEvaluator.java
Patch:
@@ -62,7 +62,7 @@
  * Based on Hive {@link GenericUDAFEvaluator}. Break up the responsibility of the old AbstractTableFunction
  * class into a Resolver and Evaluator.
  * <p>
- * The Evaluator also holds onto the {@link TableFunctionDef}. This provides information
+ * The Evaluator also holds onto the {@link PartitionedTableFunctionDef}. This provides information
  * about the arguments to the function, the shape of the Input partition and the Partitioning details.
  * The Evaluator is responsible for providing the 2 execute methods:
  * <ol>

File: ql/src/java/org/apache/hadoop/hive/ql/util/UpgradeTool.java
Patch:
@@ -83,7 +83,7 @@
  * but can be made insert-only transactional tables and generate corresponding Alter Table commands.
  *
  * Note that to convert a table to full CRUD table requires that all files follow a naming
- * convention, namely 0000N_0 or 0000N_0_copy_M, N >= 0, M > 0.  This utility can perform this
+ * convention, namely 0000N_0 or 0000N_0_copy_M, N &gt;= 0, M &gt; 0.  This utility can perform this
  * rename with "execute" option.  It will also produce a script (with and w/o "execute" to
  * perform the renames).
  *

File: ql/src/java/org/apache/hadoop/hive/ql/wm/Expression.java
Patch:
@@ -17,7 +17,7 @@
 
 /**
  * Expression that is defined in triggers.
- * Most expressions will get triggered only after exceeding a limit. As a result, only greater than (>) expression
+ * Most expressions will get triggered only after exceeding a limit. As a result, only greater than (&gt;) expression
  * is supported.
  */
 public interface Expression {
@@ -43,7 +43,7 @@ interface Builder {
   }
 
   /**
-   * Evaluate current value against this expression. Return true if expression evaluates to true (current > limit)
+   * Evaluate current value against this expression. Return true if expression evaluates to true (current &gt; limit)
    * else false otherwise
    *
    * @param current - current value against which expression will be evaluated

File: service/src/java/org/apache/hive/service/auth/ldap/GroupFilterFactory.java
Patch:
@@ -86,7 +86,7 @@ public void apply(DirSearch ldap, String user) throws AuthenticationException {
 
       for (String groupDn : memberOf) {
         String shortName = LdapUtils.getShortName(groupDn);
-        if (groupFilter.contains(shortName)) {
+        if (groupFilter.stream().anyMatch(shortName::equalsIgnoreCase)) {
           LOG.debug("GroupMembershipKeyFilter passes: user '{}' is a member of '{}' group",
               user, groupDn);
           LOG.info("Authentication succeeded based on group membership");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -534,6 +534,7 @@ public final class FunctionRegistry {
     system.registerGenericUDTF("posexplode", GenericUDTFPosExplode.class);
     system.registerGenericUDTF("stack", GenericUDTFStack.class);
     system.registerGenericUDTF("get_splits", GenericUDTFGetSplits.class);
+    system.registerGenericUDTF("get_sql_schema", GenericUDTFGetSQLSchema.class);
 
     //PTF declarations
     system.registerGenericUDF(LEAD_FUNC_NAME, GenericUDFLead.class);

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestFilterHooks.java
Patch:
@@ -356,6 +356,7 @@ protected void testFilterForTables(boolean filterAtServer) throws Exception {
     }
 
     assertEquals(0, client.getTables(DBNAME1, "*").size());
+    assertEquals(0, client.getTables(DBNAME1, "*", TableType.MANAGED_TABLE).size());
     assertEquals(0, client.getAllTables(DBNAME1).size());
     assertEquals(0, client.getTables(DBNAME1, TAB2).size());
   }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1191,7 +1191,7 @@ public static enum ConfVars {
      */
     @Deprecated
     METASTORE_EVENT_MESSAGE_FACTORY("hive.metastore.event.message.factory",
-        "org.apache.hadoop.hive.metastore.messaging.json.JSONMessageEncoder",
+        "org.apache.hadoop.hive.metastore.messaging.json.gzip.GzipJSONMessageEncoder",
         "Factory class for making encoding and decoding messages in the events generated."),
     /**
      * @deprecated Use MetastoreConf.EXECUTE_SET_UGI

File: itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java
Patch:
@@ -274,6 +274,7 @@ public static void connectToMetastore() throws Exception {
     conf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
     conf.setVar(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL, DummyRawStoreFailEvent.class.getName());
     MetastoreConf.setTimeVar(conf, MetastoreConf.ConfVars.EVENT_DB_LISTENER_CLEAN_INTERVAL, CLEANUP_SLEEP_TIME, TimeUnit.SECONDS);
+    MetastoreConf.setVar(conf, MetastoreConf.ConfVars.EVENT_MESSAGE_FACTORY, JSONMessageEncoder.class.getName());
     conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     SessionState.start(new CliSessionState(conf));

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
Patch:
@@ -544,7 +544,7 @@ public enum ConfVars {
             "Alternatively, configure hive.metastore.transactional.event.listeners to ensure both are invoked in same JDO transaction."),
     EVENT_MESSAGE_FACTORY("metastore.event.message.factory",
         "hive.metastore.event.message.factory",
-        "org.apache.hadoop.hive.metastore.messaging.json.JSONMessageEncoder",
+        "org.apache.hadoop.hive.metastore.messaging.json.gzip.GzipJSONMessageEncoder",
         "Factory class for making encoding and decoding messages in the events generated."),
     EVENT_NOTIFICATION_PARAMETERS_EXCLUDE_PATTERNS("metastore.notification.parameters.exclude.patterns",
         "hive.metastore.notification.parameters.exclude.patterns", "",

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
Patch:
@@ -901,13 +901,13 @@ protected void forward(Object row, ObjectInspector rowInspector)
   protected void vectorForward(VectorizedRowBatch batch)
       throws HiveException {
 
-    runTimeNumRows++;
     if (getDone()) {
       return;
     }
 
     // Data structures to store original values
     final int size = batch.size;
+    runTimeNumRows += size;
     final boolean selectedInUse = batch.selectedInUse;
     final boolean saveState = (selectedInUse && multiChildren);
     if (saveState) {

File: ql/src/test/org/apache/hadoop/hive/ql/plan/mapping/TestCounterMapping.java
Patch:
@@ -146,7 +146,7 @@ public void testUsageOfRuntimeInfo() throws ParseException {
     FilterOperator filter2 = filters2.get(0);
 
     assertEquals("original check", 7, filter1.getStatistics().getNumRows());
-    assertEquals("optimized check", 1, filter2.getStatistics().getNumRows());
+    assertEquals("optimized check", 6, filter2.getStatistics().getNumRows());
 
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -1289,7 +1289,6 @@ public Table getTable(final String dbName, final String tableName, boolean throw
       }
     } catch (NoSuchObjectException e) {
       if (throwException) {
-        LOG.error("Table " + dbName + "." + tableName + " not found: " + e.getMessage());
         throw new InvalidTableException(tableName);
       }
       return null;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4443,7 +4443,7 @@ public static enum ConfVars {
             "How frequently to check for idle Spark sessions. Minimum value is 60 seconds."),
     NWAYJOINREORDER("hive.reorder.nway.joins", true,
       "Runs reordering of tables within single n-way join (i.e.: picks streamtable)"),
-    HIVE_MERGE_NWAY_JOINS("hive.merge.nway.joins", true,
+    HIVE_MERGE_NWAY_JOINS("hive.merge.nway.joins", false,
       "Merge adjacent joins into a single n-way join"),
     HIVE_LOG_N_RECORDS("hive.log.every.n.records", 0L, new RangeValidator(0L, null),
       "If value is greater than 0 logs in fixed intervals of size n rather than exponentially."),

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationUtilities.java
Patch:
@@ -387,7 +387,7 @@ protected static SelectOperator replaceReduceSinkWithSelectOperator(ReduceSinkOp
     SelectDesc select = new SelectDesc(childRS.getConf().getValueCols(), childRS.getConf().getOutputValueColumnNames());
 
     Operator<?> parent = getSingleParent(childRS);
-    parent.getChildOperators().clear();
+    parent.removeChild(childRS);
 
     SelectOperator sel = (SelectOperator) OperatorFactory.getAndMakeChild(
             select, new RowSchema(inputRS.getSignature()), parent);

File: llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java
Patch:
@@ -159,7 +159,7 @@ static void populateConfWithLlapProperties(Configuration conf, Properties proper
         conf.set(key, (String) props.getValue());
       } else {
         if (key.startsWith(HiveConf.PREFIX_LLAP) || key.startsWith(HiveConf.PREFIX_HIVE_LLAP)) {
-          LOG.warn("Adding key [{}] even though it is not in the set of known llap-server keys");
+          LOG.warn("Adding key [{}] even though it is not in the set of known llap-server keys", key);
           conf.set(key, (String) props.getValue());
         } else {
           LOG.warn("Ignoring unknown llap server parameter: [{}]", key);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
Patch:
@@ -263,7 +263,7 @@ Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive hiveDb) th
                 && TableType.EXTERNAL_TABLE.equals(tableTuple.object.getTableType())
                 && !conf.getBoolVar(HiveConf.ConfVars.REPL_DUMP_METADATA_ONLY);
             if (shouldWriteExternalTableLocationInfo) {
-              LOG.debug("adding table {} to external tables list");
+              LOG.debug("adding table {} to external tables list", tblName);
               writer.dataLocationDump(tableTuple.object);
             }
             dumpTable(dbName, tblName, validTxnList, dbRoot, bootDumpBeginReplId, hiveDb,

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/CheckConstraint.java
Patch:
@@ -56,8 +56,6 @@ public CheckConstraintCol(String colName, String checkExpression) {
   public CheckConstraint() {}
 
   public CheckConstraint(List<SQLCheckConstraint> checkConstraintsList) {
-    this.tableName = tableName;
-    this.databaseName = databaseName;
     checkConstraints = new TreeMap<String, List<CheckConstraintCol>>();
     checkExpressionList = new ArrayList<>();
     if (checkConstraintsList == null) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/lineage/OpProcFactory.java
Patch:
@@ -271,9 +271,9 @@ private Predicate getPredicate(JoinOperator jop, LineageCtx lctx) {
         }
         int left = conds[i].getLeft();
         int right = conds[i].getRight();
-        if (joinKeys.length < left
+        if (joinKeys.length <= left
             || joinKeys[left].length == 0
-            || joinKeys.length < right
+            || joinKeys.length <= right
             || joinKeys[right].length == 0
             || parents < left
             || parents < right) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -2404,7 +2404,6 @@ private boolean getOnlyStructObjectInspectors(ReduceWork reduceWork,
   @Override
   public PhysicalContext resolve(PhysicalContext physicalContext) throws SemanticException {
 
-    physicalContext = physicalContext;
     hiveConf = physicalContext.getConf();
     planMapper = physicalContext.getContext().getPlanMapper();
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -281,7 +281,6 @@
 import java.io.IOException;
 import java.lang.reflect.Field;
 import java.lang.reflect.InvocationTargetException;
-import java.lang.reflect.UndeclaredThrowableException;
 import java.math.BigDecimal;
 import java.util.AbstractMap.SimpleEntry;
 import java.util.ArrayDeque;
@@ -1655,8 +1654,7 @@ public static boolean resetCause(Throwable target, Throwable newCause) {
   }
 
   private boolean isUselessCause(Throwable t) {
-    return t instanceof RuntimeException || t instanceof InvocationTargetException
-        || t instanceof UndeclaredThrowableException;
+    return t instanceof RuntimeException || t instanceof InvocationTargetException;
   }
 
   private RowResolver genRowResolver(Operator op, QB qb) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AddPartitionHandler.java
Patch:
@@ -54,7 +54,7 @@ public void handle(Context withinContext) throws Exception {
     AddPartitionMessage apm = (AddPartitionMessage) eventMessage;
     org.apache.hadoop.hive.metastore.api.Table tobj = apm.getTableObj();
     if (tobj == null) {
-      LOG.debug("Event#{} was a ADD_PTN_EVENT with no table listed");
+      LOG.debug("Event#{} was a ADD_PTN_EVENT with no table listed", fromEventId());
       return;
     }
 
@@ -65,7 +65,7 @@ public void handle(Context withinContext) throws Exception {
 
     Iterable<org.apache.hadoop.hive.metastore.api.Partition> ptns = apm.getPartitionObjs();
     if ((ptns == null) || (!ptns.iterator().hasNext())) {
-      LOG.debug("Event#{} was an ADD_PTN_EVENT with no partitions");
+      LOG.debug("Event#{} was an ADD_PTN_EVENT with no partitions", fromEventId());
       return;
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/CreateTableHandler.java
Patch:
@@ -47,7 +47,7 @@ public void handle(Context withinContext) throws Exception {
     org.apache.hadoop.hive.metastore.api.Table tobj = eventMessage.getTableObj();
 
     if (tobj == null) {
-      LOG.debug("Event#{} was a CREATE_TABLE_EVENT with no table listed");
+      LOG.debug("Event#{} was a CREATE_TABLE_EVENT with no table listed", fromEventId());
       return;
     }
 

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -924,7 +924,7 @@ public Map<String, List<ColumnStatisticsObj>> getPartitionColumnStatistics(
       List<String> colNames, String validWriteIdList)
       throws NoSuchObjectException, MetaException, TException {
     PartitionsStatsRequest rqst = new PartitionsStatsRequest(dbName, tableName, colNames,
-        partNames);
+        partNames == null ? new ArrayList<String>() : partNames);
     rqst.setCatName(catName);
     rqst.setValidWriteIdList(validWriteIdList);
     return client.get_partitions_statistics_req(rqst).getPartStats();

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -8810,7 +8810,7 @@ public List<ColumnStatistics> getPartitionColumnStatistics(
       List<String> partNames, List<String> colNames,
       String writeIdList)
       throws MetaException, NoSuchObjectException {
-    if (partNames == null && partNames.isEmpty()) {
+    if (partNames == null || partNames.isEmpty()) {
       return null;
     }
     List<ColumnStatistics> allStats = getPartitionColumnStatisticsInternal(
@@ -8897,7 +8897,7 @@ public AggrStats get_aggr_stats_for(String catName, String dbName, String tblNam
     // If the current stats in the metastore doesn't comply with
     // the isolation level of the query, return null.
     if (writeIdList != null) {
-      if (partNames == null && partNames.isEmpty()) {
+      if (partNames == null || partNames.isEmpty()) {
         return null;
       }
 

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -2028,7 +2028,7 @@ public Materialization getMaterializationInvalidationInfo(
         ValidWriteIdList tblValidWriteIdList =
             validReaderWriteIdList.getTableValidWriteIdList(fullyQualifiedName);
         if (tblValidWriteIdList == null) {
-          LOG.warn("ValidWriteIdList for table {} not present in creation metadata, this should not happen");
+          LOG.warn("ValidWriteIdList for table {} not present in creation metadata, this should not happen", fullyQualifiedName);
           return null;
         }
         query.append(" AND (ctc_writeid > " + tblValidWriteIdList.getHighWatermark());

File: hplsql/src/main/java/org/apache/hive/hplsql/Expression.java
Patch:
@@ -148,6 +148,9 @@ else if (ctx.bool_expr_logical_operator().T_OR() != null) {
   public void execBoolSql(HplsqlParser.Bool_exprContext ctx) {
     StringBuilder sql = new StringBuilder();
     if (ctx.T_OPEN_P() != null) {
+      if (ctx.T_NOT() != null) {
+        sql.append(ctx.T_NOT().getText() + " ");
+      }
       sql.append("(");
       sql.append(evalPop(ctx.bool_expr(0)).toString());
       sql.append(")");

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.metastore.api.TxnType;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.thrift.TException;
@@ -155,7 +156,7 @@ public void run() {
         if (ci.runAs == null) {
           ci.runAs = findUserToRunAs(sd.getLocation(), t);
         }
-        long compactorTxnId = msc.openTxns(ci.runAs, 1).getTxn_ids().get(0);
+        long compactorTxnId = msc.openTxn(ci.runAs, TxnType.COMPACTION);
 
         heartbeater = new CompactionHeartbeater(compactorTxnId, fullTableName, conf);
         heartbeater.start();

File: hplsql/src/main/java/org/apache/hive/hplsql/Var.java
Patch:
@@ -327,7 +327,7 @@ else if (type.equalsIgnoreCase("INT") || type.equalsIgnoreCase("INTEGER") || typ
       return Type.BIGINT;
     }
     else if (type.equalsIgnoreCase("CHAR") || type.equalsIgnoreCase("VARCHAR") || type.equalsIgnoreCase("VARCHAR2") || 
-             type.equalsIgnoreCase("STRING") || type.equalsIgnoreCase("XML")) {
+             type.equalsIgnoreCase("STRING") || type.equalsIgnoreCase("XML") || type.equalsIgnoreCase("CHARACTER")) {
       return Type.STRING;
     }
     else if (type.equalsIgnoreCase("DEC") || type.equalsIgnoreCase("DECIMAL") || type.equalsIgnoreCase("NUMERIC") ||

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliConfigs.java
Patch:
@@ -157,7 +157,6 @@ public MiniLlapCliConfig() {
 
         includesFrom(testConfigProps, "minillap.query.files");
         includesFrom(testConfigProps, "minillap.shared.query.files");
-        excludeQuery("cbo_limit.q"); //Disabled in HIVE-20860
 
         setResultsDir("ql/src/test/results/clientpositive/llap");
         setLogDir("itests/qtest/target/qfile-results/clientpositive");
@@ -256,6 +255,7 @@ public MiniLlapLocalCliConfig() {
         excludeQuery("schema_evol_orc_acidvec_part.q"); // Disabled in HIVE-19509
         excludeQuery("schema_evol_orc_vec_part_llap_io.q"); // Disabled in HIVE-19509
         excludeQuery("load_dyn_part3.q"); // Disabled in HIVE-20662. Enable in HIVE-20663.
+        excludeQuery("cbo_limit.q"); //Disabled in HIVE-20860. Enable in HIVE-20972
 
         setResultsDir("ql/src/test/results/clientpositive/llap");
         setLogDir("itests/qtest/target/qfile-results/clientpositive");

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/common/ZooKeeperHiveHelper.java
Patch:
@@ -167,10 +167,10 @@ public CuratorFramework startZookeeperClient(ACLProvider zooKeeperAclProvider,
                 .creatingParentsIfNeeded()
                 .withMode(CreateMode.PERSISTENT)
                 .forPath(ZooKeeperHiveHelper.ZOOKEEPER_PATH_SEPARATOR + rootNamespace);
-        LOG.info("Created the root name space: " + rootNamespace + " on ZooKeeper for HiveServer2");
+        LOG.info("Created the root name space: " + rootNamespace + " on ZooKeeper");
       } catch (KeeperException e) {
         if (e.code() != KeeperException.Code.NODEEXISTS) {
-          LOG.error("Unable to create HiveServer2 namespace: " + rootNamespace + " on ZooKeeper", e);
+          LOG.error("Unable to create namespace: " + rootNamespace + " on ZooKeeper", e);
           throw e;
         }
       }

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreZKBindHost.java
Patch:
@@ -21,11 +21,9 @@
 import org.apache.hadoop.hive.metastore.annotation.MetastoreCheckinTest;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf.ConfVars;
-import org.junit.Ignore;
 import org.junit.Before;
 import org.junit.experimental.categories.Category;
 
-@Ignore("HIVE-21022: disabled until fixed")
 @Category(MetastoreCheckinTest.class)
 public class TestRemoteHiveMetaStoreZKBindHost extends TestRemoteHiveMetaStoreZK {
 

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestE2EScenarios.java
Patch:
@@ -113,7 +113,8 @@ private void dropTable(String tablename) throws Exception {
 
   private void createTable(String tablename, String schema, String partitionedBy, String storageFormat)
       throws Exception {
-   AbstractHCatLoaderTest.createTable(tablename, schema, partitionedBy, driver, storageFormat);
+    AbstractHCatLoaderTest.createTableDefaultDB(tablename, schema, partitionedBy, driver,
+            storageFormat);
   }
 
   private void driverRun(String cmd) throws Exception {

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderComplexSchema.java
Patch:
@@ -96,7 +96,7 @@ private void dropTable(String tablename) throws Exception {
   }
 
   private void createTable(String tablename, String schema, String partitionedBy) throws Exception {
-    AbstractHCatLoaderTest.createTable(tablename, schema, partitionedBy, driver, storageFormat);
+    AbstractHCatLoaderTest.createTableDefaultDB(tablename, schema, partitionedBy, driver, storageFormat);
   }
 
   private void createTable(String tablename, String schema) throws Exception {

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderStorer.java
Patch:
@@ -65,8 +65,8 @@ public void testReadWrite() throws Exception {
       " row format delimited fields terminated by '\t' stored as textfile location '" +
       dataDir.toURI().getPath() + "'", driver);
     AbstractHCatLoaderTest.dropTable(tblName2, driver);
-    AbstractHCatLoaderTest.createTable(tblName2, "my_small_int smallint, my_tiny_int tinyint", null, driver,
-      "textfile");
+    AbstractHCatLoaderTest.createTableDefaultDB(tblName2, "my_small_int smallint, " +
+            "my_tiny_int " + "tinyint", null, driver, "textfile");
 
     LOG.debug("File=" + INPUT_FILE_NAME);
     TestHCatStorer.dumpFile(INPUT_FILE_NAME);

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatStorerMulti.java
Patch:
@@ -82,7 +82,7 @@ private void dropTable(String tablename) throws Exception {
   }
 
   private void createTable(String tablename, String schema, String partitionedBy) throws Exception {
-    AbstractHCatLoaderTest.createTable(tablename, schema, partitionedBy, driver, storageFormat);
+    AbstractHCatLoaderTest.createTableDefaultDB(tablename, schema, partitionedBy, driver, storageFormat);
   }
 
   private void createTable(String tablename, String schema) throws Exception {

File: druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidVectorizedWrapper.java
Patch:
@@ -77,7 +77,7 @@ public DruidVectorizedWrapper(DruidQueryRecordReader reader, Configuration jobCo
     }
 
     druidWritable = baseReader.createValue();
-    rowBoat = new Object[projectedColumns.length];
+    rowBoat = new Object[rbCtx.getDataColumnCount()];
   }
 
   @Override public boolean next(NullWritable nullWritable, VectorizedRowBatch vectorizedRowBatch) throws IOException {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveExpandDistinctAggregatesRule.java
Patch:
@@ -165,7 +165,7 @@ public void onMatch(RelOptRuleCall call) {
     final RelMetadataQuery mq = call.getMetadataQuery();
     if ((nonDistinctCount == 0) && (argListSets.size() == 1)) {
       for (Integer arg : argListSets.iterator().next()) {
-        Set<RelColumnOrigin> colOrigs = mq.getColumnOrigins(aggregate, arg);
+        Set<RelColumnOrigin> colOrigs = mq.getColumnOrigins(aggregate.getInput(), arg);
         if (null != colOrigs) {
           for (RelColumnOrigin colOrig : colOrigs) {
             RelOptHiveTable hiveTbl = (RelOptHiveTable)colOrig.getOriginTable();

File: itests/qtest-druid/src/main/java/org/apache/hive/druid/ForkingDruidNode.java
Patch:
@@ -27,13 +27,10 @@
 
 import java.io.File;
 import java.io.IOException;
-import java.net.URLClassLoader;
-import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
-import java.util.stream.Collectors;
 
 public class ForkingDruidNode extends DruidNode {
   private final static String DEFAULT_JAVA_CMD = "java";

File: service/src/java/org/apache/hive/service/cli/operation/Operation.java
Patch:
@@ -61,7 +61,7 @@ public abstract class Operation {
   private final OperationHandle opHandle;
   public static final FetchOrientation DEFAULT_FETCH_ORIENTATION = FetchOrientation.FETCH_NEXT;
   public static final Logger LOG = LoggerFactory.getLogger(Operation.class.getName());
-  protected boolean hasResultSet;
+  protected Boolean hasResultSet = null;
   protected volatile HiveSQLException operationException;
   protected volatile Future<?> backgroundHandle;
   protected OperationLog operationLog;
@@ -143,7 +143,7 @@ public OperationStatus getStatus() {
   }
 
   public boolean hasResultSet() {
-    return hasResultSet;
+    return hasResultSet == null ? false : hasResultSet;
   }
 
   protected void setHasResultSet(boolean hasResultSet) {

File: service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java
Patch:
@@ -706,7 +706,9 @@ public TGetOperationStatusResp GetOperationStatus(TGetOperationStatusReq req) th
       resp.setTaskStatus(operationStatus.getTaskStatus());
       resp.setOperationStarted(operationStatus.getOperationStarted());
       resp.setOperationCompleted(operationStatus.getOperationCompleted());
-      resp.setHasResultSet(operationStatus.getHasResultSet());
+      if (operationStatus.isHasResultSetIsSet()) {
+        resp.setHasResultSet(operationStatus.getHasResultSet());
+      }
       JobProgressUpdate progressUpdate = operationStatus.jobProgressUpdate();
       ProgressMonitorStatusMapper mapper = ProgressMonitorStatusMapper.DEFAULT;
       if ("tez".equals(hiveConf.getVar(ConfVars.HIVE_EXECUTION_ENGINE))) {

File: service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIServiceClient.java
Patch:
@@ -384,7 +384,7 @@ public OperationStatus getOperationStatus(OperationHandle opHandle, boolean getP
         opException = new HiveSQLException(resp.getErrorMessage(), resp.getSqlState(), resp.getErrorCode());
       }
       return new OperationStatus(opState, resp.getTaskStatus(), resp.getOperationStarted(),
-        resp.getOperationCompleted(), resp.isHasResultSet(), opException);
+        resp.getOperationCompleted(), resp.isSetHasResultSet() ? resp.isHasResultSet() : null, opException);
     } catch (HiveSQLException e) {
       throw e;
     } catch (Exception e) {

File: upgrade-acid/pre-upgrade/src/test/java/org/apache/hadoop/hive/upgrade/acid/TestPreUpgradeTool.java
Patch:
@@ -44,16 +44,13 @@
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TestName;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 import java.io.File;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 public class TestPreUpgradeTool {
-  private static final Logger LOG = LoggerFactory.getLogger(TestPreUpgradeTool.class);
   private static final String TEST_DATA_DIR = new File(System.getProperty("java.io.tmpdir") +
       File.separator + TestPreUpgradeTool.class.getCanonicalName() + "-" + System.currentTimeMillis()
   ).getPath().replaceAll("\\\\", "/");

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliConfigs.java
Patch:
@@ -155,6 +155,7 @@ public MiniLlapCliConfig() {
 
         includesFrom(testConfigProps, "minillap.query.files");
         includesFrom(testConfigProps, "minillap.shared.query.files");
+        excludeQuery("cbo_limit.q"); //Disabled in HIVE-20860
 
         setResultsDir("ql/src/test/results/clientpositive/llap");
         setLogDir("itests/qtest/target/qfile-results/clientpositive");

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/messaging/json/JSONCreateDatabaseMessage.java
Patch:
@@ -51,7 +51,7 @@ public JSONCreateDatabaseMessage(String server, String servicePrincipal, Databas
     try {
       this.dbJson = MessageBuilder.createDatabaseObjJson(db);
     } catch (TException ex) {
-      throw new IllegalArgumentException("Could not serialize Function object", ex);
+      throw new IllegalArgumentException("Could not serialize database object", ex);
     }
     checkValid();
   }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2468,7 +2468,7 @@ public static enum ConfVars {
         "in the number of rows filtered by a certain operator, which in turn might lead to overprovision or\n" +
         "underprovision of resources. This factor is applied to the cardinality estimation of IN clauses in\n" +
         "filter operators."),
-    HIVE_STATS_IN_MIN_RATIO("hive.stats.filter.in.min.ratio", (float) 0.05,
+    HIVE_STATS_IN_MIN_RATIO("hive.stats.filter.in.min.ratio", (float) 0.0f,
         "Output estimation of an IN filter can't be lower than this ratio"),
     HIVE_STATS_UDTF_FACTOR("hive.stats.udtf.factor", (float) 1.0,
         "UDTFs change the number of rows of the output. A common UDTF is the explode() method that creates\n" +

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFLower.java
Patch:
@@ -58,7 +58,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen
 
     if (arguments[0].getCategory() != Category.PRIMITIVE) {
       throw new UDFArgumentException(
-          "LOWER only takes primitive types, got " + argumentOI.getTypeName());
+          "LOWER only takes primitive types, got " + arguments[0].getTypeName());
     }
     argumentOI = (PrimitiveObjectInspector) arguments[0];
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUpper.java
Patch:
@@ -58,7 +58,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen
 
     if (arguments[0].getCategory() != Category.PRIMITIVE) {
       throw new UDFArgumentException(
-          "UPPER only takes primitive types, got " + argumentOI.getTypeName());
+          "UPPER only takes primitive types, got " + arguments[0].getTypeName());
     }
     argumentOI = (PrimitiveObjectInspector) arguments[0];
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java
Patch:
@@ -432,7 +432,7 @@ public static ExprNodeDesc resolveJoinKeysAsRSColumns(ExprNodeDesc source, Opera
     // Find the key/value where the ExprNodeDesc value matches the column we are searching for.
     // The key portion of the entry will be the internal column name for the join key expression.
     for (Map.Entry<String, ExprNodeDesc> mapEntry : reduceSinkOp.getColumnExprMap().entrySet()) {
-      if (mapEntry.getValue().isSame(source)) {
+      if (mapEntry.getValue().equals(source)) {
         String columnInternalName = mapEntry.getKey();
         if (source instanceof ExprNodeColumnDesc) {
           // The join key is a table column. Create the ExprNodeDesc based on this column.

File: service/src/java/org/apache/hive/service/cli/ColumnValue.java
Patch:
@@ -19,14 +19,14 @@
 package org.apache.hive.service.cli;
 
 import java.math.BigDecimal;
-import java.sql.Date;
-import java.sql.Timestamp;
 
+import org.apache.hadoop.hive.common.type.Date;
 import org.apache.hadoop.hive.common.type.HiveChar;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.common.type.HiveVarchar;
+import org.apache.hadoop.hive.common.type.Timestamp;
 import org.apache.hadoop.hive.common.type.TimestampTZ;
 import org.apache.hadoop.hive.serde2.thrift.Type;
 import org.apache.hive.service.rpc.thrift.TBoolValue;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -910,7 +910,7 @@ public static void validateCheckConstraint(List<FieldSchema> cols, List<SQLCheck
       } catch(Exception e) {
         throw new SemanticException(
             ErrorMsg.INVALID_CSTR_SYNTAX.getMsg("Invalid CHECK constraint expression: ")
-                + cc.getCheck_expression() + ". " + e.getMessage());
+                + cc.getCheck_expression() + ". " + e.getMessage(), e);
       }
     }
   }

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -1652,7 +1652,7 @@ public AllocateTableWriteIdsResponse allocateTableWriteIds(AllocateTableWriteIds
         if (transactionalListeners != null) {
           MetaStoreListenerNotifier.notifyEventWithDirectSql(transactionalListeners,
                   EventMessage.EventType.ALLOC_WRITE_ID,
-                  new AllocWriteIdEvent(txnToWriteIds, rqst.getDbName(), rqst.getTableName(), null),
+                  new AllocWriteIdEvent(txnToWriteIds, dbName, tblName, null),
                   dbConn, sqlGenerator);
         }
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3811,7 +3811,7 @@ public static enum ConfVars {
         "Turn on Tez' auto reducer parallelism feature. When enabled, Hive will still estimate data sizes\n" +
         "and set parallelism estimates. Tez will sample source vertices' output sizes and adjust the estimates at runtime as\n" +
         "necessary."),
-    TEZ_LLAP_MIN_REDUCER_PER_EXECUTOR("hive.tez.llap.min.reducer.per.executor", 0.95f,
+    TEZ_LLAP_MIN_REDUCER_PER_EXECUTOR("hive.tez.llap.min.reducer.per.executor", 0.33f,
         "If above 0, the min number of reducers for auto-parallelism for LLAP scheduling will\n" +
         "be set to this fraction of the number of executors."),
     TEZ_MAX_PARTITION_FACTOR("hive.tez.max.partition.factor", 2f,

File: kafka-handler/src/java/org/apache/hadoop/hive/kafka/KafkaSerDe.java
Patch:
@@ -360,12 +360,12 @@ private static class BytesWritableConverter implements BytesConverter<BytesWrita
   }
 
   private static class TextBytesConverter implements BytesConverter<Text> {
-    Text text = new Text();
+    final private Text text = new Text();
     @Override public byte[] getBytes(Text writable) {
       //@TODO  There is no reason to decode then encode the string to bytes really
       //@FIXME this issue with CTRL-CHAR ^0 added by Text at the end of string and Json serd does not like that.
       try {
-        return writable.decode(writable.getBytes(), 0, writable.getLength()).getBytes(Charset.forName("UTF-8"));
+        return Text.decode(writable.getBytes(), 0, writable.getLength()).getBytes(Charset.forName("UTF-8"));
       } catch (CharacterCodingException e) {
         throw new RuntimeException(e);
       }

File: kafka-handler/src/java/org/apache/hadoop/hive/kafka/KafkaTableProperties.java
Patch:
@@ -49,6 +49,7 @@ enum KafkaTableProperties {
    * {@link KafkaOutputFormat.WriteSemantic}.
    */
   WRITE_SEMANTIC_PROPERTY("kafka.write.semantic", KafkaOutputFormat.WriteSemantic.AT_LEAST_ONCE.name()),
+
   /**
    * Table property that indicates if we should commit within the task or delay it to the Metadata Hook Commit call.
    */

File: spark-client/src/main/java/org/apache/hive/spark/client/SparkSubmitSparkClient.java
Patch:
@@ -34,8 +34,6 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-import org.apache.commons.lang3.StringUtils;
-
 import org.apache.hadoop.hive.common.log.LogRedirector;
 import org.apache.hadoop.hive.conf.Constants;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -44,6 +42,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import static org.apache.hive.spark.client.SparkClientUtilities.containsErrorKeyword;
 
 /**
  * Extends the {@link AbstractSparkClient} and launches a child process to run Spark's {@code
@@ -228,7 +227,7 @@ protected Future<Void> launchDriver(String isTesting, RpcServer rpcServer, Strin
           List<String> errorMessages = new ArrayList<>();
           synchronized (childErrorLog) {
             for (String line : childErrorLog) {
-              if (StringUtils.containsIgnoreCase(line, "Error")) {
+              if (containsErrorKeyword(line)) {
                 errorMessages.add("\"" + line + "\"");
               }
             }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1637,7 +1637,7 @@ public static enum ConfVars {
                                                                  + " expressed as multiple of Local FS read cost"),
     HIVE_CBO_SHOW_WARNINGS("hive.cbo.show.warnings", true,
          "Toggle display of CBO warnings like missing column stats"),
-    HIVE_CBO_STATS_CORRELATED_MULTI_KEY_JOINS("hive.cbo.stats.correlated.multi.key.joins", false,
+    HIVE_CBO_STATS_CORRELATED_MULTI_KEY_JOINS("hive.cbo.stats.correlated.multi.key.joins", true,
         "When CBO estimates output rows for a join involving multiple columns, the default behavior assumes" +
             "the columns are independent. Setting this flag to true will cause the estimator to assume" +
             "the columns are correlated."),

File: ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
Patch:
@@ -201,7 +201,7 @@ public void addPathToAlias(Path path, ArrayList<String> aliases){
   public void addPathToAlias(Path path, String newAlias){
     ArrayList<String> aliases = pathToAliases.get(path);
     if (aliases == null) {
-      aliases = new ArrayList<>();
+      aliases = new ArrayList<>(1);
       StringInternUtils.internUriStringsInPath(path);
       pathToAliases.put(path, aliases);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRexExecutorImpl.java
Patch:
@@ -28,7 +28,6 @@
 import org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory;
 import org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter;
 import org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter;
-import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
 import org.slf4j.Logger;
@@ -63,7 +62,7 @@ public void reduce(RexBuilder rexBuilder, List<RexNode> constExps, List<RexNode>
         if (constant != null) {
           try {
             // convert constant back to RexNode
-            reducedValues.add(rexNodeConverter.convert((ExprNodeConstantDesc) constant));
+            reducedValues.add(rexNodeConverter.convert(constant));
           } catch (Exception e) {
             LOG.warn(e.getMessage());
             reducedValues.add(rexNode);

File: serde/src/java/org/apache/hadoop/hive/serde2/MultiDelimitSerDe.java
Patch:
@@ -17,7 +17,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hadoop.hive.contrib.serde2;
+package org.apache.hadoop.hive.serde2;
 
 import java.io.IOException;
 import java.util.List;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -7262,7 +7262,7 @@ protected Operator genFileSinkPlan(String dest, QB qb, Operator input)
 
       lbCtx = constructListBucketingCtx(destinationTable.getSkewedColNames(),
           destinationTable.getSkewedColValues(), destinationTable.getSkewedColValueLocationMaps(),
-          destinationTable.isStoredAsSubDirectories(), conf);
+          destinationTable.isStoredAsSubDirectories());
 
       // Create the work for moving the table
       // NOTE: specify Dynamic partitions in dest_tab for WriteEntity
@@ -7379,7 +7379,7 @@ protected Operator genFileSinkPlan(String dest, QB qb, Operator input)
 
       lbCtx = constructListBucketingCtx(destinationPartition.getSkewedColNames(),
           destinationPartition.getSkewedColValues(), destinationPartition.getSkewedColValueLocationMaps(),
-          destinationPartition.isStoredAsSubDirectories(), conf);
+          destinationPartition.isStoredAsSubDirectories());
       AcidUtils.Operation acidOp = AcidUtils.Operation.NOT_ACID;
       if (destTableIsFullAcid) {
         acidOp = getAcidType(tableDescriptor.getOutputFileFormatClass(), dest);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/incremental/IncrementalLoadEventsIterator.java
Patch:
@@ -44,7 +44,9 @@ public IncrementalLoadEventsIterator(String loadPath, HiveConf conf) throws IOEx
     FileSystem fs = eventPath.getFileSystem(conf);
     eventDirs = fs.listStatus(eventPath, EximUtil.getDirectoryFilter(fs));
     if ((eventDirs == null) || (eventDirs.length == 0)) {
-      throw new IllegalArgumentException("No data to load in path " + loadPath);
+      currentIndex = 0;
+      numEvents = 0;
+      return;
     }
     // For event dump, each sub-dir is an individual event dump.
     // We need to guarantee that the directory listing we got is in order of event id.

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -1942,7 +1942,7 @@ public Partition loadPartition(Path loadPath, Table tbl, Map<String, String> par
           replaceFiles(tbl.getPath(), loadPath, destPath, oldPartPath, getConf(), isSrcLocal,
               isAutoPurge, newFiles, FileUtils.HIDDEN_FILES_PATH_FILTER, needRecycle, isManaged);
         } else {
-          FileSystem fs = tbl.getDataLocation().getFileSystem(conf);
+          FileSystem fs = destPath.getFileSystem(conf);
           copyFiles(conf, loadPath, destPath, fs, isSrcLocal, isAcidIUDoperation,
               (loadFileType == LoadFileType.OVERWRITE_EXISTING), newFiles,
               tbl.getNumBuckets() > 0, isFullAcidTable, isManaged);

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/MetaStoreUtils.java
Patch:
@@ -325,7 +325,7 @@ public static boolean compareFieldColumns(List<FieldSchema> schema1, List<FieldS
 
   public static boolean isArchived(Partition part) {
     Map<String, String> params = part.getParameters();
-    return "TRUE".equalsIgnoreCase(params.get(hive_metastoreConstants.IS_ARCHIVED));
+    return (params != null && "TRUE".equalsIgnoreCase(params.get(hive_metastoreConstants.IS_ARCHIVED)));
   }
 
   public static Path getOriginalLocation(Partition part) {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
Patch:
@@ -394,6 +394,7 @@ public InputSplit[] getSplits(JobConf job, int splits) throws IOException {
       // ensure that both of the partitions are in the complete list.
       String[] dirs = job.get("hive.complete.dir.list").split("\t");
       assertEquals(2, dirs.length);
+      Arrays.sort(dirs);
       assertEquals(true, dirs[0].endsWith("/state=CA"));
       assertEquals(true, dirs[1].endsWith("/state=OR"));
       return super.getSplits(job, splits);

File: jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/dao/DatabaseAccessor.java
Patch:
@@ -24,6 +24,8 @@ public interface DatabaseAccessor {
 
   List<String> getColumnNames(Configuration conf) throws HiveJdbcDatabaseAccessException;
 
+  List<String> getColumnTypes(Configuration conf) throws HiveJdbcDatabaseAccessException;
+
   int getTotalNumberOfRecords(Configuration conf) throws HiveJdbcDatabaseAccessException;
 
   JdbcRecordIterator

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsAutoGatherContext.java
Patch:
@@ -277,7 +277,7 @@ public static boolean canRunAutogatherStats(Operator curr) {
         case VARCHAR:
         case BINARY:
         case DECIMAL:
-          // TODO: Support case DATE:
+        case DATE:
           break;
         default:
           return false;

File: standalone-metastore/metastore-server/src/test/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMergerTest.java
Patch:
@@ -19,8 +19,6 @@
 
 package org.apache.hadoop.hive.metastore.columnstats.merge;
 
-import java.nio.ByteBuffer;
-
 import org.apache.hadoop.hive.metastore.annotation.MetastoreUnitTest;
 import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;
 import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2045,7 +2045,7 @@ public static enum ConfVars {
        "However, if it is on, and the predicted size of the larger input for a given join is greater \n" +
        "than this number, the join will not be converted to a dynamically partitioned hash join. \n" +
        "The value \"-1\" means no limit."),
-    HIVEHASHTABLEKEYCOUNTADJUSTMENT("hive.hashtable.key.count.adjustment", 2.0f,
+    HIVEHASHTABLEKEYCOUNTADJUSTMENT("hive.hashtable.key.count.adjustment", 0.99f,
         "Adjustment to mapjoin hashtable size derived from table and column statistics; the estimate" +
         " of the number of keys is divided by this value. If the value is 0, statistics are not used" +
         "and hive.hashtable.initialCapacity is used instead."),

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1701,6 +1701,8 @@ public static enum ConfVars {
         "If the bucketing/sorting properties of the table exactly match the grouping key, whether to perform \n" +
         "the group by in the mapper by using BucketizedHiveInputFormat. The only downside to this\n" +
         "is that it limits the number of mappers to the number of files."),
+    HIVE_DEFAULT_NULLS_LAST("hive.default.nulls.last", true,
+        "Whether to set NULLS LAST as the default null ordering"),
     HIVE_GROUPBY_POSITION_ALIAS("hive.groupby.position.alias", false,
         "Whether to enable using Column Position Alias in Group By"),
     HIVE_ORDERBY_POSITION_ALIAS("hive.orderby.position.alias", true,

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -1441,12 +1441,14 @@ RelNode logicalPlan() throws SemanticException {
    * @return Optimized SQL text (or null, if failed)
    */
   public String getOptimizedSql(RelNode optimizedOptiqPlan) {
+    boolean nullsLast = HiveConf.getBoolVar(conf, ConfVars.HIVE_DEFAULT_NULLS_LAST);
+    NullCollation nullCollation = nullsLast ? NullCollation.LAST : NullCollation.LOW;
     SqlDialect dialect = new HiveSqlDialect(SqlDialect.EMPTY_CONTEXT
         .withDatabaseProduct(SqlDialect.DatabaseProduct.HIVE)
         .withDatabaseMajorVersion(4) // TODO: should not be hardcoded
         .withDatabaseMinorVersion(0)
         .withIdentifierQuoteString("`")
-        .withNullCollation(NullCollation.LOW)) {
+        .withNullCollation(nullCollation)) {
       @Override
       protected boolean allowsAs() {
         return true;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDeserializer.java
Patch:
@@ -110,7 +110,7 @@ public void initializeWindowing(WindowTableFunctionDef def) throws HiveException
     TableFunctionEvaluator tEval = def.getTFunction();
     WindowingTableFunctionResolver tResolver =
         (WindowingTableFunctionResolver) constructResolver(def.getResolverClassName());
-    tResolver.initialize(ptfDesc, def, tEval);
+    tResolver.initialize(hConf, ptfDesc, def, tEval);
 
 
     /*
@@ -171,7 +171,7 @@ protected void initialize(PartitionedTableFunctionDef def) throws HiveException
     TableFunctionEvaluator tEval = def.getTFunction();
     // TableFunctionResolver tResolver = FunctionRegistry.getTableFunctionResolver(def.getName());
     TableFunctionResolver tResolver = constructResolver(def.getResolverClassName());
-    tResolver.initialize(ptfDesc, def, tEval);
+    tResolver.initialize(hConf, ptfDesc, def, tEval);
 
     /*
      * 3. give Evaluator chance to setup for RawInput execution; setup RawInput shape

File: ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java
Patch:
@@ -143,15 +143,15 @@ public void execute(PTFPartitionIterator<Object> pItr, PTFPartition outP) throws
   private Object evaluateWindowFunction(WindowFunctionDef wFn, int rowToProcess, PTFPartition partition)
       throws HiveException {
     BasePartitionEvaluator partitionEval = wFn.getWFnEval()
-        .getPartitionWindowingEvaluator(wFn.getWindowFrame(), partition, wFn.getArgs(), wFn.getOI());
+        .getPartitionWindowingEvaluator(wFn.getWindowFrame(), partition, wFn.getArgs(), wFn.getOI(), nullsLast);
     return partitionEval.iterate(rowToProcess, ptfDesc.getLlInfo());
   }
 
   // Evaluate the result given a partition
   private Object evaluateFunctionOnPartition(WindowFunctionDef wFn,
       PTFPartition partition) throws HiveException {
     BasePartitionEvaluator partitionEval = wFn.getWFnEval()
-        .getPartitionWindowingEvaluator(wFn.getWindowFrame(), partition, wFn.getArgs(), wFn.getOI());
+        .getPartitionWindowingEvaluator(wFn.getWindowFrame(), partition, wFn.getArgs(), wFn.getOI(), nullsLast);
     return partitionEval.getPartitionAgg();
   }
 

File: itests/hive-unit/src/test/java/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hive.service.cli;
 
+import org.apache.hadoop.hive.UtilsForTest;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService;
 import org.apache.hive.service.cli.thrift.ThriftCLIService;
@@ -39,6 +40,7 @@ public static void setUpBeforeClass() throws Exception {
     HiveConf conf = new HiveConf();
     conf.setBoolean("datanucleus.schema.autoCreateTables", true);
     conf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
+    UtilsForTest.expandHiveConfParams(conf);
     service.init(conf);
     client = new ThriftCLIServiceClient(service);
   }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2793,6 +2793,8 @@ public static enum ConfVars {
         "hive.test.authz.sstd.hs2.mode", false, "test hs2 mode from .q tests", true),
     HIVE_AUTHORIZATION_ENABLED("hive.security.authorization.enabled", false,
         "enable or disable the Hive client authorization"),
+    HIVE_AUTHORIZATION_KERBEROS_USE_SHORTNAME("hive.security.authorization.kerberos.use.shortname", true,
+        "use short name in Kerberos cluster"),
     HIVE_AUTHORIZATION_MANAGER("hive.security.authorization.manager",
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory",
         "The Hive client authorization manager class name. The user defined authorization class should implement \n" +

File: itests/qtest-druid/src/main/java/org/apache/hive/druid/MiniDruidCluster.java
Patch:
@@ -67,8 +67,8 @@ public class MiniDruidCluster extends AbstractService {
                   "druid.indexer.logs.type", "file",
                   "druid.coordinator.asOverlord.enabled", "true",
                   "druid.coordinator.asOverlord.overlordService", "druid/overlord",
-                  "druid.coordinator.period", "PT10S",
-                  "druid.manager.segments.pollDuration", "PT10S"
+                  "druid.coordinator.period", "PT2S",
+                  "druid.manager.segments.pollDuration", "PT2S"
           );
   private static final int MIN_PORT_NUMBER = 60000;
   private static final int MAX_PORT_NUMBER = 65535;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java
Patch:
@@ -131,7 +131,7 @@ public VectorSMBMapJoinOperator(CompilationOpContext ctx, OperatorDesc conf,
 
     List<ExprNodeDesc> keyDesc = desc.getKeys().get(posBigTable);
     keyExpressions = vContext.getVectorExpressions(keyDesc);
-    keyOutputWriters = VectorExpressionWriterFactory.getExpressionWriters(keyDesc);
+    keyOutputWriters = VectorExpressionWriterFactory.getExpressionWriters(keyExpressions);
 
     Map<Byte, List<ExprNodeDesc>> exprs = desc.getExprs();
     bigTableValueExpressions = vContext.getVectorExpressions(exprs.get(posBigTable));

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorRowObject.java
Patch:
@@ -58,7 +58,8 @@ void testVectorRowObject(int caseNum, boolean sort, Random r) throws HiveExcepti
 
     VectorRandomRowSource source = new VectorRandomRowSource();
 
-    source.init(r, VectorRandomRowSource.SupportedTypes.ALL, 4);
+    source.init(r, VectorRandomRowSource.SupportedTypes.ALL, 4,
+        /* allowNulls */ true, /* isUnicodeOk */ true);
 
     VectorizedRowBatchCtx batchContext = new VectorizedRowBatchCtx();
     batchContext.init(source.rowStructObjectInspector(), emptyScratchTypeNames);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/VectorVerifyFast.java
Patch:
@@ -364,9 +364,9 @@ public static void serializeWrite(SerializeWrite serializeWrite,
       case STRING:
       {
         Text value = (Text) object;
-        byte[] stringBytes = value.getBytes();
-        int stringLength = stringBytes.length;
-        serializeWrite.writeString(stringBytes, 0, stringLength);
+        byte[] bytes = value.getBytes();
+        int byteLength = value.getLength();
+        serializeWrite.writeString(bytes, 0, byteLength);
       }
       break;
       case CHAR:

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorIfStatement.java
Patch:
@@ -273,7 +273,8 @@ private void doIfTestsWithDiffColumnScalar(Random random, String typeName,
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initExplicitSchema(
-        random, explicitTypeNameList, /* maxComplexDepth */ 0, /* allowNull */ true,
+        random, explicitTypeNameList, /* maxComplexDepth */ 0,
+        /* allowNull */ true, /* isUnicodeOk */ true,
         explicitDataTypePhysicalVariationList);
 
     List<String> columns = new ArrayList<String>();

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorIndex.java
Patch:
@@ -28,8 +28,6 @@
 import java.util.stream.IntStream;
 
 import org.apache.hadoop.hive.common.type.DataTypePhysicalVariation;
-import org.apache.hadoop.hive.common.type.HiveChar;
-import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory;
@@ -306,7 +304,8 @@ private boolean doIndexOnRandomDataType(Random random,
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ allowNulls,
+        random, generationSpecList, /* maxComplexDepth */ 0,
+        /* allowNull */ allowNulls,  /* isUnicodeOk */ true,
         explicitDataTypePhysicalVariationList);
 
     String[] columnNames = columns.toArray(new String[0]);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorStringConcat.java
Patch:
@@ -191,7 +191,8 @@ private void doStringConcatTestsWithDiffColumnScalar(Random random,
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0,
+        /* allowNull */ true, /* isUnicodeOk */ true,
         explicitDataTypePhysicalVariationList);
 
     Object[][] randomRows = rowSource.randomRows(100000);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorStringUnary.java
Patch:
@@ -149,7 +149,8 @@ private void doTests(Random random, String typeName, String functionName)
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0,
+        /* allowNull */ true, /* isUnicodeOk */ true,
         explicitDataTypePhysicalVariationList);
 
     List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>();

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorSubStr.java
Patch:
@@ -118,7 +118,8 @@ private void doTests(Random random, boolean useLength)
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0,
+        /* allowNull */ true, /* isUnicodeOk */ true,
         explicitDataTypePhysicalVariationList);
 
     List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>();

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTimestampExtract.java
Patch:
@@ -137,7 +137,8 @@ private void doIfTestOneTimestampExtract(Random random, String dateTimeStringTyp
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0,
+        /* allowNull */ true, /* isUnicodeOk */ true,
         explicitDataTypePhysicalVariationList);
 
     List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>();

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -7456,7 +7456,7 @@ private Partition getPartitionObj(String db, String table, List<String> partitio
 
     @Override
     public WriteNotificationLogResponse add_write_notification_log(WriteNotificationLogRequest rqst)
-            throws MetaException, NoSuchObjectException {
+            throws TException {
       Table tableObj = getTblObject(rqst.getDb(), rqst.getTable());
       Partition ptnObj = getPartitionObj(rqst.getDb(), rqst.getTable(), rqst.getPartitionVals(), tableObj);
       addTxnWriteNotificationLog(tableObj, ptnObj, rqst);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2478,7 +2478,7 @@ public static enum ConfVars {
         "allows two concurrent writes to the same partition but still lets lock manager prevent\n" +
         "DROP TABLE etc. when the table is being written to"),
     TXN_OVERWRITE_X_LOCK("hive.txn.xlock.iow", true,
-        "Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for\b" +
+        "Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for\n" +
             "transactional tables.  This ensures that inserts (w/o overwrite) running concurrently\n" +
             "are not hidden by the INSERT OVERWRITE."),
     HIVE_TXN_STATS_ENABLED("hive.txn.stats.enabled", true,

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaVoidObjectInspector.java
Patch:
@@ -32,7 +32,6 @@ public class JavaVoidObjectInspector extends
 
   @Override
   public Object getPrimitiveWritableObject(Object o) {
-    return NullWritable.get();
+    return o == null ? null : NullWritable.get();
   }
-
 }

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java
Patch:
@@ -108,7 +108,7 @@ public void testInsertOverwrite() throws Exception {
     Assert.assertEquals(1, rs.size());
     Assert.assertEquals("1", rs.get(0));
     hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);
-    runStatementOnDriver("insert into " + Table.ACIDTBL + " values(3,2)");
+    runStatementOnDriver("insert overwrite table " + Table.ACIDTBL + " values(3,2)");
     hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);
     runStatementOnDriver("insert into " + Table.ACIDTBL + " values(5,6)");
     rs = runStatementOnDriver("select a from " + Table.ACIDTBL + " order by a");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java
Patch:
@@ -129,7 +129,7 @@ public VectorSMBMapJoinOperator(CompilationOpContext ctx, OperatorDesc conf,
 
     List<ExprNodeDesc> keyDesc = desc.getKeys().get(posBigTable);
     keyExpressions = vContext.getVectorExpressions(keyDesc);
-    keyOutputWriters = VectorExpressionWriterFactory.getExpressionWriters(keyExpressions);
+    keyOutputWriters = VectorExpressionWriterFactory.getExpressionWriters(keyDesc);
 
     Map<Byte, List<ExprNodeDesc>> exprs = desc.getExprs();
     bigTableValueExpressions = vContext.getVectorExpressions(exprs.get(posBigTable));

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorRowObject.java
Patch:
@@ -58,8 +58,7 @@ void testVectorRowObject(int caseNum, boolean sort, Random r) throws HiveExcepti
 
     VectorRandomRowSource source = new VectorRandomRowSource();
 
-    source.init(r, VectorRandomRowSource.SupportedTypes.ALL, 4,
-        /* allowNulls */ true, /* isUnicodeOk */ true);
+    source.init(r, VectorRandomRowSource.SupportedTypes.ALL, 4);
 
     VectorizedRowBatchCtx batchContext = new VectorizedRowBatchCtx();
     batchContext.init(source.rowStructObjectInspector(), emptyScratchTypeNames);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/VectorVerifyFast.java
Patch:
@@ -364,9 +364,9 @@ public static void serializeWrite(SerializeWrite serializeWrite,
       case STRING:
       {
         Text value = (Text) object;
-        byte[] bytes = value.getBytes();
-        int byteLength = value.getLength();
-        serializeWrite.writeString(bytes, 0, byteLength);
+        byte[] stringBytes = value.getBytes();
+        int stringLength = stringBytes.length;
+        serializeWrite.writeString(stringBytes, 0, stringLength);
       }
       break;
       case CHAR:

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorIfStatement.java
Patch:
@@ -273,8 +273,7 @@ private void doIfTestsWithDiffColumnScalar(Random random, String typeName,
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initExplicitSchema(
-        random, explicitTypeNameList, /* maxComplexDepth */ 0,
-        /* allowNull */ true, /* isUnicodeOk */ true,
+        random, explicitTypeNameList, /* maxComplexDepth */ 0, /* allowNull */ true,
         explicitDataTypePhysicalVariationList);
 
     List<String> columns = new ArrayList<String>();

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorIndex.java
Patch:
@@ -28,6 +28,8 @@
 import java.util.stream.IntStream;
 
 import org.apache.hadoop.hive.common.type.DataTypePhysicalVariation;
+import org.apache.hadoop.hive.common.type.HiveChar;
+import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory;
@@ -304,8 +306,7 @@ private boolean doIndexOnRandomDataType(Random random,
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0,
-        /* allowNull */ allowNulls,  /* isUnicodeOk */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ allowNulls,
         explicitDataTypePhysicalVariationList);
 
     String[] columnNames = columns.toArray(new String[0]);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorStringConcat.java
Patch:
@@ -191,8 +191,7 @@ private void doStringConcatTestsWithDiffColumnScalar(Random random,
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0,
-        /* allowNull */ true, /* isUnicodeOk */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ true,
         explicitDataTypePhysicalVariationList);
 
     Object[][] randomRows = rowSource.randomRows(100000);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorStringUnary.java
Patch:
@@ -149,8 +149,7 @@ private void doTests(Random random, String typeName, String functionName)
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0,
-        /* allowNull */ true, /* isUnicodeOk */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ true,
         explicitDataTypePhysicalVariationList);
 
     List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>();

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorSubStr.java
Patch:
@@ -118,8 +118,7 @@ private void doTests(Random random, boolean useLength)
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0,
-        /* allowNull */ true, /* isUnicodeOk */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ true,
         explicitDataTypePhysicalVariationList);
 
     List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>();

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTimestampExtract.java
Patch:
@@ -137,8 +137,7 @@ private void doIfTestOneTimestampExtract(Random random, String dateTimeStringTyp
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0,
-        /* allowNull */ true, /* isUnicodeOk */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ true,
         explicitDataTypePhysicalVariationList);
 
     List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>();

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1683,7 +1683,7 @@ public static enum ConfVars {
         "How many rows with the same key value should be cached in memory per smb joined table."),
     HIVEGROUPBYMAPINTERVAL("hive.groupby.mapaggr.checkinterval", 100000,
         "Number of rows after which size of the grouping keys/aggregation classes is performed"),
-    HIVEMAPAGGRHASHMEMORY("hive.map.aggr.hash.percentmemory", (float) 0.5,
+    HIVEMAPAGGRHASHMEMORY("hive.map.aggr.hash.percentmemory", (float) 0.99,
         "Portion of total memory to be used by map-side group aggregation hash table"),
     HIVEMAPJOINFOLLOWEDBYMAPAGGRHASHMEMORY("hive.mapjoin.followby.map.aggr.hash.percentmemory", (float) 0.3,
         "Portion of total memory to be used by map-side group aggregation hash table, when this group by is followed by map join"),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java
Patch:
@@ -129,7 +129,7 @@ public VectorSMBMapJoinOperator(CompilationOpContext ctx, OperatorDesc conf,
 
     List<ExprNodeDesc> keyDesc = desc.getKeys().get(posBigTable);
     keyExpressions = vContext.getVectorExpressions(keyDesc);
-    keyOutputWriters = VectorExpressionWriterFactory.getExpressionWriters(keyDesc);
+    keyOutputWriters = VectorExpressionWriterFactory.getExpressionWriters(keyExpressions);
 
     Map<Byte, List<ExprNodeDesc>> exprs = desc.getExprs();
     bigTableValueExpressions = vContext.getVectorExpressions(exprs.get(posBigTable));

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorRowObject.java
Patch:
@@ -58,7 +58,8 @@ void testVectorRowObject(int caseNum, boolean sort, Random r) throws HiveExcepti
 
     VectorRandomRowSource source = new VectorRandomRowSource();
 
-    source.init(r, VectorRandomRowSource.SupportedTypes.ALL, 4);
+    source.init(r, VectorRandomRowSource.SupportedTypes.ALL, 4,
+        /* allowNulls */ true, /* isUnicodeOk */ true);
 
     VectorizedRowBatchCtx batchContext = new VectorizedRowBatchCtx();
     batchContext.init(source.rowStructObjectInspector(), emptyScratchTypeNames);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/VectorVerifyFast.java
Patch:
@@ -364,9 +364,9 @@ public static void serializeWrite(SerializeWrite serializeWrite,
       case STRING:
       {
         Text value = (Text) object;
-        byte[] stringBytes = value.getBytes();
-        int stringLength = stringBytes.length;
-        serializeWrite.writeString(stringBytes, 0, stringLength);
+        byte[] bytes = value.getBytes();
+        int byteLength = value.getLength();
+        serializeWrite.writeString(bytes, 0, byteLength);
       }
       break;
       case CHAR:

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorIfStatement.java
Patch:
@@ -273,7 +273,8 @@ private void doIfTestsWithDiffColumnScalar(Random random, String typeName,
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initExplicitSchema(
-        random, explicitTypeNameList, /* maxComplexDepth */ 0, /* allowNull */ true,
+        random, explicitTypeNameList, /* maxComplexDepth */ 0,
+        /* allowNull */ true, /* isUnicodeOk */ true,
         explicitDataTypePhysicalVariationList);
 
     List<String> columns = new ArrayList<String>();

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorIndex.java
Patch:
@@ -28,8 +28,6 @@
 import java.util.stream.IntStream;
 
 import org.apache.hadoop.hive.common.type.DataTypePhysicalVariation;
-import org.apache.hadoop.hive.common.type.HiveChar;
-import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory;
@@ -306,7 +304,8 @@ private boolean doIndexOnRandomDataType(Random random,
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ allowNulls,
+        random, generationSpecList, /* maxComplexDepth */ 0,
+        /* allowNull */ allowNulls,  /* isUnicodeOk */ true,
         explicitDataTypePhysicalVariationList);
 
     String[] columnNames = columns.toArray(new String[0]);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorStringConcat.java
Patch:
@@ -191,7 +191,8 @@ private void doStringConcatTestsWithDiffColumnScalar(Random random,
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0,
+        /* allowNull */ true, /* isUnicodeOk */ true,
         explicitDataTypePhysicalVariationList);
 
     Object[][] randomRows = rowSource.randomRows(100000);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorStringUnary.java
Patch:
@@ -149,7 +149,8 @@ private void doTests(Random random, String typeName, String functionName)
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0,
+        /* allowNull */ true, /* isUnicodeOk */ true,
         explicitDataTypePhysicalVariationList);
 
     List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>();

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorSubStr.java
Patch:
@@ -118,7 +118,8 @@ private void doTests(Random random, boolean useLength)
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0,
+        /* allowNull */ true, /* isUnicodeOk */ true,
         explicitDataTypePhysicalVariationList);
 
     List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>();

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTimestampExtract.java
Patch:
@@ -137,7 +137,8 @@ private void doIfTestOneTimestampExtract(Random random, String dateTimeStringTyp
     VectorRandomRowSource rowSource = new VectorRandomRowSource();
 
     rowSource.initGenerationSpecSchema(
-        random, generationSpecList, /* maxComplexDepth */ 0, /* allowNull */ true,
+        random, generationSpecList, /* maxComplexDepth */ 0,
+        /* allowNull */ true, /* isUnicodeOk */ true,
         explicitDataTypePhysicalVariationList);
 
     List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -998,10 +998,10 @@ private static void generateConstraintInfos(ASTNode child, List<String> columnNa
     // Default values
     String constraintName = null;
     //by default if user hasn't provided any optional constraint properties
-    // it will be considered ENABLE and NOVALIDATE and RELY=false
+    // it will be considered ENABLE and NOVALIDATE and RELY=true
     boolean enable = true;
     boolean validate = false;
-    boolean rely = false;
+    boolean rely = true;
     String checkOrDefaultValue = null;
     for (int i = 0; i < child.getChildCount(); i++) {
       ASTNode grandChild = (ASTNode) child.getChild(i);
@@ -1018,6 +1018,7 @@ private static void generateConstraintInfos(ASTNode child, List<String> columnNa
         enable = false;
         // validate is false by default if we disable the constraint
         validate = false;
+        rely = false;
       } else if (type == HiveParser.TOK_VALIDATE) {
         validate = true;
       } else if (type == HiveParser.TOK_NOVALIDATE) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java
Patch:
@@ -30,7 +30,7 @@
 public class DecimalUtil {
 
   public static int compare(HiveDecimalWritable writableLeft, HiveDecimal right) {
-    return writableLeft.getHiveDecimal().compareTo(right);
+    return writableLeft.compareTo(right);
   }
 
   public static int compare(HiveDecimal left, HiveDecimalWritable writableRight) {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorBetweenIn.java
Patch:
@@ -342,9 +342,9 @@ private boolean doBetweenInVariation(Random random, String typeName,
     List<Object> sortedList = new ArrayList<Object>(valueCount);
     sortedList.addAll(valueList);
 
-    Object object = valueList.get(0);
+    Object exampleObject = valueList.get(0);
     WritableComparator writableComparator =
-        WritableComparator.get((Class<? extends WritableComparable>) object.getClass());
+        WritableComparator.get((Class<? extends WritableComparable>) exampleObject.getClass());
     sortedList.sort(writableComparator);
 
     final boolean isInvert;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterLongColumnInList.java
Patch:
@@ -36,8 +36,9 @@
 public class FilterLongColumnInList extends VectorExpression implements ILongInExpr {
 
   private static final long serialVersionUID = 1L;
-  private final int inputCol;
-  private long[] inListValues;
+
+  protected final int inputCol;
+  protected long[] inListValues;
 
   // Transient members initialized by transientInit method.
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/LongColumnInList.java
Patch:
@@ -33,8 +33,8 @@ public class LongColumnInList extends VectorExpression implements ILongInExpr {
 
   private static final long serialVersionUID = 1L;
 
-  private int colNum;
-  private long[] inListValues;
+  protected int colNum;
+  protected long[] inListValues;
 
   // The set object containing the IN list. This is optimized for lookup
   // of the data type of the column.

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDate.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateLong;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateTimestamp;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -48,7 +47,7 @@
     extended = "Example:\n "
         + "  > SELECT _FUNC_('2009-07-30 04:17:52') FROM src LIMIT 1;\n"
         + "  '2009-07-30'")
-@VectorizedExpressions({VectorUDFDateString.class, VectorUDFDateLong.class, VectorUDFDateTimestamp.class})
+@VectorizedExpressions({VectorUDFDateString.class, VectorUDFDateTimestamp.class})
 public class GenericUDFDate extends GenericUDF {
   private transient TimestampConverter timestampConverter;
   private transient Converter textConverter;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToDate.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDate;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -42,7 +41,7 @@
     + "Example:\n "
     + "  > SELECT CAST('2009-01-01' AS DATE) FROM src LIMIT 1;\n"
     + "  '2009-01-01'")
-@VectorizedExpressions({CastStringToDate.class, CastLongToDate.class, CastTimestampToDate.class})
+@VectorizedExpressions({CastStringToDate.class, CastTimestampToDate.class})
 public class GenericUDFToDate extends GenericUDF {
 
   private transient PrimitiveObjectInspector argumentOI;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorArithmetic.java
Patch:
@@ -89,7 +89,7 @@ public class TestVectorArithmetic {
   public TestVectorArithmetic() {
     // Arithmetic operations rely on getting conf from SessionState, need to initialize here.
     SessionState ss = new SessionState(new HiveConf());
-    ss.getConf().setVar(HiveConf.ConfVars.HIVE_COMPAT, "latest");
+    ss.getConf().setVar(HiveConf.ConfVars.HIVE_COMPAT, "default");
     SessionState.setCurrentSessionState(ss);
   }
 
@@ -364,7 +364,7 @@ private void doTestsWithDiffColumnScalar(Random random, TypeInfo typeInfo1, Type
         new ArrayList<DataTypePhysicalVariation>();
 
     List<String> columns = new ArrayList<String>();
-    int columnNum = 0;
+    int columnNum = 1;
 
     ExprNodeDesc col1Expr;
     Object scalar1Object = null;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorDateAddSub.java
Patch:
@@ -181,7 +181,7 @@ private void doDateAddSubTestsWithDiffColumnScalar(Random random, String dateTim
         new ArrayList<DataTypePhysicalVariation>();
 
     List<String> columns = new ArrayList<String>();
-    int columnNum = 0;
+    int columnNum = 1;
     ExprNodeDesc col1Expr;
     if (columnScalarMode == ColumnScalarMode.COLUMN_COLUMN ||
         columnScalarMode == ColumnScalarMode.COLUMN_SCALAR) {
@@ -253,8 +253,8 @@ private void doDateAddSubTestsWithDiffColumnScalar(Random random, String dateTim
       // Fixup numbers to limit the range to 0 ... N-1.
       for (int i = 0; i < randomRows.length; i++) {
         Object[] row = randomRows[i];
-        if (row[columnNum - 1] != null) {
-          row[columnNum - 1] =
+        if (row[columnNum - 2] != null) {
+          row[columnNum - 2] =
               smallerRange(
                   random, integerPrimitiveCategory, /* wantWritable */ true);
         }

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorDateDiff.java
Patch:
@@ -179,7 +179,7 @@ private void doDateDiffTestsWithDiffColumnScalar(Random random, String dateTimeS
         new ArrayList<DataTypePhysicalVariation>();
 
     List<String> columns = new ArrayList<String>();
-    int columnNum = 0;
+    int columnNum = 1;
     ExprNodeDesc col1Expr;
     if (columnScalarMode == ColumnScalarMode.COLUMN_COLUMN ||
         columnScalarMode == ColumnScalarMode.COLUMN_SCALAR) {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorFilterCompare.java
Patch:
@@ -329,7 +329,7 @@ private void doTestsWithDiffColumnScalar(Random random, TypeInfo typeInfo1, Type
         new ArrayList<DataTypePhysicalVariation>();
 
     List<String> columns = new ArrayList<String>();
-    int columnNum = 0;
+    int columnNum = 1;
 
     ExprNodeDesc col1Expr;
     Object scalar1Object = null;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorIfStatement.java
Patch:
@@ -247,10 +247,10 @@ private void doIfTestsWithDiffColumnScalar(Random random, String typeName,
         explicitDataTypePhysicalVariationList);
 
     List<String> columns = new ArrayList<String>();
-    columns.add("col0");    // The boolean predicate.
+    columns.add("col1");    // The boolean predicate.
 
-    ExprNodeColumnDesc col1Expr = new  ExprNodeColumnDesc(Boolean.class, "col0", "table", false);
-    int columnNum = 1;
+    ExprNodeColumnDesc col1Expr = new  ExprNodeColumnDesc(Boolean.class, "col1", "table", false);
+    int columnNum = 2;
     ExprNodeDesc col2Expr;
     if (columnScalarMode == ColumnScalarMode.COLUMN_COLUMN ||
         columnScalarMode == ColumnScalarMode.COLUMN_SCALAR) {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorNegative.java
Patch:
@@ -195,7 +195,7 @@ private void doTests(Random random, TypeInfo typeInfo)
         new ArrayList<DataTypePhysicalVariation>();
 
     List<String> columns = new ArrayList<String>();
-    int columnNum = 0;
+    int columnNum = 1;
 
     generationSpecList.add(
         GenerationSpec.createSameType(typeInfo));

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorStringConcat.java
Patch:
@@ -145,7 +145,7 @@ private void doStringConcatTestsWithDiffColumnScalar(Random random,
         new ArrayList<DataTypePhysicalVariation>();
 
     List<String> columns = new ArrayList<String>();
-    int columnNum = 0;
+    int columnNum = 1;
     ExprNodeDesc col1Expr;
     if (columnScalarMode == ColumnScalarMode.COLUMN_COLUMN ||
         columnScalarMode == ColumnScalarMode.COLUMN_SCALAR) {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorStringUnary.java
Patch:
@@ -133,7 +133,7 @@ private void doTests(Random random, String typeName, String functionName)
         new ArrayList<DataTypePhysicalVariation>();
 
     List<String> columns = new ArrayList<String>();
-    int columnNum = 0;
+    int columnNum = 1;
     ExprNodeDesc col1Expr;
     StringGenerationOption stringGenerationOption =
         new StringGenerationOption(true, true);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorSubStr.java
Patch:
@@ -102,7 +102,7 @@ private void doTests(Random random, boolean useLength)
         new ArrayList<DataTypePhysicalVariation>();
 
     List<String> columns = new ArrayList<String>();
-    int columnNum = 0;
+    int columnNum = 1;
     ExprNodeDesc col1Expr;
     StringGenerationOption stringGenerationOption =
         new StringGenerationOption(true, true);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTimestampExtract.java
Patch:
@@ -118,7 +118,7 @@ private void doIfTestOneTimestampExtract(Random random, String dateTimeStringTyp
         new ArrayList<DataTypePhysicalVariation>();
 
     List<String> columns = new ArrayList<String>();
-    int columnNum = 0;
+    int columnNum = 1;
     ExprNodeDesc col1Expr;
     if (!isStringFamily) {
       generationSpecList.add(

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
Patch:
@@ -958,6 +958,7 @@ public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, lon
           LOG.error("Ignoring the cleanup error after another error", t);
         }
         chunk.setBuffer(null);
+        ++decompressedIx;
       }
     }
 

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDeUtils.java
Patch:
@@ -29,9 +29,6 @@ public final class DruidSerDeUtils {
 
   private static final Logger LOG = LoggerFactory.getLogger(DruidSerDeUtils.class);
 
-  protected static final String ISO_TIME_FORMAT = "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'";
-  protected static final String TIMESTAMP_FORMAT = "yyyy-MM-dd HH:mm:ss";
-
   protected static final String FLOAT_TYPE = "FLOAT";
   protected static final String DOUBLE_TYPE = "DOUBLE";
   protected static final String LONG_TYPE = "LONG";

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ExprNodeConverter.java
Patch:
@@ -48,6 +48,7 @@
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.common.type.Timestamp;
+import org.apache.hadoop.hive.common.type.TimestampTZUtil;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.metadata.Hive;
@@ -323,7 +324,7 @@ public ExprNodeDesc visitLiteral(RexLiteral literal) {
         // Calcite stores timestamp with local time-zone in UTC internally, thus
         // when we bring it back, we need to add the UTC suffix.
         return new ExprNodeConstantDesc(TypeInfoFactory.getTimestampTZTypeInfo(conf.getLocalTimeZone()),
-            literal.getValueAs(TimestampString.class).toString() + " UTC");
+            TimestampTZUtil.parse(literal.getValueAs(TimestampString.class).toString() + " UTC"));
       case BINARY:
         return new ExprNodeConstantDesc(TypeInfoFactory.binaryTypeInfo, literal.getValue3());
       case DECIMAL:

File: storage-api/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java
Patch:
@@ -38,7 +38,6 @@
  *    YYYY-MM-DD
  *
  */
-@Deprecated
 public class DateWritable implements WritableComparable<DateWritable> {
 
   private static final long MILLIS_PER_DAY = TimeUnit.DAYS.toMillis(1);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java
Patch:
@@ -74,6 +74,7 @@ public class ReplicationSemanticAnalyzer extends BaseSemanticAnalyzer {
   private static String testInjectDumpDir = null; // unit tests can overwrite this to affect default dump behaviour
   private static final String dumpSchema = "dump_dir,last_repl_id#string,string";
 
+  public static final String LAST_REPL_ID_KEY = "hive.repl.last.repl.id";
   public static final String FUNCTIONS_ROOT_DIR_NAME = "_functions";
   public static final String CONSTRAINTS_ROOT_DIR_NAME = "_constraints";
 

File: ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java
Patch:
@@ -1676,14 +1676,15 @@ public void testReplAllocWriteId() throws Exception {
 
     replAbortTxnForTest(srcTxnIdList, "destdb.*");
 
-    // Test for aborted transactions
+    // Test for aborted transactions. Idempotent case where allocate write id when txn is already
+    // aborted should do nothing.
     failed = false;
     try {
       txnHandler.allocateTableWriteIds(allocMsg).getTxnToWriteIds();
     } catch (RuntimeException e) {
       failed = true;
     }
-    assertTrue(failed);
+    assertFalse(failed);
   }
 
   private void updateTxns(Connection conn) throws SQLException {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTaskTest.java
Patch:
@@ -101,7 +101,7 @@ public void removeDBPropertyToPreventRenameWhenBootstrapDumpOfTableFails() throw
       private int tableDumpCount = 0;
 
       @Override
-      void dumpTable(String dbName, String tblName, String validTxnList, Path dbRoot)
+      void dumpTable(String dbName, String tblName, String validTxnList, Path dbRoot, long lastReplId)
           throws Exception {
         tableDumpCount++;
         if (tableDumpCount > 1) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExpressionDescriptor.java
Patch:
@@ -89,6 +89,8 @@ public enum ArgumentType {
     INT_INTERVAL_YEAR_MONTH     (INT_FAMILY.value | INTERVAL_YEAR_MONTH.value),
     INT_DATE_INTERVAL_YEAR_MONTH  (INT_FAMILY.value | DATE.value | INTERVAL_YEAR_MONTH.value),
     STRING_DATETIME_FAMILY  (STRING_FAMILY.value | DATETIME_FAMILY.value),
+    STRING_FAMILY_BINARY    (STRING_FAMILY.value | BINARY.value),
+    STRING_BINARY           (STRING.value | BINARY.value),
     ALL_FAMILY              (0xFFFFFFL);
 
     private final long value;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCharScalarStringScalar.java
Patch:
@@ -48,7 +48,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
         .setArgumentTypes(
             VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
             VectorExpressionDescriptor.ArgumentType.CHAR,
-            VectorExpressionDescriptor.ArgumentType.STRING)
+            VectorExpressionDescriptor.ArgumentType.CHAR)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprLongColumnLongColumn.java
Patch:
@@ -31,9 +31,9 @@ public class IfExprLongColumnLongColumn extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
-  private final int arg1Column;
-  private final int arg2Column;
-  private final int arg3Column;
+  protected final int arg1Column;
+  protected final int arg2Column;
+  protected final int arg3Column;
 
   public IfExprLongColumnLongColumn(int arg1Column, int arg2Column, int arg3Column,
       int outputColumnNum) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprStringGroupColumnStringGroupColumn.java
Patch:
@@ -196,8 +196,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
         .setNumArguments(3)
         .setArgumentTypes(
             VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
-            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,
-            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY)
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY_BINARY,
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY_BINARY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.COLUMN,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprStringGroupColumnStringScalar.java
Patch:
@@ -185,8 +185,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
         .setNumArguments(3)
         .setArgumentTypes(
             VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
-            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,
-            VectorExpressionDescriptor.ArgumentType.STRING)
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY_BINARY,
+            VectorExpressionDescriptor.ArgumentType.STRING_BINARY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.COLUMN,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprStringScalarStringGroupColumn.java
Patch:
@@ -188,8 +188,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
         .setNumArguments(3)
         .setArgumentTypes(
             VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
-            VectorExpressionDescriptor.ArgumentType.STRING,
-            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY)
+            VectorExpressionDescriptor.ArgumentType.STRING_BINARY,
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY_BINARY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprStringScalarStringScalar.java
Patch:
@@ -152,8 +152,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
         .setNumArguments(3)
         .setArgumentTypes(
             VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
-            VectorExpressionDescriptor.ArgumentType.STRING,
-            VectorExpressionDescriptor.ArgumentType.STRING)
+            VectorExpressionDescriptor.ArgumentType.STRING_BINARY,
+            VectorExpressionDescriptor.ArgumentType.STRING_BINARY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprVarCharScalarStringScalar.java
Patch:
@@ -47,7 +47,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
         .setArgumentTypes(
             VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
             VectorExpressionDescriptor.ArgumentType.VARCHAR,
-            VectorExpressionDescriptor.ArgumentType.STRING)
+            VectorExpressionDescriptor.ArgumentType.VARCHAR)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR,

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTimestamp.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToTimestamp;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
@@ -49,7 +50,7 @@
 @Description(name = "timestamp",
 value = "cast(date as timestamp) - Returns timestamp")
 @VectorizedExpressions({CastLongToTimestamp.class, CastDateToTimestamp.class,
-  CastDoubleToTimestamp.class, CastDecimalToTimestamp.class})
+  CastDoubleToTimestamp.class, CastDecimalToTimestamp.class, CastStringToTimestamp.class})
 public class GenericUDFTimestamp extends GenericUDF {
 
   private transient PrimitiveObjectInspector argumentOI;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizationContext.java
Patch:
@@ -56,8 +56,8 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloLongColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColEqualLongScalar;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColGreaterLongScalar;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColEqualLongScalar;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColGreaterLongScalar;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.RoundWithNumDigitsDoubleToDouble;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse;

File: standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
Patch:
@@ -366,8 +366,8 @@ public void alterTable(RawStore msdb, Warehouse wh, String catName, String dbnam
     } catch (NoSuchObjectException e) {
       LOG.debug("Object not found in metastore ", e);
       throw new InvalidOperationException(
-          "Unable to change partition or table. Database " + dbname + " does not exist"
-              + " Check metastore logs for detailed stack." + e.getMessage());
+          "Unable to change partition or table. Object " +  e.getMessage() + " does not exist."
+              + " Check metastore logs for detailed stack.");
     } finally {
       if (!success) {
         LOG.error("Failed to alter table " + TableName.getQualified(catName, dbname, name));

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
Patch:
@@ -295,7 +295,7 @@ private RexNode convert(ExprNodeGenericFuncDesc func) throws SemanticException {
     for (int i =0; i < func.getChildren().size(); ++i) {
       ExprNodeDesc childExpr = func.getChildren().get(i);
       tmpExprNode = childExpr;
-      if (tgtDT != null
+      if (tgtDT != null && tgtDT.getCategory() == Category.PRIMITIVE
           && TypeInfoUtils.isConversionRequiredForComparison(tgtDT, childExpr.getTypeInfo())) {
         if (isCompare || isBetween || isIN) {
           // For compare, we will convert requisite children

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIn.java
Patch:
@@ -86,7 +86,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments)
     conversionHelper = new GenericUDFUtils.ReturnObjectInspectorResolver(true);
 
     for (ObjectInspector oi : arguments) {
-      if(!conversionHelper.update(oi)) {
+      if(!conversionHelper.updateForComparison(oi)) {
         StringBuilder sb = new StringBuilder();
         sb.append("The arguments for IN should be the same type! Types are: {");
         sb.append(arguments[0].getTypeName());

File: ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java
Patch:
@@ -447,7 +447,7 @@ private List<String> getExistingNonPartTableStatsToUpdate(TableName fullTableNam
     }
     // TODO: we should probably skip updating if writeId is from an active txn
     boolean isTxnValid = (writeIdString == null) || ObjectStore.isCurrentStatsValidForTheQuery(
-        conf, db, tbl, params, statsWriteId , writeIdString, false);
+        conf, params, statsWriteId , writeIdString, false);
     return getExistingStatsToUpdate(existingStats, params, isTxnValid);
   }
 
@@ -472,7 +472,7 @@ private List<String> getAnyStatsToUpdate(String db, String tbl, List<String> all
     }
     // TODO: we should probably skip updating if writeId is from an active txn
     if (writeIdString != null && !ObjectStore.isCurrentStatsValidForTheQuery(
-        conf, db, tbl, params, statsWriteId, writeIdString, false)) {
+        conf, params, statsWriteId, writeIdString, false)) {
       return allCols;
     }
     List<String> colsToUpdate = new ArrayList<>();

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
Patch:
@@ -547,6 +547,7 @@ public Partition alterPartition(RawStore msdb, Warehouse wh, String catName, Str
       // 2) partition column stats if there are any because of part_name field in HMS table PART_COL_STATS
       // 3) rename the partition directory if it is not an external table
       if (!tbl.getTableType().equals(TableType.EXTERNAL_TABLE.toString())) {
+        // TODO: refactor this into a separate method after master merge, this one is too big.
         try {
           db = msdb.getDatabase(catName, dbname);
 
@@ -620,8 +621,6 @@ public Partition alterPartition(RawStore msdb, Warehouse wh, String catName, Str
       if (cs != null) {
         cs.getStatsDesc().setPartName(newPartName);
         try {
-          // Verifying ACID state again is not strictly needed here (alterPartition above does it),
-          // but we are going to use the uniform approach for simplicity.
           msdb.updatePartitionColumnStatistics(cs, new_part.getValues(),
               txnId, validWriteIds, new_part.getWriteId());
         } catch (InvalidInputException iie) {

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java
Patch:
@@ -2217,6 +2217,7 @@ void alter_partitions(String catName, String dbName, String tblName, List<Partit
    * @throws TException
    *          if error in communicating with metastore server
    */
+  @Deprecated
   void renamePartition(final String dbname, final String tableName, final List<String> part_vals,
                        final Partition newPart)
       throws InvalidOperationException, MetaException, TException;
@@ -2240,7 +2241,7 @@ void renamePartition(final String dbname, final String tableName, final List<Str
    *          if error in communicating with metastore server
    */
   void renamePartition(String catName, String dbname, String tableName, List<String> part_vals,
-                       Partition newPart)
+                       Partition newPart, long txnId, String validWriteIds)
       throws InvalidOperationException, MetaException, TException;
 
   /**

File: standalone-metastore/metastore-common/src/test/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientPreCatalog.java
Patch:
@@ -3311,8 +3311,8 @@ public void alter_partitions(String catName, String dbName, String tblName,
 
   @Override
   public void renamePartition(String catName, String dbname, String tableName,
-                              List<String> part_vals, Partition newPart) throws
-      InvalidOperationException, MetaException, TException {
+      List<String> part_vals, Partition newPart, long txnId, String validWriteIds)
+          throws InvalidOperationException, MetaException, TException {
     throw new UnsupportedOperationException();
   }
 

File: standalone-metastore/metastore-common/src/test/java/org/apache/hadoop/hive/metastore/client/TestAlterPartitions.java
Patch:
@@ -1080,7 +1080,7 @@ public void testRenamePartitionBogusCatalogName() throws Exception {
 
     Partition partToRename = oldParts.get(3);
     partToRename.setValues(Lists.newArrayList("2018", "01", "16"));
-    client.renamePartition("nosuch", DB_NAME, TABLE_NAME, oldValues.get(3), partToRename);
+    client.renamePartition("nosuch", DB_NAME, TABLE_NAME, oldValues.get(3), partToRename, -1, null);
   }
 
   @Test(expected = InvalidOperationException.class)

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorAggregateExpression.java
Patch:
@@ -84,7 +84,7 @@ public VectorAggregateExpression(VectorAggregationDesc vecAggrDesc) {
     outputTypeInfo =  vecAggrDesc.getOutputTypeInfo();
     outputDataTypePhysicalVariation = vecAggrDesc.getOutputDataTypePhysicalVariation();
 
-    mode = vecAggrDesc.getAggrDesc().getMode();
+    mode = vecAggrDesc.getUdafEvaluatorMode();
   }
 
   public VectorExpression getInputExpression() {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorDateAddSub.java
Patch:
@@ -370,13 +370,15 @@ private void doRowDateAddSubTest(TypeInfo dateTimeStringTypeInfo, TypeInfo integ
       Object[][] randomRows, ColumnScalarMode columnScalarMode,
       ObjectInspector rowInspector, Object[] resultObjects) throws Exception {
 
+    /*
     System.out.println(
         "*DEBUG* dateTimeStringTypeInfo " + dateTimeStringTypeInfo.toString() +
         " integerTypeInfo " + integerTypeInfo +
         " isAdd " + isAdd +
         " dateAddSubTestMode ROW_MODE" +
         " columnScalarMode " + columnScalarMode +
         " exprDesc " + exprDesc.toString());
+    */
 
     HiveConf hiveConf = new HiveConf();
     ExprNodeEvaluator evaluator =

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorDateDiff.java
Patch:
@@ -362,12 +362,14 @@ private void doRowDateAddSubTest(TypeInfo dateTimeStringTypeInfo1,
       Object[][] randomRows, ColumnScalarMode columnScalarMode,
       ObjectInspector rowInspector, Object[] resultObjects) throws Exception {
 
+    /*
     System.out.println(
         "*DEBUG* dateTimeStringTypeInfo " + dateTimeStringTypeInfo1.toString() +
         " dateTimeStringTypeInfo2 " + dateTimeStringTypeInfo2 +
         " dateDiffTestMode ROW_MODE" +
         " columnScalarMode " + columnScalarMode +
         " exprDesc " + exprDesc.toString());
+    */
 
     HiveConf hiveConf = new HiveConf();
     ExprNodeEvaluator evaluator =

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorStringUnary.java
Patch:
@@ -347,11 +347,13 @@ private void doVectorIfTest(TypeInfo typeInfo, TypeInfo targetTypeInfo,
     resultVectorExtractRow.init(new TypeInfo[] { targetTypeInfo }, new int[] { columns.size() });
     Object[] scrqtchRow = new Object[1];
 
+    /*
     System.out.println(
         "*DEBUG* typeInfo " + typeInfo.toString() +
         " targetTypeInfo " + targetTypeInfo.toString() +
         " stringUnaryTestMode " + stringUnaryTestMode +
         " vectorExpression " + vectorExpression.getClass().getSimpleName());
+    */
 
     batchSource.resetBatchIteration();
     int rowIndex = 0;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorSubStr.java
Patch:
@@ -326,11 +326,13 @@ private void doVectorIfTest(TypeInfo typeInfo, TypeInfo targetTypeInfo,
     resultVectorExtractRow.init(new TypeInfo[] { targetTypeInfo }, new int[] { columns.size() });
     Object[] scrqtchRow = new Object[1];
 
+    /*
     System.out.println(
         "*DEBUG* typeInfo " + typeInfo.toString() +
         " targetTypeInfo " + targetTypeInfo.toString() +
         " subStrTestMode " + subStrTestMode +
         " vectorExpression " + vectorExpression.getClass().getSimpleName());
+    */
 
     batchSource.resetBatchIteration();
     int rowIndex = 0;

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java
Patch:
@@ -81,7 +81,7 @@ public TimestampColumnVector(int len) {
 
     scratchWritable = null;     // Allocated by caller.
 
-    isUTC = true;
+    isUTC = false;
   }
 
   /**

File: storage-api/src/test/org/apache/hadoop/hive/ql/exec/vector/TestStructColumnVector.java
Patch:
@@ -105,6 +105,7 @@ public void testStringify() throws IOException {
     VectorizedRowBatch batch = new VectorizedRowBatch(2);
     LongColumnVector x1 = new LongColumnVector();
     TimestampColumnVector x2 = new TimestampColumnVector();
+    x2.setIsUTC(true);
     StructColumnVector x = new StructColumnVector(1024, x1, x2);
     BytesColumnVector y = new BytesColumnVector();
     batch.cols[0] = x;

File: ql/src/java/org/apache/hadoop/hive/metastore/SynchronizedMetaStoreClient.java
Patch:
@@ -79,8 +79,8 @@ public synchronized Partition add_partition(Partition partition) throws TExcepti
   }
 
   public synchronized void alter_partition(String dbName, String tblName,
-      Partition newPart, EnvironmentContext environmentContext) throws TException {
-    client.alter_partition(dbName, tblName, newPart, environmentContext);
+      Partition newPart, EnvironmentContext environmentContext, long txnId, String writeIdList) throws TException {
+    client.alter_partition(dbName, tblName, newPart, environmentContext, txnId, writeIdList);
   }
 
   public synchronized LockResponse checkLock(long lockid) throws TException {

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/UpdateInputAccessTimeHook.java
Patch:
@@ -63,7 +63,7 @@ public void run(HookContext hookContext) throws Exception {
           String tblName = re.getTable().getTableName();
           Table t = db.getTable(dbName, tblName);
           t.setLastAccessTime(lastAccessTime);
-          db.alterTable(dbName + "." + tblName, t, false, null, true);
+          db.alterTable(dbName + "." + tblName, t, false, null, false);
           break;
         }
         case PARTITION: {
@@ -73,9 +73,9 @@ public void run(HookContext hookContext) throws Exception {
           Table t = db.getTable(dbName, tblName);
           p = db.getPartition(t, p.getSpec(), false);
           p.setLastAccessTime(lastAccessTime);
-          db.alterPartition(dbName, tblName, p, null, true);
+          db.alterPartition(dbName, tblName, p, null, false);
           t.setLastAccessTime(lastAccessTime);
-          db.alterTable(dbName + "." + tblName, t, false, null, true);
+          db.alterTable(dbName + "." + tblName, t, false, null, false);
           break;
         }
         default:

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java
Patch:
@@ -24,6 +24,7 @@
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.common.ValidTxnList;
 import org.apache.hadoop.hive.common.ValidReadTxnList;
+import org.apache.hadoop.hive.common.ValidWriteIdList;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.ql.Context;

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/HiveTxnManager.java
Patch:
@@ -19,6 +19,7 @@
 
 import org.apache.hadoop.hive.common.ValidTxnList;
 import org.apache.hadoop.hive.common.ValidTxnWriteIdList;
+import org.apache.hadoop.hive.common.ValidWriteIdList;
 import org.apache.hadoop.hive.metastore.api.CommitTxnRequest;
 import org.apache.hadoop.hive.metastore.api.LockResponse;
 import org.apache.hadoop.hive.metastore.api.TxnToWriteId;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/RenamePartitionHandler.java
Patch:
@@ -60,7 +60,7 @@ public List<Task<? extends Serializable>> handle(Context context)
     }
 
     RenamePartitionDesc renamePtnDesc = new RenamePartitionDesc(
-            tableName, oldPartSpec, newPartSpec, context.eventOnlyReplicationSpec());
+            tableName, oldPartSpec, newPartSpec, context.eventOnlyReplicationSpec(), null);
     Task<DDLWork> renamePtnTask = TaskFactory.get(
         new DDLWork(readEntitySet, writeEntitySet, renamePtnDesc), context.hiveConf);
     context.log.debug("Added rename ptn task : {}:{}->{}",

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnConcatenate.java
Patch:
@@ -219,7 +219,7 @@ public void testRenameTable() throws Exception {
         "select count(*) from COMPACTION_QUEUE where CQ_TABLE='s'"));
     Assert.assertEquals(1, TxnDbUtil.countQueryAgent(hiveConf,
         "select count(*) from WRITE_SET where WS_TABLE='s'"));
-    Assert.assertEquals(2, TxnDbUtil.countQueryAgent(hiveConf,
+    Assert.assertEquals(3, TxnDbUtil.countQueryAgent(hiveConf,
         "select count(*) from TXN_TO_WRITE_ID where T2W_TABLE='s'"));
     Assert.assertEquals(1, TxnDbUtil.countQueryAgent(hiveConf,
         "select count(*) from NEXT_WRITE_ID where NWI_TABLE='s'"));
@@ -234,7 +234,7 @@ public void testRenameTable() throws Exception {
             "select count(*) from COMPACTION_QUEUE where CQ_TABLE='bar'"));
     Assert.assertEquals(1, TxnDbUtil.countQueryAgent(hiveConf,
             "select count(*) from WRITE_SET where WS_TABLE='bar'"));
-    Assert.assertEquals(2, TxnDbUtil.countQueryAgent(hiveConf,
+    Assert.assertEquals(4, TxnDbUtil.countQueryAgent(hiveConf,
             "select count(*) from TXN_TO_WRITE_ID where T2W_TABLE='bar'"));
     Assert.assertEquals(1, TxnDbUtil.countQueryAgent(hiveConf,
             "select count(*) from NEXT_WRITE_ID where NWI_TABLE='bar'"));

File: ql/src/test/org/apache/hadoop/hive/ql/TxnCommandsBaseForTests.java
Patch:
@@ -225,7 +225,7 @@ void checkExpected(List<String> rs, String[][] expected, String msg, Logger LOG,
         expected.length, rs.size());
     //verify data and layout
     for(int i = 0; i < expected.length; i++) {
-      Assert.assertTrue("Actual line (data) " + i + " data: " + rs.get(i), rs.get(i).startsWith(expected[i][0]));
+      Assert.assertTrue("Actual line (data) " + i + " data: " + rs.get(i) + "; expected " + expected[i][0], rs.get(i).startsWith(expected[i][0]));
       if(checkFileName) {
         Assert.assertTrue("Actual line(file) " + i + " file: " + rs.get(i), rs.get(i).endsWith(expected[i][1]));
       }

File: standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/RawStore.java
Patch:
@@ -864,7 +864,7 @@ List<Partition> listPartitionsPsWithAuth(String catName, String db_name, String
    * @throws InvalidObjectException the stats object is invalid
    * @throws InvalidInputException unable to record the stats for the table
    */
-  boolean updateTableColumnStatistics(ColumnStatistics colStats)
+  boolean updateTableColumnStatistics(ColumnStatistics colStats, long txnId, String validWriteIds, long writeId)
       throws NoSuchObjectException, MetaException, InvalidObjectException, InvalidInputException;
 
   /** Persists the given column statistics object to the metastore
@@ -875,9 +875,10 @@ boolean updateTableColumnStatistics(ColumnStatistics colStats)
    * @throws MetaException error accessing the RDBMS.
    * @throws InvalidObjectException the stats object is invalid
    * @throws InvalidInputException unable to record the stats for the table
+   * @throws TException
    */
   boolean updatePartitionColumnStatistics(ColumnStatistics statsObj,
-     List<String> partVals)
+     List<String> partVals, long txnId, String validWriteIds, long writeId)
      throws NoSuchObjectException, MetaException, InvalidObjectException, InvalidInputException;
 
   /**

File: standalone-metastore/metastore-common/src/test/java/org/apache/hadoop/hive/metastore/TestHiveAlterHandler.java
Patch:
@@ -60,7 +60,7 @@ public void testAlterTableAddColNotUpdateStats() throws MetaException, InvalidOb
         getDefaultCatalog(conf), oldTable.getDbName(), oldTable.getTableName(), Arrays.asList("col1", "col2", "col3"));
     HiveAlterHandler handler = new HiveAlterHandler();
     handler.setConf(conf);
-    handler.alterTableUpdateTableColumnStats(msdb, oldTable, newTable, null);
+    handler.alterTableUpdateTableColumnStats(msdb, oldTable, newTable, null, -1, null);
   }
 
   @Test
@@ -85,7 +85,7 @@ public void testAlterTableDelColUpdateStats() throws MetaException, InvalidObjec
     RawStore msdb = Mockito.mock(RawStore.class);
     HiveAlterHandler handler = new HiveAlterHandler();
     handler.setConf(conf);
-    handler.alterTableUpdateTableColumnStats(msdb, oldTable, newTable, null);
+    handler.alterTableUpdateTableColumnStats(msdb, oldTable, newTable, null, -1, null);
     Mockito.verify(msdb, Mockito.times(1)).getTableColumnStatistics(
         getDefaultCatalog(conf), oldTable.getDbName(), oldTable.getTableName(), Arrays.asList("col1", "col2", "col3", "col4")
     );
@@ -115,7 +115,7 @@ public void testAlterTableChangePosNotUpdateStats() throws MetaException, Invali
         getDefaultCatalog(conf), oldTable.getDbName(), oldTable.getTableName(), Arrays.asList("col1", "col2", "col3", "col4"));
     HiveAlterHandler handler = new HiveAlterHandler();
     handler.setConf(conf);
-    handler.alterTableUpdateTableColumnStats(msdb, oldTable, newTable, null);
+    handler.alterTableUpdateTableColumnStats(msdb, oldTable, newTable, null, -1, null);
   }
 
 }

File: standalone-metastore/metastore-common/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
Patch:
@@ -244,6 +244,7 @@ public void testListener() throws Exception {
     Assert.assertTrue(partEvent.getStatus());
     Partition part = msc.getPartition("hive2038", "tmptbl", "b=2011");
     Partition partAdded = partEvent.getPartitionIterator().next();
+    partAdded.setWriteId(part.getWriteId());
     validateAddPartition(part, partAdded);
     validateTableInAddPartition(tbl, partEvent.getTable());
     validateAddPartition(part, prePartEvent.getPartitions().get(0));

File: standalone-metastore/metastore-common/src/test/java/org/apache/hadoop/hive/metastore/TestObjectStore.java
Patch:
@@ -559,7 +559,7 @@ private void createPartitionedTable(boolean withPrivileges, boolean withStatisti
         ColumnStatisticsObj partStats = new ColumnStatisticsObj("test_part_col", "int", data);
         statsObjList.add(partStats);
 
-        objectStore.updatePartitionColumnStatistics(stats, part.getValues());
+        objectStore.updatePartitionColumnStatistics(stats, part.getValues(), -1, null, -1);
       }
     }
     if (withPrivileges) {

File: standalone-metastore/metastore-common/src/test/java/org/apache/hadoop/hive/metastore/TestOldSchema.java
Patch:
@@ -175,7 +175,7 @@ public void testPartitionOps() throws Exception {
       data.setLongStats(dcsd);
       obj.setStatsData(data);
       cs.addToStatsObj(obj);
-      store.updatePartitionColumnStatistics(cs, partVal);
+      store.updatePartitionColumnStatistics(cs, partVal, -1, null, -1);
 
     }
 

File: beeline/src/java/org/apache/hive/beeline/hs2connection/BeelineSiteParser.java
Patch:
@@ -63,7 +63,7 @@ public BeelineSiteParser() {
       locations
           .add(System.getenv("HIVE_CONF_DIR") + File.separator + DEFAULT_BEELINE_SITE_FILE_NAME);
     }
-    locations.add(ETC_HIVE_CONF_LOCATION + DEFAULT_BEELINE_SITE_FILE_NAME);
+    locations.add(ETC_HIVE_CONF_LOCATION + File.separator + DEFAULT_BEELINE_SITE_FILE_NAME);
   }
 
   @VisibleForTesting

File: beeline/src/java/org/apache/hive/beeline/hs2connection/UserHS2ConnectionFileParser.java
Patch:
@@ -56,7 +56,7 @@ public UserHS2ConnectionFileParser() {
       locations.add(
           System.getenv("HIVE_CONF_DIR") + File.separator + DEFAULT_CONNECTION_CONFIG_FILE_NAME);
     }
-    locations.add(ETC_HIVE_CONF_LOCATION + DEFAULT_CONNECTION_CONFIG_FILE_NAME);
+    locations.add(ETC_HIVE_CONF_LOCATION + File.separator + DEFAULT_CONNECTION_CONFIG_FILE_NAME);
   }
 
   @VisibleForTesting

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -213,6 +213,7 @@ public final class FunctionRegistry {
     system.registerGenericUDF("ceiling", GenericUDFCeil.class);
     system.registerUDF("rand", UDFRand.class, false);
     system.registerGenericUDF("abs", GenericUDFAbs.class);
+    system.registerGenericUDF("json_read", GenericUDFJsonRead.class);
     system.registerGenericUDF("sq_count_check", GenericUDFSQCountCheck.class);
     system.registerGenericUDF("enforce_constraint", GenericUDFEnforceConstraint.class);
     system.registerGenericUDF("pmod", GenericUDFPosMod.class);

File: streaming/src/java/org/apache/hive/streaming/StrictJsonWriter.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  * Streaming Writer handles utf8 encoded Json (Strict syntax).
- * Uses org.apache.hadoop.hive.serde2.JsonSerDe to process Json input
+ * Uses {@link JsonSerDe} to process Json input
  *
  * NOTE: This record writer is NOT thread-safe. Use one record writer per streaming connection.
  */

File: service/src/java/org/apache/hive/http/LlapServlet.java
Patch:
@@ -34,7 +34,7 @@
 @SuppressWarnings("serial")
 public class LlapServlet extends HttpServlet {
 
-  private static final Log LOG = LogFactory.getLog(JMXJsonServlet.class);
+  private static final Log LOG = LogFactory.getLog(LlapServlet.class);
 
   /**
    * Initialize this servlet.

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
Patch:
@@ -56,10 +56,12 @@
 import javax.annotation.Nullable;
 import java.util.Collections;
 import com.google.common.collect.Lists;
+import org.junit.Ignore;
 
 /**
  * TestReplicationScenariosAcidTables - test replication for ACID tables
  */
+@Ignore("this is ignored as its taking more time. Need to analyze further to reduce the time")
 public class TestReplicationScenariosAcidTables {
   @Rule
   public final TestName testName = new TestName();

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -7262,7 +7262,7 @@ private void addTxnWriteNotificationLog(Table tableObj, Partition ptnObj, WriteN
 
     private Table getTblObject(String db, String table) throws MetaException, NoSuchObjectException {
       GetTableRequest req = new GetTableRequest(db, table);
-      req.setCapabilities(new ClientCapabilities(Lists.newArrayList(ClientCapability.TEST_CAPABILITY)));
+      req.setCapabilities(new ClientCapabilities(Lists.newArrayList(ClientCapability.TEST_CAPABILITY, ClientCapability.INSERT_ONLY_TABLES)));
       return get_table_req(req).getTable();
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
Patch:
@@ -349,6 +349,7 @@ private void publishStats() throws HiveException {
     StatsPublisher statsPublisher = Utilities.getStatsPublisher(jc);
     StatsCollectionContext sc = new StatsCollectionContext(jc);
     sc.setStatsTmpDir(conf.getTmpStatsDir());
+    sc.setContextSuffix(getOperatorId());
     if (!statsPublisher.connect(sc)) {
       // just return, stats gathering should not block the main query.
       if (LOG.isInfoEnabled()) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -1907,12 +1907,12 @@ public static boolean isMergeRequired(List<Task<MoveWork>> mvTasks, HiveConf hco
         mvTasks, fsOp.getConf().getFinalDirName(), fsOp.getConf().isMmTable());
 
     // TODO: wtf?!! why is this in this method? This has nothing to do with anything.
-    if (mvTask != null && isInsertTable && hconf.getBoolVar(ConfVars.HIVESTATSAUTOGATHER)
+    if (isInsertTable && hconf.getBoolVar(ConfVars.HIVESTATSAUTOGATHER)
         && !fsOp.getConf().isMaterialization()) {
       // mark the MapredWork and FileSinkOperator for gathering stats
       fsOp.getConf().setGatherStats(true);
       fsOp.getConf().setStatsReliable(hconf.getBoolVar(ConfVars.HIVE_STATS_RELIABLE));
-      if (!mvTask.hasFollowingStatsTask()) {
+      if (mvTask != null && !mvTask.hasFollowingStatsTask()) {
         GenMapRedUtils.addStatsTask(fsOp, mvTask, currTask, hconf);
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java
Patch:
@@ -219,7 +219,7 @@ public static void removeUnionOperators(GenTezProcContext context, BaseWork work
     roots.addAll(context.eventOperatorSet);
 
     // need to clone the plan.
-    List<Operator<?>> newRoots = SerializationUtilities.cloneOperatorTree(roots, indexForTezUnion);
+    List<Operator<?>> newRoots = SerializationUtilities.cloneOperatorTree(roots);
 
     // we're cloning the operator plan but we're retaining the original work. That means
     // that root operators have to be replaced with the cloned ops. The replacement map

File: serde/src/java/org/apache/hadoop/hive/serde2/Deserializer.java
Patch:
@@ -68,6 +68,8 @@ public interface Deserializer {
 
   /**
    * Returns statistics collected when serializing
+   *
+   * @return {@link SerDeStats} object; or in case not supported: null
    */
   SerDeStats getSerDeStats();
 }

File: streaming/src/java/org/apache/hive/streaming/AbstractRecordWriter.java
Patch:
@@ -162,7 +162,7 @@ public void init(StreamingConnection conn, long minWriteId, long maxWriteId) thr
     try {
       this.acidOutputFormat = (AcidOutputFormat<?, ?>) ReflectionUtils
         .newInstance(JavaUtils.loadClass(outFormatName), conf);
-    } catch (ClassNotFoundException e) {
+    } catch (Exception e) {
       String shadePrefix = conf.getVar(HiveConf.ConfVars.HIVE_CLASSLOADER_SHADE_PREFIX);
       if (shadePrefix != null && !shadePrefix.trim().isEmpty()) {
         try {

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QOutProcessor.java
Patch:
@@ -273,6 +273,8 @@ private final static class PatternReplacementPair {
     ppm.add(new PatternReplacementPair(Pattern.compile("attempt_[0-9_]+"), "attempt_#ID#"));
     ppm.add(new PatternReplacementPair(Pattern.compile("vertex_[0-9_]+"), "vertex_#ID#"));
     ppm.add(new PatternReplacementPair(Pattern.compile("task_[0-9_]+"), "task_#ID#"));
+    ppm.add(new PatternReplacementPair(Pattern.compile("for Spark session.*?:"),
+            "#SPARK_SESSION_ID#:"));
     partialPlanMask = ppm.toArray(new PatternReplacementPair[ppm.size()]);
   }
   /* This list may be modified by specific cli drivers to mask strings that change on every test */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/RemoteSparkJobMonitor.java
Patch:
@@ -76,7 +76,6 @@ public int startMonitor() {
           if ((timeCount > monitorTimeoutInterval)) {
             HiveException he = new HiveException(ErrorMsg.SPARK_JOB_MONITOR_TIMEOUT,
                 Long.toString(timeCount));
-            console.printError(he.getMessage());
             sparkJobStatus.setMonitorError(he);
             running = false;
             done = true;

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -199,8 +199,8 @@ public enum ErrorMsg {
   NEED_TABLE_SPECIFICATION(10117, "Table name could be determined; It should be specified "),
   PARTITION_EXISTS(10118, "Partition already exists"),
   TABLE_DATA_EXISTS(10119, "Table exists and contains data files"),
-  INCOMPATIBLE_SCHEMA(10120, "The existing table is not compatible with the import spec. "),
-  EXIM_FOR_NON_NATIVE(10121, "Export/Import cannot be done for a non-native table. "),
+  INCOMPATIBLE_SCHEMA(10120, "The existing table is not compatible with the Export/Import spec. "),
+  EXIM_FOR_NON_NATIVE(10121, "Export/Import cannot be done for a non-native table."),
   INSERT_INTO_BUCKETIZED_TABLE(10122, "Bucketized tables do not support INSERT INTO:"),
   PARTSPEC_DIFFER_FROM_SCHEMA(10125, "Partition columns in partition specification are "
       + "not the same as that defined in the table schema. "

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExportTask.java
Patch:
@@ -55,7 +55,7 @@ protected int execute(DriverContext driverContext) {
       TableExport tableExport = new TableExport(exportPaths, work.getTableSpec(),
           work.getReplicationSpec(), db, null, conf, work.getMmContext());
       if (!tableExport.write()) {
-        throw new SemanticException(ErrorMsg.EXIM_FOR_NON_NATIVE.getMsg());
+        throw new SemanticException(ErrorMsg.INCOMPATIBLE_SCHEMA.getMsg());
       }
     } catch (Exception e) {
       LOG.error("failed", e);

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -32,7 +32,6 @@
 import java.io.InputStreamReader;
 import java.io.OutputStream;
 import java.io.PrintStream;
-import java.io.Serializable;
 import java.io.StringWriter;
 import java.io.UnsupportedEncodingException;
 import java.net.URL;
@@ -1781,7 +1780,7 @@ public void resetParser() throws SemanticException {
   }
 
 
-  public List<Task<? extends Serializable>> analyzeAST(ASTNode ast) throws Exception {
+  public List<Task<?>> analyzeAST(ASTNode ast) throws Exception {
 
     // Do semantic analysis and plan generation
     Context ctx = new Context(conf);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ExplainSemanticAnalyzer.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hadoop.hive.ql.parse;
 
 import java.io.IOException;
-import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
@@ -165,7 +164,7 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
     sem.validate();
 
     ctx.setResFile(ctx.getLocalTmpPath());
-    List<Task<? extends Serializable>> tasks = sem.getAllRootTasks();
+    List<Task<?>> tasks = sem.getAllRootTasks();
     if (tasks == null) {
       tasks = Collections.emptyList();
     }
@@ -262,7 +261,7 @@ public List<FieldSchema> getResultSchema() {
 
   @Override
   public boolean skipAuthorization() {
-    List<Task<? extends Serializable>> rootTasks = getRootTasks();
+    List<Task<?>> rootTasks = getRootTasks();
     assert rootTasks != null && rootTasks.size() == 1;
     Task task = rootTasks.get(0);
     return task instanceof ExplainTask && ((ExplainTask)task).getWork().isAuthorize();

File: ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java
Patch:
@@ -80,7 +80,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)
     }
 
     @Override
-    public void postAnalyze(HiveSemanticAnalyzerHookContext context, List<Task<? extends Serializable>> rootTasks)
+    public void postAnalyze(HiveSemanticAnalyzerHookContext context, List<Task<?>> rootTasks)
         throws SemanticException {
     }
   }

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreUtils.java
Patch:
@@ -225,7 +225,7 @@ public static FieldSchema getFieldSchemaFromTypeInfo(String fieldName,
   public static IMetaStoreClient getHiveMetastoreClient(HiveConf hiveConf)
     throws MetaException, IOException {
 
-    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.METASTORE_CLIENT_CACHE_ENABLED)){
+    if (!HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.METASTORE_CLIENT_CACHE_ENABLED)){
       // If cache is disabled, don't use it.
       return HiveClientCache.getNonCachedHiveMetastoreClient(hiveConf);
     }

File: streaming/src/java/org/apache/hive/streaming/HiveStreamingConnection.java
Patch:
@@ -1037,7 +1037,7 @@ private static void setHiveConf(HiveConf conf, HiveConf.ConfVars var, boolean va
     if (LOG.isDebugEnabled()) {
       LOG.debug("Overriding HiveConf setting : " + var + " = " + value);
     }
-    conf.setBoolVar(var, true);
+    conf.setBoolVar(var, value);
   }
 
   private static void setHiveConf(HiveConf conf, String var) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConvertDecimal64ToDecimal.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
+import org.apache.hadoop.hive.ql.exec.vector.Decimal64ColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 
@@ -38,6 +39,6 @@ public ConvertDecimal64ToDecimal(int inputColumn, int outputColumnNum) {
 
   @Override
   protected void func(DecimalColumnVector outV, LongColumnVector inV, int i) {
-    outV.vector[i].deserialize64(inV.vector[i], outV.scale);
+    outV.vector[i].deserialize64(inV.vector[i], ((Decimal64ColumnVector) inV).scale);
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -13441,6 +13441,9 @@ private void validateCreateView()
       Set<String> tableAliases = qb.getTabAliases();
       for (String alias : tableAliases) {
         try {
+          if (DUMMY_TABLE.equals(alias)) {
+            continue;
+          }
           Table table = this.getTableObjectByName(qb.getTabNameForAlias(alias));
           if (table.isTemporary()) {
             throw new SemanticException("View definition references temporary table " + alias);

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -769,7 +769,7 @@ public void alterPartitions(String tblName, List<Partition> newParts,
     try {
       AcidUtils.TableSnapshot tableSnapshot = null;
       if (transactional) {
-        tableSnapshot = AcidUtils.getTableSnapshot(conf, newParts.get(0).getTable(), true);
+        tableSnapshot = AcidUtils.getTableSnapshot(conf, newParts.get(0).getTable());
       }
       // Remove the DDL time so that it gets refreshed
       for (Partition tmpPart: newParts) {
@@ -2454,8 +2454,7 @@ public Partition createPartition(Table tbl, Map<String, String> partSpec) throws
     try {
       org.apache.hadoop.hive.metastore.api.Partition part =
           Partition.createMetaPartitionObject(tbl, partSpec, null);
-      AcidUtils.TableSnapshot tableSnapshot =
-          AcidUtils.getTableSnapshot(conf, tbl, false);
+      AcidUtils.TableSnapshot tableSnapshot = AcidUtils.getTableSnapshot(conf, tbl);
       part.setTxnId(tableSnapshot != null ? tableSnapshot.getTxnId() : 0);
       part.setValidWriteIdList(tableSnapshot != null ? tableSnapshot.getValidWriteIdList() : null);
       return new Partition(tbl, getMSC().add_partition(part));

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -5697,7 +5697,9 @@ public PartitionsStatsResult get_partitions_statistics_req(PartitionsStatsReques
       }
       try {
         List<ColumnStatistics> stats = getMS().getPartitionColumnStatistics(
-            catName, dbName, tblName, lowerCasePartNames, lowerCaseColNames);
+            catName, dbName, tblName, lowerCasePartNames, lowerCaseColNames,
+            request.isSetTxnId() ? request.getTxnId() : -1,
+            request.isSetValidWriteIdList() ? request.getValidWriteIdList() : null);
         Map<String, List<ColumnStatisticsObj>> map = new HashMap<>();
         for (ColumnStatistics stat : stats) {
           map.put(stat.getStatsDesc().getPartName(), stat.getStatsObj());

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java
Patch:
@@ -919,7 +919,7 @@ private Collection<List<ColumnStatisticsObj>> verifyAndGetPartColumnStats(
         partNames.add(part.getName());
       }
       AcidUtils.TableSnapshot tableSnapshot =
-          AcidUtils.getTableSnapshot(hive.getConf(), tbl, true);
+          AcidUtils.getTableSnapshot(hive.getConf(), tbl);
 
       Map<String, List<ColumnStatisticsObj>> result = hive.getMSC().getPartitionColumnStatistics(
           tbl.getDbName(), tbl.getTableName(), partNames, Lists.newArrayList(colName),

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java
Patch:
@@ -919,7 +919,7 @@ private Collection<List<ColumnStatisticsObj>> verifyAndGetPartColumnStats(
         partNames.add(part.getName());
       }
       AcidUtils.TableSnapshot tableSnapshot =
-          AcidUtils.getTableSnapshot(hive.getConf(), tbl);
+          AcidUtils.getTableSnapshot(hive.getConf(), tbl, true);
 
       Map<String, List<ColumnStatisticsObj>> result = hive.getMSC().getPartitionColumnStatistics(
           tbl.getDbName(), tbl.getTableName(), partNames, Lists.newArrayList(colName),

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -1886,7 +1886,7 @@ public void alter_partition(String catName, String dbName, String tblName, Parti
   public void alter_partitions(String dbName, String tblName, List<Partition> newParts)
       throws TException {
     alter_partitions(
-        getDefaultCatalog(conf), dbName, tblName, newParts, null, -1, null);
+        getDefaultCatalog(conf), dbName, tblName, newParts, new EnvironmentContext(), -1, null);
   }
 
   @Override

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java
Patch:
@@ -2155,7 +2155,7 @@ void alter_partitions(String dbName, String tblName, List<Partition> newParts,
   default void alter_partitions(String catName, String dbName, String tblName,
                                 List<Partition> newParts)
       throws InvalidOperationException, MetaException, TException {
-    alter_partitions(catName, dbName, tblName, newParts, null,-1, null);
+    alter_partitions(catName, dbName, tblName, newParts, new EnvironmentContext(), -1, null);
   }
 
   /**

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -2381,6 +2381,7 @@ public boolean addPartition(Partition part) throws InvalidObjectException,
     boolean commited = false;
 
     try {
+      openTransaction();
       String catName = part.isSetCatName() ? part.getCatName() : getDefaultCatalog(conf);
       MTable table = this.getMTable(catName, part.getDbName(), part.getTableName());
       List<MTablePrivilege> tabGrants = null;
@@ -2390,7 +2391,6 @@ public boolean addPartition(Partition part) throws InvalidObjectException,
         tabColumnGrants = this.listTableAllColumnGrants(
             catName, part.getDbName(), part.getTableName());
       }
-      openTransaction();
       MPartition mpart = convertToMPart(part, table, true);
       pm.makePersistent(mpart);
 
@@ -4202,7 +4202,7 @@ private MColumnDescriptor alterPartitionNoTxn(String catName, String dbname, Str
     catName = normalizeIdentifier(catName);
     name = normalizeIdentifier(name);
     dbname = normalizeIdentifier(dbname);
-    MTable table = this.getMTable(catName, dbname, name);
+    MTable table = this.getMTable(newPart.getCatName(), newPart.getDbName(), newPart.getTableName());
     MPartition oldp = getMPartition(catName, dbname, name, part_vals);
     MPartition newp = convertToMPart(newPart, table, false);
     MColumnDescriptor oldCD = null;

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
Patch:
@@ -151,6 +151,7 @@ public static void setUpBeforeClass() throws Exception {
     hconf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
     hconf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hconf.set(HiveConf.ConfVars.HIVE_IN_TEST_REPL.varname, "true");
+    hconf.setBoolVar(HiveConf.ConfVars.HIVE_IN_TEST, true);
     hconf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     hconf.set(HiveConf.ConfVars.HIVE_TXN_MANAGER.varname,
         "org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager");

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestUpdateDeleteSemanticAnalyzer.java
Patch:
@@ -231,6 +231,7 @@ public void setup() {
     conf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
     conf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
+    conf.setBoolVar(HiveConf.ConfVars.HIVE_IN_TEST, true);
   }
 
   public void cleanupTables() throws HiveException {

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -4877,7 +4877,8 @@ public AlterPartitionsResponse alter_partitions_with_environment_context(
             throws TException {
       alter_partitions_with_environment_context(
           req.getDbName(), req.getTableName(), req.getPartitions(), req.getEnvironmentContext(),
-          req.getTxnId(), req.getValidWriteIdList());
+          req.isSetTxnId() ? req.getTxnId() : -1,
+          req.isSetValidWriteIdList() ? req.getValidWriteIdList() : null);
       return new AlterPartitionsResponse();
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -1154,6 +1154,8 @@ protected ExprNodeDesc getXpathOrFuncExprNodeDesc(ASTNode expr,
               children.set(constIdx, new ExprNodeConstantDesc(new Byte(constVal.toString())));
             } else if (PrimitiveObjectInspectorUtils.shortTypeEntry.equals(colTypeInfo.getPrimitiveTypeEntry()) && (constVal instanceof Number || constVal instanceof String)) {
               children.set(constIdx, new ExprNodeConstantDesc(new Short(constVal.toString())));
+            } else if (PrimitiveObjectInspectorUtils.decimalTypeEntry.equals(colTypeInfo.getPrimitiveTypeEntry()) && (constVal instanceof Number || constVal instanceof String)) {
+              children.set(constIdx, NumExprProcessor.createDecimal(constVal.toString(),false));
             }
           } catch (NumberFormatException nfe) {
             LOG.trace("Failed to narrow type of constant", nfe);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2115,7 +2115,7 @@ public static enum ConfVars {
         "Whether to provide the row offset virtual column"),
 
     // Optimizer
-    HIVEOPTINDEXFILTER("hive.optimize.index.filter", false, "Whether to enable automatic use of indexes"),
+    HIVEOPTINDEXFILTER("hive.optimize.index.filter", true, "Whether to enable automatic use of indexes"),
 
     HIVEOPTPPD("hive.optimize.ppd", true,
         "Whether to enable predicate pushdown"),

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ParseContext.java
Patch:
@@ -57,6 +57,7 @@
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -128,7 +129,7 @@ public class ParseContext {
   private Set<FileSinkDesc> acidFileSinks = Collections.emptySet();
 
   private Map<ReduceSinkOperator, RuntimeValuesInfo> rsToRuntimeValuesInfo =
-          new HashMap<ReduceSinkOperator, RuntimeValuesInfo>();
+          new LinkedHashMap<ReduceSinkOperator, RuntimeValuesInfo>();
   private Map<ReduceSinkOperator, SemiJoinBranchInfo> rsToSemiJoinBranchInfo =
           new HashMap<>();
   private Map<ExprNodeDesc, GroupByOperator> colExprToGBMap =

File: ql/src/test/org/apache/hadoop/hive/ql/plan/mapping/TestOperatorCmp.java
Patch:
@@ -135,7 +135,8 @@ public void testDifferentFiltersAreNotMatched() throws ParseException {
     PlanMapper pm0 = getMapperForQuery(driver, "select u from tu where id_uv = 1 group by u");
     PlanMapper pm1 = getMapperForQuery(driver, "select u from tu where id_uv = 2 group by u");
 
-    assertHelper(AssertHelperOp.SAME, pm0, pm1, TableScanOperator.class);
+    //Since we have hive.optimize.index.filter=true we will have different table scan operator
+    assertHelper(AssertHelperOp.NOT_SAME, pm0, pm1, TableScanOperator.class);
     assertHelper(AssertHelperOp.NOT_SAME, pm0, pm1, FilterOperator.class);
 
   }

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -1754,7 +1754,7 @@ public Partition loadPartition(Path loadPath, Table tbl, Map<String, String> par
           Utilities.FILE_OP_LOGGER.trace("moving " + loadPath + " to " + destPath);
         }
 
-        boolean isManaged = tbl.getTableType().equals(TableType.MANAGED_TABLE.toString());
+        boolean isManaged = tbl.getTableType() == TableType.MANAGED_TABLE;
         // TODO: why is "&& !isAcidIUDoperation" needed here?
         if (!isTxnTable && ((loadFileType == LoadFileType.REPLACE_ALL) || (oldPart == null && !isAcidIUDoperation))) {
           //for fullAcid tables we don't delete files for commands with OVERWRITE - we create a new
@@ -2338,7 +2338,7 @@ public void loadTable(Path loadPath, String tableName, LoadFileType loadFileType
 
       perfLogger.PerfLogBegin("MoveTask", PerfLogger.FILE_MOVES);
 
-      boolean isManaged = tbl.getTableType().equals(TableType.MANAGED_TABLE.toString());
+      boolean isManaged = tbl.getTableType() == TableType.MANAGED_TABLE;
 
       if (loadFileType == LoadFileType.REPLACE_ALL && !isTxnTable) {
         //for fullAcid we don't want to delete any files even for OVERWRITE see HIVE-14988/HIVE-17361

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1607,7 +1607,7 @@ public static enum ConfVars {
         "columns in operators such as Aggregate or Join so that we try to reduce the number of shuffling stages"),
 
     // materialized views
-    HIVE_MATERIALIZED_VIEW_ENABLE_AUTO_REWRITING("hive.materializedview.rewriting", false,
+    HIVE_MATERIALIZED_VIEW_ENABLE_AUTO_REWRITING("hive.materializedview.rewriting", true,
         "Whether to try to rewrite queries using the materialized views enabled for rewriting"),
     HIVE_MATERIALIZED_VIEW_REWRITING_SELECTION_STRATEGY("hive.materializedview.rewriting.strategy", "heuristic",
         new StringSet("heuristic", "costbased"),

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPMod.java
Patch:
@@ -21,8 +21,6 @@
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumn;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumnChecked;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.*;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizationContext.java
Patch:
@@ -54,7 +54,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringGroupColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumn;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloLongColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColEqualLongScalar;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColGreaterLongScalar;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorArithmeticExpressions.java
Patch:
@@ -54,6 +54,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DecimalScalarSubtractDecimalColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DecimalScalarMultiplyDecimalColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddLongScalarChecked;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloLongColumn;
 import org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPMod.java
Patch:
@@ -21,6 +21,8 @@
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumn;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumnChecked;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.*;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizationContext.java
Patch:
@@ -54,7 +54,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringGroupColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloLongColumn;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColEqualLongScalar;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColGreaterLongScalar;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorArithmeticExpressions.java
Patch:
@@ -54,7 +54,6 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DecimalScalarSubtractDecimalColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DecimalScalarMultiplyDecimalColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddLongScalarChecked;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloLongColumn;
 import org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPMod.java
Patch:
@@ -21,8 +21,6 @@
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumn;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumnChecked;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.*;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizationContext.java
Patch:
@@ -54,7 +54,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringGroupColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColModuloLongColumn;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloLongColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColEqualLongScalar;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.LongColGreaterLongScalar;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorArithmeticExpressions.java
Patch:
@@ -54,6 +54,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DecimalScalarSubtractDecimalColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DecimalScalarMultiplyDecimalColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddLongScalarChecked;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloLongColumn;
 import org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestLocationQueries.java
Patch:
@@ -109,7 +109,8 @@ public void testAlterTablePartitionLocation_alter5() throws Exception {
 
     for (int i = 0; i < qfiles.length; i++) {
       qt[i] = new CheckResults(resDir, logDir, MiniClusterType.none, "0.20", "parta");
-      qt[i].addFile(qfiles[i]);
+      qt[i].newSession();
+      qt[i].addFile(qfiles[i], false);
       qt[i].clearTestSideEffects();
     }
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestMTQueries.java
Patch:
@@ -46,6 +46,7 @@ public void testMTQueries1() throws Exception {
       util.getConf().set("hive.stats.dbclass", "fs");
       util.getConf().set("hive.mapred.mode", "nonstrict");
       util.getConf().set("hive.stats.column.autogather", "false");
+      util.newSession(true);
     }
     boolean success = QTestUtil.queryListRunnerMultiThreaded(qfiles, qts);
     if (!success) {

File: itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloQTestUtil.java
Patch:
@@ -32,6 +32,7 @@ public AccumuloQTestUtil(String outDir, String logDir, MiniClusterType miniMr,
     super(outDir, logDir, miniMr, null, "0.20", initScript, cleanupScript, false);
     setup.setupWithHiveConf(conf);
     this.setup = setup;
+    this.savedConf = new HiveConf(conf);
   }
 
   @Override

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliConfigs.java
Patch:
@@ -84,7 +84,7 @@ public ParseNegativeConfig() {
         setResultsDir("ql/src/test/results/compiler/errors");
         setLogDir("itests/qtest/target/qfile-results/negative");
 
-        setInitScript("q_test_init.sql");
+        setInitScript("q_test_init_parse.sql");
         setCleanupScript("q_test_cleanup.sql");
 
         setHiveConfDir("data/conf/perf-reg/");

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CorePerfCliDriver.java
Patch:
@@ -68,6 +68,7 @@ public void beforeClass() {
           cleanupScript, false, null);
 
       // do a one time initialization
+      qt.newSession();
       qt.cleanUp();
       qt.createSources();
       // Manually modify the underlying metastore db to reflect statistics corresponding to
@@ -92,7 +93,7 @@ public void shutdown() throws Exception {
   @Override
   public void setUp() {
     try {
-      qt.clearPostTestEffects();
+      qt.newSession();
     } catch (Exception e) {
       System.err.println("Exception: " + e.getMessage());
       e.printStackTrace();
@@ -126,7 +127,7 @@ public void runTest(String name, String fname, String fpath) throws Exception {
       System.err.println("Begin query: " + fname);
 
       qt.addFile(fpath);
-      qt.cliInit(new File(fpath), false);
+      qt.cliInit(new File(fpath));
 
       int ecode = qt.executeClient(fname);
       if (ecode != 0) {

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/parse/CoreParseNegative.java
Patch:
@@ -57,6 +57,7 @@ public void beforeClass() {
       String hadoopVer = cliConfig.getHadoopVersion();
       qt = new QTestUtil((cliConfig.getResultsDir()), (cliConfig.getLogDir()), miniMR, null,
           hadoopVer, initScript, cleanupScript, false);
+      qt.newSession();
     } catch (Exception e) {
       System.err.println("Exception: " + e.getMessage());
       e.printStackTrace();
@@ -106,7 +107,7 @@ public void runTest(String tname, String fname, String fpath) throws Exception {
         firstRun = false;
       }
 
-      qt.cliInit(new File(fpath), false);
+      qt.cliInit(new File(fpath));
 
       ASTNode tree = qt.parseQuery(fname);
       List<Task<? extends Serializable>> tasks = qt.analyzeAST(tree);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3563,7 +3563,7 @@ public static enum ConfVars {
         "The default value is false."),
     HIVE_VECTORIZATION_ROW_DESERIALIZE_INPUTFORMAT_EXCLUDES(
         "hive.vectorized.row.serde.inputformat.excludes",
-        "org.apache.parquet.hadoop.ParquetInputFormat,org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat,org.apache.hive.storage.jdbc.JdbcInputFormat",
+        "org.apache.parquet.hadoop.ParquetInputFormat,org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat",
         "The input formats not supported by row deserialize vectorization."),
     HIVE_VECTOR_ADAPTOR_USAGE_MODE("hive.vectorized.adaptor.usage.mode", "all", new StringSet("none", "chosen", "all"),
         "Specifies the extent to which the VectorUDFAdaptor will be used for UDFs that do not have a corresponding vectorized class.\n" +

File: spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java
Patch:
@@ -101,7 +101,8 @@ public void initChannel(SocketChannel ch) throws Exception {
             Runnable cancelTask = new Runnable() {
                 @Override
                 public void run() {
-                  LOG.warn("Timed out waiting for test message from Remote Spark driver.");
+                  LOG.warn("Timed out waiting for the completion of SASL negotiation "
+                          + "between HiveServer2 and the Remote Spark Driver.");
                   newRpc.close();
                 }
             };

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hadoop.hive.metastore.ReplChangeManager;
 import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.EximUtil;
 import org.apache.hadoop.hive.ql.parse.ReplicationSpec;
 import org.apache.hadoop.hive.ql.plan.CopyWork;
@@ -165,7 +166,7 @@ protected int execute(DriverContext driverContext) {
     } catch (Exception e) {
       LOG.error(StringUtils.stringifyException(e));
       setException(e);
-      return (1);
+      return ErrorMsg.getErrorMsg(e.getMessage()).getErrorCode();
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
Patch:
@@ -40,6 +40,7 @@
 import org.apache.hadoop.hive.metastore.messaging.event.filters.EventBoundaryFilter;
 import org.apache.hadoop.hive.metastore.messaging.event.filters.MessageFormatFilter;
 import org.apache.hadoop.hive.ql.DriverContext;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.io.AcidUtils;
 import org.apache.hadoop.hive.ql.lockmgr.LockException;
@@ -123,7 +124,7 @@ protected int execute(DriverContext driverContext) {
     } catch (Exception e) {
       LOG.error("failed", e);
       setException(e);
-      return 1;
+      return ErrorMsg.getErrorMsg(e.getMessage()).getErrorCode();
     }
     return 0;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/ReplLoadTask.java
Patch:
@@ -20,6 +20,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.ql.DriverContext;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.exec.repl.ReplStateLogWork;
@@ -223,7 +224,7 @@ a database ( directory )
     } catch (Exception e) {
       LOG.error("failed replication", e);
       setException(e);
-      return 1;
+      return ErrorMsg.getErrorMsg(e.getMessage()).getErrorCode();
     }
     LOG.info("completed load task run : {}", work.executedLoadTask());
     return 0;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/events/filesystem/DatabaseEventsIterator.java
Patch:
@@ -91,8 +91,8 @@ public boolean hasNext() {
       return true;
     } catch (Exception e) {
       // may be do some retry logic here.
-      throw new RuntimeException("could not traverse the file via remote iterator " + dbLevelPath,
-          e);
+      LOG.error("could not traverse the file via remote iterator " + dbLevelPath, e);
+      throw new RuntimeException(e.getMessage(), e);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/TableExport.java
Patch:
@@ -162,7 +162,7 @@ private void writeData(PartitionIterable partitions) throws SemanticException {
             .export(replicationSpec);
       }
     } catch (Exception e) {
-      throw new SemanticException(e);
+      throw new SemanticException(e.getMessage(), e);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java
Patch:
@@ -21,19 +21,18 @@
 import java.io.IOException;
 import java.io.OutputStreamWriter;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.List;
 
 import javax.security.auth.login.LoginException;
 
-import org.apache.curator.shaded.com.google.common.collect.Lists;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.common.ValidWriteIdList;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.ReplChangeManager;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.AcidUtils;
 import org.apache.hadoop.hive.ql.io.HiveInputFormat;
@@ -161,7 +160,7 @@ private void exportFilesAsList() throws SemanticException, IOException, LoginExc
         logger.info("writeFilesList failed", e);
         if (repeat >= FileUtils.MAX_IO_ERROR_RETRY) {
           logger.error("exporting data files in dir : " + dataPathList + " to " + exportRootDataDir + " failed");
-          throw e;
+          throw new IOException(ErrorMsg.REPL_FILE_SYSTEM_OPERATION_RETRY.getMsg());
         }
 
         int sleepTime = FileUtils.getSleepTime(repeat - 1);

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/messaging/EventUtils.java
Patch:
@@ -93,7 +93,7 @@ public List<NotificationEvent> getNextNotificationEvents(
       try {
         return msc.getNextNotification(pos,getBatchSize(), filter).getEvents();
       } catch (TException e) {
-        throw new IOException(e);
+        throw new IOException(e.getMessage(), e);
       }
     }
   }
@@ -179,7 +179,7 @@ public boolean hasNext() {
         // but throwing the exception is the appropriate result here, and hasNext()
         // signature will only allow RuntimeExceptions. Iterator.hasNext() really
         // should have allowed IOExceptions
-        throw new RuntimeException(e);
+        throw new RuntimeException(e.getMessage(), e);
       }
       // New batch has been fetched. If it's not empty, we have more elements to process.
       return !batch.isEmpty();

File: ql/src/java/org/apache/hadoop/hive/ql/Context.java
Patch:
@@ -356,6 +356,7 @@ protected Context(Context ctx) {
     this.executionIndex = ctx.executionIndex;
     this.viewsTokenRewriteStreams = new HashMap<>();
     this.rewrittenStatementContexts = new HashSet<>();
+    this.opContext = new CompilationOpContext();
   }
 
   public Map<String, Path> getFsScratchDirs() {

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestReplChangeManager.java
Patch:
@@ -157,15 +157,15 @@ public void testRecyclePartTable() throws Exception {
     Partition part3 = createPartition(dbName, tblName, columns, values, serdeInfo);
     client.add_partition(part3);
 
-    Path part1Path = new Path(warehouse.getDefaultPartitionPath(db, tblName, ImmutableMap.of("dt", "20160101")), "part");
+    Path part1Path = new Path(warehouse.getDefaultPartitionPath(db, tbl, ImmutableMap.of("dt", "20160101")), "part");
     createFile(part1Path, "p1");
     String path1Chksum = ReplChangeManager.checksumFor(part1Path, fs);
 
-    Path part2Path = new Path(warehouse.getDefaultPartitionPath(db, tblName, ImmutableMap.of("dt", "20160102")), "part");
+    Path part2Path = new Path(warehouse.getDefaultPartitionPath(db, tbl, ImmutableMap.of("dt", "20160102")), "part");
     createFile(part2Path, "p2");
     String path2Chksum = ReplChangeManager.checksumFor(part2Path, fs);
 
-    Path part3Path = new Path(warehouse.getDefaultPartitionPath(db, tblName, ImmutableMap.of("dt", "20160103")), "part");
+    Path part3Path = new Path(warehouse.getDefaultPartitionPath(db, tbl, ImmutableMap.of("dt", "20160103")), "part");
     createFile(part3Path, "p3");
     String path3Chksum = ReplChangeManager.checksumFor(part3Path, fs);
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -7824,7 +7824,8 @@ private void handleLineage(LoadTableDesc ltd, Operator output)
       String tName = Utilities.getDbTableName(tableDesc.getTableName())[1];
       try {
         Warehouse wh = new Warehouse(conf);
-        tlocation = wh.getDefaultTablePath(db.getDatabase(tableDesc.getDatabaseName()), tName);
+        tlocation = wh.getDefaultTablePath(db.getDatabase(tableDesc.getDatabaseName()),
+            tName, tableDesc.isExternal());
       } catch (MetaException|HiveException e) {
         throw new SemanticException(e);
       }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java
Patch:
@@ -434,8 +434,10 @@ private void setLoadFileLocation(
   private Path getDefaultCtasLocation(final ParseContext pCtx) throws SemanticException {
     try {
       String protoName = null;
+      boolean isExternal = false;
       if (pCtx.getQueryProperties().isCTAS()) {
         protoName = pCtx.getCreateTable().getTableName();
+        isExternal = pCtx.getCreateTable().isExternal();
       } else if (pCtx.getQueryProperties().isMaterializedView()) {
         protoName = pCtx.getCreateViewDesc().getViewName();
       }
@@ -444,7 +446,7 @@ private Path getDefaultCtasLocation(final ParseContext pCtx) throws SemanticExce
         throw new SemanticException("ERROR: The database " + names[0] + " does not exist.");
       }
       Warehouse wh = new Warehouse(conf);
-      return wh.getDefaultTablePath(db.getDatabase(names[0]), names[1]);
+      return wh.getDefaultTablePath(db.getDatabase(names[0]), names[1], isExternal);
     } catch (HiveException e) {
       throw new SemanticException(e);
     } catch (MetaException e) {

File: ql/src/java/org/apache/hadoop/hive/ql/util/HiveStrictManagedMigration.java
Patch:
@@ -470,7 +470,7 @@ boolean shouldModifyPartitionLocation(Database dbObj, Table tableObj, Partition
       throws IOException, MetaException {
     String tableName = tableObj.getTableName();
     String partLocation = partObj.getSd().getLocation();
-    Path oldDefaultPartLocation = oldWh.getDefaultPartitionPath(dbObj, tableName, partSpec);
+    Path oldDefaultPartLocation = oldWh.getDefaultPartitionPath(dbObj, tableObj, partSpec);
     return arePathsEqual(conf, partLocation, oldDefaultPartLocation.toString());
   }
 

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -1827,7 +1827,7 @@ private void create_table_core(final RawStore ms, final Table tbl,
         if (!TableType.VIRTUAL_VIEW.toString().equals(tbl.getTableType())) {
           if (tbl.getSd().getLocation() == null
               || tbl.getSd().getLocation().isEmpty()) {
-            tblPath = wh.getDefaultTablePath(db, tbl.getTableName());
+            tblPath = wh.getDefaultTablePath(db, tbl);
           } else {
             if (!isExternal(tbl) && !MetaStoreUtils.isNonNativeTable(tbl)) {
               LOG.warn("Location: " + tbl.getSd().getLocation()

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/TransactionalValidationListener.java
Patch:
@@ -440,7 +440,7 @@ private void validateTableStructure(IHMSHandler hmsHandler, Table table)
         String catName = table.isSetCatName() ? table.getCatName() :
             MetaStoreUtils.getDefaultCatalog(getConf());
         tablePath = wh.getDefaultTablePath(hmsHandler.getMS().getDatabase(
-            catName, table.getDbName()), table.getTableName());
+            catName, table.getDbName()), table);
       } else {
         tablePath = wh.getDnsPath(new Path(table.getSd().getLocation()));
       }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -13144,9 +13144,7 @@ ASTNode analyzeCreateTable(
         try {
           // Generate a unique ID for temp table path.
           // This path will be fixed for the life of the temp table.
-          Path path = new Path(SessionState.getTempTableSpace(conf), UUID.randomUUID().toString());
-          path = Warehouse.getDnsPath(path, conf);
-          location = path.toString();
+          location = SessionState.generateTempTableLocation(conf);
         } catch (MetaException err) {
           throw new SemanticException("Error while generating temp table path:", err);
         }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
Patch:
@@ -822,7 +822,7 @@ public Table toTable(HiveConf conf) throws HiveException {
 
     if (DDLTask.doesTableNeedLocation(tbl)) {
       // If location is specified - ensure that it is a full qualified name
-      DDLTask.makeLocationQualified(tbl.getDbName(), tbl.getTTable().getSd(), tableName, conf);
+      DDLTask.makeLocationQualified(tbl.getDbName(), tbl, conf);
     }
 
     if (isExternal()) {

File: itests/hive-unit/src/test/java/org/apache/hive/beeline/hs2connection/TestBeelineWithUserHs2ConnectionFile.java
Patch:
@@ -29,7 +29,7 @@ public class TestBeelineWithUserHs2ConnectionFile extends BeelineWithHS2Connecti
   public void testBeelineConnectionHttp() throws Exception {
     setupHttpHs2();
     String path = createHttpHs2ConnectionFile();
-    testBeeLineConnection(path, new String[] { "-e", "show tables;" }, tableName);
+    assertBeelineOutputContains(path, new String[] { "-e", "show tables;" }, tableName);
   }
 
   private void setupHttpHs2() throws Exception {
@@ -64,7 +64,7 @@ private String createHttpHs2ConnectionFile() throws Exception {
   public void testBeelineConnectionNoAuth() throws Exception {
     setupNoAuthConfHS2();
     String path = createNoAuthHs2ConnectionFile();
-    testBeeLineConnection(path, new String[] { "-e", "show tables;" }, tableName);
+    assertBeelineOutputContains(path, new String[] { "-e", "show tables;" }, tableName);
   }
 
   private void setupNoAuthConfHS2() throws Exception {
@@ -91,7 +91,7 @@ private String createNoAuthHs2ConnectionFile() throws Exception {
   public void testBeelineConnectionSSL() throws Exception {
     setupSslHs2();
     String path = createSSLHs2ConnectionFile();
-    testBeeLineConnection(path, new String[] { "-e", "show tables;" }, tableName);
+    assertBeelineOutputContains(path, new String[] { "-e", "show tables;" }, tableName);
   }
 
   private String createSSLHs2ConnectionFile() throws Exception {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringGroupColConcatStringScalar.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import java.nio.charset.StandardCharsets;
 import java.util.Arrays;
 
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
@@ -164,7 +163,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
         .setNumArguments(2)
         .setArgumentTypes(
             VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,
-            VectorExpressionDescriptor.ArgumentType.STRING)
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringGroupConcatColCol.java
Patch:
@@ -128,7 +128,7 @@ public void evaluate(VectorizedRowBatch batch) throws HiveException {
           }
         } else {
           for(int i = 0; i != n; i++) {
-            if (!inV2.isNull[0]) {
+            if (!inV2.isNull[i]) {
               outV.setConcat(i, vector1[0], start1[0], len1[0], vector2[i], start2[i], len2[i]);
             }
           }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringScalarConcatStringGroupCol.java
Patch:
@@ -163,7 +163,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.STRING,
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,
             VectorExpressionDescriptor.ArgumentType.STRING_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.SCALAR,

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/NoOperatorReuseCheckerHook.java
Patch:
@@ -25,7 +25,7 @@
 import java.util.Map;
 import java.util.Stack;
 
-import org.apache.hadoop.hbase.shaded.com.google.common.collect.Lists;
+import com.google.common.collect.Lists;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.lib.DefaultGraphWalker;

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -350,11 +350,11 @@ private static String getIPAddress() {
       return null;
     }
 
-    private static int nextSerialNum = 0;
+    private static AtomicInteger nextSerialNum = new AtomicInteger();
     private static ThreadLocal<Integer> threadLocalId = new ThreadLocal<Integer>() {
       @Override
       protected Integer initialValue() {
-        return nextSerialNum++;
+        return nextSerialNum.getAndIncrement();
       }
     };
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2076,7 +2076,7 @@ public static enum ConfVars {
     HIVE_ENFORCE_NOT_NULL_CONSTRAINT("hive.constraint.notnull.enforce", true,
         "Should \"IS NOT NULL \" constraint be enforced?"),
 
-    HIVE_AUTO_SORTMERGE_JOIN("hive.auto.convert.sortmerge.join", false,
+    HIVE_AUTO_SORTMERGE_JOIN("hive.auto.convert.sortmerge.join", true,
         "Will the join be automatically converted to a sort-merge join, if the joined tables pass the criteria for sort-merge join."),
     HIVE_AUTO_SORTMERGE_JOIN_REDUCE("hive.auto.convert.sortmerge.join.reduce.side", true,
         "Whether hive.auto.convert.sortmerge.join (if enabled) should be applied to reduce side."),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
Patch:
@@ -734,7 +734,7 @@ private void processKey(Object row,
   @Override
   public void process(Object row, int tag) throws HiveException {
     firstRow = false;
-    ObjectInspector rowInspector = inputObjInspectors[tag];
+    ObjectInspector rowInspector = inputObjInspectors[0];
     // Total number of input rows is needed for hash aggregation only
     if (hashAggr) {
       numRowsInput++;

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -585,6 +585,8 @@ public enum ErrorMsg {
       "Cannot create Spark client on a closed session {0}", true),
 
   SPARK_JOB_INTERRUPTED(30044, "Spark job was interrupted while executing"),
+  SPARK_GET_JOB_INFO_INTERRUPTED(30045, "Spark job was interrupted while getting job info"),
+  SPARK_GET_JOB_INFO_EXECUTIONERROR(30046, "Spark job failed in execution while getting job info due to exception {0}"),
 
   //========================== 40000 range starts here ========================//
 

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcWithMiniLlapArrow.java
Patch:
@@ -217,7 +217,7 @@ public void testDataTypes() throws Exception {
     //assertEquals("d", mapVal.get("c"));
     //assertEquals(Integer.valueOf(2), listVal.get(1));
 
-    assertEquals(Timestamp.valueOf("2012-04-22 09:00:00.123456789"), rowValues[16]);
+    assertEquals(Timestamp.valueOf("2012-04-22 09:00:00.123456"), rowValues[16]);
     assertEquals(new BigDecimal("123456789.123456"), rowValues[17]);
     assertArrayEquals("abcd".getBytes("UTF-8"), (byte[]) rowValues[18]);
     assertEquals(Date.valueOf("2013-01-01"), rowValues[19]);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4206,7 +4206,7 @@ public static enum ConfVars {
         "in shuffle. This should result in less shuffled data."),
     SPARK_CLIENT_FUTURE_TIMEOUT("hive.spark.client.future.timeout",
       "60s", new TimeValidator(TimeUnit.SECONDS),
-      "Timeout for requests from Hive client to remote Spark driver."),
+      "Timeout for requests between Hive client and remote Spark driver."),
     SPARK_JOB_MONITOR_TIMEOUT("hive.spark.job.monitor.timeout",
       "60s", new TimeValidator(TimeUnit.SECONDS),
       "Timeout for job monitor to get Spark job state."),

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitions.java
Patch:
@@ -95,8 +95,10 @@ public void setUp() throws Exception {
   @After
   public void tearDown() throws Exception {
     try {
-      if (client != null) {
+      try {
         client.close();
+      } catch (Exception e) {
+        // HIVE-19729: Shallow the exceptions based on the discussion in the Jira
       }
     } finally {
       client = null;

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommandsForMmTable.java
Patch:
@@ -284,6 +284,7 @@ public void testInsertOverwriteForPartitionedMmTable() throws Exception {
     for(int h=0; h < pStrings.length; h++) {
       status = fs.listStatus(new Path(TEST_WAREHOUSE_DIR + "/" +
           (TableExtended.MMTBLPART).toString().toLowerCase() + pStrings[h]), FileUtils.STAGING_DIR_PATH_FILTER);
+      Arrays.sort(status);
       // There should be 1 delta dir, plus a base dir in the location
       Assert.assertEquals(2, status.length);
       for (int i = 0; i < status.length; i++) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDoubleToDecimal.java
Patch:
@@ -54,7 +54,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.FLOAT)
+            VectorExpressionDescriptor.ArgumentType.DOUBLE)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CharScalarConcatStringGroupCol.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.HiveChar;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
 /**
@@ -28,8 +27,8 @@
 public class CharScalarConcatStringGroupCol extends StringScalarConcatStringGroupCol {
   private static final long serialVersionUID = 1L;
 
-  public CharScalarConcatStringGroupCol(HiveChar value, int colNum, int outputColumnNum) {
-    super(value.getStrippedValue().getBytes(), colNum, outputColumnNum);
+  public CharScalarConcatStringGroupCol(byte[] value, int colNum, int outputColumnNum) {
+    super(value, colNum, outputColumnNum);
   }
 
   public CharScalarConcatStringGroupCol() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprStringGroupColumnCharScalar.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.HiveChar;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
 /**
@@ -31,8 +30,8 @@ public class IfExprStringGroupColumnCharScalar extends IfExprStringGroupColumnSt
 
   private static final long serialVersionUID = 1L;
 
-  public IfExprStringGroupColumnCharScalar(int arg1Column, int arg2Column, HiveChar arg3Scalar, int outputColumnNum) {
-    super(arg1Column, arg2Column, arg3Scalar.getValue().getBytes(), outputColumnNum);
+  public IfExprStringGroupColumnCharScalar(int arg1Column, int arg2Column, byte[] arg3Scalar, int outputColumnNum) {
+    super(arg1Column, arg2Column, arg3Scalar, outputColumnNum);
   }
 
   public IfExprStringGroupColumnCharScalar() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprStringGroupColumnVarCharScalar.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
 /**
@@ -31,8 +30,9 @@ public class IfExprStringGroupColumnVarCharScalar extends IfExprStringGroupColum
 
   private static final long serialVersionUID = 1L;
 
-  public IfExprStringGroupColumnVarCharScalar(int arg1Column, int arg2Column, HiveVarchar arg3Scalar, int outputColumnNum) {
-    super(arg1Column, arg2Column, arg3Scalar.getValue().getBytes(), outputColumnNum);
+  public IfExprStringGroupColumnVarCharScalar(int arg1Column, int arg2Column, byte[] arg3Scalar,
+      int outputColumnNum) {
+    super(arg1Column, arg2Column, arg3Scalar, outputColumnNum);
   }
 
   public IfExprStringGroupColumnVarCharScalar() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringGroupColConcatCharScalar.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.HiveChar;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
 /**
@@ -28,8 +27,8 @@
 public class StringGroupColConcatCharScalar extends StringGroupColConcatStringScalar {
   private static final long serialVersionUID = 1L;
 
-  public StringGroupColConcatCharScalar(int colNum, HiveChar value, int outputColumnNum) {
-    super(colNum, value.getStrippedValue().getBytes(), outputColumnNum);
+  public StringGroupColConcatCharScalar(int colNum, byte[] value, int outputColumnNum) {
+    super(colNum, value, outputColumnNum);
   }
 
   public StringGroupColConcatCharScalar() {

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java
Patch:
@@ -428,14 +428,13 @@ public void flatten(boolean selectedInUse, int[] sel, int size) {
       // at position 0 is undefined if the position 0 value is null.
       if (noNulls || !isNull[0]) {
 
-        // loops start at position 1 because position 0 is already set
         if (selectedInUse) {
-          for (int j = 1; j < size; j++) {
+          for (int j = 0; j < size; j++) {
             int i = sel[j];
             this.setRef(i, vector[0], start[0], length[0]);
           }
         } else {
-          for (int i = 1; i < size; i++) {
+          for (int i = 0; i < size; i++) {
             this.setRef(i, vector[0], start[0], length[0]);
           }
         }

File: ql/src/test/org/apache/hadoop/hive/ql/hooks/TestHiveProtoLoggingHook.java
Patch:
@@ -38,6 +38,8 @@
 import org.apache.hadoop.hive.ql.log.PerfLogger;
 import org.apache.hadoop.hive.ql.plan.HiveOperation;
 import org.apache.hadoop.yarn.util.SystemClock;
+import org.apache.tez.dag.history.logging.proto.DatePartitionedLogger;
+import org.apache.tez.dag.history.logging.proto.ProtoMessageReader;
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Rule;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -13253,7 +13253,7 @@ protected ASTNode analyzeCreateView(ASTNode ast, QB qb, PlannerContext plannerCt
     String dbDotTable = getDotName(qualTabName);
     List<FieldSchema> cols = null;
     boolean ifNotExists = false;
-    boolean rewriteEnabled = false;
+    boolean rewriteEnabled = true;
     boolean orReplace = false;
     boolean isAlterViewAs = false;
     String comment = null;
@@ -13278,8 +13278,8 @@ protected ASTNode analyzeCreateView(ASTNode ast, QB qb, PlannerContext plannerCt
       case HiveParser.TOK_IFNOTEXISTS:
         ifNotExists = true;
         break;
-      case HiveParser.TOK_REWRITE_ENABLED:
-        rewriteEnabled = true;
+      case HiveParser.TOK_REWRITE_DISABLED:
+        rewriteEnabled = false;
         break;
       case HiveParser.TOK_ORREPLACE:
         orReplace = true;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2007,7 +2007,7 @@ public static enum ConfVars {
         "However, if it is on, and the predicted number of entries in hashtable for a given join \n" +
         "input is larger than this number, the join will not be converted to a mapjoin. \n" +
         "The value \"-1\" means no limit."),
-    HIVECONVERTJOINMAXSHUFFLESIZE("hive.auto.convert.join.shuffle.max.size", 10000000L,
+    HIVECONVERTJOINMAXSHUFFLESIZE("hive.auto.convert.join.shuffle.max.size", 10000000000L,
        "If hive.auto.convert.join.noconditionaltask is off, this parameter does not take affect. \n" +
        "However, if it is on, and the predicted size of the larger input for a given join is greater \n" +
        "than this number, the join will not be converted to a dynamically partitioned hash join. \n" +

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/ParquetDataColumnReader.java
Patch:
@@ -110,9 +110,11 @@ public interface ParquetDataColumnReader {
    * The type of the data in Parquet files need not match the type in HMS.  In that case
    * the value returned to the user will depend on the data.  If the data value is within the valid
    * range accommodated by the HMS type, the data will be returned as is.  When data is not within
-   * the valid range, a NULL will be returned.  This function will do the appropriate check.
+   * the valid range, a NULL will be returned.  These functions will do the appropriate check.
    */
   boolean isValid(long value);
+  boolean isValid(float value);
+  boolean isValid(double value);
 
   /**
    * @return the underlying dictionary if current reader is dictionary encoded

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecWMEventsSummaryPrinter.java
Patch:
@@ -54,6 +54,7 @@ public void run(HookContext hookContext) throws Exception {
     for (TezTask tezTask : rootTasks) {
       WmContext wmContext = tezTask.getDriverContext().getCtx().getWmContext();
       if (wmContext != null) {
+        wmContext.printJson(console);
         wmContext.shortPrint(console);
       }
     }

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestTablesCreateDropAlterTruncate.java
Patch:
@@ -36,6 +36,7 @@
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
 import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.PrincipalType;
 import org.apache.hadoop.hive.metastore.api.SerDeInfo;
 import org.apache.hadoop.hive.metastore.api.SkewedInfo;
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
@@ -256,6 +257,7 @@ public void testCreateTableDefaultValues() throws Exception {
     client.createTable(table);
     Table createdTable = client.getTable(table.getDbName(), table.getTableName());
 
+    Assert.assertEquals("Comparing OwnerType", PrincipalType.USER, createdTable.getOwnerType());
     Assert.assertNull("Comparing OwnerName", createdTable.getOwner());
     Assert.assertNotEquals("Comparing CreateTime", 0, createdTable.getCreateTime());
     Assert.assertEquals("Comparing LastAccessTime", 0, createdTable.getLastAccessTime());
@@ -1334,6 +1336,7 @@ private Table getTableWithAllParametersSet() throws MetaException {
                .setDbName(DEFAULT_DATABASE)
                .setTableName("test_table_with_all_parameters_set")
                .setCreateTime(100)
+               .setOwnerType(PrincipalType.ROLE)
                .setOwner("owner")
                .setLastAccessTime(200)
                .addPartCol("part_col", "int", "part col comment")

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1911,6 +1911,9 @@ public static enum ConfVars {
       "Hive streaming ingest has auto flush mechanism to flush all open record updaters under memory pressure.\n" +
         "When memory usage exceed hive.heap.memory.monitor.default.usage.threshold, the auto-flush mechanism will \n" +
         "wait until this size (default 100Mb) of records are ingested before triggering flush."),
+    HIVE_CLASSLOADER_SHADE_PREFIX("hive.classloader.shade.prefix", "", "During reflective instantiation of a class\n" +
+      "(input, output formats, serde etc.), when classloader throws ClassNotFoundException, as a fallback this\n" +
+      "shade prefix will be used before class reference and retried."),
 
     HIVE_ORC_MS_FOOTER_CACHE_ENABLED("hive.orc.splits.ms.footer.cache.enabled", false,
         "Whether to enable using file metadata cache in metastore for ORC file footers."),

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java
Patch:
@@ -166,7 +166,7 @@ public org.apache.hadoop.hive.metastore.api.Table getTable(String dbname, String
       return deepCopy(table);  // Original method used deepCopy(), do the same here.
     }
     // Try underlying client
-    return super.getTable(DEFAULT_CATALOG_NAME, dbname, name);
+    return super.getTable(MetaStoreUtils.getDefaultCatalog(conf), dbname, name);
   }
 
   // Need to override this one too or dropTable breaks because it doesn't find the table when checks

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2688,9 +2688,6 @@ public static enum ConfVars {
             "Wait time in ms default to 30 seconds."
     ),
     HIVE_DRUID_BITMAP_FACTORY_TYPE("hive.druid.bitmap.type", "roaring", new PatternSet("roaring", "concise"), "Coding algorithm use to encode the bitmaps"),
-    HIVE_DRUID_APPROX_RESULT("hive.druid.approx.result", false,
-        "Whether to allow approximate results from druid. \n" +
-        "When set to true decimals will be stored as double and druid is allowed to return approximate results for decimal columns."),
     // For HBase storage handler
     HIVE_HBASE_WAL_ENABLED("hive.hbase.wal.enabled", true,
         "Whether writes to HBase should be forced to the write-ahead log. \n" +

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java
Patch:
@@ -123,6 +123,7 @@ private void initialize(String cmRoot, String warehouseRoot,
     hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
     hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER, false);
     if (!hiveConf.getVar(HiveConf.ConfVars.HIVE_TXN_MANAGER).equals("org.apache.hadoop.hive.ql.lockmgr.DbTxnManager")) {
       hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     }

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java
Patch:
@@ -296,6 +296,9 @@ private void repositionInStreams(TreeReaderFactory.TreeReader[] columnReaders,
       ConsumerStripeMetadata stripeMetadata) throws IOException {
     PositionProvider[] pps = createPositionProviders(
         columnReaders, batch.getBatchKey(), stripeMetadata);
+    if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {
+      LlapIoImpl.ORC_LOGGER.trace("Created pps {}", Arrays.toString(pps));
+    }
     if (pps == null) return;
     for (int i = 0; i < columnReaders.length; i++) {
       TreeReader reader = columnReaders[i];

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -214,7 +214,7 @@ public final class FunctionRegistry {
     system.registerUDF("rand", UDFRand.class, false);
     system.registerGenericUDF("abs", GenericUDFAbs.class);
     system.registerGenericUDF("sq_count_check", GenericUDFSQCountCheck.class);
-    system.registerGenericUDF("enforce_constraint", GenericUDFEnforceNotNullConstraint.class);
+    system.registerGenericUDF("enforce_constraint", GenericUDFEnforceConstraint.class);
     system.registerGenericUDF("pmod", GenericUDFPosMod.class);
 
     system.registerUDF("ln", UDFLn.class, false);

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFEnforceConstraint.java
Patch:
@@ -33,9 +33,9 @@
  *
  */
 @Description(name = "enforce_constraint",
-    value = "_FUNC_(x) - Internal UDF to enforce NOT NULL constraint",
+    value = "_FUNC_(x) - Internal UDF to enforce CHECK and NOT NULL constraint",
     extended = "For internal use only")
-public class GenericUDFEnforceNotNullConstraint extends GenericUDF {
+public class GenericUDFEnforceConstraint extends GenericUDF {
   private final BooleanWritable resultBool = new BooleanWritable();
   private transient BooleanObjectInspector boi;
 
@@ -59,7 +59,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
 
     if(!result) {
       throw new DataConstraintViolationError(
-          "NOT NULL constraint violated!");
+          "Either CHECK or NOT NULL constraint violated!");
     }
     resultBool.set(true);
     return resultBool;

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/tools/HiveSchemaHelper.java
Patch:
@@ -44,6 +44,9 @@ public class HiveSchemaHelper {
   public static final String DB_MYSQL = "mysql";
   public static final String DB_POSTGRACE = "postgres";
   public static final String DB_ORACLE = "oracle";
+  public static final String EMBEDDED_HS2_URL = "jdbc:hive2://";
+  public static final String HIVE_JDBC_DRIVER = "org.apache.hive.jdbc.HiveDriver";
+  
 
   /***
    * Get JDBC connection to metastore db

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3536,7 +3536,7 @@ public static enum ConfVars {
         "The default value is false."),
     HIVE_VECTORIZATION_ROW_DESERIALIZE_INPUTFORMAT_EXCLUDES(
         "hive.vectorized.row.serde.inputformat.excludes",
-        "org.apache.parquet.hadoop.ParquetInputFormat,org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat",
+        "org.apache.parquet.hadoop.ParquetInputFormat,org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat,org.apache.hive.storage.jdbc.JdbcInputFormat",
         "The input formats not supported by row deserialize vectorization."),
     HIVE_VECTOR_ADAPTOR_USAGE_MODE("hive.vectorized.adaptor.usage.mode", "all", new StringSet("none", "chosen", "all"),
         "Specifies the extent to which the VectorUDFAdaptor will be used for UDFs that do not have a corresponding vectorized class.\n" +

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/DumpMetaData.java
Patch:
@@ -78,7 +78,6 @@ private void loadDumpFromFile() throws SemanticException {
             Long.valueOf(lineContents[2]),
             new Path(lineContents[3]));
         setPayload(lineContents[4].equals(Utilities.nullStringOutput) ? null : lineContents[4]);
-        ReplChangeManager.setCmRoot(cmRoot);
       } else {
         throw new IOException(
             "Unable to read valid values from dumpFile:" + dumpFile.toUri().toString());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java
Patch:
@@ -181,14 +181,14 @@ public ResourceUri apply(ResourceUri resourceUri) {
     ResourceUri destinationResourceUri(ResourceUri resourceUri)
         throws IOException, SemanticException {
       String sourceUri = resourceUri.getUri();
-      String[] split = sourceUri.split(Path.SEPARATOR);
+      String[] split = ReplChangeManager.decodeFileUri(sourceUri)[0].split(Path.SEPARATOR);
       PathBuilder pathBuilder = new PathBuilder(functionsRootDir);
       Path qualifiedDestinationPath = PathBuilder.fullyQualifiedHDFSUri(
           pathBuilder
               .addDescendant(destinationDbName.toLowerCase())
               .addDescendant(metadata.function.getFunctionName().toLowerCase())
               .addDescendant(String.valueOf(System.nanoTime()))
-              .addDescendant(ReplChangeManager.getFileWithChksumFromURI(split[split.length - 1])[0])
+              .addDescendant(split[split.length - 1])
               .build(),
           new Path(functionsRootDir).getFileSystem(context.hiveConf)
       );

File: shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
Patch:
@@ -1100,8 +1100,7 @@ List<String> constructDistCpParams(List<Path> srcPaths, Path dst, Configuration
     if (params.size() == 0){
       // if no entries were added via conf, we initiate our defaults
       params.add("-update");
-      params.add("-skipcrccheck");
-      params.add("-pb");
+      params.add("-pbx");
     }
     for (Path src : srcPaths) {
       params.add(src.toString());

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/utils/HdfsUtils.java
Patch:
@@ -205,8 +205,7 @@ private static List<String> constructDistCpParams(List<Path> srcPaths, Path dst,
     if (params.size() == 0){
       // if no entries were added via conf, we initiate our defaults
       params.add("-update");
-      params.add("-skipcrccheck");
-      params.add("-pb");
+      params.add("-pbx");
     }
     for (Path src : srcPaths) {
       params.add(src.toString());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
Patch:
@@ -68,7 +68,6 @@ public abstract class Task<T extends Serializable> implements Serializable, Node
   protected static transient Logger LOG = LoggerFactory.getLogger(Task.class);
   protected int taskTag;
   private boolean isLocalMode =false;
-  private boolean retryCmdWhenFail = false;
 
   public static final int NO_TAG = 0;
   public static final int COMMON_JOIN = 1;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/QueryPlanPostProcessor.java
Patch:
@@ -17,9 +17,6 @@
  */
 package org.apache.hadoop.hive.ql.optimizer;
 
-import java.util.List;
-import java.util.Set;
-
 import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.OperatorUtils;
@@ -53,6 +50,9 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.util.List;
+import java.util.Set;
+
 /**
  * Finds Acid FileSinkDesc objects which can be created in the physical (disconnected) plan, e.g.
  * {@link org.apache.hadoop.hive.ql.parse.GenTezUtils#removeUnionOperators(GenTezProcContext, BaseWork, int)}

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/DefaultHiveMetaHook.java
Patch:
@@ -23,7 +23,7 @@
 
 public abstract class DefaultHiveMetaHook implements HiveMetaHook {
   /**
-   * Called after successfully after INSERT [OVERWRITE] statement is executed.
+   * Called after successfully INSERT [OVERWRITE] statement is executed.
    * @param table table definition
    * @param overwrite true if it is INSERT OVERWRITE
    *

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4224,12 +4224,12 @@ public static enum ConfVars {
        "exception) is the default; 'skip' will skip the invalid directories and still repair the" +
        " others; 'ignore' will skip the validation (legacy behavior, causes bugs in many cases)"),
     HIVE_MSCK_REPAIR_BATCH_SIZE(
-        "hive.msck.repair.batch.size", 0,
+        "hive.msck.repair.batch.size", 3000,
         "Batch size for the msck repair command. If the value is greater than zero,\n "
             + "it will execute batch wise with the configured batch size. In case of errors while\n"
             + "adding unknown partitions the batch size is automatically reduced by half in the subsequent\n"
-            + "retry attempt. The default value is zero which means it will execute directly (not batch wise)"),
-    HIVE_MSCK_REPAIR_BATCH_MAX_RETRIES("hive.msck.repair.batch.max.retries", 0,
+            + "retry attempt. The default value is 3000 which means it will execute in the batches of 3000."),
+    HIVE_MSCK_REPAIR_BATCH_MAX_RETRIES("hive.msck.repair.batch.max.retries", 4,
         "Maximum number of retries for the msck repair command when adding unknown partitions.\n "
         + "If the value is greater than zero it will retry adding unknown partitions until the maximum\n"
         + "number of attempts is reached or batch size is reduced to 0, whichever is earlier.\n"

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4065,7 +4065,7 @@ public static enum ConfVars {
       "llap.daemon.service.port"),
     LLAP_DAEMON_WEB_SSL("hive.llap.daemon.web.ssl", false,
       "Whether LLAP daemon web UI should use SSL.", "llap.daemon.service.ssl"),
-    LLAP_CLIENT_CONSISTENT_SPLITS("hive.llap.client.consistent.splits", false,
+    LLAP_CLIENT_CONSISTENT_SPLITS("hive.llap.client.consistent.splits", true,
         "Whether to setup split locations to match nodes on which llap daemons are running, " +
         "instead of using the locations provided by the split itself. If there is no llap daemon " +
         "running, fall back to locations provided by the split. This is effective only if " +

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliConfigs.java
Patch:
@@ -579,7 +579,7 @@ public SparkOnYarnCliConfig() {
         setInitScript("q_test_init.sql");
         setCleanupScript("q_test_cleanup.sql");
 
-        setHiveConfDir("data/conf/spark/yarn-client");
+        setHiveConfDir("data/conf/spark/yarn-cluster");
         setClusterType(MiniClusterType.miniSparkOnYarn);
       } catch (Exception e) {
         throw new RuntimeException("can't construct cliconfig", e);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/mapper/CachingStatsSource.java
Patch:
@@ -49,7 +49,7 @@ public Optional<OperatorStats> lookup(OpTreeSignature treeSig) {
 
   @Override
   public boolean canProvideStatsFor(Class<?> clazz) {
-    if (Operator.class.isAssignableFrom(clazz)) {
+    if (cache.size() > 0 && Operator.class.isAssignableFrom(clazz)) {
       return true;
     }
     return false;

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -8461,7 +8461,7 @@ public List<RuntimeStat> get_runtime_stats(GetRuntimeStatsRequest rqst) throws T
       startFunction("get_runtime_stats");
       Exception ex = null;
       try {
-        List<RuntimeStat> res = getMS().getRuntimeStats();
+        List<RuntimeStat> res = getMS().getRuntimeStats(rqst.getMaxWeight(), rqst.getMaxCreateTime());
         return res;
       } catch (MetaException e) {
         LOG.error("Caught exception", e);

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java
Patch:
@@ -29,7 +29,6 @@
 import org.apache.hadoop.classification.InterfaceStability;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.common.ValidTxnList;
-import org.apache.hadoop.hive.common.ValidTxnWriteIdList;
 import org.apache.hadoop.hive.common.ValidWriteIdList;
 import org.apache.hadoop.hive.common.classification.RetrySemantics;
 import org.apache.hadoop.hive.metastore.annotation.NoReconnect;
@@ -3648,6 +3647,6 @@ void createOrDropTriggerToPoolMapping(String resourcePlanName, String triggerNam
   void addRuntimeStat(RuntimeStat stat) throws TException;
 
   /** Reads runtime statistics. */
-  List<RuntimeStat> getRuntimeStats() throws TException;
+  List<RuntimeStat> getRuntimeStats(int maxWeight, int maxCreateTime) throws TException;
 
 }

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/RawStore.java
Patch:
@@ -1630,8 +1630,9 @@ List<SchemaVersion> getSchemaVersionsByColumns(String colName, String colNamespa
   void addRuntimeStat(RuntimeStat stat) throws MetaException;
 
   /** Reads runtime statistic entries. */
-  List<RuntimeStat> getRuntimeStats() throws MetaException;
+  List<RuntimeStat> getRuntimeStats(int maxEntries, int maxCreateTime) throws MetaException;
 
   /** Removes outdated statistics. */
-  int deleteRuntimeStats(int maxRetained, int maxRetainSecs) throws MetaException;
+  int deleteRuntimeStats(int maxRetainSecs) throws MetaException;
+
 }

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/RuntimeStatsCleanerTask.java
Patch:
@@ -53,9 +53,8 @@ public void run() {
 
     try {
       RawStore ms = HiveMetaStore.HMSHandler.getMSForConf(conf);
-      int maxRetained = MetastoreConf.getIntVar(conf, MetastoreConf.ConfVars.RUNTIME_STATS_MAX_ENTRIES);
       int maxRetainSecs=(int) MetastoreConf.getTimeVar(conf, MetastoreConf.ConfVars.RUNTIME_STATS_MAX_AGE, TimeUnit.SECONDS);
-      int deleteCnt = ms.deleteRuntimeStats(maxRetained, maxRetainSecs);
+      int deleteCnt = ms.deleteRuntimeStats(maxRetainSecs);
 
       if (deleteCnt > 0L){
         LOG.info("Number of deleted entries: " + deleteCnt);

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
Patch:
@@ -583,9 +583,6 @@ public enum ConfVars {
         TimeUnit.SECONDS, "Frequency at which timer task runs to remove outdated runtime stat entries."),
     RUNTIME_STATS_MAX_AGE("runtime.stats.max.age", "hive.metastore.runtime.stats.max.age", 86400 * 3, TimeUnit.SECONDS,
         "Stat entries which are older than this are removed."),
-    RUNTIME_STATS_MAX_ENTRIES("runtime.stats.max.entries", "hive.metastore.runtime.stats.max.entries", 100_000,
-        "Maximum number of runtime stats to keep; unit is operator stat infos - a complicated query has ~100 of these."
-            + "See also: hive.query.reexecution.stats.cache.size"),
 
     // Parameters for exporting metadata on table drop (requires the use of the)
     // org.apache.hadoop.hive.ql.parse.MetaDataExportListener preevent listener

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/DummyRawStoreForJdoConnection.java
Patch:
@@ -1157,12 +1157,12 @@ public void addRuntimeStat(RuntimeStat stat) throws MetaException {
   }
 
   @Override
-  public List<RuntimeStat> getRuntimeStats() throws MetaException {
+  public List<RuntimeStat> getRuntimeStats(int maxEntries, int maxCreateTime) throws MetaException {
     return Collections.emptyList();
   }
 
   @Override
-  public int deleteRuntimeStats(int maxRetained, int maxRetainSecs) throws MetaException {
+  public int deleteRuntimeStats(int maxRetainSecs) throws MetaException {
     return 0;
   }
 }

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClientPreCatalog.java
Patch:
@@ -53,7 +53,6 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.common.StatsSetupConst;
 import org.apache.hadoop.hive.common.ValidTxnList;
-import org.apache.hadoop.hive.common.ValidTxnWriteIdList;
 import org.apache.hadoop.hive.common.ValidWriteIdList;
 import org.apache.hadoop.hive.metastore.api.*;
 import org.apache.hadoop.hive.metastore.conf.MetastoreConf;
@@ -3359,7 +3358,7 @@ public void addRuntimeStat(RuntimeStat stat) throws TException {
   }
 
   @Override
-  public List<RuntimeStat> getRuntimeStats() throws TException {
+  public List<RuntimeStat> getRuntimeStats(int maxWeight, int maxCreateTime) throws TException {
     throw new UnsupportedOperationException();
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
Patch:
@@ -112,6 +112,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
             // The MR task is just a simple TableScanOperator
 
             BasicStatsWork statsWork = new BasicStatsWork(table.getTableSpec());
+            statsWork.setIsExplicitAnalyze(true);
 
             statsWork.setNoScanAnalyzeCommand(noScan);
             StatsWork columnStatsWork = new StatsWork(table, statsWork, parseCtx.getConf());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java
Patch:
@@ -115,6 +115,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,
         // The Tez task is just a simple TableScanOperator
 
         BasicStatsWork basicStatsWork = new BasicStatsWork(table.getTableSpec());
+        basicStatsWork.setIsExplicitAnalyze(true);
         basicStatsWork.setNoScanAnalyzeCommand(parseContext.getQueryProperties().isNoScanAnalyzeCommand());
         StatsWork columnStatsWork = new StatsWork(table, basicStatsWork, parseContext.getConf());
         columnStatsWork.collectStatsFromAggregator(tableScan.getConf());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java
Patch:
@@ -392,6 +392,7 @@ private Task<?> genTableStats(ParseContext parseContext, TableScanOperator table
       return TaskFactory.get(columnStatsWork);
     } else {
       BasicStatsWork statsWork = new BasicStatsWork(tableScan.getConf().getTableMetadata().getTableSpec());
+      statsWork.setIsExplicitAnalyze(true);
       StatsWork columnStatsWork = new StatsWork(table, statsWork, parseContext.getConf());
       columnStatsWork.collectStatsFromAggregator(tableScan.getConf());
       columnStatsWork.setSourceTask(currentTask);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkProcessAnalyzeTable.java
Patch:
@@ -123,6 +123,7 @@ public Object process(Node nd, Stack<Node> stack,
         // The Spark task is just a simple TableScanOperator
 
         BasicStatsWork basicStatsWork = new BasicStatsWork(table.getTableSpec());
+        basicStatsWork.setIsExplicitAnalyze(true);
         basicStatsWork.setNoScanAnalyzeCommand(parseContext.getQueryProperties().isNoScanAnalyzeCommand());
         StatsWork columnStatsWork = new StatsWork(table, basicStatsWork, parseContext.getConf());
         columnStatsWork.collectStatsFromAggregator(tableScan.getConf());

File: ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java
Patch:
@@ -464,7 +464,7 @@ private InputSplit[] getCombineSplits(JobConf job, int numSplits,
       CombineHiveInputSplit csplit = new CombineHiveInputSplit(job, is, pathToPartitionInfo);
       result.add(csplit);
     }
-    LOG.info("number of splits " + result.size());
+    LOG.debug("Number of splits " + result.size());
     return result.toArray(new InputSplit[result.size()]);
   }
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3425,9 +3425,9 @@ public static enum ConfVars {
         "and use it to run queries."),
 
     // Vectorization enabled
-    HIVE_VECTORIZATION_ENABLED("hive.vectorized.execution.enabled", false,
+    HIVE_VECTORIZATION_ENABLED("hive.vectorized.execution.enabled", true,
         "This flag should be set to true to enable vectorized mode of query execution.\n" +
-        "The default value is false."),
+        "The default value is true to reflect that our most expected Hive deployment will be using vectorization."),
     HIVE_VECTORIZATION_REDUCE_ENABLED("hive.vectorized.execution.reduce.enabled", true,
         "This flag should be set to true to enable vectorized mode of the reduce-side of query execution.\n" +
         "The default value is true."),

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestAcidOnTez.java
Patch:
@@ -101,6 +101,7 @@ public void setUp() throws Exception {
     hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, false);
     hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     hiveConf.setVar(HiveConf.ConfVars.HIVEINPUTFORMAT, HiveInputFormat.class.getName());
     hiveConf

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestMTQueries.java
Patch:
@@ -42,6 +42,7 @@ public void testMTQueries1() throws Exception {
       // derby fails creating multiple stats aggregator concurrently
       util.getConf().setBoolean("hive.exec.submitviachild", true);
       util.getConf().setBoolean("hive.exec.submit.local.task.via.child", true);
+      util.getConf().setBoolean("hive.vectorized.execution.enabled", false);
       util.getConf().set("hive.stats.dbclass", "fs");
       util.getConf().set("hive.mapred.mode", "nonstrict");
       util.getConf().set("hive.stats.column.autogather", "false");

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/AbstractJdbcTriggersTest.java
Patch:
@@ -66,6 +66,7 @@ public static void beforeTest() throws Exception {
     System.out.println("Setting hive-site: " + HiveConf.getHiveSiteLocation());
 
     conf = new HiveConf();
+    conf.setBoolVar(HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, false);
     conf.setBoolVar(ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     conf.setBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS, false);
     conf.setVar(ConfVars.HIVE_SERVER2_TEZ_DEFAULT_QUEUES, "default");

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestTriggersMoveWorkloadManager.java
Patch:
@@ -57,6 +57,7 @@ public static void beforeTest() throws Exception {
     System.out.println("Setting hive-site: " + HiveConf.getHiveSiteLocation());
 
     conf = new HiveConf();
+    conf.setBoolVar(HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, false);
     conf.setBoolVar(ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     conf.setBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS, false);
     conf.setTimeVar(ConfVars.HIVE_TRIGGER_VALIDATION_INTERVAL, 50, TimeUnit.MILLISECONDS);

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java
Patch:
@@ -93,6 +93,7 @@ public TestDbTxnManager2() throws Exception {
     conf
     .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
+    conf.setBoolVar(HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, false);
     TxnDbUtil.setConfValues(conf);
     conf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java
Patch:
@@ -300,6 +300,7 @@ public int execute(DriverContext driverContext) {
       Hive db = getHive();
       return persistColumnStats(db);
     } catch (Exception e) {
+      setException(e);
       LOG.info("Failed to persist stats in metastore", e);
     }
     return 1;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java
Patch:
@@ -65,7 +65,6 @@
 import org.apache.hadoop.hive.ql.security.authorization.AuthorizationFactory;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.util.StringUtils;
 import org.apache.hive.common.util.AnnotationUtils;
 import org.json.JSONArray;
 import org.json.JSONException;
@@ -383,8 +382,8 @@ public int execute(DriverContext driverContext) {
       return (0);
     }
     catch (Exception e) {
-      console.printError("Failed with exception " + e.getMessage(),
-          "\n" + StringUtils.stringifyException(e));
+      LOG.error(org.apache.hadoop.util.StringUtils.stringifyException(e));
+      setException(e);
       return (1);
     }
     finally {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MaterializedViewTask.java
Patch:
@@ -76,6 +76,7 @@ public int execute(DriverContext driverContext) {
       }
     } catch (HiveException e) {
       LOG.debug("Exception during materialized view cache update", e);
+      setException(e);
     }
     return 0;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java
Patch:
@@ -163,8 +163,8 @@ protected int execute(DriverContext driverContext) {
       }
       return 0;
     } catch (Exception e) {
-      console.printError("Failed with exception " + e.getMessage(), "\n"
-          + StringUtils.stringifyException(e));
+      LOG.error(StringUtils.stringifyException(e));
+      setException(e);
       return (1);
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
Patch:
@@ -112,6 +112,7 @@ public int execute(DriverContext driverContext) {
       }
     } catch (Exception e) {
       LOG.error("Failed to run stats task", e);
+      setException(e);
       return 1;
     }
     return 0;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
Patch:
@@ -465,9 +465,9 @@ public int execute(DriverContext driverContext) {
           jc.close();
         }
       } catch (Exception e) {
-	LOG.warn("Failed while cleaning up ", e);
+        LOG.warn("Failed while cleaning up ", e);
       } finally {
-	HadoopJobExecHelper.runningJobs.remove(rj);
+        HadoopJobExecHelper.runningJobs.remove(rj);
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionOverlayPlugin.java
Patch:
@@ -42,7 +42,7 @@ public void run(HookContext hookContext) throws Exception {
       if (hookContext.getHookType() == HookType.ON_FAILURE_HOOK) {
         Throwable exception = hookContext.getException();
         if (exception != null) {
-          if (exception.getMessage().contains("Vertex failed,")) {
+          if (exception.getMessage() != null && exception.getMessage().contains("Vertex failed,")) {
             retryPossible = true;
           }
         }

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -460,6 +460,8 @@ public enum ErrorMsg {
   LOAD_DATA_ACID_FILE(10413,
       "\"{0}\" was created created by Acid write - it cannot be loaded into anther Acid table",
       true),
+  ACID_OP_ON_INSERTONLYTRAN_TABLE(10414, "Attempt to do update or delete on table {0} that is " +
+    "insert-only transactional", true),
 
 
   //========================== 20000 range starts here ========================//

File: llap-common/src/java/org/apache/hadoop/hive/llap/protocol/LlapPluginProtocolPB.java
Patch:
@@ -20,7 +20,7 @@
 import org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos;
 import org.apache.tez.runtime.common.security.JobTokenSelector;
 
-@ProtocolInfo(protocolName = "org.apache.hadoop.hive.llap.protocol.LlapPluginProtocolBlockingPB", protocolVersion = 1)
+@ProtocolInfo(protocolName = "org.apache.hadoop.hive.llap.protocol.LlapPluginProtocolPB", protocolVersion = 1)
 @TokenInfo(JobTokenSelector.class)
 @InterfaceAudience.Private
 public interface LlapPluginProtocolPB extends LlapPluginProtocolProtos.LlapPluginProtocol.BlockingInterface {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -7363,10 +7363,11 @@ protected Operator genFileSinkPlan(String dest, QB qb, Operator input)
         throw new SemanticException("Failed to allocate write Id", ex);
       }
       ltd = new LoadTableDesc(queryTmpdir, table_desc, dest_part.getSpec(), acidOp, writeId);
+      // For the current context for generating File Sink Operator, it is either INSERT INTO or INSERT OVERWRITE.
+      // So the next line works.
+      boolean isInsertInto = !qb.getParseInfo().isDestToOpTypeInsertOverwrite(dest);
       // For Acid table, Insert Overwrite shouldn't replace the table content. We keep the old
       // deltas and base and leave them up to the cleaner to clean up
-      boolean isInsertInto = qb.getParseInfo().isInsertIntoTable(
-          dest_tab.getDbName(), dest_tab.getTableName());
       LoadFileType loadType = (!isInsertInto && !destTableIsTransactional)
           ? LoadFileType.REPLACE_ALL : LoadFileType.KEEP_EXISTING;
       ltd.setLoadFileType(loadType);

File: beeline/src/java/org/apache/hive/beeline/BeeLine.java
Patch:
@@ -60,6 +60,7 @@
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
+import java.util.LinkedHashMap;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.ListIterator;
@@ -1892,7 +1893,7 @@ String[] split(String line, String delim) {
 
 
   static Map<Object, Object> map(Object[] obs) {
-    Map<Object, Object> m = new HashMap<Object, Object>();
+    Map<Object, Object> m = new LinkedHashMap<Object, Object>();
     for (int i = 0; i < obs.length - 1; i += 2) {
       m.put(obs[i], obs[i + 1]);
     }

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordSerDe.java
Patch:
@@ -23,7 +23,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
-import java.util.HashMap;
+import java.util.LinkedHashMap;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.serde.serdeConstants;
@@ -219,7 +219,7 @@ public static Object serializeField(Object field, ObjectInspector fieldObjectIns
   private static Map<?, ?> serializeMap(Object f, MapObjectInspector moi) throws SerDeException {
     ObjectInspector koi = moi.getMapKeyObjectInspector();
     ObjectInspector voi = moi.getMapValueObjectInspector();
-    Map<Object, Object> m = new HashMap<Object, Object>();
+    Map<Object, Object> m = new LinkedHashMap<Object, Object>();
 
     Map<?, ?> readMap = moi.getMap(f);
     if (readMap == null) {

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/ReaderWriter.java
Patch:
@@ -24,7 +24,7 @@
 import java.io.IOException;
 import java.sql.Date;
 import java.util.ArrayList;
-import java.util.HashMap;
+import java.util.LinkedHashMap;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -95,7 +95,7 @@ public static Object readDatum(DataInput in) throws IOException {
 
     case DataType.MAP:
       int size = in.readInt();
-      Map<Object, Object> m = new HashMap<Object, Object>(size);
+      Map<Object, Object> m = new LinkedHashMap<Object, Object>(size);
       for (int i = 0; i < size; i++) {
         m.put(readDatum(in), readDatum(in));
       }

File: hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatBaseStorer.java
Patch:
@@ -27,6 +27,7 @@
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
@@ -353,7 +354,7 @@ private Object getJavaObj(Object pigObj, HCatFieldSchema hcatFS) throws HCatExce
         return bagContents;
       case MAP:
         Map<?, ?> pigMap = (Map<?, ?>) pigObj;
-        Map<Object, Object> typeMap = new HashMap<Object, Object>();
+        Map<Object, Object> typeMap = new LinkedHashMap<Object, Object>();
         for (Entry<?, ?> entry : pigMap.entrySet()) {
           // the value has a schema and not a FieldSchema
           typeMap.put(

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java
Patch:
@@ -40,7 +40,7 @@
 import java.io.DataOutput;
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.HashMap;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 
@@ -379,7 +379,7 @@ public Category getCategory() {
 
     @Override
     public Object create() {
-      return new HashMap<Object,Object>();
+      return new LinkedHashMap<Object,Object>();
     }
 
     @Override

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -771,7 +771,7 @@ private static ExprNodeConstantDesc toMapConstDesc(ColumnInfo colInfo, ObjectIns
     PrimitiveObjectInspector keyPoi = (PrimitiveObjectInspector)keyOI;
     PrimitiveObjectInspector valuePoi = (PrimitiveObjectInspector)valueOI;
     Map<?,?> values = (Map<?,?>)((ConstantObjectInspector) inspector).getWritableConstantValue();
-    Map<Object, Object> constant = new HashMap<Object, Object>();
+    Map<Object, Object> constant = new LinkedHashMap<Object, Object>();
     for (Map.Entry<?, ?> e : values.entrySet()) {
       constant.put(keyPoi.getPrimitiveJavaObject(e.getKey()), valuePoi.getPrimitiveJavaObject(e.getValue()));
     }

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VerifyFastRow.java
Patch:
@@ -23,7 +23,7 @@
 import java.sql.Timestamp;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.HashMap;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 
@@ -457,7 +457,7 @@ public static void serializeWrite(SerializeWrite serializeWrite,
         MapTypeInfo mapTypeInfo = (MapTypeInfo) typeInfo;
         TypeInfo keyTypeInfo = mapTypeInfo.getMapKeyTypeInfo();
         TypeInfo valueTypeInfo = mapTypeInfo.getMapValueTypeInfo();
-        HashMap<Object, Object> hashMap = (HashMap<Object, Object>) object;
+        Map<Object, Object> hashMap = (Map<Object, Object>) object;
         serializeWrite.beginMap(hashMap);
         boolean isFirst = true;
         for (Map.Entry<Object, Object> entry : hashMap.entrySet()) {
@@ -630,7 +630,7 @@ private static Object getComplexField(DeserializeRead deserializeRead,
         MapTypeInfo mapTypeInfo = (MapTypeInfo) typeInfo;
         TypeInfo keyTypeInfo = mapTypeInfo.getMapKeyTypeInfo();
         TypeInfo valueTypeInfo = mapTypeInfo.getMapValueTypeInfo();
-        HashMap<Object, Object> hashMap = new HashMap<Object, Object>();
+        Map<Object, Object> hashMap = new LinkedHashMap<Object, Object>();
         Object keyObj;
         Object valueObj;
         boolean isNull;

File: serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java
Patch:
@@ -20,7 +20,7 @@
 import java.sql.Date;
 import java.sql.Timestamp;
 import java.util.ArrayList;
-import java.util.HashMap;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -262,7 +262,7 @@ private Object serializeMap(MapTypeInfo typeInfo, MapObjectInspector fieldOI, Ob
     Map<?,?> map = fieldOI.getMap(structFieldData);
     Schema valueType = schema.getValueType();
 
-    Map<Object, Object> deserialized = new HashMap<Object, Object>(fieldOI.getMapSize(structFieldData));
+    Map<Object, Object> deserialized = new LinkedHashMap<Object, Object>(fieldOI.getMapSize(structFieldData));
 
     for (Map.Entry<?, ?> entry : map.entrySet()) {
       deserialized.put(serialize(mapKeyTypeInfo, mapKeyObjectInspector, entry.getKey(), STRING_SCHEMA),

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
Patch:
@@ -23,8 +23,8 @@
 import java.lang.reflect.Type;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.HashMap;
 import java.util.Iterator;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 
@@ -445,7 +445,7 @@ public static Object copyToStandardObject(
     }
     case MAP: {
       MapObjectInspector moi = (MapObjectInspector) oi;
-      HashMap<Object, Object> map = new HashMap<Object, Object>();
+      Map<Object, Object> map = new LinkedHashMap<Object, Object>();
       Map<? extends Object, ? extends Object> omap = moi.getMap(o);
       for (Map.Entry<? extends Object, ? extends Object> entry : omap
           .entrySet()) {

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
Patch:
@@ -313,7 +313,7 @@ public void alterTable(RawStore msdb, Warehouse wh, String catName, String dbnam
             !isPartitionedTable) {
           Database db = msdb.getDatabase(catName, newDbName);
           // Update table stats. For partitioned table, we update stats in alterPartition()
-          MetaStoreUtils.updateTableStatsFast(db, newt, wh, false, true, environmentContext, false);
+          MetaStoreUtils.updateTableStatsSlow(db, newt, wh, false, true, environmentContext);
         }
 
         if (isPartitionedTable) {

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -1796,7 +1796,7 @@ private void create_table_core(final RawStore ms, final Table tbl,
         }
         if (MetastoreConf.getBoolVar(conf, ConfVars.STATS_AUTO_GATHER) &&
             !MetaStoreUtils.isView(tbl)) {
-          MetaStoreUtils.updateTableStatsFast(db, tbl, wh, madeDir, false, envContext, true);
+          MetaStoreUtils.updateTableStatsSlow(db, tbl, wh, madeDir, false, envContext);
         }
 
         // set create time

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/CompositePartitionSpecProxy.java
Patch:
@@ -40,7 +40,7 @@ public class CompositePartitionSpecProxy extends PartitionSpecProxy {
   private List<PartitionSpecProxy> partitionSpecProxies;
   private int size = 0;
 
-  protected CompositePartitionSpecProxy(List<PartitionSpec> partitionSpecs) {
+  protected CompositePartitionSpecProxy(List<PartitionSpec> partitionSpecs) throws MetaException {
     this.partitionSpecs = partitionSpecs;
     if (partitionSpecs.isEmpty()) {
       catName = null;
@@ -63,13 +63,13 @@ protected CompositePartitionSpecProxy(List<PartitionSpec> partitionSpecs) {
   }
 
   @Deprecated
-  protected CompositePartitionSpecProxy(String dbName, String tableName, List<PartitionSpec> partitionSpecs) {
+  protected CompositePartitionSpecProxy(String dbName, String tableName, List<PartitionSpec> partitionSpecs) throws MetaException {
     this(DEFAULT_CATALOG_NAME, dbName, tableName, partitionSpecs);
 
   }
 
   protected CompositePartitionSpecProxy(String catName, String dbName, String tableName,
-                                        List<PartitionSpec> partitionSpecs) {
+                                        List<PartitionSpec> partitionSpecs) throws MetaException {
     this.catName = catName;
     this.dbName = dbName;
     this.tableName = tableName;

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliConfigs.java
Patch:
@@ -358,9 +358,8 @@ public NegativeCliConfig() {
         setQueryDir("ql/src/test/queries/clientnegative");
 
         excludesFrom(testConfigProps, "minimr.query.negative.files");
+        excludesFrom(testConfigProps, "spark.only.query.negative.files");
         excludeQuery("authorization_uri_import.q");
-        excludeQuery("spark_job_max_tasks.q");
-        excludeQuery("spark_stage_max_tasks.q");
 
         setResultsDir("ql/src/test/results/clientnegative");
         setLogDir("itests/qtest/target/qfile-results/clientnegative");
@@ -595,6 +594,7 @@ public SparkNegativeCliConfig() {
         setQueryDir("ql/src/test/queries/clientnegative");
 
         includesFrom(testConfigProps, "spark.query.negative.files");
+        includesFrom(testConfigProps, "spark.only.query.negative.files");
 
         setResultsDir("ql/src/test/results/clientnegative/spark");
         setLogDir("itests/qtest-spark/target/qfile-results/clientnegative/spark");

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -584,8 +584,8 @@ public enum ErrorMsg {
 
   //========================== 40000 range starts here ========================//
 
-  SPARK_JOB_RUNTIME_ERROR(40001,
-      "Spark job failed during runtime. Please check stacktrace for the root cause.")
+  SPARK_JOB_RUNTIME_ERROR(40001, "Spark job failed due to: {0}", true),
+  SPARK_TASK_RUNTIME_ERROR(40002, "Spark job failed due to task failures: {0}", true)
   ;
 
   private int errorCode;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
Patch:
@@ -603,7 +603,7 @@ public String getJobID() {
   public void shutdown() {
   }
 
-  Throwable getException() {
+  public Throwable getException() {
     return exception;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/LocalSparkJobMonitor.java
Patch:
@@ -128,7 +128,7 @@ public int startMonitor() {
         console.printError(msg, "\n" + org.apache.hadoop.util.StringUtils.stringifyException(e));
         rc = 1;
         done = true;
-        sparkJobStatus.setError(e);
+        sparkJobStatus.setMonitorError(e);
       } finally {
         if (done) {
           break;

File: spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java
Patch:
@@ -38,7 +38,6 @@
 import java.io.FileOutputStream;
 import java.io.IOException;
 import java.io.OutputStreamWriter;
-import java.io.PrintStream;
 import java.io.Serializable;
 import java.io.Writer;
 import java.net.URI;
@@ -64,7 +63,6 @@
 import org.apache.hive.spark.client.rpc.RpcConfiguration;
 import org.apache.hive.spark.client.rpc.RpcServer;
 import org.apache.spark.SparkContext;
-import org.apache.spark.SparkException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -592,7 +590,7 @@ private void handle(ChannelHandlerContext ctx, JobResult msg) {
       if (handle != null) {
         LOG.info("Received result for {}", msg.id);
         handle.setSparkCounters(msg.sparkCounters);
-        Throwable error = msg.error != null ? new SparkException(msg.error) : null;
+        Throwable error = msg.error;
         if (error == null) {
           handle.setSuccess(msg.result);
         } else {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java
Patch:
@@ -542,6 +542,8 @@ static public ObjectInspector createObjectInspector(TypeInfo info) {
           case DECIMAL:
             return PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(
                 (PrimitiveTypeInfo)info);
+        case TIMESTAMPLOCALTZ:
+          return PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector((PrimitiveTypeInfo) info);
           default:
             throw new IllegalArgumentException("Unknown primitive type " +
               ((PrimitiveTypeInfo) info).getPrimitiveCategory());

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliConfigs.java
Patch:
@@ -56,6 +56,7 @@ public CliConfig() {
         excludesFrom(testConfigProps, "disabled.query.files");
         excludesFrom(testConfigProps, "localSpark.only.query.files");
         excludesFrom(testConfigProps, "druid.query.files");
+        excludesFrom(testConfigProps, "druid.kafka.query.files");
 
         setResultsDir("ql/src/test/results/clientpositive");
         setLogDir("itests/qtest/target/qfile-results/clientpositive");

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -121,7 +121,7 @@ public abstract class BaseSemanticAnalyzer {
   protected final Hive db;
   protected final HiveConf conf;
   protected final QueryState queryState;
-  protected List<Task<? extends Serializable>> rootTasks;
+  protected List<Task<?>> rootTasks;
   protected FetchTask fetchTask;
   protected final Logger LOG;
   protected final LogHelper console;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -12211,7 +12211,7 @@ void analyzeInternal(ASTNode ast, PlannerContextFactory pcf) throws SemanticExce
       fetchTask = pCtx.getFetchTask();
     }
     //find all Acid FileSinkOperatorS
-    QueryPlanPostProcessor qp = new QueryPlanPostProcessor((List<Task<?>>)rootTasks, acidFileSinks, ctx.getExecutionId());
+    QueryPlanPostProcessor qp = new QueryPlanPostProcessor(rootTasks, acidFileSinks, ctx.getExecutionId());
     LOG.info("Completed plan generation");
 
     // 10. put accessed columns to readEntity

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -2668,6 +2668,9 @@ public boolean isPartitionMarkedForEvent(String catName, String db_name, String
 
   @Override
   public void createFunction(Function func) throws TException {
+    if (func == null) {
+      throw new MetaException("Function cannot be null.");
+    }
     if (!func.isSetCatName()) func.setCatName(getDefaultCatalog(conf));
     client.create_function(func);
   }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -3155,7 +3155,8 @@ private void analyzeAlterTableRenameCol(String catName, String[] qualified, ASTN
       case HiveParser.TOK_DEFAULT_VALUE:
         defaultConstraints = new ArrayList<>();
         processDefaultConstraints(catName, qualified[0], qualified[1], constraintChild,
-                                  ImmutableList.of(newColName), defaultConstraints, (ASTNode)ast.getChild(2));
+                                  ImmutableList.of(newColName), defaultConstraints, (ASTNode)ast.getChild(2),
+                                  this.ctx.getTokenRewriteStream());
         break;
       case HiveParser.TOK_NOT_NULL:
         notNullConstraints = new ArrayList<>();

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -3094,7 +3094,7 @@ public Partition append_partition_with_environment_context(final String dbName,
         final String tableName, final List<String> part_vals, final EnvironmentContext envContext)
         throws InvalidObjectException, AlreadyExistsException, MetaException {
       if (part_vals == null || part_vals.isEmpty()) {
-        throw new MetaException("The partition values must not be null.");
+        throw new MetaException("The partition values must not be null or empty.");
       }
       String[] parsedDbName = parseDbName(dbName, conf);
       startPartitionFunction("append_partition", parsedDbName[CAT_NAME], parsedDbName[DB_NAME], tableName, part_vals);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java
Patch:
@@ -1101,6 +1101,7 @@ private void handleInsert(ASTNode whenNotMatchedClause, StringBuilder rewrittenQ
     List<FieldSchema> partCols = targetTable.getPartCols();
     String valuesClause = getMatchedText((ASTNode)getWhenClauseOperation(whenNotMatchedClause).getChild(0));
     valuesClause = valuesClause.substring(1, valuesClause.length() - 1);//strip '(' and ')'
+    valuesClause = SemanticAnalyzer.replaceDefaultKeywordForMerge(valuesClause, targetTable);
 
     rewrittenQueryStr.append("INSERT INTO ").append(getFullTableNameForSQL(target));
     addPartitionColsToInsert(partCols, rewrittenQueryStr);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExtractRow.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.exec.vector;
 
 import java.util.ArrayList;
-import java.util.HashMap;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 
@@ -397,7 +397,7 @@ public Object extractRowColumn(
         final int offset = (int) mapColumnVector.offsets[adjustedIndex];
         final int size = (int) mapColumnVector.lengths[adjustedIndex];
 
-        final Map map = new HashMap();
+        final Map<Object, Object> map = new LinkedHashMap<Object, Object>();
         for (int i = 0; i < size; i++) {
           final Object key = extractRowColumn(
               mapColumnVector.keys,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java
Patch:
@@ -187,6 +187,8 @@ public static WorkloadManager create(String yarnQueue, HiveConf conf, WMFullReso
     throws ExecutionException, InterruptedException {
     assert INSTANCE == null;
     // We could derive the expected number of AMs to pass in.
+    // Note: we pass a null token here; the tokens to talk to plugin endpoints will only be
+    //       known once the AMs register, and they are different for every AM (unlike LLAP token).
     LlapPluginEndpointClientImpl amComm = new LlapPluginEndpointClientImpl(conf, null, -1);
     QueryAllocationManager qam = new GuaranteedTasksAllocator(conf, amComm);
     return (INSTANCE = new WorkloadManager(amComm, yarnQueue, conf, qam, plan));

File: beeline/src/java/org/apache/hive/beeline/hs2connection/BeelineHS2ConnectionFileParseException.java
Patch:
@@ -17,7 +17,7 @@
  */
 package org.apache.hive.beeline.hs2connection;
 
-public class BeelineHS2ConnectionFileParseException extends Exception {
+public class BeelineHS2ConnectionFileParseException extends BeelineConfFileParseException {
   private static final long serialVersionUID = -748635913718300617L;
 
   BeelineHS2ConnectionFileParseException(String msg, Exception e) {

File: beeline/src/java/org/apache/hive/beeline/hs2connection/HS2ConnectionFileParser.java
Patch:
@@ -79,7 +79,7 @@ public interface HS2ConnectionFileParser {
    * object if the connection configuration is not found
    * @throws BeelineHS2ConnectionFileParseException if there is invalid key with appropriate message
    */
-  Properties getConnectionProperties() throws BeelineHS2ConnectionFileParseException;
+  Properties getConnectionProperties() throws BeelineConfFileParseException;
   /**
    *
    * @return returns true if the configuration exists else returns false

File: beeline/src/java/org/apache/hive/beeline/hs2connection/UserHS2ConnectionFileParser.java
Patch:
@@ -86,9 +86,6 @@ public Properties getConnectionProperties() throws BeelineHS2ConnectionFileParse
         if (key.startsWith(BEELINE_CONNECTION_PROPERTY_PREFIX)) {
           props.setProperty(key.substring(BEELINE_CONNECTION_PROPERTY_PREFIX.length()),
               kv.getValue());
-        } else {
-          log.warn("Ignoring " + key + " since it does not start with "
-              + BEELINE_CONNECTION_PROPERTY_PREFIX);
         }
       }
     } catch (Exception ex) {

File: beeline/src/test/org/apache/hive/beeline/hs2connection/TestUserHS2ConnectionFileParser.java
Patch:
@@ -177,9 +177,7 @@ private String getParsedUrlFromConfigFile(String filename)
     testLocations.add(path);
     UserHS2ConnectionFileParser testHS2ConfigManager =
         new UserHS2ConnectionFileParser(testLocations);
-
-    String url = HS2ConnectionFileUtils.getUrl(testHS2ConfigManager.getConnectionProperties());
-    return url;
+    return HS2ConnectionFileUtils.getUrl(testHS2ConfigManager.getConnectionProperties());
   }
 
   private void createNewFile(final String path) throws Exception {

File: itests/hive-unit/src/test/java/org/apache/hive/beeline/hs2connection/BeelineWithHS2ConnectionFileTestBase.java
Patch:
@@ -89,7 +89,7 @@ public String getOutput() throws UnsupportedEncodingException {
     }
 
     @Override
-    public HS2ConnectionFileParser getUserHS2ConnFileParser() {
+    public UserHS2ConnectionFileParser getUserHS2ConnFileParser() {
       return testHs2ConfigFileManager;
     }
 

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java
Patch:
@@ -181,7 +181,9 @@ public void setVal(int elementNum, byte[] sourceBuf, int start, int length) {
     if ((nextFree + length) > buffer.length) {
       increaseBufferSpace(length);
     }
-    System.arraycopy(sourceBuf, start, buffer, nextFree, length);
+    if (length > 0) {
+      System.arraycopy(sourceBuf, start, buffer, nextFree, length);
+    }
     vector[elementNum] = buffer;
     this.start[elementNum] = nextFree;
     this.length[elementNum] = length;

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -2810,6 +2810,9 @@ public Partition append_partition(final String dbName, final String tableName,
     public Partition append_partition_with_environment_context(final String dbName,
         final String tableName, final List<String> part_vals, final EnvironmentContext envContext)
         throws InvalidObjectException, AlreadyExistsException, MetaException {
+      if (part_vals == null) {
+        throw new MetaException("The partition values must not be null.");
+      }
       startPartitionFunction("append_partition", dbName, tableName, part_vals);
       if (LOG.isDebugEnabled()) {
         for (String part : part_vals) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveTableScan.java
Patch:
@@ -123,7 +123,7 @@ public RelNode copy(RelTraitSet traitSet, List<RelNode> inputs) {
    */
   public HiveTableScan copy(RelDataType newRowtype) {
     return new HiveTableScan(getCluster(), getTraitSet(), ((RelOptHiveTable) table), this.tblAlias, this.concatQbIDAlias,
-            newRowtype, this.useQBIdInDigest, this.insideView);
+        newRowtype, this.useQBIdInDigest, this.insideView);
   }
 
   @Override public RelWriter explainTerms(RelWriter pw) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdPredicates.java
Patch:
@@ -71,7 +71,6 @@
 import com.google.common.collect.HashMultimap;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Iterables;
-import com.google.common.collect.Iterators;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -1276,6 +1276,8 @@ public boolean dropTable(String dbName, String tableName) throws MetaException,
   private boolean dropCreationMetadata(String dbName, String tableName) throws MetaException,
       NoSuchObjectException, InvalidObjectException, InvalidInputException {
     boolean success = false;
+    dbName = normalizeIdentifier(dbName);
+    tableName = normalizeIdentifier(tableName);
     try {
       openTransaction();
       MCreationMetadata mcm = getCreationMetadata(dbName, tableName);

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFEnforceNotNullConstraint.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
+import org.apache.hadoop.hive.ql.exec.errors.DataConstraintViolationError;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
@@ -57,7 +58,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
     boolean result = boi.get(a);
 
     if(!result) {
-      throw new UDFArgumentLengthException(
+      throw new DataConstraintViolationError(
           "NOT NULL constraint violated!");
     }
     resultBool.set(true);

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTrunc.java
Patch:
@@ -259,7 +259,7 @@ private ObjectInspector initializeDate(ObjectInspector[] arguments)
         && PrimitiveObjectInspectorUtils
             .getPrimitiveGrouping(inputType2) != PrimitiveGrouping.VOID_GROUP) {
       throw new UDFArgumentTypeException(1,
-          "trunk() only takes STRING/CHAR/VARCHAR types as second argument, got " + inputType2);
+          "trunc() only takes STRING/CHAR/VARCHAR types as second argument, got " + inputType2);
     }
 
     inputType2 = PrimitiveCategory.STRING;

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/AbstractTestAuthorizationApiAuthorizer.java
Patch:
@@ -58,8 +58,7 @@ protected static void setup() throws Exception {
 
     hiveConf = new HiveConf();
     if (isRemoteMetastoreMode) {
-      int port = MetaStoreTestUtils.startMetaStoreWithRetry();
-      hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+      MetaStoreTestUtils.startMetaStoreWithRetry(hiveConf);
     }
     hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
     hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreMetrics.java
Patch:
@@ -49,9 +49,7 @@ public static void before() throws Exception {
             "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
 
     //Increments one HMS connection
-    int port = MetaStoreTestUtils.startMetaStoreWithRetry(hiveConf);
-
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
+    MetaStoreTestUtils.startMetaStoreWithRetry(hiveConf);
 
     //Increments one HMS connection (Hive.get())
     SessionState.start(new CliSessionState(hiveConf));

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java
Patch:
@@ -126,8 +126,7 @@ private void initialize(String cmRoot, String warehouseRoot,
     System.setProperty(HiveConf.ConfVars.PREEXECHOOKS.varname, " ");
     System.setProperty(HiveConf.ConfVars.POSTEXECHOOKS.varname, " ");
 
-    int metaStorePort = MetaStoreTestUtils.startMetaStore(hiveConf);
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + metaStorePort);
+    MetaStoreTestUtils.startMetaStoreWithRetry(hiveConf);
 
     Path testPath = new Path(hiveWarehouseLocation);
     FileSystem testPathFileSystem = FileSystem.get(testPath.toUri(), hiveConf);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/StorageBasedMetastoreTestBase.java
Patch:
@@ -72,12 +72,11 @@ public void setUp() throws Exception {
         InjectableDummyAuthenticator.class.getName());
 
     clientHiveConf = createHiveConf();
-    int port = MetaStoreTestUtils.startMetaStoreWithRetry(clientHiveConf);
+    MetaStoreTestUtils.startMetaStoreWithRetry(clientHiveConf);
 
     // Turn off client-side authorization
     clientHiveConf.setBoolVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED,false);
 
-    clientHiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
     clientHiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
     clientHiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
 

File: itests/util/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java
Patch:
@@ -339,8 +339,7 @@ public MiniHS2(HiveConf hiveConf, MiniClusterType clusterType, boolean usePortsF
 
   public void start(Map<String, String> confOverlay) throws Exception {
     if (isMetastoreRemote) {
-      int metaStorePort = MetaStoreTestUtils.startMetaStoreWithRetry(getHiveConf());
-      getHiveConf().setVar(ConfVars.METASTOREURIS, "thrift://localhost:" + metaStorePort);
+      MetaStoreTestUtils.startMetaStoreWithRetry(getHiveConf());
     }
 
     // Set confOverlay parameters

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveRemote.java
Patch:
@@ -43,7 +43,7 @@ protected void setUp() throws Exception {
     hiveConf
     .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
-    hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + MetaStoreTestUtils.startMetaStore());
+    MetaStoreTestUtils.startMetaStoreWithRetry(hiveConf);
 
     try {
       hm = Hive.get(hiveConf);

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestFilterHooks.java
Patch:
@@ -151,8 +151,7 @@ public static void setUp() throws Exception {
     MetastoreConf.setClass(conf, ConfVars.FILTER_HOOK, DummyMetaStoreFilterHookImpl.class,
         MetaStoreFilterHook.class);
     MetaStoreTestUtils.setConfForStandloneMode(conf);
-    int port = MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
-    MetastoreConf.setVar(conf, ConfVars.THRIFT_URIS, "thrift://localhost:" + port);
+    MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
 
     msc = new HiveMetaStoreClient(conf);
 

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStorePartitionSpecs.java
Patch:
@@ -70,7 +70,7 @@ public static void startMetaStoreServer() throws Exception {
     MetastoreConf.setClass(metastoreConf, ConfVars.EXPRESSION_PROXY_CLASS,
       MockPartitionExpressionForMetastore.class, PartitionExpressionProxy.class);
     MetaStoreTestUtils.setConfForStandloneMode(metastoreConf);
-    msPort = MetaStoreTestUtils.startMetaStore(metastoreConf);
+    msPort = MetaStoreTestUtils.startMetaStoreWithRetry(metastoreConf);
     conf = MetastoreConf.newMetastoreConf();
     MetastoreConf.setVar(conf, ConfVars.THRIFT_URIS, "thrift://localhost:" + msPort);
     MetastoreConf.setLongVar(conf, ConfVars.THRIFT_CONNECTION_RETRIES, 3);

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreWithEnvironmentContext.java
Patch:
@@ -75,8 +75,7 @@ public void setUp() throws Exception {
     MetastoreConf.setLongVar(conf, ConfVars.THRIFT_CONNECTION_RETRIES, 3);
     MetastoreConf.setBoolVar(conf, ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     MetaStoreTestUtils.setConfForStandloneMode(conf);
-    int port = MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
-    MetastoreConf.setVar(conf, ConfVars.THRIFT_URIS, "thrift://localhost:" + port);
+    MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
     msc = new HiveMetaStoreClient(conf);
 
     msc.dropDatabase(dbName, true, true);

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEndFunctionListener.java
Patch:
@@ -57,8 +57,7 @@ public void setUp() throws Exception {
     MetastoreConf.setLongVar(conf, ConfVars.THRIFT_CONNECTION_RETRIES, 3);
     MetastoreConf.setBoolVar(conf, ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     MetaStoreTestUtils.setConfForStandloneMode(conf);
-    int port = MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
-    MetastoreConf.setVar(conf, ConfVars.THRIFT_URIS, "thrift://localhost:" + port);
+    MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
     msc = new HiveMetaStoreClient(conf);
   }
 

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
Patch:
@@ -102,8 +102,7 @@ public void setUp() throws Exception {
     MetastoreConf.setLongVar(conf, ConfVars.THRIFT_CONNECTION_RETRIES, 3);
     MetastoreConf.setBoolVar(conf, ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     MetaStoreTestUtils.setConfForStandloneMode(conf);
-    int port = MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
-    MetastoreConf.setVar(conf, ConfVars.THRIFT_URIS, "thrift://localhost:" + port);
+    MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
 
     msc = new HiveMetaStoreClient(conf);
 

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListenerOnlyOnCommit.java
Patch:
@@ -63,8 +63,7 @@ public void setUp() throws Exception {
     MetastoreConf.setLongVar(conf, ConfVars.THRIFT_CONNECTION_RETRIES, 3);
     MetastoreConf.setBoolVar(conf, ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     MetaStoreTestUtils.setConfForStandloneMode(conf);
-    int port = MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
-    MetastoreConf.setVar(conf, ConfVars.THRIFT_URIS, "thrift://localhost:" + port);
+    MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
     msc = new HiveMetaStoreClient(conf);
 
     DummyListener.notifyList.clear();

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListenerWithOldConf.java
Patch:
@@ -49,15 +49,13 @@ public void setUp() throws Exception {
     System.setProperty("hive.metastore.pre.event.listeners",
         DummyPreListener.class.getName());
 
-    int port = MetaStoreTestUtils.findFreePort();
     conf = MetastoreConf.newMetastoreConf();
 
     MetastoreConf.setVar(conf, ConfVars.PARTITION_NAME_WHITELIST_PATTERN, metaConfVal);
-    MetastoreConf.setVar(conf, ConfVars.THRIFT_URIS, "thrift://localhost:" + port);
     MetastoreConf.setLongVar(conf, ConfVars.THRIFT_CONNECTION_RETRIES, 3);
     MetastoreConf.setBoolVar(conf, ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     MetaStoreTestUtils.setConfForStandloneMode(conf);
-    MetaStoreTestUtils.startMetaStore(port, HadoopThriftAuthBridge.getBridge(), conf);
+    MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
 
     DummyListener.notifyList.clear();
     DummyPreListener.notifyList.clear();

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreInitListener.java
Patch:
@@ -43,8 +43,7 @@ public void setUp() throws Exception {
     MetastoreConf.setLongVar(conf, ConfVars.THRIFT_CONNECTION_RETRIES, 3);
     MetastoreConf.setBoolVar(conf, ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     MetaStoreTestUtils.setConfForStandloneMode(conf);
-    int port = MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
-    MetastoreConf.setVar(conf, ConfVars.THRIFT_URIS, "thrift://localhost:" + port);
+    MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
   }
 
   @Test

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreIpAddress.java
Patch:
@@ -50,9 +50,7 @@ public void setUp() throws Exception {
 
     System.setProperty(ConfVars.EVENT_LISTENERS.toString(), IpAddressListener.class.getName());
     MetaStoreTestUtils.setConfForStandloneMode(conf);
-    int port = MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
-    LOG.debug("Starting MetaStore Server on port " + port);
-    MetastoreConf.setVar(conf, ConfVars.THRIFT_URIS, "thrift://localhost:" + port);
+    MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
 
     msc = new HiveMetaStoreClient(conf);
   }

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestRetryingHMSHandler.java
Patch:
@@ -52,8 +52,7 @@ public void setUp() throws Exception {
     MetastoreConf.setTimeVar(conf, ConfVars.HMSHANDLERINTERVAL, 0, TimeUnit.MILLISECONDS);
     MetastoreConf.setBoolVar(conf, ConfVars.HMSHANDLERFORCERELOADCONF, false);
     MetaStoreTestUtils.setConfForStandloneMode(conf);
-    int port = MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
-    MetastoreConf.setVar(conf, ConfVars.THRIFT_URIS, "thrift://localhost:" + port);
+    MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(), conf);
     msc = new HiveMetaStoreClient(conf);
   }
 

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/minihms/RemoteMetaStoreForTests.java
Patch:
@@ -36,10 +36,8 @@ public RemoteMetaStoreForTests(Configuration configuration) {
 
   public void start() throws Exception {
     MetastoreConf.setBoolVar(getConfiguration(), MetastoreConf.ConfVars.EXECUTE_SET_UGI, false);
-    int port = MetaStoreTestUtils.startMetaStore(HadoopThriftAuthBridge.getBridge(),
+    MetaStoreTestUtils.startMetaStoreWithRetry(HadoopThriftAuthBridge.getBridge(),
         getConfiguration());
-    MetastoreConf.setVar(getConfiguration(), MetastoreConf.ConfVars.THRIFT_URIS,
-        "thrift://localhost:" + port);
     super.start();
   }
 }

File: storage-api/src/java/org/apache/hadoop/hive/common/ValidCompactorWriteIdList.java
Patch:
@@ -44,7 +44,7 @@ public ValidCompactorWriteIdList(String tableName, long[] abortedWriteIdList, Bi
     this(tableName, abortedWriteIdList, abortedBits, highWatermark, Long.MAX_VALUE);
   }
   /**
-   * @param tableName table which is under compaction. Full name of format <db_name>.<table_name>
+   * @param tableName table which is under compaction. Full name of format &lt;db_name&gt;.&lt;table_name&gt;
    * @param abortedWriteIdList list of all aborted write ids
    * @param abortedBits bitset marking whether the corresponding write id is aborted
    * @param highWatermark highest committed write id to be considered for compaction,

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3201,7 +3201,7 @@ public static enum ConfVars {
         "The meaning of this parameter is the inverse of the number of time ticks (cache\n" +
         " operations, currently) that cause the combined recency-frequency of a block in cache\n" +
         " to be halved."),
-    LLAP_CACHE_ALLOW_SYNTHETIC_FILEID("hive.llap.cache.allow.synthetic.fileid", false,
+    LLAP_CACHE_ALLOW_SYNTHETIC_FILEID("hive.llap.cache.allow.synthetic.fileid", true,
         "Whether LLAP cache should use synthetic file ID if real one is not available. Systems\n" +
         "like HDFS, Isilon, etc. provide a unique file/inode ID. On other FSes (e.g. local\n" +
         "FS), the cache would not work by default because LLAP is unable to uniquely track the\n" +

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LlapCacheableBuffer.java
Patch:
@@ -56,5 +56,7 @@ public String toStringForCache() {
         + lastUpdate + " " + (isLocked() ? "!" : ".") + "]";
   }
 
+  public abstract String getTag();
+
   protected abstract boolean isLocked();
 }

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCache.java
Patch:
@@ -58,7 +58,7 @@ DiskRangeList getFileData(Object fileKey, DiskRangeList range, long baseOffset,
    *         the replacement chunks from cache are updated directly in the array.
    */
   long[] putFileData(Object fileKey, DiskRange[] ranges, MemoryBuffer[] chunks,
-      long baseOffset, Priority priority, LowLevelCacheCounters qfCounters);
+      long baseOffset, Priority priority, LowLevelCacheCounters qfCounters, String tag);
 
   /** Notifies the cache that a particular buffer should be removed due to eviction. */
   void notifyEvicted(MemoryBuffer buffer);

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
Patch:
@@ -288,7 +288,7 @@ private boolean lockBuffer(LlapDataBuffer buffer, boolean doNotifyPolicy) {
 
   @Override
   public long[] putFileData(Object fileKey, DiskRange[] ranges, MemoryBuffer[] buffers,
-      long baseOffset, Priority priority, LowLevelCacheCounters qfCounters) {
+      long baseOffset, Priority priority, LowLevelCacheCounters qfCounters, String tag) {
     long[] result = null;
     assert buffers.length == ranges.length;
     FileCache<ConcurrentSkipListMap<Long, LlapDataBuffer>> subCache =
@@ -304,6 +304,7 @@ public long[] putFileData(Object fileKey, DiskRange[] ranges, MemoryBuffer[] buf
         long offset = ranges[i].getOffset() + baseOffset;
         assert buffer.declaredCachedLength == LlapDataBuffer.UNKNOWN_CACHED_LENGTH;
         buffer.declaredCachedLength = ranges[i].getLength();
+        buffer.setTag(tag);
         while (true) { // Overwhelmingly executes once, or maybe twice (replacing stale value).
           LlapDataBuffer oldVal = subCache.getCache().putIfAbsent(offset, buffer);
           if (oldVal == null) {

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.locks.ReentrantLock;
+
 import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -153,7 +154,7 @@ public void notifyUnlock(LlapCacheableBuffer buffer) {
         } finally {
           listLock.unlock();
         }
-        // Now insert the buffer in its place and restore heap property.
+        // Now insert the new buffer in its place and restore heap property.
         buffer.indexInHeap = 0;
         heapifyDownUnderLock(buffer, time);
       } else {

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleBufferManager.java
Patch:
@@ -84,9 +84,8 @@ public DiskRangeList getFileData(Object fileKey, DiskRangeList range, long baseO
   }
 
   @Override
-  public long[] putFileData(Object fileKey, DiskRange[] ranges,
-      MemoryBuffer[] chunks, long baseOffset, Priority priority,
-      LowLevelCacheCounters qfCounters) {
+  public long[] putFileData(Object fileKey, DiskRange[] ranges, MemoryBuffer[] chunks,
+      long baseOffset, Priority priority, LowLevelCacheCounters qfCounters, String tag) {
     for (int i = 0; i < chunks.length; ++i) {
       LlapAllocatorBuffer buffer = (LlapAllocatorBuffer)chunks[i];
       if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/Reader.java
Patch:
@@ -46,7 +46,7 @@ public interface Reader extends org.apache.hadoop.hive.ql.io.orc.Reader {
    * @return The reader.
    */
   EncodedReader encodedReader(Object fileKey, DataCache dataCache, DataReader dataReader,
-      PoolFactory pf, IoTrace trace, boolean useCodecPool) throws IOException;
+      PoolFactory pf, IoTrace trace, boolean useCodecPool, String tag) throws IOException;
 
   /** The factory that can create (or return) the pools used by encoded reader. */
   public interface PoolFactory {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/ReaderImpl.java
Patch:
@@ -35,8 +35,8 @@ public ReaderImpl(Path path, ReaderOptions options) throws IOException {
 
   @Override
   public EncodedReader encodedReader(Object fileKey, DataCache dataCache, DataReader dataReader,
-      PoolFactory pf, IoTrace trace, boolean useCodecPool) throws IOException {
+      PoolFactory pf, IoTrace trace, boolean useCodecPool, String tag) throws IOException {
     return new EncodedReaderImpl(fileKey, types, getSchema(), compressionKind, getWriterVersion(),
-        bufferSize, rowIndexStride, dataCache, dataReader, pf, trace, useCodecPool);
+        bufferSize, rowIndexStride, dataCache, dataReader, pf, trace, useCodecPool, tag);
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java
Patch:
@@ -437,7 +437,7 @@ private void generateEventOperatorPlan(DynamicListContext ctx, ParseContext pars
     GroupByDesc groupBy =
         new GroupByDesc(GroupByDesc.Mode.HASH, outputNames, groupByExprs,
             new ArrayList<AggregationDesc>(), false, groupByMemoryUsage, memoryThreshold,
-            null, false, 0, true);
+            null, false, -1, true);
 
     GroupByOperator groupByOp = (GroupByOperator) OperatorFactory.getAndMakeChild(
         groupBy, selectOp);
@@ -611,7 +611,7 @@ private boolean generateSemiJoinOperatorPlan(DynamicListContext ctx, ParseContex
     gbOutputNames.add(SemanticAnalyzer.getColumnInternalName(2));
     GroupByDesc groupBy = new GroupByDesc(GroupByDesc.Mode.HASH,
             gbOutputNames, new ArrayList<ExprNodeDesc>(), aggs, false,
-            groupByMemoryUsage, memoryThreshold, null, false, 0, false);
+        groupByMemoryUsage, memoryThreshold, null, false, -1, false);
 
     ArrayList<ColumnInfo> groupbyColInfos = new ArrayList<ColumnInfo>();
     groupbyColInfos.add(new ColumnInfo(gbOutputNames.get(0), key.getTypeInfo(), "", false));

File: ql/src/java/org/apache/hadoop/hive/ql/util/ResourceDownloader.java
Patch:
@@ -95,7 +95,7 @@ private List<URI> resolveAndDownloadInternal(URI source, String subDir,
 
   private String downloadResource(URI srcUri, String subDir, boolean convertToUnix)
       throws IOException, URISyntaxException {
-    LOG.info("converting to local {}", srcUri);
+    LOG.debug("Converting to local {}", srcUri);
     File destinationDir = (subDir == null) ? resourceDir : new File(resourceDir, subDir);
     ensureDirectory(destinationDir);
     File destinationFile = new File(destinationDir, new Path(srcUri.toString()).getName());

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java
Patch:
@@ -342,7 +342,7 @@ private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compa
       txnHandler.setHadoopJobId(rj.getID().toString(), id);
       rj.waitForCompletion();
       if (!rj.isSuccessful()) {
-        throw new IOException(compactionType == CompactionType.MAJOR ? "Major" : "Minor" +
+        throw new IOException((compactionType == CompactionType.MAJOR ? "Major" : "Minor") +
                " compactor job failed for " + jobName + "! Hadoop JobId: " + rj.getID());
       }
     } finally {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -575,6 +575,8 @@ public static enum ConfVars {
 
     HIVE_IN_TEST("hive.in.test", false, "internal usage only, true in test mode", true),
     HIVE_IN_TEST_SSL("hive.in.ssl.test", false, "internal usage only, true in SSL test mode", true),
+    // TODO: this needs to be removed; see TestReplicationScenarios* comments.
+    HIVE_IN_TEST_REPL("hive.in.repl.test", false, "internal usage only, true in replication test mode", true),
     HIVE_IN_TEST_IDE("hive.in.ide.test", false, "internal usage only, true if test running in ide",
         true),
     HIVE_TESTING_SHORT_LOGS("hive.testing.short.logs", false,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/CopyTask.java
Patch:
@@ -62,7 +62,7 @@ public int execute(DriverContext driverContext) {
   protected int copyOnePath(Path fromPath, Path toPath) {
     FileSystem dstFs = null;
     try {
-      Utilities.FILE_OP_LOGGER./**/debug("Copying data from {} to {} " + fromPath);
+      Utilities.FILE_OP_LOGGER.trace("Copying data from {} to {} " + fromPath);
       console.printInfo("Copying data from " + fromPath.toString(), " to "
           + toPath.toString());
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -687,7 +687,7 @@ private void checkFileFormats(Hive db, LoadTableDesc tbd, Table table)
    * has done it's job before the query ran.
    */
   WriteEntity.WriteType getWriteType(LoadTableDesc tbd, AcidUtils.Operation operation) {
-    if (tbd.getLoadFileType() == LoadFileType.REPLACE_ALL) {
+    if (tbd.getLoadFileType() == LoadFileType.REPLACE_ALL || tbd.isInsertOverwrite()) {
       return WriteEntity.WriteType.INSERT_OVERWRITE;
     }
     switch (operation) {
@@ -730,13 +730,13 @@ private void updatePartitionBucketSortColumns(Hive db, Table table, Partition pa
       //       have the correct buckets. The existing code discards the inferred data when the
       //       reducers don't produce enough files; we'll do the same for MM tables for now.
       FileSystem fileSys = partn.getDataLocation().getFileSystem(conf);
-      FileStatus[] fileStatus = HiveStatsUtils.getFileStatusRecurse(
+      List<FileStatus> fileStatus = HiveStatsUtils.getFileStatusRecurse(
           partn.getDataLocation(), 1, fileSys);
       // Verify the number of buckets equals the number of files
       // This will not hold for dynamic partitions where not every reducer produced a file for
       // those partitions.  In this case the table is not bucketed as Hive requires a files for
       // each bucket.
-      if (fileStatus.length == numBuckets) {
+      if (fileStatus.size() == numBuckets) {
         List<String> newBucketCols = new ArrayList<String>();
         updateBucketCols = true;
         for (BucketCol bucketCol : bucketCols) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/table/LoadPartitions.java
Patch:
@@ -240,6 +240,8 @@ private Task<? extends Serializable> tasksForAddPartition(Table table,
    */
   private Task<?> movePartitionTask(Table table, AddPartitionDesc.OnePartitionDesc partSpec,
       Path tmpPath) {
+    // Note: this sets LoadFileType incorrectly for ACID; is that relevant for load?
+    //       See setLoadFileType and setIsAcidIow calls elsewhere for an example.
     LoadTableDesc loadTableWork = new LoadTableDesc(
         tmpPath, Utilities.getTableDesc(table), partSpec.getPartSpec(),
         event.replicationSpec().isReplace() ? LoadFileType.REPLACE_ALL : LoadFileType.OVERWRITE_EXISTING,

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
Patch:
@@ -520,6 +520,8 @@ private static Task<?> addSinglePartition(URI fromURI, FileSystem fs, ImportTabl
 
       Task<?> addPartTask = TaskFactory.get(new DDLWork(x.getInputs(),
           x.getOutputs(), addPartitionDesc), x.getConf());
+      // Note: this sets LoadFileType incorrectly for ACID; is that relevant for import?
+      //       See setLoadFileType and setIsAcidIow calls elsewhere for an example.
       LoadTableDesc loadTableWork = new LoadTableDesc(moveTaskSrc, Utilities.getTableDesc(table),
           partSpec.getPartSpec(),
           replicationSpec.isReplace() ? LoadFileType.REPLACE_ALL : LoadFileType.OVERWRITE_EXISTING,

File: ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
Patch:
@@ -329,8 +329,9 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
       stmtId = SessionState.get().getTxnMgr().getStmtIdAndIncrement();
     }
 
-    LoadTableDesc loadTableWork;
-    loadTableWork = new LoadTableDesc(new Path(fromURI),
+    // Note: this sets LoadFileType incorrectly for ACID; is that relevant for load?
+    //       See setLoadFileType and setIsAcidIow calls elsewhere for an example.
+    LoadTableDesc loadTableWork = new LoadTableDesc(new Path(fromURI),
       Utilities.getTableDesc(ts.tableHandle), partSpec,
       isOverWrite ? LoadFileType.REPLACE_ALL : LoadFileType.KEEP_EXISTING, writeId);
     loadTableWork.setStmtId(stmtId);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/BasicStatsWork.java
Patch:
@@ -172,9 +172,11 @@ public boolean isTargetRewritten() {
       return true;
     }
     // INSERT OVERWRITE
-    if (getLoadTableDesc() != null && getLoadTableDesc().getLoadFileType() == LoadFileType.REPLACE_ALL) {
+    LoadTableDesc ltd = getLoadTableDesc();
+    if (ltd != null && (ltd.getLoadFileType() == LoadFileType.REPLACE_ALL || ltd.isInsertOverwrite())) {
       return true;
     }
+
     // CREATE TABLE ... AS
     if (getLoadFileDesc() != null && getLoadFileDesc().getCtasCreateTableDesc() != null) {
       return true;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java
Patch:
@@ -231,7 +231,8 @@ private void generateActualTasks(HiveConf conf, List<Task<? extends Serializable
       throws IOException {
     DynamicPartitionCtx dpCtx = ctx.getDPCtx();
     // get list of dynamic partitions
-    FileStatus[] status = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);
+    List<FileStatus> statusList = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);
+    FileStatus[] status = statusList.toArray(new FileStatus[statusList.size()]);
 
     // cleanup pathToPartitionInfo
     Map<Path, PartitionDesc> ptpi = work.getPathToPartitionInfo();

File: ql/src/java/org/apache/hadoop/hive/ql/stats/ColStatsProcessor.java
Patch:
@@ -184,4 +184,4 @@ public int persistColumnStats(Hive db, Table tbl) throws HiveException, MetaExce
   public void setDpPartSpecs(Collection<Partition> dpPartSpecs) {
   }
 
-}
\ No newline at end of file
+}

File: ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java
Patch:
@@ -105,9 +105,7 @@ public boolean closeConnection(StatsCollectionContext context) {
         statsFile = new Path(statsDir, StatsSetupConst.STATS_FILE_PREFIX
             + conf.getInt("mapred.task.partition", 0));
       }
-      if (Utilities.FILE_OP_LOGGER.isTraceEnabled()) {
-        Utilities.FILE_OP_LOGGER.trace("About to create stats file for this task : " + statsFile);
-      }
+      Utilities.FILE_OP_LOGGER.trace("About to create stats file for this task : {}", statsFile);
       Output output = new Output(statsFile.getFileSystem(conf).create(statsFile,true));
       LOG.debug("Created file : " + statsFile);
       LOG.debug("Writing stats in it : " + statsMap);

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/common/StatsSetupConst.java
Patch:
@@ -270,6 +270,7 @@ public static void clearColumnStatsState(Map<String, String> params) {
     if (params == null) {
       return;
     }
+
     ColumnStatsAccurate stats = parseStatsAcc(params.get(COLUMN_STATS_ACCURATE));
     stats.columnStats.clear();
 

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -1488,7 +1488,7 @@ private void create_table_core(final RawStore ms, final Table tbl,
         }
         if (MetastoreConf.getBoolVar(conf, ConfVars.STATS_AUTO_GATHER) &&
             !MetaStoreUtils.isView(tbl)) {
-          MetaStoreUtils.updateTableStatsFast(db, tbl, wh, madeDir, envContext);
+          MetaStoreUtils.updateTableStatsFast(db, tbl, wh, madeDir, false, envContext, true);
         }
 
         // set create time
@@ -2673,7 +2673,7 @@ private Partition append_partition_common(RawStore ms, String dbName, String tab
 
         if (MetastoreConf.getBoolVar(conf, ConfVars.STATS_AUTO_GATHER) &&
             !MetaStoreUtils.isView(tbl)) {
-          MetaStoreUtils.updatePartitionStatsFast(part, wh, madeDir, envContext);
+          MetaStoreUtils.updatePartitionStatsFast(part, tbl, wh, madeDir, false, envContext, true);
         }
 
         if (ms.addPartition(part)) {
@@ -3241,7 +3241,7 @@ private void initializeAddedPartition(
         final Table tbl, final PartitionSpecProxy.PartitionIterator part, boolean madeDir) throws MetaException {
       if (MetastoreConf.getBoolVar(conf, ConfVars.STATS_AUTO_GATHER) &&
           !MetaStoreUtils.isView(tbl)) {
-        MetaStoreUtils.updatePartitionStatsFast(part, wh, madeDir, false, null);
+        MetaStoreUtils.updatePartitionStatsFast(part, tbl, wh, madeDir, false, null, true);
       }
 
       // set create time

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/Warehouse.java
Patch:
@@ -543,7 +543,7 @@ public static String makePartName(List<FieldSchema> partCols,
    * @return array of FileStatus objects corresponding to the files
    * making up the passed storage description
    */
-  public FileStatus[] getFileStatusesForSD(StorageDescriptor desc)
+  public List<FileStatus> getFileStatusesForSD(StorageDescriptor desc)
       throws MetaException {
     return getFileStatusesForLocation(desc.getLocation());
   }
@@ -553,7 +553,7 @@ public FileStatus[] getFileStatusesForSD(StorageDescriptor desc)
    * @return array of FileStatus objects corresponding to the files
    * making up the passed storage description
    */
-  public FileStatus[] getFileStatusesForLocation(String location)
+  public List<FileStatus> getFileStatusesForLocation(String location)
       throws MetaException {
     try {
       Path path = new Path(location);
@@ -571,7 +571,7 @@ public FileStatus[] getFileStatusesForLocation(String location)
    * @return array of FileStatus objects corresponding to the files making up the passed
    * unpartitioned table
    */
-  public FileStatus[] getFileStatusesForUnpartitionedTable(Database db, Table table)
+  public List<FileStatus> getFileStatusesForUnpartitionedTable(Database db, Table table)
       throws MetaException {
     Path tablePath = getDnsPath(new Path(table.getSd().getLocation()));
     try {

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
Patch:
@@ -489,7 +489,7 @@ private static Partition makePartitionObject(String dbName, String tblName,
     part4.setSd(tbl.getSd().deepCopy());
     part4.getSd().setSerdeInfo(tbl.getSd().getSerdeInfo().deepCopy());
     part4.getSd().setLocation(tbl.getSd().getLocation() + ptnLocationSuffix);
-    MetaStoreUtils.updatePartitionStatsFast(part4, warehouse, null);
+    MetaStoreUtils.updatePartitionStatsFast(part4, tbl, warehouse, false, false, null, true);
     return part4;
   }
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3038,7 +3038,7 @@ public static enum ConfVars {
         "Enable (configurable) deprecated behaviors by setting desired level of backward compatibility.\n" +
         "Setting to 0.12:\n" +
         "  Maintains division behavior: int / int = double"),
-    HIVE_CONVERT_JOIN_BUCKET_MAPJOIN_TEZ("hive.convert.join.bucket.mapjoin.tez", false,
+    HIVE_CONVERT_JOIN_BUCKET_MAPJOIN_TEZ("hive.convert.join.bucket.mapjoin.tez", true,
         "Whether joins can be automatically converted to bucket map joins in hive \n" +
         "when tez is used as the execution engine."),
     HIVE_TEZ_BMJ_USE_SUBCACHE("hive.tez.bmj.use.subcache", true,

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -1715,7 +1715,8 @@ private void maskPatterns(Pattern[] patterns, String fname) throws Exception {
       "pk_-?[0-9]*_[0-9]*_[0-9]*",
       "fk_-?[0-9]*_[0-9]*_[0-9]*",
       "uk_-?[0-9]*_[0-9]*_[0-9]*",
-      "nn_-?[0-9]*_[0-9]*_[0-9]*",
+      "nn_-?[0-9]*_[0-9]*_[0-9]*", // not null constraint name
+      "dc_-?[0-9]*_[0-9]*_[0-9]*", // default constraint name
       ".*at com\\.sun\\.proxy.*",
       ".*at com\\.jolbox.*",
       ".*at com\\.zaxxer.*",

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatter.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hive.metastore.api.WMFullResourcePlan;
 import org.apache.hadoop.hive.metastore.api.WMResourcePlan;
 import org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanResponse;
+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;
 import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -89,7 +90,7 @@ public void describeTable(DataOutputStream out, String colPath,
       boolean isFormatted, boolean isExt,
       boolean isOutputPadded, List<ColumnStatisticsObj> colStats,
       PrimaryKeyInfo pkInfo, ForeignKeyInfo fkInfo,
-      UniqueConstraint ukInfo, NotNullConstraint nnInfo)
+      UniqueConstraint ukInfo, NotNullConstraint nnInfo, DefaultConstraint dInfo)
           throws HiveException;
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AddNotNullConstraintHandler.java
Patch:
@@ -22,6 +22,7 @@
 import java.util.Collections;
 import java.util.List;
 
+import org.apache.hadoop.hive.metastore.api.SQLDefaultConstraint;
 import org.apache.hadoop.hive.metastore.api.SQLForeignKey;
 import org.apache.hadoop.hive.metastore.api.SQLNotNullConstraint;
 import org.apache.hadoop.hive.metastore.api.SQLPrimaryKey;
@@ -64,7 +65,7 @@ public List<Task<? extends Serializable>> handle(Context context)
     }
 
     AlterTableDesc addConstraintsDesc = new AlterTableDesc(actualDbName + "." + actualTblName, new ArrayList<SQLPrimaryKey>(), new ArrayList<SQLForeignKey>(),
-        new ArrayList<SQLUniqueConstraint>(), nns, context.eventOnlyReplicationSpec());
+        new ArrayList<SQLUniqueConstraint>(), nns, new ArrayList<SQLDefaultConstraint>(), context.eventOnlyReplicationSpec());
     Task<DDLWork> addConstraintsTask = TaskFactory.get(new DDLWork(readEntitySet, writeEntitySet, addConstraintsDesc), context.hiveConf);
     tasks.add(addConstraintsTask);
     context.log.debug("Added add constrains task : {}:{}", addConstraintsTask.getId(), actualTblName);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ImportTableDesc.java
Patch:
@@ -82,7 +82,8 @@ public ImportTableDesc(String dbName, Table table) throws Exception {
                 null,
                 null,
                 null,
-                null);
+                null,
+            null);
         this.createTblDesc.setStoredAsSubDirectories(table.getSd().isStoredAsSubDirectories());
         break;
       case VIEW:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToChar.java
Patch:
@@ -93,6 +93,7 @@ public String getDisplayString(String[] children) {
     sb.append(" AS CHAR(");
     sb.append("" + typeInfo.getLength());
     sb.append(")");
+    sb.append(")");
     return sb.toString();
   }
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -574,6 +574,7 @@ public static enum ConfVars {
         "If not set, defaults to the codec extension for text files (e.g. \".gz\"), or no extension otherwise."),
 
     HIVE_IN_TEST("hive.in.test", false, "internal usage only, true in test mode", true),
+    HIVE_IN_TEST_SSL("hive.in.ssl.test", false, "internal usage only, true in SSL test mode", true),
     HIVE_IN_TEST_IDE("hive.in.ide.test", false, "internal usage only, true if test running in ide",
         true),
     HIVE_TESTING_SHORT_LOGS("hive.testing.short.logs", false,

File: itests/hive-unit/src/main/java/org/hadoop/hive/jdbc/SSLTestUtils.java
Patch:
@@ -74,12 +74,14 @@ public static void setHttpConfOverlay(Map<String, String> confOverlay) {
     confOverlay.put(HiveConf.ConfVars.HIVE_SERVER2_TRANSPORT_MODE.varname, HS2_HTTP_MODE);
     confOverlay.put(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_HTTP_PATH.varname, HS2_HTTP_ENDPOINT);
     confOverlay.put(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS.varname, "true");
+    confOverlay.put(HiveConf.ConfVars.HIVE_IN_TEST_SSL.varname, "true");
   }
 
   public static void setBinaryConfOverlay(Map<String, String> confOverlay) {
     confOverlay.put(HiveConf.ConfVars.HIVE_SERVER2_TRANSPORT_MODE.varname, HS2_BINARY_MODE);
     confOverlay.put(HiveConf.ConfVars.HIVE_SERVER2_AUTHENTICATION.varname, HS2_BINARY_AUTH_MODE);
     confOverlay.put(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS.varname, "true");
+    confOverlay.put(HiveConf.ConfVars.HIVE_IN_TEST_SSL.varname, "true");
   }
 
   public static void setupTestTableWithData(String tableName, Path dataFilePath,

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinTypeCheckCtx.java
Patch:
@@ -54,7 +54,7 @@ public class JoinTypeCheckCtx extends TypeCheckCtx {
   public JoinTypeCheckCtx(RowResolver leftRR, RowResolver rightRR, JoinType hiveJoinType)
       throws SemanticException {
     super(RowResolver.getCombinedRR(leftRR, rightRR), true, false, false, false, false, false, false, false,
-        false, false);
+        true, false);
     this.inputRRLst = ImmutableList.of(leftRR, rightRR);
     this.outerJoin = (hiveJoinType == JoinType.LEFTOUTER) || (hiveJoinType == JoinType.RIGHTOUTER)
         || (hiveJoinType == JoinType.FULLOUTER);

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -557,10 +557,10 @@ public enum ErrorMsg {
 
   SPARK_GET_JOB_INFO_TIMEOUT(30036,
       "Spark job timed out after {0} seconds while getting job info", true),
-  SPARK_JOB_MONITOR_TIMEOUT(30037, "Job hasn't been submitted after {0}s." +
+  SPARK_JOB_MONITOR_TIMEOUT(30037, "Job hasn''t been submitted after {0}s." +
       " Aborting it.\nPossible reasons include network issues, " +
       "errors in remote driver or the cluster has no available resources, etc.\n" +
-      "Please check YARN or Spark driver's logs for further information.\n" +
+      "Please check YARN or Spark driver''s logs for further information.\n" +
       "The timeout is controlled by " + HiveConf.ConfVars.SPARK_JOB_MONITOR_TIMEOUT + ".", true),
 
   // Various errors when creating Spark client

File: ql/src/java/org/apache/hadoop/hive/ql/exec/CopyTask.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.ql.DriverContext;
+import org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations;
 import org.apache.hadoop.hive.ql.plan.CopyWork;
 import org.apache.hadoop.hive.ql.plan.api.StageType;
 import org.apache.hadoop.util.StringUtils;
@@ -61,6 +62,7 @@ public int execute(DriverContext driverContext) {
   protected int copyOnePath(Path fromPath, Path toPath) {
     FileSystem dstFs = null;
     try {
+      Utilities.FILE_OP_LOGGER./**/debug("Copying data from {} to {} " + fromPath);
       console.printInfo("Copying data from " + fromPath.toString(), " to "
           + toPath.toString());
 
@@ -85,7 +87,7 @@ protected int copyOnePath(Path fromPath, Path toPath) {
       for (FileStatus oneSrc : srcs) {
         String oneSrcPathStr = oneSrc.getPath().toString();
         console.printInfo("Copying file: " + oneSrcPathStr);
-        LOG.debug("Copying file: {}", oneSrcPathStr);
+        Utilities.FILE_OP_LOGGER.debug("Copying file {} to {}", oneSrcPathStr, toPath);
         if (!FileUtils.copy(srcFs, oneSrc.getPath(), dstFs, toPath,
             false, // delete source
             true, // overwrite destination

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTran.java
Patch:
@@ -28,7 +28,5 @@ JavaPairRDD<KO, VO> transform(
 
   public String getName();
 
-  public void setName(String name);
-
   public Boolean isCacheEnable();
 }

File: ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java
Patch:
@@ -415,7 +415,7 @@ private InputSplit[] getCombineSplits(JobConf job, int numSplits,
           combine.createPool(job, f);
           poolMap.put(combinePathInputFormat, f);
         } else {
-          LOG.info("CombineHiveInputSplit: pool is already created for " + path +
+          LOG.debug("CombineHiveInputSplit: pool is already created for " + path +
                    "; using filter path " + filterPath);
           f.addPath(filterPath);
         }

File: standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/minihms/EmbeddedMetaStoreForTests.java
Patch:
@@ -30,4 +30,4 @@ public class EmbeddedMetaStoreForTests extends AbstractMetaStoreService {
   public EmbeddedMetaStoreForTests(Configuration configuration) {
     super(configuration);
   }
-}
\ No newline at end of file
+}

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPool.java
Patch:
@@ -104,10 +104,12 @@ void start() throws Exception {
       amRegistry.populateCache(true);
     }
 
+    this.parentSessionState = SessionState.get();
+    if (initialSize == 0) return; // May be resized later.
+
     int threadCount = Math.min(initialSize,
         HiveConf.getIntVar(initConf, ConfVars.HIVE_SERVER2_TEZ_SESSION_MAX_INIT_THREADS));
     Preconditions.checkArgument(threadCount > 0);
-    this.parentSessionState = SessionState.get();
     if (threadCount == 1) {
       for (int i = 0; i < initialSize; ++i) {
         SessionType session = sessionObjFactory.create(null);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -52,7 +52,6 @@
 import java.util.Arrays;
 import java.util.Calendar;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.Enumeration;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -244,6 +243,7 @@ public final class Utilities {
   public static final String USE_VECTORIZED_INPUT_FILE_FORMAT = "USE_VECTORIZED_INPUT_FILE_FORMAT";
   public static final String MAPNAME = "Map ";
   public static final String REDUCENAME = "Reducer ";
+  public static final String ENSURE_OPERATORS_EXECUTED = "ENSURE_OPERATORS_EXECUTED";
 
   @Deprecated
   protected static final String DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX = "mapred.dfsclient.parallelism.max";
@@ -361,7 +361,7 @@ public static Path setMergeWork(JobConf conf, MergeJoinWork mergeJoinWork, Path
   }
 
   public static BaseWork getMergeWork(Configuration jconf) {
-    String currentMergePrefix = jconf.get(DagUtils.TEZ_MERGE_CURRENT_MERGE_FILE_PREFIX); 
+    String currentMergePrefix = jconf.get(DagUtils.TEZ_MERGE_CURRENT_MERGE_FILE_PREFIX);
     if (StringUtils.isEmpty(currentMergePrefix)) {
       return null;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
Patch:
@@ -256,6 +256,7 @@ public int execute(DriverContext driverContext) {
     //See the javadoc on HiveOutputFormatImpl and HadoopShims.prepareJobOutput()
     job.setOutputFormat(HiveOutputFormatImpl.class);
 
+    job.setMapRunnerClass(ExecMapRunner.class);
     job.setMapperClass(ExecMapper.class);
 
     job.setMapOutputKeyClass(HiveKey.class);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HiveSplitGenerator.java
Patch:
@@ -29,7 +29,6 @@
 
 import com.google.common.base.Preconditions;
 
-import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.tez.common.counters.TezCounters;

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java
Patch:
@@ -178,7 +178,7 @@ public ColumnStatisticsObj aggregate(List<ColStatsObjWithSourceInfo> colStatsWit
           if (aggregateData == null) {
             aggregateData = newData.deepCopy();
           } else {
-            aggregateData.setAvgColLen(Math.min(aggregateData.getAvgColLen(),
+            aggregateData.setAvgColLen(Math.max(aggregateData.getAvgColLen(),
                 newData.getAvgColLen()));
             aggregateData.setMaxColLen(Math.max(aggregateData.getMaxColLen(),
                 newData.getMaxColLen()));

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
Patch:
@@ -337,7 +337,7 @@ private RexNode convert(ExprNodeGenericFuncDesc func) throws SemanticException {
         // If it is a floor <date> operator, we need to rewrite it
         childRexNodeLst = rewriteFloorDateChildren(calciteOp, childRexNodeLst);
       }
-      expr = cluster.getRexBuilder().makeCall(calciteOp, childRexNodeLst);
+      expr = cluster.getRexBuilder().makeCall(retType, calciteOp, childRexNodeLst);
     } else {
       retType = expr.getType();
     }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2952,7 +2952,9 @@ public static enum ConfVars {
         "internal use only, used for creating small group key vectorized row batches to exercise more logic\n" +
         "The default value is -1 which means don't restrict for testing",
         true),
-
+    HIVE_VECTORIZATION_TESTING_REUSE_SCRATCH_COLUMNS("hive.vectorized.reuse.scratch.columns", true,
+         "internal use only. Disable this to debug scratch column state issues",
+         true),
     HIVE_VECTORIZATION_COMPLEX_TYPES_ENABLED("hive.vectorized.complex.types.enabled", true,
         "This flag should be set to true to enable vectorization\n" +
         "of expressions with complex types.\n" +

File: ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestHiveSchemaConverter.java
Patch:
@@ -141,8 +141,8 @@ public void testMap() throws Exception {
             "message hive_schema {\n"
             + "  optional group mapCol (MAP) {\n"
             + "    repeated group map (MAP_KEY_VALUE) {\n"
-            + "      required binary key;\n"
-            + "      optional binary value;\n"
+            + "      required binary key (UTF8);\n"
+            + "      optional binary value (UTF8);\n"
             + "    }\n"
             + "  }\n"
             + "}\n");
@@ -155,7 +155,7 @@ public void testMapDecimal() throws Exception {
             "message hive_schema {\n"
             + "  optional group mapCol (MAP) {\n"
             + "    repeated group map (MAP_KEY_VALUE) {\n"
-            + "      required binary key;\n"
+            + "      required binary key (UTF8);\n"
             + "      optional fixed_len_byte_array(3) value (DECIMAL(5,2));\n"
             + "    }\n"
             + "  }\n"

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java
Patch:
@@ -72,7 +72,6 @@
 import org.apache.hadoop.service.CompositeService;
 import org.apache.hadoop.yarn.api.ApplicationConstants;
 import org.apache.hadoop.yarn.util.AuxiliaryServiceHelper;
-import org.apache.log4j.MDC;
 import org.apache.log4j.NDC;
 import org.apache.tez.common.counters.TezCounters;
 import org.apache.tez.common.security.JobTokenIdentifier;
@@ -86,6 +85,7 @@
 import org.apache.tez.runtime.api.impl.TezEvent;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import org.slf4j.MDC;
 
 import com.google.common.base.Preconditions;
 import com.google.protobuf.ByteString;

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryTracker.java
Patch:
@@ -27,7 +27,6 @@
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.security.token.Token;
 import org.apache.hadoop.util.StringUtils;
-import org.apache.log4j.MDC;
 import org.apache.logging.slf4j.Log4jMarker;
 import org.apache.tez.common.CallableWithNdc;
 
@@ -45,6 +44,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.slf4j.Marker;
+import org.slf4j.MDC;
 
 import java.io.IOException;
 import java.util.Collections;

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/StatsRecordingThreadPool.java
Patch:
@@ -32,14 +32,14 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hive.llap.LlapUtil;
 import org.apache.hadoop.hive.llap.io.encoded.TezCounterSource;
-import org.apache.log4j.MDC;
 import org.apache.log4j.NDC;
 import org.apache.tez.common.CallableWithNdc;
 import org.apache.tez.common.counters.FileSystemCounter;
 import org.apache.tez.common.counters.TezCounters;
 import org.apache.tez.runtime.task.TaskRunner2Callable;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import org.slf4j.MDC;
 
 /**
  * Custom thread pool implementation that records per thread file system statistics in TezCounters.

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java
Patch:
@@ -48,7 +48,6 @@
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.security.token.Token;
 import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.log4j.MDC;
 import org.apache.log4j.NDC;
 import org.apache.tez.common.CallableWithNdc;
 import org.apache.tez.common.TezCommonUtils;
@@ -71,6 +70,7 @@
 import org.apache.tez.runtime.task.TezTaskRunner2;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import org.slf4j.MDC;
 
 import javax.net.SocketFactory;
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
Patch:
@@ -77,7 +77,7 @@ public void initialize(QueryState queryState, QueryPlan queryPlan, DriverContext
         ColumnProjectionUtils.appendReadColumns(
             job, ts.getNeededColumnIDs(), ts.getNeededColumns(), ts.getNeededNestedColumnPaths());
         // push down filters
-        HiveInputFormat.pushFilters(job, ts);
+        HiveInputFormat.pushFilters(job, ts, null);
 
         AcidUtils.setAcidTableScan(job, ts.getConf().isAcidTable());
         AcidUtils.setAcidOperationalProperties(job, ts.getConf().getAcidOperationalProperties());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java
Patch:
@@ -207,7 +207,7 @@ public void initializeMapredLocalWork(MapJoinDesc mjConf, Configuration hconf,
       ColumnProjectionUtils.appendReadColumns(
           jobClone, ts.getNeededColumnIDs(), ts.getNeededColumns(), ts.getNeededNestedColumnPaths());
       // push down filters
-      HiveInputFormat.pushFilters(jobClone, ts);
+      HiveInputFormat.pushFilters(jobClone, ts, null);
 
       AcidUtils.setAcidTableScan(jobClone, ts.getConf().isAcidTable());
       AcidUtils.setAcidOperationalProperties(jobClone, ts.getConf().getAcidOperationalProperties());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java
Patch:
@@ -483,7 +483,7 @@ private void initializeOperators(Map<FetchOperator, JobConf> fetchOpJobConfMap)
       ColumnProjectionUtils.appendReadColumns(
           jobClone, ts.getNeededColumnIDs(), ts.getNeededColumns(), ts.getNeededNestedColumnPaths());
       // push down filters
-      HiveInputFormat.pushFilters(jobClone, ts);
+      HiveInputFormat.pushFilters(jobClone, ts, null);
 
       AcidUtils.setAcidTableScan(jobClone, ts.getConf().isAcidTable());
       AcidUtils.setAcidOperationalProperties(jobClone, ts.getConf().getAcidOperationalProperties());

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java
Patch:
@@ -110,7 +110,7 @@ public RecordReader<NullWritable, VectorizedRowBatch> getRecordReader(
       }
       // For non-vectorized operator case, wrap the reader if possible.
       RecordReader<NullWritable, VectorizedRowBatch> result = rr;
-      if (!Utilities.getUseVectorizedInputFileFormat(job)) {
+      if (!Utilities.getIsVectorized(job)) {
         result = wrapLlapReader(includedCols, rr, split);
         if (result == null) {
           // Cannot wrap a reader for non-vectorized pipeline.

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
Patch:
@@ -222,7 +222,7 @@ public static InputFormat<WritableComparable, Writable> wrapForLlap(
     String ifName = inputFormat.getClass().getCanonicalName();
     boolean isSupported = inputFormat instanceof LlapWrappableInputFormatInterface;
     boolean isCacheOnly = inputFormat instanceof LlapCacheOnlyInputFormatInterface;
-    boolean isVectorized = Utilities.getUseVectorizedInputFileFormat(conf);
+    boolean isVectorized = Utilities.getIsVectorized(conf);
     if (!isVectorized) {
       // Pretend it's vectorized if the non-vector wrapped is enabled.
       isVectorized = HiveConf.getBoolVar(conf, ConfVars.LLAP_IO_NONVECTOR_WRAPPER_ENABLED)

File: ql/src/java/org/apache/hadoop/hive/ql/io/NullRowsInputFormat.java
Patch:
@@ -71,7 +71,7 @@ public static class NullRowsRecordReader implements RecordReader {
     private boolean addPartitionCols = true;
 
     public NullRowsRecordReader(Configuration conf, InputSplit split) throws IOException {
-      boolean isVectorMode = Utilities.getUseVectorizedInputFileFormat(conf);
+      boolean isVectorMode = Utilities.getIsVectorized(conf);
       if (LOG.isDebugEnabled()) {
         LOG.debug(getClass().getSimpleName() + " in "
             + (isVectorMode ? "" : "non-") + "vector mode");

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
Patch:
@@ -506,7 +506,7 @@ public boolean validateInput(FileSystem fs, HiveConf conf,
                                List<FileStatus> files
                               ) throws IOException {
 
-    if (Utilities.getUseVectorizedInputFileFormat(conf)) {
+    if (Utilities.getIsVectorized(conf)) {
       return new VectorizedOrcInputFormat().validateInput(fs, conf, files);
     }
 
@@ -1890,7 +1890,7 @@ public InputSplit[] getSplits(JobConf job,
   getRecordReader(InputSplit inputSplit, JobConf conf,
                   Reporter reporter) throws IOException {
     //CombineHiveInputFormat may produce FileSplit that is not OrcSplit
-    boolean vectorMode = Utilities.getUseVectorizedInputFileFormat(conf);
+    boolean vectorMode = Utilities.getIsVectorized(conf);
     boolean isAcidRead = isAcidRead(conf, inputSplit);
     if (!isAcidRead) {
       if (vectorMode) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java
Patch:
@@ -74,7 +74,7 @@ public org.apache.hadoop.mapred.RecordReader<NullWritable, ArrayWritable> getRec
       final org.apache.hadoop.mapred.Reporter reporter
       ) throws IOException {
     try {
-      if (Utilities.getUseVectorizedInputFileFormat(job)) {
+      if (Utilities.getIsVectorized(job)) {
         if (LOG.isDebugEnabled()) {
           LOG.debug("Using vectorized record reader");
         }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/ptf/VectorPTFOperator.java
Patch:
@@ -227,7 +227,6 @@ protected void setupVOutContext() {
       int outputColumn = outputProjectionColumnMap[i];
       vOutContext.addProjectionColumn(columnName, outputColumn);
     }
-    vOutContext.setInitialTypeInfos(Arrays.asList(outputTypeInfos));
   }
 
   /*

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloDefaultIndexScanner.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloIndexLexicoder.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloIndexScanner.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloIndexScannerException.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloStorageHandler.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/Utils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements. See the NOTICE file distributed with this
  * work for additional information regarding copyright ownership. The ASF

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/AccumuloIndexDefinition.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/AccumuloIndexedOutputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/HiveAccumuloSplit.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/HiveAccumuloTableOutputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/IndexOutputConfigurator.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/AccumuloPredicateHandler.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/AccumuloRangeGenerator.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/PrimitiveComparisonFilter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
  * this work for additional information regarding copyright ownership.

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/AccumuloCompositeRowId.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/AccumuloIndexParameters.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/CompositeAccumuloRowIdFactory.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/DefaultAccumuloRowIdFactory.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/test/org/apache/hadoop/hive/accumulo/TestAccumuloDefaultIndexScanner.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: accumulo-handler/src/test/org/apache/hadoop/hive/accumulo/TestAccumuloIndexLexicoder.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/AbstractCommandHandler.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/AbstractOutputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/BeeLine.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/BeeLineCommandCompleter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/BeeLineCompleter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/BeeLineOpts.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/BeeLineSignalHandler.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/BooleanCompleter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/BufferedRows.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/ClientCommandHookFactory.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/ClientHook.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/ColorBuffer.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/CommandHandler.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/Commands.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/DatabaseConnection.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/DatabaseConnections.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/DeprecatedSeparatedValuesOutputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/DriverInfo.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/IncrementalRows.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/IncrementalRowsWithNormalization.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/OutputFile.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/OutputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/ReflectiveCommandHandler.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/Reflector.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/Rows.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/SQLCompleter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/SeparatedValuesOutputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/SunSignalHandler.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/TableNameCompletor.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/TableOutputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/VerticalOutputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/XMLAttributeOutputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/XMLElementOutputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/cli/CliOptionsProcessor.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/cli/HiveCli.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/hs2connection/BeelineHS2ConnectionFileParseException.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/hs2connection/HS2ConnectionFileParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/hs2connection/HS2ConnectionFileUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/hs2connection/HiveSiteHS2ConnectionFileParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/hs2connection/UserHS2ConnectionFileParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/java/org/apache/hive/beeline/logs/BeelineInPlaceUpdateStream.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/ProxyAuthTest.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/TestBeeLineExceptionHandling.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/TestBeeLineOpts.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/TestBeelineArgParsing.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/TestBufferedRows.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/TestClientCommandHookFactory.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/TestCommands.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/TestHiveSchemaTool.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/TestIncrementalRows.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/TestShutdownHook.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/cli/TestHiveCli.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: beeline/src/test/org/apache/hive/beeline/hs2connection/TestUserHS2ConnectionFileParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: cli/src/java/org/apache/hadoop/hive/cli/OptionsProcessor.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: cli/src/test/org/apache/hadoop/hive/cli/TestCliSessionState.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: cli/src/test/org/apache/hadoop/hive/cli/TestOptionsProcessor.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: cli/src/test/org/apache/hadoop/hive/cli/TestRCFileCat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/ant/GenHiveTemplate.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/BlobStorageUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/CompressionUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/CopyOnFirstWriteProperties.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/FileUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/HiveInterruptCallback.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/HiveInterruptUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/HiveStatsUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/JavaUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/JvmMetrics.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/JvmPauseMonitor.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/LogUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/MemoryEstimate.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/ObjectPair.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/ServerUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/StringableMap.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/auth/HiveAuthUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/cli/HiveFileProcessor.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/cli/IHiveFileProcessor.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/cli/ShellCmdExecutor.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/io/CachingPrintStream.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/io/DigestPrintStream.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/io/FetchConverter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/io/NonSyncByteArrayInputStream.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/io/NonSyncByteArrayOutputStream.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/io/SortAndDigestPrintStream.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/io/SortPrintStream.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/Connection.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/DagJsonParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/DagJsonParserUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/JsonParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/JsonParserFactory.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/Op.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/Printer.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/Stage.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/Vertex.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/spark/SparkJsonParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/tez/TezJsonParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/log/InPlaceUpdate.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/log/LogRedirector.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/log/ProgressMonitor.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/LegacyMetrics.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/MetricsMBean.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/MetricsMBeanImpl.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/common/Metrics.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/common/MetricsConstant.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/common/MetricsFactory.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/common/MetricsScope.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/common/MetricsVariable.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/CodahaleMetrics.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/CodahaleReporter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/ConsoleMetricsReporter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/JmxMetricsReporter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/JsonFileMetricsReporter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/MetricVariableRatioGauge.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/Metrics2Reporter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/MetricsReporting.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/type/Decimal128.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/type/HiveBaseChar.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/type/HiveChar.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/type/HiveIntervalYearMonth.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/type/HiveVarchar.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/type/SignedInt128.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/type/SqlMathUtil.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/type/TimestampTZ.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/type/TimestampTZUtil.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/common/type/UnsignedInt128.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/conf/Constants.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/conf/HiveConfUtil.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/conf/HiveVariableSource.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/conf/LoopingByteArrayInputStream.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/conf/SystemVariables.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/conf/Validator.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/conf/VariableSubstitution.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/conf/valcoersion/JavaIOTmpdirVariableCoercion.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/conf/valcoersion/VariableCoercion.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hadoop/hive/ql/log/PerfLogger.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/HiveCompat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/HiveVersionAnnotation.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/ACLConfigurationParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/AnnotationUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/DateParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/DateUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/Decimal128FastBuffer.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/FixedSizedObjectPool.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/HashCodeUtil.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/HiveStringUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/HiveTestUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/HiveVersionInfo.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/Ref.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/ReflectionUtil.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/RetryUtilities.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/ShutdownHookManager.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/StreamPrinter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/common/util/TimestampParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/http/AdminAuthorizedServlet.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/http/ConfServlet.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/http/HttpServer.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/http/JMXJsonServlet.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
  * this work for additional information regarding copyright ownership.

File: common/src/java/org/apache/hive/http/Log4j2ConfiguratorServlet.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/java/org/apache/hive/http/StackServlet.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/TestBlobStorageUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/TestFileUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/TestLogUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/TestTezJsonParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/jsonexplain/TestOp.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/jsonexplain/TestStage.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/jsonexplain/TestVertex.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/jsonexplain/tez/TestTezJsonParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/metrics/MetricsTestUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/metrics/TestLegacyMetrics.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/metrics/metrics2/TestCodahaleMetrics.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/metrics/metrics2/TestCodahaleReportersConf.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/metrics/metrics2/TestMetricVariableRatioGauge.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/type/HiveDecimalTestBase.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/type/TestDecimal128.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/type/TestHiveBaseChar.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/type/TestHiveChar.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimalOrcSerializationUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/type/TestHiveIntervalDayTime.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/type/TestHiveIntervalYearMonth.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/type/TestHiveVarchar.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/type/TestSignedInt128.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/type/TestSqlMathUtil.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/type/TestTimestampTZ.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/common/type/TestUnsignedInt128.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveAsyncLogging.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveConfRestrictList.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveConfUtil.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveLogging.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/conf/TestSystemVariables.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hadoop/hive/conf/TestVariableSubstitution.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hive/common/util/MockFileSystem.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hive/common/util/TestACLConfigurationParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hive/common/util/TestDateParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hive/common/util/TestFixedSizedObjectPool.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hive/common/util/TestHiveStringUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hive/common/util/TestRetryUtilities.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hive/common/util/TestShutdownHookManager.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: common/src/test/org/apache/hive/common/util/TestTimestampParser.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/base64/Base64TextInputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/base64/Base64TextOutputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/genericudf/example/GenericUDFAdd10.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/genericudf/example/GenericUDFDBOutput.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/metastore/hooks/SampleURLHook.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/mr/GenericMR.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/mr/Mapper.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/mr/Output.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/mr/Reducer.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/mr/example/IdentityMapper.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/mr/example/WordCountReduce.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/serde2/MultiDelimitSerDe.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright 2010 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one

File: contrib/src/java/org/apache/hadoop/hive/contrib/serde2/RegexSerDe.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/serde2/TypedBytesSerDe.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/serde2/s3/S3LogDeserializer.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/serde2/s3/S3LogStruct.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udaf/example/UDAFExampleAvg.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udaf/example/UDAFExampleGroupConcat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udaf/example/UDAFExampleMax.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udaf/example/UDAFExampleMaxMinNUtil.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udaf/example/UDAFExampleMaxN.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udaf/example/UDAFExampleMin.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udaf/example/UDAFExampleMinN.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udf/UDFRowSequence.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udf/example/UDFExampleAdd.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udf/example/UDFExampleArraySum.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udf/example/UDFExampleFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udf/example/UDFExampleMapConcat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udf/example/UDFExampleStructPrint.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udtf/example/GenericUDTFCount2.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/udtf/example/GenericUDTFExplode2.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/Type.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesInput.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesOutput.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordInput.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordOutput.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordReader.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesRecordWriter.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritable.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableInput.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/test/org/apache/hadoop/hive/contrib/mr/TestGenericMR.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: contrib/src/test/org/apache/hadoop/hive/contrib/serde2/TestRegexSerDe.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandlerUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidOutputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidQueryBasedInputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/io/HiveDruidSplit.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidGroupByQueryRecordReader.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidQueryRecordReader.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSelectQueryRecordReader.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDeUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidTimeseriesQueryRecordReader.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidTopNQueryRecordReader.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidWritable.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/HiveDruidSerializationModule.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/PeriodGranularitySerializer.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/test/org/apache/hadoop/hive/druid/QTestDruidSerDe.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/test/org/apache/hadoop/hive/druid/QTestDruidSerDe2.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/test/org/apache/hadoop/hive/druid/QTestDruidStorageHandler.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/test/org/apache/hadoop/hive/druid/QTestDruidStorageHandler2.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/test/org/apache/hadoop/hive/druid/TestHiveDruidQueryBasedInputFormat.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: druid-handler/src/test/org/apache/hadoop/hive/druid/serde/TestDruidSerDe.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/AbstractHBaseKeyFactory.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/AbstractHBaseKeyPredicateDecomposer.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/ColumnMappings.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/CompositeHBaseKeyFactory.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/DataInputInputStream.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/DataOutputOutputStream.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: testutils/ptest2/src/main/java/org/apache/hive/ptest/api/client/PTestClient.java
Patch:
@@ -283,6 +283,8 @@ private static void assertRequired(CommandLine commandLine, String[] requiredOpt
     }
   }
   public static void main(String[] args) throws Exception {
+    //TODO This line can be removed once precommit jenkins jobs move to Java 8
+    System.setProperty("https.protocols", "TLSv1,TLSv1.1,TLSv1.2");
     CommandLineParser parser = new GnuParser();
     Options options = new Options();
     options.addOption(HELP_SHORT, HELP_LONG, false, "Display help text and exit");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
Patch:
@@ -516,7 +516,6 @@ private TezSessionState getNewTezSessionOnError(
   }
 
   DAGClient submit(JobConf conf, DAG dag, Ref<TezSessionState> sessionStateRef) throws Exception {
-
     perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_SUBMIT_DAG);
     DAGClient dagClient = null;
     TezSessionState sessionState = sessionStateRef.value;
@@ -526,6 +525,7 @@ DAGClient submit(JobConf conf, DAG dag, Ref<TezSessionState> sessionStateRef) th
         dagClient = sessionState.getSession().submitDAG(dag);
       } catch (SessionNotRunning nr) {
         console.printInfo("Tez session was closed. Reopening...");
+        sessionStateRef.value = null;
         sessionStateRef.value = sessionState = getNewTezSessionOnError(sessionState);
         console.printInfo("Session re-established.");
         dagClient = sessionState.getSession().submitDAG(dag);
@@ -535,12 +535,13 @@ DAGClient submit(JobConf conf, DAG dag, Ref<TezSessionState> sessionStateRef) th
       try {
         console.printInfo("Dag submit failed due to " + e.getMessage() + " stack trace: "
             + Arrays.toString(e.getStackTrace()) + " retrying...");
+        sessionStateRef.value = null;
         sessionStateRef.value = sessionState = getNewTezSessionOnError(sessionState);
         dagClient = sessionState.getSession().submitDAG(dag);
       } catch (Exception retryException) {
         // we failed to submit after retrying. Destroy session and bail.
-        sessionState.destroy();
         sessionStateRef.value = null;
+        sessionState.destroy();
         throw retryException;
       }
     }

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/NotificationListener.java
Patch:
@@ -422,6 +422,9 @@ protected boolean isConnectionHealthy() {
    * @throws JMSException
    */
   protected Session createSession() throws JMSException {
+    if (conn == null) {
+      return null;
+    }
     // We want message to be sent when session commits, thus we run in
     // transacted mode.
     return conn.createSession(true, Session.SESSION_TRANSACTED);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestAcidOnTez.java
Patch:
@@ -675,7 +675,6 @@ public void testAcidInsertWithRemoveUnion() throws Exception {
   @Test
   public void testBucketedAcidInsertWithRemoveUnion() throws Exception {
     HiveConf confForTez = new HiveConf(hiveConf); // make a clone of existing hive conf
-    confForTez.setBoolean("hive.stats.column.autogather", false);
     setupTez(confForTez);
     int[][] values = {{1,2},{2,4},{5,6},{6,8},{9,10}};
     runStatementOnDriver("delete from " + Table.ACIDTBL, confForTez);
@@ -708,9 +707,9 @@ public void testBucketedAcidInsertWithRemoveUnion() throws Exception {
     String[][] expected2 = {
       {"{\"transactionid\":21,\"bucketid\":536936448,\"rowid\":0}\t1\t2", "warehouse/t/delta_0000021_0000021_0000/bucket_00001"},
       {"{\"transactionid\":21,\"bucketid\":536870912,\"rowid\":0}\t2\t4", "warehouse/t/delta_0000021_0000021_0000/bucket_00000"},
-      {"{\"transactionid\":21,\"bucketid\":536936448,\"rowid\":1}\t5\t6", "warehouse/t/delta_0000021_0000021_0000/bucket_00001"},
+      {"{\"transactionid\":21,\"bucketid\":536936448,\"rowid\":2}\t5\t6", "warehouse/t/delta_0000021_0000021_0000/bucket_00001"},
       {"{\"transactionid\":21,\"bucketid\":536870912,\"rowid\":1}\t6\t8", "warehouse/t/delta_0000021_0000021_0000/bucket_00000"},
-      {"{\"transactionid\":21,\"bucketid\":536936448,\"rowid\":2}\t9\t10", "warehouse/t/delta_0000021_0000021_0000/bucket_00001"}
+      {"{\"transactionid\":21,\"bucketid\":536936448,\"rowid\":1}\t9\t10", "warehouse/t/delta_0000021_0000021_0000/bucket_00001"}
     };
     Assert.assertEquals("Unexpected row count", expected2.length, rs.size());
     for(int i = 0; i < expected2.length; i++) {

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -1337,6 +1337,9 @@ public void releaseLocksAndCommitOrRollback(boolean commit, HiveTxnManager txnMa
     // If we've opened a transaction we need to commit or rollback rather than explicitly
     // releasing the locks.
     conf.unset(ValidTxnList.VALID_TXNS_KEY);
+    if(!checkConcurrency()) {
+      return;
+    }
     if (txnMgr.isTxnOpen()) {
       if (commit) {
         if(conf.getBoolVar(ConfVars.HIVE_IN_TEST) && conf.getBoolVar(ConfVars.HIVETESTMODEROLLBACKTXN)) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -12720,7 +12720,9 @@ protected ASTNode analyzeCreateView(ASTNode ast, QB qb, PlannerContext plannerCt
       // We need to go lookup the table and get the select statement and then parse it.
       try {
         Table tab = getTableObjectByName(dbDotTable, true);
-        String viewText = tab.getViewOriginalText();
+        // We need to use the expanded text for the materialized view, as it will contain
+        // the qualified table aliases, etc.
+        String viewText = tab.getViewExpandedText();
         if (viewText.trim().isEmpty()) {
           throw new SemanticException(ErrorMsg.MATERIALIZED_VIEW_DEF_EMPTY);
         }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/RemoteSparkJobStatus.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.spark.Statistic.SparkStatistics;
 import org.apache.hadoop.hive.ql.exec.spark.Statistic.SparkStatisticsBuilder;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -174,7 +175,8 @@ private SparkJobInfo getSparkJobInfo() throws HiveException {
       return getJobInfo.get(sparkClientTimeoutInSeconds, TimeUnit.SECONDS);
     } catch (Exception e) {
       LOG.warn("Failed to get job info.", e);
-      throw new HiveException(e);
+      throw new HiveException(e, ErrorMsg.SPARK_GET_JOB_INFO_TIMEOUT,
+          Long.toString(sparkClientTimeoutInSeconds));
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SparkDynamicPartitionPruningResolver.java
Patch:
@@ -30,7 +30,6 @@
 import org.apache.hadoop.hive.ql.exec.OperatorUtils;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.spark.SparkTask;
-import org.apache.hadoop.hive.ql.exec.spark.SparkUtilities;
 import org.apache.hadoop.hive.ql.lib.Dispatcher;
 import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.lib.TaskGraphWalker;
@@ -117,7 +116,7 @@ private void removeSparkPartitionPruningSink(BaseWork sourceWork, MapWork target
     OperatorUtils.removeBranch(pruningSinkOp);
 
     // Remove all event source info from the target MapWork
-    String sourceWorkId = SparkUtilities.getWorkId(sourceWork);
+    String sourceWorkId = pruningSinkOp.getUniqueId();
     SparkPartitionPruningSinkDesc pruningSinkDesc = pruningSinkOp.getConf();
     targetMapWork.getEventSourceTableDescMap().get(sourceWorkId).remove(pruningSinkDesc.getTable());
     targetMapWork.getEventSourceColumnNameMap().get(sourceWorkId).remove(pruningSinkDesc.getTargetColumnName());

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedListColumnReader.java
Patch:
@@ -258,7 +258,8 @@ private void fillColumnVector(PrimitiveObjectInspector.PrimitiveCategory categor
         lcv.child = new BytesColumnVector(total);
         lcv.child.init();
         for (int i = 0; i < valueList.size(); i++) {
-          ((BytesColumnVector)lcv.child).setVal(i, ((List<byte[]>)valueList).get(i));
+          byte[] src = ((List<byte[]>)valueList).get(i);
+          ((BytesColumnVector)lcv.child).setRef(i, src, 0, src.length);
         }
         break;
       case FLOAT:

File: ql/src/test/org/apache/hadoop/hive/ql/io/parquet/VectorizedColumnReaderTestBase.java
Patch:
@@ -124,6 +124,7 @@ public class VectorizedColumnReaderTestBase {
       + "repeated fixed_len_byte_array(3) list_byte_array_field;"
       + "repeated binary list_binary_field;"
       + "repeated binary list_decimal_field (DECIMAL(5,2));"
+      + "repeated binary list_binary_field_for_repeat_test;"
       + "repeated int32 list_int32_field_for_repeat_test;"
       + "repeated group map_int32 (MAP_KEY_VALUE) {\n"
       + "  required int32 key;\n"

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1738,7 +1738,7 @@ public static enum ConfVars {
         "This many percentage of rows will be estimated as number of nulls in absence of statistics."),
     HIVESTATSAUTOGATHER("hive.stats.autogather", true,
         "A flag to gather statistics (only basic) automatically during the INSERT OVERWRITE command."),
-    HIVESTATSCOLAUTOGATHER("hive.stats.column.autogather", false,
+    HIVESTATSCOLAUTOGATHER("hive.stats.column.autogather", true,
         "A flag to gather column statistics automatically."),
     HIVESTATSDBCLASS("hive.stats.dbclass", "fs", new PatternSet("custom", "fs"),
         "The storage that stores temporary Hive statistics. In filesystem based statistics collection ('fs'), \n" +

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestAcidOnTez.java
Patch:
@@ -675,6 +675,7 @@ public void testAcidInsertWithRemoveUnion() throws Exception {
   @Test
   public void testBucketedAcidInsertWithRemoveUnion() throws Exception {
     HiveConf confForTez = new HiveConf(hiveConf); // make a clone of existing hive conf
+    confForTez.setBoolean("hive.stats.column.autogather", false);
     setupTez(confForTez);
     int[][] values = {{1,2},{2,4},{5,6},{6,8},{9,10}};
     runStatementOnDriver("delete from " + Table.ACIDTBL, confForTez);

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -113,7 +113,7 @@
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObjectType;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-import org.apache.hadoop.hive.ql.wm.TriggerContext;
+import org.apache.hadoop.hive.ql.wm.WmContext;
 import org.apache.hadoop.hive.serde2.ByteStream;
 import org.apache.hadoop.mapred.ClusterStatus;
 import org.apache.hadoop.mapred.JobClient;
@@ -732,8 +732,8 @@ private void setTriggerContext(final String queryId) {
     } else {
       queryStartTime = queryDisplay.getQueryStartTime();
     }
-    TriggerContext triggerContext = new TriggerContext(queryStartTime, queryId);
-    ctx.setTriggerContext(triggerContext);
+    WmContext wmContext = new WmContext(queryStartTime, queryId);
+    ctx.setWmContext(wmContext);
   }
 
   private boolean startImplicitTxn(HiveTxnManager txnManager) throws LockException {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/KillTriggerActionHandler.java
Patch:
@@ -28,7 +28,7 @@
 /**
  * Handles only Kill Action.
  */
-public class KillTriggerActionHandler implements TriggerActionHandler {
+public class KillTriggerActionHandler implements TriggerActionHandler<TezSessionState> {
   private static final Logger LOG = LoggerFactory.getLogger(KillTriggerActionHandler.class);
 
   @Override
@@ -37,7 +37,7 @@ public void applyAction(final Map<TezSessionState, Trigger> queriesViolated) {
       switch (entry.getValue().getAction().getType()) {
         case KILL_QUERY:
           TezSessionState sessionState = entry.getKey();
-          String queryId = sessionState.getTriggerContext().getQueryId();
+          String queryId = sessionState.getWmContext().getQueryId();
           try {
             KillQuery killQuery = sessionState.getKillQuery();
             // if kill query is null then session might have been released to pool or closed already

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/monitoring/PrintSummary.java
Patch:
@@ -19,6 +19,6 @@
 
 import org.apache.hadoop.hive.ql.session.SessionState;
 
-interface PrintSummary {
+public interface PrintSummary {
   void print(SessionState.LogHelper console);
 }

File: ql/src/java/org/apache/hadoop/hive/ql/wm/Trigger.java
Patch:
@@ -15,11 +15,14 @@
  */
 package org.apache.hadoop.hive.ql.wm;
 
+import org.codehaus.jackson.map.annotate.JsonSerialize;
+
 /**
  * Trigger interface which gets mapped to CREATE TRIGGER .. queries. A trigger can have a name, expression and action.
  * Trigger is a simple expression which gets evaluated during the lifecycle of query and executes an action
  * if the expression defined in trigger evaluates to true.
  */
+@JsonSerialize
 public interface Trigger {
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/wm/TriggerActionHandler.java
Patch:
@@ -22,11 +22,11 @@
 /**
  * Interface for handling rule violations by queries and for performing actions defined in the rules.
  */
-public interface TriggerActionHandler {
+public interface TriggerActionHandler<SessionType> {
   /**
    * Applies the action defined in the rule for the specified queries
    *
    * @param queriesViolated - violated queries and the rule it violated
    */
-  void applyAction(Map<TezSessionState, Trigger> queriesViolated);
+  void applyAction(Map<SessionType, Trigger> queriesViolated);
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -1993,7 +1993,7 @@ public static Path createMoveTask(Task<? extends Serializable> currTask, boolean
 
         // Create the required temporary file in the HDFS location if the destination
         // path of the FileSinkOperator table is a blobstore path.
-        Path tmpDir = baseCtx.getTempDirForPath(fileSinkDesc.getDestPath(), true);
+        Path tmpDir = baseCtx.getTempDirForFinalJobPath(fileSinkDesc.getDestPath());
 
         // Change all the linked file sink descriptors
         if (fileSinkDesc.isLinkedFileSink()) {

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreAlterColumnPar.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hadoop.hive.metastore;
 
-import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
@@ -100,7 +100,7 @@ public void testAlterColumn() throws Exception {
       // parameter value not changed to false in connection 2.  int to smallint throws exception
       try {
         stmt2.execute("alter table t1 change column c1 c1 smallint");
-        assertTrue("Exception not thrown", true);
+        fail("Exception not thrown");
       } catch (Exception e1) {
         assertTrue("Unexpected exception: " + e1.getMessage(), e1.getMessage().contains(
             "Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions"));
@@ -109,7 +109,7 @@ public void testAlterColumn() throws Exception {
       // parameter value is still false in 1st connection.  The alter still goes through.
       stmt1.execute("alter table t1 change column c1 c1 smallint");
     } catch (Exception e2) {
-      assertTrue("Unexpected Exception: " + e2.getMessage(), true);
+      fail("Unexpected Exception: " + e2.getMessage());
     }
   }
 }

File: cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
Patch:
@@ -158,8 +158,8 @@ public Map<String, String> getHiveVariable() {
         }
       }
     } else if (cmd_trimmed.startsWith("!")) {
-
-      String shell_cmd = cmd_trimmed.substring(1);
+      // for shell commands, use unstripped command
+      String shell_cmd = cmd.trim().substring(1);
       shell_cmd = new VariableSubstitution(new HiveVariableSource() {
         @Override
         public Map<String, String> getHiveVariable() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapperBatch.java
Patch:
@@ -1007,6 +1007,8 @@ public Object getWritableKeyValue(VectorHashKeyWrapper kw, int keyIndex,
     case DECIMAL:
       return keyOutputWriter.writeValue(
           kw.getDecimal(columnTypeSpecificIndex));
+    case DECIMAL_64:
+      throw new RuntimeException("Getting writable for DECIMAL_64 not supported");
     case TIMESTAMP:
       return keyOutputWriter.writeValue(
           kw.getTimestamp(columnTypeSpecificIndex));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java
Patch:
@@ -253,12 +253,12 @@ public VectorMapJoinCommonOperator(CompilationOpContext ctx, OperatorDesc conf,
     bigTableKeyColumnMap = vectorMapJoinInfo.getBigTableKeyColumnMap();
     bigTableKeyColumnNames = vectorMapJoinInfo.getBigTableKeyColumnNames();
     bigTableKeyTypeInfos = vectorMapJoinInfo.getBigTableKeyTypeInfos();
-    bigTableKeyExpressions = vectorMapJoinInfo.getBigTableKeyExpressions();
+    bigTableKeyExpressions = vectorMapJoinInfo.getSlimmedBigTableKeyExpressions();
 
     bigTableValueColumnMap = vectorMapJoinInfo.getBigTableValueColumnMap();
     bigTableValueColumnNames = vectorMapJoinInfo.getBigTableValueColumnNames();
     bigTableValueTypeInfos = vectorMapJoinInfo.getBigTableValueTypeInfos();
-    bigTableValueExpressions = vectorMapJoinInfo.getBigTableValueExpressions();
+    bigTableValueExpressions = vectorMapJoinInfo.getSlimmedBigTableValueExpressions();
 
     bigTableRetainedMapping = vectorMapJoinInfo.getBigTableRetainedMapping();
 

File: llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java
Patch:
@@ -474,7 +474,7 @@ public LlapRecordReaderTaskUmbilicalExternalResponder() {
     public void submissionFailed(String fragmentId, Throwable throwable) {
       try {
         sendOrQueueEvent(ReaderEvent.errorEvent(
-            "Received submission failed event for fragment ID " + fragmentId));
+            "Received submission failed event for fragment ID " + fragmentId + ": " + throwable.toString()));
       } catch (Exception err) {
         LOG.error("Error during heartbeat responder:", err);
       }

File: ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java
Patch:
@@ -1981,15 +1981,14 @@ public static Range combineRange(Range range1, Range range2) {
       long min2 = range2.minValue.longValue();
       long max2 = range2.maxValue.longValue();
 
-      if (   (min1 < min2 && max1 < max2)
-          || (min1 > min2 && max1 > max2)) {
+      if (max1 < min2 || max2 < min1) {
         // No overlap between the two ranges
         return null;
       } else {
         // There is an overlap of ranges - create combined range.
         return new ColStatistics.Range(
             Math.min(min1, min2),
-            Math.max(max1,  max2));
+            Math.max(max1, max2));
       }
     }
     return null;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/UserPoolMapping.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.metastore.api.WMMapping;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import org.spark_project.guava.collect.Lists;
+import com.google.common.collect.Lists;
 
 class UserPoolMapping {
   @SuppressWarnings("unused")

File: ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestWorkloadManager.java
Patch:
@@ -149,12 +149,12 @@ public static class WorkloadManagerForTest extends WorkloadManager {
 
     public WorkloadManagerForTest(String yarnQueue, HiveConf conf, int numSessions,
         QueryAllocationManager qam) {
-      super(yarnQueue, conf, qam, createDummyPlan(numSessions));
+      super(null, yarnQueue, conf, qam, createDummyPlan(numSessions));
     }
 
     public WorkloadManagerForTest(String yarnQueue, HiveConf conf,
         QueryAllocationManager qam, WMFullResourcePlan plan) {
-      super(yarnQueue, conf, qam, plan);
+      super(null, yarnQueue, conf, qam, plan);
     }
 
     private static WMFullResourcePlan createDummyPlan(int numSessions) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -7151,6 +7151,7 @@ protected Operator genFileSinkPlan(String dest, QB qb, Operator input)
     // hive.stats.autogather=true
     // and it is an insert overwrite or insert into table
     if (dest_tab != null
+        && !dest_tab.isNonNative()
         && conf.getBoolVar(ConfVars.HIVESTATSAUTOGATHER)
         && conf.getBoolVar(ConfVars.HIVESTATSCOLAUTOGATHER)
         && ColumnStatsAutoGatherContext.canRunAutogatherStats(fso)) {

File: service/src/java/org/apache/hive/service/cli/GetInfoValue.java
Patch:
@@ -52,7 +52,7 @@ public GetInfoValue(TGetInfoValue tGetInfoValue) {
       stringValue = tGetInfoValue.getStringValue();
       break;
     default:
-      throw new IllegalArgumentException("Unreconigzed TGetInfoValue");
+      throw new IllegalArgumentException("Unrecognized TGetInfoValue");
     }
   }
 

File: service/src/java/org/apache/hive/service/cli/operation/GetColumnsOperation.java
Patch:
@@ -104,7 +104,7 @@ public class GetColumnsOperation extends MetadataOperation {
       "Schema of table that is the scope of a reference attribute "
       + "(null if the DATA_TYPE isn't REF)")
   .addPrimitiveColumn("SCOPE_TABLE", Type.STRING_TYPE,
-      "Table name that this the scope of a reference attribure "
+      "Table name that this the scope of a reference attribute "
       + "(null if the DATA_TYPE isn't REF)")
   .addPrimitiveColumn("SOURCE_DATA_TYPE", Type.SMALLINT_TYPE,
       "Source type of a distinct type or user-generated Ref type, "

File: service/src/java/org/apache/hive/service/cli/session/SessionManager.java
Patch:
@@ -415,7 +415,7 @@ public HiveSession createSession(SessionHandle sessionHandle, TProtocolVersion p
         session = (HiveSession) constructor.newInstance(sessionHandle, protocol, username, password,
           hiveConf, ipAddress, forwardedAddresses);
         } catch (Exception e) {
-          throw new HiveSQLException("Cannot initilize session class:" + sessionImplclassName, e);
+          throw new HiveSQLException("Cannot initialize session class:" + sessionImplclassName, e);
         }
       }
     }

File: service/src/java/org/apache/hive/service/server/HiveServer2.java
Patch:
@@ -171,7 +171,7 @@ public void run() {
     try {
       hiveConf.set(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST.varname, getServerHost());
     } catch (Throwable t) {
-      throw new Error("Unable to intitialize HiveServer2", t);
+      throw new Error("Unable to initialize HiveServer2", t);
     }
     if (HiveConf.getBoolVar(hiveConf, ConfVars.LLAP_HS2_ENABLE_COORDINATOR)) {
       // See method comment.
@@ -503,7 +503,7 @@ public void process(WatchedEvent event) {
           try {
             znode.close();
             LOG.warn("This HiveServer2 instance is now de-registered from ZooKeeper. "
-                + "The server will be shut down after the last client sesssion completes.");
+                + "The server will be shut down after the last client session completes.");
           } catch (IOException e) {
             LOG.error("Failed to close the persistent ephemeral znode", e);
           } finally {

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -8369,7 +8369,7 @@ private synchronized void checkSchema() throws MetaException {
               " Metastore is not upgraded or corrupt");
         } else {
           LOG.error("Version information found in metastore differs {} " +
-              "from expected schema version {}. Schema verififcation is disabled {}", 
+              "from expected schema version {}. Schema verification is disabled {}",
               dbSchemaVer, hiveSchemaVer, ConfVars.SCHEMA_VERIFICATION);
           setMetaStoreSchemaVersion(hiveSchemaVer,
             "Set by MetaStore " + USER + "@" + HOSTNAME);

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -2520,7 +2520,7 @@ private LockResponse checkLock(Connection dbConn, long extLockId)
 
           String msg = "Aborting " + JavaUtils.txnIdToString(writeSet.get(0).txnId) +
             " since a concurrent committed transaction [" + JavaUtils.txnIdToString(rs.getLong(4)) + "," + rs.getLong(5) +
-            "] has already updated resouce '" + resourceName + "'";
+            "] has already updated resource '" + resourceName + "'";
           LOG.info(msg);
           if(abortTxns(dbConn, Collections.singletonList(writeSet.get(0).txnId), true) != 1) {
             throw new IllegalStateException(msg + " FAILED!");

File: beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java
Patch:
@@ -337,7 +337,7 @@ private boolean checkMetaStorePartitionLocation(Connection conn, URI[] defaultSe
       }
       pStmt.close();
     } catch (SQLException e) {
-      throw new HiveMetaException("Failed to get Partiton Location Info.", e);
+      throw new HiveMetaException("Failed to get Partition Location Info.", e);
     }
     if (numOfInvalid > 0) {
       isValid = false;

File: beeline/src/test/org/apache/hive/beeline/ProxyAuthTest.java
Patch:
@@ -196,7 +196,7 @@ public static void main(String[] args) throws Exception {
     try {
       url = "jdbc:hive2://" + host + ":" + port + "/default;auth=delegationToken";
       con = DriverManager.getConnection(url);
-      throw new Exception ("connection should have failed after token cancelation");
+      throw new Exception ("connection should have failed after token cancellation");
     } catch (SQLException e) {
       // Expected to fail due to canceled token
     }

File: common/src/java/org/apache/hadoop/hive/common/HiveInterruptUtils.java
Patch:
@@ -64,7 +64,7 @@ public static void checkInterrupted() {
       } catch (InterruptedException e) {
         interrupt = e;
       }
-      throw new RuntimeException("Interuppted", interrupt);
+      throw new RuntimeException("Interrupted", interrupt);
     }
   }
 }

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java
Patch:
@@ -274,7 +274,7 @@ private void setupKeyRange(Scan scan, List<IndexSearchCondition> conditions, boo
         objInspector = (PrimitiveObjectInspector)eval.initialize(null);
         writable = eval.evaluate(null);
       } catch (ClassCastException cce) {
-        throw new IOException("Currently only primitve types are supported. Found: " +
+        throw new IOException("Currently only primitive types are supported. Found: " +
             sc.getConstantDesc().getTypeString());
       } catch (HiveException e) {
         throw new IOException(e);

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java
Patch:
@@ -604,7 +604,7 @@ public void run() {
         try {
           Thread.sleep(sleepTime);
         } catch (InterruptedException e) {
-          LOG.info("Cleaner thread sleep interupted", e);
+          LOG.info("Cleaner thread sleep interrupted", e);
         }
       }
     }

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/mutate/client/lock/HeartbeatTimerTask.java
Patch:
@@ -69,7 +69,7 @@ public void run() {
   }
 
   private void failLock(Exception e) {
-    LOG.debug("Lock " + lockId + " failed, cancelling heartbeat and notifiying listener: " + listener, e);
+    LOG.debug("Lock " + lockId + " failed, cancelling heartbeat and notifying listener: " + listener, e);
     // Cancel the heartbeat
     cancel();
     listener.lockFailed(lockId, transactionId, Lock.asStrings(tableDescriptors), e);
@@ -80,4 +80,4 @@ public String toString() {
     return "HeartbeatTimerTask [lockId=" + lockId + ", transactionId=" + transactionId + "]";
   }
 
-}
\ No newline at end of file
+}

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java
Patch:
@@ -135,7 +135,7 @@ public LlapDaemon(Configuration daemonConf, int numExecutors, long executorMemor
     Preconditions.checkArgument(localDirs != null && localDirs.length > 0,
         "Work dirs must be specified");
     Preconditions.checkArgument(shufflePort == 0 || (shufflePort > 1024 && shufflePort < 65536),
-        "Shuffle Port must be betwee 1024 and 65535, or 0 for automatic selection");
+        "Shuffle Port must be between 1024 and 65535, or 0 for automatic selection");
     int outputFormatServicePort = HiveConf.getIntVar(daemonConf, HiveConf.ConfVars.LLAP_DAEMON_OUTPUT_SERVICE_PORT);
     Preconditions.checkArgument(outputFormatServicePort == 0
         || (outputFormatServicePort > 1024 && outputFormatServicePort < 65536),

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapProtocolServerImpl.java
Patch:
@@ -256,7 +256,7 @@ public GetTokenResponseProto getDelegationToken(RpcController controller,
     if (isRestrictedToClusterUser && !clusterUser.equals(callingUser.getShortUserName())) {
       throw new ServiceException("Management protocol ACL is too permissive. The access has been"
           + " automatically restricted to " + clusterUser + "; " + callingUser.getShortUserName()
-          + " is denied acccess. Please set " + ConfVars.LLAP_VALIDATE_ACLS.varname + " to false,"
+          + " is denied access. Please set " + ConfVars.LLAP_VALIDATE_ACLS.varname + " to false,"
           + " or adjust " + ConfVars.LLAP_MANAGEMENT_ACL.varname + " and "
           + ConfVars.LLAP_MANAGEMENT_ACL_DENY.varname + " to a more restrictive ACL.");
     }

File: llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java
Patch:
@@ -240,7 +240,7 @@ static class ShuffleMetrics implements ChannelFutureListener {
     MutableCounterLong shuffleOutputBytes;
     @Metric("# of failed shuffle outputs")
     MutableCounterInt shuffleOutputsFailed;
-    @Metric("# of succeeeded shuffle outputs")
+    @Metric("# of succeeded shuffle outputs")
     MutableCounterInt shuffleOutputsOK;
     @Metric("# of current shuffle connections")
     MutableGaugeInt shuffleConnections;
@@ -961,7 +961,7 @@ protected void verifyRequest(String appid, ChannelHandlerContext ctx,
           ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION);
       if (LOG.isDebugEnabled()) {
         int len = reply.length();
-        LOG.debug("Fetcher request verfied. enc_str=" + enc_str + ";reply=" +
+        LOG.debug("Fetcher request verified. enc_str=" + enc_str + ";reply=" +
             reply.substring(len-len/2, len-1));
       }
     }

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyPrimitiveObjectInspectorFactory.java
Patch:
@@ -177,7 +177,7 @@ public static AbstractPrimitiveLazyObjectInspector<?> getLazyObjectInspector(
       break;
     default:
       throw new RuntimeException(
-          "Primitve type " + typeInfo.getPrimitiveCategory() + " should not take parameters");
+          "Primitive type " + typeInfo.getPrimitiveCategory() + " should not take parameters");
     }
 
     AbstractPrimitiveLazyObjectInspector<?> prev =

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/security/TokenStoreDelegationTokenSecretManager.java
Patch:
@@ -118,7 +118,7 @@ public byte[] retrievePassword(DelegationTokenIdentifier identifier) throws Inva
   public DelegationTokenIdentifier cancelToken(Token<DelegationTokenIdentifier> token,
       String canceller) throws IOException {
     DelegationTokenIdentifier id = getTokenIdentifier(token);
-    LOGGER.info("Token cancelation requested for identifier: "+id);
+    LOGGER.info("Token cancellation requested for identifier: "+id);
     this.tokenStore.removeToken(id);
     return id;
   }

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/security/ZooKeeperTokenStore.java
Patch:
@@ -442,7 +442,7 @@ public void init(Object hmsHandler, HadoopThriftAuthBridge.Server.ServerMode sMo
               MetastoreDelegationTokenManager.DELEGATION_TOKEN_STORE_ZK_CONNECT_STR_ALTERNATE,
               null);
       if (zkConnectString == null || zkConnectString.trim().isEmpty()) {
-        throw new IllegalArgumentException("Zookeeper connect string has to be specifed through "
+        throw new IllegalArgumentException("Zookeeper connect string has to be specified through "
             + "either " + MetastoreDelegationTokenManager.DELEGATION_TOKEN_STORE_ZK_CONNECT_STR
             + " or "
             + MetastoreDelegationTokenManager.DELEGATION_TOKEN_STORE_ZK_CONNECT_STR_ALTERNATE

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java
Patch:
@@ -61,7 +61,7 @@ static synchronized public Object evalExprWithPart(ExprNodeDesc expr,
     String[] partKeyTypes = pcolTypes.trim().split(":");
 
     if (partSpec.size() != partKeyTypes.length) {
-        throw new HiveException("Internal error : Partition Spec size, " + partProps.size() +
+        throw new HiveException("Internal error : Partition Spec size, " + partSpec.size() +
                 " doesn't match partition key definition size, " + partKeyTypes.length);
     }
     boolean hasVC = vcs != null && !vcs.isEmpty();

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java
Patch:
@@ -550,14 +550,14 @@ public void showResourcePlans(DataOutputStream out, List<WMResourcePlan> resourc
       for (WMResourcePlan plan : resourcePlans) {
         out.write(plan.getName().getBytes("UTF-8"));
         out.write(separator);
+        out.write(plan.getStatus().name().getBytes("UTF-8"));
+        out.write(separator);
         if (plan.isSetQueryParallelism()) {
           out.writeBytes(Integer.toString(plan.getQueryParallelism()));
         } else {
           out.writeBytes("null");
         }
         out.write(separator);
-        out.write(plan.getStatus().name().getBytes("UTF-8"));
-        out.write(separator);
         if (plan.isSetDefaultPoolPath()) {
           out.write(plan.getDefaultPoolPath().getBytes("UTF-8"));
         } else {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/GuaranteedTasksAllocator.java
Patch:
@@ -147,6 +147,7 @@ private void updateSessionAsync(final WmTezSession session, final int intAlloc)
     //       HS2 session pool paths, and this patch removes the last one (reopen).
     UpdateQueryRequestProto request = UpdateQueryRequestProto
         .newBuilder().setGuaranteedTaskCount(intAlloc).build();
+    LOG.info("Updating {} with {} guaranteed tasks", session.getSessionId(), intAlloc);
     amCommunicator.sendUpdateQuery(request, (AmPluginNode)session, new UpdateCallback(session));
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPool.java
Patch:
@@ -312,8 +312,6 @@ private void configureAmRegistry(SessionType session) {
       HiveConf conf = session.getConf();
       conf.set(ConfVars.LLAP_TASK_SCHEDULER_AM_REGISTRY_NAME.varname, amRegistryName);
       conf.set(ConfVars.HIVESESSIONID.varname, session.getSessionId());
-      // TODO: can be enable temporarily for testing
-      // conf.set(LlapTaskSchedulerService.LLAP_PLUGIN_ENDPOINT_ENABLED, "true");
     }
   }
 

File: itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/DummyRawStoreFailEvent.java
Patch:
@@ -981,9 +981,9 @@ public String getMetastoreDbUuid() throws MetaException {
   }
 
   @Override
-  public void createResourcePlan(WMResourcePlan resourcePlan)
+  public void createResourcePlan(WMResourcePlan resourcePlan, int defaultPoolSize)
       throws AlreadyExistsException, MetaException {
-    objectStore.createResourcePlan(resourcePlan);
+    objectStore.createResourcePlan(resourcePlan, defaultPoolSize);
   }
 
   @Override

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestTriggersWorkloadManager.java
Patch:
@@ -84,11 +84,9 @@ protected void setupTriggers(final List<Trigger> triggers) throws Exception {
     WMPool pool = new WMPool("rp", "llap");
     pool.setAllocFraction(1.0f);
     pool.setQueryParallelism(1);
-    WMMapping mapping = new WMMapping("rp", "DEFAULT", "");
-    mapping.setPoolName("llap");
     WMFullResourcePlan rp = new WMFullResourcePlan(
         new WMResourcePlan("rp"), Lists.newArrayList(pool));
-    rp.addToMappings(mapping);
+    rp.getPlan().setDefaultPoolPath("llap");
     for (Trigger trigger : triggers) {
       rp.addToTriggers(wmTriggerFromTrigger(trigger));
       rp.addToPoolTriggers(new WMPoolTrigger("llap", trigger.getName()));

File: metastore/src/test/org/apache/hadoop/hive/metastore/DummyRawStoreControlledCommit.java
Patch:
@@ -941,9 +941,9 @@ public String getMetastoreDbUuid() throws MetaException {
   }
 
   @Override
-  public void createResourcePlan(WMResourcePlan resourcePlan)
+  public void createResourcePlan(WMResourcePlan resourcePlan, int defaultPoolSize)
       throws AlreadyExistsException, MetaException {
-    objectStore.createResourcePlan(resourcePlan);
+    objectStore.createResourcePlan(resourcePlan, defaultPoolSize);
   }
 
   @Override

File: metastore/src/test/org/apache/hadoop/hive/metastore/DummyRawStoreForJdoConnection.java
Patch:
@@ -953,7 +953,8 @@ public String getMetastoreDbUuid() throws MetaException {
   }
 
   @Override
-  public void createResourcePlan(WMResourcePlan resourcePlan) throws MetaException {
+  public void createResourcePlan(
+      WMResourcePlan resourcePlan, int defaultPoolSize) throws MetaException {
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java
Patch:
@@ -432,6 +432,9 @@ public void showResourcePlans(DataOutputStream out, List<WMResourcePlan> resourc
         if (plan.isSetQueryParallelism()) {
           generator.writeNumberField("queryParallelism", plan.getQueryParallelism());
         }
+        if (plan.isSetDefaultPoolPath()) {
+          generator.writeStringField("defaultPoolPath", plan.getDefaultPoolPath());
+        }
         generator.writeEndObject();
       }
       generator.writeEndArray();

File: service/src/java/org/apache/hive/service/server/HiveServer2.java
Patch:
@@ -302,9 +302,7 @@ private WMFullResourcePlan createTestResourcePlan() {
     pool.setQueryParallelism(1);
     resourcePlan = new WMFullResourcePlan(
         new WMResourcePlan("testDefault"), Lists.newArrayList(pool));
-    WMMapping mapping = new WMMapping("testDefault", "DEFAULT", "");
-    mapping.setPoolName("llap");
-    resourcePlan.addToMappings(mapping);
+    resourcePlan.getPlan().setDefaultPoolPath("testDefault");
     return resourcePlan;
   }
 

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/RawStore.java
Patch:
@@ -751,7 +751,7 @@ List<String> createTableWithConstraints(Table tbl, List<SQLPrimaryKey> primaryKe
    */
   String getMetastoreDbUuid() throws MetaException;
 
-  void createResourcePlan(WMResourcePlan resourcePlan)
+  void createResourcePlan(WMResourcePlan resourcePlan, int defaultPoolSize)
       throws AlreadyExistsException, MetaException;
 
   WMResourcePlan getResourcePlan(String name) throws NoSuchObjectException, MetaException;

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java
Patch:
@@ -2242,9 +2242,9 @@ void setInitializedForTest() {
   }
 
   @Override
-  public void createResourcePlan(WMResourcePlan resourcePlan)
+  public void createResourcePlan(WMResourcePlan resourcePlan, int defaultPoolSize)
       throws AlreadyExistsException, MetaException {
-    rawStore.createResourcePlan(resourcePlan);
+    rawStore.createResourcePlan(resourcePlan, defaultPoolSize);
   }
 
   @Override

File: jdbc/src/java/org/apache/hive/jdbc/JdbcColumn.java
Patch:
@@ -161,6 +161,8 @@ static Type typeStringToHiveType(String type) throws SQLException {
       return Type.ARRAY_TYPE;
     } else if ("struct".equalsIgnoreCase(type)) {
       return Type.STRUCT_TYPE;
+    } else if ("uniontype".equalsIgnoreCase(type)) {
+      return Type.UNION_TYPE;
     } else if ("void".equalsIgnoreCase(type) || "null".equalsIgnoreCase(type)) {
       return Type.NULL_TYPE;
     }

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java
Patch:
@@ -1356,8 +1356,8 @@ public static PrimitiveGrouping getPrimitiveGrouping(PrimitiveCategory primitive
         return PrimitiveGrouping.STRING_GROUP;
       case BOOLEAN:
         return PrimitiveGrouping.BOOLEAN_GROUP;
-      case TIMESTAMP:
       case DATE:
+      case TIMESTAMP:
       case TIMESTAMPLOCALTZ:
         return PrimitiveGrouping.DATE_GROUP;
       case INTERVAL_YEAR_MONTH:

File: ql/src/java/org/apache/hadoop/hive/ql/parse/PTFTranslator.java
Patch:
@@ -613,6 +613,7 @@ private static void validateValueBoundaryExprType(ObjectInspector OI)
     case SHORT:
     case DECIMAL:
     case TIMESTAMP:
+    case TIMESTAMPLOCALTZ:
     case DATE:
     case STRING:
     case VARCHAR:

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -4429,9 +4429,10 @@ private void dropPartitions(Hive db, Table tbl, DropTableDesc dropTbl) throws Hi
       // to the  metastore to allow it to do drop a partition or not, depending on a Predicate on the
       // parameter key values.
       for (DropTableDesc.PartSpec partSpec : dropTbl.getPartSpecs()){
+        List<Partition> partitions = new ArrayList<>();
         try {
-          for (Partition p : Iterables.filter(
-              db.getPartitionsByFilter(tbl, partSpec.getPartSpec().getExprString()),
+          db.getPartitionsByExpr(tbl, partSpec.getPartSpec(), conf, partitions);
+          for (Partition p : Iterables.filter(partitions,
               replicationSpec.allowEventReplacementInto())){
             db.dropPartition(tbl.getDbName(),tbl.getTableName(),p.getValues(),true);
           }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropPartitionHandler.java
Patch:
@@ -92,7 +92,7 @@ private Map<Integer, List<ExprNodeGenericFuncDesc>> genPartSpecs(Table table,
         PrimitiveTypeInfo pti = TypeInfoFactory.getPrimitiveTypeInfo(type);
         ExprNodeColumnDesc column = new ExprNodeColumnDesc(pti, key, null, true);
         ExprNodeGenericFuncDesc op = DDLSemanticAnalyzer.makeBinaryPredicate(
-            "=", column, new ExprNodeConstantDesc(pti, val));
+            "=", column, new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, val));
         expr = (expr == null) ? op : DDLSemanticAnalyzer.makeBinaryPredicate("and", expr, op);
       }
       if (expr != null) {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -299,7 +299,8 @@ private static URL checkConfigFile(File f) {
       HiveConf.ConfVars.METASTORE_TRY_DIRECT_SQL_DDL,
       HiveConf.ConfVars.METASTORE_CLIENT_SOCKET_TIMEOUT,
       HiveConf.ConfVars.METASTORE_PARTITION_NAME_WHITELIST_PATTERN,
-      HiveConf.ConfVars.METASTORE_CAPABILITY_CHECK
+      HiveConf.ConfVars.METASTORE_CAPABILITY_CHECK,
+      HiveConf.ConfVars.METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES
   };
 
   static {

File: standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/conf/MetastoreConf.java
Patch:
@@ -176,7 +176,8 @@ public String toString() {
       ConfVars.TRY_DIRECT_SQL_DDL,
       ConfVars.CLIENT_SOCKET_TIMEOUT,
       ConfVars.PARTITION_NAME_WHITELIST_PATTERN,
-      ConfVars.CAPABILITY_CHECK
+      ConfVars.CAPABILITY_CHECK,
+      ConfVars.DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES
   };
 
   static {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java
Patch:
@@ -71,9 +71,6 @@ public void analyzeInternal(ASTNode tree) throws SemanticException {
     if (useSuper) {
       super.analyzeInternal(tree);
     } else {
-      // TODO: remove when this is enabled everywhere
-      HiveConf.setBoolVar(conf, ConfVars.HIVE_VECTORIZATION_ROW_IDENTIFIER_ENABLED, true);
-
       if (!getTxnMgr().supportsAcid()) {
         throw new SemanticException(ErrorMsg.ACID_OP_ON_NONACID_TXNMGR.getMsg());
       }

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestReaderWriter.java
Patch:
@@ -32,10 +32,8 @@
 import java.util.Map;
 import java.util.Map.Entry;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.ql.CommandNeedRetryException;
-import org.apache.hadoop.mapreduce.InputSplit;
 import org.apache.hive.hcatalog.common.HCatException;
 import org.apache.hive.hcatalog.data.transfer.DataTransferFactory;
 import org.apache.hive.hcatalog.data.transfer.HCatReader;

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatBaseTest.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.util.Shell;
 import org.apache.hive.hcatalog.common.HCatUtil;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
@@ -43,7 +42,7 @@
 /**
  * Simplify writing HCatalog tests that require a HiveMetaStore.
  */
-public class HCatBaseTest {
+public abstract class HCatBaseTest {
   protected static final Logger LOG = LoggerFactory.getLogger(HCatBaseTest.class);
   public static final String TEST_DATA_DIR = HCatUtil.makePathASafeFileName(System.getProperty("user.dir") +
           "/build/test/data/" + HCatBaseTest.class.getCanonicalName() + "-" + System.currentTimeMillis());

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatStorerWrapper.java
Patch:
@@ -27,8 +27,8 @@
 import org.apache.hadoop.hive.ql.CommandNeedRetryException;
 
 import org.apache.hive.hcatalog.HcatTestUtils;
-import org.apache.hive.hcatalog.mapreduce.HCatBaseTest;
 
+import org.apache.hive.hcatalog.mapreduce.HCatBaseTest;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
 

File: jdbc/src/test/org/apache/hive/jdbc/TestHiveStatement.java
Patch:
@@ -23,7 +23,7 @@
 
 import static org.junit.Assert.assertEquals;
 
-public class HiveStatementTest {
+public class TestHiveStatement {
 
   @Test
   public void testSetFetchSize1() throws SQLException {

File: metastore/src/test/org/apache/hadoop/hive/metastore/messaging/json/TestJSONMessageDeserializer.java
Patch:
@@ -26,9 +26,9 @@
 import java.util.HashMap;
 import java.util.Map;
 
-import static org.junit.Assert.*;
+import static org.junit.Assert.assertEquals;
 
-public class JSONMessageDeserializerTest {
+public class TestJSONMessageDeserializer {
 
   public static class MyClass {
     @JsonProperty

File: ql/src/test/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/TestTaskTracker.java
Patch:
@@ -29,7 +29,7 @@
 import static org.junit.Assert.assertTrue;
 
 @RunWith(PowerMockRunner.class)
-  public class TaskTrackerTest {
+  public class TestTaskTracker {
   @Mock
   private Task<? extends Serializable> task;
 

File: ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestInputSplitComparator.java
Patch:
@@ -25,7 +25,7 @@
 import static org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.InputSplitComparator;
 import static org.junit.Assert.assertEquals;
 
-public class InputSplitComparatorTest {
+public class TestInputSplitComparator {
 
   private static final String[] EMPTY = new String[]{};
 

File: ql/src/test/org/apache/hadoop/hive/ql/parse/repl/TestCopyUtils.java
Patch:
@@ -23,7 +23,7 @@
 
 import static org.junit.Assert.assertFalse;
 
-public class CopyUtilsTest {
+public class TestCopyUtils {
   /*
   Distcp currently does not copy a single file in a distributed manner hence we dont care about
   the size of file, if there is only file, we dont want to launch distcp.

File: ql/src/test/org/apache/hadoop/hive/ql/parse/repl/dump/TestHiveWrapper.java
Patch:
@@ -29,7 +29,7 @@
 import org.mockito.runners.MockitoJUnitRunner;
 
 @RunWith(MockitoJUnitRunner.class)
-public class HiveWrapperTest {
+public class TestHiveWrapper {
   @Mock
   private HiveWrapper.Tuple.Function<ReplicationSpec> specFunction;
   @Mock

File: service/src/test/org/apache/hive/service/cli/TestCLIServiceRestore.java
Patch:
@@ -17,13 +17,11 @@
  */
 package org.apache.hive.service.cli;
 
-
 import org.apache.hadoop.hive.conf.HiveConf;
-
 import org.junit.Assert;
 import org.junit.Test;
 
-public class CLIServiceRestoreTest {
+public class TestCLIServiceRestore {
 
   CLIService service = getService();
 

File: storage-api/src/test/org/apache/hadoop/hive/ql/util/TestJavaDataModel.java
Patch:
@@ -24,7 +24,7 @@
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertSame;
 
-public final class JavaDataModelTest {
+public final class TestJavaDataModel {
 
   private static final String DATA_MODEL_PROPERTY = "sun.arch.data.model";
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java
Patch:
@@ -297,7 +297,8 @@ private void initializeSourceForTag(ReduceWork redWork, int tag, ObjectInspector
     boolean vectorizedRecordSource = (tag == bigTablePosition) && redWork.getVectorMode();
     sources[tag].init(jconf, redWork.getReducer(), vectorizedRecordSource, keyTableDesc,
         valueTableDesc, reader, tag == bigTablePosition, (byte) tag,
-        redWork.getVectorizedRowBatchCtx(), redWork.getVectorizedVertexNum());
+        redWork.getVectorizedRowBatchCtx(), redWork.getVectorizedVertexNum(),
+        redWork.getVectorizedTestingReducerBatchSize());
     ois[tag] = sources[tag].getObjectInspector();
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java
Patch:
@@ -42,6 +42,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTableResult;
 import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashMapResult;
 import org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedCreateHashTable;
+import org.apache.hadoop.hive.ql.exec.vector.rowbytescontainer.VectorRowBytesContainer;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.serde2.SerDeException;
@@ -481,7 +482,7 @@ private void spillSerializeRow(VectorizedRowBatch batch, int batchIndex,
     HybridHashTableContainer ht = (HybridHashTableContainer) mapJoinTables[posSingleVectorMapJoinSmallTable];
     HashPartition hp = ht.getHashPartitions()[partitionId];
 
-    VectorMapJoinRowBytesContainer rowBytesContainer = hp.getMatchfileRowBytesContainer();
+    VectorRowBytesContainer rowBytesContainer = hp.getMatchfileRowBytesContainer();
     Output output = rowBytesContainer.getOuputForRowBytes();
 //  int offset = output.getLength();
     bigTableVectorSerializeRow.setOutputAppend(output);
@@ -568,7 +569,7 @@ protected void reProcessBigTable(int partitionId)
     int batchCount = 0;
 
     try {
-      VectorMapJoinRowBytesContainer bigTable = partition.getMatchfileRowBytesContainer();
+      VectorRowBytesContainer bigTable = partition.getMatchfileRowBytesContainer();
       bigTable.prepareForReading();
 
       while (bigTable.readNext()) {

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java
Patch:
@@ -78,7 +78,7 @@ public BytesColumnVector() {
    * @param size  number of elements in the column vector
    */
   public BytesColumnVector(int size) {
-    super(size);
+    super(Type.BYTES, size);
     vector = new byte[size][];
     start = new int[size];
     length = new int[size];

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java
Patch:
@@ -40,7 +40,7 @@ public DecimalColumnVector(int precision, int scale) {
   }
 
   public DecimalColumnVector(int size, int precision, int scale) {
-    super(size);
+    super(Type.DECIMAL, size);
     this.precision = (short) precision;
     this.scale = (short) scale;
     vector = new HiveDecimalWritable[size];

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DoubleColumnVector.java
Patch:
@@ -48,7 +48,7 @@ public DoubleColumnVector() {
    * @param len
    */
   public DoubleColumnVector(int len) {
-    super(len);
+    super(Type.DOUBLE, len);
     vector = new double[len];
   }
 

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/IntervalDayTimeColumnVector.java
Patch:
@@ -69,7 +69,7 @@ public IntervalDayTimeColumnVector() {
    * @param len the number of rows
    */
   public IntervalDayTimeColumnVector(int len) {
-    super(len);
+    super(Type.INTERVAL_DAY_TIME, len);
 
     totalSeconds = new long[len];
     nanos = new int[len];

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/ListColumnVector.java
Patch:
@@ -40,7 +40,7 @@ public ListColumnVector() {
    * @param child The child vector
    */
   public ListColumnVector(int len, ColumnVector child) {
-    super(len);
+    super(Type.LIST, len);
     this.child = child;
   }
 

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/LongColumnVector.java
Patch:
@@ -48,7 +48,7 @@ public LongColumnVector() {
    * @param len the number of rows
    */
   public LongColumnVector(int len) {
-    super(len);
+    super(Type.LONG, len);
     vector = new long[len];
   }
 

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/MapColumnVector.java
Patch:
@@ -42,7 +42,7 @@ public MapColumnVector() {
    * @param values The values column vector
    */
   public MapColumnVector(int len, ColumnVector keys, ColumnVector values) {
-    super(len);
+    super(Type.MAP, len);
     this.keys = keys;
     this.values = values;
   }

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/MultiValuedColumnVector.java
Patch:
@@ -40,8 +40,8 @@ public abstract class MultiValuedColumnVector extends ColumnVector {
    *
    * @param len Vector length
    */
-  public MultiValuedColumnVector(int len) {
-    super(len);
+  public MultiValuedColumnVector(Type type, int len) {
+    super(type, len);
     childCount = 0;
     offsets = new long[len];
     lengths = new long[len];

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/StructColumnVector.java
Patch:
@@ -40,7 +40,7 @@ public StructColumnVector() {
    * @param fields the field column vectors
    */
   public StructColumnVector(int len, ColumnVector... fields) {
-    super(len);
+    super(Type.STRUCT, len);
     this.fields = fields;
   }
 

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java
Patch:
@@ -67,7 +67,7 @@ public TimestampColumnVector() {
    * @param len the number of rows
    */
   public TimestampColumnVector(int len) {
-    super(len);
+    super(Type.TIMESTAMP, len);
 
     time = new long[len];
     nanos = new int[len];

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/UnionColumnVector.java
Patch:
@@ -41,7 +41,7 @@ public UnionColumnVector() {
    * @param fields the field column vectors
    */
   public UnionColumnVector(int len, ColumnVector... fields) {
-    super(len);
+    super(Type.UNION, len);
     tags = new int[len];
     this.fields = fields;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/SelectStringColLikeStringScalar.java
Patch:
@@ -109,7 +109,7 @@ public void evaluate(VectorizedRowBatch batch) {
             outputVector[i] = (checker.check(vector[i], start[i], length[i]) ? 1 : 0);
             outV.isNull[i] = false;
           } else {
-            outputVector[0] = LongColumnVector.NULL_VALUE;
+            outputVector[i] = LongColumnVector.NULL_VALUE;
             outV.isNull[i] = true;
           }
         }
@@ -119,7 +119,7 @@ public void evaluate(VectorizedRowBatch batch) {
             outputVector[i] = (checker.check(vector[i], start[i], length[i]) ? 1 : 0);
             outV.isNull[i] = false;
           } else {
-            outputVector[0] = LongColumnVector.NULL_VALUE;
+            outputVector[i] = LongColumnVector.NULL_VALUE;
             outV.isNull[i] = true;
           }
         }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToByte.java
Patch:
@@ -172,7 +172,7 @@ public ByteWritable evaluate(Text i) {
           return null;
         }
       try {
-        byteWritable.set(LazyByte.parseByte(i.getBytes(), 0, i.getLength(), 10));
+        byteWritable.set(LazyByte.parseByte(i.getBytes(), 0, i.getLength(), 10, true));
         return byteWritable;
       } catch (NumberFormatException e) {
         // MySQL returns 0 if the string is not a well-formed numeric value.

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToInteger.java
Patch:
@@ -175,7 +175,7 @@ public IntWritable evaluate(Text i) {
       }
       try {
         intWritable.set(LazyInteger
-            .parseInt(i.getBytes(), 0, i.getLength(), 10));
+            .parseInt(i.getBytes(), 0, i.getLength(), 10, true));
         return intWritable;
       } catch (NumberFormatException e) {
         // MySQL returns 0 if the string is not a well-formed numeric value.

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToLong.java
Patch:
@@ -184,7 +184,7 @@ public LongWritable evaluate(Text i) {
       }
       try {
         longWritable
-            .set(LazyLong.parseLong(i.getBytes(), 0, i.getLength(), 10));
+            .set(LazyLong.parseLong(i.getBytes(), 0, i.getLength(), 10, true));
         return longWritable;
       } catch (NumberFormatException e) {
         // MySQL returns 0 if the string is not a well-formed numeric value.

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToShort.java
Patch:
@@ -174,7 +174,7 @@ public ShortWritable evaluate(Text i) {
       }
       try {
         shortWritable.set(LazyShort.parseShort(i.getBytes(), 0, i.getLength(),
-            10));
+            10, true));
         return shortWritable;
       } catch (NumberFormatException e) {
         // MySQL returns 0 if the string is not a well-formed numeric value.

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/TableExport.java
Patch:
@@ -297,7 +297,6 @@ public AuthEntities getAuthEntities() throws SemanticException {
             throw new IllegalStateException("partitions cannot be null for partitionTable :"
                 + tableSpec.tableName);
           }
-          new PartitionExport(paths, partitions, distCpDoAsUser, conf).write(replicationSpec);
           for (Partition partition : partitions) {
             authEntities.inputs.add(new ReadEntity(partition));
           }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java
Patch:
@@ -1112,8 +1112,8 @@ private boolean checkShuffleSizeForLargeTable(JoinOperator joinOp, int position,
     long max = HiveConf.getLongVar(context.parseContext.getConf(),
             HiveConf.ConfVars.HIVECONVERTJOINMAXSHUFFLESIZE);
     if (max < 1) {
-      // Max is disabled, we can safely return true
-      return true;
+      // Max is disabled, we can safely return false
+      return false;
     }
     // Evaluate
     ReduceSinkOperator rsOp = (ReduceSinkOperator) joinOp.getParentOperators().get(position);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -786,7 +786,7 @@ public static enum ConfVars {
     NOTIFICATION_SEQUENCE_LOCK_MAX_RETRIES("hive.notification.sequence.lock.max.retries", 5,
         "Number of retries required to acquire a lock when getting the next notification sequential ID for entries "
             + "in the NOTIFICATION_LOG table."),
-    NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL("hive.notification.sequence.lock.retry.sleep.interval", 500,
+    NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL("hive.notification.sequence.lock.retry.sleep.interval", 500L,
         new TimeValidator(TimeUnit.MILLISECONDS),
         "Sleep interval between retries to acquire a notification lock as described part of property "
             + NOTIFICATION_SEQUENCE_LOCK_MAX_RETRIES.name()),

File: metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -8588,6 +8588,9 @@ public void run() throws MetaException {
         }
       }
     }
+    public long getSleepInterval() {
+      return sleepInterval;
+    }
   }
 
   @Override

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CoreBeeLineDriver.java
Patch:
@@ -49,7 +49,7 @@ public class CoreBeeLineDriver extends CliAdapter {
   private final File testDataDirectory;
   private final File testScriptDirectory;
   private boolean overwrite = false;
-  private boolean rewriteSourceTables = true;
+  private boolean useSharedDatabase = false;
   private MiniHS2 miniHS2;
   private QFileClientBuilder clientBuilder;
   private QFileBuilder fileBuilder;
@@ -111,7 +111,7 @@ boolean getBooleanPropertyValue(String name, boolean defaultValue) {
   public void beforeClass() throws Exception {
     overwrite = getBooleanPropertyValue("test.output.overwrite", Boolean.FALSE);
 
-    rewriteSourceTables = getBooleanPropertyValue("test.rewrite.source.tables", Boolean.TRUE);
+    useSharedDatabase = getBooleanPropertyValue("test.beeline.shared.database", Boolean.FALSE);
 
     String beeLineUrl = System.getProperty("test.beeline.url");
     if (StringUtils.isEmpty(beeLineUrl)) {
@@ -132,7 +132,7 @@ public void beforeClass() throws Exception {
         .setLogDirectory(logDirectory)
         .setQueryDirectory(queryDirectory)
         .setResultsDirectory(resultsDirectory)
-        .setRewriteSourceTables(rewriteSourceTables)
+        .setUseSharedDatabase(useSharedDatabase)
         .setComparePortable(comparePortable);
 
     runInfraScript(initScript, new File(logDirectory, "init.beeline"),

File: ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java
Patch:
@@ -1065,8 +1065,7 @@ else if(colTypeLowerCase.equals(serdeConstants.SMALLINT_TYPE_NAME)){
       // epoch, days since epoch
       cs.setRange(0, 25201);
     } else {
-      // Columns statistics for complex datatypes are not supported yet
-      return null;
+      cs.setAvgColLen(getSizeOfComplexTypes(conf, cinfo.getObjectInspector()));
     }
     return cs;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java
Patch:
@@ -61,7 +61,6 @@ protected int execute(DriverContext driverContext) {
     FileSystem dstFs = null;
     Path toPath = null;
     try {
-      // TODO: merge with real CopyTask logic?
       Path fromPath = work.getFromPaths()[0];
       toPath = work.getToPaths()[0];
 

File: hplsql/src/main/java/org/apache/hive/hplsql/Copy.java
Patch:
@@ -268,7 +268,9 @@ else if (sqlInsert) {
     }
     long elapsed = timer.stop();
     if (info) {
-      info(ctx, "COPY completed: " + rows + " row(s), " + Utils.formatSizeInBytes(bytes) + ", " + timer.format() + ", " + rows/elapsed/1000 + " rows/sec");
+      DecimalFormat df = new DecimalFormat("#,##0.00");
+      df.setRoundingMode(RoundingMode.HALF_UP);
+      info(ctx, "COPY completed: " + rows + " row(s), " + Utils.formatSizeInBytes(bytes) + ", " + timer.format() + ", " + df.format(rows/(elapsed/1000.0)) + " rows/sec");
     }
   }
   

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4500,7 +4500,7 @@ private static String[] convertVarsToRegex(String[] paramList) {
     "mapred\\.map\\..*",
     "mapred\\.reduce\\..*",
     "mapred\\.output\\.compression\\.codec",
-    "mapred\\.job\\.queuename",
+    "mapred\\.job\\.queue\\.name",
     "mapred\\.output\\.compression\\.type",
     "mapred\\.min\\.split\\.size",
     "mapreduce\\.job\\.reduce\\.slowstart\\.completedmaps",

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -4198,7 +4198,7 @@ private void dropPartitions(Hive db, Table tbl, DropTableDesc dropTbl) throws Hi
 
   private void dropTable(Hive db, Table tbl, DropTableDesc dropTbl) throws HiveException {
     // This is a true DROP TABLE
-    if (tbl != null && dropTbl.getExpectedType() != null) {
+    if (tbl != null && dropTbl.getValidationRequired()) {
       if (tbl.isView()) {
         if (!dropTbl.getExpectView()) {
           if (dropTbl.getIfExists()) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/DropTableHandler.java
Patch:
@@ -37,7 +37,7 @@ public List<Task<? extends Serializable>> handle(Context context)
     String actualTblName = context.isTableNameEmpty() ? msg.getTable() : context.tableName;
     DropTableDesc dropTableDesc = new DropTableDesc(
         actualDbName + "." + actualTblName,
-        null, true, true, context.eventOnlyReplicationSpec()
+        null, true, true, context.eventOnlyReplicationSpec(), false
     );
     Task<DDLWork> dropTableTask = TaskFactory.get(
         new DDLWork(readEntitySet, writeEntitySet, dropTableDesc),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java
Patch:
@@ -128,10 +128,10 @@ public static <T> Set<T> findOperatorsUpstreamJoinAccounted(Operator<?> start, C
       for (Operator<?> parent : start.getParentOperators()) {
         if (onlyIncludeIndex >= 0) {
           if (onlyIncludeIndex == i) {
-            findOperatorsUpstream(parent, clazz, found);
+            findOperatorsUpstreamJoinAccounted(parent, clazz, found);
           }
         } else {
-          findOperatorsUpstream(parent, clazz, found);
+          findOperatorsUpstreamJoinAccounted(parent, clazz, found);
         }
         i++;
       }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java
Patch:
@@ -1251,8 +1251,8 @@ private Object convertUnionRowColumn(
 
     final Object union = unionOI.create();
     final int tag = deserializeRead.currentInt;
-    unionOI.addField(union, new StandardUnion((byte) tag,
-        convertComplexFieldRowColumn(unionColumnVector.fields[tag], batchIndex, fields[tag])));
+    unionOI.setFieldAndTag(union, new StandardUnion((byte) tag,
+        convertComplexFieldRowColumn(unionColumnVector.fields[tag], batchIndex, fields[tag])), (byte) tag);
     deserializeRead.finishComplexVariableFieldsType();
     return union;
   }

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
Patch:
@@ -471,7 +471,7 @@ public Object convert(Object input) {
       }
 
       Object inputFieldValue = inputOI.getField(input);
-      Object inputFieldTag = inputOI.getTag(input);
+      byte inputFieldTag = inputOI.getTag(input);
       Object outputFieldValue = null;
 
       int inputFieldTagIndex = ((Byte)inputFieldTag).intValue();
@@ -480,7 +480,7 @@ public Object convert(Object input) {
          outputFieldValue = fieldConverters.get(inputFieldTagIndex).convert(inputFieldValue);
       }
 
-      outputOI.addField(output, outputFieldValue);
+      outputOI.setFieldAndTag(output, outputFieldValue, inputFieldTag);
 
       return output;
     }

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/SettableUnionObjectInspector.java
Patch:
@@ -26,9 +26,9 @@
 public abstract class SettableUnionObjectInspector implements
     UnionObjectInspector {
 
-  /* Create an empty object */
+  /* Creates an empty union object. */
   public abstract Object create();
 
-  /* Add field to the object */
-  public abstract Object addField(Object union, Object field);
+  /* Sets the field and tag in the union. Returns the union. */
+  public abstract Object setFieldAndTag(Object union, Object field, byte tag);
 }

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
Patch:
@@ -1977,7 +1977,6 @@ public static List<String> getColumnNames(List<FieldSchema> schema) {
    * @param params table properties
    * @return true if table is an INSERT_ONLY table, false otherwise
    */
-  // TODO# also check that transactional is true
   public static boolean isInsertOnlyTable(Map<String, String> params) {
     return isInsertOnlyTable(params, false);
   }
@@ -1998,7 +1997,7 @@ public static boolean isInsertOnlyTable(Properties params) {
 
    /** The method for altering table props; may set the table to MM, non-MM, or not affect MM. */
   public static Boolean isToInsertOnlyTable(Map<String, String> props) {
-    // TODO# Setting these separately is a very hairy issue in certain combinations, since we
+    // TODO: Setting these separately is a very hairy issue in certain combinations, since we
     //       cannot decide what type of table this becomes without taking both into account, and
     //       in many cases the conversion might be illegal.
     //       The only thing we allow is tx = true w/o tx-props, for backward compat.

File: metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java
Patch:
@@ -19,6 +19,7 @@ Licensed to the Apache Software Foundation (ASF) under one
 
 
 import com.codahale.metrics.Counter;
+import com.google.common.base.Supplier;
 import com.google.common.collect.ImmutableList;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.CurrentNotificationEventId;
@@ -57,8 +58,6 @@ Licensed to the Apache Software Foundation (ASF) under one
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import com.google.common.base.Supplier;
-import com.google.common.collect.ImmutableList;
 import javax.jdo.Query;
 import java.util.Arrays;
 import java.util.HashMap;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
Patch:
@@ -301,9 +301,10 @@ protected void flushToFile() throws IOException, HiveException {
       String bigBucketFileName = getExecContext().getCurrentBigBucketFile();
       String fileName = getExecContext().getLocalWork().getBucketFileName(bigBucketFileName);
       // get the tmp URI path; it will be a hdfs path if not local mode
-      // TODO# this doesn't work... the path for writer and reader mismatch
+      // TODO: this doesn't work... the path for writer and reader mismatch
       //      Dump the side-table for tag ... -local-10004/HashTable-Stage-1/MapJoin-a-00-(ds%3D2008-04-08)mm_2.hashtable
       //      Load back 1 hashtable file      -local-10004/HashTable-Stage-1/MapJoin-a-00-srcsortbucket3outof4.txt.hashtable
+      //      Hive3 probably won't support MR so do we really care? 
       String dumpFilePrefix = conf.getDumpFilePrefix();
       Path path = Utilities.generatePath(tmpURI, dumpFilePrefix, tag, fileName);
       console.printInfo(Utilities.now() + "\tDump the side-table for tag: " + tag +

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -268,7 +268,7 @@ public TaskInformation(Task task, String path) {
 
   @Override
   public int execute(DriverContext driverContext) {
-    if (work.isNoop()) return 0; // TODO# temporary flag for HIVE-14990
+    if (work.isNoop()) return 0;
     Utilities.LOG14535.info("Executing MoveWork " + System.identityHashCode(work)
         + " with " + work.getLoadFileWork() + "; " + work.getLoadTableWork() + "; "
         + work.getLoadMultiFilesWork());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ReplCopyTask.java
Patch:
@@ -61,7 +61,7 @@ protected int execute(DriverContext driverContext) {
     FileSystem dstFs = null;
     Path toPath = null;
     try {
-      // TODO# merge with real CopyTask logic
+      // TODO: merge with real CopyTask logic?
       Path fromPath = work.getFromPaths()[0];
       toPath = work.getToPaths()[0];
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -1911,7 +1911,7 @@ private void setStatsPropAndAlterPartition(boolean hasFollowingStatsTask, Table
 private void walkDirTree(FileStatus fSta, FileSystem fSys,
     Map<List<String>, String> skewedColValueLocationMaps, Path newPartPath, SkewedInfo skewedInfo)
     throws IOException {
-  // TODO# HERE broken
+  // TODO: may be broken? no LB bugs for now but if any are found.
 
   /* Base Case. It's leaf. */
   if (!fSta.isDir()) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketingSortingReduceSinkOptimizer.java
Patch:
@@ -237,7 +237,6 @@ private void storeBucketPathMapping(TableScanOperator tsOp, FileStatus[] srcs) {
       Map<String, Integer> bucketFileNameMapping = new HashMap<String, Integer>();
       for (int pos = 0; pos < srcs.length; pos++) {
         if (ShimLoader.getHadoopShims().isDirectory(srcs[pos])) {
-          // TODO# HERE
           throw new RuntimeException("Was expecting '" + srcs[pos].getPath() + "' to be bucket file.");
         }
         bucketFileNameMapping.put(srcs[pos].getPath().getName(), pos);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/BucketMapJoinContext.java
Patch:
@@ -39,7 +39,6 @@ public class BucketMapJoinContext implements Serializable {
 
   private static final long serialVersionUID = 1L;
 
-  // TODO# this is completely broken, esp. w/load into bucketed tables (should perhaps be forbidden for MM tables)
   // table alias (small) --> input file name (big) --> target file names (small)
   private Map<String, Map<String, List<String>>> aliasBucketFileNameMapping;
   private String mapJoinBigTableAlias;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/MoveWork.java
Patch:
@@ -153,7 +153,6 @@ public void setSrcLocal(boolean srcLocal) {
     this.srcLocal = srcLocal;
   }
 
-  // TODO# temporary test flag
   public void setNoop(boolean b) {
     this.isNoop = true;
   }

File: metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java
Patch:
@@ -26,13 +26,13 @@
 import org.antlr.runtime.ANTLRStringStream;
 import org.antlr.runtime.CharStream;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.metastore.ColumnType;
 import org.apache.hadoop.hive.metastore.HiveMetaStore;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.Warehouse;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
-import org.apache.hadoop.hive.serde.serdeConstants;
 
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.Sets;
@@ -432,8 +432,8 @@ private String getJdoFilterPushdownParam(Table table, int partColIndex,
       boolean isIntegralSupported = canPushDownIntegral && canJdoUseStringsWithIntegral();
       String colType = table.getPartitionKeys().get(partColIndex).getType();
       // Can only support partitions whose types are string, or maybe integers
-      if (!colType.equals(serdeConstants.STRING_TYPE_NAME)
-          && (!isIntegralSupported || !serdeConstants.IntegralTypes.contains(colType))) {
+      if (!colType.equals(ColumnType.STRING_TYPE_NAME)
+          && (!isIntegralSupported || !ColumnType.IntegralTypes.contains(colType))) {
         filterBuilder.setError("Filtering is supported only on partition keys of type " +
             "string" + (isIntegralSupported ? ", or integral types" : ""));
         return null;

File: metastore/src/test/org/apache/hadoop/hive/metastore/MockPartitionExpressionForMetastore.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.metastore;
 
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.FileMetadataExprType;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
@@ -35,8 +36,8 @@ public String convertExprToFilter(byte[] expr) throws MetaException {
   }
 
   @Override
-  public boolean filterPartitionsByExpr(List<String> partColumnNames,
-      List<PrimitiveTypeInfo> partColumnTypeInfos, byte[] expr, String defaultPartitionName,
+  public boolean filterPartitionsByExpr(List<FieldSchema> partColumns,
+      byte[] expr, String defaultPartitionName,
       List<String> partitionNames) throws MetaException {
     return false;
   }

File: metastore/src/test/org/apache/hadoop/hive/metastore/TestOldSchema.java
Patch:
@@ -62,9 +62,9 @@ public String convertExprToFilter(byte[] expr) throws MetaException {
     }
 
     @Override
-    public boolean filterPartitionsByExpr(List<String> partColumnNames,
-        List<PrimitiveTypeInfo> partColumnTypeInfos, byte[] expr, String defaultPartitionName,
-        List<String> partitionNames) throws MetaException {
+    public boolean filterPartitionsByExpr(List<FieldSchema> partColumns, byte[] expr,
+                                          String defaultPartitionName,
+                                          List<String> partitionNames) throws MetaException {
       return false;
     }
 

File: shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
Patch:
@@ -1378,8 +1378,8 @@ public void addDelegationTokens(FileSystem fs, Credentials cred, String uname) t
   }
 
   @Override
-  public long getFileId(DistributedFileSystem fs, String path) throws IOException {
-    return fs.getClient().getFileInfo(path).getFileId();
+  public long getFileId(FileSystem fs, String path) throws IOException {
+    return ensureDfs(fs).getClient().getFileInfo(path).getFileId();
   }
 
 

File: shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
Patch:
@@ -17,8 +17,6 @@
  */
 package org.apache.hadoop.hive.shims;
 
-import org.apache.hadoop.hdfs.DistributedFileSystem;
-
 import java.io.IOException;
 import java.net.InetSocketAddress;
 import java.net.MalformedURLException;
@@ -659,7 +657,7 @@ public List<String> getKeys() throws IOException{
    * Gets file ID. Only supported on hadoop-2.
    * @return inode ID of the file.
    */
-  long getFileId(DistributedFileSystem fs, String path) throws IOException;
+  long getFileId(FileSystem fs, String path) throws IOException;
 
   /** Clones the UGI and the Subject. */
   UserGroupInformation cloneUgi(UserGroupInformation baseUgi) throws IOException;

File: service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
Patch:
@@ -257,11 +257,12 @@ private void runQuery() throws HiveSQLException {
       /**
        * If the operation was cancelled by another thread, or the execution timed out, Driver#run
        * may return a non-zero response code. We will simply return if the operation state is
-       * CANCELED, TIMEDOUT or CLOSED, otherwise throw an exception
+       * CANCELED, TIMEDOUT, CLOSED or FINISHED, otherwise throw an exception
        */
       if ((getStatus().getState() == OperationState.CANCELED)
           || (getStatus().getState() == OperationState.TIMEDOUT)
-          || (getStatus().getState() == OperationState.CLOSED)) {
+          || (getStatus().getState() == OperationState.CLOSED)
+          || (getStatus().getState() == OperationState.FINISHED)) {
         LOG.warn("Ignore exception in terminal state", e);
         return;
       }

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java
Patch:
@@ -208,7 +208,7 @@ private List<Long> determineFileIdsToQuery(
   }
 
   private Long generateTestFileId(final FileStatus fs, List<HdfsFileStatusWithId> files, int i) {
-    final Long fileId = HdfsUtils.createFileId(fs.getPath().toUri().getPath(), fs, false, null);
+    final Long fileId = HdfsUtils.createTestFileId(fs.getPath().toUri().getPath(), fs, false, null);
     files.set(i, new HdfsFileStatusWithId() {
       @Override
       public FileStatus getFileStatus() {

File: shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
Patch:
@@ -1378,8 +1378,8 @@ public void addDelegationTokens(FileSystem fs, Credentials cred, String uname) t
   }
 
   @Override
-  public long getFileId(FileSystem fs, String path) throws IOException {
-    return ensureDfs(fs).getClient().getFileInfo(path).getFileId();
+  public long getFileId(DistributedFileSystem fs, String path) throws IOException {
+    return fs.getClient().getFileInfo(path).getFileId();
   }
 
 

File: shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
Patch:
@@ -17,6 +17,8 @@
  */
 package org.apache.hadoop.hive.shims;
 
+import org.apache.hadoop.hdfs.DistributedFileSystem;
+
 import java.io.IOException;
 import java.net.InetSocketAddress;
 import java.net.MalformedURLException;
@@ -657,7 +659,7 @@ public List<String> getKeys() throws IOException{
    * Gets file ID. Only supported on hadoop-2.
    * @return inode ID of the file.
    */
-  long getFileId(FileSystem fs, String path) throws IOException;
+  long getFileId(DistributedFileSystem fs, String path) throws IOException;
 
   /** Clones the UGI and the Subject. */
   UserGroupInformation cloneUgi(UserGroupInformation baseUgi) throws IOException;

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java
Patch:
@@ -29,11 +29,11 @@
 import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler;
-import org.apache.hadoop.hive.metastore.MetaStoreEventListener;
 import org.apache.hadoop.hive.metastore.MetaStoreEventListenerConstants;
 import org.apache.hadoop.hive.metastore.RawStore;
 import org.apache.hadoop.hive.metastore.RawStoreProxy;
 import org.apache.hadoop.hive.metastore.ReplChangeManager;
+import org.apache.hadoop.hive.metastore.TransactionalMetaStoreEventListener;
 import org.apache.hadoop.hive.metastore.Warehouse;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.metastore.api.Function;
@@ -87,7 +87,7 @@
  * the database.  Also, occasionally the thread needs to clean the database of old records.  We
  * definitely don't want to do that as part of another metadata operation.
  */
-public class DbNotificationListener extends MetaStoreEventListener {
+public class DbNotificationListener extends TransactionalMetaStoreEventListener {
 
   private static final Logger LOG = LoggerFactory.getLogger(DbNotificationListener.class.getName());
   private static CleanerThread cleaner = null;

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
Patch:
@@ -121,7 +121,7 @@ public void alterTable(RawStore msdb, Warehouse wh, String dbname,
     boolean success = false;
     boolean dataWasMoved = false;
     Table oldt = null;
-    List<MetaStoreEventListener> transactionalListeners = null;
+    List<TransactionalMetaStoreEventListener> transactionalListeners = null;
     if (handler != null) {
       transactionalListeners = handler.getTransactionalListeners();
     }
@@ -377,7 +377,7 @@ public Partition alterPartition(final RawStore msdb, Warehouse wh, final String
       throws InvalidOperationException, InvalidObjectException, AlreadyExistsException, MetaException {
     boolean success = false;
     Partition oldPart = null;
-    List<MetaStoreEventListener> transactionalListeners = null;
+    List<TransactionalMetaStoreEventListener> transactionalListeners = null;
     if (handler != null) {
       transactionalListeners = handler.getTransactionalListeners();
     }
@@ -599,7 +599,7 @@ public List<Partition> alterPartitions(final RawStore msdb, Warehouse wh, final
       throws InvalidOperationException, InvalidObjectException, AlreadyExistsException, MetaException {
     List<Partition> oldParts = new ArrayList<Partition>();
     List<List<String>> partValsList = new ArrayList<List<String>>();
-    List<MetaStoreEventListener> transactionalListeners = null;
+    List<TransactionalMetaStoreEventListener> transactionalListeners = null;
     if (handler != null) {
       transactionalListeners = handler.getTransactionalListeners();
     }

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -447,7 +447,7 @@ public HiveConf getHiveConf() {
     private AlterHandler alterHandler;
     private List<MetaStorePreEventListener> preListeners;
     private List<MetaStoreEventListener> listeners;
-    private List<MetaStoreEventListener> transactionalListeners;
+    private List<TransactionalMetaStoreEventListener> transactionalListeners;
     private List<MetaStoreEndFunctionListener> endFunctionListeners;
     private List<MetaStoreInitListener> initListeners;
     private Pattern partitionValidationPattern;
@@ -460,7 +460,7 @@ public HiveConf getHiveConf() {
       }
     }
 
-    List<MetaStoreEventListener> getTransactionalListeners() {
+    List<TransactionalMetaStoreEventListener> getTransactionalListeners() {
       return transactionalListeners;
     }
 
@@ -534,7 +534,7 @@ public Object getValue() {
           hiveConf.getVar(HiveConf.ConfVars.METASTORE_EVENT_LISTENERS));
       listeners.add(new SessionPropertiesListener(hiveConf));
       listeners.add(new AcidEventListener(hiveConf));
-      transactionalListeners = MetaStoreUtils.getMetaStoreListeners(MetaStoreEventListener.class,hiveConf,
+      transactionalListeners = MetaStoreUtils.getMetaStoreListeners(TransactionalMetaStoreEventListener.class,hiveConf,
               hiveConf.getVar(ConfVars.METASTORE_TRANSACTIONAL_EVENT_LISTENERS));
       if (metrics != null) {
         listeners.add(new HMSMetricsListener(hiveConf, metrics));

File: metastore/src/java/org/apache/hadoop/hive/metastore/TransactionalValidationListener.java
Patch:
@@ -39,7 +39,6 @@ public final class TransactionalValidationListener extends MetaStorePreEventList
 
   // These constants are also imported by org.apache.hadoop.hive.ql.io.AcidUtils.
   public static final String DEFAULT_TRANSACTIONAL_PROPERTY = "default";
-  public static final String LEGACY_TRANSACTIONAL_PROPERTY = "legacy";
 
   TransactionalValidationListener(Configuration conf) {
     super(conf);
@@ -276,7 +275,6 @@ private String validateTransactionalProperties(String transactionalProperties) {
     boolean isValid = false;
     switch (transactionalProperties) {
       case DEFAULT_TRANSACTIONAL_PROPERTY:
-      case LEGACY_TRANSACTIONAL_PROPERTY:
         isValid = true;
         break;
       default:

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -148,8 +148,6 @@ public class FSPaths implements Cloneable {
     RecordWriter[] outWriters;
     RecordUpdater[] updaters;
     Stat stat;
-    int acidLastBucket = -1;
-    int acidFileOffset = -1;
 
     public FSPaths(Path specPath) {
       tmpPath = Utilities.toTempPath(specPath);

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java
Patch:
@@ -296,7 +296,7 @@ public RecordUpdater getRecordUpdater(Path path,
           .rowIndexStride(0);
     }
     final OrcRecordUpdater.KeyIndexBuilder watcher =
-        new OrcRecordUpdater.KeyIndexBuilder();
+        new OrcRecordUpdater.KeyIndexBuilder("compactor");
     opts.inspector(options.getInspector())
         .callback(watcher);
     final Writer writer = OrcFile.createWriter(filename, opts);

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2WithSplitUpdateAndVectorization.java
Patch:
@@ -23,11 +23,11 @@
 import org.junit.Test;
 
 /**
- * Same as TestTxnCommands2WithSplitUpdate but tests ACID tables with vectorization turned on by
+ * Same as TestTxnCommands2 but tests ACID tables with vectorization turned on by
  * default, and having 'transactional_properties' set to 'default'. This specifically tests the
  * fast VectorizedOrcAcidRowBatchReader for ACID tables with split-update turned on.
  */
-public class TestTxnCommands2WithSplitUpdateAndVectorization extends TestTxnCommands2WithSplitUpdate {
+public class TestTxnCommands2WithSplitUpdateAndVectorization extends TestTxnCommands2 {
 
   public TestTxnCommands2WithSplitUpdateAndVectorization() {
     super();

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -2031,7 +2031,7 @@ private boolean drop_table_core(final RawStore ms, final String dbname, final St
             transactionalListenerResponses =
                 MetaStoreListenerNotifier.notifyEvent(transactionalListeners,
                                                       EventType.DROP_TABLE,
-                                                      new DropTableEvent(tbl, deleteData, true, this),
+                                                      new DropTableEvent(tbl, true, deleteData, this),
                                                       envContext);
           }
           success = ms.commitTransaction();
@@ -2051,7 +2051,7 @@ private boolean drop_table_core(final RawStore ms, final String dbname, final St
         if (!listeners.isEmpty()) {
           MetaStoreListenerNotifier.notifyEvent(listeners,
                                                 EventType.DROP_TABLE,
-                                                new DropTableEvent(tbl, deleteData, success, this),
+                                                new DropTableEvent(tbl, success, deleteData, this),
                                                 envContext,
                                                 transactionalListenerResponses, ms);
         }

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
Patch:
@@ -181,7 +181,6 @@ private static MapWork findMapWork(JobConf job) throws HiveException {
 
     BaseWork work = null;
     // HIVE-16985: try to find the fake merge work for SMB join, that is really another MapWork.
-    /*
     if (inputName != null) {
       if (prefixes == null ||
           !Lists.newArrayList(prefixes.split(",")).contains(inputName)) {
@@ -191,7 +190,7 @@ private static MapWork findMapWork(JobConf job) throws HiveException {
     if (inputName != null) {
       work = Utilities.getMergeWork(job, inputName);
     }
-    */
+
     if (work == null || !(work instanceof MapWork)) {
       work = Utilities.getMapWork(job);
     }

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/AddPartitionMessage.java
Patch:
@@ -37,6 +37,8 @@ protected AddPartitionMessage() {
    */
   public abstract String getTable();
 
+  public abstract String getTableType();
+
   /**
    * Getter for list of partitions added.
    * @return List of maps, where each map identifies values for each partition-key, for every added partition.

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/AlterPartitionMessage.java
Patch:
@@ -31,6 +31,8 @@ protected AlterPartitionMessage() {
 
   public abstract String getTable();
 
+  public abstract String getTableType();
+
   public abstract Map<String,String> getKeyValues();
 
   @Override

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/AlterTableMessage.java
Patch:
@@ -28,6 +28,7 @@ protected AlterTableMessage() {
   }
 
   public abstract String getTable();
+  public abstract String getTableType();
 
   @Override
   public HCatEventMessage checkValid() {

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/CreateTableMessage.java
Patch:
@@ -34,6 +34,8 @@ protected CreateTableMessage() {
    */
   public abstract String getTable();
 
+  public abstract String getTableType();
+
   @Override
   public HCatEventMessage checkValid() {
     if (getTable() == null)

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/DropPartitionMessage.java
Patch:
@@ -32,6 +32,7 @@ protected DropPartitionMessage() {
   }
 
   public abstract String getTable();
+  public abstract String getTableType();
   public abstract List<Map<String, String>> getPartitions ();
 
   @Override

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/DropTableMessage.java
Patch:
@@ -33,6 +33,7 @@ protected DropTableMessage() {
    * @return Table-name (String).
    */
   public abstract String getTable();
+  public abstract String getTableType();
 
   @Override
   public HCatEventMessage checkValid() {

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/InsertMessage.java
Patch:
@@ -36,6 +36,7 @@ protected InsertMessage() {
    * @return Table-name (String).
    */
   public abstract String getTable();
+  public abstract String getTableType();
 
   /**
    * Get the map of partition keyvalues.  Will be null if this insert is to a table and not a

File: metastore/src/java/org/apache/hadoop/hive/metastore/messaging/AddPartitionMessage.java
Patch:
@@ -37,6 +37,8 @@ protected AddPartitionMessage() {
    */
   public abstract String getTable();
 
+  public abstract String getTableType();
+
   public abstract Table getTableObj() throws Exception;
 
   /**

File: metastore/src/java/org/apache/hadoop/hive/metastore/messaging/AlterPartitionMessage.java
Patch:
@@ -31,6 +31,8 @@ protected AlterPartitionMessage() {
 
   public abstract String getTable();
 
+  public abstract String getTableType();
+
   public abstract boolean getIsTruncateOp();
 
   public abstract Map<String,String> getKeyValues();

File: metastore/src/java/org/apache/hadoop/hive/metastore/messaging/AlterTableMessage.java
Patch:
@@ -28,6 +28,8 @@ protected AlterTableMessage() {
 
   public abstract String getTable();
 
+  public abstract String getTableType();
+
   public abstract boolean getIsTruncateOp();
 
   public abstract Table getTableObjBefore() throws Exception;

File: metastore/src/java/org/apache/hadoop/hive/metastore/messaging/CreateTableMessage.java
Patch:
@@ -33,6 +33,8 @@ protected CreateTableMessage() {
    */
   public abstract String getTable();
 
+  public abstract String getTableType();
+
   public abstract Table getTableObj() throws Exception;
 
   /**

File: metastore/src/java/org/apache/hadoop/hive/metastore/messaging/DropPartitionMessage.java
Patch:
@@ -32,6 +32,8 @@ protected DropPartitionMessage() {
 
   public abstract String getTable();
 
+  public abstract String getTableType();
+
   public abstract Table getTableObj() throws Exception;
 
   public abstract List<Map<String, String>> getPartitions ();

File: metastore/src/java/org/apache/hadoop/hive/metastore/messaging/DropTableMessage.java
Patch:
@@ -31,6 +31,8 @@ protected DropTableMessage() {
    */
   public abstract String getTable();
 
+  public abstract String getTableType();
+
   @Override
   public EventMessage checkValid() {
     if (getTable() == null)

File: metastore/src/java/org/apache/hadoop/hive/metastore/messaging/InsertMessage.java
Patch:
@@ -37,6 +37,8 @@ protected InsertMessage() {
    */
   public abstract String getTable();
 
+  public abstract String getTableType();
+
   /**
    * Getter for the replace flag being insert into/overwrite
    * @return Replace flag to represent INSERT INTO or INSERT OVERWRITE (Boolean).

File: metastore/src/java/org/apache/hadoop/hive/metastore/messaging/json/JSONMessageFactory.java
Patch:
@@ -111,7 +111,7 @@ public AlterTableMessage buildAlterTableMessage(Table before, Table after, boole
   @Override
   public DropTableMessage buildDropTableMessage(Table table) {
     return new JSONDropTableMessage(MS_SERVER_URL, MS_SERVICE_PRINCIPAL, table.getDbName(),
-        table.getTableName(), now());
+        table.getTableName(), table.getTableType(), now());
   }
 
   @Override

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3003,7 +3003,7 @@ public static enum ConfVars {
     LLAP_IO_TRACE_SIZE("hive.llap.io.trace.size", "2Mb",
         new SizeValidator(0L, true, (long)Integer.MAX_VALUE, false),
         "The buffer size for a per-fragment LLAP debug trace. 0 to disable."),
-    LLAP_IO_TRACE_ALWAYS_DUMP("hive.llap.io.trace.always.dump", true, // TODO#
+    LLAP_IO_TRACE_ALWAYS_DUMP("hive.llap.io.trace.always.dump", false,
         "Whether to always dump the LLAP IO trace (if enabled); the default is on error."),
     LLAP_IO_NONVECTOR_WRAPPER_ENABLED("hive.llap.io.nonvector.wrapper.enabled", true,
         "Whether the LLAP IO layer is enabled for non-vectorized queries that read inputs\n" +

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/EncodedDataConsumer.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl;
 import org.apache.hadoop.hive.llap.metrics.LlapDaemonIOMetrics;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Consumer;
+import org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace;
 import org.apache.hive.common.util.FixedSizedObjectPool;
 import org.apache.orc.TypeDescription;
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/Reader.java
Patch:
@@ -45,7 +45,7 @@ public interface Reader extends org.apache.hadoop.hive.ql.io.orc.Reader {
    * @return The reader.
    */
   EncodedReader encodedReader(Object fileKey, DataCache dataCache, DataReader dataReader,
-      PoolFactory pf) throws IOException;
+      PoolFactory pf, IoTrace trace) throws IOException;
 
   /** The factory that can create (or return) the pools used by encoded reader. */
   public interface PoolFactory {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/ReaderImpl.java
Patch:
@@ -33,9 +33,9 @@ public ReaderImpl(Path path, ReaderOptions options) throws IOException {
   }
 
   @Override
-  public EncodedReader encodedReader(
-      Object fileKey, DataCache dataCache, DataReader dataReader, PoolFactory pf) throws IOException {
+  public EncodedReader encodedReader(Object fileKey, DataCache dataCache, DataReader dataReader,
+      PoolFactory pf, IoTrace trace) throws IOException {
     return new EncodedReaderImpl(fileKey, types,
-        codec, bufferSize, rowIndexStride, dataCache, dataReader, pf);
+        codec, bufferSize, rowIndexStride, dataCache, dataReader, pf, trace);
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorFactory.java
Patch:
@@ -38,6 +38,8 @@
 import org.apache.hadoop.hive.ql.exec.vector.VectorSparkHashTableSinkOperator;
 import org.apache.hadoop.hive.ql.exec.vector.VectorSparkPartitionPruningSinkOperator;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;
+import org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator;
+import org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFOperator;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.optimizer.spark.SparkPartitionPruningSinkDesc;
 import org.apache.hadoop.hive.ql.parse.spark.SparkPartitionPruningSinkOperator;
@@ -138,6 +140,7 @@ public final class OperatorFactory {
     vectorOpvec.put(FileSinkDesc.class, VectorFileSinkOperator.class);
     vectorOpvec.put(FilterDesc.class, VectorFilterOperator.class);
     vectorOpvec.put(LimitDesc.class, VectorLimitOperator.class);
+    vectorOpvec.put(PTFDesc.class, VectorPTFOperator.class);
     vectorOpvec.put(SparkHashTableSinkDesc.class, VectorSparkHashTableSinkOperator.class);
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ptf/WindowFrameDef.java
Patch:
@@ -78,6 +78,6 @@ public int getWindowSize() {
 
   @Override
   public String toString() {
-    return start + "~" + end;
+    return windowType + " " + start + "~" + end;
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFLeadLag.java
Patch:
@@ -146,7 +146,9 @@ public void setAmt(int amt) {
 
   @Override
   public String getDisplayString(String[] children) {
-    assert (children.length == 2);
+    if (children.length != 2) {
+      return _getFnName() + "(...)";
+    }
     return getStandardDisplayString(_getFnName(), children);
   }
 

File: llap-common/src/test/org/apache/hadoop/hive/llap/io/TestChunkedInputStream.java
Patch:
@@ -150,8 +150,8 @@ public TestStreams(boolean useChunkedStream) throws Exception {
       pout = new PipedOutputStream();
       pin = new PipedInputStream(pout);
       if (useChunkedStream) {
-        out = new ChunkedOutputStream(pout, bufferSize);
-        in = new ChunkedInputStream(pin);
+        out = new ChunkedOutputStream(pout, bufferSize, "test");
+        in = new ChunkedInputStream(pin, "test");
       } else {
         // Test behavior with non-chunked streams
         out = new FilterOutputStream(pout);
@@ -209,7 +209,7 @@ public void testBasicUsage() throws Exception {
     chunkedStreams.values = values;
     BasicUsageWriter writer2 = new BasicUsageWriter(chunkedStreams, false, false);
     BasicUsageReader reader2 = new BasicUsageReader(chunkedStreams);
-    runTest(writer2, reader2, nonChunkedStreams);
+    runTest(writer2, reader2, chunkedStreams);
     assertTrue(reader2.allValuesRead);
     assertTrue(((ChunkedInputStream) chunkedStreams.in).isEndOfData());
     assertNull(writer2.getError());

File: ql/src/java/org/apache/hadoop/hive/llap/LlapOutputFormatService.java
Patch:
@@ -198,10 +198,10 @@ private void registerReader(ChannelHandlerContext ctx, String id, byte[] tokenBy
       int maxPendingWrites = HiveConf.getIntVar(conf,
           HiveConf.ConfVars.LLAP_DAEMON_OUTPUT_SERVICE_MAX_PENDING_WRITES);
       @SuppressWarnings("rawtypes")
-      LlapRecordWriter writer = new LlapRecordWriter(
+      LlapRecordWriter writer = new LlapRecordWriter(id,
           new ChunkedOutputStream(
               new ChannelOutputStream(ctx, id, sendBufferSize, maxPendingWrites),
-              sendBufferSize));
+              sendBufferSize, id));
       boolean isFailed = true;
       synchronized (lock) {
         if (!writers.containsKey(id)) {

File: beeline/src/java/org/apache/hive/beeline/BeeLine.java
Patch:
@@ -2182,8 +2182,7 @@ private Driver[] scanDriversOLD(String line) {
       output(getColorBuffer().pad(loc("scanning", f.getAbsolutePath()), 60),
           false);
 
-      try {
-        ZipFile zf = new ZipFile(f);
+      try (ZipFile zf = new ZipFile(f)) {
         int total = zf.size();
         int index = 0;
 

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/StaticPermanentFunctionChecker.java
Patch:
@@ -43,8 +43,7 @@ public StaticPermanentFunctionChecker(Configuration conf) {
       LOG.warn("Could not find UDF whitelist in configuration: " + PERMANENT_FUNCTIONS_LIST);
       return;
     }
-    try {
-      BufferedReader r = new BufferedReader(new InputStreamReader(logger.openStream()));
+    try (BufferedReader r = new BufferedReader(new InputStreamReader(logger.openStream()))) {
       String klassName = r.readLine();
       while (klassName != null) {
         try {

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreSchemaInfo.java
Patch:
@@ -216,8 +216,8 @@ public String getMetaStoreSchemaVersion(MetaStoreConnectionInfo connectionInfo)
       versionQuery = "select t.SCHEMA_VERSION from VERSION t";
     }
     try (Connection metastoreDbConnection =
-        HiveSchemaHelper.getConnectionToMetastore(connectionInfo)) {
-      Statement stmt = metastoreDbConnection.createStatement();
+        HiveSchemaHelper.getConnectionToMetastore(connectionInfo); Statement stmt =
+        metastoreDbConnection.createStatement()) {
       ResultSet res = stmt.executeQuery(versionQuery);
       if (!res.next()) {
         throw new HiveMetaException("Could not find version info in metastore VERSION table.");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/FlatRowContainer.java
Patch:
@@ -292,7 +292,7 @@ public int size() {
     }
 
     public Iterator<Object> iterator() {
-      return listIterator();
+      return super.listIterator();
     }
 
     public ListIterator<Object> listIterator(int index) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/Repeated.java
Patch:
@@ -147,7 +147,6 @@ class RepeatedGroupConverter extends HiveGroupConverter
     private final ConverterParent parent;
     private final int index;
     private final List<Writable> list = new ArrayList<Writable>();
-    private final Map<String, String> metadata = new HashMap<String, String>();
 
 
     public RepeatedGroupConverter(GroupType groupType, ConverterParent parent, int index, TypeInfo hiveTypeInfo) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/DumpMetaData.java
Patch:
@@ -66,10 +66,9 @@ public void setDump(DumpType lvl, Long eventFrom, Long eventTo, Path cmRoot) {
   }
 
   private void loadDumpFromFile() throws SemanticException {
-    try {
+    try (FileSystem fs = dumpFile.getFileSystem(hiveConf); BufferedReader br =
+        new BufferedReader(new InputStreamReader(fs.open(dumpFile)))) {
       // read from dumpfile and instantiate self
-      FileSystem fs = dumpFile.getFileSystem(hiveConf);
-      BufferedReader br = new BufferedReader(new InputStreamReader(fs.open(dumpFile)));
       String line = null;
       if ((line = br.readLine()) != null) {
         String[] lineContents = line.split("\t", 5);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/TezEdgeProperty.java
Patch:
@@ -51,6 +51,7 @@ public TezEdgeProperty(HiveConf hiveConf, EdgeType edgeType, boolean isAutoReduc
       boolean isSlowStart, int minReducer, int maxReducer, long bytesPerReducer) {
     this(hiveConf, edgeType, -1);
     setAutoReduce(hiveConf, isAutoReduce, minReducer, maxReducer, bytesPerReducer);
+    this.isSlowStart = isSlowStart;
   }
 
   public void setAutoReduce(HiveConf hiveConf, boolean isAutoReduce, int minReducer,
@@ -60,7 +61,6 @@ public void setAutoReduce(HiveConf hiveConf, boolean isAutoReduce, int minReduce
     this.maxReducer = maxReducer;
     this.isAutoReduce = isAutoReduce;
     this.inputSizePerReducer = bytesPerReducer;
-    this.isSlowStart = isSlowStart;
   }
 
   public TezEdgeProperty(EdgeType edgeType) {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMkCollectionEvaluator.java
Patch:
@@ -134,7 +134,7 @@ public Object terminatePartial(AggregationBuffer agg) throws HiveException {
   public void merge(AggregationBuffer agg, Object partial)
       throws HiveException {
     MkArrayAggregationBuffer myagg = (MkArrayAggregationBuffer) agg;
-    List<Object> partialResult = (ArrayList<Object>) internalMergeOI.getList(partial);
+    List<Object> partialResult = (List<Object>) internalMergeOI.getList(partial);
     if (partialResult != null) {
       for(Object i : partialResult) {
         putIntoCollection(i, myagg);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3175,7 +3175,7 @@ public static enum ConfVars {
       new TimeValidator(TimeUnit.MILLISECONDS),
       "Amount of time to wait on connection failures to the AM from an LLAP daemon before\n" +
       "considering the AM to be dead.", "llap.am.liveness.connection.timeout-millis"),
-    LLAP_DAEMON_AM_USE_FQDN("hive.llap.am.use.fqdn", false,
+    LLAP_DAEMON_AM_USE_FQDN("hive.llap.am.use.fqdn", true,
         "Whether to use FQDN of the AM machine when submitting work to LLAP."),
     // Not used yet - since the Writable RPC engine does not support this policy.
     LLAP_DAEMON_AM_LIVENESS_CONNECTION_SLEEP_BETWEEN_RETRIES_MS(

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAppMasterEventOperator.java
Patch:
@@ -130,6 +130,7 @@ public void process(Object data, int tag) throws HiveException {
       throw new HiveException(e);
     }
 
-    forward(data, rowInspector);
+    forward(data, rowInspector, true);
   }
+
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorFilterOperator.java
Patch:
@@ -120,7 +120,7 @@ public void process(Object row, int tag) throws HiveException {
         // All are selected, do nothing
     }
     if (vrg.size > 0) {
-      forward(vrg, null);
+      forward(vrg, null, true);
     }
 
     // Restore the original selected vector

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
Patch:
@@ -1057,7 +1057,7 @@ private void writeSingleRow(VectorHashKeyWrapper kw, VectorAggregationBufferRow
       for (int i = 0; i < aggregators.length; ++i) {
         forwardCache[fi++] = aggregators[i].evaluateOutput(agg.getAggregationBuffer(i));
       }
-      forward(forwardCache, outputObjInspector);
+      forward(forwardCache, outputObjInspector, false);
     } else {
       // Output keys and aggregates into the output batch.
       for (int i = 0; i < outputKeyLength; ++i) {
@@ -1097,7 +1097,7 @@ private void writeGroupRow(VectorAggregationBufferRow agg, DataOutputBuffer buff
   }
 
   private void flushOutput() throws HiveException {
-    forward(outputBatch, null);
+    forward(outputBatch, null, true);
     outputBatch.reset();
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorLimitOperator.java
Patch:
@@ -75,7 +75,7 @@ public void process(Object row, int tag) throws HiveException {
           batch.selected[i] = batch.selected[skipSize + i];
         }
       }
-      forward(row, inputObjInspectors[tag]);
+      forward(row, inputObjInspectors[tag], true);
       currCount += batch.size;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapJoinBaseOperator.java
Patch:
@@ -126,7 +126,7 @@ protected void internalForward(Object row, ObjectInspector outputOI) throws Hive
   }
 
   private void flushOutput() throws HiveException {
-    forward(outputBatch, null);
+    forward(outputBatch, null, true);
     outputBatch.reset();
   }
 
@@ -185,4 +185,5 @@ protected void reProcessBigTable(int partitionId)
   public VectorizationContext getOuputVectorizationContext() {
     return vOutContext;
   }
+
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java
Patch:
@@ -307,7 +307,7 @@ protected void internalForward(Object row, ObjectInspector outputOI) throws Hive
   }
 
   private void flushOutput() throws HiveException {
-    forward(outputBatch, null);
+    forward(outputBatch, null, true);
     outputBatch.reset();
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSelectOperator.java
Patch:
@@ -115,7 +115,7 @@ public void process(Object row, int tag) throws HiveException {
 
     // Just forward the row as is
     if (conf.isSelStarNoCompute()) {
-      forward(row, inputObjInspectors[tag]);
+      forward(row, inputObjInspectors[tag], true);
       return;
     }
 
@@ -134,7 +134,7 @@ public void process(Object row, int tag) throws HiveException {
     int originalProjectionSize = vrg.projectionSize;
     vrg.projectionSize = projectedOutputColumns.length;
     vrg.projectedColumns = this.projectedOutputColumns;
-    forward(vrg, outputObjInspector);
+    forward(vrg, outputObjInspector, true);
 
     // Revert the projected columns back, because vrg will be re-used.
     vrg.projectionSize = originalProjectionSize;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java
Patch:
@@ -635,7 +635,7 @@ public void forwardBigTableBatch(VectorizedRowBatch batch) throws HiveException
     batch.projectionSize = outputProjection.length;
     batch.projectedColumns = outputProjection;
 
-    forward(batch, null);
+    forward(batch, null, true);
 
     // Revert the projected columns back, because batch can be re-used by our parent operators.
     batch.projectionSize = originalProjectionSize;
@@ -647,7 +647,7 @@ public void forwardBigTableBatch(VectorizedRowBatch batch) throws HiveException
    * Forward the overflow batch and reset the batch.
    */
   protected void forwardOverflow() throws HiveException {
-    forward(overflowBatch, null);
+    forward(overflowBatch, null, true);
     overflowBatch.reset();
     maybeCheckInterrupt();
   }
@@ -664,7 +664,7 @@ private void maybeCheckInterrupt() throws HiveException {
    * Forward the overflow batch, but do not reset the batch.
    */
   private void forwardOverflowNoReset() throws HiveException {
-    forward(overflowBatch, null);
+    forward(overflowBatch, null, true);
   }
 
   /*

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorSelectOperator.java
Patch:
@@ -61,7 +61,8 @@ public ValidatorVectorSelectOperator(CompilationOpContext ctx,
      * Override forward to do validation
      */
     @Override
-    public void forward(Object row, ObjectInspector rowInspector) throws HiveException {
+    public void forward(Object row, ObjectInspector rowInspector, boolean isVectorized)
+            throws HiveException {
       VectorizedRowBatch vrg = (VectorizedRowBatch) row;
 
       int[] projections = vrg.projectedColumns;

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java
Patch:
@@ -842,7 +842,7 @@ public static String getString(Object o, PrimitiveObjectInspector oi) {
       }
       break;
     case BOOLEAN:
-      result = String.valueOf((((BooleanObjectInspector) oi).get(o)));
+      result = (((BooleanObjectInspector) oi).get(o)) ? "TRUE" : "FALSE";
       break;
     case BYTE:
       result = String.valueOf((((ByteObjectInspector) oi).get(o)));

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ProjectionPusher.java
Patch:
@@ -115,7 +115,9 @@ private void pushProjectionsAndFilters(final JobConf jobConf,
           allColumnsNeeded = true;
         } else {
           neededColumnIDs.addAll(ts.getNeededColumnIDs());
-          neededNestedColumnPaths.addAll(ts.getNeededNestedColumnPaths());
+          if (ts.getNeededNestedColumnPaths() != null) {
+            neededNestedColumnPaths.addAll(ts.getNeededNestedColumnPaths());
+          }
         }
 
         rowSchema = ts.getSchema();

File: itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java
Patch:
@@ -62,7 +62,6 @@
 import org.apache.hadoop.mapreduce.InputSplit;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.parquet.hadoop.ParquetInputFormat;
-import org.apache.parquet.hadoop.ParquetInputSplit;
 import org.apache.parquet.hadoop.api.ReadSupport;
 import org.apache.parquet.hadoop.example.GroupReadSupport;
 import org.openjdk.jmh.annotations.Param;
@@ -339,7 +338,7 @@ public RecordReader getVectorizedRecordReader(Path inputPath) throws Exception {
       Job vectorJob = new Job(conf, "read vector");
       ParquetInputFormat.setInputPaths(vectorJob, inputPath);
       ParquetInputFormat parquetInputFormat = new ParquetInputFormat(GroupReadSupport.class);
-      ParquetInputSplit split = (ParquetInputSplit) parquetInputFormat.getSplits(vectorJob).get(0);
+      InputSplit split = (InputSplit) parquetInputFormat.getSplits(vectorJob).get(0);
       initialVectorizedRowBatchCtx(conf);
       return new VectorizedParquetRecordReader(split, new JobConf(conf));
     }

File: ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestVectorizedColumnReader.java
Patch:
@@ -106,7 +106,7 @@ public void testNullSplitForParquetReader() throws Exception {
     HiveConf.setVar(conf, HiveConf.ConfVars.PLAN, "//tmp");
     initialVectorizedRowBatchCtx(conf);
     VectorizedParquetRecordReader reader =
-        new VectorizedParquetRecordReader((org.apache.hadoop.mapred.InputSplit)null, new JobConf(conf));
+        new VectorizedParquetRecordReader((InputSplit)null, new JobConf(conf));
     assertFalse(reader.next(reader.createKey(), reader.createValue()));
   }
 }

File: ql/src/test/org/apache/hadoop/hive/ql/io/parquet/VectorizedColumnReaderTestBase.java
Patch:
@@ -47,7 +47,6 @@
 import org.apache.parquet.example.data.Group;
 import org.apache.parquet.example.data.simple.SimpleGroupFactory;
 import org.apache.parquet.hadoop.ParquetInputFormat;
-import org.apache.parquet.hadoop.ParquetInputSplit;
 import org.apache.parquet.hadoop.ParquetWriter;
 import org.apache.parquet.hadoop.example.GroupReadSupport;
 import org.apache.parquet.hadoop.example.GroupWriteSupport;
@@ -223,7 +222,7 @@ protected VectorizedParquetRecordReader createParquetReader(String schemaString,
     Job vectorJob = new Job(conf, "read vector");
     ParquetInputFormat.setInputPaths(vectorJob, file);
     ParquetInputFormat parquetInputFormat = new ParquetInputFormat(GroupReadSupport.class);
-    ParquetInputSplit split = (ParquetInputSplit) parquetInputFormat.getSplits(vectorJob).get(0);
+    InputSplit split = (InputSplit) parquetInputFormat.getSplits(vectorJob).get(0);
     initialVectorizedRowBatchCtx(conf);
     return new VectorizedParquetRecordReader(split, new JobConf(conf));
   }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1360,7 +1360,7 @@ public static enum ConfVars {
     HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION("hive.parquet.timestamp.skip.conversion", true,
         "Current Hive implementation of parquet stores timestamps to UTC, this flag allows skipping of the conversion" +
             "on reading parquet files from other tools"),
-    HIVE_PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE("hive.parquet.mr.int96.enable.utc.write.zone", false,
+    PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE("parquet.mr.int96.enable.utc.write.zone", false,
         "Enable this variable to use UTC as the default timezone for new Parquet tables."),
     HIVE_INT_TIMESTAMP_CONVERSION_IN_SECONDS("hive.int.timestamp.conversion.in.seconds", false,
         "Boolean/tinyint/smallint/int/bigint value is interpreted as milliseconds during the timestamp conversion.\n" +

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -4384,11 +4384,11 @@ private int createTable(Hive db, CreateTableDesc crtTbl) throws HiveException {
       }
     }
 
-    // If HIVE_PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE is set to True, then set new Parquet tables timezone
+    // If PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE is set to True, then set new Parquet tables timezone
     // to UTC by default (only if the table property is not set)
     if (tbl.getSerializationLib().equals(ParquetHiveSerDe.class.getName())) {
       SessionState ss = SessionState.get();
-      if (ss.getConf().getBoolVar(ConfVars.HIVE_PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE)) {
+      if (ss.getConf().getBoolVar(ConfVars.PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE)) {
         String parquetTimezone = tbl.getProperty(ParquetTableUtils.PARQUET_INT96_WRITE_ZONE_PROPERTY);
         if (parquetTimezone == null || parquetTimezone.isEmpty()) {
           tbl.setProperty(ParquetTableUtils.PARQUET_INT96_WRITE_ZONE_PROPERTY, ParquetTableUtils.PARQUET_INT96_NO_ADJUSTMENT_ZONE);

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -471,6 +471,8 @@ public enum ErrorMsg {
   INVALID_JOIN_CONDITION(10407, "Error parsing condition in join"),
   INVALID_TARGET_COLUMN_IN_SET_CLAUSE(10408, "Target column \"{0}\" of set clause is not found in table \"{1}\".", true),
   HIVE_GROUPING_FUNCTION_EXPR_NOT_IN_GROUPBY(10409, "Expression in GROUPING function not present in GROUP BY"),
+  ALTER_TABLE_NON_PARTITIONED_TABLE_CASCADE_NOT_SUPPORTED(10410,
+      "Alter table with non-partitioned table does not support cascade"),
   //========================== 20000 range starts here ========================//
   SCRIPT_INIT_ERROR(20000, "Unable to initialize custom script."),
   SCRIPT_IO_ERROR(20001, "An error occurred while reading or writing to your custom script. "

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
Patch:
@@ -947,7 +947,9 @@ protected void initializeOp(Configuration hconf) throws HiveException {
 
       for (int i = 0; i < aggregators.length; ++i) {
         aggregators[i].init(conf.getAggregators().get(i));
-        objectInspectors.add(aggregators[i].getOutputObjectInspector());
+        ObjectInspector objInsp = aggregators[i].getOutputObjectInspector();
+        Preconditions.checkState(objInsp != null);
+        objectInspectors.add(objInsp);
       }
 
       keyWrappersBatch = VectorHashKeyWrapperBatch.compileKeyWrapperBatch(keyExpressions);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorGroupByOperator.java
Patch:
@@ -2157,7 +2157,7 @@ public void validate(String key, Object expected, Object result) {
       } else {
         assertEquals (true, arr[0] instanceof Object[]);
         Object[] vals = (Object[]) arr[0];
-        assertEquals (2, vals.length);
+        assertEquals (3, vals.length);
 
         assertEquals (true, vals[0] instanceof LongWritable);
         LongWritable lw = (LongWritable) vals[0];

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/comparator/FirstInFirstOutComparator.java
Patch:
@@ -32,8 +32,8 @@ public class FirstInFirstOutComparator implements Comparator<TaskWrapper> {
   public int compare(TaskWrapper t1, TaskWrapper t2) {
     TaskRunnerCallable o1 = t1.getTaskRunnerCallable();
     TaskRunnerCallable o2 = t2.getTaskRunnerCallable();
-    boolean o1CanFinish = o1.canFinish();
-    boolean o2CanFinish = o2.canFinish();
+    boolean o1CanFinish = o1.canFinishForPriority();
+    boolean o2CanFinish = o2.canFinishForPriority();
     if (o1CanFinish == true && o2CanFinish == false) {
       return -1;
     } else if (o1CanFinish == false && o2CanFinish == true) {

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/comparator/ShortestJobFirstComparator.java
Patch:
@@ -28,8 +28,8 @@ public class ShortestJobFirstComparator implements Comparator<TaskWrapper> {
   public int compare(TaskWrapper t1, TaskWrapper t2) {
     TaskRunnerCallable o1 = t1.getTaskRunnerCallable();
     TaskRunnerCallable o2 = t2.getTaskRunnerCallable();
-    boolean o1CanFinish = o1.canFinish();
-    boolean o2CanFinish = o2.canFinish();
+    boolean o1CanFinish = o1.canFinishForPriority();
+    boolean o2CanFinish = o2.canFinishForPriority();
     if (o1CanFinish == true && o2CanFinish == false) {
       return -1;
     } else if (o1CanFinish == false && o2CanFinish == true) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LlapDecider.java
Patch:
@@ -129,7 +129,7 @@ public LlapDecisionDispatcher(PhysicalContext pctx, LlapMode mode) {
       shouldUber = HiveConf.getBoolVar(conf, ConfVars.LLAP_AUTO_ALLOW_UBER) && (mode != all);
       minReducersPerExec = HiveConf.getFloatVar(
           conf, ConfVars.TEZ_LLAP_MIN_REDUCER_PER_EXECUTOR);
-      executorsPerNode = HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_NUM_EXECUTORS); // TODO# hmm
+      executorsPerNode = HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_NUM_EXECUTORS);
       mapJoinOpList = new ArrayList<MapJoinOperator>();
       rules = getRules();
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java
Patch:
@@ -105,7 +105,6 @@ public static ReduceWork createReduceWork(
       final int maxReducers = context.conf.getIntVar(HiveConf.ConfVars.MAXREDUCERS);
       // estimated number of reducers
       final int nReducers = reduceSink.getConf().getNumReducers();
-      // TODO# HERE
 
       // min we allow tez to pick
       int minPartition = Math.max(1, (int) (nReducers * minPartitionFactor));

File: ql/src/java/org/apache/hadoop/hive/ql/session/OperationLog.java
Patch:
@@ -51,7 +51,7 @@ public enum LoggingLevel {
     NONE, EXECUTION, PERFORMANCE, VERBOSE, UNKNOWN
   }
 
-  public OperationLog(String name, File file, HiveConf hiveConf) throws FileNotFoundException {
+  public OperationLog(String name, File file, HiveConf hiveConf) {
     operationName = name;
     logFile = new LogFile(file);
 
@@ -133,7 +133,7 @@ private class LogFile {
     private BufferedReader in;
     private volatile boolean isRemoved;
 
-    LogFile(File file) throws FileNotFoundException {
+    LogFile(File file) {
       this.file = file;
       isRemoved = false;
     }
@@ -157,7 +157,7 @@ synchronized void close(boolean removeLog) {
         if (in != null) {
           in.close();
         }
-        if (!isRemoved && removeLog) {
+        if (!isRemoved && removeLog && file.exists()) {
           FileUtils.forceDelete(file);
           isRemoved = true;
         }

File: llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java
Patch:
@@ -196,7 +196,6 @@ public InetSocketAddress getAddress() {
 
   /**
    * Submit the work for actual execution.
-   * @throws InvalidProtocolBufferException 
    */
   public void submitWork(SubmitWorkRequestProto request, String llapHost, int llapPort) {
     // Register the pending events to be sent for this spec.

File: llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceInstance.java
Patch:
@@ -20,7 +20,7 @@
 public interface ServiceInstance {
 
   /**
-   * Worker identity is a UUID (unique across restarts), to identify a node which died & was brought
+   * Worker identity is a UUID (unique across restarts), to identify a node which died &amp; was brought
    * back on the same host/port
    */
   public String getWorkerIdentity();

File: metastore/src/java/org/apache/hadoop/hive/metastore/DatabaseProduct.java
Patch:
@@ -27,7 +27,7 @@ public enum DatabaseProduct {
 
   /**
    * Determine the database product type
-   * @param conn database connection
+   * @param productName string to defer database connection
    * @return database product type
    */
   public static DatabaseProduct determineDatabaseProduct(String productName) throws SQLException {

File: metastore/src/java/org/apache/hadoop/hive/metastore/Deadline.java
Patch:
@@ -82,7 +82,7 @@ public static void registerIfNot(long timeout) {
 
   /**
    * reset the timeout value of this timer.
-   * @param timeout
+   * @param timeoutMs
    */
   public static void resetTimeout(long timeoutMs) throws MetaException {
     if (timeoutMs <= 0) {
@@ -139,7 +139,7 @@ public static void clear() {
 
   /**
    * Check whether the long running method timeout.
-   * @throws DeadlineException when the method timeout
+   * @throws MetaException when the method timeout
    */
   public static void checkTimeout() throws MetaException {
     Deadline deadline = getCurrentDeadline();

File: metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreSchemaInfo.java
Patch:
@@ -99,7 +99,7 @@ String getMetaStoreSchemaVersion(
    * 'rolling downgrade' is happening. This is a state where hive is functional and returning non
    * zero status for it is misleading.
    *
-   * @param hiveVersion version of hive software
+   * @param productVersion version of hive software
    * @param dbVersion version of metastore rdbms schema
    * @return true if versions are compatible
    */

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreFilterHook.java
Patch:
@@ -58,7 +58,7 @@ public interface MetaStoreFilterHook {
    * Filter given list of tables
    * @param dbName
    * @param tableList
-   * @returnList of filtered table names
+   * @return List of filtered table names
    */
   public List<String> filterTableNames(String dbName, List<String> tableList) throws MetaException;
 
@@ -72,9 +72,8 @@ public interface MetaStoreFilterHook {
 
   /**
    * Filter given list of tables
-   * @param dbName
    * @param tableList
-   * @returnList of filtered table names
+   * @return List of filtered table names
    */
   public List<Table> filterTables(List<Table> tableList) throws MetaException;
 

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreThread.java
Patch:
@@ -55,7 +55,7 @@ public interface MetaStoreThread {
 
   /**
    * Run the thread in the background.  This must not be called until
-   * {@link ##init(java.util.concurrent.atomic.AtomicBoolean, java.util.concurrent.atomic.AtomicBoolean)} has
+   * {@link MetaStoreThread#init(java.util.concurrent.atomic.AtomicBoolean,java.util.concurrent.atomic.AtomicBoolean)} has
    * been called.
    */
   void start();

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
Patch:
@@ -576,7 +576,6 @@ public static List<String> getPvals(List<FieldSchema> partCols,
    * @param conf
    *          hive configuration
    * @return true or false depending on conformance
-   * @exception MetaException
    *              if it doesn't match the pattern.
    */
   static public boolean validateName(String name, Configuration conf) {
@@ -700,7 +699,7 @@ static private boolean areColTypesCompatible(String oldType, String newType) {
    * validate column type
    *
    * if it is predefined, yes. otherwise no
-   * @param name
+   * @param type
    * @return
    */
   static public String validateColumnType(String type) {
@@ -858,7 +857,7 @@ public static String typeToThriftType(String type) {
    * @return String containing "Thrift
    *         DDL#comma-separated-column-names#colon-separated-columntypes
    *         Example:
-   *         "struct result { a string, map<int,string> b}#a,b#string:map<int,string>"
+   *         "struct result { a string, map&lt;int,string&gt; b}#a,b#string:map&lt;int,string&gt;"
    */
   public static String getFullDDLFromFieldSchema(String structName,
       List<FieldSchema> fieldSchemas) {

File: metastore/src/java/org/apache/hadoop/hive/metastore/PartFilterExprUtil.java
Patch:
@@ -62,7 +62,7 @@ public static ExpressionTree makeExpressionTree(PartitionExpressionProxy express
 
   /**
    * Creates the proxy used to evaluate expressions. This is here to prevent circular
-   * dependency - ql -&gt; metastore client &lt;-&gt metastore server -&gt ql. If server and
+   * dependency - ql -&gt; metastore client &lt;-&gt; metastore server -&gt; ql. If server and
    * client are split, this can be removed.
    * @param conf Configuration.
    * @return The partition expression proxy.

File: metastore/src/java/org/apache/hadoop/hive/metastore/RawStore.java
Patch:
@@ -407,7 +407,6 @@ public abstract boolean updatePartitionColumnStatistics(ColumnStatistics statsOb
    * @return Relevant column statistics for the column for the given table
    * @throws NoSuchObjectException
    * @throws MetaException
-   * @throws InvalidInputException
    *
    */
   public abstract ColumnStatistics getTableColumnStatistics(String dbName, String tableName,
@@ -543,7 +542,6 @@ public void alterFunction(String dbName, String funcName, Function newFunction)
    * Drop a function definition.
    * @param dbName
    * @param funcName
-   * @return
    * @throws MetaException
    * @throws NoSuchObjectException
    * @throws InvalidObjectException

File: metastore/src/java/org/apache/hadoop/hive/metastore/events/InsertEvent.java
Patch:
@@ -43,7 +43,7 @@ public class InsertEvent extends ListenerEvent {
    * @param db name of the database the table is in
    * @param table name of the table being inserted into
    * @param partVals list of partition values, can be null
-   * @param insertData the inserted files & their checksums
+   * @param insertData the inserted files and their checksums
    * @param status status of insert, true = success, false = failure
    * @param handler handler that is firing the event
    */

File: metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java
Patch:
@@ -2297,7 +2297,6 @@ ByteBuffer[] getFileMetadata(List<Long> fileIds) throws IOException {
 
   /**
    * @param fileIds file ID list.
-   * @return Serialized file metadata.
    */
   @Override
   public void getFileMetadata(List<Long> fileIds, ByteBuffer[] result) throws IOException {

File: metastore/src/java/org/apache/hadoop/hive/metastore/hbase/MetadataStore.java
Patch:
@@ -41,7 +41,7 @@ void storeFileMetadata(List<Long> fileIds, List<ByteBuffer> metadataBuffers,
 
   /**
    * @param fileId The file ID.
-   * @param metadataBuffers Serialized file metadata.
+   * @param metadata Serialized file metadata.
    * @param addedCols The column names for additional columns created by file-format-specific
    *                  metadata handler, to be stored in the cache.
    * @param addedVals The values for addedCols; one value per added column.

File: metastore/src/java/org/apache/hadoop/hive/metastore/messaging/event/filters/DatabaseAndTableFilter.java
Patch:
@@ -23,7 +23,7 @@
  * Utility function that constructs a notification filter to match a given db name and/or table name.
  * If dbName == null, fetches all warehouse events.
  * If dnName != null, but tableName == null, fetches all events for the db
- * If dbName != null && tableName != null, fetches all events for the specified table
+ * If dbName != null &amp;&amp; tableName != null, fetches all events for the specified table
  */
 public class DatabaseAndTableFilter extends BasicFilter {
   private final String databaseName, tableName;

File: metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java
Patch:
@@ -230,8 +230,7 @@ protected void accept(TreeVisitor visitor) throws MetaException {
      *        tables that match the filter.
      * @param params
      *        A map of parameter key to values for the filter statement.
-     * @param filterBuilder The filter builder that is used to build filter.
-     * @return a JDO filter statement
+     * @param filterBuffer The filter builder that is used to build filter.
      * @throws MetaException
      */
     public void generateJDOFilter(Configuration conf, Table table,
@@ -385,7 +384,6 @@ private void generateJDOFilterOverPartitions(Configuration conf, Table table,
     }
 
     /**
-     * @param operator operator
      * @return true iff filter pushdown for this operator can be done for integral types.
      */
     public boolean canJdoUseStringsWithIntegral() {

File: metastore/src/java/org/apache/hadoop/hive/metastore/tools/HiveSchemaHelper.java
Patch:
@@ -48,7 +48,7 @@ public class HiveSchemaHelper {
    * @param printInfo print connection parameters
    * @param hiveConf hive config object
    * @return metastore connection object
-   * @throws org.apache.hadoop.hive.metastore.api.MetaException
+   * @throws org.apache.hadoop.hive.metastore.HiveMetaException
    */
   public static Connection getConnectionToMetastore(String userName,
       String password, String url, String driver, boolean printInfo,

File: metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnStore.java
Patch:
@@ -424,7 +424,7 @@ public static interface LockHandle {
   }
 
   /**
-   * Once a {@link java.util.concurrent.ThreadPoolExecutor.Worker} submits a job to the cluster,
+   * Once a {@link java.util.concurrent.ThreadPoolExecutor} Worker submits a job to the cluster,
    * it calls this to update the metadata.
    * @param id {@link CompactionInfo#id}
    */

File: metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java
Patch:
@@ -75,7 +75,7 @@ public static ValidTxnList createValidReadTxnList(GetOpenTxnsResponse txns, long
    * Transform a {@link org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse} to a
    * {@link org.apache.hadoop.hive.common.ValidTxnList}.  This assumes that the caller intends to
    * compact the files, and thus treats only open transactions as invalid.  Additionally any
-   * txnId > highestOpenTxnId is also invalid.  This is to avoid creating something like
+   * txnId &gt; highestOpenTxnId is also invalid.  This is to avoid creating something like
    * delta_17_120 where txnId 80, for example, is still open.
    * @param txns txn list from the metastore
    * @return a valid txn list.

File: metastore/src/model/org/apache/hadoop/hive/metastore/model/MStorageDescriptor.java
Patch:
@@ -252,7 +252,7 @@ public Map<MStringList, String> getSkewedColValueLocationMaps() {
   }
 
   /**
-   * @param skewedColValueLocationMaps the skewedColValueLocationMaps to set
+   * @param listBucketColValuesMapping the skewedColValueLocationMaps to set
    */
   public void setSkewedColValueLocationMaps(Map<MStringList, String> listBucketColValuesMapping) {
     this.skewedColValueLocationMaps = listBucketColValuesMapping;

File: llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java
Patch:
@@ -127,7 +127,7 @@ public class ShuffleHandler implements AttemptRegistrationListener {
 
   public static final String SHUFFLE_HANDLER_LOCAL_DIRS = "llap.shuffle.handler.local-dirs";
 
-  public static final String SHUFFLE_MANAGE_OS_CACHE = "lla[.shuffle.manage.os.cache";
+  public static final String SHUFFLE_MANAGE_OS_CACHE = "llap.shuffle.manage.os.cache";
   public static final boolean DEFAULT_SHUFFLE_MANAGE_OS_CACHE = true;
 
   public static final String SHUFFLE_READAHEAD_BYTES = "llap.shuffle.readahead.bytes";

File: metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -290,6 +290,8 @@ public void setConf(Configuration conf) {
       if (propsChanged) {
         if (pmf != null){
           clearOutPmfClassLoaderCache(pmf);
+          // close the underlying connection pool to avoid leaks
+          pmf.close();
         }
         pmf = null;
         prop = null;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveFilterAggregateTransposeRule.java
Patch:
@@ -20,16 +20,16 @@
 import org.apache.calcite.plan.RelOptRuleCall;
 import org.apache.calcite.rel.core.Aggregate;
 import org.apache.calcite.rel.core.Filter;
-import org.apache.calcite.rel.core.RelFactories.FilterFactory;
 import org.apache.calcite.rel.rules.FilterAggregateTransposeRule;
 import org.apache.calcite.rex.RexNode;
+import org.apache.calcite.tools.RelBuilderFactory;
 import org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil;
 
 public class HiveFilterAggregateTransposeRule extends FilterAggregateTransposeRule {
 
   public HiveFilterAggregateTransposeRule(Class<? extends Filter> filterClass,
-      FilterFactory filterFactory, Class<? extends Aggregate> aggregateClass) {
-    super(filterClass, filterFactory, aggregateClass);
+      RelBuilderFactory builderFactory, Class<? extends Aggregate> aggregateClass) {
+    super(filterClass, builderFactory, aggregateClass);
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -1670,7 +1670,8 @@ private RelNode applyPreJoinOrderingTransforms(RelNode basePlan, RelMetadataProv
       rules.add(HiveFilterSortTransposeRule.INSTANCE);
       rules.add(HiveFilterJoinRule.JOIN);
       rules.add(HiveFilterJoinRule.FILTER_ON_JOIN);
-      rules.add(new HiveFilterAggregateTransposeRule(Filter.class, HiveRelFactories.HIVE_FILTER_FACTORY, Aggregate.class));
+      rules.add(new HiveFilterAggregateTransposeRule(Filter.class, HiveRelFactories.HIVE_BUILDER,
+          Aggregate.class));
       rules.add(new FilterMergeRule(HiveRelFactories.HIVE_BUILDER));
       if (conf.getBoolVar(HiveConf.ConfVars.HIVE_OPTIMIZE_REDUCE_WITH_STATS)) {
         rules.add(HiveReduceExpressionsWithStatsRule.INSTANCE);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -4566,8 +4566,8 @@ private int createTableLike(Hive db, CreateTableLikeDesc crtTbl) throws Exceptio
 
     if (crtTbl.getLocation() == null && !tbl.isPartitioned()
         && conf.getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {
-      StatsSetupConst.setBasicStatsStateForCreateTable(tbl.getTTable().getParameters(),
-          StatsSetupConst.TRUE);
+      StatsSetupConst.setStatsStateForCreateTable(tbl.getTTable().getParameters(),
+          MetaStoreUtils.getColumnNames(tbl.getCols()), StatsSetupConst.TRUE);
     }
 
     // create the table

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -1752,7 +1752,7 @@ public Partition loadPartition(Path loadPath, Table tbl,
       if (oldPart == null) {
         newTPart.getTPartition().setParameters(new HashMap<String,String>());
         if (this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {
-          StatsSetupConst.setBasicStatsStateForCreateTable(newTPart.getParameters(),
+          StatsSetupConst.setStatsStateForCreateTable(newTPart.getParameters(), null,
               StatsSetupConst.TRUE);
         }
         MetaStoreUtils.populateQuickStats(HiveStatsUtils.getFileStatusRecurse(newPartPath, -1, newPartPath.getFileSystem(conf)), newTPart.getParameters());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -2951,8 +2951,8 @@ private void analyzeAlterTableAddParts(String[] qualified, CommonTree ast, boole
           if (desc.getPartParams() == null) {
             desc.setPartParams(new HashMap<String, String>());
           }
-          StatsSetupConst.setBasicStatsStateForCreateTable(desc.getPartParams(),
-              StatsSetupConst.TRUE);
+          StatsSetupConst.setStatsStateForCreateTable(desc.getPartParams(),
+              MetaStoreUtils.getColumnNames(tab.getCols()), StatsSetupConst.TRUE);
         }
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashTable.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError;
 import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTable;
 
 public abstract class VectorMapJoinFastHashTable implements VectorMapJoinHashTable {
@@ -47,7 +48,7 @@ public abstract class VectorMapJoinFastHashTable implements VectorMapJoinHashTab
   public static final int ONE_SIXTH_LIMIT = HIGHEST_INT_POWER_OF_2 / 6;
 
   public void throwExpandError(int limit, String dataTypeName) {
-    throw new RuntimeException(
+    throw new MapJoinMemoryExhaustionError(
         "Vector MapJoin " + dataTypeName + " Hash Table cannot grow any more -- use a smaller container size. " +
         "Current logical size is " + logicalHashBucketCount + " and " +
         "the limit is " + limit + ". " +

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
Patch:
@@ -378,7 +378,9 @@ protected Void performDataRead() throws IOException {
                 orcReader.getSchema(), orcReader.getWriterVersion());
             counters.incrTimeCounter(LlapIOCounters.HDFS_TIME_NS, startTimeHdfs);
             if (hasFileId && metadataCache != null) {
-              stripeMetadata = metadataCache.putStripeMetadata(stripeMetadata);
+              OrcStripeMetadata newMetadata = metadataCache.putStripeMetadata(stripeMetadata);
+              isFoundInCache = newMetadata != stripeMetadata; // May be cached concurrently.
+              stripeMetadata = newMetadata;
               if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {
                 LlapIoImpl.ORC_LOGGER.trace("Caching stripe {} metadata with includes: {}",
                     stripeKey.stripeIx, DebugUtils.toString(globalIncludes));

File: metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java
Patch:
@@ -99,7 +99,7 @@ public static ValidTxnList createValidCompactTxnList(GetOpenTxnsInfoResponse txn
     }
     highWater = minOpenTxn == Long.MAX_VALUE ? highWater : minOpenTxn - 1;
     BitSet bitSet = new BitSet(exceptions.length);
-    bitSet.set(0, bitSet.length()); // for ValidCompactorTxnList, everything in exceptions are aborted
+    bitSet.set(0, exceptions.length); // for ValidCompactorTxnList, everything in exceptions are aborted
     return new ValidCompactorTxnList(exceptions, bitSet, highWater);
   }
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
Patch:
@@ -104,7 +104,7 @@ protected void setUp() {
         db.dropTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, src, true, true);
         db.createTable(src, cols, null, TextInputFormat.class,
             IgnoreKeyTextOutputFormat.class);
-        db.loadTable(hadoopDataFile[i], src, false, false, false, false, false, null, 0);
+        db.loadTable(hadoopDataFile[i], src, false, false, false, false, false, null, 0, false);
         i++;
       }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractFileMergeOperator.java
Patch:
@@ -89,7 +89,7 @@ public void initializeOp(Configuration hconf) throws HiveException {
         .isListBucketingAlterTableConcatenate();
     listBucketingDepth = conf.getListBucketingDepth();
     Path specPath = conf.getOutputPath();
-    isMmTable = conf.getTxnId() != null;
+    isMmTable = conf.getIsMmTable();
     if (isMmTable) {
       updatePaths(specPath, null);
     } else {
@@ -282,7 +282,7 @@ public void jobCloseOp(Configuration hconf, boolean success)
       FileSystem fs = outputDir.getFileSystem(hconf);
       Long mmWriteId = conf.getTxnId();
       int stmtId = conf.getStmtId();
-      if (mmWriteId == null) {
+      if (!isMmTable) {
         Path backupPath = backupOutputPath(fs, outputDir);
         Utilities.mvFileToFinalPath(
             outputDir, hconf, success, LOG, conf.getDpCtx(), null, reporter);
@@ -298,7 +298,7 @@ public void jobCloseOp(Configuration hconf, boolean success)
         // We don't expect missing buckets from mere (actually there should be no buckets),
         // so just pass null as bucketing context. Union suffix should also be accounted for.
         Utilities.handleMmTableFinalPath(outputDir.getParent(), null, hconf, success,
-            dpLevels, lbLevels, null, mmWriteId, stmtId, reporter, false);
+            dpLevels, lbLevels, null, mmWriteId, stmtId, reporter, isMmTable, false);
       }
 
     } catch (IOException e) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SkewJoinResolver.java
Patch:
@@ -86,7 +86,7 @@ public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
       ParseContext pc = physicalContext.getParseContext();
       if (pc.getLoadTableWork() != null) {
         for (LoadTableDesc ltd : pc.getLoadTableWork()) {
-          if (ltd.getTxnId() == null) continue;
+          if (!ltd.isMmTable()) continue;
           // See the path in FSOP that calls fs.exists on finalPath.
           LOG.debug("Not using skew join because the destination table "
               + ltd.getTable().getTableName() + " is an insert_only table");

File: ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
Patch:
@@ -53,6 +53,7 @@
 import org.apache.hadoop.hive.ql.plan.LoadTableDesc;
 import org.apache.hadoop.hive.ql.plan.MoveWork;
 import org.apache.hadoop.hive.ql.plan.StatsWork;
+import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.mapred.InputFormat;
 
 import com.google.common.collect.Lists;
@@ -273,7 +274,7 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
     int stmtId = 0;
     Table tbl = ts.tableHandle;
     if (MetaStoreUtils.isInsertOnlyTable(tbl.getParameters())) {
-      txnId = 0l; //todo to be replaced with txnId in Driver
+      txnId = SessionState.get().getTxnMgr().getCurrentTxnId();
     }
 
     LoadTableDesc loadTableWork;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -7076,7 +7076,6 @@ protected Operator genFileSinkPlan(String dest, QB qb, Operator input)
       genPartnCols(dest, input, qb, table_desc, dest_tab, rsCtx);
     }
 
-    assert isMmTable == (txnId != null);
     FileSinkDesc fileSinkDesc = createFileSinkDesc(dest, table_desc, dest_part,
         dest_path, currentTableId, destTableIsAcid, destTableIsTemporary,
         destTableIsMaterialization, queryTmpdir, rsCtx, dpCtx, lbCtx, fsRS,

File: ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java
Patch:
@@ -23,6 +23,7 @@
 import java.util.Map;
 
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.AcidUtils;
 import org.apache.hadoop.hive.ql.plan.Explain.Level;
@@ -159,11 +160,11 @@ public boolean getReplace() {
 
   @Explain(displayName = "micromanaged table")
   public Boolean isMmTableExplain() {
-    return txnId != null? true : null;
+    return isMmTable() ? true : null;
   }
 
   public boolean isMmTable() {
-    return txnId != null;
+    return MetaStoreUtils.isInsertOnlyTable(table.getProperties());
   }
 
   public void setReplace(boolean replace) {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestExecDriver.java
Patch:
@@ -140,7 +140,7 @@ public class TestExecDriver extends TestCase {
         db.dropTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, src, true, true);
         db.createTable(src, cols, null, TextInputFormat.class,
             HiveIgnoreKeyTextOutputFormat.class);
-        db.loadTable(hadoopDataFile[i], src, false, true, false, false, false, null, 0);
+        db.loadTable(hadoopDataFile[i], src, false, true, false, false, false, null, 0, false);
         i++;
       }
 

File: serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java
Patch:
@@ -20,7 +20,6 @@
 import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.IOException;
-import java.io.OutputStream;
 import java.sql.Timestamp;
 import java.text.DateFormat;
 import java.text.SimpleDateFormat;
@@ -133,7 +132,8 @@ public void set(Timestamp t) {
       timestamp.setNanos(0);
       return;
     }
-    this.timestamp = t;
+    timestamp.setTime(t.getTime());
+    timestamp.setNanos(t.getNanos());
     bytesEmpty = true;
     timestampEmpty = false;
   }

File: serde/src/java/org/apache/hadoop/hive/serde2/AbstractSerDe.java
Patch:
@@ -118,7 +118,7 @@ public String getConfigurationErrors() {
   }
 
   /**
-   * @rturn Whether the SerDe that can store schema both inside and outside of metastore
+   * @return Whether the SerDe that can store schema both inside and outside of metastore
    *        does, in fact, store it inside metastore, based on table parameters.
    */
   public boolean shouldStoreFieldsInMetastore(Map<String, String> tableParams) {

File: serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
Patch:
@@ -163,7 +163,7 @@ public static String lightEscapeString(String str) {
    * Convert a Object to a standard Java object in compliance with JDBC 3.0 (see JDBC 3.0
    * Specification, Table B-3: Mapping from JDBC Types to Java Object Types).
    *
-   * This method is kept consistent with {@link HiveResultSetMetaData#hiveTypeToSqlType}.
+   * This method is kept consistent with HiveResultSetMetaData#hiveTypeToSqlType .
    */
   public static Object toThriftPayload(Object val, ObjectInspector valOI, int version) {
     if (valOI.getCategory() == ObjectInspector.Category.PRIMITIVE) {

File: serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroLazyObjectInspector.java
Patch:
@@ -110,7 +110,7 @@ public void setReaderSchema(Schema readerSchema) {
   /**
    * Set the {@link AvroSchemaRetriever} for the {@link AvroLazyObjectInspector} to the given class
    *
-   * @param scheamRetrieverClass the schema retriever class to be set
+   * @param schemaRetriever the schema retriever class to be set
    * */
   public void setSchemaRetriever(AvroSchemaRetriever schemaRetriever) {
     this.schemaRetriever = schemaRetriever;

File: serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSchemaRetriever.java
Patch:
@@ -22,7 +22,7 @@
 /**
  * Retrieves the avro schema from the given source. "Source" is a little loose term here in the
  * sense it can range from being an HDFS url location pointing to the schema or it can be even as
- * simple as a {@link Properties properties} file with a simple key-value mapping to the schema. For
+ * simple as a {@link java.util.Properties properties} file with a simple key-value mapping to the schema. For
  * cases where the {@link Schema schema} is a part of the serialized data itself, "Source" would
  * refer to the data bytes from which the {@link Schema schema} has to retrieved.
  *

File: serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarSerDe.java
Patch:
@@ -88,7 +88,7 @@ public ColumnarSerDe() throws SerDeException {
   /**
    * Initialize the SerDe given the parameters.
    *
-   * @see AbstractSerDe#initialize(Configuration, Properties)
+   * @see org.apache.hadoop.hive.serde2.AbstractSerDe#initialize(Configuration, Properties)
    */
   @Override
   public void initialize(Configuration conf, Properties tbl) throws SerDeException {
@@ -123,7 +123,7 @@ public void initialize(Configuration conf, Properties tbl) throws SerDeException
    * @param objInspector
    *          The ObjectInspector for the row object
    * @return The serialized Writable object
-   * @see AbstractSerDe#serialize(Object, ObjectInspector)
+   * @see org.apache.hadoop.hive.serde2.AbstractSerDe#serialize(Object, ObjectInspector)
    */
   @Override
   public Writable serialize(Object obj, ObjectInspector objInspector) throws SerDeException {

File: serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarStructBase.java
Patch:
@@ -183,7 +183,7 @@ public Object getField(int fieldID) {
    * @param length
    *          the length
    *
-   * @return -1 for null, >=0 for length
+   * @return -1 for null, &gt;=0 for length
    */
   protected abstract int getLength(ObjectInspector objectInspector,
       ByteArrayRef cachedByteArrayRef, int start, int length);

File: serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeStructBase.java
Patch:
@@ -86,7 +86,7 @@ public Object deserialize(Object reuse, TProtocol iprot)
    * @param o
    *          - this list should be in the order of the function's params for
    *          now. If we wanted to remove this requirement, we'd need to make it
-   *          a List<Pair<String, Object>> with the String being the field name.
+   *          a List&lt;Pair&lt;String, Object&gt;&gt; with the String being the field name.
    * 
    */
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/ParseException.java
Patch:
@@ -19,7 +19,7 @@ public class ParseException extends Exception {
    * to indicate that this constructor was used to create this object. This
    * constructor calls its super class with the empty string to force the
    * "toString" method of parent class "Throwable" to print the error message in
-   * the form: ParseException: <result of getMessage>
+   * the form: ParseException: &lt;result of getMessage&gt;
    */
   public ParseException(Token currentTokenVal,
       int[][] expectedTokenSequencesVal, String[] tokenImageVal) {

File: serde/src/java/org/apache/hadoop/hive/serde2/fast/DeserializeRead.java
Patch:
@@ -60,11 +60,11 @@ public abstract class DeserializeRead {
    *
    * if (deserializeRead.readNextField()) {
    *   if (deserializeRead.currentExternalBufferNeeded) {
-   *     <Ensure external buffer is as least deserializeRead.currentExternalBufferNeededLen bytes>
+   *     &lt;Ensure external buffer is as least deserializeRead.currentExternalBufferNeededLen bytes&gt;
    *     deserializeRead.copyToExternalBuffer(externalBuffer, externalBufferStart);
    *   } else {
-   *     <Otherwise, field data is available in the currentBytes, currentBytesStart, and
-   *      currentBytesLength of deserializeRead>
+   *     &lt;Otherwise, field data is available in the currentBytes, currentBytesStart, and
+   *      currentBytesLength of deserializeRead&gt;
    *   }
    *
    * @param typeInfos

File: serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java
Patch:
@@ -47,9 +47,9 @@
  * The fractional portion is reversed, and encoded as a VInt
  * so timestamps with less precision use fewer bytes.
  *
- *      0.1    -> 1
- *      0.01   -> 10
- *      0.001  -> 100
+ *      0.1    -&gt; 1
+ *      0.01   -&gt; 10
+ *      0.001  -&gt; 100
  *
  */
 public class TimestampWritable implements WritableComparable<TimestampWritable> {

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyDate.java
Patch:
@@ -77,7 +77,7 @@ public void init(ByteArrayRef bytes, int start, int length) {
    * Writes a Date in SQL date format to the output stream.
    * @param out
    *          The output stream
-   * @param i
+   * @param d
    *          The Date to write
    * @throws IOException
    */

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java
Patch:
@@ -126,7 +126,7 @@ public static void writeUTF8(
   /**
    * Writes HiveDecimalWritable object to output stream as string
    * @param outputStream
-   * @param hiveDecimal
+   * @param hiveDecimalWritable
    * @throws IOException
    */
   public static void writeUTF8(

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryArray.java
Patch:
@@ -30,7 +30,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 
 /**
- * LazyBinaryArray is serialized as follows: start A b b b b b b end bytes[] ->
+ * LazyBinaryArray is serialized as follows: start A b b b b b b end bytes[] -&gt;
  * |--------|---|---|---|---| ... |---|---|
  * 
  * Section A is the null-bytes. Suppose the list has N elements, then there are

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryMap.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 
 /**
- * LazyBinaryMap is serialized as follows: start A b c b c b c end bytes[] ->
+ * LazyBinaryMap is serialized as follows: start A b c b c b c end bytes[] -&gt;
  * |--------|---|---|---|---| ... |---|---|
  * 
  * Section A is the null-bytes. Suppose the map has N key-value pairs, then

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java
Patch:
@@ -353,7 +353,7 @@ public static void writeToByteStream(RandomAccessOutput byteStream,
    * @param byteStream
    * @param dec
    * @param scratchLongs
-   * @param buffer
+   * @param scratchBytes
    */
   public static void writeToByteStream(
       RandomAccessOutput byteStream,
@@ -379,9 +379,9 @@ public static void writeToByteStream(
   * And, allocate scratch buffer with HiveDecimal.SCRATCH_BUFFER_LEN_BIG_INTEGER_BYTES bytes.
   *
   * @param byteStream
-  * @param dec
+  * @param decWritable
   * @param scratchLongs
-  * @param buffer
+  * @param scratchBytes
   */
   public static void writeToByteStream(
       RandomAccessOutput byteStream,

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryString.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hadoop.io.Text;
 
 /**
- * The serialization of LazyBinaryString is very simple: start A end bytes[] ->
+ * The serialization of LazyBinaryString is very simple: start A end bytes[] -&gt;
  * |---------------------------------|
  * 
  * Section A is just an array of bytes which are exactly the Text contained in

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryStruct.java
Patch:
@@ -35,7 +35,7 @@
 import org.apache.hadoop.io.BinaryComparable;
 
 /**
- * LazyBinaryStruct is serialized as follows: start A B A B A B end bytes[] ->
+ * LazyBinaryStruct is serialized as follows: start A B A B A B end bytes[] -&gt;
  * |-----|---------|--- ... ---|-----|---------|
  *
  * Section A is one null-byte, corresponding to eight struct fields in Section

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryUnion.java
Patch:
@@ -29,7 +29,7 @@
   import org.apache.hadoop.hive.serde2.objectinspector.*;
 
 /**
- * LazyBinaryUnion is serialized as follows: start TAG FIELD end bytes[] ->
+ * LazyBinaryUnion is serialized as follows: start TAG FIELD end bytes[] -&gt;
  * |-----|---------|--- ... ---|-----|---------|
  *
  * Section TAG is one byte, corresponding to tag of set union field

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryUtils.java
Patch:
@@ -100,7 +100,7 @@ public static short byteArrayToShort(byte[] b, int offset) {
   /**
    * Record is the unit that data is serialized in. A record includes two parts.
    * The first part stores the size of the element and the second part stores
-   * the real element. size element record -> |----|-------------------------|
+   * the real element. size element record -&gt; |----|-------------------------|
    *
    * A RecordInfo stores two information of a record, the size of the "size"
    * part which is the element offset and the size of the element part which is

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspector.java
Patch:
@@ -50,7 +50,7 @@ public static enum Category {
    * ObjectInspector. This is used to display the type information to the user.
    *
    * For primitive types, the type name is standardized. For other types, the
-   * type name can be something like "list<int>", "map<int,string>", java class
+   * type name can be something like "list&lt;int&gt;", "map&lt;int,string&gt;", java class
    * names, or user-defined type names similar to typedef.
    */
   String getTypeName();

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
Patch:
@@ -1385,8 +1385,8 @@ public static Boolean hasAllFieldsSettable(ObjectInspector oi) {
    *
    * @param oi - Input object inspector
    * @param oiSettableProperties - Lookup map to cache the result.(If no caching, pass null)
-   * @return - true if : (1) oi is an instance of settable<DataType>OI.
-   *                     (2) All the embedded object inspectors are instances of settable<DataType>OI.
+   * @return - true if : (1) oi is an instance of settable&lt;DataType&gt;OI.
+   *                     (2) All the embedded object inspectors are instances of settable&lt;DataType&gt;OI.
    *           If (1) or (2) is false, return false.
    */
   public static boolean hasAllFieldsSettable(ObjectInspector oi,

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorFactory.java
Patch:
@@ -280,7 +280,7 @@ public static AbstractPrimitiveWritableObjectInspector getPrimitiveWritableObjec
 
   /**
    * Returns the PrimitiveWritableObjectInspector for the given type info
-   * @param PrimitiveTypeInfo    PrimitiveTypeInfo instance
+   * @param typeInfo PrimitiveTypeInfo instance
    * @return AbstractPrimitiveWritableObjectInspector instance
    */
   public static AbstractPrimitiveWritableObjectInspector getPrimitiveWritableObjectInspector(
@@ -316,7 +316,7 @@ public static AbstractPrimitiveWritableObjectInspector getPrimitiveWritableObjec
    * Returns a PrimitiveWritableObjectInspector which implements ConstantObjectInspector
    * for the PrimitiveCategory.
    *
-   * @param primitiveCategory
+   * @param typeInfo
    * @param value
    */
   public static ConstantObjectInspector getPrimitiveWritableConstantObjectInspector(
@@ -385,7 +385,7 @@ public static AbstractPrimitiveJavaObjectInspector getPrimitiveJavaObjectInspect
 
   /**
    * Returns the PrimitiveJavaObjectInspector for the given PrimitiveTypeInfo instance,
-   * @param PrimitiveTypeInfo    PrimitiveTypeInfo instance
+   * @param typeInfo PrimitiveTypeInfo instance
    * @return AbstractPrimitiveJavaObjectInspector instance
    */
   public static AbstractPrimitiveJavaObjectInspector getPrimitiveJavaObjectInspector(

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
Patch:
@@ -158,7 +158,7 @@ private static TypeInfo getExtendedTypeInfoFromJavaType(Type t, Method m) {
 
   /**
    * Returns the array element type, if the Type is an array (Object[]), or
-   * GenericArrayType (Map<String,String>[]). Otherwise return null.
+   * GenericArrayType (Map&lt;String,String&gt;[]). Otherwise return null.
    */
   public static Type getArrayElementType(Type t) {
     if (t instanceof Class && ((Class<?>) t).isArray()) {
@@ -176,7 +176,7 @@ public static Type getArrayElementType(Type t) {
    *
    * @param size
    *          In case the last parameter of Method is an array, we will try to
-   *          return a List<TypeInfo> with the specified size by repeating the
+   *          return a List&lt;TypeInfo&gt; with the specified size by repeating the
    *          element of the array at the end. In case the size is smaller than
    *          the minimum possible number of arguments for the method, null will
    *          be returned.

File: common/src/java/org/apache/hadoop/hive/common/JavaUtils.java
Patch:
@@ -138,14 +138,14 @@ public static void closeClassLoader(ClassLoader loader) throws IOException {
 
   /**
    * Utility method for ACID to normalize logging info.  Matches
-   * {@link org.apache.hadoop.hive.metastore.api.LockRequest#toString()}
+   * org.apache.hadoop.hive.metastore.api.LockRequest#toString
    */
   public static String lockIdToString(long extLockId) {
     return "lockid:" + extLockId;
   }
   /**
    * Utility method for ACID to normalize logging info.  Matches
-   * {@link org.apache.hadoop.hive.metastore.api.LockResponse#toString()}
+   * org.apache.hadoop.hive.metastore.api.LockResponse#toString
    */
   public static String txnIdToString(long txnId) {
     return "txnid:" + txnId;

File: common/src/java/org/apache/hadoop/hive/common/StatsSetupConst.java
Patch:
@@ -110,18 +110,18 @@ public String getAggregator(Configuration conf) {
 
   public static final String STATS_FILE_PREFIX = "tmpstats-";
   /**
-   * @return List of all supported statistics
+   * List of all supported statistics
    */
   public static final String[] supportedStats = {NUM_FILES,ROW_COUNT,TOTAL_SIZE,RAW_DATA_SIZE};
 
   /**
-   * @return List of all statistics that need to be collected during query execution. These are
+   * List of all statistics that need to be collected during query execution. These are
    * statistics that inherently require a scan of the data.
    */
   public static final String[] statsRequireCompute = new String[] {ROW_COUNT,RAW_DATA_SIZE};
 
   /**
-   * @return List of statistics that can be collected quickly without requiring a scan of the data.
+   * List of statistics that can be collected quickly without requiring a scan of the data.
    */
   public static final String[] fastStats = new String[] {NUM_FILES,TOTAL_SIZE};
 

File: common/src/java/org/apache/hadoop/hive/common/ValidCompactorTxnList.java
Patch:
@@ -32,7 +32,7 @@
  * open transaction when choosing which files to compact, but that it still ignores aborted
  * records when compacting.
  * 
- * See {@link org.apache.hadoop.hive.metastore.txn.TxnUtils#createValidCompactTxnList()} for proper
+ * See org.apache.hadoop.hive.metastore.txn.TxnUtils#createValidCompactTxnList() for proper
  * way to construct this.
  */
 public class ValidCompactorTxnList extends ValidReadTxnList {
@@ -70,7 +70,7 @@ public ValidCompactorTxnList(String value) {
     super(value);
   }
   /**
-   * Returns {@link org.apache.hadoop.hive.common.ValidTxnList.RangeResponse.ALL} if all txns in
+   * Returns org.apache.hadoop.hive.common.ValidTxnList.RangeResponse.ALL if all txns in
    * the range are resolved and RangeResponse.NONE otherwise
    */
   @Override

File: common/src/java/org/apache/hadoop/hive/common/classification/RetrySemantics.java
Patch:
@@ -28,7 +28,6 @@
  * Initially meant for Metastore API when made across a network, i.e. asynchronously where
  * the response may not reach the caller and thus it cannot know if the operation was actually
  * performed on the server.
- * @see RetryingMetastoreClient
  */
 @InterfaceStability.Evolving
 @InterfaceAudience.LimitedPrivate("Hive developer")

File: common/src/java/org/apache/hadoop/hive/common/cli/CommonCliOptions.java
Patch:
@@ -35,8 +35,8 @@
  * all your own options or processing instructions), parse, and then use
  * the resulting information.
  * <p>
- * See {@link org.apache.hadoop.hive.service.HiveServer} or
- *     {@link org.apache.hadoop.hive.metastore.HiveMetaStore}
+ * See org.apache.hadoop.hive.service.HiveServer or
+ *     org.apache.hadoop.hive.metastore.HiveMetaStore
  *     for examples of use.
  *
  */

File: common/src/java/org/apache/hadoop/hive/common/metrics/common/MetricsVariable.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.hadoop.hive.common.metrics.common;
 
 /**
- * Interface for metrics variables. <p/> For example a the database service could expose the number of
+ * Interface for metrics variables. For example a the database service could expose the number of
  * currently active connections.
  */
 public interface MetricsVariable<T> {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConfUtil.java
Patch:
@@ -151,6 +151,7 @@ public int compare(Map.Entry<String, String> ent, Map.Entry<String, String> ent2
    * password is through a file which stores the password in clear-text which needs to be readable
    * by all the consumers and therefore is not supported.
    *
+   *<ul>
    * <li>If HIVE_SERVER2_JOB_CREDENTIAL_PROVIDER_PATH is set in the hive configuration this method
    * overrides the MR job configuration property hadoop.security.credential.provider.path with its
    * value. If not set then it does not change the value of hadoop.security.credential.provider.path
@@ -161,7 +162,7 @@ public int compare(Map.Entry<String, String> ent, Map.Entry<String, String> ent2
    *   (2) If password is not set using (1) above we use HADOOP_CREDSTORE_PASSWORD if it is set.
    *   (3) If none of those are set, we do not set any password in the MR task environment. In this
    *       case the hadoop credential provider should use the default password of "none" automatically
-   *
+   *</ul>
    * @param jobConf - job specific configuration
    */
   public static void updateJobCredentialProviders(Configuration jobConf) {

File: common/src/java/org/apache/hive/common/util/HiveStringUtils.java
Patch:
@@ -426,7 +426,7 @@ public static Collection<String> getStringCollection(String str){
 
   /**
    * Splits a comma separated value <code>String</code>, trimming leading and trailing whitespace on each value.
-   * @param str a comma separated <String> with values
+   * @param str a comma separated <code>String</code> with values
    * @return a <code>Collection</code> of <code>String</code> values
    */
   public static Collection<String> getTrimmedStringCollection(String str){
@@ -436,7 +436,7 @@ public static Collection<String> getTrimmedStringCollection(String str){
 
   /**
    * Splits a comma separated value <code>String</code>, trimming leading and trailing whitespace on each value.
-   * @param str a comma separated <String> with values
+   * @param str a comma separated <code>String</code> with values
    * @return an array of <code>String</code> values
    */
   public static String[] getTrimmedStrings(String str){

File: common/src/java/org/apache/hive/common/util/ShutdownHookManager.java
Patch:
@@ -89,7 +89,7 @@ public static boolean removeShutdownHook(Runnable shutdownHook) {
   /**
    * register file to delete-on-exit hook
    *
-   * @see {@link org.apache.hadoop.hive.common.FileUtils#createTempFile}
+   * {@link org.apache.hadoop.hive.common.FileUtils#createTempFile}
    */
   public static void deleteOnExit(File file) {
     if (MGR.isShutdownInProgress()) {

File: common/src/java/org/apache/hive/http/JMXJsonServlet.java
Patch:
@@ -70,7 +70,7 @@
  * <p>
  * The optional <code>get</code> parameter is used to query a specific
  * attribute of a JMX bean.  The format of the URL is
- * <code>http://.../jmx?get=MXBeanName::AttributeName<code>
+ * <code>http://.../jmx?get=MXBeanName::AttributeName</code>
  * <p>
  * For example 
  * <code>
@@ -85,7 +85,7 @@
  * <p>
  * The return format is JSON and in the form
  * <p>
- *  <code><pre>
+ *  <code>
  *  {
  *    "beans" : [
  *      {
@@ -94,7 +94,7 @@
  *      }
  *    ]
  *  }
- *  </pre></code>
+ *  </code>
  *  <p>
  *  The servlet attempts to convert the JMXBeans into JSON. Each
  *  bean's attributes will be converted to a JSON object member.

File: shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
Patch:
@@ -168,7 +168,6 @@ enum JobTrackerState { INITIALIZING, RUNNING };
    * All updates to jobtracker/resource manager rpc address
    * in the configuration should be done through this shim
    * @param conf
-   * @return
    */
   public void setJobLauncherRpcAddress(Configuration conf, String val);
 
@@ -252,12 +251,12 @@ BlockLocation[] getLocations(FileSystem fs,
 
   /**
    * For the block locations returned by getLocations() convert them into a Treemap
-   * <Offset,blockLocation> by iterating over the list of blockLocation.
+   * &lt;Offset,blockLocation&gt; by iterating over the list of blockLocation.
    * Using TreeMap from offset to blockLocation, makes it O(logn) to get a particular
    * block based upon offset.
    * @param fs the file system
    * @param status the file information
-   * @return TreeMap<Long, BlockLocation>
+   * @return TreeMap&lt;Long, BlockLocation&gt;
    * @throws IOException
    */
   TreeMap<Long, BlockLocation> getLocationsWithOffset(FileSystem fs,

File: shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java
Patch:
@@ -169,9 +169,9 @@ public static class Client {
     /**
      * Create a client-side SASL transport that wraps an underlying transport.
      *
-     * @param method The authentication method to use. Currently only KERBEROS is
+     * @param methodStr The authentication method to use. Currently only KERBEROS is
      *               supported.
-     * @param serverPrincipal The Kerberos principal of the target server.
+     * @param principalConfig The Kerberos principal of the target server.
      * @param underlyingTransport The underlying transport mechanism, usually a TSocket.
      * @param saslProps the sasl properties to create the client with
      */

File: shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java
Patch:
@@ -184,7 +184,7 @@ public static void decodeWritable(Writable w, String idStr) throws IOException {
 
   /**
    * Synchronize master key updates / sequence generation for multiple nodes.
-   * NOTE: {@Link AbstractDelegationTokenSecretManager} keeps currentKey private, so we need
+   * NOTE: {@link AbstractDelegationTokenSecretManager} keeps currentKey private, so we need
    * to utilize this "hook" to manipulate the key through the object reference.
    * This .20S workaround should cease to exist when Hadoop supports token store.
    */

File: metastore/src/java/org/apache/hadoop/hive/metastore/cache/CacheUtils.java
Patch:
@@ -103,7 +103,7 @@ public static boolean matches(String name, String pattern) {
     String[] subpatterns = pattern.trim().split("\\|");
     for (String subpattern : subpatterns) {
       subpattern = "(?i)" + subpattern.replaceAll("\\?", ".{1}").replaceAll("\\*", ".*")
-          .replaceAll("\\^", "\\\\^").replaceAll("\\$", "\\\\$");;
+          .replaceAll("\\^", "\\\\^").replaceAll("\\$", "\\\\$");
       if (Pattern.matches(subpattern, HiveStringUtils.normalizeIdentifier(name))) {
         return true;
       }

File: metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseUtils.java
Patch:
@@ -653,7 +653,7 @@ public static byte[] hashStorageDescriptor(StorageDescriptor sd, MessageDigest m
       }
     }
     if (sd.getBucketCols() != null) {
-      SortedSet<String> bucketCols = new TreeSet<>(sd.getBucketCols());
+      List<String> bucketCols = new ArrayList<>(sd.getBucketCols());
       for (String bucket : bucketCols) md.update(bucket.getBytes(ENCODING));
     }
     if (sd.getSortCols() != null) {
@@ -688,6 +688,7 @@ public static byte[] hashStorageDescriptor(StorageDescriptor sd, MessageDigest m
           md.update(e.getValue().getBytes(ENCODING));
         }
       }
+      md.update(sd.isStoredAsSubDirectories() ? "true".getBytes(ENCODING) : "false".getBytes(ENCODING));
     }
 
     return md.digest();

File: common/src/test/org/apache/hadoop/hive/common/TestLogUtils.java
Patch:
@@ -1,4 +1,3 @@
-package org.apache.hadoop.hive.common;
 /**
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -16,6 +15,9 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+ 
+package org.apache.hadoop.hive.common;
+
 import org.junit.Assert;
 import org.junit.Test;
 

File: itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/thrift/TestHadoopAuthBridge23.java
Patch:
@@ -1,4 +1,3 @@
-package org.apache.hadoop.hive.thrift;
 /**
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -16,7 +15,8 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
+ 
+package org.apache.hadoop.hive.thrift;
 
 
 import org.apache.hadoop.conf.Configuration;

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
Patch:
@@ -1,5 +1,3 @@
-package org.apache.hadoop.hive.ql.history;
-
 /**
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -17,6 +15,9 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+ 
+package org.apache.hadoop.hive.ql.history;
+
 
 import java.io.PrintStream;
 import java.io.UnsupportedEncodingException;

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CorePerfCliDriver.java
Patch:
@@ -1,4 +1,3 @@
-package org.apache.hadoop.hive.cli.control;
 /**
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -16,6 +15,8 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+ 
+package org.apache.hadoop.hive.cli.control;
 
 
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/llap/ext/TestLlapInputSplit.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hadoop.hive.llap.LlapInputSplit;
 import org.apache.hadoop.hive.llap.Schema;
 import org.apache.hadoop.hive.llap.FieldDesc;
-import org.apache.hadoop.hive.llap.TypeDesc;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 import org.apache.hadoop.mapred.SplitLocationInfo;
 import org.junit.Test;
@@ -46,8 +46,8 @@ public void testWritable() throws Exception {
     };
 
     ArrayList<FieldDesc> colDescs = new ArrayList<FieldDesc>();
-    colDescs.add(new FieldDesc("col1", new TypeDesc(TypeDesc.Type.STRING)));
-    colDescs.add(new FieldDesc("col2", new TypeDesc(TypeDesc.Type.INT)));
+    colDescs.add(new FieldDesc("col1", TypeInfoFactory.stringTypeInfo));
+    colDescs.add(new FieldDesc("col2", TypeInfoFactory.intTypeInfo));
     Schema schema = new Schema(colDescs);
 
     byte[] tokenBytes = new byte[] { 1 };

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
Patch:
@@ -60,7 +60,6 @@
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.hadoop.hive.llap.TypeDesc;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
 import org.apache.hadoop.hive.ql.exec.SerializationUtilities;
 import org.apache.hadoop.hive.ql.exec.Utilities;

File: druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidQueryBasedInputFormat.java
Patch:
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.net.URL;
+import java.net.URLEncoder;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashMap;
@@ -213,7 +214,7 @@ private static HiveDruidSplit[] distributeSelectQuery(Configuration conf, String
             StringUtils.join(query.getIntervals(), ","); // Comma-separated intervals without brackets
     final String request = String.format(
             "http://%s/druid/v2/datasources/%s/candidates?intervals=%s",
-            address, query.getDataSource().getNames().get(0), intervals);
+            address, query.getDataSource().getNames().get(0), URLEncoder.encode(intervals, "UTF-8"));
     final InputStream response;
     try {
       response = DruidStorageHandlerUtils.submitRequest(client, new Request(HttpMethod.GET, new URL(request)));

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2894,7 +2894,7 @@ public static enum ConfVars {
             "Bloom filter should be of at max certain size to be effective"),
     TEZ_BLOOM_FILTER_FACTOR("hive.tez.bloom.filter.factor", (float) 2.0,
             "Bloom filter should be a multiple of this factor with nDV"),
-    TEZ_BIGTABLE_MIN_SIZE_SEMIJOIN_REDUCTION("hive.tez.bigtable.minsize.semijoin.reduction", 1000000L,
+    TEZ_BIGTABLE_MIN_SIZE_SEMIJOIN_REDUCTION("hive.tez.bigtable.minsize.semijoin.reduction", 100000000L,
             "Big table for runtime filteting should be of atleast this size"),
     TEZ_DYNAMIC_SEMIJOIN_REDUCTION_THRESHOLD("hive.tez.dynamic.semijoin.reduction.threshold", (float) 0.50,
             "Only perform semijoin optimization if the estimated benefit at or above this fraction of the target table"),

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java
Patch:
@@ -124,7 +124,8 @@ public BuddyAllocator(boolean isDirectVal, boolean isMappedVal, int minAllocVal,
     } else {
       cacheDir = null;
     }
-    int arenaSizeVal = (arenaCount == 0) ? MAX_ARENA_SIZE : (int)(maxSizeVal / arenaCount);
+    long arenaSizeVal = (arenaCount == 0) ? MAX_ARENA_SIZE : maxSizeVal / arenaCount;
+    // The math.min, and the fact that maxAllocation is an int, ensures we don't overflow.
     arenaSizeVal = Math.max(maxAllocation, Math.min(arenaSizeVal, MAX_ARENA_SIZE));
     if (LlapIoImpl.LOG.isInfoEnabled()) {
       LlapIoImpl.LOG.info("Buddy allocator with " + (isDirect ? "direct" : "byte") + " buffers; "
@@ -153,7 +154,7 @@ public BuddyAllocator(boolean isDirectVal, boolean isMappedVal, int minAllocVal,
       LlapIoImpl.LOG.warn("Rounding arena size to " + arenaSizeVal + " from " + oldArenaSize
           + " to be divisible by allocation size " + maxAllocation);
     }
-    arenaSize = arenaSizeVal;
+    arenaSize = (int)arenaSizeVal;
     if ((maxSizeVal % arenaSize) > 0) {
       long oldMaxSize = maxSizeVal;
       maxSizeVal = (maxSizeVal / arenaSize) * arenaSize;

File: druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidOutputFormat.java
Patch:
@@ -241,6 +241,6 @@ public RecordWriter<K, DruidWritable> getRecordWriter(
 
   @Override
   public void checkOutputSpecs(FileSystem ignored, JobConf job) throws IOException {
-    throw new UnsupportedOperationException("not implemented yet");
+    // NOOP
   }
 }

File: llap-server/src/java/org/apache/hadoop/hive/llap/configuration/LlapDaemonConfiguration.java
Patch:
@@ -36,7 +36,7 @@ public class LlapDaemonConfiguration extends Configuration {
   public static final String[] SSL_DAEMON_CONFIGS = { "ssl-server.xml" };
 
   public LlapDaemonConfiguration() {
-    super(false);
+    super(true); // Load the defaults.
     for (String conf : DAEMON_CONFIGS) {
       addResource(conf);
     }

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java
Patch:
@@ -79,9 +79,9 @@ public ReadPipeline createReadPipeline(
     cacheMetrics.incrCacheReadRequests();
     OrcEncodedDataConsumer edc = new OrcEncodedDataConsumer(consumer, columnIds.size(),
         _skipCorrupt, counters, ioMetrics);
-    // Note: we use global conf here and ignore JobConf.
-    OrcEncodedDataReader reader = new OrcEncodedDataReader(lowLevelCache, bufferManager,
-        metadataCache, conf, split, columnIds, sarg, columnNames, edc, counters, readerSchema);
+    OrcEncodedDataReader reader = new OrcEncodedDataReader(
+        lowLevelCache, bufferManager, metadataCache, conf, job, split, columnIds, sarg,
+        columnNames, edc, counters, readerSchema);
     edc.init(reader, reader);
     return edc;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -7154,7 +7154,9 @@ protected Operator genFileSinkPlan(String dest, QB qb, Operator input)
       throw new SemanticException("Unknown destination type: " + dest_type);
     }
 
-    input = genConversionSelectOperator(dest, qb, input, table_desc, dpCtx);
+    if (!(dest_type.intValue() == QBMetaData.DEST_DFS_FILE && qb.getIsQuery())) {
+      input = genConversionSelectOperator(dest, qb, input, table_desc, dpCtx);
+    }
 
     inputRR = opParseCtx.get(input).getRowResolver();
 

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -170,7 +170,7 @@ public class QTestUtil {
   private final Set<String> qNoSessionReuseQuerySet;
   private final Set<String> qJavaVersionSpecificOutput;
   private static final String SORT_SUFFIX = ".sorted";
-  private final HashSet<String> srcTables;
+  private final Set<String> srcTables;
   private final Set<String> srcUDFs;
   private final MiniClusterType clusterType;
   private final FsType fsType;
@@ -203,7 +203,7 @@ public interface SuiteAddTestFunctor {
   }
   private HBaseTestingUtility utility;
 
-  HashSet<String> getSrcTables() {
+  public static Set<String> getSrcTables() {
     HashSet<String> srcTables = new HashSet<String>();
     // FIXME: moved default value to here...for now
     // i think this features is never really used from the command line

File: itests/util/src/main/java/org/apache/hive/beeline/qfile/package-info.java
Patch:
@@ -17,6 +17,6 @@
  */
 
 /**
- * Package for the BeeLine specific QTest file classes.
+ * Package for the BeeLine specific QTest classes.
  */
-package org.apache.hive.beeline.qfile;
+package org.apache.hive.beeline;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -535,7 +535,7 @@ public static List<String> getFieldSchemaString(List<FieldSchema> fl) {
   public static void setMapRedWork(Configuration conf, MapredWork w, Path hiveScratchDir) {
     String useName = conf.get(INPUT_NAME);
     if (useName == null) {
-      useName = "mapreduce";
+      useName = "mapreduce:" + hiveScratchDir;
     }
     conf.set(INPUT_NAME, useName);
     setMapWork(conf, w.getMapWork(), hiveScratchDir, true);

File: common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/CodahaleReporter.java
Patch:
@@ -20,7 +20,7 @@
 import com.codahale.metrics.Reporter;
 import java.io.Closeable;
 
-interface CodahaleReporter extends Closeable, Reporter {
+public interface CodahaleReporter extends Closeable, Reporter {
 
   /**
    * Start the reporter.

File: metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -3139,15 +3139,16 @@ private static synchronized void setupJdbcConnectionPool(HiveConf conf) throws S
     String passwd = getMetastoreJdbcPasswd(conf);
     String connectionPooler = conf.getVar(
       HiveConf.ConfVars.METASTORE_CONNECTION_POOLING_TYPE).toLowerCase();
+    int maxPoolSize = conf.getIntVar(HiveConf.ConfVars.METASTORE_CONNECTION_POOLING_MAX_CONNECTIONS);
 
     if ("bonecp".equals(connectionPooler)) {
       BoneCPConfig config = new BoneCPConfig();
       config.setJdbcUrl(driverUrl);
       //if we are waiting for connection for 60s, something is really wrong
       //better raise an error than hang forever
       config.setConnectionTimeoutInMs(60000);
-      config.setMaxConnectionsPerPartition(10);
       config.setPartitionCount(1);
+      config.setMaxConnectionsPerPartition(maxPoolSize);
       config.setUser(user);
       config.setPassword(passwd);
       connPool = new BoneCPDataSource(config);
@@ -3162,6 +3163,7 @@ private static synchronized void setupJdbcConnectionPool(HiveConf conf) throws S
       connPool = new PoolingDataSource(objectPool);
     } else if ("hikaricp".equals(connectionPooler)) {
       HikariConfig config = new HikariConfig();
+      config.setMaximumPoolSize(maxPoolSize);
       config.setJdbcUrl(driverUrl);
       config.setUsername(user);
       config.setPassword(passwd);

File: common/src/java/org/apache/hadoop/hive/common/LogUtils.java
Patch:
@@ -24,13 +24,13 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
-import org.apache.log4j.MDC;
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.core.config.Configurator;
 import org.apache.logging.log4j.core.impl.Log4jContextFactory;
 import org.apache.logging.log4j.spi.DefaultThreadContextMap;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import org.slf4j.MDC;
 
 import com.google.common.annotations.VisibleForTesting;
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
Patch:
@@ -48,7 +48,6 @@
 import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
-import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDefaultDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc;
@@ -554,7 +553,7 @@ public static boolean prunePartitionNames(List<String> partColumnNames,
         PrimitiveTypeInfo typeInfo = partColumnTypeInfos.get(i);
 
         if (partitionValue.equals(defaultPartitionName)) {
-          convertedValues.add(new ExprNodeConstantDefaultDesc(typeInfo, defaultPartitionName));
+          convertedValues.add(null); // Null for default partition.
         } else {
           Object o = ObjectInspectorConverters.getConverter(
               PrimitiveObjectInspectorFactory.javaStringObjectInspector,

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java
Patch:
@@ -1201,9 +1201,8 @@ private Map<Integer, List<ExprNodeGenericFuncDesc>> genPartSpecs(Table table,
         ;
         PrimitiveTypeInfo pti = TypeInfoFactory.getPrimitiveTypeInfo(type);
         ExprNodeColumnDesc column = new ExprNodeColumnDesc(pti, key, null, true);
-        ExprNodeGenericFuncDesc op =
-            DDLSemanticAnalyzer
-                .makeBinaryPredicate("=", column, new ExprNodeConstantDesc(pti, val));
+        ExprNodeGenericFuncDesc op = DDLSemanticAnalyzer.makeBinaryPredicate(
+            "=", column, new ExprNodeConstantDesc(pti, val));
         expr = (expr == null) ? op : DDLSemanticAnalyzer.makeBinaryPredicate("and", expr, op);
       }
       if (expr != null) {

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecOrcRowGroupCountPrinter.java
Patch:
@@ -60,10 +60,10 @@ public void run(HookContext hookContext) throws Exception {
       if (counters != null) {
         for (CounterGroup group : counters) {
           if (group.getName().equals(LlapIOCounters.class.getName())) {
-            console.printError(tezTask.getId() + " LLAP IO COUNTERS:");
+            console.printInfo(tezTask.getId() + " LLAP IO COUNTERS:", false);
             for (TezCounter counter : group) {
               if (counter.getDisplayName().equals(LlapIOCounters.SELECTED_ROWGROUPS.name())) {
-                console.printError("   " + counter.getDisplayName() + ": " + counter.getValue());
+                console.printInfo("   " + counter.getDisplayName() + ": " + counter.getValue(), false);
               }
             }
           }

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java
Patch:
@@ -116,8 +116,8 @@ public void run(QueryState queryState, Set<ReadEntity> inputs,
     }
 
     if (queryState != null) {
-      console.printError("POSTHOOK: query: " + queryState.getQueryString().trim());
-      console.printError("POSTHOOK: type: " + queryState.getCommandType());
+      console.printInfo("POSTHOOK: query: " + queryState.getQueryString().trim(), false);
+      console.printInfo("POSTHOOK: type: " + queryState.getCommandType(), false);
     }
 
     PreExecutePrinter.printEntities(console, inputs, "POSTHOOK: Input: ");
@@ -167,7 +167,7 @@ public void run(QueryState queryState, Set<ReadEntity> inputs,
         }
         sb.append("]");
 
-        console.printError(sb.toString());
+        console.printInfo(sb.toString(), false);
       }
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java
Patch:
@@ -65,8 +65,8 @@ public void run(QueryState queryState, Set<ReadEntity> inputs,
     }
 
     if (queryState != null) {
-      console.printError("PREHOOK: query: " + queryState.getQueryString().trim());
-      console.printError("PREHOOK: type: " + queryState.getCommandType());
+      console.printInfo("PREHOOK: query: " + queryState.getQueryString().trim(), false);
+      console.printInfo("PREHOOK: type: " + queryState.getCommandType(), false);
     }
 
     printEntities(console, inputs, "PREHOOK: Input: ");
@@ -80,7 +80,7 @@ static void printEntities(LogHelper console, Set<?> entities, String prefix) {
     }
     Collections.sort(strings);
     for (String s : strings) {
-      console.printError(prefix + s);
+      console.printInfo(prefix + s, false);
     }
   }
 }

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelFifoCachePolicy.java
Patch:
@@ -34,7 +34,7 @@ public class LowLevelFifoCachePolicy implements LowLevelCachePolicy {
   private EvictionListener evictionListener;
   private LlapOomDebugDump parentDebugDump;
 
-  public LowLevelFifoCachePolicy(Configuration conf) {
+  public LowLevelFifoCachePolicy() {
     LlapIoImpl.LOG.info("FIFO cache policy");
     buffers = new LinkedList<LlapCacheableBuffer>();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -457,6 +457,7 @@ public final class FunctionRegistry {
     system.registerGenericUDF("struct", GenericUDFStruct.class);
     system.registerGenericUDF("named_struct", GenericUDFNamedStruct.class);
     system.registerGenericUDF("create_union", GenericUDFUnion.class);
+    system.registerGenericUDF("extract_union", GenericUDFExtractUnion.class);
 
     system.registerGenericUDF("case", GenericUDFCase.class);
     system.registerGenericUDF("when", GenericUDFWhen.class);

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/MemoryManager.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.hadoop.hive.llap.cache;
 
 public interface MemoryManager extends LlapOomDebugDump {
-  boolean reserveMemory(long memoryToReserve, boolean waitForEviction);
   void releaseMemory(long memUsage);
   void updateMaxSize(long maxSize);
   /** TODO: temporary method until we get a better allocator. */
   long forceReservedMemory(int allocationSize, int count);
+  void reserveMemory(long memoryToReserve);
 }

File: llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestBuddyAllocator.java
Patch:
@@ -58,8 +58,7 @@ public TestBuddyAllocator(boolean direct, boolean mmap) {
 
   private static class DummyMemoryManager implements MemoryManager {
     @Override
-    public boolean reserveMemory(long memoryToReserve, boolean waitForEviction) {
-      return true;
+    public void reserveMemory(long memoryToReserve) {
     }
 
     @Override

File: llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java
Patch:
@@ -72,9 +72,8 @@ private static class DummyMemoryManager implements MemoryManager {
     int allocs = 0;
 
     @Override
-    public boolean reserveMemory(long memoryToReserve, boolean waitForEviction) {
+    public void reserveMemory(long memoryToReserve) {
       ++allocs;
-      return true;
     }
 
     @Override

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
Patch:
@@ -220,8 +220,8 @@ public String toString() {
 
   @Override
   public void readEncodedColumns(int stripeIx, StripeInformation stripe,
-      OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings, List<OrcProto.Stream> streamList,
-      boolean[] included, boolean[][] colRgs,
+      OrcProto.RowIndex[] indexes, List<OrcProto.ColumnEncoding> encodings,
+      List<OrcProto.Stream> streamList, boolean[] included, boolean[][] colRgs,
       Consumer<OrcEncodedColumnBatch> consumer) throws IOException {
     // Note: for now we don't have to setError here, caller will setError if we throw.
     // We are also not supposed to call setDone, since we are only part of the operation.

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2751,9 +2751,9 @@ public static enum ConfVars {
     HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT("hive.vectorized.use.vectorized.input.format", true,
         "This flag should be set to true to enable vectorizing with vectorized input file format capable SerDe.\n" +
         "The default value is true."),
-    HIVE_VECTORIZATION_USE_VECTOR_DESERIALIZE("hive.vectorized.use.vector.serde.deserialize", false,
+    HIVE_VECTORIZATION_USE_VECTOR_DESERIALIZE("hive.vectorized.use.vector.serde.deserialize", true,
         "This flag should be set to true to enable vectorizing rows using vector deserialize.\n" +
-        "The default value is false."),
+        "The default value is true."),
     HIVE_VECTORIZATION_USE_ROW_DESERIALIZE("hive.vectorized.use.row.serde.deserialize", false,
         "This flag should be set to true to enable vectorizing using row deserialize.\n" +
         "The default value is false."),

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -749,6 +749,7 @@ private boolean verifyAndSetVectorPartDesc(PartitionDesc pd, boolean isAcidTable
       if (!isSchemaEvolution) {
         enabledConditionsNotMetList.add(
             "Vectorizing tables without Schema Evolution requires " + HiveConf.ConfVars.HIVE_VECTORIZATION_USE_VECTORIZED_INPUT_FILE_FORMAT.varname);
+        return false;
       }
 
       String deserializerClassName = pd.getDeserializerClassName();

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCachePolicy.java
Patch:
@@ -28,5 +28,5 @@ public interface LowLevelCachePolicy extends LlapOomDebugDump {
   void setEvictionListener(EvictionListener listener);
   void setParentDebugDumper(LlapOomDebugDump dumper);
   /** TODO: temporary method until we have a better allocator */
-  int tryEvictContiguousData(int allocationSize, int count);
+  long tryEvictContiguousData(int allocationSize, int count);
 }

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/MemoryManager.java
Patch:
@@ -23,5 +23,5 @@ public interface MemoryManager extends LlapOomDebugDump {
   void releaseMemory(long memUsage);
   void updateMaxSize(long maxSize);
   /** TODO: temporary method until we get a better allocator. */
-  void forceReservedMemory(int allocationSize, int count);
+  long forceReservedMemory(int allocationSize, int count);
 }

File: llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestBuddyAllocator.java
Patch:
@@ -76,7 +76,8 @@ public void updateMaxSize(long maxSize) {
     }
 
     @Override
-    public void forceReservedMemory(int allocationSize, int count) {
+    public long forceReservedMemory(int allocationSize, int count) {
+      return allocationSize * count;
     }
   }
 

File: llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelCacheImpl.java
Patch:
@@ -116,8 +116,8 @@ public void setParentDebugDumper(LlapOomDebugDump dumper) {
     }
 
     @Override
-    public int tryEvictContiguousData(int allocationSize, int count) {
-      return count;
+    public long tryEvictContiguousData(int allocationSize, int count) {
+      return count * allocationSize;
     }
   }
 

File: llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java
Patch:
@@ -63,7 +63,7 @@ public void verifyEquals(int i) {
     }
 
     @Override
-    public int tryEvictContiguousData(int allocationSize, int count) {
+    public long tryEvictContiguousData(int allocationSize, int count) {
       return 0;
     }
   }
@@ -92,7 +92,8 @@ public void updateMaxSize(long maxSize) {
     }
 
     @Override
-    public void forceReservedMemory(int allocationSize, int count) {
+    public long forceReservedMemory(int allocationSize, int count) {
+      return allocationSize * count;
     }
   }
 

File: beeline/src/java/org/apache/hive/beeline/BeeLineOpts.java
Patch:
@@ -62,7 +62,7 @@ class BeeLineOpts implements Completer {
   public static final int DEFAULT_MAX_COLUMN_WIDTH = 50;
   public static final int DEFAULT_INCREMENTAL_BUFFER_ROWS = 1000;
 
-  public static String URL_ENV_PREFIX = "BEELINE_URL_";
+  public static final String URL_ENV_PREFIX = "BEELINE_URL_";
 
   private final BeeLine beeLine;
   private boolean autosave = false;

File: beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java
Patch:
@@ -89,7 +89,7 @@ public HiveSchemaTool(String hiveHome, HiveConf hiveConf, String dbType)
     }
     this.hiveConf = hiveConf;
     this.dbType = dbType;
-    this.metaStoreSchemaInfo = new MetaStoreSchemaInfo(hiveHome, hiveConf, dbType);
+    this.metaStoreSchemaInfo = new MetaStoreSchemaInfo(hiveHome, dbType);
     userName = hiveConf.get(ConfVars.METASTORE_CONNECTION_USER_NAME.varname);
     try {
       passWord = ShimLoader.getHadoopShims().getPassword(hiveConf,

File: cli/src/test/org/apache/hadoop/hive/cli/TestRCFileCat.java
Patch:
@@ -25,8 +25,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.io.PrintStream;
-import java.net.URI;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -77,7 +75,7 @@ public void testRCFileCat() throws Exception {
     writer.close();
 
     RCFileCat fileCat = new RCFileCat();
-    RCFileCat.test=true;
+    fileCat.test=true;
     fileCat.setConf(new Configuration());
 
     // set fake input and output streams

File: common/src/java/org/apache/hadoop/hive/common/LogUtils.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.core.LoggerContext;
 import org.apache.logging.log4j.core.config.Configurator;
 import org.apache.logging.log4j.core.impl.Log4jContextFactory;
 import org.slf4j.Logger;
@@ -45,8 +44,8 @@ public class LogUtils {
   /**
    * Constants for log masking
    */
-  private static String KEY_TO_MASK_WITH = "password";
-  private static String MASKED_VALUE = "###_MASKED_###";
+  private static final String KEY_TO_MASK_WITH = "password";
+  private static final String MASKED_VALUE = "###_MASKED_###";
 
   @SuppressWarnings("serial")
   public static class LogInitializationException extends Exception {

File: common/src/java/org/apache/hadoop/hive/common/StatsSetupConst.java
Patch:
@@ -49,7 +49,7 @@
 
 public class StatsSetupConst {
 
-  protected final static Logger LOG = LoggerFactory.getLogger(StatsSetupConst.class.getName());
+  protected static final Logger LOG = LoggerFactory.getLogger(StatsSetupConst.class.getName());
 
   public enum StatDB {
     fs {

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java
Patch:
@@ -189,13 +189,13 @@ public void testVersionCompatibility () throws Exception {
 
   //  write the given version to metastore
   private String getVersion(HiveConf conf) throws HiveMetaException {
-    MetaStoreSchemaInfo schemInfo = new MetaStoreSchemaInfo(metaStoreRoot, conf, "derby");
+    MetaStoreSchemaInfo schemInfo = new MetaStoreSchemaInfo(metaStoreRoot, "derby");
     return getMetaStoreVersion();
   }
 
   //  write the given version to metastore
   private void setVersion(HiveConf conf, String version) throws HiveMetaException {
-    MetaStoreSchemaInfo schemInfo = new MetaStoreSchemaInfo(metaStoreRoot, conf, "derby");
+    MetaStoreSchemaInfo schemInfo = new MetaStoreSchemaInfo(metaStoreRoot, "derby");
     setMetaStoreVersion(version, "setVersion test");
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -774,7 +774,7 @@ public static TypeInfo getCommonClassForUnionAll(TypeInfo a, TypeInfo b) {
    *
    * @return null if no common class could be found.
    */
-  public static TypeInfo getCommonClassForComparison(TypeInfo a, TypeInfo b) {
+  public static synchronized TypeInfo getCommonClassForComparison(TypeInfo a, TypeInfo b) {
     // If same return one of them
     if (a.equals(b)) {
       return a;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
Patch:
@@ -1359,7 +1359,7 @@ private String getNewInstanceArgumentString(Object [] args) {
     return "arguments: " + Arrays.toString(args) + ", argument classes: " + argClasses.toString();
   }
 
-  private static int STACK_LENGTH_LIMIT = 15;
+  private static final int STACK_LENGTH_LIMIT = 15;
 
   public static String getStackTraceAsSingleLine(Throwable e) {
     StringBuilder sb = new StringBuilder();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CuckooSetBytes.java
Patch:
@@ -39,8 +39,8 @@ public class CuckooSetBytes {
   private int salt = 0;
   private Random gen = new Random(676983475);
   private int rehashCount = 0;
-  private static long INT_MASK  = 0x00000000ffffffffL;
-  private static long BYTE_MASK = 0x00000000000000ffL;
+  private static final long INT_MASK  = 0x00000000ffffffffL;
+  private static final long BYTE_MASK = 0x00000000000000ffL;
 
   /**
    * Allocate a new set to hold expectedSize values. Re-allocation to expand

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashTable.java
Patch:
@@ -40,10 +40,10 @@ public abstract class VectorMapJoinFastHashTable implements VectorMapJoinHashTab
   protected int metricExpands;
 
   // 2^30 (we cannot use Integer.MAX_VALUE which is 2^31-1).
-  public static int HIGHEST_INT_POWER_OF_2 = 1073741824;
+  public static final int HIGHEST_INT_POWER_OF_2 = 1073741824;
 
-  public static int ONE_QUARTER_LIMIT = HIGHEST_INT_POWER_OF_2 / 4;
-  public static int ONE_SIXTH_LIMIT = HIGHEST_INT_POWER_OF_2 / 6;
+  public static final int ONE_QUARTER_LIMIT = HIGHEST_INT_POWER_OF_2 / 4;
+  public static final int ONE_SIXTH_LIMIT = HIGHEST_INT_POWER_OF_2 / 6;
 
   public void throwExpandError(int limit, String dataTypeName) {
     throw new RuntimeException(

File: ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryImpl.java
Patch:
@@ -46,13 +46,12 @@
  * Each session uses a new object, which creates a new file.
  */
 public class HiveHistoryImpl implements HiveHistory{
+  private static final Logger LOG = LoggerFactory.getLogger("hive.ql.exec.HiveHistoryImpl");
 
   PrintWriter histStream; // History File stream
 
   String histFileName; // History file name
 
-  private static final Logger LOG = LoggerFactory.getLogger("hive.ql.exec.HiveHistoryImpl");
-
   private static final Random randGen = new Random();
 
   private LogHelper console;
@@ -305,7 +304,7 @@ public void progressTask(String queryId, Task<? extends Serializable> task) {
   /**
    * write out counters.
    */
-  static ThreadLocal<Map<String,String>> ctrMapFactory =
+  static final ThreadLocal<Map<String,String>> ctrMapFactory =
       new ThreadLocal<Map<String, String>>() {
     @Override
     protected Map<String,String> initialValue() {

File: ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndex.java
Patch:
@@ -26,10 +26,8 @@
  * Holds index related constants
  */
 public class HiveIndex {
-
   public static final Logger l4j = LoggerFactory.getLogger("HiveIndex");
-
-  public static String INDEX_TABLE_CREATETIME = "hive.index.basetbl.dfs.lastModifiedTime";
+  public static final String INDEX_TABLE_CREATETIME = "hive.index.basetbl.dfs.lastModifiedTime";
 
   public static enum IndexType {
     AGGREGATE_TABLE("aggregate",  AggregateIndexHandler.class.getName()),

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveFileFormatUtils.java
Patch:
@@ -87,7 +87,7 @@ public final class HiveFileFormatUtils {
   public static class FileChecker {
     // we don't have many file formats that implement InputFormatChecker. We won't be holding
     // multiple instances of such classes
-    private static int MAX_CACHE_SIZE = 16;
+    private static final int MAX_CACHE_SIZE = 16;
 
     // immutable maps
     Map<Class<? extends InputFormat>, Class<? extends InputFormatChecker>> inputFormatCheckerMap;

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
Patch:
@@ -83,14 +83,13 @@
  */
 public class HiveInputFormat<K extends WritableComparable, V extends Writable>
     implements InputFormat<K, V>, JobConfigurable {
-
   private static final String CLASS_NAME = HiveInputFormat.class.getName();
   private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);
 
   /**
    * A cache of InputFormat instances.
    */
-  private static Map<Class, InputFormat<WritableComparable, Writable>> inputFormats
+  private static final Map<Class, InputFormat<WritableComparable, Writable>> inputFormats
     = new ConcurrentHashMap<Class, InputFormat<WritableComparable, Writable>>();
 
   private JobConf job;

File: ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
Patch:
@@ -839,7 +839,7 @@ public static class Writer {
     // the max size of memory for buffering records before writes them out
     private int columnsBufferSize = 4 * 1024 * 1024; // 4M
     // the conf string for COLUMNS_BUFFER_SIZE
-    public static String COLUMNS_BUFFER_SIZE_CONF_STR = "hive.io.rcfile.record.buffer.size";
+    public static final String COLUMNS_BUFFER_SIZE_CONF_STR = "hive.io.rcfile.record.buffer.size";
 
     // how many records already buffered
     private int bufferedRecords = 0;

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/VirtualColumn.java
Patch:
@@ -60,7 +60,7 @@ public enum VirtualColumn {
    */
   GROUPINGID("GROUPING__ID", TypeInfoFactory.intTypeInfo);
 
-  public static ImmutableSet<String> VIRTUAL_COLUMN_NAMES =
+  public static final ImmutableSet<String> VIRTUAL_COLUMN_NAMES =
       ImmutableSet.of(FILENAME.getName(), BLOCKOFFSET.getName(), ROWOFFSET.getName(),
           RAWDATASIZE.getName(), GROUPINGID.getName(), ROWID.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/ListBucketingPrunerUtils.java
Patch:
@@ -37,10 +37,10 @@
 public final class ListBucketingPrunerUtils {
 
   /* Default list bucketing directory name. internal use only not for client. */
-  public static String HIVE_LIST_BUCKETING_DEFAULT_DIR_NAME =
+  public static final String HIVE_LIST_BUCKETING_DEFAULT_DIR_NAME =
       "HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME";
   /* Default list bucketing directory key. internal use only not for client. */
-  public static String HIVE_LIST_BUCKETING_DEFAULT_KEY = "HIVE_DEFAULT_LIST_BUCKETING_KEY";
+  public static final String HIVE_LIST_BUCKETING_DEFAULT_KEY = "HIVE_DEFAULT_LIST_BUCKETING_KEY";
 
   /**
    * Decide if pruner skips the skewed directory

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/VectorizerReason.java
Patch:
@@ -27,7 +27,7 @@
  */
 public class VectorizerReason  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public static enum VectorizerNodeIssue {
     NONE,

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -1808,7 +1808,7 @@ static HashMap<String, String> getProps(ASTNode prop) {
   static class QualifiedNameUtil {
 
     // delimiter to check DOT delimited qualified names
-    static String delimiter = "\\.";
+    static final String delimiter = "\\.";
 
     /**
      * Get the fully qualified name in the ast. e.g. the ast of the form ^(DOT

File: ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java
Patch:
@@ -586,7 +586,7 @@ public static enum WindowType
    */
   public static class BoundarySpec implements Comparable<BoundarySpec>
   {
-    public static int UNBOUNDED_AMOUNT = Integer.MAX_VALUE;
+    public static final int UNBOUNDED_AMOUNT = Integer.MAX_VALUE;
 
     Direction direction;
     int amt;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractVectorDesc.java
Patch:
@@ -18,11 +18,9 @@
 
 package org.apache.hadoop.hive.ql.plan;
 
-import org.apache.hadoop.hive.ql.exec.Operator;
-
 public class AbstractVectorDesc implements VectorDesc {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   private Class<?> vectorOpClass;
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java
Patch:
@@ -50,7 +50,7 @@ public class GroupByDesc extends AbstractOperatorDesc {
    * MERGEPARTIAL: FINAL for non-distinct aggregations, COMPLETE for distinct
    * aggregations.
    */
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   /**
    * Mode.

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java
Patch:
@@ -482,7 +482,7 @@ public void setHasOrderBy(boolean hasOrderBy) {
   }
 
   // Use LinkedHashSet to give predictable display order.
-  private static Set<String> vectorizableReduceSinkNativeEngines =
+  private static final Set<String> vectorizableReduceSinkNativeEngines =
       new LinkedHashSet<String>(Arrays.asList("tez", "spark"));
 
   public class ReduceSinkOperatorExplainVectorization extends OperatorExplainVectorization {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorAppMasterEventDesc.java
Patch:
@@ -28,7 +28,7 @@
  */
 public class VectorAppMasterEventDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public VectorAppMasterEventDesc() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorFileSinkDesc.java
Patch:
@@ -28,7 +28,7 @@
  */
 public class VectorFileSinkDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public VectorFileSinkDesc() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorFilterDesc.java
Patch:
@@ -30,7 +30,7 @@
  */
 public class VectorFilterDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   private VectorExpression predicateExpression;
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java
Patch:
@@ -31,7 +31,7 @@
  */
 public class VectorGroupByDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   /**
    *     GLOBAL         No key.  All rows --> 1 full aggregation on end of input

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorLimitDesc.java
Patch:
@@ -28,7 +28,7 @@
  */
 public class VectorLimitDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public VectorLimitDesc() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinDesc.java
Patch:
@@ -35,7 +35,7 @@
  */
 public class VectorMapJoinDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public static enum HashTableImplementationType {
     NONE,

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinInfo.java
Patch:
@@ -36,7 +36,7 @@
  */
 public class VectorMapJoinInfo {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   private int[] bigTableKeyColumnMap;
   private String[] bigTableKeyColumnNames;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorPartitionDesc.java
Patch:
@@ -34,7 +34,7 @@
  */
 public class VectorPartitionDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   // Data Type Conversion Needed?
   //

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkDesc.java
Patch:
@@ -28,7 +28,7 @@
  */
 public class VectorReduceSinkDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public static enum ReduceSinkKeyType {
     NONE,

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkInfo.java
Patch:
@@ -33,7 +33,7 @@
  */
 public class VectorReduceSinkInfo {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   private int[] reduceSinkKeyColumnMap;
   private TypeInfo[] reduceSinkKeyTypeInfos;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSMBJoinDesc.java
Patch:
@@ -28,7 +28,7 @@
  */
 public class VectorSMBJoinDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public VectorSMBJoinDesc() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSelectDesc.java
Patch:
@@ -30,7 +30,7 @@
  */
 public class VectorSelectDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   private VectorExpression[] selectExpressions;
   private int[] projectedOutputColumns;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSparkHashTableSinkDesc.java
Patch:
@@ -28,7 +28,7 @@
  */
 public class VectorSparkHashTableSinkDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public VectorSparkHashTableSinkDesc() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorSparkPartitionPruningSinkDesc.java
Patch:
@@ -28,7 +28,7 @@
  */
 public class VectorSparkPartitionPruningSinkDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   public VectorSparkPartitionPruningSinkDesc() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/VectorTableScanDesc.java
Patch:
@@ -28,7 +28,7 @@
  */
 public class VectorTableScanDesc extends AbstractVectorDesc  {
 
-  private static long serialVersionUID = 1L;
+  private static final long serialVersionUID = 1L;
 
   private int[] projectedOutputColumns;
 

File: ql/src/java/org/apache/hadoop/hive/ql/processors/HiveCommand.java
Patch:
@@ -36,7 +36,7 @@ public enum HiveCommand {
   DELETE(),
   COMPILE();
 
-  public static boolean ONLY_FOR_TESTING = true;
+  public static final boolean ONLY_FOR_TESTING = true;
   private boolean usedOnlyForTesting;
 
   HiveCommand() {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFInternalInterval.java
Patch:
@@ -51,7 +51,7 @@
 
 public class GenericUDFInternalInterval extends GenericUDF {
 
-  private static Map<Integer, IntervalProcessor> processorMap;
+  private Map<Integer, IntervalProcessor> processorMap;
 
   private transient IntervalProcessor processor;
   private transient PrimitiveObjectInspector inputOI;
@@ -286,7 +286,7 @@ protected HiveIntervalYearMonth getIntervalYearMonth(String arg) {
     }
   }
 
-  private static Map<Integer, IntervalProcessor> getProcessorMap() {
+  private Map<Integer, IntervalProcessor> getProcessorMap() {
 
     if (processorMap != null) {
       return processorMap;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CommonFastHashTable.java
Patch:
@@ -31,9 +31,9 @@ public class CommonFastHashTable {
   protected static final int LARGE_CAPACITY = 8388608;
   protected static Random random;
 
-  protected static int MAX_KEY_LENGTH = 100;
+  protected static final int MAX_KEY_LENGTH = 100;
 
-  protected static int MAX_VALUE_LENGTH = 1000;
+  protected static final int MAX_VALUE_LENGTH = 1000;
 
   public static int generateLargeCount() {
     int count = 0;

File: serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
Patch:
@@ -65,7 +65,7 @@ public final class SerDeUtils {
   public static final char COMMA = ',';
   // we should use '\0' for COLUMN_NAME_DELIMITER if column name contains COMMA
   // but we should also take care of the backward compatibility
-  public static char COLUMN_COMMENTS_DELIMITER = '\0';
+  public static final char COLUMN_COMMENTS_DELIMITER = '\0';
   public static final String LBRACKET = "[";
   public static final String RBRACKET = "]";
   public static final String LBRACE = "{";

File: serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java
Patch:
@@ -75,7 +75,7 @@ class AvroDeserializer {
    * Flag to print the re-encoding warning message only once. Avoid excessive logging for each
    * record encoding.
    */
-  private static boolean warnedOnce = false;
+  private boolean warnedOnce = false;
   /**
    * When encountering a record with an older schema than the one we're trying
    * to read, it is necessary to re-encode with a reader against the newer schema.

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java
Patch:
@@ -666,9 +666,7 @@ public void logExceptionMessage(byte[] bytes, int bytesStart, int bytesLength, S
 
   //------------------------------------------------------------------------------------------------
 
-  private static byte[] maxLongBytes = ((Long) Long.MAX_VALUE).toString().getBytes();
-  private static int maxLongDigitsCount = maxLongBytes.length;
-  private static byte[] minLongNoSignBytes = ((Long) Long.MIN_VALUE).toString().substring(1).getBytes();
+  private static final byte[] maxLongBytes = ((Long) Long.MAX_VALUE).toString().getBytes();
 
   public static int byteArrayCompareRanges(byte[] arg1, int start1, byte[] arg2, int start2, int len) {
     for (int i = 0; i < len; i++) {

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/StringToDouble.java
Patch:
@@ -15,12 +15,12 @@
 import java.nio.charset.StandardCharsets;
 
 public class StringToDouble {
-  static int maxExponent = 511;	/* Largest possible base 10 exponent.  Any
+  static final int maxExponent = 511;	/* Largest possible base 10 exponent.  Any
 				 * exponent larger than this will already
 				 * produce underflow or overflow, so there's
 				 * no need to worry about additional digits.
 				 */
-  static double powersOf10[] = {	/* Table giving binary powers of 10.  Entry */
+  static final double powersOf10[] = {	/* Table giving binary powers of 10.  Entry */
       10.,			/* is 10^2^i.  Used to convert decimal */
       100.,			/* exponents into floating-point numbers. */
       1.0e4,

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryUtils.java
Patch:
@@ -402,7 +402,7 @@ public static int writeVLongToByteArray(byte[] bytes, int offset, long l) {
     return 1 + len;
   }
 
-  public static int VLONG_BYTES_LEN = 9;
+  public static final int VLONG_BYTES_LEN = 9;
 
   private static ThreadLocal<byte[]> vLongBytesThreadLocal = new ThreadLocal<byte[]>() {
     @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
Patch:
@@ -869,7 +869,7 @@ public static int getCharacterLengthForType(PrimitiveTypeInfo typeInfo) {
     }
   }
 
-  public static void registerNumericType(PrimitiveCategory primitiveCategory, int level) {
+  public static synchronized void registerNumericType(PrimitiveCategory primitiveCategory, int level) {
     numericTypeList.add(primitiveCategory);
     numericTypes.put(primitiveCategory, level);
   }

File: shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java
Patch:
@@ -49,12 +49,12 @@
 import com.google.common.collect.Iterables;
 
 public class HdfsUtils {
+  private static final Logger LOG = LoggerFactory.getLogger("shims.HdfsUtils");
 
   // TODO: this relies on HDFS not changing the format; we assume if we could get inode ID, this
   //       is still going to work. Otherwise, file IDs can be turned off. Later, we should use
   //       as public utility method in HDFS to obtain the inode-based path.
-  private static String HDFS_ID_PATH_PREFIX = "/.reserved/.inodes/";
-  static Logger LOG = LoggerFactory.getLogger("shims.HdfsUtils");
+  private static final String HDFS_ID_PATH_PREFIX = "/.reserved/.inodes/";
 
   public static Path getFileIdPath(
       FileSystem fileSystem, Path path, long fileId) {

File: shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerChain.java
Patch:
@@ -35,7 +35,7 @@
  */
 public class HiveIOExceptionHandlerChain {
 
-  public static String HIVE_IO_EXCEPTION_HANDLE_CHAIN = "hive.io.exception.handlers";
+  public static final String HIVE_IO_EXCEPTION_HANDLE_CHAIN = "hive.io.exception.handlers";
 
   @SuppressWarnings("unchecked")
   public static HiveIOExceptionHandlerChain getHiveIOExceptionHandlerChain(

File: shims/common/src/main/java/org/apache/hadoop/hive/io/HiveIOExceptionHandlerUtil.java
Patch:
@@ -24,10 +24,10 @@
 
 public class HiveIOExceptionHandlerUtil {
 
-  private static ThreadLocal<HiveIOExceptionHandlerChain> handlerChainInstance =
+  private static final ThreadLocal<HiveIOExceptionHandlerChain> handlerChainInstance =
     new ThreadLocal<HiveIOExceptionHandlerChain>();
 
-  private static HiveIOExceptionHandlerChain get(JobConf job) {
+  private static synchronized HiveIOExceptionHandlerChain get(JobConf job) {
     HiveIOExceptionHandlerChain cache = HiveIOExceptionHandlerUtil.handlerChainInstance
         .get();
     if (cache == null) {

File: shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
Patch:
@@ -32,7 +32,7 @@
  */
 public abstract class ShimLoader {
   private static final Logger LOG = LoggerFactory.getLogger(ShimLoader.class);
-  public static String HADOOP23VERSIONNAME = "0.23";
+  public static final String HADOOP23VERSIONNAME = "0.23";
 
   private static volatile HadoopShims hadoopShims;
   private static JettyShims jettyShims;

File: testutils/src/java/org/apache/hive/testutils/jdbc/HiveBurnInClient.java
Patch:
@@ -26,10 +26,10 @@
 
 
 public class HiveBurnInClient {
-  private static String driverName = "org.apache.hive.jdbc.HiveDriver";
+  private static final String driverName = "org.apache.hive.jdbc.HiveDriver";
 
   //default 80k (runs slightly over 1 day long)
-  private final static int NUM_QUERY_ITERATIONS = 80000;
+  private static final int NUM_QUERY_ITERATIONS = 80000;
 
   /**
    * Creates 2 tables to query from

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1967,8 +1967,8 @@ public static enum ConfVars {
     HIVE_DRUID_SLEEP_TIME("hive.druid.sleep.time", "PT10S",
             "Sleep time between retries in ISO8601 format (for example P2W, P3M, PT1H30M, PT0.750S), default is period of 10 seconds."
     ),
-    HIVE_DRUID_BASE_PERSIST_DIRECTORY("hive.druid.basePersistDirectory", "/tmp",
-            "Local temporary directory used to persist intermediate indexing state."
+    HIVE_DRUID_BASE_PERSIST_DIRECTORY("hive.druid.basePersistDirectory", "",
+            "Local temporary directory used to persist intermediate indexing state, will default to JVM system property java.io.tmpdir."
     ),
     DRUID_SEGMENT_DIRECTORY("hive.druid.storage.storageDirectory", "/druid/segments"
             , "druid deep storage location."),

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1176,10 +1176,10 @@ public static enum ConfVars {
         "when using UDTF's to prevent the task getting killed because of inactivity.  Users should be cautious \n" +
         "because this may prevent TaskTracker from killing tasks with infinite loops."),
 
-    HIVEDEFAULTFILEFORMAT("hive.default.fileformat", "TextFile", new StringSet("TextFile", "SequenceFile", "RCfile", "ORC"),
+    HIVEDEFAULTFILEFORMAT("hive.default.fileformat", "TextFile", new StringSet("TextFile", "SequenceFile", "RCfile", "ORC", "parquet"),
         "Default file format for CREATE TABLE statement. Users can explicitly override it by CREATE TABLE ... STORED AS [FORMAT]"),
     HIVEDEFAULTMANAGEDFILEFORMAT("hive.default.fileformat.managed", "none",
-        new StringSet("none", "TextFile", "SequenceFile", "RCfile", "ORC"),
+        new StringSet("none", "TextFile", "SequenceFile", "RCfile", "ORC", "parquet"),
         "Default file format for CREATE TABLE statement applied to managed tables only. External tables will be \n" +
         "created with format specified by hive.default.fileformat. Leaving this null will result in using hive.default.fileformat \n" +
         "for all tables."),

File: ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java
Patch:
@@ -67,7 +67,8 @@ public LoadTableDesc(final Path sourcePath,
       final boolean replace,
       final AcidUtils.Operation writeType, Long mmWriteId) {
     super(sourcePath);
-    Utilities.LOG14535.info("creating part LTD from " + sourcePath + " to " + table.getTableName()/*, new Exception()*/);
+    Utilities.LOG14535.info("creating part LTD from " + sourcePath + " to "
+        + ((table.getProperties() == null) ? "null" : table.getTableName()));
     init(table, partitionSpec, replace, writeType, mmWriteId);
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHive.java
Patch:
@@ -326,8 +326,8 @@ private void validateTable(Table tbl, String tableName) throws MetaException {
       tbl.setCreateTime(ft.getTTable().getCreateTime());
       tbl.getParameters().put(hive_metastoreConstants.DDL_TIME,
           ft.getParameters().get(hive_metastoreConstants.DDL_TIME));
-      assertTrue("Tables  doesn't match: " + tableName, ft.getTTable()
-          .equals(tbl.getTTable()));
+      assertTrue("Tables  doesn't match: " + tableName + " (" + ft.getTTable()
+          + "; " + tbl.getTTable() + ")", ft.getTTable().equals(tbl.getTTable()));
       assertEquals("SerializationLib is not set correctly", tbl
           .getSerializationLib(), ft.getSerializationLib());
       assertEquals("Serde is not set correctly", tbl.getDeserializer()

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
Patch:
@@ -2534,7 +2534,8 @@ private Timestamp getTimestampScalar(ExprNodeDesc expr) throws HiveException {
           "Non-constant argument not supported for vectorization.");
     }
     ExprNodeConstantDesc constExpr = (ExprNodeConstantDesc) expr;
-    if (isStringFamily(constExpr.getTypeString())) {
+    String constTypeString = constExpr.getTypeString();
+    if (isStringFamily(constTypeString) || isDatetimeFamily(constTypeString)) {
 
       // create expression tree with type cast from string to timestamp
       ExprNodeGenericFuncDesc expr2 = new ExprNodeGenericFuncDesc();
@@ -2549,7 +2550,7 @@ private Timestamp getTimestampScalar(ExprNodeDesc expr) throws HiveException {
     }
 
     throw new HiveException("Udf: unhandled constant type for scalar argument. "
-        + "Expecting string.");
+        + "Expecting string/date/timestamp.");
   }
 
   private Timestamp evaluateCastToTimestamp(ExprNodeDesc expr) throws HiveException {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1425,7 +1425,7 @@ public static enum ConfVars {
         "This controls how many partitions can be scanned for each partitioned table.\n" +
         "The default value \"-1\" means no limit. (DEPRECATED: Please use " + ConfVars.METASTORE_LIMIT_PARTITION_REQUEST + " in the metastore instead.)"),
 
-    HIVECONVERTJOINMAXENTRIESHASHTABLE("hive.auto.convert.join.hashtable.max.entries", 4194304L,
+    HIVECONVERTJOINMAXENTRIESHASHTABLE("hive.auto.convert.join.hashtable.max.entries", 40000000L,
         "If hive.auto.convert.join.noconditionaltask is off, this parameter does not take affect. \n" +
         "However, if it is on, and the predicated number of entries in hashtable for a given join \n" +
         "input is larger than this number, the join will not be converted to a mapjoin. \n" +

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java
Patch:
@@ -217,7 +217,7 @@ public Boolean call() throws Exception {
 
         if (response.shouldDie) {
           // AM sent a shouldDie=true
-          LOG.info("Asked to die via task heartbeat");
+          LOG.info("Asked to die via task heartbeat: {}", task.getTaskAttemptID());
           return false;
         } else {
           if (response.numEvents < maxEventsToGet) {
@@ -297,7 +297,7 @@ private synchronized ResponseWrapper heartbeat(Collection<TezEvent> eventsArg) t
       }
 
       if (response.shouldDie()) {
-        LOG.info("Received should die response from AM");
+        LOG.info("Received should die response from AM: {}", task.getTaskAttemptID());
         askedToDie.set(true);
         return new ResponseWrapper(true, 1);
       }

File: common/src/java/org/apache/hadoop/hive/common/metrics/common/MetricsConstant.java
Patch:
@@ -80,5 +80,5 @@ public class MetricsConstant {
   public static final String HS2_COMPILING_QUERIES = "hs2_compiling_queries";
   public static final String HS2_EXECUTING_QUERIES = "hs2_executing_queries";
   public static final String HS2_FAILED_QUERIES = "hs2_failed_queries";
-  public static final String HS2_SUCEEDED_QUERIES = "hs2_suceeded_queries";
+  public static final String HS2_SUCCEEDED_QUERIES = "hs2_succeeded_queries";
 }
\ No newline at end of file

File: service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
Patch:
@@ -666,7 +666,7 @@ protected void onNewState(OperationState state, OperationState prevState) {
       markQueryMetric(MetricsFactory.getInstance(), MetricsConstant.HS2_FAILED_QUERIES);
     }
     if (state == OperationState.FINISHED) {
-      markQueryMetric(MetricsFactory.getInstance(), MetricsConstant.HS2_SUCEEDED_QUERIES);
+      markQueryMetric(MetricsFactory.getInstance(), MetricsConstant.HS2_SUCCEEDED_QUERIES);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java
Patch:
@@ -558,9 +558,9 @@ private void checkPartitionDirsInParallel(final ThreadPoolExecutor pool,
    */
   private void checkPartitionDirsSingleThreaded(Queue<Path> basePaths, final Set<Path> allDirs,
       final FileSystem fs, final int depth, final int maxDepth) throws IOException, HiveException {
-    final Queue<Path> nextLevel = new LinkedList<>();
     for (final Path path : basePaths) {
       FileStatus[] statuses = fs.listStatus(path, FileUtils.HIDDEN_FILES_PATH_FILTER);
+      final Queue<Path> nextLevel = new LinkedList<>();
       boolean fileFound = false;
       for (FileStatus status : statuses) {
         if (status.isDirectory()) {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1322,7 +1322,7 @@ public static enum ConfVars {
     HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION("hive.parquet.timestamp.skip.conversion", true,
         "Current Hive implementation of parquet stores timestamps to UTC, this flag allows skipping of the conversion" +
             "on reading parquet files from other tools"),
-    PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE("parquet.mr.int96.enable.utc.write.zone", false,
+    HIVE_PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE("hive.parquet.mr.int96.enable.utc.write.zone", false,
         "Enable this variable to use UTC as the default timezone for new Parquet tables."),
     HIVE_INT_TIMESTAMP_CONVERSION_IN_SECONDS("hive.int.timestamp.conversion.in.seconds", false,
         "Boolean/tinyint/smallint/int/bigint value is interpreted as milliseconds during the timestamp conversion.\n" +

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -4224,11 +4224,11 @@ private int createTable(Hive db, CreateTableDesc crtTbl) throws HiveException {
       }
     }
 
-    // If PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE is set to True, then set new Parquet tables timezone
+    // If HIVE_PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE is set to True, then set new Parquet tables timezone
     // to UTC by default (only if the table property is not set)
     if (tbl.getSerializationLib().equals(ParquetHiveSerDe.class.getName())) {
       SessionState ss = SessionState.get();
-      if (ss.getConf().getBoolVar(ConfVars.PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE)) {
+      if (ss.getConf().getBoolVar(ConfVars.HIVE_PARQUET_INT96_DEFAULT_UTC_WRITE_ZONE)) {
         String parquetTimezone = tbl.getProperty(ParquetTableUtils.PARQUET_INT96_WRITE_ZONE_PROPERTY);
         if (parquetTimezone == null || parquetTimezone.isEmpty()) {
           tbl.setProperty(ParquetTableUtils.PARQUET_INT96_WRITE_ZONE_PROPERTY, ParquetTableUtils.PARQUET_INT96_NO_ADJUSTMENT_ZONE);

File: itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java
Patch:
@@ -62,6 +62,7 @@
 import org.apache.hadoop.mapreduce.InputSplit;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.parquet.hadoop.ParquetInputFormat;
+import org.apache.parquet.hadoop.ParquetInputSplit;
 import org.apache.parquet.hadoop.api.ReadSupport;
 import org.apache.parquet.hadoop.example.GroupReadSupport;
 import org.openjdk.jmh.annotations.Param;
@@ -338,7 +339,7 @@ public RecordReader getVectorizedRecordReader(Path inputPath) throws Exception {
       Job vectorJob = new Job(conf, "read vector");
       ParquetInputFormat.setInputPaths(vectorJob, inputPath);
       ParquetInputFormat parquetInputFormat = new ParquetInputFormat(GroupReadSupport.class);
-      InputSplit split = (InputSplit) parquetInputFormat.getSplits(vectorJob).get(0);
+      ParquetInputSplit split = (ParquetInputSplit) parquetInputFormat.getSplits(vectorJob).get(0);
       initialVectorizedRowBatchCtx(conf);
       return new VectorizedParquetRecordReader(split, new JobConf(conf));
     }

File: ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestVectorizedColumnReader.java
Patch:
@@ -106,7 +106,7 @@ public void testNullSplitForParquetReader() throws Exception {
     HiveConf.setVar(conf, HiveConf.ConfVars.PLAN, "//tmp");
     initialVectorizedRowBatchCtx(conf);
     VectorizedParquetRecordReader reader =
-        new VectorizedParquetRecordReader((InputSplit)null, new JobConf(conf));
+        new VectorizedParquetRecordReader((org.apache.hadoop.mapred.InputSplit)null, new JobConf(conf));
     assertFalse(reader.next(reader.createKey(), reader.createValue()));
   }
 }

File: ql/src/test/org/apache/hadoop/hive/ql/io/parquet/VectorizedColumnReaderTestBase.java
Patch:
@@ -47,6 +47,7 @@
 import org.apache.parquet.example.data.Group;
 import org.apache.parquet.example.data.simple.SimpleGroupFactory;
 import org.apache.parquet.hadoop.ParquetInputFormat;
+import org.apache.parquet.hadoop.ParquetInputSplit;
 import org.apache.parquet.hadoop.ParquetWriter;
 import org.apache.parquet.hadoop.example.GroupReadSupport;
 import org.apache.parquet.hadoop.example.GroupWriteSupport;
@@ -222,7 +223,7 @@ protected VectorizedParquetRecordReader createParquetReader(String schemaString,
     Job vectorJob = new Job(conf, "read vector");
     ParquetInputFormat.setInputPaths(vectorJob, file);
     ParquetInputFormat parquetInputFormat = new ParquetInputFormat(GroupReadSupport.class);
-    InputSplit split = (InputSplit) parquetInputFormat.getSplits(vectorJob).get(0);
+    ParquetInputSplit split = (ParquetInputSplit) parquetInputFormat.getSplits(vectorJob).get(0);
     initialVectorizedRowBatchCtx(conf);
     return new VectorizedParquetRecordReader(split, new JobConf(conf));
   }

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
Patch:
@@ -669,6 +669,9 @@ protected static PartitionDesc getPartitionDescFromPath(
       throws IOException {
     PartitionDesc partDesc = pathToPartitionInfo.get(dir);
     if (partDesc == null) {
+      // TODO: HiveFileFormatUtils.getPartitionDescFromPathRecursively for MM tables?
+      //       So far, the only case when this is called for a MM directory was in error.
+      //       Keep it like this for now; may need replacement if we find a valid usage like this.
       partDesc = pathToPartitionInfo.get(Path.getPathWithoutSchemeAndAuthority(dir));
     }
     if (partDesc == null) {

File: llap-server/src/test/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorTestHelpers.java
Patch:
@@ -134,6 +134,7 @@ public static SubmitWorkRequestProto createSubmitWorkRequestProto(
             VertexOrBinary.newBuilder().setVertex(
             SignableVertexSpec.newBuilder()
                 .setDagName(dagName)
+                .setHiveQueryId(dagName)
                 .setUser("MockUser")
                 .setTokenIdentifier("MockToken_1")
                 .setQueryIdentifier(

File: llap-server/src/test/org/apache/hadoop/hive/llap/daemon/impl/comparator/TestFirstInFirstOutComparator.java
Patch:
@@ -77,6 +77,7 @@ private SubmitWorkRequestProto createRequest(int fragmentNumber, int numSelfAndU
                             .build())
                     .setVertexIndex(vId.getId())
                     .setDagName(dagName)
+                    .setHiveQueryId(dagName)
                     .setVertexName("MockVertex")
                     .setUser("MockUser")
                     .setTokenIdentifier("MockToken_1")

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
Patch:
@@ -1947,7 +1947,6 @@ public static boolean isInsertOnlyTable(Map<String, String> params) {
 
   public static boolean isInsertOnlyTable(Map<String, String> params, boolean isCtas) {
     String transactionalProp = params.get(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);
-    // TODO# for the test, all non-ACID tables are MM
     return (transactionalProp != null && "insert_only".equalsIgnoreCase(transactionalProp));
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
Patch:
@@ -734,7 +734,6 @@ public FetchInputFormatSplit(InputSplit split, InputFormat inputFormat) {
     }
 
     public RecordReader<WritableComparable, Writable> getRecordReader(JobConf job) throws IOException {
-      LOG.error("TODO# calling origina getRr on " + inputFormat + "; " + getInputSplit());
       return inputFormat.getRecordReader(getInputSplit(), job, Reporter.NULL);
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -960,7 +960,7 @@ private FSPaths createNewPaths(String dirName) throws HiveException {
     fsp2.configureDynPartPath(dirName, !conf.isMmTable() && isUnionDp ? unionPath : null);
     Utilities.LOG14535.info("creating new paths " + System.identityHashCode(fsp2) + " for "
         + dirName + ", childSpec " + unionPath + ": tmpPath " + fsp2.getTmpPath()
-        + ", task path " + fsp2.getTaskOutputTempPath(), new Exception());
+        + ", task path " + fsp2.getTaskOutputTempPath());
     if(!conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED)) {
       createBucketFiles(fsp2);
       valToPaths.put(dirName, fsp2);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -453,7 +453,7 @@ private DataContainer handleDynParts(Hive db, Table table, LoadTableDesc tbd,
 
     // publish DP columns to its subscribers
     if (dps != null && dps.size() > 0) {
-      pushFeed(FeedType.DYNAMIC_PARTITIONS, dps);
+      pushFeed(FeedType.DYNAMIC_PARTITIONS, dp.values());
     }
 
     String loadTime = "\t Time taken to load dynamic partitions: "  +

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java
Patch:
@@ -71,8 +71,7 @@ public class TestDbTxnManager2 {
   private Driver driver;
   TxnStore txnHandler;
 
-  @BeforeClass
-  public static void setUpClass() throws Exception {
+  public TestDbTxnManager2() throws Exception {
     conf
     .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
@@ -374,6 +373,8 @@ public void testDummyTxnManagerOnAcidTable() throws Exception {
     cpr = driver.compileAndRespond("delete from T10");
     Assert.assertEquals(ErrorMsg.ACID_OP_ON_NONACID_TXNMGR.getErrorCode(), cpr.getResponseCode());
     Assert.assertTrue(cpr.getErrorMessage().contains("Attempt to do update or delete using transaction manager that does not support these operations."));
+
+    conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
   }
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java
Patch:
@@ -314,7 +314,7 @@ private TezSessionState getSession(HiveConf conf, boolean doOpen)
         LOG.warn("User has specified " + queueName + " queue; ignoring the setting");
         queueName = null;
         hasQueue = false;
-        conf.set("tez.queue.name", null);
+        conf.unset("tez.queue.name");
       }
       default: // All good.
       }

File: llap-server/src/test/org/apache/hadoop/hive/llap/daemon/MiniLlapCluster.java
Patch:
@@ -164,7 +164,7 @@ public void serviceInit(Configuration conf) throws IOException, InterruptedExcep
     LOG.info("Initializing {} llap instances for MiniLlapCluster with name={}", numInstances, clusterNameTrimmed);
     for (int i = 0 ;i < numInstances ; i++) {
       llapDaemons[i] = new LlapDaemon(conf, numExecutorsPerService, execBytesPerService, llapIoEnabled,
-          ioIsDirect, ioBytesPerService, localDirs, rpcPort, mngPort, shufflePort, webPort, clusterNameTrimmed, 0);
+          ioIsDirect, ioBytesPerService, localDirs, rpcPort, mngPort, shufflePort, webPort, clusterNameTrimmed);
       llapDaemons[i].init(new Configuration(conf));
     }
     LOG.info("Initialized {} llap instances for MiniLlapCluster with name={}", numInstances, clusterNameTrimmed);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
Patch:
@@ -2240,7 +2240,7 @@ private VectorExpression getBetweenFilterExpression(List<ExprNodeDesc> childExpr
       cl = FilterDecimalColumnNotBetween.class;
     } else if (isDateFamily(colType) && !notKeywordPresent) {
       cl =  (hasDynamicValues ?
-          FilterLongColumnBetweenDynamicValue.class :
+          FilterDateColumnBetweenDynamicValue.class :
           FilterLongColumnBetween.class);
     } else if (isDateFamily(colType) && notKeywordPresent) {
       cl = FilterLongColumnNotBetween.class;

File: service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
Patch:
@@ -346,6 +346,7 @@ public Object run() throws HiveSQLException {
             }
             runQuery();
           } catch (HiveSQLException e) {
+            // TODO: why do we invent our own error path op top of the one from Future.get?
             setOperationException(e);
             LOG.error("Error running hive query: ", e);
           } finally {
@@ -361,8 +362,7 @@ public Object run() throws HiveSQLException {
       } catch (Exception e) {
         setOperationException(new HiveSQLException(e));
         LOG.error("Error running hive query as user : " + currentUGI.getShortUserName(), e);
-      }
-      finally {
+      } finally {
         /**
          * We'll cache the ThreadLocal RawStore object for this background thread for an orderly cleanup
          * when this thread is garbage collected later.

File: storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java
Patch:
@@ -947,11 +947,13 @@ public int hashCode() {
     return fastHashCode();
   }
 
+  private static final byte[] EMPTY_ARRAY = new byte[0];
+
   @HiveDecimalWritableVersionV1
   public byte[] getInternalStorage() {
     if (!isSet()) {
       // don't break old callers that are trying to reuse storages
-      return new byte[0];
+      return EMPTY_ARRAY;
     }
 
     if (internalScratchLongs == null) {

File: storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java
Patch:
@@ -950,7 +950,8 @@ public int hashCode() {
   @HiveDecimalWritableVersionV1
   public byte[] getInternalStorage() {
     if (!isSet()) {
-      throw new RuntimeException("no value set");
+      // don't break old callers that are trying to reuse storages
+      return new byte[0];
     }
 
     if (internalScratchLongs == null) {

File: druid-handler/src/test/org/apache/hadoop/hive/ql/io/DruidRecordWriterTest.java
Patch:
@@ -231,7 +231,7 @@ private void verifyRows(List<ImmutableMap<String, Object>> expectedRows,
   @Test
   public void testSerDesr() throws IOException {
     String segment = "{\"dataSource\":\"datasource2015\",\"interval\":\"2015-06-01T00:00:00.000-04:00/2015-06-02T00:00:00.000-04:00\",\"version\":\"2016-11-04T19:24:01.732-04:00\",\"loadSpec\":{\"type\":\"hdfs\",\"path\":\"hdfs://cn105-10.l42scl.hortonworks.com:8020/apps/hive/warehouse/druid.db/.hive-staging_hive_2016-11-04_19-23-50_168_1550339856804207572-1/_task_tmp.-ext-10002/_tmp.000000_0/datasource2015/20150601T000000.000-0400_20150602T000000.000-0400/2016-11-04T19_24_01.732-04_00/0/index.zip\"},\"dimensions\":\"dimension1\",\"metrics\":\"bigint\",\"shardSpec\":{\"type\":\"linear\",\"partitionNum\":0},\"binaryVersion\":9,\"size\":1765,\"identifier\":\"datasource2015_2015-06-01T00:00:00.000-04:00_2015-06-02T00:00:00.000-04:00_2016-11-04T19:24:01.732-04:00\"}";
-    DataSegment dataSegment = objectMapper.readerFor(DataSegment.class)
+    DataSegment dataSegment = objectMapper.reader(DataSegment.class)
             .readValue(segment);
     Assert.assertTrue(dataSegment.getDataSource().equals("datasource2015"));
   }

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
Patch:
@@ -834,11 +834,11 @@ public DiskRangeList getFileData(Object fileKey, DiskRangeList range,
           fileKey, range, baseOffset, factory, counters, gotAllData);
       if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {
         LlapIoImpl.ORC_LOGGER.trace("Disk ranges after data cache (file " + fileKey +
-            ", base offset " + baseOffset + "): " + RecordReaderUtils.stringifyDiskRanges(range));
+            ", base offset " + baseOffset + "): " + RecordReaderUtils.stringifyDiskRanges(result));
       }
       if (gotAllData.value) return result;
-      return (metadataCache == null) ? range
-          : metadataCache.getIncompleteCbs(fileKey, range, baseOffset, factory, gotAllData);
+      return (metadataCache == null) ? result
+          : metadataCache.getIncompleteCbs(fileKey, result, baseOffset, factory, gotAllData);
     }
 
     @Override

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/CacheChunk.java
Patch:
@@ -43,6 +43,7 @@ public void init(MemoryBuffer buffer, long offset, long end) {
     this.buffer = buffer;
     this.offset = offset;
     this.end = end;
+    this.next = this.prev = null; // Just in case.
   }
 
   @Override
@@ -87,4 +88,4 @@ public void reset() {
   public void adjustEnd(long l) {
     this.end += l;
   }
-}
\ No newline at end of file
+}

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
Patch:
@@ -1475,8 +1475,7 @@ protected Writer createOrcWriter(CacheWriter cacheWriter, Configuration conf,
         Path path, StructObjectInspector oi) throws IOException {
       // TODO: this is currently broken. We need to set memory manager to a bogus implementation
       //       to avoid problems with memory manager actually tracking the usage.
-      return OrcFile.createWriter(path, createOrcWriterOptions(
-          sourceOi, conf, cacheWriter, allocSize));
+      return OrcFile.createWriter(path, createOrcWriterOptions(oi, conf, cacheWriter, allocSize));
     }
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/io/parquet/VectorizedColumnReaderTestBase.java
Patch:
@@ -295,7 +295,7 @@ protected static void writeData(ParquetWriter<Group> writer, boolean isDictionar
     writer.close();
   }
 
-  private void initialVectorizedRowBatchCtx(Configuration conf) throws HiveException {
+  protected void initialVectorizedRowBatchCtx(Configuration conf) throws HiveException {
     MapWork mapWork = new MapWork();
     VectorizedRowBatchCtx rbCtx = new VectorizedRowBatchCtx();
     rbCtx.init(createStructObjectInspector(conf), new String[0]);

File: druid-handler/src/test/org/apache/hadoop/hive/druid/TestDruidSerDe.java
Patch:
@@ -502,7 +502,7 @@ public void testDruidDeserializer()
           IllegalAccessException, IOException, InterruptedException,
           NoSuchMethodException, InvocationTargetException {
     // Create, initialize, and test the SerDe
-    DruidSerDe serDe = new DruidSerDe();
+    QTestDruidSerDe serDe = new QTestDruidSerDe();
     Configuration conf = new Configuration();
     Properties tbl;
     // Timeseries query

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTConverter.java
Patch:
@@ -360,7 +360,8 @@ private QueryBlockInfo convertSource(RelNode r) throws CalciteSemanticException
       s = new Schema(left.schema, right.schema);
       ASTNode cond = join.getCondition().accept(new RexVisitor(s));
       boolean semiJoin = join instanceof SemiJoin;
-      if (join.getRight() instanceof Join) {
+      if (join.getRight() instanceof Join && !semiJoin) {
+          // should not be done for semijoin since it will change the semantics
         // Invert join inputs; this is done because otherwise the SemanticAnalyzer
         // methods to merge joins will not kick in
         JoinRelType type;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStringToMap.java
Patch:
@@ -44,7 +44,7 @@
     + "Creates a map by parsing text ", extended = "Split text into key-value pairs"
     + " using two delimiters. The first delimiter separates pairs, and the"
     + " second delimiter sperates key and value. If only one parameter is given, default"
-    + " delimiters are used: ',' as delimiter1 and '=' as delimiter2.")
+    + " delimiters are used: ',' as delimiter1 and ':' as delimiter2.")
 public class GenericUDFStringToMap extends GenericUDF {
   // Must be deterministic order map for consistent q-test output across Java versions - see HIVE-9161
   private final LinkedHashMap<Object, Object> ret = new LinkedHashMap<Object, Object>();

File: beeline/src/java/org/apache/hive/beeline/cli/CliOptionsProcessor.java
Patch:
@@ -55,12 +55,12 @@ public CliOptionsProcessor() {
 
     // Substitution option -d, --define
     options.addOption(OptionBuilder.withValueSeparator().hasArgs(2).withArgName("key=value")
-        .withLongOpt("define").withDescription("Variable subsitution to apply to hive commands. e" +
+        .withLongOpt("define").withDescription("Variable substitution to apply to Hive commands. e" +
             ".g. -d A=B or --define A=B").create('d'));
 
     // Substitution option --hivevar
     options.addOption(OptionBuilder.withValueSeparator().hasArgs(2).withArgName("key=value")
-        .withLongOpt("hivevar").withDescription("Variable subsitution to apply to hive commands. " +
+        .withLongOpt("hivevar").withDescription("Variable substitution to apply to Hive commands. " +
             "e.g. --hivevar A=B").create());
 
     // [-S|--silent]

File: cli/src/java/org/apache/hadoop/hive/cli/OptionsProcessor.java
Patch:
@@ -90,7 +90,7 @@ public OptionsProcessor() {
         .hasArgs(2)
         .withArgName("key=value")
         .withLongOpt("define")
-        .withDescription("Variable subsitution to apply to hive commands. e.g. -d A=B or --define A=B")
+        .withDescription("Variable substitution to apply to Hive commands. e.g. -d A=B or --define A=B")
         .create('d'));
 
     // Substitution option --hivevar
@@ -99,7 +99,7 @@ public OptionsProcessor() {
         .hasArgs(2)
         .withArgName("key=value")
         .withLongOpt("hivevar")
-        .withDescription("Variable subsitution to apply to hive commands. e.g. --hivevar A=B")
+        .withDescription("Variable substitution to apply to Hive commands. e.g. --hivevar A=B")
         .create());
 
     // [-S|--silent]

File: common/src/java/org/apache/hadoop/hive/common/JvmPauseMonitor.java
Patch:
@@ -37,6 +37,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.concurrent.TimeUnit;
 
 /**
  * Based on the JvmPauseMonitor from Hadoop.
@@ -181,7 +182,7 @@ public void run() {
         } catch (InterruptedException ie) {
           return;
         }
-        long extraSleepTime = sw.elapsedMillis() - SLEEP_INTERVAL_MS;
+        long extraSleepTime = sw.elapsed(TimeUnit.MILLISECONDS) - SLEEP_INTERVAL_MS;
         Map<String, GcTimes> gcTimesAfterSleep = getGcTimes();
 
         if (extraSleepTime > warnThresholdMs) {

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java
Patch:
@@ -150,7 +150,7 @@ public void testSparkConfigUpdate(){
     HiveConf conf = new HiveConf();
     Assert.assertFalse(conf.getSparkConfigUpdated());
 
-    conf.verifyAndSet("spark.master", "yarn-cluster");
+    conf.verifyAndSet("spark.master", "yarn");
     Assert.assertTrue(conf.getSparkConfigUpdated());
     conf.verifyAndSet("hive.execution.engine", "spark");
     Assert.assertTrue("Expected spark config updated.", conf.getSparkConfigUpdated());

File: contrib/src/java/org/apache/hadoop/hive/contrib/serde2/TypedBytesSerDe.java
Patch:
@@ -109,8 +109,9 @@ public void initialize(Configuration conf, Properties tbl)
     // Read the configuration parameters
     String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
     String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
-
-    columnNames = Arrays.asList(columnNameProperty.split(","));
+    final String columnNameDelimiter = tbl.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ? tbl
+        .getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);
+    columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));
     columnTypes = null;
     if (columnTypeProperty.length() == 0) {
       columnTypes = new ArrayList<TypeInfo>();

File: druid-handler/src/java/org/apache/hadoop/hive/druid/io/HiveDruidSplit.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hadoop.hive.druid;
+package org.apache.hadoop.hive.druid.io;
 
 import java.io.DataInput;
 import java.io.DataOutput;
@@ -30,6 +30,7 @@
 public class HiveDruidSplit extends FileSplit implements org.apache.hadoop.mapred.InputSplit {
 
   private String address;
+
   private String druidQuery;
 
   // required for deserialization
@@ -64,7 +65,7 @@ public long getLength() {
 
   @Override
   public String[] getLocations() {
-    return new String[] {""} ;
+    return new String[] { "" };
   }
 
   public String getAddress() {

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java
Patch:
@@ -116,7 +116,7 @@ public RecordWriter getHiveRecordWriter(
           job.getConfiguration(), progressable);
 
     final Path outputdir = FileOutputFormat.getOutputPath(tac);
-    final Path taskAttemptOutputdir = FileOutputCommitter.getTaskAttemptPath(tac, outputdir);
+    final Path taskAttemptOutputdir = new FileOutputCommitter(outputdir, tac).getWorkPath();
     final org.apache.hadoop.mapreduce.RecordWriter<
       ImmutableBytesWritable, KeyValue> fileWriter = getFileWriter(tac);
 

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java
Patch:
@@ -114,12 +114,13 @@ public void initialize(Configuration conf, Properties tbl)
     // Get column names and types
     String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
     String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
-
+    final String columnNameDelimiter = tbl.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ? tbl
+        .getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);
     // all table column names
     if (columnNameProperty.length() == 0) {
       columnNames = new ArrayList<String>();
     } else {
-      columnNames = Arrays.asList(columnNameProperty.split(","));
+      columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));
     }
 
     // all column types

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.ql.metadata.Table;
+import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
@@ -162,6 +163,8 @@ private static Properties getSerdeProperties(HCatTableInfo info, HCatSchema s)
     List<FieldSchema> fields = HCatUtil.getFieldSchemaList(s.getFields());
     props.setProperty(org.apache.hadoop.hive.serde.serdeConstants.LIST_COLUMNS,
       MetaStoreUtils.getColumnNamesFromFieldSchema(fields));
+    props.setProperty(serdeConstants.COLUMN_NAME_DELIMITER,
+        MetaStoreUtils.getColumnNameDelimiter(fields));
     props.setProperty(org.apache.hadoop.hive.serde.serdeConstants.LIST_COLUMN_TYPES,
       MetaStoreUtils.getColumnTypesFromFieldSchema(fields));
     props.setProperty("columns.comments",

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/common/TestHCatUtil.java
Patch:
@@ -24,8 +24,6 @@
 import java.util.List;
 import java.util.Map;
 
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
 import org.apache.hadoop.fs.permission.FsAction;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.hive.metastore.TableType;
@@ -40,6 +38,9 @@
 import org.junit.Assert;
 import org.junit.Test;
 
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+
 public class TestHCatUtil {
 
   @Test

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestE2EScenarios.java
Patch:
@@ -62,7 +62,7 @@
 
 public class TestE2EScenarios {
   private static final String TEST_DATA_DIR = System.getProperty("java.io.tmpdir") + File.separator
-      + TestHCatLoader.class.getCanonicalName() + "-" + System.currentTimeMillis();
+      + TestE2EScenarios.class.getCanonicalName() + "-" + System.currentTimeMillis();
   private static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
 
   private static final String TEXTFILE_LOCN = TEST_DATA_DIR + "/textfile";

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderEncryption.java
Patch:
@@ -84,9 +84,9 @@
 @RunWith(Parameterized.class)
 public class TestHCatLoaderEncryption {
   private static final AtomicInteger salt = new AtomicInteger(new Random().nextInt());
-  private static final Logger LOG = LoggerFactory.getLogger(TestHCatLoader.class);
+  private static final Logger LOG = LoggerFactory.getLogger(TestHCatLoaderEncryption.class);
   private final String TEST_DATA_DIR = HCatUtil.makePathASafeFileName(System.getProperty
-      ("java.io.tmpdir") + File.separator + TestHCatLoader.class.getCanonicalName() + "-" +
+      ("java.io.tmpdir") + File.separator + TestHCatLoaderEncryption.class.getCanonicalName() + "-" +
       System.currentTimeMillis() + "_" + salt.getAndIncrement());
   private final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
   private final String BASIC_FILE_NAME = TEST_DATA_DIR + "/basic.input.data";

File: hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/mutate/worker/TestWarehousePartitionHelper.java
Patch:
@@ -25,13 +25,13 @@
 import java.util.Collections;
 import java.util.List;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.conf.HiveConf;
 import org.junit.Test;
 
 public class TestWarehousePartitionHelper {
 
-  private static final Configuration CONFIGURATION = new Configuration();
+  private static final HiveConf CONFIGURATION = new HiveConf();
   private static final Path TABLE_PATH = new Path("table");
   
   private static final List<String> UNPARTITIONED_COLUMNS = Collections.emptyList();

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonStorage.java
Patch:
@@ -93,7 +93,7 @@ public void saveField(Type type, String id, String key, String val)
    * Get the id of each data grouping of a given type in the storage
    * system.
    * @param type The data type (as listed above)
-   * @return An ArrayList<String> of ids.
+   * @return A list of ids.
    */
   public List<String> getAllForType(Type type);
 

File: hplsql/src/main/java/org/apache/hive/hplsql/Arguments.java
Patch:
@@ -76,7 +76,7 @@ public class Arguments {
         .hasArgs(2)
         .withArgName("key=value")
         .withLongOpt("define")
-        .withDescription("Variable subsitution e.g. -d A=B or --define A=B")
+        .withDescription("Variable substitution e.g. -d A=B or --define A=B")
         .create('d'));
 
     // Substitution option --hivevar
@@ -85,7 +85,7 @@ public class Arguments {
         .hasArgs(2)
         .withArgName("key=value")
         .withLongOpt("hivevar")
-        .withDescription("Variable subsitution e.g. --hivevar A=B")
+        .withDescription("Variable substitution e.g. --hivevar A=B")
         .create());
     
     // [-version|--version]

File: itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe1.java
Patch:
@@ -49,9 +49,10 @@ public void initialize(Configuration conf, Properties tbl)
     // Read the configuration parameters
     String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
     String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
-
+    final String columnNameDelimiter = tbl.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ? tbl
+        .getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);
     // The input column can either be a string or a list of integer values.
-    List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
+    List<String> columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));
     List<TypeInfo> columnTypes = TypeInfoUtils
         .getTypeInfosFromTypeString(columnTypeProperty);
     assert columnNames.size() == columnTypes.size();

File: itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe2.java
Patch:
@@ -50,9 +50,10 @@ public void initialize(Configuration conf, Properties tbl)
     // Read the configuration parameters
     String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
     String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
-
+    final String columnNameDelimiter = tbl.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ? tbl
+        .getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);
     // The input column can either be a string or a list of integer values.
-    List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
+    List<String> columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));
     List<TypeInfo> columnTypes = TypeInfoUtils
         .getTypeInfosFromTypeString(columnTypeProperty);
     assert columnNames.size() == columnTypes.size();

File: itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe4.java
Patch:
@@ -41,9 +41,10 @@ public void initialize(Configuration conf, Properties tbl)
       // Read the configuration parameters
       String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
       String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
-
+      final String columnNameDelimiter = tbl.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ? tbl
+          .getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);
       // The input column can either be a string or a list of integer values.
-      List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
+      List<String> columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));
       List<TypeInfo> columnTypes = TypeInfoUtils
           .getTypeInfosFromTypeString(columnTypeProperty);
       assert columnNames.size() == columnTypes.size();

File: itests/custom-serde/src/main/java/org/apache/hadoop/hive/serde2/CustomSerDe5.java
Patch:
@@ -39,9 +39,10 @@ public void initialize(Configuration conf, Properties tbl)
       // Read the configuration parameters
       String columnNameProperty = tbl.getProperty(serdeConstants.LIST_COLUMNS);
       String columnTypeProperty = tbl.getProperty(serdeConstants.LIST_COLUMN_TYPES);
-
+      final String columnNameDelimiter = tbl.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ? tbl
+          .getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);
       // The input column can either be a string or a list of integer values.
-      List<String> columnNames = Arrays.asList(columnNameProperty.split(","));
+      List<String> columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));
       List<TypeInfo> columnTypes = TypeInfoUtils
           .getTypeInfosFromTypeString(columnTypeProperty);
       assert columnNames.size() == columnTypes.size();

File: druid-handler/src/java/org/apache/hadoop/hive/druid/io/DruidOutputFormat.java
Patch:
@@ -140,6 +140,7 @@ public FileSinkOperator.RecordWriter getHiveRecordWriter(
           break;
         case serdeConstants.FLOAT_TYPE_NAME:
         case serdeConstants.DOUBLE_TYPE_NAME:
+        case serdeConstants.DECIMAL_TYPE_NAME:
           af = new DoubleSumAggregatorFactory(columnNames.get(i), columnNames.get(i));
           break;
         default:

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryUtils.java
Patch:
@@ -402,10 +402,12 @@ public static int writeVLongToByteArray(byte[] bytes, int offset, long l) {
     return 1 + len;
   }
 
+  public static int VLONG_BYTES_LEN = 9;
+
   private static ThreadLocal<byte[]> vLongBytesThreadLocal = new ThreadLocal<byte[]>() {
     @Override
     public byte[] initialValue() {
-      return new byte[9];
+      return new byte[VLONG_BYTES_LEN];
     }
   };
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -3201,7 +3201,7 @@ public static enum ConfVars {
         "Whether to create the LLAP coordinator; since execution engine and container vs llap\n" +
         "settings are both coming from job configs, we don't know at start whether this should\n" +
         "be created. Default true."),
-    LLAP_DAEMON_LOGGER("hive.llap.daemon.logger", Constants.LLAP_LOGGER_NAME_RFA,
+    LLAP_DAEMON_LOGGER("hive.llap.daemon.logger", Constants.LLAP_LOGGER_NAME_QUERY_ROUTING,
         new StringSet(Constants.LLAP_LOGGER_NAME_QUERY_ROUTING,
             Constants.LLAP_LOGGER_NAME_RFA,
             Constants.LLAP_LOGGER_NAME_CONSOLE),

File: llap-common/src/java/org/apache/hadoop/hive/llap/security/LlapTokenIdentifier.java
Patch:
@@ -105,7 +105,8 @@ public boolean equals(Object obj) {
 
   @Override
   public String toString() {
-    return KIND + "; " + super.toString() + ", cluster " + clusterId + ", app ID " + appId;
+    return KIND + "; " + super.toString() + ", cluster " + clusterId
+        + ", app ID " + appId + ", signing " + isSigningRequired;
   }
 
   @InterfaceAudience.Private

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTokenChecker.java
Patch:
@@ -72,7 +72,7 @@ public static void warnMultipleTokens(List<LlapTokenIdentifier> tokens) {
     }
   }
 
-  private static List<LlapTokenIdentifier> getLlapTokens(
+  static List<LlapTokenIdentifier> getLlapTokens(
       UserGroupInformation ugi, String clusterId) {
     List<LlapTokenIdentifier> tokens = null;
     for (TokenIdentifier id : ugi.getTokenIdentifiers()) {
@@ -81,7 +81,7 @@ private static List<LlapTokenIdentifier> getLlapTokens(
         LOG.debug("Token {}", id);
       }
       LlapTokenIdentifier llapId = (LlapTokenIdentifier)id;
-      if (!clusterId.equals(llapId.getClusterId())) continue;
+      if (clusterId != null && !clusterId.equals(llapId.getClusterId())) continue;
       if (tokens == null) {
         tokens = new ArrayList<>();
       }

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -1669,6 +1669,7 @@ private void maskPatterns(Pattern[] patterns, String fname) throws Exception {
           out.write(line);
           out.write("\n");
           lastWasMasked = true;
+          partialMaskWasMatched = false;
         }
       } else {
         out.write(line);

File: metastore/src/java/org/apache/hadoop/hive/metastore/messaging/json/JSONMessageFactory.java
Patch:
@@ -138,8 +138,8 @@ public AlterPartitionMessage buildAlterPartitionMessage(Table table, Partition b
   @Override
   public DropPartitionMessage buildDropPartitionMessage(Table table,
       Iterator<Partition> partitionsIterator) {
-    return new JSONDropPartitionMessage(MS_SERVER_URL, MS_SERVICE_PRINCIPAL, table.getDbName(),
-        table.getTableName(), getPartitionKeyValues(table, partitionsIterator), now());
+    return new JSONDropPartitionMessage(MS_SERVER_URL, MS_SERVICE_PRINCIPAL, table,
+        getPartitionKeyValues(table, partitionsIterator), now());
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExtractRow.java
Patch:
@@ -306,7 +306,8 @@ public Object extractRowColumn(VectorizedRowBatch batch, int batchIndex, int log
                 maxLengths[logicalColumnIndex]);
 
             HiveCharWritable hiveCharWritable = (HiveCharWritable) primitiveWritable;
-            hiveCharWritable.set(new String(bytes, start, adjustedLength, Charsets.UTF_8), -1);
+            hiveCharWritable.set(new String(bytes, start, adjustedLength, Charsets.UTF_8),
+                maxLengths[logicalColumnIndex]);
             return primitiveWritable;
           }
         case DECIMAL:

File: storage-api/src/java/org/apache/hadoop/hive/common/type/FastHiveDecimal.java
Patch:
@@ -235,7 +235,7 @@ protected boolean fastSetFromDouble(double doubleValue) {
 
   protected void fastFractionPortion() {
     FastHiveDecimalImpl.fastFractionPortion(
-        fastSignum, fast0, fast1, fast2, fastIntegerDigitCount, fastScale,
+        fastSignum, fast0, fast1, fast2, fastScale,
         this);
   }
 

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java
Patch:
@@ -387,7 +387,7 @@ public void stringifyValue(StringBuilder buffer, int row) {
     }
     if (noNulls || !isNull[row]) {
       buffer.append('"');
-      buffer.append(new String(this.buffer, start[row], length[row]));
+      buffer.append(new String(vector[row], start[row], length[row]));
       buffer.append('"');
     } else {
       buffer.append("null");

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/ColumnVector.java
Patch:
@@ -191,13 +191,13 @@ public void init() {
      * This method is deliberately *not* recursive because the complex types
      * can easily have more (or less) children than the upper levels.
      * @param size the new minimum size
-     * @param presesrveData should the old data be preserved?
+     * @param preserveData should the old data be preserved?
      */
-    public void ensureSize(int size, boolean presesrveData) {
+    public void ensureSize(int size, boolean preserveData) {
       if (isNull.length < size) {
         boolean[] oldArray = isNull;
         isNull = new boolean[size];
-        if (presesrveData && !noNulls) {
+        if (preserveData && !noNulls) {
           if (isRepeating) {
             isNull[0] = oldArray[0];
           } else {

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java
Patch:
@@ -182,7 +182,7 @@ public double getDouble(int elementNum) {
 
   /**
    * Return a double representation of a Timestamp.
-   * @param elementNum
+   * @param timestamp
    * @return
    */
   public static double getDouble(Timestamp timestamp) {

File: storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java
Patch:
@@ -109,7 +109,7 @@ public static Timestamp decimalToTimestamp(HiveDecimal dec) {
    * is the nanoseconds and integer part is the number of seconds.
    *
    * This is a HiveDecimalWritable variation with supplied scratch objects.
-   * @param decdecWritable
+   * @param decWritable
    * @param scratchDecWritable1
    * @param scratchDecWritable2
    * @return

File: ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java
Patch:
@@ -48,6 +48,7 @@
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.parse.WindowingSpec.Direction;
+import org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowType;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
@@ -61,7 +62,6 @@
 import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;
 import org.apache.hadoop.hive.ql.plan.TableScanDesc;
 import org.apache.hadoop.hive.ql.plan.ptf.BoundaryDef;
-import org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef;
 import org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef;
 import org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef;
 import org.apache.hadoop.hive.ql.plan.ptf.WindowTableFunctionDef;
@@ -332,7 +332,7 @@ private boolean canPushLimitToReduceSink(WindowTableFunctionDef wTFn) {
         }
         WindowFrameDef wdwFrame = wFnDef.getWindowFrame();
         BoundaryDef end = wdwFrame.getEnd();
-        if ( end instanceof ValueBoundaryDef ) {
+        if (wdwFrame.getWindowType() == WindowType.RANGE) {
           return false;
         }
         if ( end.getDirection() == Direction.FOLLOWING ) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
Patch:
@@ -217,7 +217,7 @@ private static StructTypeInfo pruneFromPaths(
 
   private static void pruneFromSinglePath(PrunedStructTypeInfo prunedInfo, String path) {
     Preconditions.checkArgument(prunedInfo != null,
-      "PrunedStructTypeInfo for path " + path + " should not be null");
+      "PrunedStructTypeInfo for path '" + path + "' should not be null");
 
     int index = path.indexOf('.');
     if (index < 0) {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -4127,7 +4127,7 @@ private static String[] convertVarsToRegex(String[] paramList) {
     "hive\\.cbo\\..*",
     "hive\\.convert\\..*",
     "hive\\.exec\\.dynamic\\.partition.*",
-    "hive\\.exec\\..*\\.dynamic\\.partitions\\..*",
+    "hive\\.exec\\.max\\.dynamic\\.partitions.*",
     "hive\\.exec\\.compress\\..*",
     "hive\\.exec\\.infer\\..*",
     "hive\\.exec\\.mode.local\\..*",

File: ql/src/test/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/TestSQLStdHiveAccessControllerHS2.java
Patch:
@@ -89,6 +89,7 @@ private List<String> getSettableParams() throws SecurityException, NoSuchFieldEx
     List<String> confVarRegexList = Arrays.asList("hive.convert.join.bucket.mapjoin.tez",
         "hive.optimize.index.filter.compact.maxsize", "hive.tez.dummy", "tez.task.dummy",
         "hive.exec.dynamic.partition", "hive.exec.dynamic.partition.mode",
+        "hive.exec.max.dynamic.partitions", "hive.exec.max.dynamic.partitions.pernode",
         "oozie.HadoopAccessorService.created", "tez.queue.name");
 
     // combine two lists

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
Patch:
@@ -244,7 +244,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
 
       if (limit >= 0 && memUsage > 0) {
         reducerHash = conf.isPTFReduceSink() ? new PTFTopNHash() : new TopNHash();
-        reducerHash.initialize(limit, memUsage, conf.isMapGroupBy(), this);
+        reducerHash.initialize(limit, memUsage, conf.isMapGroupBy(), this, conf, hconf);
       }
 
       useUniformHash = conf.getReducerTraits().contains(UNIFORM);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/OperatorDesc.java
Patch:
@@ -30,6 +30,8 @@ public interface OperatorDesc extends Serializable, Cloneable {
   public Map<String, String> getOpProps();
   public long getMemoryNeeded();
   public void setMemoryNeeded(long memoryNeeded);
+  public long getMaxMemoryAvailable();
+  public void setMaxMemoryAvailable(long memoryAvailble);
   public String getRuntimeStatsTmpDir();
   public void setRuntimeStatsTmpDir(String runtimeStatsTmpDir);
 }

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliConfigs.java
Patch:
@@ -479,6 +479,7 @@ public SparkOnYarnCliConfig() {
         setQueryDir("ql/src/test/queries/clientpositive");
 
         includesFrom(testConfigProps, "miniSparkOnYarn.query.files");
+        includesFrom(testConfigProps, "spark.only.query.files");
 
         setResultsDir("ql/src/test/results/clientpositive/spark");
         setLogDir("itests/qtest-spark/target/qfile-results/clientpositive/spark");

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -2501,6 +2501,7 @@ Operator<? extends OperatorDesc> vectorizeOperator(Operator<? extends OperatorDe
       case EXTRACT:
       case EVENT:
       case HASHTABLESINK:
+      case SPARKPRUNINGSINK:
         vectorOp = OperatorFactory.getVectorOperator(
             op.getCompilationOpContext(), op.getConf(), vContext);
         break;

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -332,8 +332,8 @@ public enum ErrorMsg {
             + "fails to construct aggregation for the partition "),
   ANALYZE_TABLE_PARTIALSCAN_AUTOGATHER(10233, "Analyze partialscan is not allowed " +
             "if hive.stats.autogather is set to false"),
-  PARTITION_VALUE_NOT_CONTINUOUS(10234, "Parition values specifed are not continuous." +
-            " A subpartition value is specified without specififying the parent partition's value"),
+  PARTITION_VALUE_NOT_CONTINUOUS(10234, "Partition values specified are not continuous." +
+            " A subpartition value is specified without specifying the parent partition's value"),
   TABLES_INCOMPATIBLE_SCHEMAS(10235, "Tables have incompatible schemas and their partitions " +
             " cannot be exchanged."),
 
@@ -440,7 +440,7 @@ public enum ErrorMsg {
   CANNOT_DROP_INDEX(10317, "Error while dropping index"),
   INVALID_AST_TREE(10318, "Internal error : Invalid AST"),
   ERROR_SERIALIZE_METASTORE(10319, "Error while serializing the metastore objects"),
-  IO_ERROR(10320, "Error while peforming IO operation "),
+  IO_ERROR(10320, "Error while performing IO operation "),
   ERROR_SERIALIZE_METADATA(10321, "Error while serializing the metadata"),
   INVALID_LOAD_TABLE_FILE_WORK(10322, "Invalid Load Table Work or Load File Work"),
   CLASSPATH_ERROR(10323, "Classpath error"),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ArchiveUtils.java
Patch:
@@ -87,7 +87,7 @@ static public PartSpecInfo create(Table tbl, Map<String, String> partSpec)
         }
         if (!itrPsKeys.next().toLowerCase().equals(
             fs.getName().toLowerCase())) {
-          throw new HiveException("Invalid partition specifiation: "
+          throw new HiveException("Invalid partition specification: "
               + partSpec);
         }
         prefixFields.add(fs);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -992,7 +992,7 @@ private int alterIndex(Hive db, AlterIndexDesc alterIndex) throws HiveException
 
       break;
     default:
-      console.printError("Unsupported Alter commnad");
+      console.printError("Unsupported Alter command");
       return 1;
     }
 
@@ -1691,7 +1691,7 @@ private int unarchive(Hive db, AlterTableSimpleDesc simpleDesc)
         if (ret != 0) {
           throw new HiveException("Error while copying files from archive, return code=" + ret);
         } else {
-          console.printInfo("Succefully Copied " + copySource + " to " + copyDest);
+          console.printInfo("Successfully Copied " + copySource + " to " + copyDest);
         }
 
         console.printInfo("Moving " + tmpPath + " to " + intermediateExtractedDir);
@@ -2633,7 +2633,7 @@ public static void dumpLockInfo(DataOutputStream os, ShowLocksResponse rsp) thro
     os.write(separator);
     os.writeBytes("Transaction ID");
     os.write(separator);
-    os.writeBytes("Last Hearbeat");
+    os.writeBytes("Last Heartbeat");
     os.write(separator);
     os.writeBytes("Acquired At");
     os.write(separator);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -489,7 +489,7 @@ private void dpSetup() {
     this.maxPartitions = dpCtx.getMaxPartitionsPerNode();
 
     assert numDynParts == dpColNames.size()
-        : "number of dynamic paritions should be the same as the size of DP mapping";
+        : "number of dynamic partitions should be the same as the size of DP mapping";
 
     if (dpColNames != null && dpColNames.size() > 0) {
       this.bDynParts = true;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
Patch:
@@ -322,7 +322,7 @@ public void process(Object row, int tag) throws HiveException {
         if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||
             conf.getWriteType() == AcidUtils.Operation.DELETE) {
           assert rowInspector instanceof StructObjectInspector :
-              "Exptected rowInspector to be instance of StructObjectInspector but it is a " +
+              "Expected rowInspector to be instance of StructObjectInspector but it is a " +
                   rowInspector.getClass().getName();
           acidRowInspector = (StructObjectInspector)rowInspector;
           // The record identifier is always in the first column

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java
Patch:
@@ -510,7 +510,7 @@ private void addFunction(String functionName, FunctionInfo function) {
       if (prev != null) {
         if (isBuiltInFunc(prev.getFunctionClass())) {
           throw new RuntimeException("Function " + functionName + " is hive builtin function, " +
-              "which cannot be overriden.");
+              "which cannot be overridden.");
         }
         prev.discarded();
       }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/SkewJoinHandler.java
Patch:
@@ -233,7 +233,7 @@ public void handleSkew(int tag) throws HiveException {
       // right now we assume that the group by is an ArrayList object. It may
       // change in future.
       if (!(dummyKey instanceof List)) {
-        throw new RuntimeException("Bug in handle skew key in a seperate job.");
+        throw new RuntimeException("Bug in handle skew key in a separate job.");
       }
 
       skewKeyInCurrentGroup = true;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -3379,7 +3379,7 @@ public static File createTempDir(String baseDir){
       }
     }
     throw new IllegalStateException("Failed to create a temp dir under "
-    + baseDir + " Giving up after " + MAX_ATTEMPS + " attemps");
+    + baseDir + " Giving up after " + MAX_ATTEMPS + " attempts");
 
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinEagerRowContainer.java
Patch:
@@ -161,7 +161,7 @@ public void write(MapJoinObjectSerDeContext context, ObjectOutputStream out)
       ++numRowsWritten;      
     }
     if(numRows != rowCount()) {
-      throw new ConcurrentModificationException("Values was modifified while persisting");
+      throw new ConcurrentModificationException("Values was modified while persisting");
     }
     if(numRowsWritten != numRows) {
       throw new IllegalStateException("Expected to write " + numRows + " but wrote " + numRowsWritten);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinTableContainerSerDe.java
Patch:
@@ -312,7 +312,7 @@ private MapJoinPersistableTableContainer create(
           clazz.getDeclaredConstructor(Map.class);
       return constructor.newInstance(metaData);
     } catch (Exception e) {
-      String msg = "Error while attemping to create table container" +
+      String msg = "Error while attempting to create table container" +
           " of type: " + name + ", with metaData: " + metaData;
       throw new HiveException(msg, e);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
Patch:
@@ -643,7 +643,7 @@ public DiskRangeList readEncodedStream(long baseOffset, DiskRangeList start, lon
       LOG.error("Failed " + (isCompressed ? "" : "un") + " compressed read; cOffset " + cOffset
           + ", endCOffset " + endCOffset + ", streamOffset " + streamOffset
           + ", unlockUntilCOffset " + unlockUntilCOffset + "; ranges passed in "
-          + RecordReaderUtils.stringifyDiskRanges(start) + "; ranges passed to prepate "
+          + RecordReaderUtils.stringifyDiskRanges(start) + "; ranges passed to prepare "
           + RecordReaderUtils.stringifyDiskRanges(current)); // Don't log exception here.
       throw (ex instanceof IOException) ? (IOException)ex : new IOException(ex);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java
Patch:
@@ -454,7 +454,7 @@ public void heartbeat() throws LockException {
           Hive db;
           try {
             db = Hive.get(conf);
-            // Create a new threadlocal synchronized metastore client for use in hearbeater threads.
+            // Create a new threadlocal synchronized metastore client for use in heartbeater threads.
             // This makes the concurrent use of heartbeat thread safe, and won't cause transaction
             // abort due to a long metastore client call blocking the heartbeat call.
             heartbeaterClient = new SynchronizedMetaStoreClient(db.getMSC());
@@ -465,7 +465,7 @@ public void heartbeat() throws LockException {
           }
           // Increment the threadlocal metastore client count
           if (heartbeaterMSClientCount.incrementAndGet() >= heartbeaterThreadPoolSize) {
-            LOG.warn("The number of hearbeater metastore clients - + "
+            LOG.warn("The number of heartbeater metastore clients - + "
                 + heartbeaterMSClientCount.get() + ", has exceeded the max limit - "
                 + heartbeaterThreadPoolSize);
           }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcCtx.java
Patch:
@@ -236,7 +236,7 @@ public Map<ColumnInfo, ExprNodeDesc> getPropagatedConstants(Operator<? extends S
         }
       }
     }
-    LOG.debug("Offerring constants " + constants.keySet() + " to operator " + op.toString());
+    LOG.debug("Offering constants " + constants.keySet() + " to operator " + op.toString());
     return constants;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java
Patch:
@@ -357,7 +357,7 @@ private static ExprNodeDesc foldExprShortcut(ExprNodeDesc desc, Map<ColumnInfo,
       // Don't evaluate nondeterministic function since the value can only calculate during runtime.
       if (!isDeterministicUdf(udf, newExprs)) {
         if (LOG.isDebugEnabled()) {
-          LOG.debug("Function " + udf.getClass() + " is undeterministic. Don't evalulate immediately.");
+          LOG.debug("Function " + udf.getClass() + " is undeterministic. Don't evaluate immediately.");
         }
         ((ExprNodeGenericFuncDesc) desc).setChildren(newExprs);
         return desc;
@@ -990,7 +990,7 @@ private static ExprNodeDesc evaluateFunction(GenericUDF udf, List<ExprNodeDesc>
       return new ExprNodeConstantDesc(o).setFoldedFromVal(constStr);
     } catch (HiveException e) {
       LOG.error("Evaluation function " + udf.getClass()
-          + " failed in Constant Propagatation Optimizer.");
+          + " failed in Constant Propagation Optimizer.");
       throw new RuntimeException(e);
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -558,7 +558,7 @@ public static void setMapWork(MapWork plan, ParseContext parseCtx, Set<ReadEntit
           LOG.info("Skip optimization to reduce input size of 'limit'");
           parseCtx.getGlobalLimitCtx().disableOpt();
         } else if (parts.isEmpty()) {
-          LOG.info("Empty input: skip limit optimiztion");
+          LOG.info("Empty input: skip limit optimization");
         } else {
           LOG.info("Try to reduce input size for 'limit' " +
               "sizeNeeded: " + sizeNeeded +
@@ -1479,7 +1479,7 @@ public static void addStatsTask(FileSinkOperator nd, MoveTask mvTask,
     } else if (mvWork.getLoadFileWork() != null) {
       statsWork = new StatsWork(mvWork.getLoadFileWork());
     }
-    assert statsWork != null : "Error when genereting StatsTask";
+    assert statsWork != null : "Error when generating StatsTask";
 
     statsWork.setSourceTask(currTask);
     statsWork.setStatsReliable(hconf.getBoolVar(ConfVars.HIVE_STATS_RELIABLE));

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
Patch:
@@ -491,7 +491,7 @@ private InputCtx getInputCtx(ExprNodeColumnDesc col) throws SemanticException {
       }
 
       if (noInp > 1)
-        throw new RuntimeException("Ambigous column mapping");
+        throw new RuntimeException("Ambiguous column mapping");
     }
 
     return ctxLookingFor;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/QueryPlanTreeTransformation.java
Patch:
@@ -229,7 +229,7 @@ protected static void applyCorrelation(
               handledRSs.add((ReduceSinkOperator)op);
               parentsOfMux.add(CorrelationUtilities.getSingleParent(op, true));
             } else {
-              throw new SemanticException("An slibing of ReduceSinkOperator is nethier a " +
+              throw new SemanticException("A sibling of ReduceSinkOperator is neither a " +
                   "DemuxOperator nor a ReduceSinkOperator");
             }
           }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/listbucketingpruner/ListBucketingPrunerUtils.java
Patch:
@@ -247,7 +247,7 @@ private static Boolean startComparisonInEqualNode(final List<String> skewedCols,
     String constantValueInFilter = ((ExprNodeConstantDesc) right).getValue().toString();
     assert (skewedCols.contains(columnNameInFilter)) : "List bucketing pruner has a column name "
         + columnNameInFilter
-        + " which is not found in the partiton's skewed column list";
+        + " which is not found in the partition's skewed column list";
     int index = skewedCols.indexOf(columnNameInFilter);
     assert (index < cell.size()) : "GenericUDFOPEqual has a ExprNodeColumnDesc ("
         + columnNameInFilter + ") which is " + index + "th" + "skewed column. "

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -1935,7 +1935,7 @@ private Pair<Boolean,Boolean> validateAggregationDesc(AggregationDesc aggDesc, P
     } catch (Exception e) {
       // We should have already attempted to vectorize in validateAggregationDesc.
       if (LOG.isDebugEnabled()) {
-        LOG.debug("Vectorization of aggreation should have succeeded ", e);
+        LOG.debug("Vectorization of aggregation should have succeeded ", e);
       }
       return new Pair<Boolean,Boolean>(false, false);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
Patch:
@@ -170,7 +170,7 @@ public static PrunedPartitionList prune(Table tab, ExprNodeDesc prunerExpr,
           throws SemanticException {
 
     if (LOG.isTraceEnabled()) {
-      LOG.trace("Started pruning partiton");
+      LOG.trace("Started pruning partition");
       LOG.trace("dbname = " + tab.getDbName());
       LOG.trace("tabname = " + tab.getTableName());
       LOG.trace("prune Expression = " + (prunerExpr == null ? "" : prunerExpr));

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -3333,7 +3333,7 @@ private RelNode genSelectLogicalPlan(QB qb, RelNode srcRel, RelNode starSrcRel)
         } else if (ParseUtils.containsTokenOfType(expr, HiveParser.TOK_FUNCTIONDI)
             && !(srcRel instanceof HiveAggregate)) {
           // Likely a malformed query eg, select hash(distinct c1) from t1;
-          throw new CalciteSemanticException("Distinct without an aggreggation.",
+          throw new CalciteSemanticException("Distinct without an aggregation.",
               UnsupportedFeature.Distinct_without_an_aggreggation);
         } else {
           // Case when this is an expression

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -1637,7 +1637,7 @@ private void analyzeAlterTablePartMergeFiles(ASTNode ast,
       if (!((inputFormatClass.equals(RCFileInputFormat.class) ||
           (inputFormatClass.equals(OrcInputFormat.class))))) {
         throw new SemanticException(
-            "Only RCFile and ORCFile Formats are supportted right now.");
+            "Only RCFile and ORCFile Formats are supported right now.");
       }
       mergeDesc.setInputFormatClass(inputFormatClass);
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -7174,7 +7174,7 @@ private void genAutoColumnStatsGatheringPipeline(QB qb, TableDesc table_desc,
     } catch (HiveException e) {
       throw new SemanticException(e.getMessage());
     }
-    LOG.info("Generate an operator pipleline to autogather column stats for table " + tableName
+    LOG.info("Generate an operator pipeline to autogather column stats for table " + tableName
         + " in query " + ctx.getCmd());
     ColumnStatsAutoGatherContext columnStatsAutoGatherContext = null;
     columnStatsAutoGatherContext = new ColumnStatsAutoGatherContext(this, conf, curr, table, partSpec, isInsertInto, ctx);
@@ -11824,7 +11824,7 @@ ASTNode analyzeCreateTable(
         }
       } catch (HiveException e) {
         // should not occur since second parameter to getTableWithQN is false
-        throw new IllegalStateException("Unxpected Exception thrown: " + e.getMessage(), e);
+        throw new IllegalStateException("Unexpected Exception thrown: " + e.getMessage(), e);
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java
Patch:
@@ -202,7 +202,7 @@ public void compile(final ParseContext pCtx, final List<Task<? extends Serializa
       int fetchLimit = HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVELIMITOPTMAXFETCH);
       if (globalLimitCtx.isEnable() && globalLimitCtx.getGlobalLimit() > fetchLimit) {
         LOG.info("For FetchTask, LIMIT " + globalLimitCtx.getGlobalLimit() + " > " + fetchLimit
-            + ". Doesn't qualify limit optimiztion.");
+            + ". Doesn't qualify limit optimization.");
         globalLimitCtx.disableOpt();
 
       }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
Patch:
@@ -350,7 +350,7 @@ protected void generateTaskTree(List<Task<? extends Serializable>> rootTasks, Pa
     opRules.put(new RuleRegExp("No more walking on ReduceSink-MapJoin",
         MapJoinOperator.getOperatorName() + "%"), new ReduceSinkMapJoinProc());
 
-    opRules.put(new RuleRegExp("Recoginze a Sorted Merge Join operator to setup the right edge and"
+    opRules.put(new RuleRegExp("Recognize a Sorted Merge Join operator to setup the right edge and"
         + " stop traversing the DummyStore-MapJoin", CommonMergeJoinOperator.getOperatorName()
         + "%"), new MergeJoinProc());
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/FetchWork.java
Patch:
@@ -218,7 +218,7 @@ public ArrayList<PartitionDesc> getPartDescOrderedByPartDir() {
     if (partDir != null && partDir.size() > 1) {
       if (partDesc == null || partDir.size() != partDesc.size()) {
         throw new RuntimeException(
-            "Partiton Directory list size doesn't match Partition Descriptor list size");
+            "Partition Directory list size doesn't match Partition Descriptor list size");
       }
 
       // Construct a sorted Map of Partition Dir - Partition Descriptor; ordering is based on

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/SettableConfigUpdater.java
Patch:
@@ -47,7 +47,7 @@ public static void setHiveConfWhiteList(HiveConf hiveConf) throws HiveAuthzPlugi
     if(whiteListParamsStr == null || whiteListParamsStr.trim().isEmpty()) {
       throw new HiveAuthzPluginException("Configuration parameter "
           + ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST.varname
-          + " is not iniatialized.");
+          + " is not initialized.");
     }
 
     // append regexes that user wanted to add

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStringToMap.java
Patch:
@@ -42,7 +42,7 @@
  */
 @Description(name = "str_to_map", value = "_FUNC_(text, delimiter1, delimiter2) - "
     + "Creates a map by parsing text ", extended = "Split text into key-value pairs"
-    + " using two delimiters. The first delimiter seperates pairs, and the"
+    + " using two delimiters. The first delimiter separates pairs, and the"
     + " second delimiter sperates key and value. If only one parameter is given, default"
     + " delimiters are used: ',' as delimiter1 and '=' as delimiter2.")
 public class GenericUDFStringToMap extends GenericUDF {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTrunc.java
Patch:
@@ -126,7 +126,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen
         return initializeNumber(arguments);
       } else {
         throw new UDFArgumentException(
-            "Only primitive type arguments are accepted, when arguments lenght is one, got "
+            "Only primitive type arguments are accepted, when arguments length is one, got "
                 + arguments[1].getTypeName());
       }
     }
@@ -478,4 +478,4 @@ protected BigDecimal trunc(BigDecimal input, int scale) {
     return output;
   }
   
-}
\ No newline at end of file
+}

File: itests/hive-unit/src/test/java/org/apache/hive/service/cli/session/TestHiveSessionImpl.java
Patch:
@@ -48,7 +48,7 @@ public void testLeakOperationHandle() throws HiveSQLException {
     String password = "";
     HiveConf serverhiveConf = new HiveConf();
     String ipAddress = null;
-    HiveSessionImpl session = new HiveSessionImpl(protocol, username, password,
+    HiveSessionImpl session = new HiveSessionImpl(null, protocol, username, password,
       serverhiveConf, ipAddress) {
       @Override
       protected synchronized void acquire(boolean userAccess, boolean isOperation) {

File: service/src/java/org/apache/hive/service/AbstractService.java
Patch:
@@ -50,7 +50,7 @@ public abstract class AbstractService implements Service {
   /**
    * The configuration. Will be null until the service is initialized.
    */
-  private HiveConf hiveConf;
+  protected HiveConf hiveConf;
 
   /**
    * List of state change listeners; it is final to ensure

File: service/src/java/org/apache/hive/service/cli/CLIService.java
Patch:
@@ -62,7 +62,6 @@ public class CLIService extends CompositeService implements ICLIService {
 
   private final Logger LOG = LoggerFactory.getLogger(CLIService.class.getName());
 
-  private HiveConf hiveConf;
   private SessionManager sessionManager;
   private UserGroupInformation serviceUGI;
   private UserGroupInformation httpUGI;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -2289,7 +2289,7 @@ public static boolean isEmptyPath(JobConf job, Path dirPath, Context ctx)
     return isEmptyPath(job, dirPath);
   }
 
-  public static boolean isEmptyPath(JobConf job, Path dirPath) throws Exception {
+  public static boolean isEmptyPath(Configuration job, Path dirPath) throws IOException {
     FileSystem inpFs = dirPath.getFileSystem(job);
     try {
       FileStatus[] fStats = inpFs.listStatus(dirPath, FileUtils.HIDDEN_FILES_PATH_FILTER);
@@ -3073,7 +3073,7 @@ private static Path createDummyFileForEmptyPartition(Path path, JobConf job, Map
         props, oneRow);
 
     if (LOG.isInfoEnabled()) {
-      LOG.info("Changed input file " + strPath + " to empty file " + newPath);
+      LOG.info("Changed input file " + strPath + " to empty file " + newPath + " (" + oneRow + ")");
     }
 
     // update the work

File: ql/src/java/org/apache/hadoop/hive/ql/io/NullRowsInputFormat.java
Patch:
@@ -73,7 +73,8 @@ public static class NullRowsRecordReader implements RecordReader {
     public NullRowsRecordReader(Configuration conf, InputSplit split) throws IOException {
       boolean isVectorMode = Utilities.getUseVectorizedInputFileFormat(conf);
       if (LOG.isDebugEnabled()) {
-        LOG.debug("Null record reader in " + (isVectorMode ? "" : "non-") + "vector mode");
+        LOG.debug(getClass().getSimpleName() + " in "
+            + (isVectorMode ? "" : "non-") + "vector mode");
       }
       if (isVectorMode) {
         rbCtx = Utilities.getVectorizedRowBatchCtx(conf);

File: ql/src/java/org/apache/hadoop/hive/ql/io/OneNullRowInputFormat.java
Patch:
@@ -48,7 +48,7 @@ public OneNullRowRecordReader(Configuration conf, InputSplit split) throws IOExc
       super(conf, split);
     }
 
-    private boolean processed;
+    protected boolean processed;
 
     @Override
     public long getPos() throws IOException {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
Patch:
@@ -1709,7 +1709,7 @@ private VectorExpression getInExpression(List<ExprNodeDesc> childExpr,
       cl = (mode == VectorExpressionDescriptor.Mode.FILTER ? FilterLongColumnInList.class : LongColumnInList.class);
       long[] inVals = new long[childrenForInList.size()];
       for (int i = 0; i != inVals.length; i++) {
-        inVals[i] = (Integer) getVectorTypeScalarValue((ExprNodeConstantDesc) childrenForInList.get(i));
+        inVals[i] = (Long) getVectorTypeScalarValue((ExprNodeConstantDesc) childrenForInList.get(i));
       }
       expr = createVectorExpression(cl, childExpr.subList(0, 1), VectorExpressionDescriptor.Mode.PROJECTION, returnType);
       ((ILongInExpr) expr).setInListValues(inVals);
@@ -2326,7 +2326,7 @@ private Object getVectorTypeScalarValue(ExprNodeConstantDesc constDesc) throws H
     Object scalarValue = getScalarValue(constDesc);
     switch (type) {
       case DATE:
-        return DateWritable.dateToDays((Date) scalarValue);
+        return new Long(DateWritable.dateToDays((Date) scalarValue));
       case INTERVAL_YEAR_MONTH:
         return ((HiveIntervalYearMonth) scalarValue).getTotalMonths();
       default:

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateAddScalarCol.java
Patch:
@@ -56,6 +56,8 @@ public VectorUDFDateAddScalarCol(Object object, int colNum, int outputColumn) {
         this.timestampValue = (Timestamp) object;
     } else if (object instanceof byte []) {
       this.stringValue = (byte[]) object;
+    } else {
+      throw new RuntimeException("Unexpected scalar object " + object.getClass().getName() + " " + object.toString());
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffColScalar.java
Patch:
@@ -57,6 +57,8 @@ public VectorUDFDateDiffColScalar(int colNum, Object object, int outputColumn) {
       this.timestampValue = (Timestamp) object;
     } else if (object instanceof byte []) {
       this.stringValue = (byte []) object;
+    } else {
+      throw new RuntimeException("Unexpected scalar object " + object.getClass().getName() + " " + object.toString());
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffScalarCol.java
Patch:
@@ -56,6 +56,8 @@ public VectorUDFDateDiffScalarCol(Object object, int colNum, int outputColumn) {
       this.timestampValue = (Timestamp) object;
     } else if (object instanceof byte []) {
       this.stringValue = (byte[]) object;
+    } else {
+      throw new RuntimeException("Unexpected scalar object " + object.getClass().getName() + " " + object.toString());
     }
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorGenericDateExpressions.java
Patch:
@@ -511,7 +511,7 @@ public void testDateDiffColScalar() {
     byte[] bytes = "error".getBytes(utf8);
     VectorizedRowBatch batch = new VectorizedRowBatch(2, 1);
 
-    udf = new VectorUDFDateDiffColScalar(0, 0, 1);
+    udf = new VectorUDFDateDiffColScalar(0, 0L, 1);
     udf.setInputTypes(VectorExpression.Type.TIMESTAMP, VectorExpression.Type.STRING);
     batch.cols[0] = new BytesColumnVector(1);
     batch.cols[1] = new LongColumnVector(1);

File: llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java
Patch:
@@ -279,9 +279,7 @@ private ServiceInstance selectServiceInstance(Set<ServiceInstance> serviceInstan
 
     // Get the first live service instance
     for (ServiceInstance serviceInstance : serviceInstances) {
-      if (serviceInstance.isAlive()) {
-        return serviceInstance;
-      }
+      return serviceInstance;
     }
 
     LOG.info("No live service instances were found");

File: llap-tez/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapTaskSchedulerService.java
Patch:
@@ -210,7 +210,7 @@ public void testNodeDisabled() throws IOException, InterruptedException {
       assertEquals(3, tsWrapper.ts.instanceToNodeMap.size());
       LlapTaskSchedulerService.NodeInfo disabledNodeInfo = tsWrapper.ts.disabledNodesQueue.peek();
       assertNotNull(disabledNodeInfo);
-      assertEquals(HOST1, disabledNodeInfo.serviceInstance.getHost());
+      assertEquals(HOST1, disabledNodeInfo.getHost());
       assertEquals((10000l), disabledNodeInfo.getDelay(TimeUnit.MILLISECONDS));
       assertEquals((10000l + 10000l), disabledNodeInfo.expireTimeMillis);
 

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java
Patch:
@@ -35,6 +35,7 @@
 import org.apache.hadoop.hive.ql.io.orc.encoded.Consumer;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
 import org.apache.hadoop.mapred.FileSplit;
+import org.apache.orc.TypeDescription;
 
 public class OrcColumnVectorProducer implements ColumnVectorProducer {
 
@@ -64,12 +65,12 @@ public OrcColumnVectorProducer(OrcMetadataCache metadataCache,
   public ReadPipeline createReadPipeline(
       Consumer<ColumnVectorBatch> consumer, FileSplit split,
       List<Integer> columnIds, SearchArgument sarg, String[] columnNames,
-      QueryFragmentCounters counters) throws IOException {
+      QueryFragmentCounters counters, TypeDescription readerSchema) throws IOException {
     cacheMetrics.incrCacheReadRequests();
     OrcEncodedDataConsumer edc = new OrcEncodedDataConsumer(consumer, columnIds.size(),
         _skipCorrupt, counters, ioMetrics);
     OrcEncodedDataReader reader = new OrcEncodedDataReader(lowLevelCache, bufferManager,
-        metadataCache, conf, split, columnIds, sarg, columnNames, edc, counters);
+        metadataCache, conf, split, columnIds, sarg, columnNames, edc, counters, readerSchema);
     edc.init(reader, reader);
     return edc;
   }

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/ReadPipeline.java
Patch:
@@ -26,5 +26,6 @@
 public interface ReadPipeline extends ConsumerFeedback<ColumnVectorBatch> {
   public Callable<Void> getReadCallable();
   TypeDescription getFileSchema();
+  TypeDescription getReaderSchema();
   boolean[] getIncludedColumns();
 }
\ No newline at end of file

File: orc/src/java/org/apache/orc/impl/TreeReaderFactory.java
Patch:
@@ -2082,8 +2082,7 @@ public static TreeReader createTreeReader(TypeDescription readerType,
                                             boolean skipCorrupt
                                             ) throws IOException {
     TypeDescription fileType = evolution.getFileType(readerType);
-    if (fileType == null ||
-        (included != null && !included[readerType.getId()])) {
+    if (fileType == null || !evolution.includeReaderColumn(readerType.getId())){
       return new NullTreeReader(0);
     }
     TypeDescription.Category readerTypeCategory = readerType.getCategory();

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java
Patch:
@@ -43,6 +43,7 @@
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.orc.OrcProto;
+import org.apache.orc.OrcUtils;
 import org.apache.orc.TypeDescription;
 
 /**
@@ -74,7 +75,6 @@ static class VectorizedOrcRecordReader
       /**
        * Do we have schema on read in the configuration variables?
        */
-      List<OrcProto.Type> types = file.getTypes();
       int dataColumns = rbCtx.getDataColumnCount();
       TypeDescription schema =
           OrcInputFormat.getDesiredRowTypeDescr(conf, false, dataColumns);
@@ -91,12 +91,13 @@ static class VectorizedOrcRecordReader
           }
         }
       }
+      List<OrcProto.Type> types = OrcUtils.getOrcTypes(schema);
       Reader.Options options = new Reader.Options().schema(schema);
 
       this.offset = fileSplit.getStart();
       this.length = fileSplit.getLength();
       options.range(offset, length);
-      options.include(OrcInputFormat.genIncludedColumns(types, conf, true));
+      options.include(OrcInputFormat.genIncludedColumns(schema, conf));
       OrcInputFormat.setSearchArgument(options, types, conf, true);
 
       this.reader = file.rowsOptions(options);

File: metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -3615,7 +3615,7 @@ private void addForeignKeys(
       }
 
       MColumnDescriptor parentCD = retrieveCD ? nParentTable.mcd : parentTable.getSd().getCD();
-      List<MFieldSchema> parentCols = parentCD.getCols();
+      List<MFieldSchema> parentCols = parentCD == null ? null : parentCD.getCols();
       int parentIntegerIndex =
         getColumnIndexFromTableColumns(parentCols, fks.get(i).getPkcolumn_name());
       if (parentIntegerIndex == -1) {
@@ -3690,7 +3690,7 @@ private void addPrimaryKeys(List<SQLPrimaryKey> pks, boolean retrieveCD) throws
 
       MColumnDescriptor parentCD = retrieveCD ? nParentTable.mcd : parentTable.getSd().getCD();
       int parentIntegerIndex =
-        getColumnIndexFromTableColumns(parentCD.getCols(), pks.get(i).getColumn_name());
+        getColumnIndexFromTableColumns(parentCD == null ? null : parentCD.getCols(), pks.get(i).getColumn_name());
 
       if (parentIntegerIndex == -1) {
         throw new InvalidObjectException("Parent column not found: " + pks.get(i).getColumn_name());

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonStorage.java
Patch:
@@ -93,7 +93,7 @@ public void saveField(Type type, String id, String key, String val)
    * Get the id of each data grouping of a given type in the storage
    * system.
    * @param type The data type (as listed above)
-   * @return An ArrayList<String> of ids.
+   * @return A list of ids.
    */
   public List<String> getAllForType(Type type);
 

File: orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java
Patch:
@@ -292,7 +292,7 @@ void startStripe(Map<StreamName, InStream> streams,
     }
 
     @Override
-    void seek(PositionProvider[] index) throws IOException {
+    public void seek(PositionProvider[] index) throws IOException {
      // Pass-thru.
       convertTreeReader.seek(index);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -3636,7 +3636,7 @@ public static StandardStructObjectInspector constructVectorizedReduceRowOI(
    * @param conf - configuration
    * @return false for types not supported by vectorization, true otherwise
    */
-  public static boolean checkLlapIOSupportedTypes(final Configuration conf) {
+  public static boolean checkVectorizerSupportedTypes(final Configuration conf) {
     final String[] readColumnNames = ColumnProjectionUtils.getReadColumnNames(conf);
     final String columnNames = conf.get(serdeConstants.LIST_COLUMNS);
     final String columnTypes = conf.get(serdeConstants.LIST_COLUMN_TYPES);
@@ -3649,7 +3649,7 @@ public static boolean checkLlapIOSupportedTypes(final Configuration conf) {
     final List<String> allColumnNames = Lists.newArrayList(columnNames.split(","));
     final List<TypeInfo> typeInfos = TypeInfoUtils.getTypeInfosFromTypeString(columnTypes);
     final List<String> allColumnTypes = TypeInfoUtils.getTypeStringsFromTypeInfo(typeInfos);
-    return checkLlapIOSupportedTypes(Lists.newArrayList(readColumnNames), allColumnNames,
+    return checkVectorizerSupportedTypes(Lists.newArrayList(readColumnNames), allColumnNames,
         allColumnTypes);
   }
 
@@ -3660,7 +3660,7 @@ public static boolean checkLlapIOSupportedTypes(final Configuration conf) {
    * @param allColumnTypes - all column types
    * @return false for types not supported by vectorization, true otherwise
    */
-  public static boolean checkLlapIOSupportedTypes(final List<String> readColumnNames,
+  public static boolean checkVectorizerSupportedTypes(final List<String> readColumnNames,
       final List<String> allColumnNames, final List<String> allColumnTypes) {
     final String[] readColumnTypes = getReadColumnTypes(readColumnNames, allColumnNames,
         allColumnTypes);

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
Patch:
@@ -282,7 +282,7 @@ public SerDeStats getStats() {
    * @param isOriginal is the file in the original format?
    * @return the column number for the root of row.
    */
-  static int getRootColumn(boolean isOriginal) {
+  public static int getRootColumn(boolean isOriginal) {
     return isOriginal ? 0 : (OrcRecordUpdater.ROW + 1);
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReader.java
Patch:
@@ -55,4 +55,4 @@ void readEncodedColumns(int stripeIx, StripeInformation stripe,
    * to just checking the constant in the first place.
    */
   void setTracing(boolean isEnabled);
-}
\ No newline at end of file
+}

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -825,9 +825,9 @@ private String processTable(QB qb, ASTNode tabref) throws SemanticException {
 
     qb.getParseInfo().setSrcForAlias(alias, tableTree);
 
-    // if alias to CTE contains the alias, we do not do the translation because
+    // if alias to CTE contains the table name, we do not do the translation because
     // cte is actually a subquery.
-    if (!this.aliasToCTEs.containsKey(alias)) {
+    if (!this.aliasToCTEs.containsKey(tabIdName)) {
       unparseTranslator.addTableNameTranslation(tableTree, SessionState.get().getCurrentDatabase());
       if (aliasIndex != 0) {
         unparseTranslator.addIdentifierTranslation((ASTNode) tabref.getChild(aliasIndex));

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/timestamp/NanoTimeUtils.java
Patch:
@@ -49,7 +49,7 @@ private static Calendar getLocalCalendar() {
      return parquetLocalCalendar.get();
    }
 
-   private static Calendar getCalendar(boolean skipConversion) {
+   public static Calendar getCalendar(boolean skipConversion) {
      Calendar calendar = skipConversion ? getLocalCalendar() : getGMTCalendar();
      calendar.clear(); // Reset all fields before reusing this instance
      return calendar;

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloStorageHandler.java
Patch:
@@ -53,7 +53,7 @@
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;
 import org.apache.hadoop.hive.serde2.Deserializer;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobConf;
@@ -148,7 +148,7 @@ public void setConf(Configuration conf) {
 
   @SuppressWarnings("deprecation")
   @Override
-  public Class<? extends SerDe> getSerDeClass() {
+  public Class<? extends AbstractSerDe> getSerDeClass() {
     return AccumuloSerDe.class;
   }
 

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/AccumuloSerDe.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hadoop.hive.accumulo.LazyAccumuloRow;
 import org.apache.hadoop.hive.accumulo.columns.ColumnMapping;
 import org.apache.hadoop.hive.accumulo.columns.HiveAccumuloRowIdColumnMapping;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeStats;
 import org.apache.hadoop.hive.serde2.lazy.LazyFactory;
@@ -45,7 +45,7 @@
  * Deserialization from Accumulo to LazyAccumuloRow for Hive.
  *
  */
-public class AccumuloSerDe implements SerDe {
+public class AccumuloSerDe extends AbstractSerDe {
 
   private AccumuloSerDeParameters accumuloSerDeParameters;
   private LazyAccumuloRow cachedRow;

File: beeline/src/java/org/apache/hive/beeline/BeeLineOpts.java
Patch:
@@ -71,7 +71,7 @@ class BeeLineOpts implements Completer {
   private boolean showDbInPrompt = false;
   private int headerInterval = 100;
   private boolean fastConnect = true;
-  private boolean autoCommit = false;
+  private boolean autoCommit = true;
   private boolean verbose = false;
   private boolean force = false;
   private boolean incremental = true;

File: contrib/src/test/org/apache/hadoop/hive/contrib/serde2/TestRegexSerDe.java
Patch:
@@ -23,7 +23,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -36,7 +36,7 @@
  */
 public class TestRegexSerDe extends TestCase {
 
-  private SerDe createSerDe(String fieldNames, String fieldTypes,
+  private AbstractSerDe createSerDe(String fieldNames, String fieldTypes,
       String inputRegex, String outputFormatString) throws Throwable {
     Properties schema = new Properties();
     schema.setProperty(serdeConstants.LIST_COLUMNS, fieldNames);
@@ -55,7 +55,7 @@ private SerDe createSerDe(String fieldNames, String fieldTypes,
   public void testRegexSerDe() throws Throwable {
     try {
       // Create the SerDe
-      SerDe serDe = createSerDe(
+      AbstractSerDe serDe = createSerDe(
           "host,identity,user,time,request,status,size,referer,agent",
           "string,string,string,string,string,string,string,string,string",
           "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") " 

File: druid-handler/src/java/org/apache/hadoop/hive/druid/DruidStorageHandler.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.OutputFormat;
 import org.slf4j.Logger;
@@ -50,7 +50,7 @@ public Class<? extends OutputFormat> getOutputFormatClass() {
   }
 
   @Override
-  public Class<? extends SerDe> getSerDeClass() {
+  public Class<? extends AbstractSerDe> getSerDeClass() {
     return DruidSerDe.class;
   }
 

File: druid-handler/src/test/org/apache/hadoop/hive/druid/QTestDruidStorageHandler.java
Patch:
@@ -17,7 +17,7 @@
  */
 package org.apache.hadoop.hive.druid;
 
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 
 /**
  * Storage handler for Druid to be used in tests. It cannot connect to
@@ -27,7 +27,7 @@
 public class QTestDruidStorageHandler extends DruidStorageHandler {
 
   @Override
-  public Class<? extends SerDe> getSerDeClass() {
+  public Class<? extends AbstractSerDe> getSerDeClass() {
     return QTestDruidSerDe.class;
   }
 

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDe.java
Patch:
@@ -30,7 +30,7 @@
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.AbstractSerDe;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeSpec;
 import org.apache.hadoop.hive.serde2.SerDeStats;
@@ -117,7 +117,7 @@ public HBaseSerDe() throws SerDeException {
 
   /**
    * Initialize the SerDe given parameters.
-   * @see SerDe#initialize(Configuration, Properties)
+   * @see AbstractSerDe#initialize(Configuration, Properties)
    */
   @Override
   public void initialize(Configuration conf, Properties tbl)
@@ -268,7 +268,7 @@ public HBaseSerDeParameters getHBaseSerdeParam() {
    * Deserialize a row from the HBase Result writable to a LazyObject
    * @param result the HBase Result Writable containing the row
    * @return the deserialized object
-   * @see SerDe#deserialize(Writable)
+   * @see AbstractSerDe#deserialize(Writable)
    */
   @Override
   public Object deserialize(Writable result) throws SerDeException {

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java
Patch:
@@ -65,7 +65,7 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan;
 import org.apache.hadoop.hive.serde2.Deserializer;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping;
@@ -315,7 +315,7 @@ public Class<? extends OutputFormat> getOutputFormatClass() {
   }
 
   @Override
-  public Class<? extends SerDe> getSerDeClass() {
+  public Class<? extends AbstractSerDe> getSerDeClass() {
     return HBaseSerDe.class;
   }
 

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordSerDe.java
Patch:
@@ -27,7 +27,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeSpec;
 import org.apache.hadoop.hive.serde2.SerDeStats;
@@ -56,7 +56,7 @@
 @SerDeSpec(schemaProps = {serdeConstants.LIST_COLUMNS,
                           serdeConstants.LIST_COLUMN_TYPES})
 
-public class HCatRecordSerDe implements SerDe {
+public class HCatRecordSerDe extends AbstractSerDe {
 
   private static final Logger LOG = LoggerFactory.getLogger(HCatRecordSerDe.class);
 

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java
Patch:
@@ -38,7 +38,7 @@
 import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeSpec;
 import org.apache.hadoop.hive.serde2.SerDeStats;
@@ -89,7 +89,7 @@
                           serdeConstants.LIST_COLUMN_TYPES,
                           serdeConstants.TIMESTAMP_FORMATS})
 
-public class JsonSerDe implements SerDe {
+public class JsonSerDe extends AbstractSerDe {
 
   private static final Logger LOG = LoggerFactory.getLogger(JsonSerDe.class);
   private List<String> columnNames;

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DefaultRecordWriterContainer.java
Patch:
@@ -22,7 +22,7 @@
 import java.io.IOException;
 
 import org.apache.hadoop.hive.ql.metadata.HiveStorageHandler;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.io.Writable;
@@ -39,7 +39,7 @@
 class DefaultRecordWriterContainer extends RecordWriterContainer {
 
   private final HiveStorageHandler storageHandler;
-  private final SerDe serDe;
+  private final AbstractSerDe serDe;
   private final OutputJobInfo jobInfo;
   private final ObjectInspector hcatRecordOI;
 

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputFormatContainer.java
Patch:
@@ -29,7 +29,7 @@
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
 import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
@@ -82,8 +82,8 @@ public RecordWriter<WritableComparable<?>, HCatRecord> getRecordWriter(TaskAttem
     StorerInfo storeInfo = jobInfo.getTableInfo().getStorerInfo();
     HiveStorageHandler storageHandler = HCatUtil.getStorageHandler(
       context.getConfiguration(), storeInfo);
-    Class<? extends SerDe> serde = storageHandler.getSerDeClass();
-    SerDe sd = (SerDe) ReflectionUtils.newInstance(serde,
+    Class<? extends AbstractSerDe> serde = storageHandler.getSerDeClass();
+    AbstractSerDe sd = (AbstractSerDe) ReflectionUtils.newInstance(serde,
       context.getConfiguration());
     context.getConfiguration().set("mapred.output.value.class",
       sd.getSerializedClass().getName());

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.serde2.Deserializer;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
@@ -141,7 +141,7 @@ private static ObjectInspector getObjectInspector(TypeInfo type) throws IOExcept
 
   //TODO this has to find a better home, it's also hardcoded as default in hive would be nice
   // if the default was decided by the serde
-  static void initializeOutputSerDe(SerDe serDe, Configuration conf, OutputJobInfo jobInfo)
+  static void initializeOutputSerDe(AbstractSerDe serDe, Configuration conf, OutputJobInfo jobInfo)
     throws SerDeException {
     SerDeUtils.initializeSerDe(serDe, conf,
                                getSerdeProperties(jobInfo.getTableInfo(),

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.ql.io.AcidOutputFormat;
 import org.apache.hadoop.hive.ql.io.RecordUpdater;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -152,7 +152,7 @@ private List<Integer> getBucketColIDs(List<String> bucketCols, List<FieldSchema>
    * @return serde
    * @throws SerializationError
    */
-  public abstract SerDe getSerde() throws SerializationError;
+  public abstract AbstractSerDe getSerde() throws SerializationError;
 
   /**
    * Encode a record as an Object that Hive can read with the ObjectInspector associated with the

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters;
@@ -270,7 +270,7 @@ public void write(long transactionId, byte[] record)
   }
 
   @Override
-  public SerDe getSerde() {
+  public AbstractSerDe getSerde() {
     return serde;
   }
 

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/StrictJsonWriter.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -98,7 +98,7 @@ public StrictJsonWriter(HiveEndPoint endPoint, HiveConf conf, StreamingConnectio
   }
 
   @Override
-  public SerDe getSerde() {
+  public AbstractSerDe getSerde() {
     return serde;
   }
 

File: itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;
 import org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector;
 import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@@ -248,12 +248,12 @@ private Object createRandomRow(final String columnTypes) throws SerDeException {
    * methods.
    */
   private class StorageFormatTest {
-    private SerDe serDe;
+    private AbstractSerDe serDe;
     private JobConf jobConf;
     private HiveOutputFormat outputFormat;
     private InputFormat inputFormat;
 
-    public StorageFormatTest(SerDe serDeImpl, HiveOutputFormat outputFormatImpl, InputFormat inputFormatImpl) throws SerDeException {
+    public StorageFormatTest(AbstractSerDe serDeImpl, HiveOutputFormat outputFormatImpl, InputFormat inputFormatImpl) throws SerDeException {
       jobConf = new JobConf();
       serDe = serDeImpl;
       outputFormat = outputFormatImpl;

File: itests/hive-unit/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java
Patch:
@@ -224,15 +224,15 @@ private MiniHS2(HiveConf hiveConf, MiniClusterType miniClusterType, boolean useM
       switch (miniClusterType) {
       case TEZ:
         // TODO: This should be making use of confDir to load configs setup for Tez, etc.
-        mr = ShimLoader.getHadoopShims().getMiniTezCluster(hiveConf, 2, uriString);
+        mr = ShimLoader.getHadoopShims().getMiniTezCluster(hiveConf, 2, uriString, false);
         break;
       case LLAP:
         if (usePortsFromConf) {
           hiveConf.setBoolean("minillap.usePortsFromConf", true);
         }
         llapCluster = LlapItUtils.startAndGetMiniLlapCluster(hiveConf, null, null);
 
-        mr = ShimLoader.getHadoopShims().getMiniTezCluster(hiveConf, 2, uriString);
+        mr = ShimLoader.getHadoopShims().getMiniTezCluster(hiveConf, 2, uriString, true);
         break;
       case MR:
         mr = ShimLoader.getHadoopShims().getMiniMrCluster(hiveConf, 2, uriString, 1);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
Patch:
@@ -18,15 +18,15 @@
 
 package org.apache.hadoop.hive.metastore;
 
+import com.google.common.collect.Lists;
+
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-
-import com.google.common.collect.Lists;
 import junit.framework.TestCase;
 
 import org.apache.hadoop.hive.cli.CliSessionState;
@@ -455,5 +455,4 @@ public void testListener() throws Exception {
     assertEquals("true", event.getOldValue());
     assertEquals("false", event.getNewValue());
   }
-
 }

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -662,7 +662,8 @@ private void setupMiniCluster(HadoopShims shims, String confDir) throws
       if (EnumSet.of(MiniClusterType.llap_local, MiniClusterType.tez_local).contains(clusterType)) {
         mr = shims.getLocalMiniTezCluster(conf, clusterType == MiniClusterType.llap_local);
       } else {
-        mr = shims.getMiniTezCluster(conf, numTrackers, uriString);
+        mr = shims.getMiniTezCluster(conf, numTrackers, uriString,
+            EnumSet.of(MiniClusterType.llap, MiniClusterType.llap_local).contains(clusterType));
       }
     } else if (clusterType == MiniClusterType.miniSparkOnYarn) {
       mr = shims.getMiniSparkCluster(conf, 2, uriString, 1);
@@ -1705,6 +1706,7 @@ private void maskPatterns(Pattern[] patterns, String fname) throws Exception {
       "fk_-?[0-9]*_[0-9]*_[0-9]*",
       ".*at com\\.sun\\.proxy.*",
       ".*at com\\.jolbox.*",
+      ".*at com\\.zaxxer.*",
       "org\\.apache\\.hadoop\\.hive\\.metastore\\.model\\.MConstraint@([0-9]|[a-z])*",
       "(s3.?|swift|wasb.?):\\/\\/[\\w\\.\\/-]*"
   });

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java
Patch:
@@ -129,9 +129,8 @@ public ContainerRunnerImpl(Configuration conf, int numExecutors, int waitQueueSi
 
     addIfService(executorService);
 
-    // 80% of memory considered for accounted buffers. Rest for objects.
-    // TODO Tune this based on the available size.
-    this.memoryPerExecutor = (long)(totalMemoryAvailableBytes * 0.8 / (float) numExecutors);
+    // Distribute the available memory between the tasks.
+    this.memoryPerExecutor = (long)(totalMemoryAvailableBytes / (float) numExecutors);
     this.metrics = metrics;
 
     confParams = new TaskRunnerCallable.ConfParams(

File: llap-server/src/test/org/apache/hadoop/hive/llap/daemon/impl/TestLlapDaemonProtocolServerImpl.java
Patch:
@@ -38,15 +38,14 @@ public class TestLlapDaemonProtocolServerImpl {
 
 
   @Test(timeout = 10000)
-  public void test() throws ServiceException, IOException {
+  public void testSimpleCall() throws ServiceException, IOException {
     LlapDaemonConfiguration daemonConf = new LlapDaemonConfiguration();
-    int rpcPort = HiveConf.getIntVar(daemonConf, ConfVars.LLAP_DAEMON_RPC_PORT);
     int numHandlers = HiveConf.getIntVar(daemonConf, ConfVars.LLAP_DAEMON_RPC_NUM_HANDLERS);
     ContainerRunner containerRunnerMock = mock(ContainerRunner.class);
     LlapProtocolServerImpl server =
         new LlapProtocolServerImpl(null, numHandlers, containerRunnerMock,
            new AtomicReference<InetSocketAddress>(), new AtomicReference<InetSocketAddress>(),
-           rpcPort, rpcPort + 1, null);
+           0, 0, null);
     when(containerRunnerMock.submitWork(any(SubmitWorkRequestProto.class))).thenReturn(
         SubmitWorkResponseProto
             .newBuilder()

File: llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
Patch:
@@ -459,6 +459,8 @@ public void onVertexStateUpdated(VertexStateUpdate vertexStateUpdate) {
         .sourceStateUpdated(vertexStateUpdate.getVertexName(), vertexStateUpdate.getVertexState());
   }
 
+  // TODO HIVE-15163. Handle cases where nodes go down and come back on the same port. Historic information
+  // can prevent updates from being sent out to the new node.
   public void sendStateUpdate(final LlapNodeId nodeId,
                               final SourceStateUpdatedRequestProto request) {
     communicator.sendSourceStateUpdate(request, nodeId,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
Patch:
@@ -46,7 +46,7 @@
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.api.OperatorType;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -180,7 +180,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     }
     try {
       TableDesc keyTableDesc = conf.getKeyTblDesc();
-      SerDe keySerde = (SerDe) ReflectionUtils.newInstance(keyTableDesc.getDeserializerClass(),
+      AbstractSerDe keySerde = (AbstractSerDe) ReflectionUtils.newInstance(keyTableDesc.getDeserializerClass(),
           null);
       SerDeUtils.initializeSerDe(keySerde, null, keyTableDesc.getProperties(), null);
       MapJoinObjectSerDeContext keyContext = new MapJoinObjectSerDeContext(keySerde, false);
@@ -190,7 +190,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
         }
         mapJoinTables[pos] = new HashMapWrapper(hconf, -1);
         TableDesc valueTableDesc = conf.getValueTblFilteredDescs().get(pos);
-        SerDe valueSerDe = (SerDe) ReflectionUtils.newInstance(valueTableDesc.getDeserializerClass(), null);
+        AbstractSerDe valueSerDe = (AbstractSerDe) ReflectionUtils.newInstance(valueTableDesc.getDeserializerClass(), null);
         SerDeUtils.initializeSerDe(valueSerDe, null, valueTableDesc.getProperties(), null);
         mapJoinTableSerdes[pos] = new MapJoinTableContainerSerDe(keyContext, new MapJoinObjectSerDeContext(
             valueSerDe, hasFilter(pos)));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
Patch:
@@ -58,7 +58,7 @@
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.api.OperatorType;
 import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -275,7 +275,7 @@ public void generateMapMetaData() throws HiveException {
 
     try {
       TableDesc keyTableDesc = conf.getKeyTblDesc();
-      SerDe keySerializer = (SerDe) ReflectionUtil.newInstance(
+      AbstractSerDe keySerializer = (AbstractSerDe) ReflectionUtil.newInstance(
           keyTableDesc.getDeserializerClass(), null);
       SerDeUtils.initializeSerDe(keySerializer, null, keyTableDesc.getProperties(), null);
       MapJoinObjectSerDeContext keyContext = new MapJoinObjectSerDeContext(keySerializer, false);
@@ -289,7 +289,7 @@ public void generateMapMetaData() throws HiveException {
         } else {
           valueTableDesc = conf.getValueFilteredTblDescs().get(pos);
         }
-        SerDe valueSerDe = (SerDe) ReflectionUtil.newInstance(
+        AbstractSerDe valueSerDe = (AbstractSerDe) ReflectionUtil.newInstance(
             valueTableDesc.getDeserializerClass(), null);
         SerDeUtils.initializeSerDe(valueSerDe, null, valueTableDesc.getProperties(), null);
         MapJoinObjectSerDeContext valueContext =

File: ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java
Patch:
@@ -37,7 +37,7 @@
 import org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag;
 import org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
@@ -403,7 +403,7 @@ private void createInputPartition() throws HiveException {
       ObjectInspector inputOI = conf.getStartOfChain() == tabDef ?
           inputObjInspectors[0] : inputDef.getOutputShape().getOI();
 
-      SerDe serde = conf.isMapSide() ? tabDef.getInput().getOutputShape().getSerde() :
+      AbstractSerDe serde = conf.isMapSide() ? tabDef.getInput().getOutputShape().getSerde() :
         tabDef.getRawInputShape().getSerde();
       StructObjectInspector outputOI = conf.isMapSide() ? tabDef.getInput().getOutputShape().getOI() :
         tabDef.getRawInputShape().getOI();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/PTFRollingPartition.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
@@ -70,7 +70,7 @@ public class PTFRollingPartition extends PTFPartition {
    */
   List<Object> currWindow;
 
-  protected PTFRollingPartition(Configuration cfg, SerDe serDe,
+  protected PTFRollingPartition(Configuration cfg, AbstractSerDe serDe,
       StructObjectInspector inputOI, StructObjectInspector outputOI,
       int startPos, int endPos) throws HiveException {
     super(cfg, serDe, inputOI, outputOI, false);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java
Patch:
@@ -46,6 +46,7 @@
 import org.apache.hadoop.hive.ql.plan.MapredLocalWork;
 import org.apache.hadoop.hive.ql.plan.MapredWork;
 import org.apache.hadoop.hive.ql.plan.MoveWork;
+import org.apache.hadoop.hive.ql.plan.ReplCopyWork;
 import org.apache.hadoop.hive.ql.plan.SparkWork;
 import org.apache.hadoop.hive.ql.plan.StatsNoJobWork;
 import org.apache.hadoop.hive.ql.plan.StatsWork;
@@ -77,6 +78,7 @@ public TaskTuple(Class<T> workClass, Class<? extends Task<T>> taskClass) {
     taskvec.add(new TaskTuple<MoveWork>(MoveWork.class, MoveTask.class));
     taskvec.add(new TaskTuple<FetchWork>(FetchWork.class, FetchTask.class));
     taskvec.add(new TaskTuple<CopyWork>(CopyWork.class, CopyTask.class));
+    taskvec.add(new TaskTuple<ReplCopyWork>(ReplCopyWork.class, ReplCopyTask.class));
     taskvec.add(new TaskTuple<DDLWork>(DDLWork.class, DDLTask.class));
     taskvec.add(new TaskTuple<FunctionWork>(FunctionWork.class,
         FunctionTask.class));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/FlatRowContainer.java
Patch:
@@ -31,7 +31,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -69,7 +69,7 @@ public FlatRowContainer() {
   /** Called when loading the hashtable. */
   public void add(MapJoinObjectSerDeContext context,
       BytesWritable value) throws HiveException {
-    SerDe serde = context.getSerDe();
+    AbstractSerDe serde = context.getSerDe();
     isAliasFilterSet = !context.hasFilterTag(); // has tag => need to set later
     if (rowLength == UNKNOWN) {
       try {
@@ -197,7 +197,7 @@ public List<Object> next() {
     }
   }
 
-  private void read(SerDe serde, Writable writable, int rowOffset) throws HiveException {
+  private void read(AbstractSerDe serde, Writable writable, int rowOffset) throws HiveException {
     try {
       ObjectInspectorUtils.copyStructToArray(
           serde.deserialize(writable), serde.getObjectInspector(),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java
Patch:
@@ -47,7 +47,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.HiveUtils;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.WriteBuffers;
 import org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe;
@@ -1166,7 +1166,7 @@ public int size() {
   @Override
   public void setSerde(MapJoinObjectSerDeContext keyCtx, MapJoinObjectSerDeContext valCtx)
       throws SerDeException {
-    SerDe keySerde = keyCtx.getSerDe(), valSerde = valCtx.getSerDe();
+    AbstractSerDe keySerde = keyCtx.getSerDe(), valSerde = valCtx.getSerDe();
 
     if (writeHelper == null) {
       LOG.info("Initializing container with " + keySerde.getClass().getName() + " and "

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinEagerRowContainer.java
Patch:
@@ -26,7 +26,7 @@
 import java.util.ConcurrentModificationException;
 import java.util.List;
 
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -134,7 +134,7 @@ public void read(MapJoinObjectSerDeContext context, ObjectInputStream in, Writab
 
   @SuppressWarnings("unchecked")
   public void read(MapJoinObjectSerDeContext context, Writable currentValue) throws SerDeException {
-    SerDe serde = context.getSerDe();
+    AbstractSerDe serde = context.getSerDe();
     List<Object> value = (List<Object>)ObjectInspectorUtils.copyToStandardObject(serde.deserialize(currentValue),
         serde.getObjectInspector(), ObjectInspectorCopyOption.WRITABLE);
     if(value == null) {
@@ -151,7 +151,7 @@ public void read(MapJoinObjectSerDeContext context, Writable currentValue) throw
   @Override
   public void write(MapJoinObjectSerDeContext context, ObjectOutputStream out)
   throws IOException, SerDeException {
-    SerDe serde = context.getSerDe();
+    AbstractSerDe serde = context.getSerDe();
     ObjectInspector valueObjectInspector = context.getStandardOI();
     long numRows = rowCount();
     long numRowsWritten = 0L;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java
Patch:
@@ -29,7 +29,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe;
 import org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe;
@@ -56,7 +56,7 @@ public abstract void write(MapJoinObjectSerDeContext context, ObjectOutputStream
   @SuppressWarnings("deprecation")
   public static MapJoinKey read(Output output, MapJoinObjectSerDeContext context,
       Writable writable) throws SerDeException, HiveException {
-    SerDe serde = context.getSerDe();
+    AbstractSerDe serde = context.getSerDe();
     Object obj = serde.deserialize(writable);
     MapJoinKeyObject result = new MapJoinKeyObject();
     result.read(serde.getObjectInspector(), obj);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java
Patch:
@@ -30,7 +30,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -119,7 +119,7 @@ protected void read(ObjectInspector oi, Object obj) throws SerDeException {
   @Override
   public void write(MapJoinObjectSerDeContext context, ObjectOutputStream out)
       throws IOException, SerDeException {
-    SerDe serde = context.getSerDe();
+    AbstractSerDe serde = context.getSerDe();
     ObjectInspector objectInspector = context.getStandardOI();
     Writable container = serde.serialize(key, objectInspector);
     container.write(out);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java
Patch:
@@ -39,7 +39,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.HiveUtils;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
@@ -97,7 +97,7 @@ public class RowContainer<ROW extends List<Object>>
   private int itrCursor; // iterator cursor in the currBlock
   private int readBlockSize; // size of current read block
   private int addCursor; // append cursor in the lastBlock
-  private SerDe serde; // serialization/deserialization for the row
+  private AbstractSerDe serde; // serialization/deserialization for the row
   private ObjectInspector standardOI; // object inspector for the row
 
   private List<Object> keyObject;
@@ -160,7 +160,7 @@ private JobConf getLocalFSJobConfClone(Configuration jc) {
   }
 
 
-  public void setSerDe(SerDe sd, ObjectInspector oi) {
+  public void setSerDe(AbstractSerDe sd, ObjectInspector oi) {
     this.serde = sd;
     this.standardOI = oi;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java
Patch:
@@ -41,7 +41,7 @@
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.serde2.Deserializer;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe;
@@ -79,7 +79,7 @@ public class ReduceRecordSource implements RecordSource {
 
   // Input value serde needs to be an array to support different SerDe
   // for different tags
-  private SerDe inputValueDeserializer;
+  private AbstractSerDe inputValueDeserializer;
 
   private TableDesc keyTableDesc;
   private TableDesc valueTableDesc;
@@ -151,7 +151,7 @@ void init(JobConf jconf, Operator<?> reducer, boolean vectorized, TableDesc keyT
 
       // We should initialize the SerDe with the TypeInfo when available.
       this.valueTableDesc = valueTableDesc;
-      inputValueDeserializer = (SerDe) ReflectionUtils.newInstance(
+      inputValueDeserializer = (AbstractSerDe) ReflectionUtils.newInstance(
           valueTableDesc.getDeserializerClass(), null);
       SerDeUtils.initializeSerDe(inputValueDeserializer, null,
           valueTableDesc.getProperties(), null);

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java
Patch:
@@ -138,7 +138,7 @@ public void run() {
               explain.initialize(queryState, plan, null, null);
               String query = plan.getQueryStr();
               JSONObject explainPlan = explain.getJSONPlan(null, work);
-              String logID = conf.getLogIdVar(SessionState.get().getSessionId());
+              String logID = conf.getLogIdVar(hookContext.getSessionId());
               fireAndForget(conf, createPreHookEvent(queryId, query, explainPlan, queryStartTime,
                 user, requestuser, numMrJobs, numTezJobs, opId, logID));
               break;

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java
Patch:
@@ -53,6 +53,9 @@ public class ReadEntity extends Entity implements Serializable {
   // important because in that case we shouldn't acquire a lock for it or authorize the read.
   // These will be handled by the output to the table instead.
   private boolean isUpdateOrDelete = false;
+  //https://issues.apache.org/jira/browse/HIVE-15048
+  public transient boolean isFromTopLevelQuery = true;
+
 
   // For views, the entities can be nested - by default, entities are at the top level
   // Must be deterministic order set for consistent q-test output across Java versions

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSerde.java
Patch:
@@ -30,7 +30,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedSerde;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeSpec;
 import org.apache.hadoop.hive.serde2.SerDeStats;
@@ -45,7 +45,7 @@
  * It transparently passes the object to/from the ORC file reader/writer.
  */
 @SerDeSpec(schemaProps = {serdeConstants.LIST_COLUMNS, serdeConstants.LIST_COLUMN_TYPES, OrcSerde.COMPRESSION})
-public class OrcSerde implements SerDe, VectorizedSerde {
+public class OrcSerde extends VectorizedSerde {
 
   private static final Logger LOG = LoggerFactory.getLogger(OrcSerde.class);
 

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java
Patch:
@@ -63,7 +63,7 @@ public long getCurrentTxnId() {
   }
 
   @Override
-  public int getStatementId() {
+  public int getWriteIdAndIncrement() {
     return 0;
   }
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/DefaultStorageHandler.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.metastore.HiveMetaHook;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;
 import org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
@@ -54,7 +54,7 @@ public Class<? extends OutputFormat> getOutputFormatClass() {
   }
 
   @Override
-  public Class<? extends SerDe> getSerDeClass() {
+  public Class<? extends AbstractSerDe> getSerDeClass() {
     return LazySimpleSerDe.class;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java
Patch:
@@ -420,7 +420,9 @@ private void checkPartitionDirs(Path basePath, Set<Path> allDirs, int maxDepth)
           + ((ThreadPoolExecutor) pool).getPoolSize());
     }
     checkPartitionDirs(pool, basePaths, dirSet, basePath.getFileSystem(conf), maxDepth, maxDepth);
-    pool.shutdown();
+    if (pool != null) {
+      pool.shutdown();
+    }
     allDirs.addAll(dirSet);
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveStorageHandler.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.conf.Configurable;
 import org.apache.hadoop.hive.metastore.HiveMetaHook;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobConf;
@@ -59,9 +59,9 @@ public interface HiveStorageHandler extends Configurable {
   public Class<? extends OutputFormat> getOutputFormatClass();
 
   /**
-   * @return Class providing an implementation of {@link SerDe}
+   * @return Class providing an implementation of {@link AbstractSerDe}
    */
-  public Class<? extends SerDe> getSerDeClass();
+  public Class<? extends AbstractSerDe> getSerDeClass();
 
   /**
    * @return metadata hook implementation, or null if this

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/PartitionColumnsSeparator.java
Patch:
@@ -64,7 +64,7 @@
  */
 public class PartitionColumnsSeparator extends Transform {
 
-  private static final Log LOG = LogFactory.getLog(PointLookupOptimizer.class);
+  private static final Log LOG = LogFactory.getLog(PartitionColumnsSeparator.class);
   private static final String IN_UDF =
     GenericUDFIn.class.getAnnotation(Description.class).name();
   private static final String STRUCT_UDF =

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -749,8 +749,8 @@ private boolean validateInputFormatAndSchemaEvolution(MapWork mapWork, String al
           return false;
         }
         VectorPartitionDesc vectorPartDesc = partDesc.getVectorPartitionDesc();
-        if (LOG.isInfoEnabled()) {
-          LOG.info("Vectorizer path: " + path + ", " + vectorPartDesc.toString() +
+          if (LOG.isDebugEnabled()) {
+          LOG.debug("Vectorizer path: " + path + ", " + vectorPartDesc.toString() +
               ", aliases " + aliases);
         }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
Patch:
@@ -71,7 +71,7 @@ public static FileStatus[] matchFilesOrDir(FileSystem fs, Path path)
       @Override
       public boolean accept(Path p) {
         String name = p.getName();
-        return name.equals("_metadata") ? true : !name.startsWith("_") && !name.startsWith(".");
+        return name.equals(EximUtil.METADATA_NAME) ? true : !name.startsWith("_") && !name.startsWith(".");
       }
     });
     if ((srcs != null) && srcs.length == 1) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/MetaDataExportListener.java
Patch:
@@ -38,7 +38,7 @@
 import org.apache.hadoop.hive.metastore.events.PreEventContext;
 import org.apache.hadoop.hive.metastore.events.PreEventContext.PreEventType;
 import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer;
+
 /**
  * This class listens for drop events and, if set, exports the table's metadata as JSON to the trash
  * of the user performing the drop
@@ -83,7 +83,7 @@ private void export_meta_data(PreDropTableEvent tableEvent) throws MetaException
     } catch (IOException e) {
       throw new MetaException(e.getMessage());
     }
-    Path outFile = new Path(metaPath, name + ImportSemanticAnalyzer.METADATA_NAME);
+    Path outFile = new Path(metaPath, name + EximUtil.METADATA_NAME);
     try {
       SessionState.getConsole().printInfo("Beginning metadata export");
       EximUtil.createExportDump(fs, outFile, mTbl, null, null);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
Patch:
@@ -520,7 +520,7 @@ public void validate(HiveConf conf)
           }
         }
         if (!found) {
-          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg());
+          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg(" \'" + bucketCol + "\'"));
         }
       }
     }
@@ -540,7 +540,7 @@ public void validate(HiveConf conf)
           }
         }
         if (!found) {
-          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg());
+          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg(" \'" + sortCol + "\'"));
         }
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDeserializer.java
Patch:
@@ -49,7 +49,7 @@
 import org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator;
 import org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver;
 import org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingTableFunctionResolver;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -265,8 +265,8 @@ protected void initialize(ShapeDetails shp, StructObjectInspector OI) throws Hiv
       serDeProps.setProperty(serdeName, serdePropsMap.get(serdeName));
     }
     try {
-      SerDe serDe =  ReflectionUtils.newInstance(hConf.getClassByName(serdeClassName).
-          asSubclass(SerDe.class), hConf);
+      AbstractSerDe serDe =  ReflectionUtils.newInstance(hConf.getClassByName(serdeClassName).
+          asSubclass(AbstractSerDe.class), hConf);
       SerDeUtils.initializeSerDe(serDe, hConf, serDeProps, null);
       shp.setSerde(serDe);
       StructObjectInspector outOI = PTFPartition.setupPartitionOutputOI(serDe, OI);

File: ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java
Patch:
@@ -56,7 +56,7 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AggregationBuffer;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.SumAvgEnhancer;
 import org.apache.hadoop.hive.ql.udf.generic.ISupportStreamingModeForWindowing;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
@@ -1618,7 +1618,7 @@ class StreamingState {
     StreamingState(Configuration cfg, StructObjectInspector inputOI,
         boolean isMapSide, WindowTableFunctionDef tabDef, int precedingSpan,
         int followingSpan) throws HiveException {
-      SerDe serde = isMapSide ? tabDef.getInput().getOutputShape().getSerde()
+      AbstractSerDe serde = isMapSide ? tabDef.getInput().getOutputShape().getSerde()
           : tabDef.getRawInputShape().getSerde();
       StructObjectInspector outputOI = isMapSide ? tabDef.getInput()
           .getOutputShape().getOI() : tabDef.getRawInputShape().getOI();

File: ql/src/test/org/apache/hadoop/hive/metastore/txn/TestCompactionTxnHandler.java
Patch:
@@ -417,6 +417,7 @@ public void addDynamicPartitions() throws Exception {
     long txnId = openTxns.getTxn_ids().get(0);
     // lock a table, as in dynamic partitions
     LockComponent lc = new LockComponent(LockType.SHARED_WRITE, LockLevel.TABLE, dbName);
+    lc.setIsDynamicPartitionWrite(true);
     lc.setTablename(tableName);
     DataOperationType dop = DataOperationType.UPDATE; 
     lc.setOperationType(dop);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/persistence/TestPTFRowContainer.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@@ -47,7 +47,7 @@ public class TestPTFRowContainer {
   private static final String COL_NAMES = "x,y,z,a,b,v";
   private static final String COL_TYPES = "int,string,double,int,string,string";
 
-  static SerDe serDe;
+  static AbstractSerDe serDe;
   static Configuration cfg;
 
   @BeforeClass

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestIUD.java
Patch:
@@ -32,7 +32,6 @@
  */
 public class TestIUD {
   private static HiveConf conf;
-
   private ParseDriver pd;
 
   @BeforeClass
@@ -47,6 +46,9 @@ public void setup() throws SemanticException, IOException {
   }
 
   ASTNode parse(String query) throws ParseException {
+    return parse(query, pd, conf);
+  }
+  static ASTNode parse(String query, ParseDriver pd, HiveConf conf) throws ParseException {
     ASTNode nd = null;
     try {
       nd = pd.parse(query, new Context(conf));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractFileMergeOperator.java
Patch:
@@ -123,7 +123,7 @@ private void updatePaths(Path tp, Path ttp) {
       outPath = new Path(ttp, Utilities.toTempPath(taskId));
     }
     Utilities.LOG14535.info("Paths for merge " + taskId + ": tmp " + tmpPath + ", task "
-        + taskTmpPath + ", final " + finalPath + ", out " + outPath, new Exception());
+        + taskTmpPath + ", final " + finalPath + ", out " + outPath);
   }
 
   /**
@@ -297,7 +297,7 @@ public void jobCloseOp(Configuration hconf, boolean success)
         // We don't expect missing buckets from mere (actually there should be no buckets),
         // so just pass null as bucketing context. Union suffix should also be accounted for.
         Utilities.handleMmTableFinalPath(outputDir.getParent(), null, hconf, success,
-            dpLevels, lbLevels, null, mmWriteId, reporter);
+            dpLevels, lbLevels, null, mmWriteId, reporter, false);
       }
 
     } catch (IOException e) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/CopyTask.java
Patch:
@@ -110,6 +110,7 @@ protected int copyOnePath(Path fromPath, Path toPath) {
   // Note: initially copied from LoadSemanticAnalyzer.
   private static FileStatus[] matchFilesOrDir(
       FileSystem fs, Path path, boolean isSourceMm) throws IOException {
+    if (!fs.exists(path)) return null;
     if (!isSourceMm) return matchFilesOneDir(fs, path, null);
     // TODO: this doesn't handle list bucketing properly. Does the original exim do that?
     FileStatus[] mmDirs = fs.listStatus(path, new ValidWriteIds.AnyIdDirFilter());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -651,7 +651,8 @@ private int mergeFiles(Hive db, AlterTablePartMergeFilesDesc mergeFilesDesc,
 
     // merge work only needs input and output.
     MergeFileWork mergeWork = new MergeFileWork(mergeFilesDesc.getInputDir(),
-        mergeFilesDesc.getOutputDir(), mergeFilesDesc.getInputFormatClass().getName());
+        mergeFilesDesc.getOutputDir(), mergeFilesDesc.getInputFormatClass().getName(),
+        mergeFilesDesc.getTableDesc());
     LinkedHashMap<Path, ArrayList<String>> pathToAliases = new LinkedHashMap<>();
     ArrayList<String> inputDirstr = new ArrayList<String>(1);
     inputDirstr.add(mergeFilesDesc.getInputDir().toString());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -1230,7 +1230,7 @@ public void jobCloseOp(Configuration hconf, boolean success)
           MissingBucketsContext mbc = new MissingBucketsContext(
               conf.getTableInfo(), numBuckets, conf.getCompressed());
           Utilities.handleMmTableFinalPath(specPath, unionSuffix, hconf, success,
-              dpLevels, lbLevels, mbc, conf.getMmWriteId(), reporter);
+              dpLevels, lbLevels, mbc, conf.getMmWriteId(), reporter, conf.isMmCtas());
         }
       }
     } catch (IOException e) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
Patch:
@@ -78,11 +78,11 @@ public class TableScanOperator extends Operator<TableScanDesc> implements
   private String schemaEvolutionColumns;
   private String schemaEvolutionColumnsTypes;
 
-  public TableDesc getTableDesc() {
+  public TableDesc getTableDescSkewJoin() {
     return tableDesc;
   }
 
-  public void setTableDesc(TableDesc tableDesc) {
+  public void setTableDescSkewJoin(TableDesc tableDesc) {
     this.tableDesc = tableDesc;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanMapper.java
Patch:
@@ -88,8 +88,9 @@ public void map(Object k, RCFileValueBufferWrapper value,
     }
 
     try {
-      //CombineHiveInputFormat is set in PartialScanTask.
-      RCFileKeyBufferWrapper key = (RCFileKeyBufferWrapper) ((CombineHiveKey) k).getKey();
+      //CombineHiveInputFormat may be set in PartialScanTask.
+      RCFileKeyBufferWrapper key = (RCFileKeyBufferWrapper)
+          ((k instanceof CombineHiveKey) ?  ((CombineHiveKey) k).getKey() : k);
 
       // calculate rawdatasize
       KeyBuffer keyBuffer = key.getKeyBuffer();

File: ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java
Patch:
@@ -350,7 +350,7 @@ public static void main(String[] args) {
     }
 
     QueryState queryState = new QueryState(new HiveConf(conf, PartialScanTask.class));
-    PartialScanWork mergeWork = new PartialScanWork(inputPaths);
+    PartialScanWork mergeWork = new PartialScanWork(inputPaths, null);
     DriverContext driverCxt = new DriverContext();
     PartialScanTask taskExec = new PartialScanTask();
     taskExec.initialize(queryState, null, driverCxt, new CompilationOpContext());

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java
Patch:
@@ -238,7 +238,7 @@ private List<Path> makeTableStatusLocations(Table tbl, Hive db, Partition par)
    * @param tblPath not NULL
    * @throws IOException
    */
-  // Duplicates logic in TextMetaDataFormatter
+  // Duplicates logic in TextMetaDataFormatter TODO: wtf?!!
   private void putFileSystemsStats(MapBuilder builder, List<Path> locations,
       HiveConf conf, Path tblPath)
           throws IOException {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.exec.mr.MapRedTask;
 import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
 import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;
@@ -195,7 +196,8 @@ private void handlePartialScanCommand(TableScanOperator op, GenMRProcContext ctx
     aggregationKey = aggregationKeyBuffer.toString();
 
     // scan work
-    PartialScanWork scanWork = new PartialScanWork(inputPaths);
+    PartialScanWork scanWork = new PartialScanWork(inputPaths,
+        Utilities.getTableDesc(op.getConf().getTableMetadata()));
     scanWork.setMapperCannotSpanPartns(true);
     scanWork.setAggKey(aggregationKey);
     scanWork.setStatsTmpDir(op.getConf().getTmpStatsDir(), parseCtx.getConf());

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -1598,7 +1598,7 @@ public static MapWork createMergeTask(FileSinkDesc fsInputDesc, Path finalName,
     Utilities.LOG14535.info("creating mergefilework from " + inputDirs + " to " + finalName);
     // create the merge file work
     MergeFileWork work = new MergeFileWork(inputDirs, finalName,
-        hasDynamicPartitions, tblDesc.getInputFileFormatClass().getName());
+        hasDynamicPartitions, tblDesc.getInputFileFormatClass().getName(), tblDesc);
     LinkedHashMap<Path, ArrayList<String>> pathToAliases = new LinkedHashMap<>();
     pathToAliases.put(inputDir, inputDirstr);
     work.setMapperCannotSpanPartns(true);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java
Patch:
@@ -255,7 +255,7 @@ public static void processSkewJoin(JoinOperator joinOp,
         Operator<? extends OperatorDesc> ts =
             GenMapRedUtils.createTemporaryTableScanOperator(
                 joinOp.getCompilationOpContext(), rowSchemaList.get((byte)k));
-        ((TableScanOperator)ts).setTableDesc(tableDescList.get((byte)k));
+        ((TableScanOperator)ts).setTableDescSkewJoin(tableDescList.get((byte)k));
         parentOps[k] = ts;
       }
       Operator<? extends OperatorDesc> tblScan_op = parentOps[i];

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenSparkSkewJoinProcessor.java
Patch:
@@ -231,7 +231,7 @@ public static void processSkewJoin(JoinOperator joinOp, Task<? extends Serializa
       for (int k = 0; k < tags.length; k++) {
         Operator<? extends OperatorDesc> ts = GenMapRedUtils.createTemporaryTableScanOperator(
             joinOp.getCompilationOpContext(), rowSchemaList.get((byte) k));
-        ((TableScanOperator) ts).setTableDesc(tableDescList.get((byte) k));
+        ((TableScanOperator) ts).setTableDescSkewJoin(tableDescList.get((byte) k));
         parentOps[k] = ts;
       }
 
@@ -362,7 +362,7 @@ private static void insertSHTS(byte tag, TableScanOperator tableScan, MapWork bi
     HashTableDummyDesc desc = new HashTableDummyDesc();
     HashTableDummyOperator dummyOp = (HashTableDummyOperator) OperatorFactory.get(
         tableScan.getCompilationOpContext(), desc);
-    dummyOp.getConf().setTbl(tableScan.getTableDesc());
+    dummyOp.getConf().setTbl(tableScan.getTableDescSkewJoin());
     MapJoinOperator mapJoinOp = (MapJoinOperator) tableScan.getChildOperators().get(0);
     mapJoinOp.replaceParent(tableScan, dummyOp);
     List<Operator<? extends OperatorDesc>> mapJoinChildren =

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java
Patch:
@@ -209,7 +209,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object..
 
         if (parent.getSchema() == null) {
           if (parent instanceof TableScanOperator) {
-            tbl = ((TableScanOperator) parent).getTableDesc();
+            tbl = ((TableScanOperator) parent).getTableDescSkewJoin();
           } else {
             throw new SemanticException("Expected parent operator of type TableScanOperator." +
               "Found " + parent.getClass().getName() + " instead.");

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/index/IndexWhereProcessor.java
Patch:
@@ -118,7 +118,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
     // get potential reentrant index queries from each index
     Map<Index, HiveIndexQueryContext> queryContexts = new HashMap<Index, HiveIndexQueryContext>();
     // make sure we have an index on the table being scanned
-    TableDesc tblDesc = operator.getTableDesc();
+    TableDesc tblDesc = operator.getTableDescSkewJoin();
 
     Map<String, List<Index>> indexesByType = new HashMap<String, List<Index>>();
     for (Index indexOnTable : indexes) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -1603,6 +1603,7 @@ private void analyzeAlterTablePartMergeFiles(ASTNode ast,
       if (MetaStoreUtils.isInsertOnlyTable(tblObj.getParameters())) {
         throw new SemanticException("Merge is not supported for MM tables");
       }
+      mergeDesc.setTableDesc(Utilities.getTableDesc(tblObj));
 
       List<String> bucketCols = null;
       Class<? extends InputFormat> inputFormatClass = null;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
 import org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork;
 import org.apache.hadoop.hive.ql.lib.Node;
@@ -179,7 +180,8 @@ private void handlePartialScanCommand(TableScanOperator tableScan, ParseContext
     aggregationKey = aggregationKeyBuffer.toString();
 
     // scan work
-    PartialScanWork scanWork = new PartialScanWork(inputPaths);
+    PartialScanWork scanWork = new PartialScanWork(inputPaths,
+        Utilities.getTableDesc(tableScan.getConf().getTableMetadata()));
     scanWork.setMapperCannotSpanPartns(true);
     scanWork.setAggKey(aggregationKey);
     scanWork.setStatsTmpDir(tableScan.getConf().getTmpStatsDir(), parseContext.getConf());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkProcessAnalyzeTable.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
 import org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork;
 import org.apache.hadoop.hive.ql.lib.Node;
@@ -174,7 +175,8 @@ private void handlePartialScanCommand(TableScanOperator tableScan, ParseContext
     aggregationKey = aggregationKeyBuffer.toString();
 
     // scan work
-    PartialScanWork scanWork = new PartialScanWork(inputPaths);
+    PartialScanWork scanWork = new PartialScanWork(inputPaths,
+        Utilities.getTableDesc(tableScan.getConf().getTableMetadata()));
     scanWork.setMapperCannotSpanPartns(true);
     scanWork.setAggKey(aggregationKey);
     scanWork.setStatsTmpDir(tableScan.getConf().getTmpStatsDir(), parseContext.getConf());

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestFileSinkOperator.java
Patch:
@@ -285,7 +285,8 @@ private FileSinkOperator getFileSink(AcidUtils.Operation writeType,
       partColMap.put(PARTCOL_NAME, null);
       DynamicPartitionCtx dpCtx = new DynamicPartitionCtx(null, partColMap, "Sunday", 100);
       //todo: does this need the finalDestination?
-      desc = new FileSinkDesc(basePath, tableDesc, false, 1, false, false, 1, 1, partCols, dpCtx, null, null);
+      desc = new FileSinkDesc(basePath, tableDesc, false, 1, false,
+          false, 1, 1, partCols, dpCtx, null, null, false);
     } else {
       desc = new FileSinkDesc(basePath, tableDesc, false);
     }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2958,8 +2958,8 @@ public static enum ConfVars {
     LLAP_DAEMON_NUM_EXECUTORS("hive.llap.daemon.num.executors", 4,
       "Number of executors to use in LLAP daemon; essentially, the number of tasks that can be\n" +
       "executed in parallel.", "llap.daemon.num.executors"),
-    LLAP_DAEMON_RPC_PORT("hive.llap.daemon.rpc.port", 15001, "The LLAP daemon RPC port.",
-      "llap.daemon.rpc.port"),
+    LLAP_DAEMON_RPC_PORT("hive.llap.daemon.rpc.port", 0, "The LLAP daemon RPC port.",
+      "llap.daemon.rpc.port. A value of 0 indicates a dynamic port"),
     LLAP_DAEMON_MEMORY_PER_INSTANCE_MB("hive.llap.daemon.memory.per.instance.mb", 3276,
       "The total amount of memory to use for the executors inside LLAP (in megabytes).",
       "llap.daemon.memory.per.instance.mb"),

File: llap-server/src/test/org/apache/hadoop/hive/llap/daemon/impl/TestLlapDaemonProtocolServerImpl.java
Patch:
@@ -38,15 +38,14 @@ public class TestLlapDaemonProtocolServerImpl {
 
 
   @Test(timeout = 10000)
-  public void test() throws ServiceException, IOException {
+  public void testSimpleCall() throws ServiceException, IOException {
     LlapDaemonConfiguration daemonConf = new LlapDaemonConfiguration();
-    int rpcPort = HiveConf.getIntVar(daemonConf, ConfVars.LLAP_DAEMON_RPC_PORT);
     int numHandlers = HiveConf.getIntVar(daemonConf, ConfVars.LLAP_DAEMON_RPC_NUM_HANDLERS);
     ContainerRunner containerRunnerMock = mock(ContainerRunner.class);
     LlapProtocolServerImpl server =
         new LlapProtocolServerImpl(null, numHandlers, containerRunnerMock,
            new AtomicReference<InetSocketAddress>(), new AtomicReference<InetSocketAddress>(),
-           rpcPort, rpcPort + 1, null);
+           0, 0, null);
     when(containerRunnerMock.submitWork(any(SubmitWorkRequestProto.class))).thenReturn(
         SubmitWorkResponseProto
             .newBuilder()

File: llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
Patch:
@@ -459,6 +459,8 @@ public void onVertexStateUpdated(VertexStateUpdate vertexStateUpdate) {
         .sourceStateUpdated(vertexStateUpdate.getVertexName(), vertexStateUpdate.getVertexState());
   }
 
+  // TODO HIVE-15163. Handle cases where nodes go down and come back on the same port. Historic information
+  // can prevent updates from being sent out to the new node.
   public void sendStateUpdate(final LlapNodeId nodeId,
                               final SourceStateUpdatedRequestProto request) {
     communicator.sendSourceStateUpdate(request, nodeId,

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/PartitionColumnsSeparator.java
Patch:
@@ -64,7 +64,7 @@
  */
 public class PartitionColumnsSeparator extends Transform {
 
-  private static final Log LOG = LogFactory.getLog(PointLookupOptimizer.class);
+  private static final Log LOG = LogFactory.getLog(PartitionColumnsSeparator.class);
   private static final String IN_UDF =
     GenericUDFIn.class.getAnnotation(Description.class).name();
   private static final String STRUCT_UDF =

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
Patch:
@@ -1944,5 +1944,4 @@ public static boolean isRemovedInsertOnlyTable(Set<String> removedSet) {
         hasProps = removedSet.contains(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);
     return hasTxn || hasProps;
   }
-
 }

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
Patch:
@@ -265,7 +265,7 @@ public static void populateQuickStats(FileStatus[] fileStatus, Map<String, Strin
         numFiles += 1;
       }
     }
-    LOG.info(s, new Exception());
+    LOG.info(s/*, new Exception()*/);
     params.put(StatsSetupConst.NUM_FILES, Integer.toString(numFiles));
     params.put(StatsSetupConst.TOTAL_SIZE, Long.toString(tableSize));
   }

File: ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java
Patch:
@@ -77,7 +77,7 @@ public boolean connect(StatsCollectionContext context) {
 
   @Override
   public boolean publishStat(String partKV, Map<String, String> stats) {
-    LOG.debug("Putting in map : " + partKV + "\t" + stats);
+    Utilities.LOG14535.info("Putting in map : " + partKV + "\t" + stats);
     // we need to do new hashmap, since stats object is reused across calls.
     Map<String,String> cpy = new HashMap<String, String>(stats);
     Map<String,String> statMap = statsMap.get(partKV);

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java
Patch:
@@ -732,7 +732,7 @@ private void discoverPartitions(JobContext context) throws IOException {
         for (FileStatus st : status) {
           LinkedHashMap<String, String> fullPartSpec = new LinkedHashMap<String, String>();
           if (!customDynamicLocationUsed) {
-            Warehouse.makeSpecFromName(fullPartSpec, st.getPath());
+            Warehouse.makeSpecFromName(fullPartSpec, st.getPath(), null);
           } else {
             HCatFileUtil.getPartKeyValuesForCustomLocation(fullPartSpec, jobInfo,
                 st.getPath().toString());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
Patch:
@@ -305,6 +305,9 @@ protected void flushToFile() throws IOException, HiveException {
       String bigBucketFileName = getExecContext().getCurrentBigBucketFile();
       String fileName = getExecContext().getLocalWork().getBucketFileName(bigBucketFileName);
       // get the tmp URI path; it will be a hdfs path if not local mode
+      // TODO# this doesn't work... the path for writer and reader mismatch
+      //      Dump the side-table for tag ... -local-10004/HashTable-Stage-1/MapJoin-a-00-(ds%3D2008-04-08)mm_2.hashtable
+      //      Load back 1 hashtable file      -local-10004/HashTable-Stage-1/MapJoin-a-00-srcsortbucket3outof4.txt.hashtable
       String dumpFilePrefix = conf.getDumpFilePrefix();
       Path path = Utilities.generatePath(tmpURI, dumpFilePrefix, tag, fileName);
       console.printInfo(Utilities.now() + "\tDump the side-table for tag: " + tag +

File: ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
Patch:
@@ -210,7 +210,7 @@ private int aggregateStats(Hive db) {
         if (conf.getBoolVar(ConfVars.TEZ_EXEC_SUMMARY)) {
           console.printInfo("Table " + tableFullName + " stats: [" + toString(parameters) + ']');
         }
-        LOG.info("Table " + tableFullName + " stats: [" + toString(parameters) + ']');
+        Utilities.LOG14535.info("Table " + tableFullName + " stats: [" + toString(parameters) + ']');
       } else {
         // Partitioned table:
         // Need to get the old stats of the partition

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -6702,7 +6702,7 @@ protected Operator genFileSinkPlan(String dest, QB qb, Operator input)
         field_schemas = new ArrayList<FieldSchema>();
         destTableIsTemporary = tblDesc.isTemporary();
         destTableIsMaterialization = tblDesc.isMaterialization();
-        if (!destTableIsTemporary && MetaStoreUtils.isInsertOnlyTable(tblDesc.getTblProps())) {
+        if (!destTableIsTemporary && MetaStoreUtils.isInsertOnlyTable(tblDesc.getTblProps(), true)) {
           isMmTable = isMmCtas = true;
           // TODO# this should really get current ACID txn; assuming ACID works correctly the txn
           //       should have been opened to create the ACID table. For now use the first ID.

File: ql/src/java/org/apache/hadoop/hive/ql/plan/BucketMapJoinContext.java
Patch:
@@ -39,6 +39,7 @@ public class BucketMapJoinContext implements Serializable {
 
   private static final long serialVersionUID = 1L;
 
+  // TODO# this is completely broken, esp. w/load into bucketed tables (should perhaps be forbidden for MM tables)
   // table alias (small) --> input file name (big) --> target file names (small)
   private Map<String, Map<String, List<String>>> aliasBucketFileNameMapping;
   private String mapJoinBigTableAlias;

File: ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.StatsSetupConst;
 import org.apache.hadoop.hive.ql.exec.SerializationUtilities;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.slf4j.Logger;
@@ -105,7 +106,7 @@ public boolean closeConnection(StatsCollectionContext context) {
         statsFile = new Path(statsDir, StatsSetupConst.STATS_FILE_PREFIX
             + conf.getInt("mapred.task.partition", 0));
       }
-      LOG.debug("About to create stats file for this task : " + statsFile);
+      Utilities.LOG14535.info("About to create stats file for this task : " + statsFile);
       Output output = new Output(statsFile.getFileSystem(conf).create(statsFile,true));
       LOG.debug("Created file : " + statsFile);
       LOG.debug("Writing stats in it : " + statsMap);
@@ -118,7 +119,7 @@ public boolean closeConnection(StatsCollectionContext context) {
       output.close();
       return true;
     } catch (IOException e) {
-      LOG.error("Failed to persist stats on filesystem",e);
+      Utilities.LOG14535.error("Failed to persist stats on filesystem",e);
       return false;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -3375,7 +3375,6 @@ private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {
       }
     }
 
-    // TODO# WRONG!! HERE
     try {
       if (allPartitions == null) {
         db.alterTable(alterTbl.getOldName(), tbl, alterTbl.getIsCascade(), alterTbl.getEnvironmentContext());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -256,6 +256,7 @@ public TaskInformation(Task task, String path) {
 
   @Override
   public int execute(DriverContext driverContext) {
+    if (work.isNoop()) return 0; // TODO# temporary flag for HIVE-14990
     Utilities.LOG14535.info("Executing MoveWork " + System.identityHashCode(work)
         + " with " + work.getLoadFileWork() + "; " + work.getLoadTableWork() + "; "
         + work.getLoadMultiFilesWork());

File: ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java
Patch:
@@ -475,7 +475,7 @@ public Set<Integer> getNonCombinablePathIndices(JobConf job, Path[] paths, int n
     ExecutorService executor = Executors.newFixedThreadPool(numThreads);
     List<Future<Set<Integer>>> futureList = new ArrayList<Future<Set<Integer>>>(numThreads);
     try {
-      boolean isMerge = mrwork.isMergeFromResolver();
+      boolean isMerge = mrwork != null && mrwork.isMergeFromResolver();
       for (int i = 0; i < numThreads; i++) {
         int start = i * numPathPerThread;
         int length = i != numThreads - 1 ? numPathPerThread : paths.length - start;

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
Patch:
@@ -594,8 +594,8 @@ private void updatePartColumnStatsForAlterColumns(RawStore msdb, Partition oldPa
         for (ColumnStatisticsObj statsObj : statsObjs) {
           boolean found =false;
           for (FieldSchema newCol : newCols) {
-            if (statsObj.getColName().equals(newCol.getName())
-                && statsObj.getColType().equals(newCol.getType())) {
+            if (statsObj.getColName().equalsIgnoreCase(newCol.getName())
+                && statsObj.getColType().equalsIgnoreCase(newCol.getType())) {
               found = true;
               break;
             }
@@ -692,7 +692,7 @@ private void alterTableUpdateTableColumnStats(RawStore msdb,
                 boolean found = false;
                 for (FieldSchema newCol : newCols) {
                   if (statsObj.getColName().equalsIgnoreCase(newCol.getName())
-                      && statsObj.getColType().equals(newCol.getType())) {
+                      && statsObj.getColType().equalsIgnoreCase(newCol.getType())) {
                     found = true;
                     break;
                   }

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -4642,6 +4642,7 @@ public boolean update_table_column_statistics(ColumnStatistics colStats)
       for (ColumnStatisticsObj statsObj:statsObjs) {
         colName = statsObj.getColName().toLowerCase();
         statsObj.setColName(colName);
+        statsObj.setColType(statsObj.getColType().toLowerCase());
       }
 
      colStats.setStatsDesc(statsDesc);
@@ -4684,6 +4685,7 @@ private boolean updatePartitonColStats(Table tbl, ColumnStatistics colStats)
       for (ColumnStatisticsObj statsObj:statsObjs) {
         colName = statsObj.getColName().toLowerCase();
         statsObj.setColName(colName);
+        statsObj.setColType(statsObj.getColType().toLowerCase());
       }
 
       colStats.setStatsDesc(statsDesc);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
Patch:
@@ -374,6 +374,7 @@ protected FetchInputFormatSplit[] getNextSplits() throws Exception {
       Utilities.copyTableJobPropertiesToConf(currDesc.getTableDesc(), job);
       InputFormat inputFormat = getInputFormatFromCache(formatter, job);
       String inputs = processCurrPathForMmWriteIds(inputFormat);
+      Utilities.LOG14535.info("Setting fetch inputs to " + inputs);
       if (inputs == null) return null;
       job.set("mapred.input.dir", inputs);
 

File: druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java
Patch:
@@ -254,7 +254,7 @@ private static HiveDruidSplit[] splitSelectQuery(Configuration conf, String addr
       }
 
       intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),
-              timeList.get(0).getValue().getMaxTime().getMillis()));
+              timeList.get(0).getValue().getMaxTime().getMillis(), ISOChronology.getInstanceUTC()));
     } else {
       intervals.addAll(query.getIntervals());
     }
@@ -289,13 +289,13 @@ private static List<List<Interval>> createSplitsIntervals(List<Interval> interva
         final long expectedRange = rangeSize - currTime;
         if (interval.getEndMillis() - startTime >= expectedRange) {
           endTime = startTime + expectedRange;
-          currentIntervals.add(new Interval(startTime, endTime));
+          currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));
           startTime = endTime;
           currTime = 0;
           break;
         }
         endTime = interval.getEndMillis();
-        currentIntervals.add(new Interval(startTime, endTime));
+        currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));
         currTime += (endTime - startTime);
         startTime = intervals.get(++posIntervals).getStartMillis();
       }

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSelectQueryRecordReader.java
Patch:
@@ -62,7 +62,7 @@ public boolean nextKeyValue() throws IOException {
     if (results.hasNext()) {
       current = results.next();
       values = current.getValue().getEvents().iterator();
-      return true;
+      return nextKeyValue();
     }
     return false;
   }

File: druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidTopNQueryRecordReader.java
Patch:
@@ -62,7 +62,7 @@ public boolean nextKeyValue() {
     if (results.hasNext()) {
       current = results.next();
       values = current.getValue().getValue().iterator();
-      return true;
+      return nextKeyValue();
     }
     return false;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java
Patch:
@@ -82,7 +82,7 @@ static ASTNode table(RelNode scan) {
                       dq.getQueryString()) + "\""));
       propList.add(ASTBuilder.construct(HiveParser.TOK_TABLEPROPERTY, "TOK_TABLEPROPERTY")
               .add(HiveParser.StringLiteral, "\"" + Constants.DRUID_QUERY_TYPE + "\"")
-              .add(HiveParser.StringLiteral, "\"" + dq.getQueryType() + "\""));
+              .add(HiveParser.StringLiteral, "\"" + dq.getQueryType().getQueryName() + "\""));
     }
     if (hts.isInsideView()) {
       // We need to carry the insideView information from calcite into the ast.

File: beeline/src/test/org/apache/hive/beeline/TestBeelineArgParsing.java
Patch:
@@ -272,6 +272,7 @@ public void testAddLocalJarWithoutAddDriverClazz() throws Exception {
       Assert.assertNull(bl.findLocalDriver(connectionString));
     } else {
       // no need to add for the default supported local jar driver
+      Assert.assertNotNull(bl.findLocalDriver(connectionString));
       Assert.assertEquals(bl.findLocalDriver(connectionString).getClass().getName(), driverClazzName);
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -3961,6 +3961,7 @@ public static void handleMmTableFinalPath(Path specPath, String unionSuffix, Con
     }
 
     Utilities.LOG14535.info("Looking for manifests in: " + manifestDir + " (" + mmWriteId + ")");
+    // TODO# may be wrong if there are no splits (empty insert/CTAS)
     FileStatus[] manifestFiles = fs.listStatus(manifestDir);
     List<Path> manifests = new ArrayList<>();
     if (manifestFiles != null) {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/LoadTableDesc.java
Patch:
@@ -144,8 +144,8 @@ public boolean getReplace() {
   }
 
   @Explain(displayName = "micromanaged table")
-  public boolean isMmTable() {
-    return mmWriteId != null;
+  public Boolean isMmTable() {
+    return mmWriteId != null? true : null;
   }
 
   public void setReplace(boolean replace) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/cost/HiveAlgorithmsUtil.java
Patch:
@@ -34,7 +34,6 @@
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin;
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.MapJoinStreamingRelation;
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveRelNode;
-import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan;
 
 import com.google.common.collect.ImmutableList;
 

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java
Patch:
@@ -148,7 +148,7 @@ public boolean equals(Object o) {
 
     if (o instanceof ReadEntity) {
       ReadEntity ore = (ReadEntity) o;
-      return (toString().equalsIgnoreCase(ore.toString()));
+      return (getName().equalsIgnoreCase(ore.getName()));
     } else {
       return false;
     }

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/authorization/TestJdbcWithSQLAuthUDFBlacklist.java
Patch:
@@ -72,7 +72,7 @@ public void testBlackListedUdfUsage() throws Exception {
     Connection hs2Conn = DriverManager.getConnection(miniHS2.getJdbcURL(), "user1", "bar");
 
     Statement stmt = hs2Conn.createStatement();
-    String tableName1 = "test_jdbc_sql_auth_udf";
+    String tableName1 = "test_jdbc_sql_auth_udf_blacklist";
     stmt.execute("create table " + tableName1 + "(i int) ");
 
     verifyUDFNotAllowed(stmt, tableName1, "sqrt(1)", "sqrt");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -389,7 +389,8 @@ private DataContainer handleStaticParts(Hive db, Table table, LoadTableDesc tbd,
     db.loadSinglePartition(tbd.getSourcePath(), tbd.getTable().getTableName(),
         tbd.getPartitionSpec(), tbd.getReplace(),
         tbd.getInheritTableSpecs(), isSkewedStoredAsDirs(tbd), work.isSrcLocal(),
-        work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID,
+        (work.getLoadTableWork().getWriteType() != AcidUtils.Operation.NOT_ACID &&
+         work.getLoadTableWork().getWriteType() != AcidUtils.Operation.INSERT_ONLY),
         hasFollowingStatsTask(), tbd.getMmWriteId());
     Partition partn = db.getPartition(table, tbd.getPartitionSpec(), false);
 

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliConfigs.java
Patch:
@@ -129,7 +129,7 @@ public MiniTezCliConfig() {
 
         setHiveConfDir("data/conf/tez");
         setClusterType(MiniClusterType.tez);
-        setMetastoreType(MetastoreType.hbase);
+        setMetastoreType(MetastoreType.sql);
         setFsType(QTestUtil.FsType.hdfs);
       } catch (Exception e) {
         throw new RuntimeException("can't construct cliconfig", e);

File: itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/JdbcWithMiniKdcSQLAuthTest.java
Patch:
@@ -97,6 +97,8 @@ public void testAuthorization1() throws Exception {
 
       Statement stmt = hs2Conn.createStatement();
 
+      stmt.execute("drop table if exists " + tableName1);
+      stmt.execute("drop table if exists " + tableName2);
       // create tables
       stmt.execute("create table " + tableName1 + "(i int) ");
       stmt.execute("create table " + tableName2 + "(i int) ");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreStatsMerge.java
Patch:
@@ -66,7 +66,7 @@ public class TestHiveMetaStoreStatsMerge extends TestCase {
   private final Database db = new Database();
   private Table table = new Table();
 
-  private static final String dbName = "hive3252";
+  private static final String dbName = "hive3253";
   private static final String tblName = "tmptbl";
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -1535,7 +1535,8 @@ public static boolean isPermanentFunction(ExprNodeGenericFuncDesc fnExpr) {
     }
 
     if (clazz != null) {
-      return system.isPermanentFunc(clazz);
+      // Use session registry - see Registry.isPermanentFunc()
+      return SessionState.getRegistryForWrite().isPermanentFunc(clazz);
     }
     return false;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/WindowFunctionInfo.java
Patch:
@@ -28,9 +28,9 @@ public class WindowFunctionInfo extends FunctionInfo {
   private final boolean pivotResult;
   private final boolean impliesOrder;
 
-  public WindowFunctionInfo(boolean isNative, String functionName,
+  public WindowFunctionInfo(FunctionType functionType, String functionName,
       GenericUDAFResolver resolver, FunctionResource[] resources) {
-    super(isNative, functionName, resolver, resources);
+    super(functionType, functionName, resolver, resources);
     WindowFunctionDescription def =
         AnnotationUtils.getAnnotation(resolver.getClass(), WindowFunctionDescription.class);
     supportsWindow = def == null ? true : def.supportsWindow();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java
Patch:
@@ -191,7 +191,7 @@ private static FunctionInfo handleCastForParameterizedType(TypeInfo ti, Function
       throw new RuntimeException(e);
     }
     return new FunctionInfo(
-        fi.isNative(), fi.getDisplayName(), (GenericUDF) udf, fi.getResources());
+        fi.getFunctionType(), fi.getDisplayName(), (GenericUDF) udf, fi.getResources());
   }
 
   // TODO: 1) handle Agg Func Name translation 2) is it correct to add func

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CoreCliDriver.java
Patch:
@@ -61,7 +61,7 @@ public void beforeClass() {
         @Override
         public QTestUtil invokeInternal() throws Exception {
           return new QTestUtil((cliConfig.getResultsDir()), (cliConfig.getLogDir()), miniMR,
-              hiveConfDir, hadoopVer, initScript, cleanupScript, useHBaseMetastore, true, false,
+              hiveConfDir, hadoopVer, initScript, cleanupScript, useHBaseMetastore, true,
               cliConfig.getFsType());
         }
       }.invoke("QtestUtil instance created", LOG, true);

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java
Patch:
@@ -116,8 +116,6 @@ public LlapDaemon(Configuration daemonConf, int numExecutors, long executorMemor
       int mngPort, int shufflePort, int webPort, String appName) {
     super("LlapDaemon");
 
-    initializeLogging(daemonConf);
-
     printAsciiArt();
 
     Preconditions.checkArgument(numExecutors > 0);
@@ -278,7 +276,7 @@ public LlapDaemon(Configuration daemonConf, int numExecutors, long executorMemor
     addIfService(amReporter);
   }
 
-  private void initializeLogging(final Configuration conf) {
+  private static void initializeLogging(final Configuration conf) {
     long start = System.currentTimeMillis();
     URL llap_l4j2 = LlapDaemon.class.getClassLoader().getResource(
         LlapConstants.LOG4j2_PROPERTIES_FILE);
@@ -450,6 +448,7 @@ public static void main(String[] args) throws Exception {
       long ioMemoryBytes = HiveConf.getSizeVar(daemonConf, ConfVars.LLAP_IO_MEMORY_MAX_SIZE);
       boolean isDirectCache = HiveConf.getBoolVar(daemonConf, ConfVars.LLAP_ALLOCATOR_DIRECT);
       boolean isLlapIo = HiveConf.getBoolVar(daemonConf, HiveConf.ConfVars.LLAP_IO_ENABLED, true);
+      LlapDaemon.initializeLogging(daemonConf);
       llapDaemon = new LlapDaemon(daemonConf, numExecutors, executorMemoryBytes, isLlapIo,
           isDirectCache, ioMemoryBytes, localDirs, rpcPort, mngPort, shufflePort, webPort,
           appName);

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CoreCliDriver.java
Patch:
@@ -61,7 +61,8 @@ public void beforeClass() {
         @Override
         public QTestUtil invokeInternal() throws Exception {
           return new QTestUtil((cliConfig.getResultsDir()), (cliConfig.getLogDir()), miniMR,
-              hiveConfDir, hadoopVer, initScript, cleanupScript, useHBaseMetastore, true);
+              hiveConfDir, hadoopVer, initScript, cleanupScript, useHBaseMetastore, true, false,
+              cliConfig.getFsType());
         }
       }.invoke("QtestUtil instance created", LOG, true);
 

File: beeline/src/java/org/apache/hive/beeline/BeeLineOpts.java
Patch:
@@ -74,7 +74,7 @@ class BeeLineOpts implements Completer {
   private boolean autoCommit = false;
   private boolean verbose = false;
   private boolean force = false;
-  private boolean incremental = false;
+  private boolean incremental = true;
   private int incrementalBufferRows = DEFAULT_INCREMENTAL_BUFFER_ROWS;
   private boolean showWarnings = false;
   private boolean showNestedErrs = false;

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
Patch:
@@ -874,6 +874,9 @@ public static void registerNumericType(PrimitiveCategory primitiveCategory, int
     numericTypes.put(primitiveCategory, level);
   }
 
+  /**
+   * Test if it's implicitly convertible for data comparison.
+   */
   public static boolean implicitConvertible(PrimitiveCategory from, PrimitiveCategory to) {
     if (from == to) {
       return true;

File: testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/conf/TestParser.java
Patch:
@@ -23,7 +23,6 @@
 import java.io.File;
 import java.io.FileInputStream;
 import java.io.IOException;
-import java.io.OutputStreamWriter;
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -35,8 +34,6 @@
 import java.util.regex.Pattern;
 
 import org.apache.commons.io.FileUtils;
-import org.apache.log4j.ConsoleAppender;
-import org.apache.log4j.PatternLayout;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -860,7 +860,8 @@ public void cleanUp(String tname) throws Exception {
       SessionState.get().getConf().setBoolean("hive.test.shutdown.phase", true);
       int result = cliDriver.processLine(cleanupCommands);
       if (result != 0) {
-        Assert.fail("Failed during cleanup processLine with code=" + result);
+        LOG.error("Failed during cleanup processLine with code={}. Ignoring", result);
+        // TODO Convert this to an Assert.fail once HIVE-14682 is fixed
       }
       SessionState.get().getConf().setBoolean("hive.test.shutdown.phase", false);
     } else {

File: testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/conf/TestParser.java
Patch:
@@ -23,6 +23,7 @@
 import java.io.File;
 import java.io.FileInputStream;
 import java.io.IOException;
+import java.io.OutputStreamWriter;
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -34,6 +35,8 @@
 import java.util.regex.Pattern;
 
 import org.apache.commons.io.FileUtils;
+import org.apache.log4j.ConsoleAppender;
+import org.apache.log4j.PatternLayout;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliConfigs.java
Patch:
@@ -319,7 +319,7 @@ public HBaseMinimrCliConfig() {
 
         setResultsDir("hbase-handler/src/test/results/positive");
         setLogDir("itests/qtest/target/qfile-results/hbase-handler/minimrpositive");
-        setInitScript("q_test_init.sql");
+        setInitScript("q_test_init_for_minimr.sql");
         setCleanupScript("q_test_cleanup.sql");
         setHiveConfDir("");
         setClusterType(MiniClusterType.mr);

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAbs.java
Patch:
@@ -130,6 +130,9 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
     case STRING:
     case DOUBLE:
       valObject = inputConverter.convert(valObject);
+      if (valObject == null) {
+        return null;
+      }
       resultDouble.set(Math.abs(((DoubleWritable) valObject).get()));
       return resultDouble;
     case DECIMAL:

File: testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/conf/TestParser.java
Patch:
@@ -23,7 +23,6 @@
 import java.io.File;
 import java.io.FileInputStream;
 import java.io.IOException;
-import java.io.OutputStreamWriter;
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -35,8 +34,6 @@
 import java.util.regex.Pattern;
 
 import org.apache.commons.io.FileUtils;
-import org.apache.log4j.ConsoleAppender;
-import org.apache.log4j.PatternLayout;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: beeline/src/java/org/apache/hive/beeline/BeeLineOpts.java
Patch:
@@ -624,6 +624,7 @@ public void setDelimiterForDSV(char delimiterForDSV) {
     this.delimiterForDSV = delimiterForDSV;
   }
 
+  @Ignore
   public HiveConf getConf() {
     return conf;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -1662,8 +1662,10 @@ private RelNode genTableLogicalPlan(String tableAlias, QB qb) throws SemanticExc
       try {
 
         // 1. If the table has a Sample specified, bail from Calcite path.
+        // 2. if returnpath is on and hivetestmode is on bail
         if (qb.getParseInfo().getTabSample(tableAlias) != null
-            || getNameToSplitSampleMap().containsKey(tableAlias)) {
+            || getNameToSplitSampleMap().containsKey(tableAlias)
+            || (conf.getBoolVar(HiveConf.ConfVars.HIVE_CBO_RETPATH_HIVEOP)) && (conf.getBoolVar(HiveConf.ConfVars.HIVETESTMODE)) ) {
           String msg = String.format("Table Sample specified for %s."
               + " Currently we don't support Table Sample clauses in CBO,"
               + " turn off cbo for queries on tableSamples.", tableAlias);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1892,8 +1892,6 @@ public static enum ConfVars {
     HIVEOPTGBYUSINGINDEX("hive.optimize.index.groupby", false,
         "Whether to enable optimization of group-by queries using Aggregate indexes."),
 
-    HIVEOUTERJOINSUPPORTSFILTERS("hive.outerjoin.supports.filters", true, ""),
-
     HIVEFETCHTASKCONVERSION("hive.fetch.task.conversion", "more", new StringSet("none", "minimal", "more"),
         "Some select queries can be converted to single FETCH task minimizing latency.\n" +
         "Currently the query should be single sourced not having any subquery and should not have\n" +

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -191,8 +191,6 @@ public enum ErrorMsg {
   ARCHIVE_ON_TABLE(10110, "ARCHIVE can only be run on partitions"),
   RESERVED_PART_VAL(10111, "Partition value contains a reserved substring"),
   OFFLINE_TABLE_OR_PARTITION(10113, "Query against an offline table or partition"),
-  OUTERJOIN_USES_FILTERS(10114, "The query results could be wrong. " +
-                         "Turn on hive.outerjoin.supports.filters"),
   NEED_PARTITION_SPECIFICATION(10115, "Table is partitioned and partition specification is needed"),
   INVALID_METADATA(10116, "The metadata file could not be parsed "),
   NEED_TABLE_SPECIFICATION(10117, "Table name could be determined; It should be specified "),

File: ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java
Patch:
@@ -53,7 +53,7 @@ public class SetProcessor implements CommandProcessor {
   private static final Logger LOG = LoggerFactory.getLogger(SetProcessor.class);
 
   private static final String prefix = "set: ";
-  private static final Set<String> removedConfigs = Sets.newHashSet("hive.mapred.supports.subdirectories","hive.enforce.sorting","hive.enforce.bucketing");
+  private static final Set<String> removedConfigs = Sets.newHashSet("hive.mapred.supports.subdirectories","hive.enforce.sorting","hive.enforce.bucketing", "hive.outerjoin.supports.filters");
 
   public static boolean getBoolean(String value) {
     if (value.equals("on") || value.equals("true")) {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1912,7 +1912,7 @@ public static enum ConfVars {
         "final aggregations in single reduce task. If this is set true, Hive delegates final aggregation\n" +
         "stage to fetch task, possibly decreasing the query time."),
 
-    HIVEOPTIMIZEMETADATAQUERIES("hive.compute.query.using.stats", false,
+    HIVEOPTIMIZEMETADATAQUERIES("hive.compute.query.using.stats", true,
         "When set to true Hive will answer a few queries like count(1) purely using stats\n" +
         "stored in metastore. For basic stats collection turn on the config hive.stats.autogather to true.\n" +
         "For more advanced stats collection need to run analyze table queries."),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
Patch:
@@ -145,7 +145,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     // the task in. On MR: The cache is a no-op.
     String queryId = HiveConf.getVar(hconf, HiveConf.ConfVars.HIVEQUERYID);
     cacheKey = "HASH_MAP_" + this.getOperatorId() + "_container";
-    cache = ObjectCacheFactory.getCache(hconf, queryId);
+    cache = ObjectCacheFactory.getCache(hconf, queryId, false);
     loader = getHashTableLoader(hconf);
 
     hashMapRowGetters = null;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ObjectCacheFactory.java
Patch:
@@ -44,10 +44,11 @@ private ObjectCacheFactory() {
   /**
    * Returns the appropriate cache
    */
-  public static ObjectCache getCache(Configuration conf, String queryId) {
+  public static ObjectCache getCache(Configuration conf, String queryId, boolean isPlanCache) {
     if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez")) {
       if (LlapProxy.isDaemon()) { // daemon
-        if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_OBJECT_CACHE_ENABLED)) {
+        if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_OBJECT_CACHE_ENABLED)
+            && !isPlanCache) {
           // LLAP object cache, unlike others, does not use globals. Thus, get the existing one.
           return getLlapObjectCache(queryId);
         } else { // no cache

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java
Patch:
@@ -63,7 +63,7 @@ public class MergeFileRecordProcessor extends RecordProcessor {
   private MergeFileWork mfWork;
   MRInputLegacy mrInput = null;
   private final Object[] row = new Object[2];
-  ObjectCache cache;
+  org.apache.hadoop.hive.ql.exec.ObjectCache cache;
 
   public MergeFileRecordProcessor(final JobConf jconf, final ProcessorContext context) {
     super(jconf, context);
@@ -95,8 +95,7 @@ void init(
     }
 
     String queryId = HiveConf.getVar(jconf, HiveConf.ConfVars.HIVEQUERYID);
-    org.apache.hadoop.hive.ql.exec.ObjectCache cache = ObjectCacheFactory
-      .getCache(jconf, queryId);
+    cache = ObjectCacheFactory.getCache(jconf, queryId, true);
 
     try {
       execContext.setJc(jconf);

File: metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.common.classification.InterfaceAudience.Public;
 import org.apache.hadoop.hive.common.classification.InterfaceStability.Evolving;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.annotation.NoReconnect;
 import org.apache.hadoop.hive.metastore.api.AggrStats;
 import org.apache.hadoop.hive.metastore.api.AlreadyExistsException;
 import org.apache.hadoop.hive.metastore.api.ColumnStatistics;
@@ -127,6 +128,7 @@ public interface IMetaStoreClient {
   /**
    * close connection to meta store
    */
+  @NoReconnect
   void close();
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexResult.java
Patch:
@@ -45,7 +45,7 @@
  * HiveIndexResult parses the input stream from an index query
  * to generate a list of file splits to query.
  */
-public class HiveIndexResult {
+public class HiveIndexResult implements IndexResult {
 
   public static final Logger l4j =
     LoggerFactory.getLogger(HiveIndexResult.class.getSimpleName());
@@ -182,6 +182,7 @@ private void add(Text line) throws HiveException {
     bucket.getOffsets().add(Long.parseLong(one_offset));
   }
 
+  @Override
   public boolean contains(FileSplit split) throws HiveException {
 
     if (buckets == null) {

File: hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java
Patch:
@@ -265,7 +265,7 @@ private void enqueue(NotificationEvent event) {
   private static class CleanerThread extends Thread {
     private RawStore rs;
     private int ttl;
-
+    static private long sleepTime = 60000;
 
     CleanerThread(HiveConf conf, RawStore rs) {
       super("CleanerThread");
@@ -281,8 +281,9 @@ public void run() {
         synchronized(NOTIFICATION_TBL_LOCK) {
           rs.cleanNotificationEvents(ttl);
         }
+        LOG.debug("Cleaner thread done");
         try {
-          Thread.sleep(60000);
+          Thread.sleep(sleepTime);
         } catch (InterruptedException e) {
           LOG.info("Cleaner thread sleep interupted", e);
         }

File: llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java
Patch:
@@ -41,6 +41,7 @@
 import org.apache.hadoop.hive.llap.daemon.impl.StaticPermanentFunctionChecker;
 import org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos;
 import org.apache.hadoop.hive.llap.tezplugins.LlapTezUtils;
+import org.apache.hadoop.registry.client.binding.RegistryUtils;
 import org.apache.tez.dag.api.TezConfiguration;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -320,6 +321,7 @@ private void run(String[] args) throws Exception {
         LlapInputFormat.class, // llap-server
         HiveInputFormat.class, // hive-exec
         SslSocketConnector.class, // hive-common (https deps)
+        RegistryUtils.ServiceRecordMarshal.class, // ZK registry
         // log4j2
         com.lmax.disruptor.RingBuffer.class, // disruptor
         org.apache.logging.log4j.Logger.class, // log4j-api

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java
Patch:
@@ -292,7 +292,7 @@ private void generateHashMapResultLargeMultiValue(VectorizedRowBatch batch,
     }
 
     ByteSegmentRef byteSegmentRef = hashMapResult.first();
-    while (true) {
+    while (byteSegmentRef != null) {
 
       // Fill up as much of the overflow batch as possible with small table values.
       while (byteSegmentRef != null) {
@@ -304,7 +304,7 @@ private void generateHashMapResultLargeMultiValue(VectorizedRowBatch batch,
           int length = byteSegmentRef.getLength();
           smallTableVectorDeserializeRow.setBytes(bytes, offset, length);
 
-          smallTableVectorDeserializeRow.deserialize(overflowBatch, overflowBatch.DEFAULT_SIZE);
+          smallTableVectorDeserializeRow.deserialize(overflowBatch, overflowBatch.size);
         }
 
         overflowBatch.size++;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapJoinOperator.java
Patch:
@@ -154,7 +154,7 @@ public void assign(VectorExpressionWriter[] writers, List<ObjectInspector> oids)
 
       // This is a vectorized aware evaluator
       ExprNodeEvaluator eval = new ExprNodeEvaluator<ExprNodeDesc>(desc) {
-        int columnIndex;;
+        int columnIndex;
         int writerIndex;
 
         public ExprNodeEvaluator initVectorExpr(int columnIndex, int writerIndex) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java
Patch:
@@ -636,6 +636,7 @@ protected void completeInitializationOp(Object[] os) throws HiveException {
     default:
       throw new RuntimeException("Unknown vector map join hash table implementation type " + hashTableImplementationType.name());
     }
+    LOG.info("Using " + vectorMapJoinHashTable.getClass().getSimpleName() + " from " + this.getClass().getSimpleName());
   }
 
   /*

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java
Patch:
@@ -507,6 +507,7 @@ protected void reloadHashTable(byte pos, int partitionId)
     vectorMapJoinHashTable = VectorMapJoinOptimizedCreateHashTable.createHashTable(conf,
         smallTable);
     needHashTableSetup = true;
+    LOG.info("Created " + vectorMapJoinHashTable.getClass().getSimpleName() + " from " + this.getClass().getSimpleName());
 
     if (isLogDebugEnabled) {
       LOG.debug(CLASS_NAME + " reloadHashTable!");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashMap.java
Patch:
@@ -81,7 +81,7 @@ public JoinUtil.JoinResult lookup(byte[] keyBytes, int keyStart, int keyLength,
     optimizedHashMapResult.forget();
 
     long hashCode = HashCodeUtil.murmurHash(keyBytes, keyStart, keyLength);
-    long valueRefWord = findReadSlot(keyBytes, keyStart, keyLength, hashCode);
+    long valueRefWord = findReadSlot(keyBytes, keyStart, keyLength, hashCode, hashMapResult.getReadPos());
     JoinUtil.JoinResult joinResult;
     if (valueRefWord == -1) {
       joinResult = JoinUtil.JoinResult.NOMATCH;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashMultiSet.java
Patch:
@@ -75,7 +75,7 @@ public JoinUtil.JoinResult contains(byte[] keyBytes, int keyStart, int keyLength
     optimizedHashMultiSetResult.forget();
 
     long hashCode = HashCodeUtil.murmurHash(keyBytes, keyStart, keyLength);
-    long count = findReadSlot(keyBytes, keyStart, keyLength, hashCode);
+    long count = findReadSlot(keyBytes, keyStart, keyLength, hashCode, hashMultiSetResult.getReadPos());
     JoinUtil.JoinResult joinResult;
     if (count == -1) {
       joinResult = JoinUtil.JoinResult.NOMATCH;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashSet.java
Patch:
@@ -65,7 +65,7 @@ public JoinUtil.JoinResult contains(byte[] keyBytes, int keyStart, int keyLength
     optimizedHashSetResult.forget();
 
     long hashCode = HashCodeUtil.murmurHash(keyBytes, keyStart, keyLength);
-    long existance = findReadSlot(keyBytes, keyStart, keyLength, hashCode);
+    long existance = findReadSlot(keyBytes, keyStart, keyLength, hashCode, hashSetResult.getReadPos());
     JoinUtil.JoinResult joinResult;
     if (existance == -1) {
       joinResult = JoinUtil.JoinResult.NOMATCH;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashTable.java
Patch:
@@ -29,7 +29,7 @@ public abstract class VectorMapJoinFastHashTable implements VectorMapJoinHashTab
   protected int logicalHashBucketMask;
 
   protected float loadFactor;
-  protected int writeBuffersSize;
+  protected final int writeBuffersSize;
 
   protected int metricPutConflict;
   protected int largestNumberOfSteps;

File: service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java
Patch:
@@ -641,7 +641,8 @@ public TGetOperationStatusResp GetOperationStatus(TGetOperationStatusReq req) th
       if (opException != null) {
         resp.setSqlState(opException.getSQLState());
         resp.setErrorCode(opException.getErrorCode());
-        resp.setErrorMessage(opException.getMessage());
+        resp.setErrorMessage(org.apache.hadoop.util.StringUtils.
+            stringifyException(opException));
       }
       resp.setStatus(OK_STATUS);
     } catch (Exception e) {

File: service/src/test/org/apache/hive/service/cli/TestColumn.java
Patch:
@@ -80,7 +80,7 @@ public void testFloatAndDoubleValues() {
     floatColumn.addValue(Type.FLOAT_TYPE, 2.033f);
 
     // FLOAT_TYPE is treated as DOUBLE_TYPE
-    assertEquals(Type.DOUBLE_TYPE, floatColumn.getType());
+    assertEquals(Type.FLOAT_TYPE, floatColumn.getType());
     assertEquals(2, floatColumn.size());
     assertEquals(1.1, floatColumn.get(0));
     assertEquals(2.033, floatColumn.get(1));

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -990,7 +990,7 @@ public static enum ConfVars {
         "Enabling strict large query checks disallows the following:\n" +
         "  Cartesian product (cross join)."),
     @Deprecated
-    HIVEMAPREDMODE("hive.mapred.mode", "nonstrict",
+    HIVEMAPREDMODE("hive.mapred.mode", null,
         "Deprecated; use hive.strict.checks.* settings instead."),
     HIVEALIAS("hive.alias", "", ""),
     HIVEMAPSIDEAGGREGATE("hive.map.aggr", true, "Whether to use map-side aggregation in Hive Group By queries"),

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
Patch:
@@ -171,7 +171,7 @@ private void getOverlappingRanges(long baseOffset, DiskRangeList currentNotCache
       metrics.incrCacheHitBytes(Math.min(requestedLength, currentCached.getLength()));
     }
     if (currentNotCached != null) {
-      assert !currentNotCached.hasData();
+      assert !currentNotCached.hasData(); // Assumes no ranges passed to cache to read have data.
       if (gotAllData != null) {
         gotAllData.value = false;
       }

File: accumulo-handler/src/test/org/apache/hadoop/hive/accumulo/mr/TestHiveAccumuloTypes.java
Patch:
@@ -245,7 +245,7 @@ public void testBinaryTypes() throws Exception {
     Timestamp timestampValue = new Timestamp(now.getTime());
     ByteStream.Output output = new ByteStream.Output();
     TimestampWritable timestampWritable = new TimestampWritable(new Timestamp(now.getTime()));
-    timestampWritable.write(output);
+    timestampWritable.write(new DataOutputStream(output));
     output.close();
     m.put(cfBytes, "timestamp".getBytes(), output.toByteArray());
 

File: metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -18,7 +18,6 @@
 package org.apache.hadoop.hive.metastore.txn;
 
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.util.concurrent.Service;
 import com.jolbox.bonecp.BoneCPConfig;
 import com.jolbox.bonecp.BoneCPDataSource;
 import org.apache.commons.dbcp.ConnectionFactory;
@@ -3254,9 +3253,8 @@ public LockHandle acquireLock(String key) throws MetaException {
       }
     }
     catch(RetryException ex) {
-      acquireLock(key);
+      return acquireLock(key);
     }
-    throw new MetaException("This can't happen because checkRetryable() has a retry limit");
   }
   public void acquireLock(String key, LockHandle handle) {
     //the idea is that this will use LockHandle.dbConn

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java
Patch:
@@ -994,6 +994,7 @@ private static JoinOperator genJoin(RelNode join, ExprNodeDesc[][] joinExpressio
         childOps[0].getCompilationOpContext(), desc, new RowSchema(outputColumns), childOps);
     joinOp.setColumnExprMap(colExprMap);
     joinOp.setPosToAliasMap(posToAliasMap);
+    joinOp.getConf().setBaseSrc(baseSrc);
 
     if (LOG.isDebugEnabled()) {
       LOG.debug("Generated " + joinOp + " with row schema: [" + joinOp.getSchema() + "]");

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatBaseTest.java
Patch:
@@ -84,6 +84,7 @@ protected void setUpHiveConf() {
     hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     hiveConf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE, TEST_WAREHOUSE_DIR);
     hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVEOPTIMIZEMETADATAQUERIES, true);
     hiveConf
     .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
         "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatMapReduceTest.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.LocalFileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.common.StatsSetupConst;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.TableType;
@@ -55,7 +56,6 @@
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
 import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
-
 import org.apache.hive.hcatalog.common.HCatConstants;
 import org.apache.hive.hcatalog.common.HCatUtil;
 import org.apache.hive.hcatalog.data.DefaultHCatRecord;
@@ -71,7 +71,6 @@
 import org.junit.BeforeClass;
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -217,6 +216,7 @@ public void createTable() throws Exception {
     if (isTableImmutable()){
       tableParams.put(hive_metastoreConstants.IS_IMMUTABLE,"true");
     }
+    StatsSetupConst.setBasicStatsState(tableParams, StatsSetupConst.TRUE);
     tbl.setParameters(tableParams);
 
     client.createTable(tbl);

File: orc/src/java/org/apache/orc/impl/SchemaEvolution.java
Patch:
@@ -33,7 +33,7 @@
  * has been schema evolution.
  */
 public class SchemaEvolution {
-  private final Map<TypeDescription, TypeDescription> readerToFile;
+  private final Map<Integer, TypeDescription> readerToFile;
   private final boolean[] included;
   private final TypeDescription readerSchema;
   private static final Log LOG = LogFactory.getLog(SchemaEvolution.class);
@@ -70,7 +70,7 @@ public TypeDescription getFileType(TypeDescription readerType) {
         result = null;
       }
     } else {
-      result = readerToFile.get(readerType);
+      result = readerToFile.get(readerType.getId());
     }
     return result;
   }
@@ -141,7 +141,7 @@ void buildMapping(TypeDescription fileType,
       isOk = ConvertTreeReaderFactory.canConvert(fileType, readerType);
     }
     if (isOk) {
-      readerToFile.put(readerType, fileType);
+      readerToFile.put(readerType.getId(), fileType);
     } else {
       throw new IOException(
           String.format(

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/HCatCli.java
Patch:
@@ -80,6 +80,8 @@ public static void main(String[] args) {
     HiveConf conf = ss.getConf();
 
     HiveConf.setVar(conf, ConfVars.SEMANTIC_ANALYZER_HOOK, HCatSemanticAnalyzer.class.getName());
+    conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     String engine = HiveConf.getVar(conf, ConfVars.HIVE_EXECUTION_ENGINE);
     final String MR_ENGINE = "mr";
     if(!MR_ENGINE.equalsIgnoreCase(engine)) {

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/TestPermsGrp.java
Patch:
@@ -98,6 +98,9 @@ protected void setUp() throws Exception {
     hcatConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hcatConf.setTimeVar(HiveConf.ConfVars.METASTORE_CLIENT_SOCKET_TIMEOUT, 60, TimeUnit.SECONDS);
     hcatConf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
+    hcatConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     clientWH = new Warehouse(hcatConf);
     msc = new HiveMetaStoreClient(hcatConf);
     System.setProperty(HiveConf.ConfVars.PREEXECHOOKS.varname, " ");

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/TestSemanticAnalysis.java
Patch:
@@ -64,6 +64,9 @@ public class TestSemanticAnalysis extends HCatBaseTest {
   public void setUpHCatDriver() throws IOException {
     if (hcatDriver == null) {
       HiveConf hcatConf = new HiveConf(hiveConf);
+      hcatConf
+      .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+          "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
       hcatConf.set(HiveConf.ConfVars.HIVEDEFAULTRCFILESERDE.varname,
           "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe");
       hcatConf.set(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK.varname,

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatBaseTest.java
Patch:
@@ -84,6 +84,9 @@ protected void setUpHiveConf() {
     hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     hiveConf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE, TEST_WAREHOUSE_DIR);
     hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     if (Shell.WINDOWS) {
       WindowsPathUtil.convertPathsFromWindowsToHdfs(hiveConf);
     }

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestPassProperties.java
Patch:
@@ -64,6 +64,9 @@ public void Initialize() throws Exception {
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     driver = new Driver(hiveConf);
     SessionState.start(new CliSessionState(hiveConf));
 

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestE2EScenarios.java
Patch:
@@ -88,6 +88,9 @@ public void setUp() throws Exception {
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     driver = new Driver(hiveConf);
     SessionState.start(new CliSessionState(hiveConf));
 

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java
Patch:
@@ -179,6 +179,9 @@ public void setup() throws Exception {
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
     hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
 
     if (Shell.WINDOWS) {
       WindowsPathUtil.convertPathsFromWindowsToHdfs(hiveConf);

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderComplexSchema.java
Patch:
@@ -125,6 +125,9 @@ public static void setUpBeforeClass() throws Exception {
     hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
 
     if (Shell.WINDOWS) {
       WindowsPathUtil.convertPathsFromWindowsToHdfs(hiveConf);

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderEncryption.java
Patch:
@@ -176,6 +176,9 @@ public void setup() throws Exception {
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
 
     String s = hiveConf.get("hdfs.minidfs.basedir");
     if(s == null || s.length() <= 0) {

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatStorerMulti.java
Patch:
@@ -118,6 +118,9 @@ public void setUp() throws Exception {
       hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
       hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
       hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
+      hiveConf
+      .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+          "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
       driver = new Driver(hiveConf);
       SessionState.start(new CliSessionState(hiveConf));
     }

File: hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/TestStreaming.java
Patch:
@@ -190,6 +190,9 @@ public TestStreaming() throws Exception {
     conf = new HiveConf(this.getClass());
     conf.set("fs.raw.impl", RawFileSystem.class.getName());
     conf.set("hive.enforce.bucketing", "true");
+    conf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     TxnDbUtil.setConfValues(conf);
     if (metaStoreURI!=null) {
       conf.setVar(HiveConf.ConfVars.METASTOREURIS, metaStoreURI);

File: hcatalog/webhcat/java-client/src/test/java/org/apache/hive/hcatalog/api/repl/commands/TestCommands.java
Patch:
@@ -77,7 +77,9 @@ public static void setUpBeforeClass() throws Exception {
     TestHCatClient.startMetaStoreServer();
     hconf = TestHCatClient.getConf();
     hconf.set(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK.varname,"");
-
+    hconf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     TEST_PATH = System.getProperty("test.warehouse.dir","/tmp") + Path.SEPARATOR +
         TestCommands.class.getCanonicalName() + "-" + System.currentTimeMillis();
     Path testPath = new Path(TEST_PATH);

File: itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/hbase/TestPigHBaseStorageHandler.java
Patch:
@@ -78,6 +78,9 @@ public void Initialize() throws Exception {
     hcatConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
     hcatConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hcatConf.set(ConfVars.METASTOREWAREHOUSE.varname, whPath.toString());
+    hcatConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
 
     //Add hbase properties
     for (Map.Entry<String, String> el : getHbaseConf()) {

File: itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java
Patch:
@@ -73,6 +73,9 @@ public static void connectToMetastore() throws Exception {
     conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     conf.setBoolVar(HiveConf.ConfVars.FIRE_EVENTS_FOR_DML, true);
     conf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
+    conf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     SessionState.start(new CliSessionState(conf));
     msClient = new HiveMetaStoreClient(conf);
     driver = new Driver(conf);

File: itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java
Patch:
@@ -76,6 +76,9 @@ public void setup() throws Exception {
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, warehouseDir);
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     driver = new Driver(hiveConf);
     SessionState.start(new CliSessionState(hiveConf));
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetaStoreMetrics.java
Patch:
@@ -49,6 +49,9 @@ public static void before() throws Exception {
     hiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
     hiveConf.setBoolVar(HiveConf.ConfVars.METASTORE_METRICS, true);
     hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
+    hiveConf
+        .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+            "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
 
     MetricsFactory.close();
     MetricsFactory.init(hiveConf);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -2965,7 +2965,7 @@ private RelNode genSelectLogicalPlan(QB qb, RelNode srcRel, RelNode starSrcRel)
         if (expr.getType() == HiveParser.TOK_ALLCOLREF) {
           pos = genColListRegex(".*", expr.getChildCount() == 0 ? null : SemanticAnalyzer
               .getUnescapedName((ASTNode) expr.getChild(0)).toLowerCase(), expr, col_list,
-              excludedColumns, inputRR, starRR, pos, out_rwsch, tableMask.isEnabled() ? qb.getAliases() : tabAliasesForAllProjs, true);
+              excludedColumns, inputRR, starRR, pos, out_rwsch, qb.getAliases(), true);
           selectStar = true;
         } else if (expr.getType() == HiveParser.TOK_TABLE_OR_COL
             && !hasAsClause
@@ -2977,7 +2977,7 @@ private RelNode genSelectLogicalPlan(QB qb, RelNode srcRel, RelNode starSrcRel)
           // We don't allow this for ExprResolver - the Group By case
           pos = genColListRegex(SemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getText()),
               null, expr, col_list, excludedColumns, inputRR, starRR, pos, out_rwsch,
-              tableMask.isEnabled() ? qb.getAliases() : tabAliasesForAllProjs, true);
+              qb.getAliases(), true);
         } else if (expr.getType() == HiveParser.DOT
             && expr.getChild(0).getType() == HiveParser.TOK_TABLE_OR_COL
             && inputRR.hasTableAlias(SemanticAnalyzer.unescapeIdentifier(expr.getChild(0)
@@ -2993,7 +2993,7 @@ private RelNode genSelectLogicalPlan(QB qb, RelNode srcRel, RelNode starSrcRel)
               SemanticAnalyzer.unescapeIdentifier(expr.getChild(1).getText()),
               SemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getChild(0).getText()
                   .toLowerCase()), expr, col_list, excludedColumns, inputRR, starRR, pos,
-              out_rwsch, tableMask.isEnabled() ? qb.getAliases() : tabAliasesForAllProjs, true);
+              out_rwsch, qb.getAliases(), true);
         } else if (ParseUtils.containsTokenOfType(expr, HiveParser.TOK_FUNCTIONDI)
             && !(srcRel instanceof HiveAggregate)) {
           // Likely a malformed query eg, select hash(distinct c1) from t1;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -10580,7 +10580,9 @@ private void walkASTMarkTABREF(ASTNode ast, Set<String> cteAlias)
         try {
           table = getTableObjectByName(tabIdName);
         } catch (HiveException e) {
-          throw new SemanticException("Table " + tabIdName + " is not found.");
+          // Table may not be found when materialization of CTE is on.
+          LOG.info("Table " + tabIdName + " is not found in walkASTMarkTABREF.");
+          continue;
         }
 
         List<String> colNames = new ArrayList<>();

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java
Patch:
@@ -95,6 +95,9 @@ public void setUp() throws Exception {
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
     hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     TxnDbUtil.setConfValues(hiveConf);
     TxnDbUtil.prepDb();
     File f = new File(TEST_WAREHOUSE_DIR);

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java
Patch:
@@ -108,6 +108,9 @@ public void setUp() throws Exception {
     hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
     hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     hiveConf.setVar(HiveConf.ConfVars.HIVEINPUTFORMAT, HiveInputFormat.class.getName());
+    hiveConf
+        .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+            "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     TxnDbUtil.setConfValues(hiveConf);
     TxnDbUtil.prepDb();
     File f = new File(TEST_WAREHOUSE_DIR);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestExecDriver.java
Patch:
@@ -88,6 +88,8 @@ public class TestExecDriver extends TestCase {
       conf = queryState.getConf();
       conf.setBoolVar(HiveConf.ConfVars.SUBMITVIACHILD, true);
       conf.setBoolVar(HiveConf.ConfVars.SUBMITLOCALTASKVIACHILD, true);
+      conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+          "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
 
       SessionState.start(conf);
 

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
Patch:
@@ -398,6 +398,8 @@ public void testFetchOperatorContext() throws Exception {
     HiveConf conf = new HiveConf();
     conf.set("hive.support.concurrency", "false");
     conf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
+    conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     SessionState.start(conf);
     String cmd = "create table fetchOp (id int, name string) " +
         "partitioned by (state string) " +

File: ql/src/test/org/apache/hadoop/hive/ql/io/TestSymlinkTextInputFormat.java
Patch:
@@ -133,7 +133,9 @@ public void testCombine() throws Exception {
 
 
     HiveConf hiveConf = new HiveConf(TestSymlinkTextInputFormat.class);
-
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     HiveConf.setBoolVar(hiveConf, HiveConf.ConfVars.HIVE_REWORK_MAPREDWORK, true);
     HiveConf.setBoolVar(hiveConf, HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     Driver drv = new Driver(hiveConf);

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager.java
Patch:
@@ -62,6 +62,9 @@ public class TestDbTxnManager {
   HashSet<WriteEntity> writeEntities;
 
   public TestDbTxnManager() throws Exception {
+    conf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     TxnDbUtil.setConfValues(conf);
     SessionState.start(conf);
     ctx = new Context(conf);

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java
Patch:
@@ -70,6 +70,9 @@ public class TestDbTxnManager2 {
 
   @BeforeClass
   public static void setUpClass() throws Exception {
+    conf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     TxnDbUtil.setConfValues(conf);
     conf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
   }

File: ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDummyTxnManager.java
Patch:
@@ -63,6 +63,9 @@ public class TestDummyTxnManager {
   public void setUp() throws Exception {
     conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, true);
     conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, DummyTxnManager.class.getName());
+    conf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     SessionState.start(conf);
     ctx = new Context(conf);
 

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHive.java
Patch:
@@ -78,6 +78,9 @@ public class TestHive extends TestCase {
   protected void setUp() throws Exception {
     super.setUp();
     hiveConf = new HiveConf(this.getClass());
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     // enable trash so it can be tested
     hiveConf.setFloat("fs.trash.checkpoint.interval", 30);  // FS_TRASH_CHECKPOINT_INTERVAL_KEY (hadoop-2)
     hiveConf.setFloat("fs.trash.interval", 30);             // FS_TRASH_INTERVAL_KEY (hadoop-2)

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveRemote.java
Patch:
@@ -40,6 +40,9 @@ public class TestHiveRemote extends TestHive {
   protected void setUp() throws Exception {
     super.setUp();
     hiveConf = new HiveConf(this.getClass());
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     hiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + MetaStoreUtils.startMetaStore());
 
     try {

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestColumnAccess.java
Patch:
@@ -184,6 +184,9 @@ private Map<String, List<String>> getColsFromReadEntity(HashSet<ReadEntity> inpu
 
   private static Driver createDriver() {
     HiveConf conf = new HiveConf(Driver.class);
+    conf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     conf.setBoolVar(HiveConf.ConfVars.HIVE_STATS_COLLECT_SCANCOLS, true);
     SessionState.start(conf);

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestHiveDecimalParse.java
Patch:
@@ -132,6 +132,9 @@ public void testDecimalType9() throws ParseException {
 
   private Driver createDriver() {
     HiveConf conf = new HiveConf(Driver.class);
+    conf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
 
     SessionState.start(conf);
     Driver driver = new Driver(conf);

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestQBCompact.java
Patch:
@@ -51,6 +51,9 @@ public class TestQBCompact {
   public static void init() throws Exception {
     queryState = new QueryState(null);
     conf = queryState.getConf();
+    conf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     SessionState.start(conf);
 
     // Create a table so we can work against it

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestUpdateDeleteSemanticAnalyzer.java
Patch:
@@ -225,6 +225,9 @@ public void testInsertValuesPartitioned() throws Exception {
   public void setup() {
     queryState = new QueryState(null);
     conf = queryState.getConf();
+    conf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     conf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
     conf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");

File: ql/src/test/org/apache/hadoop/hive/ql/parse/authorization/TestHiveAuthorizationTaskFactory.java
Patch:
@@ -103,6 +103,9 @@ public void setup() throws Exception {
     HiveConf conf = queryState.getConf();
     conf.setVar(ConfVars.HIVE_AUTHORIZATION_TASK_FACTORY,
         TestHiveAuthorizationTaskFactory.DummyHiveAuthorizationTaskFactoryImpl.class.getName());
+    conf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     db = Mockito.mock(Hive.class);
     table = new Table(DB, TABLE);
     partition = new Partition(table);

File: ql/src/test/org/apache/hadoop/hive/ql/plan/TestReadEntityDirect.java
Patch:
@@ -178,6 +178,9 @@ public void testSelectEntityInDirectJoinAlias() throws ParseException {
    */
   private static Driver createDriver() {
     HiveConf conf = new HiveConf(Driver.class);
+    conf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     conf.setVar(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK,
         CheckInputReadEntityDirect.class.getName());
     HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);

File: ql/src/test/org/apache/hadoop/hive/ql/plan/TestViewEntity.java
Patch:
@@ -59,6 +59,9 @@ public void postAnalyze(HiveSemanticAnalyzerHookContext context,
   @BeforeClass
   public static void onetimeSetup() throws Exception {
     HiveConf conf = new HiveConf(Driver.class);
+    conf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     conf.setVar(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK,
         CheckInputReadEntity.class.getName());
     HiveConf

File: service/src/test/org/apache/hive/service/auth/TestPlainSaslHelper.java
Patch:
@@ -35,6 +35,9 @@ public class TestPlainSaslHelper extends TestCase {
   public void testDoAsSetting(){
 
     HiveConf hconf = new HiveConf();
+    hconf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     assertTrue("default value of hive server2 doAs should be true",
         hconf.getBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS));
 

File: service/src/test/org/apache/hive/service/cli/TestRetryingThriftCLIServiceClient.java
Patch:
@@ -60,6 +60,9 @@ public void init() {
     hiveConf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_THRIFT_CLIENT_CONNECTION_RETRY_LIMIT, 3);
     hiveConf.setIntVar(HiveConf.ConfVars.HIVE_SERVER2_ASYNC_EXEC_THREADS, 10);
     hiveConf.setVar(HiveConf.ConfVars.HIVE_SERVER2_ASYNC_EXEC_SHUTDOWN_TIMEOUT, "1s");
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
   }
 
   private void startHiveServer() throws InterruptedException {

File: service/src/test/org/apache/hive/service/cli/session/TestSessionGlobalInitFile.java
Patch:
@@ -90,6 +90,9 @@ public void setUp() throws Exception {
     hiveConf = new HiveConf();
     hiveConf.setVar(HiveConf.ConfVars.HIVE_SERVER2_GLOBAL_INIT_FILE_LOCATION,
         initFile.getParentFile().getAbsolutePath());
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     service = new FakeEmbeddedThriftBinaryCLIService(hiveConf);
     service.init(new HiveConf());
     client = new ThriftCLIServiceClient(service);

File: service/src/test/org/apache/hive/service/server/TestHS2HttpServer.java
Patch:
@@ -58,6 +58,9 @@ public static void beforeTests() throws Exception {
     hiveConf = new HiveConf();
     hiveConf.set(ConfVars.METASTOREPWD.varname, metastorePasswd);
     hiveConf.set(ConfVars.HIVE_SERVER2_WEBUI_PORT.varname, webUIPort.toString());
+    hiveConf
+    .setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory");
     hiveServer2 = new HiveServer2();
     hiveServer2.init(hiveConf);
     hiveServer2.start();

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java
Patch:
@@ -79,6 +79,7 @@ public RecordReader rowsOptions(Options options) throws IOException {
     boolean[] include = options.getInclude();
     // if included columns is null, then include all columns
     if (include == null) {
+      options = options.clone();
       include = new boolean[types.size()];
       Arrays.fill(include, true);
       options.include(include);

File: beeline/src/java/org/apache/hive/beeline/Commands.java
Patch:
@@ -1316,7 +1316,7 @@ public boolean connect(String line) throws Exception {
     Properties props = new Properties();
     if (url != null) {
       String saveUrl = getUrlToUse(url);
-      props.setProperty(JdbcConnectionParams.PROPERTY_URL, url);
+      props.setProperty(JdbcConnectionParams.PROPERTY_URL, saveUrl);
     }
 
     String value = null;

File: llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java
Patch:
@@ -170,8 +170,8 @@ public RecordReader<NullWritable, V> getRecordReader(InputSplit split, JobConf j
     LOG.info("Registered id: " + fragmentId);
 
     @SuppressWarnings("rawtypes")
-    LlapBaseRecordReader recordReader = new LlapBaseRecordReader(
-        socket.getInputStream(), llapSplit.getSchema(), Text.class, job, llapClient);
+    LlapBaseRecordReader recordReader = new LlapBaseRecordReader(socket.getInputStream(),
+        llapSplit.getSchema(), Text.class, job, llapClient, (java.io.Closeable)socket);
     umbilicalResponder.setRecordReader(recordReader);
     return recordReader;
   }

File: ql/src/test/org/apache/hadoop/hive/llap/TestLlapOutputFormat.java
Patch:
@@ -103,7 +103,8 @@ public void testValues() throws Exception {
       writer.close(null);
 
       InputStream in = socket.getInputStream();
-      LlapBaseRecordReader reader = new LlapBaseRecordReader(in, null, Text.class, job, null);
+      LlapBaseRecordReader reader = new LlapBaseRecordReader(
+          in, null, Text.class, job, null, null);
 
       LOG.debug("Have record reader");
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExtractRow.java
Patch:
@@ -183,8 +183,8 @@ public Object extractRowColumn(VectorizedRowBatch batch, int batchIndex, int log
     final int projectionColumnNum = projectionColumnNums[logicalColumnIndex];
     ColumnVector colVector = batch.cols[projectionColumnNum];
     if (colVector == null) {
-      // In rare cases, the planner will not include columns for reading but other parts of
-      // execution will ask for but not use them..
+      // The planner will not include unneeded columns for reading but other parts of execution
+      // may ask for them..
       return null;
     }
     int adjustedIndex = (colVector.isRepeating ? 0 : batchIndex);

File: service/src/java/org/apache/hive/http/LlapServlet.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor;
 import org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver;
 
 @SuppressWarnings("serial")
@@ -97,7 +98,7 @@ public void doGet(HttpServletRequest request, HttpServletResponse response) {
 
         LOG.info("Retrieving info for cluster: " + clusterName);
         LlapStatusServiceDriver driver = new LlapStatusServiceDriver();
-        int ret = driver.run(new String[] { "-n", clusterName });
+        int ret = driver.run(new LlapStatusOptionsProcessor.LlapStatusOptions(clusterName));
         if (ret == LlapStatusServiceDriver.ExitCode.SUCCESS.getInt()) {
           driver.outputJson(writer);
         }

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
Patch:
@@ -501,13 +501,11 @@ private void pushProjection(final JobConf newjob, final StringBuilder readColumn
       final StringBuilder readColumnNamesBuffer) {
     String readColIds = readColumnsBuffer.toString();
     String readColNames = readColumnNamesBuffer.toString();
-    boolean readAllColumns = readColIds.isEmpty() ? true : false;
-    newjob.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, readAllColumns);
+    newjob.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);
     newjob.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, readColIds);
     newjob.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, readColNames);
 
     if (LOG.isInfoEnabled()) {
-      LOG.info("{} = {}", ColumnProjectionUtils.READ_ALL_COLUMNS, readAllColumns);
       LOG.info("{} = {}", ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, readColIds);
       LOG.info("{} = {}", ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, readColNames);
     }

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java
Patch:
@@ -305,8 +305,8 @@ private synchronized ResponseWrapper heartbeat(Collection<TezEvent> eventsArg) t
         task.setNextPreRoutedEventId(response.getNextPreRoutedEventId());
         List<TezEvent> taskEvents = null;
         if (response.getEvents() != null && !response.getEvents().isEmpty()) {
-          if (LOG.isDebugEnabled()) {
-            LOG.debug("Routing events from heartbeat response to task" + ", currentTaskAttemptId="
+          if (LOG.isInfoEnabled()) {
+            LOG.info("Routing events from heartbeat response to task" + ", currentTaskAttemptId="
                 + task.getTaskAttemptID() + ", eventCount=" + response.getEvents().size()
                 + " fromEventId=" + fromEventId
                 + " nextFromEventId=" + response.getNextFromEventId());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapredContext.java
Patch:
@@ -52,7 +52,9 @@ public static MapredContext init(boolean isMap, JobConf jobConf) {
         HiveConf.getVar(jobConf, ConfVars.HIVE_EXECUTION_ENGINE).equals("tez") ?
             new TezContext(isMap, jobConf) : new MapredContext(isMap, jobConf);
     contexts.set(context);
-    logger.debug("MapredContext initialized.");
+    if (logger.isDebugEnabled()) {
+      logger.debug("MapredContext initialized.");
+    }
     return context;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
Patch:
@@ -503,6 +503,7 @@ protected void initializeChildren(Configuration hconf) throws HiveException {
   }
 
   public void abort() {
+    LOG.info("Received abort in operator: {}", getName());
     abortOp.set(true);
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/RecordProcessor.java
Patch:
@@ -89,6 +89,8 @@ void init(MRTaskReporter mrReporter,
     isLogInfoEnabled = l4j.isInfoEnabled();
     isLogTraceEnabled = l4j.isTraceEnabled();
 
+    checkAbortCondition();
+
     //log classpaths
     try {
       if (l4j.isDebugEnabled()) {

File: orc/src/java/org/apache/orc/impl/TreeReaderFactory.java
Patch:
@@ -2034,7 +2034,7 @@ public static TreeReader createTreeReader(TypeDescription readerType,
       return new NullTreeReader(0);
     }
     TypeDescription.Category readerTypeCategory = readerType.getCategory();
-    if (!fileType.getCategory().equals(readerTypeCategory) &&
+    if (!fileType.equals(readerType) &&
         (readerTypeCategory != TypeDescription.Category.STRUCT &&
          readerTypeCategory != TypeDescription.Category.MAP &&
          readerTypeCategory != TypeDescription.Category.LIST &&

File: metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -37,9 +37,9 @@
 import org.apache.commons.pool.ObjectPool;
 import org.apache.commons.pool.impl.GenericObjectPool;
 import org.apache.hadoop.hive.common.JavaUtils;
+import org.apache.hadoop.hive.common.StringableMap;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.*;
-import org.apache.hadoop.hive.metastore.txn.TxnUtils.StringableMap;
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.util.StringUtils;
 

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.JavaUtils;
+import org.apache.hadoop.hive.common.StringableMap;
 import org.apache.hadoop.hive.common.ValidTxnList;
 import org.apache.hadoop.hive.common.ValidReadTxnList;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -33,7 +34,6 @@
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.metastore.txn.CompactionInfo;
-import org.apache.hadoop.hive.metastore.txn.TxnUtils.StringableMap;
 import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;
 import org.apache.hadoop.hive.ql.io.AcidInputFormat;
 import org.apache.hadoop.hive.ql.io.AcidOutputFormat;

File: ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.common.StringableMap;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.CompactionRequest;
 import org.apache.hadoop.hive.metastore.api.CompactionType;
@@ -31,7 +32,6 @@
 import org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement;
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.metastore.txn.TxnStore;
-import org.apache.hadoop.hive.metastore.txn.TxnUtils.StringableMap;
 import org.apache.hadoop.hive.ql.io.AcidUtils;
 import org.junit.Assert;
 import org.junit.Test;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -1580,7 +1580,7 @@ private static String normalizeDateCol(
       Object colValue, String originalColSpec) throws SemanticException {
     Date value;
     if (colValue instanceof DateWritable) {
-      value = ((DateWritable) colValue).get();
+      value = ((DateWritable) colValue).get(false); // Time doesn't matter.
     } else if (colValue instanceof Date) {
       value = (Date) colValue;
     } else {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDayOfMonth.java
Patch:
@@ -90,7 +90,7 @@ public IntWritable evaluate(DateWritable d) {
       return null;
     }
 
-    calendar.setTime(d.get());
+    calendar.setTime(d.get(false)); // Time doesn't matter.
     result.set(calendar.get(Calendar.DAY_OF_MONTH));
     return result;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFMonth.java
Patch:
@@ -88,7 +88,7 @@ public IntWritable evaluate(DateWritable d) {
       return null;
     }
 
-    calendar.setTime(d.get());
+    calendar.setTime(d.get(false));  // Time doesn't matter.
     result.set(1 + calendar.get(Calendar.MONTH));
     return result;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFWeekOfYear.java
Patch:
@@ -87,7 +87,7 @@ public IntWritable evaluate(DateWritable d) {
       return null;
     }
 
-    calendar.setTime(d.get());
+    calendar.setTime(d.get(false));  // Time doesn't matter.
     result.set(calendar.get(Calendar.WEEK_OF_YEAR));
     return result;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFYear.java
Patch:
@@ -90,7 +90,7 @@ public IntWritable evaluate(DateWritable d) {
       return null;
     }
 
-    calendar.setTime(d.get());
+    calendar.setTime(d.get(false));  // Time doesn't matter.
     result.set(calendar.get(Calendar.YEAR));
     return result;
   }

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorDateExpressions.java
Patch:
@@ -440,7 +440,7 @@ public Void call() throws Exception {
       VectorUDFDateString udf = new VectorUDFDateString(0, 1);
       VectorizedRowBatch batch = new VectorizedRowBatch(2, batchSize);
       BytesColumnVector in = new BytesColumnVector(batchSize);
-      BytesColumnVector out = new BytesColumnVector(batchSize);
+      LongColumnVector out = new LongColumnVector(batchSize);
       batch.cols[0] = in;
       batch.cols[1] = out;
       for (int i = 0; i < batchSize; i++) {

File: hplsql/src/main/java/org/apache/hive/hplsql/Package.java
Patch:
@@ -133,7 +133,7 @@ public boolean execFunc(String name, HplsqlParser.Expr_func_paramsContext ctx) {
   }
   
   /**
-   * Execute rocedure
+   * Execute procedure
    */
   public boolean execProc(String name, HplsqlParser.Expr_func_paramsContext ctx, boolean traceNotExists) {
     Create_procedure_stmtContext p = proc.get(name.toUpperCase());

File: hplsql/src/main/java/org/apache/hive/hplsql/Select.java
Patch:
@@ -147,10 +147,10 @@ else if (ctx.parent instanceof HplsqlParser.StmtContext) {
     }
     catch (SQLException e) {
       exec.signal(query);
-      exec.closeQuery(query, exec.conf.defaultConnection);
+      exec.closeQuery(query, conn);
       return 1;
     }
-    exec.closeQuery(query, exec.conf.defaultConnection);
+    exec.closeQuery(query, conn);
     return 0; 
   }  
 

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java
Patch:
@@ -852,6 +852,7 @@ private int getNumPartitionsViaSqlFilterInternal(String dbName, String tblName,
 
     long start = doTrace ? System.nanoTime() : 0;
     Query query = pm.newQuery("javax.jdo.query.SQL", queryText);
+    query.setUnique(true);
     int sqlResult = extractSqlInt(query.executeWithArray(params));
     long queryTime = doTrace ? System.nanoTime() : 0;
     timingTrace(doTrace, queryText, start, queryTime);

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -1760,7 +1760,7 @@ public Map<Map<String, String>, Partition> loadDynamicPartitions(Path loadPath,
           partNames.add(p.getName());
         }
         metaStoreClient.addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),
-          partNames, operation.toDataOperationType());
+          partNames, AcidUtils.toDataOperationType(operation));
       }
       return partitionsMap;
     } catch (IOException e) {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2663,7 +2663,7 @@ public static enum ConfVars {
         "cases of file overwrites. This is supported on HDFS."),
     LLAP_ORC_ENABLE_TIME_COUNTERS("hive.llap.io.orc.time.counters", true,
         "Whether to enable time counters for LLAP IO layer (time spent in HDFS, etc.)"),
-    LLAP_AUTO_ALLOW_UBER("hive.llap.auto.allow.uber", true,
+    LLAP_AUTO_ALLOW_UBER("hive.llap.auto.allow.uber", false,
         "Whether or not to allow the planner to run vertices in the AM."),
     LLAP_AUTO_ENFORCE_TREE("hive.llap.auto.enforce.tree", true,
         "Enforce that all parents are in llap, before considering vertex"),

File: ql/src/java/org/apache/hadoop/hive/ql/processors/DfsProcessor.java
Patch:
@@ -90,7 +90,7 @@ public Map<String, String> getHiveVariable() {
 
       int ret = dfs.run(tokens);
       if (ret != 0) {
-        console.printError("Command failed with exit code = " + ret);
+        console.printError("Command " + command + " failed with exit code = " + ret);
       }
 
       System.setOut(oldOut);

File: llap-common/src/test/org/apache/hadoop/hive/llap/tez/TestConverters.java
Patch:
@@ -42,7 +42,7 @@
 
 public class TestConverters {
 
-  @Test(timeout = 5000)
+  @Test(timeout = 10000)
   public void testTaskSpecToFragmentSpec() {
     ByteBuffer procBb = ByteBuffer.allocate(4);
     procBb.putInt(0, 200);
@@ -98,7 +98,7 @@ public void testTaskSpecToFragmentSpec() {
 
   }
 
-  @Test (timeout = 5000)
+  @Test (timeout = 10000)
   public void testFragmentSpecToTaskSpec() {
 
     ByteBuffer procBb = ByteBuffer.allocate(4);
@@ -142,7 +142,7 @@ public void testFragmentSpecToTaskSpec() {
 
     SignableVertexSpec vertexProto = builder.build();
 
-    TaskSpec taskSpec = Converters.getTaskSpecfromProto(vertexProto, 0, 0, null);
+    TaskSpec taskSpec = Converters.getTaskSpecfromProto(vertexProto, 0, 0, tezTaskAttemptId);
 
     assertEquals("dagName", taskSpec.getDAGName());
     assertEquals("vertexName", taskSpec.getVertexName());

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/AccumuloPredicateHandler.java
Patch:
@@ -149,7 +149,7 @@ public CompareOp getCompareOp(String udfType, IndexSearchCondition sc)
       return clz.newInstance();
     } catch (ClassCastException e) {
       throw new SerDeException("Column type mismatch in WHERE clause "
-          + sc.getComparisonExpr().getExprString() + " found type "
+          + sc.getIndexExpr().getExprString() + " found type "
           + sc.getConstantDesc().getTypeString() + " instead of "
           + sc.getColumnDesc().getTypeString());
     } catch (IllegalAccessException e) {
@@ -181,7 +181,7 @@ public PrimitiveComparison getPrimitiveComparison(String type, IndexSearchCondit
       return clz.newInstance();
     } catch (ClassCastException e) {
       throw new SerDeException("Column type mismatch in WHERE clause "
-          + sc.getComparisonExpr().getExprString() + " found type "
+          + sc.getIndexExpr().getExprString() + " found type "
           + sc.getConstantDesc().getTypeString() + " instead of "
           + sc.getColumnDesc().getTypeString());
     } catch (IllegalAccessException e) {

File: accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/PushdownTuple.java
Patch:
@@ -60,7 +60,7 @@ public PushdownTuple(IndexSearchCondition sc, PrimitiveComparison pCompare, Comp
     } catch (ClassCastException cce) {
       log.info(StringUtils.stringifyException(cce));
       throw new SerDeException(" Column type mismatch in where clause "
-          + sc.getComparisonExpr().getExprString() + " found type "
+          + sc.getIndexExpr().getExprString() + " found type "
           + sc.getConstantDesc().getTypeString() + " instead of "
           + sc.getColumnDesc().getTypeString());
     } catch (HiveException e) {

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/AbstractHBaseKeyFactory.java
Patch:
@@ -18,16 +18,16 @@
 
 package org.apache.hadoop.hive.hbase;
 
+import java.io.IOException;
+import java.util.Properties;
+
 import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.mapred.JobConf;
 
-import java.io.IOException;
-import java.util.Properties;
-
 public abstract class AbstractHBaseKeyFactory implements HBaseKeyFactory {
 
   protected HBaseSerDeParameters hbaseParams;

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveStoragePredicateHandler.java
Patch:
@@ -18,13 +18,13 @@
 
 package org.apache.hadoop.hive.ql.metadata;
 
+import java.io.Serializable;
+
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.mapred.JobConf;
 
-import java.io.Serializable;
-
 /**
  * HiveStoragePredicateHandler is an optional companion to {@link
  * HiveStorageHandler}; it should only be implemented by handlers which

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java
Patch:
@@ -259,7 +259,7 @@ public VectorMapJoinFastLongHashTable(
     super(initialCapacity, loadFactor, writeBuffersSize);
     this.isOuterJoin = isOuterJoin;
     this.hashTableKeyType = hashTableKeyType;
-    PrimitiveTypeInfo[] primitiveTypeInfos = { TypeInfoFactory.longTypeInfo };
+    PrimitiveTypeInfo[] primitiveTypeInfos = { hashTableKeyType.getPrimitiveTypeInfo() };
     keyBinarySortableDeserializeRead = new BinarySortableDeserializeRead(primitiveTypeInfos);
     allocateBucketArray();
     useMinMax = minMaxEnabled;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java
Patch:
@@ -161,7 +161,7 @@ public VectorMapJoinOptimizedLongCommon(
     min = Long.MAX_VALUE;
     max = Long.MIN_VALUE;
     this.hashTableKeyType = hashTableKeyType;
-    // PrimitiveTypeInfo[] primitiveTypeInfos = { TypeInfoFactory.longTypeInfo };
+    // PrimitiveTypeInfo[] primitiveTypeInfos = { hashTableKeyType.getPrimitiveTypeInfo() };
     // keyBinarySortableDeserializeRead = new BinarySortableDeserializeRead(primitiveTypeInfos);
     keyBinarySortableSerializeWrite = new BinarySortableSerializeWrite(1);
     output = new Output();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortLimitPullUpConstantsRule.java
Patch:
@@ -154,7 +154,9 @@ public void onMatch(RelOptRuleCall call) {
             relBuilder.fields(RelCollations.of(fieldCollations));
     relBuilder.sortLimit(sort.offset == null ? -1 : RexLiteral.intValue(sort.offset),
             sort.fetch == null ? -1 : RexLiteral.intValue(sort.fetch), sortFields);
+    // Create top Project fixing nullability of fields
     relBuilder.project(topChildExprs, topChildExprsFields);
+    relBuilder.convert(sort.getRowType(), false);
 
     call.transformTo(parent.copy(parent.getTraitSet(), ImmutableList.of(relBuilder.build())));
   }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveUnionPullUpConstantsRule.java
Patch:
@@ -131,7 +131,9 @@ public void onMatch(RelOptRuleCall call) {
       relBuilder.project(Pair.left(newChildExprs), Pair.right(newChildExprs));
     }
     relBuilder.union(union.all, union.getInputs().size());
+    // Create top Project fixing nullability of fields
     relBuilder.project(topChildExprs, topChildExprsFields);
+    relBuilder.convert(union.getRowType(), false);
 
     call.transformTo(relBuilder.build());
   }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2720,7 +2720,7 @@ public static enum ConfVars {
         "LLAP delegation token lifetime, in seconds if specified without a unit."),
     LLAP_MANAGEMENT_RPC_PORT("hive.llap.management.rpc.port", 15004,
         "RPC port for LLAP daemon management service."),
-    LLAP_WEB_AUTO_AUTH("hive.llap.auto.auth", true,
+    LLAP_WEB_AUTO_AUTH("hive.llap.auto.auth", false,
         "Whether or not to set Hadoop configs to enable auth in LLAP web app."),
     LLAP_CREATE_TOKEN_LOCALLY("hive.llap.create.token.locally", "hs2",
         new StringSet("true", "hs2", "false"),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -938,7 +938,8 @@ protected FSPaths getDynOutPaths(List<String> row, String lbDirName) throws Hive
           // we cannot proceed and need to tell the hive client that retries won't succeed either
           throw new HiveFatalException(
                ErrorMsg.DYNAMIC_PARTITIONS_TOO_MANY_PER_NODE_ERROR.getErrorCodedMsg()
-               + "Maximum was set to: " + maxPartitions);
+               + "Maximum was set to " + maxPartitions + " partitions per node"
+               + ", number of dynamic partitions on this node: " + valToPaths.size());
         }
 
         if (!conf.getDpSortState().equals(DPSortState.NONE) && prevFsp != null) {

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java
Patch:
@@ -143,7 +143,7 @@ public LlapDaemon(Configuration daemonConf, int numExecutors, long executorMemor
     }
     String hostName = MetricsUtils.getHostName();
     try {
-      daemonId = new DaemonId(UserGroupInformation.getCurrentUser().getUserName(),
+      daemonId = new DaemonId(UserGroupInformation.getCurrentUser().getShortUserName(),
           LlapUtil.generateClusterName(daemonConf), hostName, appName, System.currentTimeMillis());
     } catch (IOException ex) {
       throw new RuntimeException(ex);

File: llap-server/src/test/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorTestHelpers.java
Patch:
@@ -79,7 +79,7 @@ public static QueryInfo createQueryInfo() {
     QueryInfo queryInfo =
         new QueryInfo(queryIdentifier, "fake_app_id_string", "fake_dag_name", 1, "fakeUser",
             new ConcurrentHashMap<String, LlapDaemonProtocolProtos.SourceStateProto>(),
-            new String[0], null, null, null);
+            new String[0], null, "fakeUser", null);
     return queryInfo;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java
Patch:
@@ -39,6 +39,7 @@
 import org.apache.tez.runtime.api.LogicalInput;
 import org.apache.tez.runtime.api.LogicalOutput;
 import org.apache.tez.runtime.api.ProcessorContext;
+import org.apache.tez.runtime.api.TaskFailureType;
 import org.apache.tez.runtime.library.api.KeyValueWriter;
 
 /**
@@ -178,6 +179,8 @@ protected void initializeAndRunProcessor(Map<String, LogicalInput> inputs,
     } finally {
       if (originalThrowable != null && originalThrowable instanceof Error) {
         LOG.error(StringUtils.stringifyException(originalThrowable));
+        getContext().reportFailure(TaskFailureType.FATAL, originalThrowable,
+            "Cannot recover from this error");
         throw new RuntimeException(originalThrowable);
       }
 

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java
Patch:
@@ -42,7 +42,7 @@
 import org.apache.hadoop.hive.ql.io.orc.encoded.OrcBatchKey;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Reader.OrcEncodedColumnBatch;
 import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl;
-import org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory;
+import org.apache.orc.impl.TreeReaderFactory;
 import org.apache.hadoop.hive.ql.io.orc.WriterImpl;
 import org.apache.orc.OrcProto;
 

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcFileMetadata.java
Patch:
@@ -29,11 +29,11 @@
 import org.apache.hadoop.hive.ql.io.SyntheticFileId;
 import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
 import org.apache.hadoop.hive.ql.io.orc.Reader;
-import org.apache.hadoop.hive.ql.io.orc.ReaderImpl.StripeInformationImpl;
 import org.apache.orc.CompressionKind;
 import org.apache.orc.FileMetadata;
 import org.apache.orc.OrcProto;
 import org.apache.orc.StripeInformation;
+import org.apache.orc.impl.ReaderImpl;
 
 /** ORC file metadata. Currently contains some duplicate info due to how different parts
  * of ORC use different info. Ideally we would get rid of protobuf structs in code beyond reading,
@@ -72,7 +72,7 @@ public final class OrcFileMetadata extends LlapCacheableBuffer implements FileMe
   @VisibleForTesting
   public static OrcFileMetadata createDummy(Object fileKey) {
     OrcFileMetadata ofm = new OrcFileMetadata(fileKey);
-    ofm.stripes.add(new StripeInformationImpl(
+    ofm.stripes.add(new ReaderImpl.StripeInformationImpl(
         OrcProto.StripeInformation.getDefaultInstance()));
     ofm.fileStats.add(OrcProto.ColumnStatistics.getDefaultInstance());
     ofm.stripeStats.add(OrcProto.StripeStatistics.newBuilder().addColStats(createStatsDummy()).build());

File: orc/src/java/org/apache/orc/FileFormatException.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hadoop.hive.ql.io;
+package org.apache.orc;
 
 import java.io.IOException;
 

File: orc/src/java/org/apache/orc/Reader.java
Patch:
@@ -334,7 +334,7 @@ public String toString() {
    * @return a new RecordReader
    * @throws IOException
    */
-  RecordReader rowsOptions(Options options) throws IOException;
+  RecordReader rows(Options options) throws IOException;
 
   /**
    * @return List of integers representing version of the file, in order from major to minor.

File: orc/src/java/org/apache/orc/impl/IntegerReader.java
Patch:
@@ -78,4 +78,5 @@ void nextVector(ColumnVector column,
   void nextVector(ColumnVector column,
                   int[] data,
                   int length
-                  ) throws IOException;}
+                  ) throws IOException;
+}

File: orc/src/java/org/apache/orc/impl/SchemaEvolution.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.io.orc;
+package org.apache.orc.impl;
 
 import java.io.IOException;
 import java.util.ArrayList;

File: orc/src/test/org/apache/orc/TestTypeDescription.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hadoop.hive.ql.io.orc;
+package org.apache.orc;
 
 import static org.junit.Assert.assertEquals;
 

File: orc/src/test/org/apache/orc/impl/TestOrcWideTable.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.io.orc;
+package org.apache.orc.impl;
 
 import static org.junit.Assert.assertEquals;
 
@@ -61,4 +61,4 @@ public void testBufferSizeFor25000Col() throws IOException {
     assertEquals(4 * 1024, WriterImpl.getEstimatedBufferSize(512 * 1024 * 1024,
         25000, 256*1024));
   }
-}
\ No newline at end of file
+}

File: orc/src/test/org/apache/orc/impl/TestStreamName.java
Patch:
@@ -16,10 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.io.orc;
+package org.apache.orc.impl;
 
 import org.apache.orc.OrcProto;
-import org.apache.orc.impl.StreamName;
 import org.junit.Test;
 
 import static org.junit.Assert.assertEquals;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastLongToTimestamp.java
Patch:
@@ -39,9 +39,7 @@ public CastLongToTimestamp() {
   }
 
   private void setSeconds(TimestampColumnVector timestampColVector, long[] vector, int elementNum) {
-    TimestampWritable.setTimestampFromLong(
-        timestampColVector.getScratchTimestamp(), vector[elementNum],
-        /* intToTimestampInSeconds */ true);
+    timestampColVector.getScratchTimestamp().setTime(vector[elementNum] * 1000);
     timestampColVector.setFromScratchTimestamp(elementNum);
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecOrcFileDump.java
Patch:
@@ -30,8 +30,8 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.QueryPlan;
 import org.apache.hadoop.hive.ql.exec.FetchTask;
-import org.apache.hadoop.hive.ql.io.FileFormatException;
-import org.apache.hadoop.hive.ql.io.orc.FileDump;
+import org.apache.orc.FileFormatException;
+import org.apache.orc.tools.FileDump;
 import org.apache.hadoop.hive.ql.io.orc.OrcFile;
 import org.apache.hadoop.hive.ql.plan.FetchWork;
 import org.apache.hadoop.hive.ql.session.SessionState;

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java
Patch:
@@ -38,7 +38,7 @@
 import org.apache.orc.DataReader;
 import org.apache.orc.OrcConf;
 import org.apache.orc.impl.OutStream;
-import org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils;
+import org.apache.orc.impl.RecordReaderUtils;
 import org.apache.orc.impl.StreamName;
 import org.apache.orc.StripeInformation;
 import org.apache.orc.impl.BufferChunk;

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.orc.CompressionCodec;
 import org.apache.orc.impl.PositionProvider;
 import org.apache.orc.impl.SettableUncompressedStream;
-import org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory;
+import org.apache.orc.impl.TreeReaderFactory;
 import org.apache.orc.OrcProto;
 
 public class EncodedTreeReaderFactory extends TreeReaderFactory {

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.txn.TxnDbUtil;
 import org.apache.hadoop.hive.ql.io.AcidUtils;
-import org.apache.hadoop.hive.ql.io.orc.FileDump;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService;
@@ -36,7 +35,6 @@
 import org.junit.rules.TestName;
 
 import java.io.File;
-import java.io.FileNotFoundException;
 import java.io.FileOutputStream;
 import java.util.ArrayList;
 import java.util.Arrays;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/udf/TestVectorUDFAdaptor.java
Patch:
@@ -27,8 +27,6 @@
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr;
-import org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor;
-import org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc;
 import org.apache.hadoop.hive.ql.exec.vector.udf.generic.GenericUDFIsNull;
 import org.apache.hadoop.hive.ql.exec.vector.udf.legacy.ConcatTextLongDoubleUDF;
 import org.apache.hadoop.hive.ql.exec.vector.udf.legacy.LongUDF;

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcRecordUpdater.java
Patch:
@@ -40,6 +40,8 @@
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapred.Reporter;
+import org.apache.orc.impl.OrcAcidUtils;
+import org.apache.orc.tools.FileDump;
 import org.junit.Test;
 
 public class TestOrcRecordUpdater {
@@ -115,7 +117,7 @@ public void testWriter() throws Exception {
     assertEquals(5L, updater.getStats().getRowCount());
 
     Path bucketPath = AcidUtils.createFilename(root, options);
-    Path sidePath = OrcRecordUpdater.getSideFile(bucketPath);
+    Path sidePath = OrcAcidUtils.getSideFile(bucketPath);
     DataInputStream side = fs.open(sidePath);
 
     // read the stopping point for the first flush and make sure we only see

File: itests/hive-unit/src/test/java/org/apache/hive/service/cli/session/TestHiveSessionImpl.java
Patch:
@@ -70,7 +70,7 @@ protected synchronized void release(boolean userAccess) {
     Map<String, String> confOverlay = new HashMap<String, String>();
     String hql = "drop table if exists table_not_exists";
     Mockito.when(operationManager.newExecuteStatementOperation(same(session), eq(hql),
-        (Map<String, String>)Mockito.any(), eq(true), eq(0))).thenReturn(operation);
+        (Map<String, String>)Mockito.any(), eq(true), eq(0L))).thenReturn(operation);
 
     try {
 

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveOperationType.java
Patch:
@@ -52,6 +52,7 @@ public enum HiveOperationType {
   ALTERTABLE_PROPERTIES,
   ALTERTABLE_SERIALIZER,
   ALTERTABLE_PARTCOLTYPE,
+  ALTERTABLE_DROPCONSTRAINT,
   ALTERPARTITION_SERIALIZER,
   ALTERTABLE_SERDEPROPERTIES,
   ALTERPARTITION_SERDEPROPERTIES,

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/Operation2Privilege.java
Patch:
@@ -231,6 +231,8 @@ public HivePrivilegeObjectType getObjectType() {
 (OWNER_PRIV_AR,  OWNER_PRIV_AR));
     op2Priv.put(HiveOperationType.TRUNCATETABLE, PrivRequirement.newIOPrivRequirement
 (OWNER_PRIV_AR, OWNER_PRIV_AR));
+    op2Priv.put(HiveOperationType.ALTERTABLE_DROPCONSTRAINT, PrivRequirement.newIOPrivRequirement
+(OWNER_PRIV_AR, OWNER_PRIV_AR));
 
     //table ownership for create/drop/alter index
     op2Priv.put(HiveOperationType.CREATEINDEX, PrivRequirement.newIOPrivRequirement

File: llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java
Patch:
@@ -328,7 +328,7 @@ private void checkAcls() throws Exception {
     if (ix > 0) {
       pathToCheck = pathToCheck.substring(0, ix);
     }
-    List<ACL> acls = zooKeeperClient.usingNamespace(null).getACL().forPath(pathToCheck);
+    List<ACL> acls = zooKeeperClient.getACL().forPath(pathToCheck);
     if (acls == null || acls.isEmpty()) {
       // Can there be no ACLs? There's some access (to get ACLs), so assume it means free for all.
       throw new SecurityException("No ACLs on "  + pathToCheck);

File: llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
Patch:
@@ -267,7 +267,7 @@ public LlapTaskSchedulerService(TaskSchedulerContext taskSchedulerContext, Clock
 
     if (initMetrics) {
       // Initialize the metrics system
-      LlapMetricsSystem.initialize("LlapDaemon");
+      LlapMetricsSystem.initialize("LlapTaskScheduler");
       this.pauseMonitor = new JvmPauseMonitor(conf);
       pauseMonitor.start();
       String displayName = "LlapTaskSchedulerMetrics-" + MetricsUtils.getHostName();

File: llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/metrics/LlapTaskSchedulerMetrics.java
Patch:
@@ -46,9 +46,9 @@
 import org.apache.hadoop.metrics2.source.JvmMetrics;
 
 /**
- * Metrics about the llap daemon task scheduler.
+ * Metrics about the llap task scheduler.
  */
-@Metrics(about = "LlapDaemon Task Scheduler Metrics", context = "scheduler")
+@Metrics(about = "Llap Task Scheduler Metrics", context = "scheduler")
 public class LlapTaskSchedulerMetrics implements MetricsSource {
 
   private final String name;
@@ -99,7 +99,7 @@ public static LlapTaskSchedulerMetrics create(String displayName, String session
   public void getMetrics(MetricsCollector collector, boolean b) {
     MetricsRecordBuilder rb = collector.addRecord(SchedulerMetrics)
         .setContext("scheduler")
-        .tag(ProcessName, MetricsUtils.METRICS_PROCESS_NAME)
+        .tag(ProcessName, "DAGAppMaster")
         .tag(SessionId, sessionId);
     getTaskSchedulerStats(rb);
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java
Patch:
@@ -83,7 +83,7 @@ public static <T> T findSingleOperatorUpstream(Operator<?> start, Class<T> clazz
 
   public static <T> T findSingleOperatorUpstreamJoinAccounted(Operator<?> start, Class<T> clazz) {
     Set<T> found = findOperatorsUpstreamJoinAccounted(start, clazz, new HashSet<T>());
-    return found.size() == 1 ? found.iterator().next(): null;
+    return found.size() >= 1 ? found.iterator().next(): null;
   }
 
   public static <T> Set<T> findOperatorsUpstream(Collection<Operator<?>> starts, Class<T> clazz) {

File: hplsql/src/main/java/org/apache/hive/hplsql/Signal.java
Patch:
@@ -22,7 +22,7 @@
  * Signals and exceptions
  */
 public class Signal {
-  public enum Type { LEAVE_LOOP, LEAVE_ROUTINE, SQLEXCEPTION, NOTFOUND, UNSUPPORTED_OPERATION, USERDEFINED };
+  public enum Type { LEAVE_LOOP, LEAVE_ROUTINE, LEAVE_PROGRAM, SQLEXCEPTION, NOTFOUND, UNSUPPORTED_OPERATION, USERDEFINED };
   Type type;
   String value = "";
   Exception exception = null;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1106,7 +1106,7 @@ public static enum ConfVars {
         "than this threshold, it will try to convert the common join into map join"),
 
 
-    HIVE_SCHEMA_EVOLUTION("hive.exec.schema.evolution", false,
+    HIVE_SCHEMA_EVOLUTION("hive.exec.schema.evolution", true,
         "Use schema evolution to convert self-describing file format's data to the schema desired by the reader."),
 
     HIVE_TRANSACTIONAL_TABLE_SCAN("hive.transactional.table.scan", false,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
Patch:
@@ -787,7 +787,7 @@ public static boolean isNonVectorizedPathUDF(ExprNodeGenericFuncDesc expr, Mode
                 || arg0Type(expr).equals("float"))) {
       return true;
     } else if (gudf instanceof GenericUDFBetween && (mode == Mode.PROJECTION)) {
-      // between has 4 args here, but can be vectorized like this 
+      // between has 4 args here, but can be vectorized like this
       return true;
     }
     return false;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
Patch:
@@ -289,7 +289,7 @@ private boolean isConvertible(FetchData fetch) {
 
   private boolean isConvertible(FetchData fetch, Operator<?> operator, Set<Operator<?>> traversed) {
     if (operator instanceof ReduceSinkOperator || operator instanceof CommonJoinOperator
-        || operator instanceof ScriptOperator || operator instanceof UDTFOperator) {
+        || operator instanceof ScriptOperator) {
       return false;
     }
 

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java
Patch:
@@ -622,7 +622,7 @@ public Void run() throws TransactionError {
 
     private void beginNextTransactionImpl() throws TransactionError {
       state = TxnState.INACTIVE;//clear state from previous txn
-      if ( currentTxnIndex >= txnIds.size() )
+      if ( currentTxnIndex + 1 >= txnIds.size() )
         throw new InvalidTrasactionState("No more transactions available in" +
                 " current batch for end point : " + endPt);
       ++currentTxnIndex;
@@ -874,6 +874,7 @@ private void abortImpl(boolean abortAllRemaining) throws TransactionError, Strea
               currentTxnIndex < txnIds.size(); currentTxnIndex++) {
             msClient.rollbackTxn(txnIds.get(currentTxnIndex));
           }
+          currentTxnIndex--;//since the loop left it == txnId.size()
         }
         else {
           if (getCurrentTxnId() > 0) {

File: hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/TestStreaming.java
Patch:
@@ -1714,6 +1714,8 @@ public void testErrorHandling() throws Exception {
     }
     catch(StreamingIOFailure ex) {
       expectedEx = ex;
+      txnBatch.getCurrentTransactionState();
+      txnBatch.getCurrentTxnId();//test it doesn't throw ArrayIndexOutOfBounds...
     }
     Assert.assertTrue("Wrong exception: " + (expectedEx != null ? expectedEx.getMessage() : "?"),
       expectedEx != null && expectedEx.getMessage().contains("Simulated fault occurred"));

File: metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java
Patch:
@@ -30,6 +30,7 @@
 import java.sql.SQLException;
 import java.sql.Statement;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
@@ -451,7 +452,7 @@ public void cleanEmptyAbortedTxns() throws MetaException {
         if(txnids.size() <= 0) {
           return;
         }
-
+        Collections.sort(txnids);//easier to read logs
         List<String> queries = new ArrayList<String>();
         StringBuilder prefix = new StringBuilder();
         StringBuilder suffix = new StringBuilder();

File: metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -2345,6 +2345,7 @@ private void timeOutLocks(Connection dbConn, long now) {
         deletedLocks += stmt.executeUpdate(query);
       }
       if(deletedLocks > 0) {
+        Collections.sort(extLockIDs);////easier to read logs
         LOG.info("Deleted " + deletedLocks + " ext locks from HIVE_LOCKS due to timeout (vs. " +
             extLockIDs.size() + " found. List: " + extLockIDs + ") maxHeartbeatTime=" + maxHeartbeatTime);
       }
@@ -2444,6 +2445,7 @@ public void performTimeOuts() {
             dbConn.commit();
             numTxnsAborted += batchToAbort.size();
             //todo: add TXNS.COMMENT filed and set it to 'aborted by system due to timeout'
+            Collections.sort(batchToAbort);//easier to read logs
             LOG.info("Aborted the following transactions due to timeout: " + batchToAbort.toString());
           }
           else {

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcStripeMetadata.java
Patch:
@@ -28,9 +28,9 @@
 import org.apache.hadoop.hive.llap.cache.LlapCacheableBuffer;
 import org.apache.hadoop.hive.ql.io.SyntheticFileId;
 import org.apache.hadoop.hive.ql.io.orc.encoded.OrcBatchKey;
+import org.apache.orc.DataReader;
 import org.apache.orc.OrcProto;
 import org.apache.orc.StripeInformation;
-import org.apache.orc.impl.MetadataReader;
 import org.apache.orc.impl.OrcIndex;
 
 public class OrcStripeMetadata extends LlapCacheableBuffer {
@@ -54,7 +54,7 @@ public class OrcStripeMetadata extends LlapCacheableBuffer {
     SIZE_ESTIMATOR = SIZE_ESTIMATORS.get(OrcStripeMetadata.class);
   }
 
-  public OrcStripeMetadata(OrcBatchKey stripeKey, MetadataReader mr, StripeInformation stripe,
+  public OrcStripeMetadata(OrcBatchKey stripeKey, DataReader mr, StripeInformation stripe,
       boolean[] includes, boolean[] sargColumns) throws IOException {
     this.stripeKey = stripeKey;
     OrcProto.StripeFooter footer = mr.readStripeFooter(stripe);
@@ -95,7 +95,7 @@ public boolean hasAllIndexes(boolean[] includes) {
     return true;
   }
 
-  public void loadMissingIndexes(MetadataReader mr, StripeInformation stripe, boolean[] includes,
+  public void loadMissingIndexes(DataReader mr, StripeInformation stripe, boolean[] includes,
       boolean[] sargColumns) throws IOException {
     // TODO: should we save footer to avoid a read here?
     rowIndex = mr.readRowIndex(stripe, null, includes, rowIndex.getRowGroupIndex(),

File: orc/src/java/org/apache/orc/impl/BitFieldReader.java
Patch:
@@ -137,7 +137,7 @@ public void nextVector(LongColumnVector previous,
                          long previousLen) throws IOException {
     previous.isRepeating = true;
     for (int i = 0; i < previousLen; i++) {
-      if (!previous.isNull[i]) {
+      if (previous.noNulls || !previous.isNull[i]) {
         previous.vector[i] = next();
       } else {
         // The default value of null for int types in vectorized
@@ -150,7 +150,8 @@ public void nextVector(LongColumnVector previous,
       // when determining the isRepeating flag.
       if (previous.isRepeating
           && i > 0
-          && ((previous.vector[i - 1] != previous.vector[i]) || (previous.isNull[i - 1] != previous.isNull[i]))) {
+          && ((previous.vector[0] != previous.vector[i]) ||
+          (previous.isNull[0] != previous.isNull[i]))) {
         previous.isRepeating = false;
       }
     }

File: orc/src/java/org/apache/orc/impl/InStream.java
Patch:
@@ -53,6 +53,9 @@ public long getStreamLength() {
     return length;
   }
 
+  @Override
+  public abstract void close();
+
   public static class UncompressedStream extends InStream {
     private List<DiskRange> bytes;
     private long length;
@@ -423,7 +426,6 @@ private static void logEmptySeek(String name) {
 
   /**
    * Create an input stream from a list of buffers.
-   * @param fileName name of the file
    * @param streamName the name of the stream
    * @param buffers the list of ranges of bytes for the stream
    * @param offsets a list of offsets (the same length as input) that must

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java
Patch:
@@ -447,7 +447,8 @@ static Reader.Options createEventOptions(Reader.Options options) {
     this.length = options.getLength();
     this.validTxnList = validTxnList;
 
-    TypeDescription typeDescr = OrcInputFormat.getDesiredRowTypeDescr(conf, /* isAcidRead */ true);
+    TypeDescription typeDescr =
+        OrcInputFormat.getDesiredRowTypeDescr(conf, true, Integer.MAX_VALUE);
 
     objectInspector = OrcRecordUpdater.createEventSchema
         (OrcStruct.createObjectInspector(0, OrcUtils.getOrcTypes(typeDescr)));

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
Patch:
@@ -101,8 +101,6 @@ public class WriterImpl extends org.apache.orc.impl.WriterImpl implements Writer
     }
   }
 
-  private static final long NANOS_PER_MILLI = 1000000;
-
   /**
    * Set the value for a given column value within a batch.
    * @param rowId the row to set

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestTypeDescription.java
Patch:
@@ -51,11 +51,11 @@ public void testJson() {
             .addField("f4", TypeDescription.createDouble())
             .addField("f5", TypeDescription.createBoolean()))
         .addField("f6", TypeDescription.createChar().withMaxLength(100));
-    assertEquals("struct<f1:union<tinyint,decimal(20,10)>,f2:struct<f3:date,f4:double,f5:boolean>,f6:char(100)>",
+    assertEquals("struct<f1:uniontype<tinyint,decimal(20,10)>,f2:struct<f3:date,f4:double,f5:boolean>,f6:char(100)>",
         struct.toString());
     assertEquals(
         "{\"category\": \"struct\", \"id\": 0, \"max\": 8, \"fields\": [\n" +
-            "  \"f1\": {\"category\": \"union\", \"id\": 1, \"max\": 3, \"children\": [\n" +
+            "  \"f1\": {\"category\": \"uniontype\", \"id\": 1, \"max\": 3, \"children\": [\n" +
             "    {\"category\": \"tinyint\", \"id\": 2, \"max\": 2},\n" +
             "    {\"category\": \"decimal\", \"id\": 3, \"max\": 3, \"precision\": 20, \"scale\": 10}]},\n" +
             "  \"f2\": {\"category\": \"struct\", \"id\": 4, \"max\": 7, \"fields\": [\n" +

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java
Patch:
@@ -140,8 +140,8 @@ public void setNullDataValue(int elementNum) {
 
   @Override
   public void ensureSize(int size, boolean preserveData) {
+    super.ensureSize(size, preserveData);
     if (size > vector.length) {
-      super.ensureSize(size, preserveData);
       HiveDecimalWritable[] oldArray = vector;
       vector = new HiveDecimalWritable[size];
       if (preserveData) {

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DoubleColumnVector.java
Patch:
@@ -162,8 +162,8 @@ public void stringifyValue(StringBuilder buffer, int row) {
 
   @Override
   public void ensureSize(int size, boolean preserveData) {
+    super.ensureSize(size, preserveData);
     if (size > vector.length) {
-      super.ensureSize(size, preserveData);
       double[] oldArray = vector;
       vector = new double[size];
       if (preserveData) {

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/LongColumnVector.java
Patch:
@@ -208,8 +208,8 @@ public void stringifyValue(StringBuilder buffer, int row) {
 
   @Override
   public void ensureSize(int size, boolean preserveData) {
+    super.ensureSize(size, preserveData);
     if (size > vector.length) {
-      super.ensureSize(size, preserveData);
       long[] oldArray = vector;
       vector = new long[size];
       if (preserveData) {

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/MultiValuedColumnVector.java
Patch:
@@ -111,8 +111,8 @@ public void flatten(boolean selectedInUse, int[] sel, int size) {
 
   @Override
   public void ensureSize(int size, boolean preserveData) {
+    super.ensureSize(size, preserveData);
     if (size > offsets.length) {
-      super.ensureSize(size, preserveData);
       long[] oldOffsets = offsets;
       offsets = new long[size];
       long oldLengths[] = lengths;

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java
Patch:
@@ -392,4 +392,4 @@ public void stringifyValue(StringBuilder buffer, int row) {
       buffer.append("null");
     }
   }
-}
\ No newline at end of file
+}

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/UnionColumnVector.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector;
 
-import java.util.Arrays;
-
 /**
  * The representation of a vectorized column of struct objects.
  *

File: testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/ExecutionPhase.java
Patch:
@@ -86,8 +86,6 @@ public void execute() throws Throwable {
         isolatedWorkQueue.add(batch);
       }
     }
-    logger.info("ParallelWorkQueueSize={}, IsolatedWorkQueueSize={}", parallelWorkQueue.size(),
-        isolatedWorkQueue.size());
     try {
       int expectedNumHosts = hostExecutors.size();
       initalizeHosts();

File: testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/PrepPhase.java
Patch:
@@ -62,7 +62,6 @@ public void execute() throws Exception {
     // source prep
     start = System.currentTimeMillis();
     File sourcePrepScript = new File(mScratchDir, "source-prep.sh");
-    logger.info("Writing {} from template", sourcePrepScript);
     Templates.writeTemplateResult("source-prep.vm", sourcePrepScript, getTemplateDefaults());
     execLocally("bash " + sourcePrepScript.getPath());
     logger.debug("Deleting " + sourcePrepScript + ": " + sourcePrepScript.delete());

File: testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/conf/Host.java
Patch:
@@ -47,9 +47,6 @@ public int getThreads() {
   public String[] getLocalDirectories() {
     return localDirectories;
   }
-  public String toShortString() {
-    return name;
-  }
   @Override
   public String toString() {
     return "Host [name=" + name + ", user=" + user + ", threads=" + threads

File: beeline/src/java/org/apache/hive/beeline/SeparatedValuesOutputFormat.java
Patch:
@@ -108,7 +108,7 @@ private boolean isQuotingDisabled() {
     }
     String parsedOptionStr = quotingDisabledStr.toLowerCase();
     if (parsedOptionStr.equals("false") || parsedOptionStr.equals("true")) {
-      return Boolean.valueOf(parsedOptionStr);
+      return Boolean.parseBoolean(parsedOptionStr);
     } else {
       beeLine.error("System Property disable.quoting.for.sv is now " + parsedOptionStr
           + " which only accepts boolean value");

File: common/src/java/org/apache/hadoop/hive/common/FileUtils.java
Patch:
@@ -295,7 +295,7 @@ public static String unescapePathName(String path) {
       if (c == '%' && i + 2 < path.length()) {
         int code = -1;
         try {
-          code = Integer.valueOf(path.substring(i + 1, i + 3), 16);
+          code = Integer.parseInt(path.substring(i + 1, i + 3), 16);
         } catch (Exception e) {
           code = -1;
         }

File: common/src/java/org/apache/hadoop/hive/common/type/HiveVarchar.java
Patch:
@@ -58,10 +58,10 @@ public int compareTo(HiveVarchar rhs) {
     return this.getValue().compareTo(rhs.getValue());
   }
 
-  public boolean equals(HiveVarchar rhs) {
+  public boolean equals(Object rhs) {
     if (rhs == this) {
       return true;
     }
-    return this.getValue().equals(rhs.getValue());
+    return this.getValue().equals(((HiveVarchar)rhs).getValue());
   }
 }

File: common/src/java/org/apache/hadoop/hive/conf/Validator.java
Patch:
@@ -199,7 +199,7 @@ class RatioValidator implements Validator {
     @Override
     public String validate(String value) {
       try {
-        float fvalue = Float.valueOf(value);
+        float fvalue = Float.parseFloat(value);
         if (fvalue < 0 || fvalue > 1) {
           return "Invalid ratio " + value + ", which should be in between 0 to 1";
         }

File: common/src/test/org/apache/hadoop/hive/common/type/TestHiveBaseChar.java
Patch:
@@ -80,6 +80,7 @@ public void testStringLength() throws Exception {
         assertEquals(strLen, enforcedString.codePointCount(0, enforcedString.length()));
       }
     }
+    assertNull(HiveBaseChar.enforceMaxLength(null, 0));
   }
 
   public void testGetPaddedValue() {
@@ -96,5 +97,6 @@ public void testGetPaddedValue() {
 
     assertEquals("abc       ", HiveBaseChar.getPaddedValue("abc", 10));
     assertEquals("abc       ", HiveBaseChar.getPaddedValue("abc ", 10));
+    assertNull(HiveBaseChar.getPaddedValue(null, 0));
   }
 }

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDeParameters.java
Patch:
@@ -69,8 +69,8 @@ public class HBaseSerDeParameters {
     // Read configuration parameters
     columnMappingString = tbl.getProperty(HBaseSerDe.HBASE_COLUMNS_MAPPING);
     doColumnRegexMatching =
-        Boolean.valueOf(tbl.getProperty(HBaseSerDe.HBASE_COLUMNS_REGEX_MATCHING, "true"));
-    doColumnPrefixCut = Boolean.valueOf(tbl.getProperty(HBaseSerDe.HBASE_COLUMNS_PREFIX_HIDE, "false"));
+        Boolean.parseBoolean(tbl.getProperty(HBaseSerDe.HBASE_COLUMNS_REGEX_MATCHING, "true"));
+    doColumnPrefixCut = Boolean.parseBoolean(tbl.getProperty(HBaseSerDe.HBASE_COLUMNS_PREFIX_HIDE, "false"));
     // Parse and initialize the HBase columns mapping
     columnMappings = HBaseSerDe.parseColumnsMapping(columnMappingString, doColumnRegexMatching, doColumnPrefixCut);
 
@@ -95,7 +95,7 @@ public class HBaseSerDeParameters {
     }
 
     this.serdeParams = new LazySerDeParameters(job, tbl, serdeName);
-    this.putTimestamp = Long.valueOf(tbl.getProperty(HBaseSerDe.HBASE_PUT_TIMESTAMP, "-1"));
+    this.putTimestamp = Long.parseLong(tbl.getProperty(HBaseSerDe.HBASE_PUT_TIMESTAMP, "-1"));
 
     columnMappings.setHiveColumnDescription(serdeName, serdeParams.getColumnNames(),
         serdeParams.getColumnTypes());

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseInputFormatUtil.java
Patch:
@@ -124,15 +124,15 @@ public static Scan getScan(JobConf jobConf) throws IOException {
 
     String scanCache = jobConf.get(HBaseSerDe.HBASE_SCAN_CACHE);
     if (scanCache != null) {
-      scan.setCaching(Integer.valueOf(scanCache));
+      scan.setCaching(Integer.parseInt(scanCache));
     }
     String scanCacheBlocks = jobConf.get(HBaseSerDe.HBASE_SCAN_CACHEBLOCKS);
     if (scanCacheBlocks != null) {
-      scan.setCacheBlocks(Boolean.valueOf(scanCacheBlocks));
+      scan.setCacheBlocks(Boolean.parseBoolean(scanCacheBlocks));
     }
     String scanBatch = jobConf.get(HBaseSerDe.HBASE_SCAN_BATCH);
     if (scanBatch != null) {
-      scan.setBatch(Integer.valueOf(scanBatch));
+      scan.setBatch(Integer.parseInt(scanBatch));
     }
     return scan;
   }

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java
Patch:
@@ -107,7 +107,7 @@ public FileOutputCommitterContainer(JobContext context,
     this.partitionsDiscovered = !dynamicPartitioningUsed;
     cachedStorageHandler = HCatUtil.getStorageHandler(context.getConfiguration(), jobInfo.getTableInfo().getStorerInfo());
     Table table = new Table(jobInfo.getTableInfo().getTable());
-    if (dynamicPartitioningUsed && Boolean.valueOf((String)table.getProperty("EXTERNAL"))
+    if (dynamicPartitioningUsed && Boolean.parseBoolean((String)table.getProperty("EXTERNAL"))
         && jobInfo.getCustomDynamicPath() != null
         && jobInfo.getCustomDynamicPath().length() > 0) {
       customDynamicLocationUsed = true;
@@ -355,7 +355,7 @@ private Partition constructPartition(
     if (customDynamicLocationUsed) {
       partPath = new Path(dynPartPath);
     } else if (!dynamicPartitioningUsed
-         && Boolean.valueOf((String)table.getProperty("EXTERNAL"))
+         && Boolean.parseBoolean((String)table.getProperty("EXTERNAL"))
          && jobInfo.getLocation() != null && jobInfo.getLocation().length() > 0) {
       // Now, we need to de-scratchify this location - i.e., get rid of any
       // _SCRATCH[\d].?[\d]+ from the location.

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FosterStorageHandler.java
Patch:
@@ -157,7 +157,7 @@ public void configureOutputJobProperties(TableDesc tableDesc,
       // we create a temp dir for the associated write job
       if (dynHash != null) {
         // if external table and custom root specified, update the parent path
-        if (Boolean.valueOf((String)tableDesc.getProperties().get("EXTERNAL"))
+        if (Boolean.parseBoolean((String)tableDesc.getProperties().get("EXTERNAL"))
             && jobInfo.getCustomDynamicRoot() != null
             && jobInfo.getCustomDynamicRoot().length() > 0) {
           parentPath = new Path(parentPath, jobInfo.getCustomDynamicRoot()).toString();
@@ -170,14 +170,14 @@ public void configureOutputJobProperties(TableDesc tableDesc,
       String outputLocation;
 
       if ((dynHash != null)
-          && Boolean.valueOf((String)tableDesc.getProperties().get("EXTERNAL"))
+          && Boolean.parseBoolean((String)tableDesc.getProperties().get("EXTERNAL"))
           && jobInfo.getCustomDynamicPath() != null
           && jobInfo.getCustomDynamicPath().length() > 0) {
         // dynamic partitioning with custom path; resolve the custom path
         // using partition column values
         outputLocation = HCatFileUtil.resolveCustomPath(jobInfo, null, true);
       } else if ((dynHash == null)
-           && Boolean.valueOf((String)tableDesc.getProperties().get("EXTERNAL"))
+           && Boolean.parseBoolean((String)tableDesc.getProperties().get("EXTERNAL"))
            && jobInfo.getLocation() != null && jobInfo.getLocation().length() > 0) {
         // honor custom location for external table apart from what metadata specifies
         outputLocation = jobInfo.getLocation();

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/authorization/plugin/TestHiveAuthorizerCheckInvocation.java
Patch:
@@ -373,7 +373,7 @@ private Pair<List<HivePrivilegeObject>, List<HivePrivilegeObject>> getHivePrivil
 
     verify(mockedAuthorizer).checkPrivileges(any(HiveOperationType.class),
         inputsCapturer.capture(), outputsCapturer.capture(),
-        any(HiveAuthzContext.class));
+        any(QueryContext.class));
 
     return new ImmutablePair(inputsCapturer.getValue(), outputsCapturer.getValue());
   }

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/authorization/plugin/TestHiveAuthorizerShowFilters.java
Patch:
@@ -77,7 +77,7 @@ protected static class MockedHiveAuthorizerFactory implements HiveAuthorizerFact
     protected abstract class AuthorizerWithFilterCmdImpl implements HiveAuthorizer {
       @Override
       public List<HivePrivilegeObject> filterListCmdObjects(List<HivePrivilegeObject> listObjs,
-          HiveAuthzContext context) throws HiveAuthzPluginException, HiveAccessControlException {
+          QueryContext context) throws HiveAuthzPluginException, HiveAccessControlException {
         // capture arguments in static
         filterArguments = listObjs;
         // return static variable with results, if it is set to some set of
@@ -101,7 +101,7 @@ public HiveAuthorizer createHiveAuthorizer(HiveMetastoreClientFactory metastoreC
       try {
         Mockito.when(
             mockedAuthorizer.filterListCmdObjects((List<HivePrivilegeObject>) any(),
-                (HiveAuthzContext) any())).thenCallRealMethod();
+                (QueryContext) any())).thenCallRealMethod();
       } catch (Exception e) {
         org.junit.Assert.fail("Caught exception " + e);
       }

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/authorization/TestJdbcMetadataApiAuth.java
Patch:
@@ -39,7 +39,7 @@
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerFactory;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl;
-import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext;
+import org.apache.hadoop.hive.ql.security.authorization.plugin.QueryContext;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveMetastoreClientFactory;
@@ -76,7 +76,7 @@ public TestAuthValidator(HiveMetastoreClientFactory metastoreClientFactory, Hive
 
     @Override
     public void checkPrivileges(HiveOperationType hiveOpType, List<HivePrivilegeObject> inputHObjs,
-        List<HivePrivilegeObject> outputHObjs, HiveAuthzContext context)
+        List<HivePrivilegeObject> outputHObjs, QueryContext context)
         throws HiveAuthzPluginException, HiveAccessControlException {
       if (!allowActions) {
         throw new HiveAccessControlException(DENIED_ERR);

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/StartMiniHS2Cluster.java
Patch:
@@ -30,7 +30,7 @@ public void testRunCluster() throws Exception {
 
     MiniClusterType clusterType = MiniClusterType.valueOf(System.getProperty("miniHS2.clusterType", "MR").toUpperCase());
     String confFilesProperty = System.getProperty("miniHS2.conf", "../../data/conf/hive-site.xml");
-    boolean usePortsFromConf = Boolean.valueOf(System.getProperty("miniHS2.usePortsFromConf", "false"));
+    boolean usePortsFromConf = Boolean.parseBoolean(System.getProperty("miniHS2.usePortsFromConf", "false"));
 
     // Load conf files
     String[] confFiles = confFilesProperty.split(",");

File: itests/hive-unit/src/test/java/org/apache/hive/service/cli/session/TestQueryDisplay.java
Patch:
@@ -133,8 +133,10 @@ private void verifyDDL(SQLOperationDisplay display, String stmt, String handle,
     if (finished) {
       Assert.assertTrue(display.getEndTime() > 0 && display.getEndTime() >= display.getBeginTime()
         && display.getEndTime() <= System.currentTimeMillis());
+      Assert.assertTrue(display.getRuntime() > 0);
     } else {
       Assert.assertNull(display.getEndTime());
+      //For runtime, query may have finished.
     }
 
     QueryDisplay qDisplay1 = display.getQueryDisplay();

File: llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceRegistry.java
Patch:
@@ -37,9 +37,11 @@ public interface ServiceRegistry {
   /**
    * Register the current instance - the implementation takes care of the endpoints to register.
    *
+   * @return self identifying name
+   * 
    * @throws IOException
    */
-  public void register() throws IOException;
+  public String register() throws IOException;
 
   /**
    * Remove the current registration cleanly (implementation defined cleanup)

File: llap-server/src/java/org/apache/hadoop/hive/llap/metrics/LlapDaemonCacheMetrics.java
Patch:
@@ -45,7 +45,7 @@
 /**
  * Llap daemon cache metrics source.
  */
-@Metrics(about = "LlapDaemon Cache Metrics", context = MetricsUtils.METRICS_CONTEXT)
+@Metrics(about = "LlapDaemon Cache Metrics", context = "cache")
 public class LlapDaemonCacheMetrics implements MetricsSource {
   final String name;
   private String sessionId;
@@ -127,7 +127,7 @@ public long getCacheHitBytes() {
   @Override
   public void getMetrics(MetricsCollector collector, boolean b) {
     MetricsRecordBuilder rb = collector.addRecord(CacheMetrics)
-        .setContext(MetricsUtils.METRICS_CONTEXT)
+        .setContext("cache")
         .tag(ProcessName, MetricsUtils.METRICS_PROCESS_NAME)
         .tag(SessionId, sessionId);
     getCacheStats(rb);

File: llap-server/src/java/org/apache/hadoop/hive/llap/metrics/LlapDaemonExecutorMetrics.java
Patch:
@@ -52,7 +52,7 @@
 /**
  * Metrics about the llap daemon executors.
  */
-@Metrics(about = "LlapDaemon Executor Metrics", context = MetricsUtils.METRICS_CONTEXT)
+@Metrics(about = "LlapDaemon Executor Metrics", context = "executors")
 public class LlapDaemonExecutorMetrics implements MetricsSource {
 
   private final String name;
@@ -117,7 +117,7 @@ public static LlapDaemonExecutorMetrics create(String displayName, String sessio
   @Override
   public void getMetrics(MetricsCollector collector, boolean b) {
     MetricsRecordBuilder rb = collector.addRecord(ExecutorMetrics)
-        .setContext(MetricsUtils.METRICS_CONTEXT)
+        .setContext("executors")
         .tag(ProcessName, MetricsUtils.METRICS_PROCESS_NAME)
         .tag(SessionId, sessionId);
     getExecutorStats(rb);

File: llap-server/src/java/org/apache/hadoop/hive/llap/metrics/LlapDaemonQueueMetrics.java
Patch:
@@ -38,7 +38,7 @@
 /**
  *
  */
-@Metrics(about = "LlapDaemon Queue Metrics", context = MetricsUtils.METRICS_CONTEXT)
+@Metrics(about = "LlapDaemon Queue Metrics", context = "queue")
 public class LlapDaemonQueueMetrics implements MetricsSource {
   private final String name;
   private final String sessionId;
@@ -78,7 +78,7 @@ public static LlapDaemonQueueMetrics create(String displayName, String sessionId
   @Override
   public void getMetrics(MetricsCollector collector, boolean b) {
     MetricsRecordBuilder rb = collector.addRecord(QueueMetrics)
-        .setContext(MetricsUtils.METRICS_CONTEXT)
+        .setContext("queue")
         .tag(ProcessName, MetricsUtils.METRICS_PROCESS_NAME)
         .tag(SessionId, sessionId);
     getQueueStats(rb);

File: llap-server/src/java/org/apache/hadoop/hive/llap/metrics/MetricsUtils.java
Patch:
@@ -26,7 +26,6 @@
  */
 public class MetricsUtils {
   private static final String LOCALHOST = "localhost";
-  public static final String METRICS_CONTEXT = "llap";
   public static final String METRICS_PROCESS_NAME = "LlapDaemon";
 
 

File: llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java
Patch:
@@ -658,7 +658,7 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt)
       final List<String> keepAliveList = q.get("keepAlive");
       boolean keepAliveParam = false;
       if (keepAliveList != null && keepAliveList.size() == 1) {
-        keepAliveParam = Boolean.valueOf(keepAliveList.get(0));
+        keepAliveParam = Boolean.parseBoolean(keepAliveList.get(0));
         if (LOG.isDebugEnabled()) {
           LOG.debug("KeepAliveParam : " + keepAliveList
             + " : " + keepAliveParam);

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreSchemaInfo.java
Patch:
@@ -193,8 +193,8 @@ public static boolean isVersionCompatible(String hiveVersion, String dbVersion)
     }
 
     for (int i = 0; i < dbVerParts.length; i++) {
-      Integer dbVerPart = Integer.valueOf(dbVerParts[i]);
-      Integer hiveVerPart = Integer.valueOf(hiveVerParts[i]);
+      int dbVerPart = Integer.parseInt(dbVerParts[i]);
+      int hiveVerPart = Integer.parseInt(hiveVerParts[i]);
       if (dbVerPart > hiveVerPart) {
         return true;
       } else if (dbVerPart < hiveVerPart) {

File: metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseImport.java
Patch:
@@ -206,7 +206,7 @@ private int init(String... args) throws ParseException {
       doAll = true;
     }
     if (cli.hasOption('b')) {
-      batchSize = Integer.valueOf(cli.getOptionValue('b'));
+      batchSize = Integer.parseInt(cli.getOptionValue('b'));
     }
     if (cli.hasOption('d')) {
       hasCmd = true;
@@ -217,7 +217,7 @@ private int init(String... args) throws ParseException {
       functionsToImport = Arrays.asList(cli.getOptionValues('f'));
     }
     if (cli.hasOption('p')) {
-      parallel = Integer.valueOf(cli.getOptionValue('p'));
+      parallel = Integer.parseInt(cli.getOptionValue('p'));
     }
     if (cli.hasOption('r')) {
       hasCmd = true;

File: orc/src/java/org/apache/orc/impl/BitFieldReader.java
Patch:
@@ -137,7 +137,7 @@ public void nextVector(LongColumnVector previous,
                          long previousLen) throws IOException {
     previous.isRepeating = true;
     for (int i = 0; i < previousLen; i++) {
-      if (!previous.isNull[i]) {
+      if (previous.noNulls || !previous.isNull[i]) {
         previous.vector[i] = next();
       } else {
         // The default value of null for int types in vectorized
@@ -150,7 +150,8 @@ public void nextVector(LongColumnVector previous,
       // when determining the isRepeating flag.
       if (previous.isRepeating
           && i > 0
-          && ((previous.vector[i - 1] != previous.vector[i]) || (previous.isNull[i - 1] != previous.isNull[i]))) {
+          && ((previous.vector[0] != previous.vector[i]) ||
+          (previous.isNull[0] != previous.isNull[i]))) {
         previous.isRepeating = false;
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/AppMasterEventOperator.java
Patch:
@@ -20,9 +20,7 @@
 
 import java.io.IOException;
 import java.nio.ByteBuffer;
-import java.util.Collection;
 import java.util.Collections;
-import java.util.concurrent.Future;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -151,7 +149,7 @@ public OperatorType getType() {
    */
   @Override
   public String getName() {
-    return getOperatorName();
+    return AppMasterEventOperator.getOperatorName();
   }
 
   static public String getOperatorName() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -542,7 +542,7 @@ protected void createBucketFiles(FSPaths fsp) throws HiveException {
           int numReducers = totalFiles / numFiles;
 
           if (numReducers > 1) {
-            int currReducer = Integer.valueOf(Utilities.getTaskIdFromFilename(Utilities
+            int currReducer = Integer.parseInt(Utilities.getTaskIdFromFilename(Utilities
                 .getTaskId(hconf)));
 
             int reducerIdx = prtner.getPartition(key, null, numReducers);
@@ -626,7 +626,7 @@ protected void createBucketForFileIdx(FSPaths fsp, int filesIdx)
         // Only set up the updater for insert.  For update and delete we don't know unitl we see
         // the row.
         ObjectInspector inspector = bDynParts ? subSetOI : outputObjInspector;
-        int acidBucketNum = Integer.valueOf(Utilities.getTaskIdFromFilename(taskId));
+        int acidBucketNum = Integer.parseInt(Utilities.getTaskIdFromFilename(taskId));
         fsp.updaters[filesIdx] = HiveFileFormatUtils.getAcidRecordUpdater(jc, conf.getTableInfo(),
             acidBucketNum, conf, fsp.outPaths[filesIdx], inspector, reporter, -1);
       }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
Patch:
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import java.io.Serializable;
-import java.util.Collection;
-import java.util.concurrent.Future;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -134,7 +132,7 @@ public void process(Object row, int tag) throws HiveException {
    */
   @Override
   public String getName() {
-    return getOperatorName();
+    return FilterOperator.getOperatorName();
   }
 
   static public String getOperatorName() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java
Patch:
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import java.io.Serializable;
-import java.util.Collection;
-import java.util.concurrent.Future;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.CompilationOpContext;
@@ -55,7 +53,7 @@ public boolean acceptLimitPushdown() {
    */
   @Override
   public String getName() {
-    return getOperatorName();
+    return ForwardOperator.getOperatorName();
   }
 
   static public String getOperatorName() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
Patch:
@@ -24,14 +24,12 @@
 import java.sql.Timestamp;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collection;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.concurrent.Future;
 
 import javolution.util.FastBitSet;
 
@@ -1131,7 +1129,7 @@ public List<String> genColLists(
    */
   @Override
   public String getName() {
-    return getOperatorName();
+    return GroupByOperator.getOperatorName();
   }
 
   static public String getOperatorName() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableDummyOperator.java
Patch:
@@ -18,8 +18,6 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import java.io.Serializable;
-import java.util.Collection;
-import java.util.concurrent.Future;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.CompilationOpContext;
@@ -67,7 +65,7 @@ public void closeOp(boolean abort) throws HiveException {
 
   @Override
   public String getName() {
-    return getOperatorName();
+    return HashTableDummyOperator.getOperatorName();
   }
 
   static public String getOperatorName() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/LateralViewJoinOperator.java
Patch:
@@ -19,9 +19,7 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import java.util.ArrayList;
-import java.util.Collection;
 import java.util.List;
-import java.util.concurrent.Future;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.CompilationOpContext;
@@ -149,7 +147,7 @@ public void process(Object row, int tag) throws HiveException {
 
   @Override
   public String getName() {
-    return getOperatorName();
+    return LateralViewJoinOperator.getOperatorName();
   }
 
   static public String getOperatorName() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
Patch:
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import java.io.Serializable;
-import java.util.Collection;
-import java.util.concurrent.Future;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.CompilationOpContext;
@@ -73,7 +71,7 @@ public void process(Object row, int tag) throws HiveException {
 
   @Override
   public String getName() {
-    return getOperatorName();
+    return LimitOperator.getOperatorName();
   }
 
   static public String getOperatorName() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -527,6 +527,7 @@ public int execute(DriverContext driverContext) {
     } catch (Exception e) {
       console.printError("Failed with exception " + e.getMessage(), "\n"
           + StringUtils.stringifyException(e));
+      setException(e);
       return (1);
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java
Patch:
@@ -19,11 +19,9 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import java.io.Serializable;
-import java.util.Collection;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Stack;
-import java.util.concurrent.Future;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.CompilationOpContext;
@@ -184,7 +182,7 @@ protected void setupKeysWrapper(ObjectInspector inputOI) throws HiveException {
    */
   @Override
   public String getName() {
-    return getOperatorName();
+    return PTFOperator.getOperatorName();
   }
 
   static public String getOperatorName() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
Patch:
@@ -19,9 +19,7 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import java.io.Serializable;
-import java.util.Collection;
 import java.util.List;
-import java.util.concurrent.Future;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -102,7 +100,7 @@ public void process(Object row, int tag) throws HiveException {
    */
   @Override
   public String getName() {
-    return getOperatorName();
+    return SelectOperator.getOperatorName();
   }
 
   static public String getOperatorName() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
Patch:
@@ -277,7 +277,7 @@ public void closeOp(boolean abort) throws HiveException {
    **/
   @Override
   public String getName() {
-    return getOperatorName();
+    return TableScanOperator.getOperatorName();
   }
 
   static public String getOperatorName() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TaskRunner.java
Patch:
@@ -32,7 +32,6 @@
  **/
 
 public class TaskRunner extends Thread {
-
   protected Task<? extends Serializable> tsk;
   protected TaskResult result;
   protected SessionState ss;
@@ -103,7 +102,7 @@ public void runSequential() {
       if (tsk.getException() == null) {
         tsk.setException(t);
       }
-      t.printStackTrace();
+      LOG.error("Error in executeTask", t);
     }
     result.setExitVal(exitVal, tsk.getException());
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java
Patch:
@@ -156,7 +156,7 @@ public synchronized void process(Object row, int tag) throws HiveException {
    */
   @Override
   public String getName() {
-    return getOperatorName();
+    return UnionOperator.getOperatorName();
   }
 
   static public String getOperatorName() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
Patch:
@@ -301,8 +301,7 @@ public int execute(DriverContext driverContext) {
 
       return exitVal;
     } catch (Exception e) {
-      e.printStackTrace();
-      LOG.error("Exception: " + e.getMessage());
+      LOG.error("Got exception", e);
       return (1);
     } finally {
       try {
@@ -313,7 +312,7 @@ public int execute(DriverContext driverContext) {
         }
 
       } catch (Exception e) {
-        LOG.error("Exception: " + e.getMessage());
+        LOG.error("Exception: ", e);
       }
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSparkHashTableSinkOperator.java
Patch:
@@ -26,8 +26,7 @@
 import org.apache.hadoop.hive.ql.plan.SparkHashTableSinkDesc;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 
-import java.util.Collection;
-import java.util.concurrent.Future;
+import com.google.common.annotations.VisibleForTesting;
 
 /**
  * Vectorized version of SparkHashTableSinkOperator
@@ -52,7 +51,8 @@ public class VectorSparkHashTableSinkOperator extends SparkHashTableSinkOperator
   protected transient Object[] singleRow;
 
   /** Kryo ctor. */
-  protected VectorSparkHashTableSinkOperator() {
+  @VisibleForTesting
+  public VectorSparkHashTableSinkOperator() {
     super();
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
Patch:
@@ -101,8 +101,6 @@ public class WriterImpl extends org.apache.orc.impl.WriterImpl implements Writer
     }
   }
 
-  private static final long NANOS_PER_MILLI = 1000000;
-
   /**
    * Set the value for a given column value within a batch.
    * @param rowId the row to set

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
Patch:
@@ -165,7 +165,8 @@ protected TimestampWritable convert(Binary binary) {
           Map<String, String> metadata = parent.getMetadata();
           //Current Hive parquet timestamp implementation stores it in UTC, but other components do not do that.
           //If this file written by current Hive implementation itself, we need to do the reverse conversion, else skip the conversion.
-          boolean skipConversion = Boolean.valueOf(metadata.get(HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION.varname));
+          boolean skipConversion = Boolean.parseBoolean(
+              metadata.get(HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION.varname));
           Timestamp ts = NanoTimeUtils.getTimestamp(nt, skipConversion);
           return new TimestampWritable(ts);
         }

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveStructConverter.java
Patch:
@@ -118,7 +118,7 @@ private TypeInfo getFieldTypeIgnoreCase(TypeInfo hiveTypeInfo, String fieldName,
 
   private TypeInfo getStructFieldTypeInfo(String field, int fieldIndex) {
     String fieldLowerCase = field.toLowerCase();
-    if (Boolean.valueOf(getMetadata().get(DataWritableReadSupport.PARQUET_COLUMN_INDEX_ACCESS))
+    if (Boolean.parseBoolean(getMetadata().get(DataWritableReadSupport.PARQUET_COLUMN_INDEX_ACCESS))
         && fieldIndex < hiveFieldNames.size()) {
       return hiveFieldTypeInfos.get(fieldIndex);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/ParquetRecordWriterWrapper.java
Patch:
@@ -77,15 +77,15 @@ private void initializeSerProperties(JobContext job, Properties tableProperties)
     Configuration conf = ContextUtil.getConfiguration(job);
     if (blockSize != null && !blockSize.isEmpty()) {
       LOG.debug("get override parquet.block.size property via tblproperties");
-      conf.setInt(ParquetOutputFormat.BLOCK_SIZE, Integer.valueOf(blockSize));
+      conf.setInt(ParquetOutputFormat.BLOCK_SIZE, Integer.parseInt(blockSize));
     }
 
     String enableDictionaryPage =
       tableProperties.getProperty(ParquetOutputFormat.ENABLE_DICTIONARY);
     if (enableDictionaryPage != null && !enableDictionaryPage.isEmpty()) {
       LOG.debug("get override parquet.enable.dictionary property via tblproperties");
       conf.setBoolean(ParquetOutputFormat.ENABLE_DICTIONARY,
-        Boolean.valueOf(enableDictionaryPage));
+        Boolean.parseBoolean(enableDictionaryPage));
     }
 
     String compressionName = tableProperties.getProperty(ParquetOutputFormat.COMPRESSION);

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -2697,7 +2697,7 @@ private static boolean isSubDir(Path srcf, Path destf, FileSystem srcFs, FileSys
     String fullF1 = getQualifiedPathWithoutSchemeAndAuthority(srcf, srcFs);
     String fullF2 = getQualifiedPathWithoutSchemeAndAuthority(destf, destFs);
 
-    boolean isInTest = Boolean.valueOf(HiveConf.getBoolVar(srcFs.getConf(), ConfVars.HIVE_IN_TEST));
+    boolean isInTest = HiveConf.getBoolVar(srcFs.getConf(), ConfVars.HIVE_IN_TEST);
     // In the automation, the data warehouse is the local file system based.
     LOG.debug("The source path is " + fullF1 + " and the destination path is " + fullF2);
     if (isInTest) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/IndexUtils.java
Patch:
@@ -238,6 +238,8 @@ public static Task<?> createRootTask(
     indexMetaChangeTsk.setWork(indexMetaChange);
     rootTask.addDependentTask(indexMetaChangeTsk);
 
+    driver.destroy();
+
     return rootTask;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/SizeBasedBigTableSelectorForAutoSMJ.java
Patch:
@@ -57,7 +57,7 @@ private long getSize(HiveConf conf, String size, Path path) {
     // If the size is present in the metastore, use it
     if (size != null) {
       try {
-        return Long.valueOf(size);
+        return Long.parseLong(size);
       } catch (NumberFormatException e) {
         return -1;
       }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -126,6 +126,7 @@
 import org.apache.hadoop.hive.ql.udf.UDFDayOfMonth;
 import org.apache.hadoop.hive.ql.udf.UDFDegrees;
 import org.apache.hadoop.hive.ql.udf.UDFExp;
+import org.apache.hadoop.hive.ql.udf.UDFFromUnixTime;
 import org.apache.hadoop.hive.ql.udf.UDFHex;
 import org.apache.hadoop.hive.ql.udf.UDFHour;
 import org.apache.hadoop.hive.ql.udf.UDFLength;
@@ -247,6 +248,7 @@ public Vectorizer() {
     supportedGenericUDFs.add(UDFSecond.class);
     supportedGenericUDFs.add(UDFWeekOfYear.class);
     supportedGenericUDFs.add(GenericUDFToUnixTimeStamp.class);
+    supportedGenericUDFs.add(UDFFromUnixTime.class);
 
     supportedGenericUDFs.add(GenericUDFDateAdd.class);
     supportedGenericUDFs.add(GenericUDFDateSub.class);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java
Patch:
@@ -92,7 +92,7 @@ public ArrayList<Node> getChildren() {
    */
   @Override
   public String getName() {
-    return (Integer.valueOf(super.getToken().getType())).toString();
+    return String.valueOf(super.getToken().getType());
   }
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -438,8 +438,6 @@ public static void readProps(
     }
   }
 
-  private static final int[] multiplier = new int[] {1000, 100, 10, 1};
-
   @SuppressWarnings("nls")
   public static String unescapeSQLString(String b) {
     Character enclosure = null;
@@ -469,7 +467,7 @@ public static String unescapeSQLString(String b) {
         int base = i + 2;
         for (int j = 0; j < 4; j++) {
           int digit = Character.digit(b.charAt(j + base), 16);
-          code += digit * multiplier[j];
+          code = (code << 4) + digit;
         }
         sb.append((char)code);
         i += 5;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -1702,10 +1702,10 @@ private void analyzeAlterTableClusterSort(ASTNode ast, String tableName,
       List<Order> sortCols = new ArrayList<Order>();
       int numBuckets = -1;
       if (buckets.getChildCount() == 2) {
-        numBuckets = (Integer.valueOf(buckets.getChild(1).getText())).intValue();
+        numBuckets = Integer.parseInt(buckets.getChild(1).getText());
       } else {
         sortCols = getColumnNamesOrder((ASTNode) buckets.getChild(1));
-        numBuckets = (Integer.valueOf(buckets.getChild(2).getText())).intValue();
+        numBuckets = Integer.parseInt(buckets.getChild(2).getText());
       }
       if (numBuckets <= 0) {
         throw new SemanticException(ErrorMsg.INVALID_BUCKET_NUMBER.getMsg());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSpec.java
Patch:
@@ -126,7 +126,8 @@ public ReplicationSpec(Function<String, String> keyFetcher) {
     }
     this.eventId = keyFetcher.apply(ReplicationSpec.KEY.EVENT_ID.toString());
     this.currStateId = keyFetcher.apply(ReplicationSpec.KEY.CURR_STATE_ID.toString());
-    this.isNoop = Boolean.valueOf(keyFetcher.apply(ReplicationSpec.KEY.NOOP.toString())).booleanValue();
+    this.isNoop = Boolean.parseBoolean(
+        keyFetcher.apply(ReplicationSpec.KEY.NOOP.toString()));
   }
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TableSample.java
Patch:
@@ -68,8 +68,8 @@ public class TableSample {
    *          The list of expressions in the ON part of the TABLESAMPLE clause
    */
   public TableSample(String num, String den, ArrayList<ASTNode> exprs) {
-    numerator = Integer.valueOf(num).intValue();
-    denominator = Integer.valueOf(den).intValue();
+    numerator = Integer.parseInt(num);
+    denominator = Integer.parseInt(den);
     this.exprs = exprs;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java
Patch:
@@ -440,7 +440,7 @@ public static SparkEdgeProperty getEdgeProperty(ReduceSinkOperator reduceSink,
     if (fso != null) {
       String bucketCount = fso.getConf().getTableInfo().getProperties().getProperty(
           hive_metastoreConstants.BUCKET_COUNT);
-      if (bucketCount != null && Integer.valueOf(bucketCount) > 1) {
+      if (bucketCount != null && Integer.parseInt(bucketCount) > 1) {
         edgeProperty.setMRShuffle();
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
Patch:
@@ -574,7 +574,7 @@ public static List<FieldSchema> getFieldSchemasFromColumnInfo(
     List<FieldSchema> schemas = new ArrayList<FieldSchema>(cols.size());
     for (int i = 0; i < cols.size(); i++) {
       String name = cols.get(i).getInternalName();
-      if (name.equals(Integer.valueOf(i).toString())) {
+      if (name.equals(String.valueOf(i))) {
         name = fieldPrefix + name;
       }
       schemas.add(MetaStoreUtils.getFieldSchemaFromTypeInfo(name, cols.get(i)

File: ql/src/java/org/apache/hadoop/hive/ql/processors/CommandUtil.java
Patch:
@@ -25,7 +25,7 @@
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException;
-import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext;
+import org.apache.hadoop.hive.ql.security.authorization.plugin.QueryContext;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject;
@@ -80,9 +80,9 @@ static CommandProcessorResponse authorizeCommand(SessionState ss, HiveOperationT
   static void authorizeCommandThrowEx(SessionState ss, HiveOperationType type,
       List<String> command) throws HiveAuthzPluginException, HiveAccessControlException {
     HivePrivilegeObject commandObj = HivePrivilegeObject.createHivePrivilegeObject(command);
-    HiveAuthzContext.Builder ctxBuilder = new HiveAuthzContext.Builder();
+    QueryContext.Builder ctxBuilder = new QueryContext.Builder();
     ctxBuilder.setCommandString(Joiner.on(' ').join(command));
-    ctxBuilder.setUserIpAddress(ss.getUserIpAddress());
+    ctxBuilder.setForwardedAddresses(ss.getForwardedAddresses());
     ss.getAuthorizerV2().checkPrivileges(type, Arrays.asList(commandObj), null, ctxBuilder.build());
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/security/HiveAuthenticationProvider.java
Patch:
@@ -32,6 +32,8 @@ public interface HiveAuthenticationProvider extends Configurable{
 
   public String getUserName();
 
+  public String getUserIpAddress();
+
   public List<String> getGroupNames();
 
   public void destroy() throws HiveException;

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/AuthorizationMetaStoreFilterHook.java
Patch:
@@ -73,8 +73,8 @@ private List<String> getTableNames(List<HivePrivilegeObject> filteredObjects) {
 
   private List<HivePrivilegeObject> getFilteredObjects(List<HivePrivilegeObject> listObjs) throws MetaException {
     SessionState ss = SessionState.get();
-    HiveAuthzContext.Builder authzContextBuilder = new HiveAuthzContext.Builder();
-    authzContextBuilder.setUserIpAddress(ss.getUserIpAddress());
+    QueryContext.Builder authzContextBuilder = new QueryContext.Builder();
+    authzContextBuilder.setForwardedAddresses(ss.getForwardedAddresses());
     try {
       return ss.getAuthorizerV2().filterListCmdObjects(listObjs, authzContextBuilder.build());
     } catch (HiveAuthzPluginException e) {

File: ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsAggregator.java
Patch:
@@ -94,7 +94,7 @@ public String aggregateStats(String partID, String statType) {
       if (null == statVal) { // partition was found, but was empty.
         continue;
       }
-      counter += Long.valueOf(statVal);
+      counter += Long.parseLong(statVal);
     }
     LOG.info("Read stats for : " + partID + "\t" + statType + "\t" + counter);
 

File: ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java
Patch:
@@ -83,7 +83,8 @@ public boolean publishStat(String partKV, Map<String, String> stats) {
     if (null != statMap) {
       // In case of LB, we might get called repeatedly.
       for (Entry<String, String> e : statMap.entrySet()) {
-        cpy.put(e.getKey(), String.valueOf(Long.valueOf(e.getValue()) + Long.valueOf(cpy.get(e.getKey()))));
+        cpy.put(e.getKey(),
+            String.valueOf(Long.parseLong(e.getValue()) + Long.parseLong(cpy.get(e.getKey()))));
       }
     }
     statsMap.put(partKV, cpy);

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBetween.java
Patch:
@@ -75,7 +75,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
   public String getDisplayString(String[] children) {
     StringBuilder sb = new StringBuilder();
     sb.append(children[1]);
-    if (Boolean.valueOf(children[0])) {
+    if (Boolean.parseBoolean(children[0])) {
       sb.append(" NOT");
     }
     sb.append(" BETWEEN ");

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestTypeDescription.java
Patch:
@@ -51,11 +51,11 @@ public void testJson() {
             .addField("f4", TypeDescription.createDouble())
             .addField("f5", TypeDescription.createBoolean()))
         .addField("f6", TypeDescription.createChar().withMaxLength(100));
-    assertEquals("struct<f1:union<tinyint,decimal(20,10)>,f2:struct<f3:date,f4:double,f5:boolean>,f6:char(100)>",
+    assertEquals("struct<f1:uniontype<tinyint,decimal(20,10)>,f2:struct<f3:date,f4:double,f5:boolean>,f6:char(100)>",
         struct.toString());
     assertEquals(
         "{\"category\": \"struct\", \"id\": 0, \"max\": 8, \"fields\": [\n" +
-            "  \"f1\": {\"category\": \"union\", \"id\": 1, \"max\": 3, \"children\": [\n" +
+            "  \"f1\": {\"category\": \"uniontype\", \"id\": 1, \"max\": 3, \"children\": [\n" +
             "    {\"category\": \"tinyint\", \"id\": 2, \"max\": 2},\n" +
             "    {\"category\": \"decimal\", \"id\": 3, \"max\": 3, \"precision\": 20, \"scale\": 10}]},\n" +
             "  \"f2\": {\"category\": \"struct\", \"id\": 4, \"max\": 7, \"fields\": [\n" +

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFOPMinus.java
Patch:
@@ -224,7 +224,7 @@ public void testReturnTypeBackwardCompat() throws Exception {
 
     verifyReturnType(new GenericUDFOPMinus(), "float", "float", "float");
     verifyReturnType(new GenericUDFOPMinus(), "float", "double", "double");
-    verifyReturnType(new GenericUDFOPMinus(), "float", "decimal(10,2)", "double");
+    verifyReturnType(new GenericUDFOPMinus(), "float", "decimal(10,2)", "float");
 
     verifyReturnType(new GenericUDFOPMinus(), "double", "double", "double");
     verifyReturnType(new GenericUDFOPMinus(), "double", "decimal(10,2)", "double");
@@ -246,7 +246,7 @@ public void testReturnTypeAnsiSql() throws Exception {
 
     verifyReturnType(new GenericUDFOPMinus(), "float", "float", "float");
     verifyReturnType(new GenericUDFOPMinus(), "float", "double", "double");
-    verifyReturnType(new GenericUDFOPMinus(), "float", "decimal(10,2)", "double");
+    verifyReturnType(new GenericUDFOPMinus(), "float", "decimal(10,2)", "float");
 
     verifyReturnType(new GenericUDFOPMinus(), "double", "double", "double");
     verifyReturnType(new GenericUDFOPMinus(), "double", "decimal(10,2)", "double");

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFOPMultiply.java
Patch:
@@ -215,7 +215,7 @@ public void testReturnTypeBackwardCompat() throws Exception {
 
     verifyReturnType(new GenericUDFOPMultiply(), "float", "float", "float");
     verifyReturnType(new GenericUDFOPMultiply(), "float", "double", "double");
-    verifyReturnType(new GenericUDFOPMultiply(), "float", "decimal(10,2)", "double");
+    verifyReturnType(new GenericUDFOPMultiply(), "float", "decimal(10,2)", "float");
 
     verifyReturnType(new GenericUDFOPMultiply(), "double", "double", "double");
     verifyReturnType(new GenericUDFOPMultiply(), "double", "decimal(10,2)", "double");
@@ -237,7 +237,7 @@ public void testReturnTypeAnsiSql() throws Exception {
 
     verifyReturnType(new GenericUDFOPMultiply(), "float", "float", "float");
     verifyReturnType(new GenericUDFOPMultiply(), "float", "double", "double");
-    verifyReturnType(new GenericUDFOPMultiply(), "float", "decimal(10,2)", "double");
+    verifyReturnType(new GenericUDFOPMultiply(), "float", "decimal(10,2)", "float");
 
     verifyReturnType(new GenericUDFOPMultiply(), "double", "double", "double");
     verifyReturnType(new GenericUDFOPMultiply(), "double", "decimal(10,2)", "double");

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFOPPlus.java
Patch:
@@ -230,7 +230,7 @@ public void testReturnTypeBackwardCompat() throws Exception {
 
     verifyReturnType(new GenericUDFOPPlus(), "float", "float", "float");
     verifyReturnType(new GenericUDFOPPlus(), "float", "double", "double");
-    verifyReturnType(new GenericUDFOPPlus(), "float", "decimal(10,2)", "double");
+    verifyReturnType(new GenericUDFOPPlus(), "float", "decimal(10,2)", "float");
 
     verifyReturnType(new GenericUDFOPPlus(), "double", "double", "double");
     verifyReturnType(new GenericUDFOPPlus(), "double", "decimal(10,2)", "double");
@@ -252,7 +252,7 @@ public void testReturnTypeAnsiSql() throws Exception {
 
     verifyReturnType(new GenericUDFOPPlus(), "float", "float", "float");
     verifyReturnType(new GenericUDFOPPlus(), "float", "double", "double");
-    verifyReturnType(new GenericUDFOPPlus(), "float", "decimal(10,2)", "double");
+    verifyReturnType(new GenericUDFOPPlus(), "float", "decimal(10,2)", "float");
 
     verifyReturnType(new GenericUDFOPPlus(), "double", "double", "double");
     verifyReturnType(new GenericUDFOPPlus(), "double", "decimal(10,2)", "double");

File: orc/src/java/org/apache/orc/impl/MetadataReader.java
Patch:
@@ -17,18 +17,17 @@
  */
 package org.apache.orc.impl;
 
+import java.io.Closeable;
 import java.io.IOException;
 
 import org.apache.orc.OrcProto;
 import org.apache.orc.StripeInformation;
 
-public interface MetadataReader {
+public interface MetadataReader extends Closeable {
   OrcIndex readRowIndex(StripeInformation stripe,
                                       OrcProto.StripeFooter footer,
       boolean[] included, OrcProto.RowIndex[] indexes, boolean[] sargColumns,
       OrcProto.BloomFilterIndex[] bloomFilterIndices) throws IOException;
 
   OrcProto.StripeFooter readStripeFooter(StripeInformation stripe) throws IOException;
-
-  void close() throws IOException;
 }

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -520,8 +520,7 @@ public void run() {
         }
       }
 
-      if (conf.getBoolVar(ConfVars.HIVE_LOG_EXPLAIN_OUTPUT) ||
-           conf.isWebUiQueryInfoCacheEnabled()) {
+      if (conf.getBoolVar(ConfVars.HIVE_LOG_EXPLAIN_OUTPUT)) {
         String explainOutput = getExplainOutput(sem, plan, tree);
         if (explainOutput != null) {
           if (conf.getBoolVar(ConfVars.HIVE_LOG_EXPLAIN_OUTPUT)) {

File: ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
Patch:
@@ -658,7 +658,7 @@ private void createSessionDirs(String userName) throws IOException {
     conf.set(HDFS_SESSION_PATH_KEY, hdfsSessionPath.toUri().toString());
     // 5. hold a lock file in HDFS session dir to indicate the it is in use
     if (conf.getBoolVar(HiveConf.ConfVars.HIVE_SCRATCH_DIR_LOCK)) {
-      FileSystem fs = FileSystem.get(conf);
+      FileSystem fs = hdfsSessionPath.getFileSystem(conf);
       hdfsSessionPathLockFile = fs.create(new Path(hdfsSessionPath, LOCK_FILE_NAME), true);
       hdfsSessionPathLockFile.writeUTF("hostname: " + InetAddress.getLocalHost().getHostName() + "\n");
       hdfsSessionPathLockFile.writeUTF("process: " + ManagementFactory.getRuntimeMXBean().getName() + "\n");

File: orc/src/java/org/apache/orc/impl/BitFieldReader.java
Patch:
@@ -137,7 +137,7 @@ public void nextVector(LongColumnVector previous,
                          long previousLen) throws IOException {
     previous.isRepeating = true;
     for (int i = 0; i < previousLen; i++) {
-      if (previous.noNulls || !previous.isNull[i]) {
+      if (!previous.isNull[i]) {
         previous.vector[i] = next();
       } else {
         // The default value of null for int types in vectorized
@@ -150,8 +150,7 @@ public void nextVector(LongColumnVector previous,
       // when determining the isRepeating flag.
       if (previous.isRepeating
           && i > 0
-          && ((previous.vector[0] != previous.vector[i]) ||
-          (previous.isNull[0] != previous.isNull[i]))) {
+          && ((previous.vector[i - 1] != previous.vector[i]) || (previous.isNull[i - 1] != previous.isNull[i]))) {
         previous.isRepeating = false;
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java
Patch:
@@ -447,8 +447,7 @@ static Reader.Options createEventOptions(Reader.Options options) {
     this.length = options.getLength();
     this.validTxnList = validTxnList;
 
-    TypeDescription typeDescr =
-        OrcInputFormat.getDesiredRowTypeDescr(conf, true, Integer.MAX_VALUE);
+    TypeDescription typeDescr = OrcInputFormat.getDesiredRowTypeDescr(conf, /* isAcidRead */ true);
 
     objectInspector = OrcRecordUpdater.createEventSchema
         (OrcStruct.createObjectInspector(0, OrcUtils.getOrcTypes(typeDescr)));

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
Patch:
@@ -101,6 +101,8 @@ public class WriterImpl extends org.apache.orc.impl.WriterImpl implements Writer
     }
   }
 
+  private static final long NANOS_PER_MILLI = 1000000;
+
   /**
    * Set the value for a given column value within a batch.
    * @param rowId the row to set

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestTypeDescription.java
Patch:
@@ -51,11 +51,11 @@ public void testJson() {
             .addField("f4", TypeDescription.createDouble())
             .addField("f5", TypeDescription.createBoolean()))
         .addField("f6", TypeDescription.createChar().withMaxLength(100));
-    assertEquals("struct<f1:uniontype<tinyint,decimal(20,10)>,f2:struct<f3:date,f4:double,f5:boolean>,f6:char(100)>",
+    assertEquals("struct<f1:union<tinyint,decimal(20,10)>,f2:struct<f3:date,f4:double,f5:boolean>,f6:char(100)>",
         struct.toString());
     assertEquals(
         "{\"category\": \"struct\", \"id\": 0, \"max\": 8, \"fields\": [\n" +
-            "  \"f1\": {\"category\": \"uniontype\", \"id\": 1, \"max\": 3, \"children\": [\n" +
+            "  \"f1\": {\"category\": \"union\", \"id\": 1, \"max\": 3, \"children\": [\n" +
             "    {\"category\": \"tinyint\", \"id\": 2, \"max\": 2},\n" +
             "    {\"category\": \"decimal\", \"id\": 3, \"max\": 3, \"precision\": 20, \"scale\": 10}]},\n" +
             "  \"f2\": {\"category\": \"struct\", \"id\": 4, \"max\": 7, \"fields\": [\n" +

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java
Patch:
@@ -392,4 +392,4 @@ public void stringifyValue(StringBuilder buffer, int row) {
       buffer.append("null");
     }
   }
-}
+}
\ No newline at end of file

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/UnionColumnVector.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec.vector;
 
+import java.util.Arrays;
+
 /**
  * The representation of a vectorized column of struct objects.
  *

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java
Patch:
@@ -151,7 +151,7 @@ public HivePrivilegeObject(HivePrivilegeObjectType type, String dbname, String o
   }
 
   public HivePrivilegeObject(String dbname, String objectName, List<String> columns) {
-    this(null, dbname, objectName, null, columns, null);
+    this(HivePrivilegeObjectType.TABLE_OR_VIEW, dbname, objectName, null, columns, null);
   }
 
   public HivePrivilegeObject(HivePrivilegeObjectType type, String dbname, String objectName,

File: orc/src/java/org/apache/orc/impl/BitFieldReader.java
Patch:
@@ -137,7 +137,7 @@ public void nextVector(LongColumnVector previous,
                          long previousLen) throws IOException {
     previous.isRepeating = true;
     for (int i = 0; i < previousLen; i++) {
-      if (!previous.isNull[i]) {
+      if (previous.noNulls || !previous.isNull[i]) {
         previous.vector[i] = next();
       } else {
         // The default value of null for int types in vectorized
@@ -150,7 +150,8 @@ public void nextVector(LongColumnVector previous,
       // when determining the isRepeating flag.
       if (previous.isRepeating
           && i > 0
-          && ((previous.vector[i - 1] != previous.vector[i]) || (previous.isNull[i - 1] != previous.isNull[i]))) {
+          && ((previous.vector[0] != previous.vector[i]) ||
+          (previous.isNull[0] != previous.isNull[i]))) {
         previous.isRepeating = false;
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java
Patch:
@@ -447,7 +447,8 @@ static Reader.Options createEventOptions(Reader.Options options) {
     this.length = options.getLength();
     this.validTxnList = validTxnList;
 
-    TypeDescription typeDescr = OrcInputFormat.getDesiredRowTypeDescr(conf, /* isAcidRead */ true);
+    TypeDescription typeDescr =
+        OrcInputFormat.getDesiredRowTypeDescr(conf, true, Integer.MAX_VALUE);
 
     objectInspector = OrcRecordUpdater.createEventSchema
         (OrcStruct.createObjectInspector(0, OrcUtils.getOrcTypes(typeDescr)));

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
Patch:
@@ -101,8 +101,6 @@ public class WriterImpl extends org.apache.orc.impl.WriterImpl implements Writer
     }
   }
 
-  private static final long NANOS_PER_MILLI = 1000000;
-
   /**
    * Set the value for a given column value within a batch.
    * @param rowId the row to set

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestTypeDescription.java
Patch:
@@ -51,11 +51,11 @@ public void testJson() {
             .addField("f4", TypeDescription.createDouble())
             .addField("f5", TypeDescription.createBoolean()))
         .addField("f6", TypeDescription.createChar().withMaxLength(100));
-    assertEquals("struct<f1:union<tinyint,decimal(20,10)>,f2:struct<f3:date,f4:double,f5:boolean>,f6:char(100)>",
+    assertEquals("struct<f1:uniontype<tinyint,decimal(20,10)>,f2:struct<f3:date,f4:double,f5:boolean>,f6:char(100)>",
         struct.toString());
     assertEquals(
         "{\"category\": \"struct\", \"id\": 0, \"max\": 8, \"fields\": [\n" +
-            "  \"f1\": {\"category\": \"union\", \"id\": 1, \"max\": 3, \"children\": [\n" +
+            "  \"f1\": {\"category\": \"uniontype\", \"id\": 1, \"max\": 3, \"children\": [\n" +
             "    {\"category\": \"tinyint\", \"id\": 2, \"max\": 2},\n" +
             "    {\"category\": \"decimal\", \"id\": 3, \"max\": 3, \"precision\": 20, \"scale\": 10}]},\n" +
             "  \"f2\": {\"category\": \"struct\", \"id\": 4, \"max\": 7, \"fields\": [\n" +

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java
Patch:
@@ -392,4 +392,4 @@ public void stringifyValue(StringBuilder buffer, int row) {
       buffer.append("null");
     }
   }
-}
\ No newline at end of file
+}

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/UnionColumnVector.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector;
 
-import java.util.Arrays;
-
 /**
  * The representation of a vectorized column of struct objects.
  *

File: common/src/java/org/apache/hadoop/hive/common/type/HiveVarchar.java
Patch:
@@ -58,10 +58,10 @@ public int compareTo(HiveVarchar rhs) {
     return this.getValue().compareTo(rhs.getValue());
   }
 
-  public boolean equals(HiveVarchar rhs) {
+  public boolean equals(Object rhs) {
     if (rhs == this) {
       return true;
     }
-    return this.getValue().equals(rhs.getValue());
+    return this.getValue().equals(((HiveVarchar)rhs).getValue());
   }
 }

File: common/src/test/org/apache/hadoop/hive/common/type/TestHiveBaseChar.java
Patch:
@@ -80,6 +80,7 @@ public void testStringLength() throws Exception {
         assertEquals(strLen, enforcedString.codePointCount(0, enforcedString.length()));
       }
     }
+    assertNull(HiveBaseChar.enforceMaxLength(null, 0));
   }
 
   public void testGetPaddedValue() {
@@ -96,5 +97,6 @@ public void testGetPaddedValue() {
 
     assertEquals("abc       ", HiveBaseChar.getPaddedValue("abc", 10));
     assertEquals("abc       ", HiveBaseChar.getPaddedValue("abc ", 10));
+    assertNull(HiveBaseChar.getPaddedValue(null, 0));
   }
 }

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFOPMinus.java
Patch:
@@ -224,7 +224,7 @@ public void testReturnTypeBackwardCompat() throws Exception {
 
     verifyReturnType(new GenericUDFOPMinus(), "float", "float", "float");
     verifyReturnType(new GenericUDFOPMinus(), "float", "double", "double");
-    verifyReturnType(new GenericUDFOPMinus(), "float", "decimal(10,2)", "double");
+    verifyReturnType(new GenericUDFOPMinus(), "float", "decimal(10,2)", "float");
 
     verifyReturnType(new GenericUDFOPMinus(), "double", "double", "double");
     verifyReturnType(new GenericUDFOPMinus(), "double", "decimal(10,2)", "double");
@@ -246,7 +246,7 @@ public void testReturnTypeAnsiSql() throws Exception {
 
     verifyReturnType(new GenericUDFOPMinus(), "float", "float", "float");
     verifyReturnType(new GenericUDFOPMinus(), "float", "double", "double");
-    verifyReturnType(new GenericUDFOPMinus(), "float", "decimal(10,2)", "double");
+    verifyReturnType(new GenericUDFOPMinus(), "float", "decimal(10,2)", "float");
 
     verifyReturnType(new GenericUDFOPMinus(), "double", "double", "double");
     verifyReturnType(new GenericUDFOPMinus(), "double", "decimal(10,2)", "double");

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFOPMultiply.java
Patch:
@@ -215,7 +215,7 @@ public void testReturnTypeBackwardCompat() throws Exception {
 
     verifyReturnType(new GenericUDFOPMultiply(), "float", "float", "float");
     verifyReturnType(new GenericUDFOPMultiply(), "float", "double", "double");
-    verifyReturnType(new GenericUDFOPMultiply(), "float", "decimal(10,2)", "double");
+    verifyReturnType(new GenericUDFOPMultiply(), "float", "decimal(10,2)", "float");
 
     verifyReturnType(new GenericUDFOPMultiply(), "double", "double", "double");
     verifyReturnType(new GenericUDFOPMultiply(), "double", "decimal(10,2)", "double");
@@ -237,7 +237,7 @@ public void testReturnTypeAnsiSql() throws Exception {
 
     verifyReturnType(new GenericUDFOPMultiply(), "float", "float", "float");
     verifyReturnType(new GenericUDFOPMultiply(), "float", "double", "double");
-    verifyReturnType(new GenericUDFOPMultiply(), "float", "decimal(10,2)", "double");
+    verifyReturnType(new GenericUDFOPMultiply(), "float", "decimal(10,2)", "float");
 
     verifyReturnType(new GenericUDFOPMultiply(), "double", "double", "double");
     verifyReturnType(new GenericUDFOPMultiply(), "double", "decimal(10,2)", "double");

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFOPPlus.java
Patch:
@@ -230,7 +230,7 @@ public void testReturnTypeBackwardCompat() throws Exception {
 
     verifyReturnType(new GenericUDFOPPlus(), "float", "float", "float");
     verifyReturnType(new GenericUDFOPPlus(), "float", "double", "double");
-    verifyReturnType(new GenericUDFOPPlus(), "float", "decimal(10,2)", "double");
+    verifyReturnType(new GenericUDFOPPlus(), "float", "decimal(10,2)", "float");
 
     verifyReturnType(new GenericUDFOPPlus(), "double", "double", "double");
     verifyReturnType(new GenericUDFOPPlus(), "double", "decimal(10,2)", "double");
@@ -252,7 +252,7 @@ public void testReturnTypeAnsiSql() throws Exception {
 
     verifyReturnType(new GenericUDFOPPlus(), "float", "float", "float");
     verifyReturnType(new GenericUDFOPPlus(), "float", "double", "double");
-    verifyReturnType(new GenericUDFOPPlus(), "float", "decimal(10,2)", "double");
+    verifyReturnType(new GenericUDFOPPlus(), "float", "decimal(10,2)", "float");
 
     verifyReturnType(new GenericUDFOPPlus(), "double", "double", "double");
     verifyReturnType(new GenericUDFOPPlus(), "double", "decimal(10,2)", "double");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -527,6 +527,7 @@ public int execute(DriverContext driverContext) {
     } catch (Exception e) {
       console.printError("Failed with exception " + e.getMessage(), "\n"
           + StringUtils.stringifyException(e));
+      setException(e);
       return (1);
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java
Patch:
@@ -546,13 +546,11 @@ static long getLastFlushLength(FileSystem fs,
                                          Path deltaFile) throws IOException {
     Path lengths = OrcRecordUpdater.getSideFile(deltaFile);
     long result = Long.MAX_VALUE;
-    try {
-      FSDataInputStream stream = fs.open(lengths);
+    try (FSDataInputStream stream = fs.open(lengths)) {
       result = -1;
       while (stream.available() > 0) {
         result = stream.readLong();
       }
-      stream.close();
       return result;
     } catch (IOException ioe) {
       return result;

File: llap-client/src/java/org/apache/hadoop/hive/llap/tez/LlapProtocolClientProxy.java
Patch:
@@ -139,10 +139,8 @@ public void sendSubmitWork(SubmitWorkRequestProto request, String host, int port
     requestManager.queueRequest(new SubmitWorkCallable(nodeId, request, callback));
   }
 
-  public void sendSourceStateUpdate(final SourceStateUpdatedRequestProto request, final String host,
-                                    final int port,
+  public void sendSourceStateUpdate(final SourceStateUpdatedRequestProto request, final LlapNodeId nodeId,
                                     final ExecuteRequestCallback<SourceStateUpdatedResponseProto> callback) {
-    LlapNodeId nodeId = LlapNodeId.getInstance(host, port);
     requestManager.queueRequest(
         new SendSourceStateUpdateCallable(nodeId, request, callback));
   }

File: llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/helpers/SourceStateTracker.java
Patch:
@@ -280,7 +280,7 @@ private void maybeRegisterForVertexUpdates(String sourceName) {
 
 
   void sendStateUpdateToNode(LlapNodeId nodeId, String sourceName, VertexState state) {
-    taskCommunicator.sendStateUpdate(nodeId.getHostname(), nodeId.getPort(),
+    taskCommunicator.sendStateUpdate(nodeId,
         SourceStateUpdatedRequestProto.newBuilder().setQueryIdentifier(currentQueryIdentifier)
             .setSrcName(sourceName).setState(Converters.fromVertexState(state)).build());
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java
Patch:
@@ -105,6 +105,9 @@ public int execute(DriverContext driverContext) {
         }
         LOG.info("Execution completed successfully");
       } else if (rc == 2) { // Cancel job if the monitor found job submission timeout.
+        // TODO: If the timeout is because of lack of resources in the cluster, we should
+        // ideally also cancel the app request here. But w/o facilities from Spark or YARN,
+        // it's difficult to do it on hive side alone. See HIVE-12650.
         jobRef.cancelJob();
       }
       sparkJobStatus.cleanup();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/LocalSparkJobMonitor.java
Patch:
@@ -59,7 +59,7 @@ public int startMonitor() {
         if (state == null) {
           long timeCount = (System.currentTimeMillis() - startTime)/1000;
           if (timeCount > monitorTimeoutInteval) {
-            LOG.info("Job hasn't been submitted after " + timeCount + "s. Aborting it.");
+            console.printError("Job hasn't been submitted after " + timeCount + "s. Aborting it.");
             console.printError("Status: " + state);
             running = false;
             done = true;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToTimestamp.java
Patch:
@@ -44,7 +44,6 @@ public CastDecimalToTimestamp() {
 
   @Override
   protected void func(TimestampColumnVector outV, DecimalColumnVector inV,  int i) {
-    Timestamp timestamp = TimestampWritable.decimalToTimestamp(inV.vector[i].getHiveDecimal());
-    outV.set(i, timestamp);
+    outV.set(i, TimestampWritable.decimalToTimestamp(inV.vector[i].getHiveDecimal()));
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToBoolean.java
Patch:
@@ -41,8 +41,8 @@ public CastTimestampToBoolean() {
   }
 
   private int toBool(TimestampColumnVector timestampColVector, int index) {
-    return (timestampColVector.getEpochDay(index) != 0 ||
-            timestampColVector.getNanoOfDay(index) != 0) ? 1 : 0;
+    return (timestampColVector.getTime(index) != 0 ||
+            timestampColVector.getNanos(index) != 0) ? 1 : 0;
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDate.java
Patch:
@@ -44,6 +44,6 @@ public CastTimestampToDate(int inputColumn, int outputColumn) {
   @Override
   protected void func(LongColumnVector outV, TimestampColumnVector inV, int i) {
 
-    outV.vector[i] = DateWritable.millisToDays(inV.getTimestampMilliseconds(i));
+    outV.vector[i] = DateWritable.millisToDays(inV.getTime(i));
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnScalar.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.sql.Timestamp;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
 /**
@@ -35,7 +34,7 @@ public class IfExprTimestampColumnScalar extends IfExprTimestampColumnScalarBase
 
   public IfExprTimestampColumnScalar(int arg1Column, int arg2Column, Timestamp arg3Scalar,
       int outputColumn) {
-    super(arg1Column, arg2Column, new PisaTimestamp(arg3Scalar), outputColumn);
+    super(arg1Column, arg2Column, arg3Scalar, outputColumn);
   }
 
   public IfExprTimestampColumnScalar() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarColumn.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.sql.Timestamp;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
 /**
@@ -35,7 +34,7 @@ public class IfExprTimestampScalarColumn extends IfExprTimestampScalarColumnBase
 
   public IfExprTimestampScalarColumn(int arg1Column, Timestamp arg2Scalar, int arg3Column,
       int outputColumn) {
-    super(arg1Column, new PisaTimestamp(arg2Scalar), arg3Column, outputColumn);
+    super(arg1Column, arg2Scalar, arg3Column, outputColumn);
   }
 
   public IfExprTimestampScalarColumn() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarScalar.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
 import java.sql.Timestamp;
@@ -35,7 +34,7 @@ public class IfExprTimestampScalarScalar extends IfExprTimestampScalarScalarBase
 
   public IfExprTimestampScalarScalar(int arg1Column, Timestamp arg2Scalar, Timestamp arg3Scalar,
       int outputColumn) {
-    super(arg1Column, new PisaTimestamp(arg2Scalar), new PisaTimestamp(arg3Scalar), outputColumn);
+    super(arg1Column, arg2Scalar, arg3Scalar, outputColumn);
   }
 
   public IfExprTimestampScalarScalar() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateAddColCol.java
Patch:
@@ -166,7 +166,7 @@ protected byte[] evaluateDate(ColumnVector columnVector, int index, long numDays
 
   protected byte[] evaluateTimestamp(ColumnVector columnVector, int index, long numDays) {
     TimestampColumnVector tcv = (TimestampColumnVector) columnVector;
-    calendar.setTimeInMillis(tcv.getTimestampMilliseconds(index));
+    calendar.setTimeInMillis(tcv.getTime(index));
     if (isPositive) {
       calendar.add(Calendar.DATE, (int) numDays);
     } else {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateAddColScalar.java
Patch:
@@ -210,7 +210,7 @@ public void evaluate(VectorizedRowBatch batch) {
 
   protected byte[] evaluateTimestamp(ColumnVector columnVector, int index) {
     TimestampColumnVector tcv = (TimestampColumnVector) columnVector;
-    calendar.setTimeInMillis(tcv.getTimestampMilliseconds(index));
+    calendar.setTimeInMillis(tcv.getTime(index));
     if (isPositive) {
       calendar.add(Calendar.DATE, numDays);
     } else {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffColScalar.java
Patch:
@@ -238,7 +238,7 @@ public void evaluate(VectorizedRowBatch batch) {
 
   protected int evaluateTimestamp(ColumnVector columnVector, int index) {
     TimestampColumnVector tcv = (TimestampColumnVector) columnVector;
-    date.setTime(tcv.getTimestampMilliseconds(index));
+    date.setTime(tcv.getTime(index));
     return DateWritable.dateToDays(date) - baseDate;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffScalarCol.java
Patch:
@@ -237,7 +237,7 @@ public void evaluate(VectorizedRowBatch batch) {
 
   protected int evaluateTimestamp(ColumnVector columnVector, int index) {
     TimestampColumnVector tcv = (TimestampColumnVector) columnVector;
-    date.setTime(tcv.getTimestampMilliseconds(index));
+    date.setTime(tcv.getTime(index));
     return baseDate - DateWritable.dateToDays(date);
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateTimestamp.java
Patch:
@@ -45,7 +45,7 @@ public VectorUDFDateTimestamp(int inputColumn, int outputColumn) {
   protected void func(BytesColumnVector outV, TimestampColumnVector inV, int i) {
     switch (inputTypes[0]) {
       case TIMESTAMP:
-        date.setTime(inV.getTimestampMilliseconds(i));
+        date.setTime(inV.getTime(i));
         break;
 
       default:

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorFilterExpressions.java
Patch:
@@ -25,7 +25,6 @@
 import java.sql.Timestamp;
 
 import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/util/FakeVectorRowBatchFromObjectIterables.java
Patch:
@@ -27,7 +27,6 @@
 import java.util.regex.Pattern;
 
 import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
@@ -111,7 +110,7 @@ public void assign(
               Object value) {
             TimestampColumnVector lcv = (TimestampColumnVector) columnVector;
             Timestamp t = (Timestamp) value;
-            lcv.set(row, new PisaTimestamp(t));
+            lcv.set(row, t);
           }
         };
 

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
Patch:
@@ -1946,7 +1946,7 @@ public void testVectorizationWithAcid() throws Exception {
       long millis = (long) i * MILLIS_IN_DAY;
       millis -= LOCAL_TIMEZONE.getOffset(millis);
       assertEquals("checking timestamp " + i, millis,
-          timestampColumn.getTimestampMilliseconds(i));
+          timestampColumn.getTime(i));
     }
     assertEquals(false, reader.next(key, value));
   }

File: storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/ColumnVector.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector;
 
-import java.io.IOException;
 import java.util.Arrays;
 
 /**
@@ -43,6 +42,7 @@ public static enum Type {
     BYTES,
     DECIMAL,
     TIMESTAMP,
+    INTERVAL_DAY_TIME,
     STRUCT,
     LIST,
     MAP,

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheMemoryManager.java
Patch:
@@ -50,9 +50,9 @@ public LowLevelCacheMemoryManager(
     this.usedMemory = new AtomicLong(0);
     this.metrics = metrics;
     metrics.setCacheCapacityTotal(maxSize);
-    if (LlapIoImpl.LOGL.isInfoEnabled()) {
-      LlapIoImpl.LOG.info("Memory manager initialized with max size " + maxSize + " and "
-          + ((evictor == null) ? "no " : "") + "ability to evict blocks");
+    if (LlapIoImpl.LOG.isInfoEnabled()) {
+      LlapIoImpl.LOG.info("Memory manager initialized with max size {} and" +
+          " {} ability to evict blocks", maxSize, ((evictor == null) ? "no " : ""));
     }
   }
 

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelFifoCachePolicy.java
Patch:
@@ -35,9 +35,7 @@ public class LowLevelFifoCachePolicy implements LowLevelCachePolicy {
   private LlapOomDebugDump parentDebugDump;
 
   public LowLevelFifoCachePolicy(Configuration conf) {
-    if (LlapIoImpl.LOGL.isInfoEnabled()) {
-      LlapIoImpl.LOG.info("FIFO cache policy");
-    }
+    LlapIoImpl.LOG.info("FIFO cache policy");
     buffers = new LinkedList<LlapCacheableBuffer>();
   }
 

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java
Patch:
@@ -49,9 +49,7 @@ public class OrcColumnVectorProducer implements ColumnVectorProducer {
   public OrcColumnVectorProducer(OrcMetadataCache metadataCache,
       LowLevelCacheImpl lowLevelCache, BufferUsageManager bufferManager,
       Configuration conf, LlapDaemonCacheMetrics metrics, LlapDaemonQueueMetrics queueMetrics) {
-    if (LlapIoImpl.LOGL.isInfoEnabled()) {
-      LlapIoImpl.LOG.info("Initializing ORC column vector producer");
-    }
+    LlapIoImpl.LOG.info("Initializing ORC column vector producer");
 
     this.metadataCache = metadataCache;
     this.lowLevelCache = lowLevelCache;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
Patch:
@@ -815,7 +815,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
         aggregationBatchInfo = new VectorAggregationBufferBatch();
         aggregationBatchInfo.compileAggregationBatchInfo(aggregators);
       }
-      LOG.warn("VectorGroupByOperator is vector output " + isVectorOutput);
+      LOG.info("VectorGroupByOperator is vector output {}", isVectorOutput);
       outputObjInspector = ObjectInspectorFactory.getStandardStructObjectInspector(
           outputFieldNames, objectInspectors);
       if (isVectorOutput) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReader.java
Patch:
@@ -54,5 +54,5 @@ void readEncodedColumns(int stripeIx, StripeInformation stripe,
    * checks are entirely eliminated because this method is called with constant value, similar
    * to just checking the constant in the first place.
    */
-  void setDebugTracing(boolean isEnabled);
+  void setTracing(boolean isEnabled);
 }
\ No newline at end of file

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2522,7 +2522,7 @@ public static enum ConfVars {
         "hive.tez.exec.inplace.progress",
         true,
         "Updates tez job execution progress in-place in the terminal."),
-    LLAP_IO_ENABLED("hive.llap.io.enabled", false, "Whether the LLAP IO layer is enabled."),
+    LLAP_IO_ENABLED("hive.llap.io.enabled", null, "Whether the LLAP IO layer is enabled."),
     LLAP_IO_MEMORY_MODE("hive.llap.io.memory.mode", "cache",
         new StringSet("cache", "allocator", "none"),
         "LLAP IO memory usage; 'cache' (the default) uses data and metadata cache with a\n" +

File: llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapOptionsProcessor.java
Patch:
@@ -235,7 +235,6 @@ public LlapOptions processOptions(String argv[]) throws ParseException, IOExcept
     String name = commandLine.getOptionValue(OPTION_NAME, null);
 
     final int executors = Integer.parseInt(commandLine.getOptionValue(OPTION_EXECUTORS, "-1"));
-    // TODO# here
     final int ioThreads = Integer.parseInt(
         commandLine.getOptionValue(OPTION_IO_THREADS, Integer.toString(executors)));
     final long cache = parseSuffixed(commandLine.getOptionValue(OPTION_CACHE, "-1"));

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java
Patch:
@@ -348,8 +348,8 @@ public static void main(String[] args) throws Exception {
 
       long ioMemoryBytes = HiveConf.getSizeVar(daemonConf, ConfVars.LLAP_IO_MEMORY_MAX_SIZE);
       boolean isDirectCache = HiveConf.getBoolVar(daemonConf, ConfVars.LLAP_ALLOCATOR_DIRECT);
-      boolean llapIoEnabled = HiveConf.getBoolVar(daemonConf, HiveConf.ConfVars.LLAP_IO_ENABLED);
-      llapDaemon = new LlapDaemon(daemonConf, numExecutors, executorMemoryBytes, llapIoEnabled,
+      boolean isLlapIo = HiveConf.getBoolVar(daemonConf, HiveConf.ConfVars.LLAP_IO_ENABLED, true);
+      llapDaemon = new LlapDaemon(daemonConf, numExecutors, executorMemoryBytes, isLlapIo,
               isDirectCache, ioMemoryBytes, localDirs, rpcPort, mngPort, shufflePort);
 
       LOG.info("Adding shutdown hook for LlapDaemon");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobMonitor.java
Patch:
@@ -225,7 +225,7 @@ public int monitorExecution(final DAGClient dagClient, HiveConf conf,
     long startTime = 0;
     boolean isProfileEnabled = HiveConf.getBoolVar(conf, HiveConf.ConfVars.TEZ_EXEC_SUMMARY) ||
         Utilities.isPerfOrAboveLogging(conf);
-    boolean llapIoEnabled = HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_IO_ENABLED);
+    boolean llapIoEnabled = HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_IO_ENABLED, true);
 
     boolean inPlaceEligible = InPlaceUpdates.inPlaceEligible(conf);
     synchronized(shutdownList) {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.OperatorUtils;
@@ -207,7 +208,8 @@ public void deriveExplainAttributes() {
 
   public void deriveLlap(Configuration conf) {
     boolean hasLlap = false, hasNonLlap = false, hasAcid = false;
-    boolean isLlapOn = HiveInputFormat.isLlapEnabled(conf),
+    // Assume the IO is enabled on the daemon by default. We cannot reasonably check it here.
+    boolean isLlapOn = HiveConf.getBoolVar(conf, ConfVars.LLAP_IO_ENABLED, llapMode),
         canWrapAny = isLlapOn && HiveInputFormat.canWrapAnyForLlap(conf, this);
     boolean hasPathToPartInfo = (pathToPartitionInfo != null && !pathToPartitionInfo.isEmpty());
     if (canWrapAny && hasPathToPartInfo) {

File: storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java
Patch:
@@ -110,7 +110,7 @@ public void setAllStreamsData(int colIxMod, int colIx, ColumnStreamData[] sbs) {
   }
 
   public BatchKey getBatchKey() {
-    return batchKey; // TODO#: who uses this? can we remove fileId?
+    return batchKey;
   }
 
   public ColumnStreamData[][] getColumnData() {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1298,7 +1298,7 @@ public static enum ConfVars {
         " of the number of keys is divided by this value. If the value is 0, statistics are not used" +
         "and hive.hashtable.initialCapacity is used instead."),
     HIVEHASHTABLETHRESHOLD("hive.hashtable.initialCapacity", 100000, "Initial capacity of " +
-        "mapjoin hashtable if statistics are absent, or if hive.hashtable.stats.key.estimate.adjustment is set to 0"),
+        "mapjoin hashtable if statistics are absent, or if hive.hashtable.key.count.adjustment is set to 0"),
     HIVEHASHTABLELOADFACTOR("hive.hashtable.loadfactor", (float) 0.75, ""),
     HIVEHASHTABLEFOLLOWBYGBYMAXMEMORYUSAGE("hive.mapjoin.followby.gby.localtask.max.memory.usage", (float) 0.55,
         "This number means how much memory the local task can take to hold the key/value into an in-memory hash table \n" +

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -1706,7 +1706,8 @@ private RelNode genTableLogicalPlan(String tableAlias, QB qb) throws SemanticExc
         tableRel = new HiveTableScan(cluster, cluster.traitSetOf(HiveRelNode.CONVENTION), optTable,
             null == tableAlias ? tabMetaData.getTableName() : tableAlias,
             getAliasId(tableAlias, qb), HiveConf.getBoolVar(conf,
-                HiveConf.ConfVars.HIVE_CBO_RETPATH_HIVEOP));
+                HiveConf.ConfVars.HIVE_CBO_RETPATH_HIVEOP), qb.isInsideView()
+                || qb.getAliasInsideView().contains(tableAlias.toLowerCase()));
 
         // 6. Add Schema(RR) to RelNode-Schema map
         ImmutableMap<String, Integer> hiveToCalciteColMap = buildHiveToCalciteColumnMap(rr,

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -1460,7 +1460,8 @@ private void maskPatterns(Pattern[] patterns, String fname) throws Exception {
       ".*Input:.*/data/files/.*",
       ".*Output:.*/data/files/.*",
       ".*total number of created files now is.*",
-      ".*.hive-staging.*"
+      ".*.hive-staging.*",
+      "table_.*"
   });
 
   private final Pattern[] partialReservedPlanMask = toPattern(new String[] {

File: jdbc/src/java/org/apache/hive/jdbc/LlapInputFormat.java
Patch:
@@ -64,6 +64,8 @@ public class LlapInputFormat<V extends WritableComparable> implements InputForma
   public final String USER_KEY = "llap.if.user";
   public final String PWD_KEY = "llap.if.pwd";
 
+  public final String SPLIT_QUERY = "select get_splits(\"%s\",%d)";
+
   private Connection con;
   private Statement stmt;
 
@@ -105,7 +107,7 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
     try {
       con = DriverManager.getConnection(url,user,pwd);
       stmt = con.createStatement();
-      String sql = "select r.if_class as ic, r.split_class as sc, r.split as s from (select explode(get_splits(\""+query+"\","+numSplits+")) as r) t";
+      String sql = String.format(SPLIT_QUERY, query, numSplits);
       ResultSet res = stmt.executeQuery(sql);
       while (res.next()) {
         // deserialize split

File: llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
Patch:
@@ -349,7 +349,7 @@ public void unregisterRunningTaskAttempt(final TezTaskAttemptID taskAttemptId,
   private void sendTaskTerminated(final TezTaskAttemptID taskAttemptId,
                                   boolean invokedByContainerEnd) {
     LOG.info(
-        "DBG: Attempting to send terminateRequest for fragment {} due to internal preemption invoked by {}",
+        "Attempting to send terminateRequest for fragment {} due to internal preemption invoked by {}",
         taskAttemptId.toString(), invokedByContainerEnd ? "containerEnd" : "taskEnd");
     LlapNodeId nodeId = entityTracker.getNodeIdForTaskAttempt(taskAttemptId);
     // NodeId can be null if the task gets unregistered due to failure / being killed by the daemon itself

File: llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
Patch:
@@ -931,7 +931,7 @@ private void preemptTasks(int forPriority, int numTasksToPreempt, String []poten
           }
         } else {
           // No tasks qualify as preemptable
-          LOG.info("DBG: No tasks qualify as killable to schedule tasks at priority {}", forPriority);
+          LOG.info("No tasks qualify as killable to schedule tasks at priority {}", forPriority);
           break;
         }
       }
@@ -941,7 +941,7 @@ private void preemptTasks(int forPriority, int numTasksToPreempt, String []poten
     // Send out the preempted request outside of the lock.
     if (preemptedTaskList != null) {
       for (TaskInfo taskInfo : preemptedTaskList) {
-        LOG.info("DBG: Preempting task {}", taskInfo);
+        LOG.info("Preempting task {}", taskInfo);
         getContext().preemptContainer(taskInfo.containerId);
         // Preemption will finally be registered as a deallocateTask as a result of preemptContainer
         // That resets preemption info and allows additional tasks to be pre-empted if required.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -344,8 +344,6 @@ public final class FunctionRegistry {
     system.registerGenericUDF("ewah_bitmap_or", GenericUDFEWAHBitmapOr.class);
     system.registerGenericUDF("ewah_bitmap_empty", GenericUDFEWAHBitmapEmpty.class);
 
-    system.registerGenericUDF("get_splits", GenericUDFGetSplits.class);
-
     // Aliases for Java Class Names
     // These are used in getImplicitConvertUDFMethod
     system.registerUDF(serdeConstants.BOOLEAN_TYPE_NAME, UDFToBoolean.class, false, UDFToBoolean.class.getSimpleName());
@@ -444,6 +442,8 @@ public final class FunctionRegistry {
     system.registerGenericUDTF("parse_url_tuple", GenericUDTFParseUrlTuple.class);
     system.registerGenericUDTF("posexplode", GenericUDTFPosExplode.class);
     system.registerGenericUDTF("stack", GenericUDTFStack.class);
+    system.registerGenericUDTF("get_splits", GenericUDTFGetSplits.class);
+    system.registerGenericUDTF("execute_splits", GenericUDTFExecuteSplits.class);
 
     //PTF declarations
     system.registerGenericUDF(LEAD_FUNC_NAME, GenericUDFLead.class);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HiveSplitGenerator.java
Patch:
@@ -92,6 +92,7 @@ public void initializeSplitGenerator(Configuration conf, MapWork work) throws IO
 
     this.conf = conf;
     this.work = work;
+    this.jobConf = new JobConf(conf);
 
     // TODO RSHACK - assuming grouping enabled always.
     userPayloadProto = MRInputUserPayloadProto.newBuilder().setGroupingEnabled(true).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java
Patch:
@@ -95,7 +95,9 @@ public MapRecordProcessor(final JobConf jconf, final ProcessorContext context) t
     super(jconf, context);
     String queryId = HiveConf.getVar(jconf, HiveConf.ConfVars.HIVEQUERYID);
     if (LlapProxy.isDaemon()) { // do not cache plan
-      jconf.set(LlapOutputFormat.LLAP_OF_ID_KEY, queryId + "_" + context.getTaskIndex());
+      String id = queryId + "_" + context.getTaskIndex();
+      l4j.info("LLAP_OF_ID: "+id);
+      jconf.set(LlapOutputFormat.LLAP_OF_ID_KEY, id);
       cache = new org.apache.hadoop.hive.ql.exec.mr.ObjectCache();
     } else {
       cache = ObjectCacheFactory.getCache(jconf, queryId);

File: ql/src/java/org/apache/tez/dag/api/TaskSpecBuilder.java
Patch:
@@ -17,7 +17,7 @@
 // Proxy class within the tez.api package to access package private methods.
 public class TaskSpecBuilder {
 
-  public TaskSpec constructTaskSpec(DAG dag, String vertexName, int numSplits, ApplicationId appId) {
+  public TaskSpec constructTaskSpec(DAG dag, String vertexName, int numSplits, ApplicationId appId, int index) {
     Vertex vertex = dag.getVertex(vertexName);
     ProcessorDescriptor processorDescriptor = vertex.getProcessorDescriptor();
     List<RootInputLeafOutput<InputDescriptor, InputInitializerDescriptor>> inputs =
@@ -43,7 +43,7 @@ public TaskSpec constructTaskSpec(DAG dag, String vertexName, int numSplits, App
 
     TezDAGID dagId = TezDAGID.getInstance(appId, 0);
     TezVertexID vertexId = TezVertexID.getInstance(dagId, 0);
-    TezTaskID taskId = TezTaskID.getInstance(vertexId, 0);
+    TezTaskID taskId = TezTaskID.getInstance(vertexId, index);
     TezTaskAttemptID taskAttemptId = TezTaskAttemptID.getInstance(taskId, 0);
     return new TaskSpec(taskAttemptId, dag.getName(), vertexName, numSplits, processorDescriptor, inputSpecs, outputSpecs, null);
   }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
Patch:
@@ -141,6 +141,9 @@ public static TableDesc getDefaultTableDesc(CreateTableDesc directoryDesc,
         properties.setProperty(
             serdeConstants.SERIALIZATION_LIB, directoryDesc.getSerName());
       }
+      if (directoryDesc.getSerdeProps() != null) {
+        properties.putAll(directoryDesc.getSerdeProps());
+      }
       if (directoryDesc.getOutputFormat() != null){
         ret.setOutputFileFormatClass(JavaUtils.loadClass(directoryDesc.getOutputFormat()));
       }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRexUtil.java
Patch:
@@ -91,7 +91,7 @@ private static RexNode simplifyCase(RexBuilder rexBuilder, RexCall call) {
     assert newOperands.size() % 2 == 1;
     switch (newOperands.size()) {
     case 1:
-      return newOperands.get(0);
+      return rexBuilder.makeCast(call.getType(), newOperands.get(0));
     }
   trueFalse:
     if (call.getType().getSqlTypeName() == SqlTypeName.BOOLEAN) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveReduceExpressionsRule.java
Patch:
@@ -582,7 +582,7 @@ protected static class RexReplacer extends RexShuttle {
       }
       node = super.visitCall(call);
       if (node != call) {
-        node = RexUtil.simplify(rexBuilder, node);
+        node = HiveRexUtil.simplify(rexBuilder, node);
       }
       return node;
     }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2625,7 +2625,7 @@ public static enum ConfVars {
     LLAP_DAEMON_COMMUNICATOR_NUM_THREADS("hive.llap.daemon.communicator.num.threads", 10,
       "Number of threads to use in LLAP task communicator in Tez AM.",
       "llap.daemon.communicator.num.threads"),
-    LLAP_DAEMON_ALLOW_PERMANENT_FNS("hive.llap.daemon.allow.permanent.fns", true,
+    LLAP_DAEMON_ALLOW_PERMANENT_FNS("hive.llap.daemon.allow.permanent.fns", false,
         "Whether LLAP daemon should localize the resources for permanent UDFs."),
     LLAP_TASK_SCHEDULER_NODE_REENABLE_MIN_TIMEOUT_MS(
       "hive.llap.task.scheduler.node.reenable.min.timeout.ms", "200ms",

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
Patch:
@@ -455,7 +455,7 @@ protected RexNode convert(ExprNodeConstantDesc literal) throws CalciteSemanticEx
         // An alternative would be to throw CboSemanticException and fall back
         // to no CBO.
         RelDataType relType = cluster.getTypeFactory().createSqlType(SqlTypeName.DECIMAL,
-            bd.scale(), unscaled.toString().length());
+            unscaled.toString().length(), bd.scale());
         calciteLiteral = rexBuilder.makeExactLiteral(bd, relType);
       }
       break;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/functions/HiveSqlCountAggFunction.java
Patch:
@@ -30,7 +30,7 @@
 import org.apache.calcite.sql.type.SqlTypeName;
 import org.apache.calcite.util.ImmutableIntList;
 
-public class HiveSqlCountAggFunction extends SqlAggFunction {
+public class HiveSqlCountAggFunction extends SqlAggFunction implements CanAggregateDistinct {
 
   final boolean                isDistinct;
   final SqlReturnTypeInference returnTypeInference;
@@ -52,6 +52,7 @@ public HiveSqlCountAggFunction(boolean isDistinct, SqlReturnTypeInference return
     this.operandTypeInference = operandTypeInference;
   }
 
+  @Override
   public boolean isDistinct() {
     return isDistinct;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/functions/HiveSqlSumAggFunction.java
Patch:
@@ -46,7 +46,7 @@
  * <code>long</code>, <code>float</code>, <code>double</code>), and the result
  * is the same type.
  */
-public class HiveSqlSumAggFunction extends SqlAggFunction {
+public class HiveSqlSumAggFunction extends SqlAggFunction implements CanAggregateDistinct{
   final boolean isDistinct;
   final SqlReturnTypeInference returnTypeInference;
   final SqlOperandTypeInference operandTypeInference;
@@ -70,7 +70,7 @@ public HiveSqlSumAggFunction(boolean isDistinct, SqlReturnTypeInference returnTy
   }
 
   //~ Methods ----------------------------------------------------------------
-
+  @Override
   public boolean isDistinct() {
     return isDistinct;
   }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -907,7 +907,7 @@ public static enum ConfVars {
   "Default file format for CREATE TABLE statement applied to managed tables only. External tables will be \n" +
   "created with format specified by hive.default.fileformat. Leaving this null will result in using hive.default.fileformat \n" +
   "for all tables."),
-    HIVEQUERYRESULTFILEFORMAT("hive.query.result.fileformat", "TextFile", new StringSet("TextFile", "SequenceFile", "RCfile"),
+    HIVEQUERYRESULTFILEFORMAT("hive.query.result.fileformat", "SequenceFile", new StringSet("TextFile", "SequenceFile", "RCfile"),
         "Default file format for storing result of the query."),
     HIVECHECKFILEFORMAT("hive.fileformat.check", true, "Whether to check file format or not when loading data files"),
 

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -380,6 +380,7 @@ public enum ErrorMsg {
   TXN_ABORTED(10263, "Transaction manager has aborted the transaction {0}.", true),
   DBTXNMGR_REQUIRES_CONCURRENCY(10264,
       "To use DbTxnManager you must set hive.support.concurrency=true"),
+  TXNMGR_NOT_ACID(10265, "This command is not allowed on an ACID table {0}.{1} with a non-ACID transaction manager", true),
 
   LOCK_NO_SUCH_LOCK(10270, "No record of lock {0} could be found, " +
       "may have timed out", true),

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -350,7 +350,7 @@ public enum ErrorMsg {
   TABLE_NOT_PARTITIONED(10241, "Table {0} is not a partitioned table", true),
   DATABSAE_ALREADY_EXISTS(10242, "Database {0} already exists", true),
   CANNOT_REPLACE_COLUMNS(10243, "Replace columns is not supported for table {0}. SerDe may be incompatible.", true),
-  BAD_LOCATION_VALUE(10244, "{0}  is not absolute or has no scheme information.  Please specify a complete absolute uri with scheme information."),
+  BAD_LOCATION_VALUE(10244, "{0}  is not absolute.  Please specify a complete absolute uri."),
   UNSUPPORTED_ALTER_TBL_OP(10245, "{0} alter table options is not supported"),
   INVALID_BIGTABLE_MAPJOIN(10246, "{0} table chosen for streaming is not valid", true),
   MISSING_OVER_CLAUSE(10247, "Missing over clause for function : "),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -3504,8 +3504,7 @@ private int alterTableOrSinglePartition(AlterTableDesc alterTbl, Table tbl, Part
       String newLocation = alterTbl.getNewLocation();
       try {
         URI locUri = new URI(newLocation);
-        if (!locUri.isAbsolute() || locUri.getScheme() == null
-            || locUri.getScheme().trim().equals("")) {
+        if (!new Path(locUri).isAbsolute()) {
           throw new HiveException(ErrorMsg.BAD_LOCATION_VALUE, newLocation);
         }
         sd.setLocation(newLocation);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java
Patch:
@@ -187,7 +187,9 @@ public void initialize(HiveConf hiveConf) {
         HiveConf.ConfVars.TEZ_OPTIMIZE_BUCKET_PRUNING)
         && HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTPPD)
         && HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTINDEXFILTER)) {
-      transformations.add(new FixedBucketPruningOptimizer());
+      final boolean compatMode =
+          HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.TEZ_OPTIMIZE_BUCKET_PRUNING_COMPAT);
+      transformations.add(new FixedBucketPruningOptimizer(compatMode));
     }
 
     if(HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTREDUCEDEDUPLICATION)) {

File: ql/src/java/org/apache/hadoop/hive/ql/ppd/PredicatePushDown.java
Patch:
@@ -61,8 +61,8 @@
  * the join is unnecessarily processing these rows. With predicate pushdown,
  * these two predicates will be processed before the join.
  *
- * Predicate pushdown is enabled by setting hive.optimize.ppd to true. It is
- * disable by default.
+ * Predicate pushdown is disabled by setting hive.optimize.ppd to false. It is
+ * enabled by default.
  *
  * The high-level algorithm is describe here - An operator is processed after
  * all its children have been processed - An operator processes its own

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java
Patch:
@@ -27,6 +27,7 @@
 import java.util.Set;
 
 import org.apache.commons.compress.utils.CharsetNames;
+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -133,7 +134,7 @@ public static Map<String, String> initiateSparkConf(HiveConf hiveConf) {
         LOG.info(String.format(
           "load yarn property from hive configuration in %s mode (%s -> %s).",
           sparkMaster, propertyName, value));
-      } else if (propertyName.equals(HiveConf.ConfVars.HADOOPFS.varname)) {
+      } else if (propertyName.equals(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY)) {
         String value = hiveConf.get(propertyName);
         if (value != null && !value.isEmpty()) {
           sparkConf.put("spark.hadoop." + propertyName, value);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java
Patch:
@@ -386,11 +386,11 @@ private Map<String, List<String>> getOperatorCounters() {
       for (Operator<? extends OperatorDesc> operator : work.getAllOperators()) {
         if (operator instanceof FileSinkOperator) {
           for (FileSinkOperator.Counter counter : FileSinkOperator.Counter.values()) {
-            hiveCounters.add(counter.toString());
+            hiveCounters.add(((FileSinkOperator) operator).getCounterName(counter));
           }
         } else if (operator instanceof ReduceSinkOperator) {
           for (ReduceSinkOperator.Counter counter : ReduceSinkOperator.Counter.values()) {
-            hiveCounters.add(counter.toString());
+            hiveCounters.add(((ReduceSinkOperator) operator).getCounterName(counter, conf));
           }
         } else if (operator instanceof ScriptOperator) {
           for (ScriptOperator.Counter counter : ScriptOperator.Counter.values()) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java
Patch:
@@ -161,7 +161,6 @@ private MapJoinTableContainer load(FileSystem fs, Path path,
     }
     MapJoinTableContainer mapJoinTable = SmallTableCache.get(path);
     if (mapJoinTable == null) {
-      // TODO#: HERE?
       synchronized (path.toString().intern()) {
         mapJoinTable = SmallTableCache.get(path);
         if (mapJoinTable == null) {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionPool.java
Patch:
@@ -152,7 +152,7 @@ public void testLlapSessionQueuing() {
       poolManager.setupPool(conf);
       poolManager.startPool();
     } catch (Exception e) {
-      e.printStackTrace();
+      LOG.error("Initialization error", e);
       fail();
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java
Patch:
@@ -161,6 +161,7 @@ private MapJoinTableContainer load(FileSystem fs, Path path,
     }
     MapJoinTableContainer mapJoinTable = SmallTableCache.get(path);
     if (mapJoinTable == null) {
+      // TODO#: HERE?
       synchronized (path.toString().intern()) {
         mapJoinTable = SmallTableCache.get(path);
         if (mapJoinTable == null) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HashTableLoader.java
Patch:
@@ -206,9 +206,10 @@ public void load(MapJoinTableContainer[] mapJoinTables,
 
         LOG.info("Using tableContainer " + tableContainer.getClass().getSimpleName());
 
+        tableContainer.setSerde(keyCtx, valCtx);
         while (kvReader.next()) {
-          tableContainer.putRow(keyCtx, (Writable)kvReader.getCurrentKey(),
-              valCtx, (Writable)kvReader.getCurrentValue());
+          tableContainer.putRow(
+              (Writable)kvReader.getCurrentKey(), (Writable)kvReader.getCurrentValue());
         }
         tableContainer.seal();
         mapJoinTables[pos] = tableContainer;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashTableLoader.java
Patch:
@@ -93,10 +93,10 @@ public void load(MapJoinTableContainer[] mapJoinTables,
         VectorMapJoinFastTableContainer vectorMapJoinFastTableContainer =
                 new VectorMapJoinFastTableContainer(desc, hconf, keyCount);
 
+        vectorMapJoinFastTableContainer.setSerde(null, null); // No SerDes here.
         while (kvReader.next()) {
-          vectorMapJoinFastTableContainer.putRow(
-              null, (BytesWritable) kvReader.getCurrentKey(),
-              null, (BytesWritable) kvReader.getCurrentValue());
+          vectorMapJoinFastTableContainer.putRow((BytesWritable)kvReader.getCurrentKey(),
+              (BytesWritable)kvReader.getCurrentValue());
         }
 
         vectorMapJoinFastTableContainer.seal();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -781,7 +781,7 @@ public TableSpec(Hive db, HiveConf conf, ASTNode ast, boolean allowDynamicPartit
         throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(ast
             .getChild(0)), ite);
       } catch (HiveException e) {
-        throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg(ast
+        throw new SemanticException(ErrorMsg.CANNOT_RETRIEVE_TABLE_METADATA.getMsg(ast
             .getChild(childIndex), e.getMessage()), e);
       }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java
Patch:
@@ -166,7 +166,7 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
           + " and then copied to " + toURI.toString());
     } catch (Exception e) {
       throw new SemanticException(
-          ErrorMsg.GENERIC_ERROR
+          ErrorMsg.IO_ERROR
               .getMsg("Exception while writing out the local file"), e);
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
Patch:
@@ -237,7 +237,7 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
     } catch (SemanticException e) {
       throw e;
     } catch (Exception e) {
-      throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg(), e);
+      throw new SemanticException(ErrorMsg.IMPORT_SEMANTIC_ERROR.getMsg(), e);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java
Patch:
@@ -108,7 +108,7 @@ public void compile(final ParseContext pCtx, final List<Task<? extends Serializa
      */
     if (pCtx.getQueryProperties().isQuery() && !isCStats) {
       if ((!loadTableWork.isEmpty()) || (loadFileWork.size() != 1)) {
-        throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg());
+        throw new SemanticException(ErrorMsg.INVALID_LOAD_TABLE_FILE_WORK.getMsg());
       }
 
       LoadFileDesc loadFileDesc = loadFileWork.get(0);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
Patch:
@@ -430,7 +430,7 @@ public void validate(HiveConf conf)
             .getMsg());
         }
       } catch (ClassNotFoundException e) {
-        throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg(), e);
+        throw new SemanticException(ErrorMsg.CLASSPATH_ERROR.getMsg(), e);
       }
     }
 

File: llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
Patch:
@@ -63,7 +63,6 @@
 import org.apache.hadoop.yarn.api.records.LocalResource;
 import org.apache.tez.common.TezTaskUmbilicalProtocol;
 import org.apache.tez.common.security.JobTokenSecretManager;
-import org.apache.tez.dag.api.TaskCommunicatorContext;
 import org.apache.tez.dag.api.TezConfiguration;
 import org.apache.tez.dag.api.TezException;
 import org.apache.tez.dag.api.TezUncheckedException;
@@ -75,6 +74,7 @@
 import org.apache.tez.runtime.api.impl.TezHeartbeatResponse;
 import org.apache.tez.serviceplugins.api.ContainerEndReason;
 import org.apache.tez.serviceplugins.api.TaskAttemptEndReason;
+import org.apache.tez.serviceplugins.api.TaskCommunicatorContext;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/helpers/SourceStateTracker.java
Patch:
@@ -24,16 +24,15 @@
 import java.util.Set;
 
 import org.apache.commons.lang3.mutable.MutableInt;
-import org.apache.hadoop.hive.llap.daemon.impl.QueryIdentifier;
 import org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryIdentifierProto;
+import org.apache.tez.serviceplugins.api.TaskCommunicatorContext;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.llap.LlapNodeId;
 import org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo;
 import org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.SourceStateUpdatedRequestProto;
 import org.apache.hadoop.hive.llap.tezplugins.Converters;
 import org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator;
-import org.apache.tez.dag.api.TaskCommunicatorContext;
 import org.apache.tez.dag.api.event.VertexState;
 import org.apache.tez.mapreduce.input.MRInput;
 import org.apache.tez.mapreduce.input.MRInputLegacy;

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java
Patch:
@@ -350,13 +350,15 @@ private TaskStatusUpdateEvent getStatusUpdateEvent(boolean sendCounters) {
       float progress = 0;
       if (task.hasInitialized()) {
         progress = task.getProgress();
+        // TODO HIVE-12449. Make use of progress notifications once Hive starts sending them out.
+        // progressNotified = task.getAndClearProgressNotification();
         if (sendCounters) {
           // send these potentially large objects at longer intervals to avoid overloading the AM
           counters = task.getCounters();
           stats = task.getTaskStatistics();
         }
       }
-      return new TaskStatusUpdateEvent(counters, progress, stats);
+      return new TaskStatusUpdateEvent(counters, progress, stats, true);
     }
 
     /**

File: llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java
Patch:
@@ -172,7 +172,8 @@ private void run(String[] args) throws Exception {
     }
 
     if (options.getCache() != -1) {
-      conf.setLong(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname, options.getCache());
+      conf.set(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname,
+          Long.toString(options.getCache()));
     }
 
     if (options.getXmx() != -1) {

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java
Patch:
@@ -311,7 +311,7 @@ public static void main(String[] args) throws Exception {
       long executorMemoryBytes = HiveConf.getIntVar(
           daemonConf, ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB) * 1024l * 1024l;
 
-      long ioMemoryBytes = HiveConf.getLongVar(daemonConf, ConfVars.LLAP_IO_MEMORY_MAX_SIZE);
+      long ioMemoryBytes = HiveConf.getSizeVar(daemonConf, ConfVars.LLAP_IO_MEMORY_MAX_SIZE);
       boolean isDirectCache = HiveConf.getBoolVar(daemonConf, ConfVars.LLAP_ALLOCATOR_DIRECT);
       boolean llapIoEnabled = HiveConf.getBoolVar(daemonConf, HiveConf.ConfVars.LLAP_IO_ENABLED);
       llapDaemon = new LlapDaemon(daemonConf, numExecutors, executorMemoryBytes, llapIoEnabled,

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
Patch:
@@ -513,7 +513,7 @@ private static String getDbAndTableName(Path path) {
   private void validateFileMetadata() throws IOException {
     if (fileMetadata.getCompressionKind() == CompressionKind.NONE) return;
     int bufferSize = fileMetadata.getCompressionBufferSize();
-    int minAllocSize = HiveConf.getIntVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_MIN_ALLOC);
+    long minAllocSize = HiveConf.getSizeVar(conf, ConfVars.LLAP_ALLOCATOR_MIN_ALLOC);
     if (bufferSize < minAllocSize) {
       LOG.warn("ORC compression buffer size (" + bufferSize + ") is smaller than LLAP low-level "
             + "cache minimum allocation size (" + minAllocSize + "). Decrease the value for "

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1649,7 +1649,7 @@ public static enum ConfVars {
     HIVE_AUTHORIZATION_ENABLED("hive.security.authorization.enabled", false,
         "enable or disable the Hive client authorization"),
     HIVE_AUTHORIZATION_MANAGER("hive.security.authorization.manager",
-        "org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider",
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory",
         "The Hive client authorization manager class name. The user defined authorization class should implement \n" +
         "interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider."),
     HIVE_AUTHENTICATOR_MANAGER("hive.security.authenticator.manager",

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/TestHCatAuthUtil.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;
+import org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerFactory;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException;
@@ -49,12 +50,13 @@ public HiveAuthorizer createHiveAuthorizer(HiveMetastoreClientFactory metastoreC
   }
 
   /**
-   * Test with auth enabled and v1 auth
+   * Test with auth enabled and StorageBasedAuthorizationProvider
    */
   @Test
   public void authEnabledV1Auth() throws Exception {
     HiveConf hcatConf = new HiveConf(this.getClass());
     hcatConf.setBoolVar(ConfVars.HIVE_AUTHORIZATION_ENABLED, true);
+    hcatConf.setVar(ConfVars.HIVE_AUTHORIZATION_MANAGER, StorageBasedAuthorizationProvider.class.getName());
     SessionState.start(hcatConf);
     assertTrue("hcat auth should be enabled", HCatAuthUtil.isAuthorizationEnabled(hcatConf));
   }

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestMetastoreAuthorizationProvider.java
Patch:
@@ -90,6 +90,8 @@ protected void setUp() throws Exception {
         AuthorizationPreEventListener.class.getName());
     System.setProperty(HiveConf.ConfVars.HIVE_METASTORE_AUTHORIZATION_MANAGER.varname,
         getAuthorizationProvider());
+    System.setProperty(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER.varname,
+        getAuthorizationProvider());
     setupMetaStoreReadAuthorization();
     System.setProperty(HiveConf.ConfVars.HIVE_METASTORE_AUTHENTICATOR_MANAGER.varname,
         InjectableDummyAuthenticator.class.getName());

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
Patch:
@@ -132,6 +132,7 @@ public static void setUpBeforeClass() throws SQLException, ClassNotFoundExceptio
     Connection con1 = getConnection("default");
     System.setProperty(ConfVars.HIVE_SERVER2_LOGGING_OPERATION_LEVEL.varname, "verbose");
     System.setProperty(ConfVars.HIVEMAPREDMODE.varname, "nonstrict");
+    System.setProperty(ConfVars.HIVE_AUTHORIZATION_MANAGER.varname, "org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider");
 
     Statement stmt1 = con1.createStatement();
     assertNotNull("Statement is null", stmt1);

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -726,6 +726,8 @@ public void clearTablesCreatedDuringTests() throws Exception {
       return;
     }
 
+    db.getConf().set("hive.metastore.filter.hook",
+        "org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl");
     // Delete any tables other than the source tables
     // and any databases other than the default database.
     for (String dbName : db.getAllDatabases()) {

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/TestHs2Metrics.java
Patch:
@@ -75,12 +75,12 @@ public static void setup() throws Exception {
     confOverlay = new HashMap<String, String>();
     confOverlay.put(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     confOverlay.put(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK.varname, MetricCheckingHook.class.getName());
+    confOverlay.put(HiveConf.ConfVars.HIVE_SERVER2_METRICS_ENABLED.varname, "true");
+    confOverlay.put(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     miniHS2.start(confOverlay);
 
     HiveConf conf = new HiveConf();
-    conf.setBoolVar(HiveConf.ConfVars.HIVE_SERVER2_METRICS_ENABLED, true);
-    conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
-    MetricsFactory.init(conf);
+
 
     metrics = (CodahaleMetrics) MetricsFactory.getInstance();
   }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2430,7 +2430,8 @@ public static enum ConfVars {
       "Number of RPC handlers for LLAP daemon.", "llap.daemon.rpc.num.handlers"),
     LLAP_DAEMON_WORK_DIRS("hive.llap.daemon.work.dirs", "",
       "Working directories for the daemon. Needs to be set for a secure cluster, since LLAP may\n" +
-      "not have access to the default YARN working directories.", "llap.daemon.work.dirs"),
+      "not have access to the default YARN working directories. yarn.nodemanager.local-dirs is\n" +
+      "used if this is not set", "llap.daemon.work.dirs"),
     LLAP_DAEMON_YARN_SHUFFLE_PORT("hive.llap.daemon.yarn.shuffle.port", 15551,
       "YARN shuffle port for LLAP-daemon-hosted shuffle.", "llap.daemon.yarn.shuffle.port"),
     LLAP_DAEMON_YARN_CONTAINER_MB("hive.llap.daemon.yarn.container.mb", -1,

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java
Patch:
@@ -286,6 +286,9 @@ public static void main(String[] args) throws Exception {
       int numExecutors = HiveConf.getIntVar(daemonConf, ConfVars.LLAP_DAEMON_NUM_EXECUTORS);
 
       String localDirList = HiveConf.getVar(daemonConf, ConfVars.LLAP_DAEMON_WORK_DIRS);
+      if (localDirList == null || localDirList.isEmpty()) {
+        localDirList = daemonConf.get("yarn.nodemanager.local-dirs");
+      }
       String[] localDirs = (localDirList == null || localDirList.isEmpty()) ?
           new String[0] : StringUtils.getTrimmedStrings(localDirList);
       int rpcPort = HiveConf.getIntVar(daemonConf, ConfVars.LLAP_DAEMON_RPC_PORT);

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemonProtocolServerImpl.java
Patch:
@@ -215,6 +215,7 @@ private RPC.Server createServer(Class<?> pbProtocol, InetSocketAddress addr, Con
     return server;
   }
 
+
   @Override
   public GetTokenResponseProto getDelegationToken(RpcController controller,
       GetTokenRequestProto request) throws ServiceException {

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/services/impl/LlapWebServices.java
Patch:
@@ -43,7 +43,6 @@ public LlapWebServices() {
 
   @Override
   public void serviceInit(Configuration conf) {
-
     this.conf = new Configuration(conf);
     this.conf.addResource(YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);
 
@@ -55,7 +54,6 @@ public void serviceInit(Configuration conf) {
   @Override
   public void serviceStart() throws Exception {
     String bindAddress = "0.0.0.0";
-    Configuration conf = getConfig();
     if (UserGroupInformation.isSecurityEnabled()
         && HiveConf.getBoolVar(conf, ConfVars.LLAP_WEB_AUTO_AUTH)) {
       conf.set("hadoop.http.authentication.type", "kerberos");

File: beeline/src/java/org/apache/hive/beeline/BeeLine.java
Patch:
@@ -988,6 +988,7 @@ public ConsoleReader getConsoleReader(InputStream inputStream) throws IOExceptio
       InputStream inputStreamAppendedNewline = new SequenceInputStream(inputStream,
           new ByteArrayInputStream((new String("\n")).getBytes()));
       consoleReader = new ConsoleReader(inputStreamAppendedNewline, getOutputStream());
+      consoleReader.setCopyPasteDetection(true); // jline will detect if <tab> is regular character
     } else {
       consoleReader = new ConsoleReader();
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -1620,7 +1620,7 @@ private RelNode genTableLogicalPlan(String tableAlias, QB qb) throws SemanticExc
           VirtualColumn vc = vcs.next();
           colInfo = new ColumnInfo(vc.getName(), vc.getTypeInfo(), tableAlias, true,
               vc.getIsHidden());
-          rr.put(tableAlias, vc.getName(), colInfo);
+          rr.put(tableAlias, vc.getName().toLowerCase(), colInfo);
           cInfoLst.add(colInfo);
           virtualCols.add(vc);
         }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/RowResolver.java
Patch:
@@ -112,7 +112,6 @@ public boolean addMappingOnly(String tab_alias, String col_alias, ColumnInfo col
     if (tab_alias != null) {
       tab_alias = tab_alias.toLowerCase();
     }
-    col_alias = col_alias.toLowerCase();
 
     /*
      * allow multiple mappings to the same ColumnInfo.
@@ -169,7 +168,6 @@ public boolean hasTableAlias(String tab_alias) {
    * @throws SemanticException
    */
   public ColumnInfo get(String tab_alias, String col_alias) throws SemanticException {
-    col_alias = col_alias.toLowerCase();
     ColumnInfo ret = null;
 
     if (tab_alias != null) {
@@ -476,4 +474,4 @@ public RowResolver duplicate() {
     resolver.isExprResolver = isExprResolver;
     return resolver;
   }
-}
+}
\ No newline at end of file

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -378,7 +378,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
       default:
         // HiveParser.identifier | HiveParse.KW_IF | HiveParse.KW_LEFT |
         // HiveParse.KW_RIGHT
-        str = BaseSemanticAnalyzer.unescapeIdentifier(expr.getText());
+        str = BaseSemanticAnalyzer.unescapeIdentifier(expr.getText().toLowerCase());
         break;
       }
       return new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, str);
@@ -818,7 +818,7 @@ static ExprNodeDesc getFuncExprNodeDescWithUdfData(String udfName, TypeInfo type
           ((SettableUDF)genericUDF).setTypeInfo(typeInfo);
         }
       }
-      
+
       List<ExprNodeDesc> childrenList = new ArrayList<ExprNodeDesc>(children.length);
 
       childrenList.addAll(Arrays.asList(children));

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveOperationType.java
Patch:
@@ -57,6 +57,7 @@ public enum HiveOperationType {
   ALTERPARTITION_SERDEPROPERTIES,
   ALTERTABLE_CLUSTER_SORT,
   ANALYZE_TABLE,
+  CACHE_METADATA,
   ALTERTABLE_BUCKETNUM,
   ALTERPARTITION_BUCKETNUM,
   ALTERTABLE_UPDATETABLESTATS,

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/Operation2Privilege.java
Patch:
@@ -255,6 +255,8 @@ public HivePrivilegeObjectType getObjectType() {
 (OWNER_PRIV_AR, OWNER_PRIV_AR));
 
     op2Priv.put(HiveOperationType.ANALYZE_TABLE, PrivRequirement.newIOPrivRequirement
+(arr(SQLPrivTypeGrant.SELECT_NOGRANT, SQLPrivTypeGrant.INSERT_NOGRANT), null));
+    op2Priv.put(HiveOperationType.CACHE_METADATA, PrivRequirement.newIOPrivRequirement
 (arr(SQLPrivTypeGrant.SELECT_NOGRANT, SQLPrivTypeGrant.INSERT_NOGRANT), null));
     op2Priv.put(HiveOperationType.SHOWDATABASES, PrivRequirement.newIOPrivRequirement
 (null, null));

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNvl.java
Patch:
@@ -64,11 +64,11 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
   @Override
   public String getDisplayString(String[] children) {
     StringBuilder sb = new StringBuilder();
-    sb.append("if ");
+    sb.append("NVL(");
     sb.append(children[0]);
-    sb.append(" is null ");
-    sb.append("returns");
+    sb.append(',');
     sb.append(children[1]);
+    sb.append(')');
     return sb.toString() ;
   }
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1646,7 +1646,7 @@ public static enum ConfVars {
     HIVE_AUTHORIZATION_ENABLED("hive.security.authorization.enabled", false,
         "enable or disable the Hive client authorization"),
     HIVE_AUTHORIZATION_MANAGER("hive.security.authorization.manager",
-        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory",
+        "org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider",
         "The Hive client authorization manager class name. The user defined authorization class should implement \n" +
         "interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider."),
     HIVE_AUTHENTICATOR_MANAGER("hive.security.authenticator.manager",

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;
+import org.apache.hadoop.hive.ql.session.SessionState;
 
 final class HCatAuthUtil {
   public static boolean isAuthorizationEnabled(Configuration conf) {
@@ -31,7 +31,6 @@ public static boolean isAuthorizationEnabled(Configuration conf) {
     // additional checks if a V2 authorizer is in use. The reccomended configuration is to
     // use storage based authorization in metastore server
     return HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)
-        && HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER)
-        == StorageBasedAuthorizationProvider.class.getName();
+        && SessionState.get().getAuthorizer() != null;
   }
 }

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/TestHCatAuthUtil.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;
-import org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerFactory;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException;
@@ -50,13 +49,12 @@ public HiveAuthorizer createHiveAuthorizer(HiveMetastoreClientFactory metastoreC
   }
 
   /**
-   * Test with auth enabled and StorageBasedAuthorizationProvider
+   * Test with auth enabled and v1 auth
    */
   @Test
   public void authEnabledV1Auth() throws Exception {
     HiveConf hcatConf = new HiveConf(this.getClass());
     hcatConf.setBoolVar(ConfVars.HIVE_AUTHORIZATION_ENABLED, true);
-    hcatConf.setVar(ConfVars.HIVE_AUTHORIZATION_MANAGER, StorageBasedAuthorizationProvider.class.getName());
     SessionState.start(hcatConf);
     assertTrue("hcat auth should be enabled", HCatAuthUtil.isAuthorizationEnabled(hcatConf));
   }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortJoinReduceRule.java
Patch:
@@ -57,8 +57,9 @@ public boolean matches(RelOptRuleCall call) {
     final HiveSortLimit sortLimit = call.rel(0);
     final HiveJoin join = call.rel(1);
 
-    // If sort does not contain a limit operation, we bail out
-    if (!HiveCalciteUtil.limitRelNode(sortLimit)) {
+    // If sort does not contain a limit operation or limit is 0, we bail out
+    if (!HiveCalciteUtil.limitRelNode(sortLimit) ||
+            RexLiteral.intValue(sortLimit.fetch) == 0) {
       return false;
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
Patch:
@@ -1271,7 +1271,7 @@ public String toString() {
     return getName() + "[" + getIdentifier() + "]";
   }
 
-  public static String toString(Collection<Operator<? extends OperatorDesc>> top) {
+  public static String toString(Collection<TableScanOperator> top) {
     StringBuilder builder = new StringBuilder();
     Set<String> visited = new HashSet<String>();
     for (Operator<?> op : top) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java
Patch:
@@ -194,7 +194,7 @@ protected boolean checkConvertBucketMapJoin(
     LinkedHashMap<String, List<List<String>>> tblAliasToBucketedFilePathsInEachPartition =
         new LinkedHashMap<String, List<List<String>>>();
 
-    HashMap<String, Operator<? extends OperatorDesc>> topOps = pGraphContext.getTopOps();
+    HashMap<String, TableScanOperator> topOps = pGraphContext.getTopOps();
 
     HashMap<String, String> aliasToNewAliasMap = new HashMap<String, String>();
 
@@ -228,7 +228,7 @@ protected boolean checkConvertBucketMapJoin(
 
       // For nested sub-queries, the alias mapping is not maintained in QB currently.
       if (topOps.containsValue(tso)) {
-        for (Map.Entry<String, Operator<? extends OperatorDesc>> topOpEntry : topOps.entrySet()) {
+        for (Map.Entry<String, TableScanOperator> topOpEntry : topOps.entrySet()) {
           if (topOpEntry.getValue() == tso) {
             String newAlias = topOpEntry.getKey();
             if (!newAlias.equals(alias)) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractSMBJoinProc.java
Patch:
@@ -291,7 +291,7 @@ private boolean isEligibleForBucketSortMergeJoin(
      * The table alias should be subq2:subq1:a which needs to be fetched from topOps.
      */
     if (pGraphContext.getTopOps().containsValue(tso)) {
-      for (Map.Entry<String, Operator<? extends OperatorDesc>> topOpEntry :
+      for (Map.Entry<String, TableScanOperator> topOpEntry :
         this.pGraphContext.getTopOps().entrySet()) {
         if (topOpEntry.getValue() == tso) {
           alias = topOpEntry.getKey();
@@ -444,13 +444,13 @@ protected boolean canConvertJoinToBucketMapJoin(
       String selector = HiveConf.getVar(pGraphContext.getConf(),
           HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN_BIGTABLE_SELECTOR);
       bigTableMatcherClass =
-        (Class<? extends BigTableSelectorForAutoSMJ>) JavaUtils.loadClass(selector);
+        JavaUtils.loadClass(selector);
     } catch (ClassNotFoundException e) {
       throw new SemanticException(e.getMessage());
     }
 
     BigTableSelectorForAutoSMJ bigTableMatcher =
-      (BigTableSelectorForAutoSMJ) ReflectionUtils.newInstance(bigTableMatcherClass, null);
+      ReflectionUtils.newInstance(bigTableMatcherClass, null);
     JoinDesc joinDesc = joinOp.getConf();
     JoinCondDesc[] joinCondns = joinDesc.getConds();
     Set<Integer> joinCandidates = MapJoinProcessor.getBigTableCandidates(joinCondns);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java
Patch:
@@ -77,7 +77,7 @@ private Object processMapOnlyUnion(UnionOperator union, Stack<Node> stack,
 
     UnionParseContext uPrsCtx = uCtx.getUnionParseContext(union);
     ctx.getMapCurrCtx().put(
-        (Operator<? extends OperatorDesc>) union,
+        union,
         new GenMapRedCtx(ctx.getCurrTask(),
             ctx.getCurrAliasId()));
 
@@ -170,7 +170,7 @@ private void processSubQueryUnionMerge(GenMRProcContext ctx,
     // plan
     Task<? extends Serializable> uTask = uCtxTask.getUTask();
     ctx.setCurrTask(uTask);
-    Operator<? extends OperatorDesc> topOp = ctx.getCurrTopOp();
+    TableScanOperator topOp = ctx.getCurrTopOp();
     if (topOp != null && !ctx.isSeenOp(uTask, topOp)) {
       GenMapRedUtils.setTaskPlan(ctx.getCurrAliasId(), ctx
           .getCurrTopOp(), uTask, false, ctx);
@@ -189,6 +189,7 @@ private void processSubQueryUnionMerge(GenMRProcContext ctx,
    * @param opProcCtx
    *          context
    */
+  @Override
   public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
       Object... nodeOutputs) throws SemanticException {
     UnionOperator union = (UnionOperator) nd;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/SkewJoinOptimizer.java
Patch:
@@ -190,7 +190,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
 
       // Update the topOps appropriately
       Map<String, Operator<? extends OperatorDesc>> topOps = getTopOps(joinOpClone);
-      Map<String, Operator<? extends OperatorDesc>> origTopOps = parseContext.getTopOps();
+      Map<String, TableScanOperator> origTopOps = parseContext.getTopOps();
 
       for (Entry<String, Operator<? extends OperatorDesc>> topOp : topOps.entrySet()) {
         TableScanOperator tso = (TableScanOperator) topOp.getValue();
@@ -283,7 +283,7 @@ private boolean getTableScanOps(
      * @param op The join operator being optimized
      * @param tableScanOpsForJoin table scan operators which are parents of the join operator
      * @return map<join keys intersection skewedkeys, list of skewed values>.
-     * @throws SemanticException 
+     * @throws SemanticException
      */
     private Map<List<ExprNodeDesc>, List<List<String>>>
       getSkewedValues(
@@ -406,7 +406,7 @@ private TableScanOperator getTableScanOperator(
             return tsOp;
           }
         }
-        if ((op.getParentOperators() == null) || (op.getParentOperators().isEmpty()) || 
+        if ((op.getParentOperators() == null) || (op.getParentOperators().isEmpty()) ||
             (op.getParentOperators().size() > 1)) {
           return null;
         }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java
Patch:
@@ -49,7 +49,6 @@
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.FileSinkDesc;
 import org.apache.hadoop.hive.ql.plan.MapWork;
-import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.ReduceWork;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.TezEdgeProperty;
@@ -179,7 +178,7 @@ public MapWork createMapWork(GenTezProcContext context, Operator<?> root,
 
     String alias = ts.getConf().getAlias();
 
-    setupMapWork(mapWork, context, partitions, root, alias);
+    setupMapWork(mapWork, context, partitions, ts, alias);
 
     if (ts.getConf().getTableMetadata() != null && ts.getConf().getTableMetadata().isDummyTable()) {
       mapWork.setDummyTableScan(true);
@@ -197,7 +196,7 @@ public MapWork createMapWork(GenTezProcContext context, Operator<?> root,
 
   // this method's main use is to help unit testing this class
   protected void setupMapWork(MapWork mapWork, GenTezProcContext context,
-      PrunedPartitionList partitions, Operator<? extends OperatorDesc> root,
+      PrunedPartitionList partitions, TableScanOperator root,
       String alias) throws SemanticException {
     // All the setup is done in GenMapRedUtils
     GenMapRedUtils.setMapWork(mapWork, context.parseContext,

File: ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkProcContext.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
 import org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.exec.UnionOperator;
@@ -135,7 +136,7 @@ public class GenSparkProcContext implements NodeProcessorCtx {
 
   // Alias to operator map, from the semantic analyzer.
   // This is necessary as sometimes semantic analyzer's mapping is different than operator's own alias.
-  public final Map<String, Operator<? extends OperatorDesc>> topOps;
+  public final Map<String, TableScanOperator> topOps;
 
   // The set of pruning sinks
   public final Set<Operator<?>> pruningSinkSet;
@@ -151,7 +152,7 @@ public GenSparkProcContext(HiveConf conf,
       List<Task<? extends Serializable>> rootTasks,
       Set<ReadEntity> inputs,
       Set<WriteEntity> outputs,
-      Map<String, Operator<? extends OperatorDesc>> topOps) {
+      Map<String, TableScanOperator> topOps) {
     this.conf = conf;
     this.parseContext = parseContext;
     this.moveTask = moveTask;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java
Patch:
@@ -160,7 +160,7 @@ public MapWork createMapWork(GenSparkProcContext context, Operator<?> root,
     String alias = ((TableScanOperator) root).getConf().getAlias();
 
     if (!deferSetup) {
-      setupMapWork(mapWork, context, partitions, root, alias);
+      setupMapWork(mapWork, context, partitions,(TableScanOperator) root, alias);
     }
 
     // add new item to the Spark work
@@ -171,7 +171,7 @@ public MapWork createMapWork(GenSparkProcContext context, Operator<?> root,
 
   // this method's main use is to help unit testing this class
   protected void setupMapWork(MapWork mapWork, GenSparkProcContext context,
-      PrunedPartitionList partitions, Operator<? extends OperatorDesc> root,
+      PrunedPartitionList partitions, TableScanOperator root,
       String alias) throws SemanticException {
     // All the setup is done in GenMapRedUtils
     GenMapRedUtils.setMapWork(mapWork, context.parseContext,

File: ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java
Patch:
@@ -26,8 +26,6 @@
 import java.util.Set;
 import java.util.Stack;
 
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.exec.ConditionalTask;
@@ -97,7 +95,6 @@
 public class SparkCompiler extends TaskCompiler {
   private static final String CLASS_NAME = SparkCompiler.class.getName();
   private static final PerfLogger PERF_LOGGER = SessionState.getPerfLogger();
-  private static final Logger LOGGER = LoggerFactory.getLogger(SparkCompiler.class);
 
   public SparkCompiler() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
Patch:
@@ -372,7 +372,7 @@ public void replaceRoots(Map<Operator<?>, Operator<?>> replacementMap) {
 
   @Override
   @Explain(displayName = "Map Operator Tree", explainLevels = { Level.USER, Level.DEFAULT, Level.EXTENDED })
-  public Set<Operator<?>> getAllRootOperators() {
+  public Set<Operator<? extends OperatorDesc>> getAllRootOperators() {
     Set<Operator<?>> opSet = new LinkedHashSet<Operator<?>>();
 
     for (Operator<?> op : getAliasToWork().values()) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -11771,9 +11771,9 @@ private BoundarySpec processBoundary(int frameType, ASTNode node)  throws Semant
       else
       {
         int amt = Integer.parseInt(amtNode.getText());
-        if ( amt < 0 ) {
+        if ( amt <= 0 ) {
           throw new SemanticException(
-              "Window Frame Boundary Amount must be a +ve integer, amount provide is: " + amt);
+              "Window Frame Boundary Amount must be a positive integer, provided amount is: " + amt);
         }
         bs.setAmt(amt);
       }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1641,7 +1641,7 @@ public static enum ConfVars {
     HIVE_AUTHORIZATION_ENABLED("hive.security.authorization.enabled", false,
         "enable or disable the Hive client authorization"),
     HIVE_AUTHORIZATION_MANAGER("hive.security.authorization.manager",
-        "org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider",
+        "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory",
         "The Hive client authorization manager class name. The user defined authorization class should implement \n" +
         "interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider."),
     HIVE_AUTHENTICATOR_MANAGER("hive.security.authenticator.manager",

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;
 
 final class HCatAuthUtil {
   public static boolean isAuthorizationEnabled(Configuration conf) {
@@ -31,6 +31,7 @@ public static boolean isAuthorizationEnabled(Configuration conf) {
     // additional checks if a V2 authorizer is in use. The reccomended configuration is to
     // use storage based authorization in metastore server
     return HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)
-        && SessionState.get().getAuthorizer() != null;
+        && HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER)
+        == StorageBasedAuthorizationProvider.class.getName();
   }
 }

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/TestHCatAuthUtil.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;
+import org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerFactory;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException;
@@ -49,12 +50,13 @@ public HiveAuthorizer createHiveAuthorizer(HiveMetastoreClientFactory metastoreC
   }
 
   /**
-   * Test with auth enabled and v1 auth
+   * Test with auth enabled and StorageBasedAuthorizationProvider
    */
   @Test
   public void authEnabledV1Auth() throws Exception {
     HiveConf hcatConf = new HiveConf(this.getClass());
     hcatConf.setBoolVar(ConfVars.HIVE_AUTHORIZATION_ENABLED, true);
+    hcatConf.setVar(ConfVars.HIVE_AUTHORIZATION_MANAGER, StorageBasedAuthorizationProvider.class.getName());
     SessionState.start(hcatConf);
     assertTrue("hcat auth should be enabled", HCatAuthUtil.isAuthorizationEnabled(hcatConf));
   }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -9643,7 +9643,7 @@ public Operator genPlan(QB qb, boolean skipAmbiguityCheck)
          * as Join conditions
          */
         Set<String> dests = qb.getParseInfo().getClauseNames();
-        if ( dests.size() == 1 ) {
+        if ( dests.size() == 1 && joinTree.getNoOuterJoin()) {
           String dest = dests.iterator().next();
           ASTNode whereClause = qb.getParseInfo().getWhrForClause(dest);
           if ( whereClause != null ) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedBatchUtil.java
Patch:
@@ -685,7 +685,7 @@ public static void debugDisplayOneRow(VectorizedRowBatch batch, int index, Strin
     LOG.info(sb.toString());
   }
 
-  public static void debugDisplayBatch(VectorizedRowBatch batch, String prefix) throws HiveException {
+  public static void debugDisplayBatch(VectorizedRowBatch batch, String prefix) {
     for (int i = 0; i < batch.size; i++) {
       int index = (batch.selectedInUse ? batch.selected[i] : i);
       debugDisplayOneRow(batch, index, prefix);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -781,7 +781,7 @@ public static enum ConfVars {
         "hive.txn.valid.txns,hive.script.operator.env.blacklist",
         "Comma separated list of keys from the configuration file not to convert to environment " +
         "variables when envoking the script operator"),
-    HIVEMAPREDMODE("hive.mapred.mode", "nonstrict",
+    HIVEMAPREDMODE("hive.mapred.mode", "strict",
         "The mode in which the Hive operations are being performed. \n" +
         "In strict mode, some risky queries are not allowed to run. They include:\n" +
         "  Cartesian Product.\n" +

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatBaseTest.java
Patch:
@@ -83,7 +83,7 @@ protected void setUpHiveConf() {
     hiveConf.setVar(HiveConf.ConfVars.POSTEXECHOOKS, "");
     hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
     hiveConf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE, TEST_WAREHOUSE_DIR);
-
+    hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     if (Shell.WINDOWS) {
       WindowsPathUtil.convertPathsFromWindowsToHdfs(hiveConf);
     }
@@ -99,7 +99,7 @@ protected void logAndRegister(PigServer server, String query, int lineNumber) th
   }
 
   /**
-   * creates PigServer in LOCAL mode.  
+   * creates PigServer in LOCAL mode.
    * http://pig.apache.org/docs/r0.12.0/perf.html#error-handling
    * @param stopOnFailure equivalent of "-stop_on_failure" command line arg, setting to 'true' makes
    *                      debugging easier

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatExternalDynamicPartitioned.java
Patch:
@@ -19,7 +19,6 @@
 
 package org.apache.hive.hcatalog.mapreduce;
 
-import org.junit.BeforeClass;
 import org.junit.Test;
 
 public class TestHCatExternalDynamicPartitioned extends TestHCatDynamicPartitioned {

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java
Patch:
@@ -105,7 +105,7 @@ public class TestHCatLoader {
         }});
       }};
 
-  private String storageFormat;
+  private final String storageFormat;
 
   @Parameterized.Parameters
   public static Collection<Object[]> generateParameters() {
@@ -176,6 +176,7 @@ public void setup() throws Exception {
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
+    hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
 
     if (Shell.WINDOWS) {
       WindowsPathUtil.convertPathsFromWindowsToHdfs(hiveConf);

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatStorerMulti.java
Patch:
@@ -18,8 +18,6 @@
  */
 package org.apache.hive.hcatalog.pig;
 
-import com.google.common.collect.ImmutableSet;
-
 import java.io.File;
 import java.io.FileWriter;
 import java.io.IOException;
@@ -77,7 +75,7 @@ public class TestHCatStorerMulti {
         }});
       }};
 
-  private String storageFormat;
+  private final String storageFormat;
 
   @Parameterized.Parameters
   public static Collection<Object[]> generateParameters() {
@@ -119,6 +117,7 @@ public void setUp() throws Exception {
       hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
       hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
       hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
+      hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
       driver = new Driver(hiveConf);
       SessionState.start(new CliSessionState(hiveConf));
     }

File: itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestExtendedAcls.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.fs.permission.FsAction;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.junit.Assert;
 import org.junit.BeforeClass;
 
@@ -44,6 +45,7 @@ public static void setup() throws Exception {
     conf = new HiveConf(TestExtendedAcls.class);
     //setup the mini DFS with acl's enabled.
     conf.set("dfs.namenode.acls.enabled", "true");
+    conf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     baseSetup();
   }
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/HBaseIntegrationTests.java
Patch:
@@ -94,6 +94,7 @@ protected void setupDriver() {
         SessionStateConfigUserAuthenticator.class.getName());
     conf.setBoolVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED, true);
     conf.setVar(HiveConf.ConfVars.USERS_IN_ADMIN_ROLE, System.getProperty("user.name"));
+    conf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE,"nonstrict");
     //HBaseReadWrite.setTestConnection(hconn);
 
     SessionState.start(new CliSessionState(conf));

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestMTQueries.java
Patch:
@@ -43,6 +43,7 @@ public void testMTQueries1() throws Exception {
       util.getConf().setBoolean("hive.exec.submitviachild", true);
       util.getConf().setBoolean("hive.exec.submit.local.task.via.child", true);
       util.getConf().set("hive.stats.dbclass", "fs");
+      util.getConf().set("hive.mapred.mode", "nonstrict");
     }
     boolean success = QTestUtil.queryListRunnerMultiThreaded(qfiles, qts);
     if (!success) {

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestClientSideAuthorizationProvider.java
Patch:
@@ -75,7 +75,7 @@ protected void setUp() throws Exception {
     clientHiveConf.set(HiveConf.ConfVars.HIVE_AUTHENTICATOR_MANAGER.varname,
         InjectableDummyAuthenticator.class.getName());
     clientHiveConf.set(HiveConf.ConfVars.HIVE_AUTHORIZATION_TABLE_OWNER_GRANTS.varname, "");
-
+    clientHiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     clientHiveConf.setVar(HiveConf.ConfVars.METASTOREURIS, "thrift://localhost:" + port);
     clientHiveConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);
     clientHiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java
Patch:
@@ -30,6 +30,7 @@ public class TestFolderPermissions extends FolderPermissionBase {
   @BeforeClass
   public static void setup() throws Exception {
     conf = new HiveConf(TestFolderPermissions.class);
+    conf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     baseSetup();
   }
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/authorization/plugin/TestHiveAuthorizerCheckInvocation.java
Patch:
@@ -91,6 +91,7 @@ public static void beforeTest() throws Exception {
     conf.setBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS, false);
     conf.setBoolVar(ConfVars.HIVE_SUPPORT_CONCURRENCY, true);
     conf.setVar(ConfVars.HIVE_TXN_MANAGER, DbTxnManager.class.getName());
+    conf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
 
     SessionState.start(conf);
     driver = new Driver(conf);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java
Patch:
@@ -110,6 +110,7 @@ public void setup() throws Exception {
     hiveConf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE, TEST_WAREHOUSE_DIR);
     hiveConf.setVar(HiveConf.ConfVars.HIVEINPUTFORMAT, HiveInputFormat.class.getName());
     hiveConf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
+    hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     //"org.apache.hadoop.hive.ql.io.HiveInputFormat"
 
     TxnDbUtil.setConfValues(hiveConf);
@@ -343,7 +344,7 @@ public void schemaEvolutionAddColDynamicPartitioningUpdate() throws Exception {
    * 5. Trigger major compaction (which should update stats)
    * 6. check that stats have been updated
    * @throws Exception
-   * todo: 
+   * todo:
    * 2. add non-partitioned test
    * 4. add a test with sorted table?
    */
@@ -391,7 +392,7 @@ public void testStatsAfterCompactionPartTbl() throws Exception {
     su = Worker.StatsUpdater.init(ciPart2, colNames, conf, System.getProperty("user.name"));
     su.gatherStats();//compute stats before compaction
     LOG.debug("List of stats columns after analyze Part2: " + txnHandler.findColumnsWithStats(ci));
-    
+
     //now make sure we get the stats we expect for partition we are going to add data to later
     Map<String, List<ColumnStatisticsObj>> stats = msClient.getPartitionColumnStatistics(ci.dbname,
       ci.tableName, Arrays.asList(ci.partName), colNames);

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
Patch:
@@ -131,6 +131,7 @@ public static void setUpBeforeClass() throws SQLException, ClassNotFoundExceptio
     Class.forName(driverName);
     Connection con1 = getConnection("default");
     System.setProperty(ConfVars.HIVE_SERVER2_LOGGING_OPERATION_LEVEL.varname, "verbose");
+    System.setProperty(ConfVars.HIVEMAPREDMODE.varname, "nonstrict");
 
     Statement stmt1 = con1.createStatement();
     assertNotNull("Statement is null", stmt1);

File: itests/hive-unit/src/test/java/org/apache/hive/service/cli/TestEmbeddedThriftBinaryCLIService.java
Patch:
@@ -36,7 +36,9 @@ public class TestEmbeddedThriftBinaryCLIService extends CLIServiceTest {
   @BeforeClass
   public static void setUpBeforeClass() throws Exception {
     service = new EmbeddedThriftBinaryCLIService();
-    service.init(new HiveConf());
+    HiveConf conf = new HiveConf();
+    conf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
+    service.init(conf);
     client = new ThriftCLIServiceClient(service);
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java
Patch:
@@ -86,6 +86,7 @@ public void setUp() throws Exception {
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
+    hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     TxnDbUtil.setConfValues(hiveConf);
     TxnDbUtil.prepDb();
     File f = new File(TEST_WAREHOUSE_DIR);

File: ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java
Patch:
@@ -81,6 +81,7 @@ public void setUp() throws Exception {
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
     hiveConf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, TEST_WAREHOUSE_DIR);
+    hiveConf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     TxnDbUtil.setConfValues(hiveConf);
     TxnDbUtil.prepDb();
     File f = new File(TEST_WAREHOUSE_DIR);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
Patch:
@@ -397,6 +397,7 @@ public InputSplit[] getSplits(JobConf job, int splits) throws IOException {
   public void testFetchOperatorContext() throws Exception {
     HiveConf conf = new HiveConf();
     conf.set("hive.support.concurrency", "false");
+    conf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     SessionState.start(conf);
     String cmd = "create table fetchOp (id int, name string) " +
         "partitioned by (state string) " +

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestUpdateDeleteSemanticAnalyzer.java
Patch:
@@ -223,6 +223,7 @@ public void testInsertValuesPartitioned() throws Exception {
   public void setup() {
     conf = new HiveConf();
     conf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, "nonstrict");
+    conf.setVar(HiveConf.ConfVars.HIVEMAPREDMODE, "nonstrict");
     conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager");
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
Patch:
@@ -576,8 +576,9 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,
       // exist before jobClose (before renaming after job completion)
       Path tempOutPath = Utilities.toTempPath(outputPath);
       try {
-        if (!fs.exists(tempOutPath)) {
-          fs.mkdirs(tempOutPath);
+        FileSystem tmpOutFS = tempOutPath.getFileSystem(conf);
+        if (!tmpOutFS.exists(tempOutPath)) {
+          tmpOutFS.mkdirs(tempOutPath);
         }
       } catch (IOException e) {
         throw new RuntimeException(

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HiveSplitGenerator.java
Patch:
@@ -140,7 +140,7 @@ public List<Event> initialize() throws Exception {
               TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_MIN_SIZE,
               TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_MIN_SIZE_DEFAULT);
           final long preferredSplitSize = Math.min(blockSize / 2, minGrouping);
-          HiveConf.setLongVar(conf, HiveConf.ConfVars.MAPREDMINSPLITSIZE, preferredSplitSize);
+          HiveConf.setLongVar(jobConf, HiveConf.ConfVars.MAPREDMINSPLITSIZE, preferredSplitSize);
           LOG.info("The preferred split size is " + preferredSplitSize);
         }
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1156,8 +1156,8 @@ public static enum ConfVars {
     HIVELIMITOPTMAXFETCH("hive.limit.optimize.fetch.max", 50000,
         "Maximum number of rows allowed for a smaller subset of data for simple LIMIT, if it is a fetch query. \n" +
         "Insert queries are not restricted by this limit."),
-    HIVELIMITPUSHDOWNMEMORYUSAGE("hive.limit.pushdown.memory.usage", -1f,
-        "The max memory to be used for hash in RS operator for top K selection."),
+    HIVELIMITPUSHDOWNMEMORYUSAGE("hive.limit.pushdown.memory.usage", 0.1f, new RatioValidator(),
+        "The fraction of available memory to be used for buffering rows in Reducesink operator for limit pushdown optimization."),
     HIVELIMITTABLESCANPARTITION("hive.limit.query.max.table.partition", -1,
         "This controls how many partitions can be scanned for each partitioned table.\n" +
         "The default value \"-1\" means no limit."),

File: beeline/src/test/org/apache/hive/beeline/cli/TestHiveCli.java
Patch:
@@ -278,6 +278,7 @@ private File generateTmpFile(String context) {
   public void setup() {
     System.setProperty("datanucleus.fixedDatastore", "false");
     System.setProperty("datanucleus.autoCreateSchema", "true");
+    System.setProperty("hive.metastore.schema.verification", "false");
     cli = new HiveCli();
     redirectOutputStream();
     initFromFile();

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -540,7 +540,7 @@ public static enum ConfVars {
     METASTORE_AUTO_CREATE_SCHEMA("datanucleus.autoCreateSchema", false,
         "creates necessary schema on a startup if one doesn't exist. set this to false, after creating it once"),
     METASTORE_FIXED_DATASTORE("datanucleus.fixedDatastore", true, "Dictates whether to allow updates to schema or not."),
-    METASTORE_SCHEMA_VERIFICATION("hive.metastore.schema.verification", false,
+    METASTORE_SCHEMA_VERIFICATION("hive.metastore.schema.verification", true,
         "Enforce metastore schema version consistency.\n" +
         "True: Verify that version information stored in metastore matches with one from Hive jars.  Also disable automatic\n" +
         "      schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures\n" +

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HiveSplitGenerator.java
Patch:
@@ -259,7 +259,7 @@ public void handleInputInitializerEvent(List<InputInitializerEvent> events) thro
   }
 
   // Descending sort based on split size| Followed by file name. Followed by startPosition.
-  private static class InputSplitComparator implements Comparator<InputSplit> {
+  static class InputSplitComparator implements Comparator<InputSplit> {
     @Override
     public int compare(InputSplit o1, InputSplit o2) {
       try {
@@ -278,7 +278,7 @@ public int compare(InputSplit o1, InputSplit o2) {
                 // Compare start Position
                 long startPos1 = fs1.getStart();
                 long startPos2 = fs2.getStart();
-                if (startPos1 > startPos1) {
+                if (startPos1 > startPos2) {
                   return 1;
                 } else if (startPos1 < startPos2) {
                   return -1;

File: service/src/java/org/apache/hive/service/cli/operation/GetTablesOperation.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hive.service.cli.operation;
 
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.List;
 
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -78,7 +79,7 @@ protected GetTablesOperation(HiveSession parentSession,
     if (tableTypes != null) {
       tableTypeList = new ArrayList<String>();
       for (String tableType : tableTypes) {
-        tableTypeList.add(tableTypeMapping.mapToHiveType(tableType.trim()));
+        tableTypeList.addAll(Arrays.asList(tableTypeMapping.mapToHiveType(tableType.trim())));
       }
     } else {
       tableTypeList = null;

File: hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/TestStreaming.java
Patch:
@@ -451,6 +451,8 @@ private void checkDataWritten(Path partitionPath, long minTxn, long maxTxn, int
     JobConf job = new JobConf();
     job.set("mapred.input.dir", partitionPath.toString());
     job.set("bucket_count", Integer.toString(buckets));
+    job.set("columns", "id,msg");
+    job.set("columns.types", "bigint:string");
     job.set(ValidTxnList.VALID_TXNS_KEY, txns.toString());
     InputSplit[] splits = inf.getSplits(job, buckets);
     Assert.assertEquals(buckets, splits.length);

File: hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/mutate/StreamingAssert.java
Patch:
@@ -128,6 +128,8 @@ List<Record> readRecords() throws Exception {
     JobConf job = new JobConf();
     job.set("mapred.input.dir", partitionLocation.toString());
     job.set("bucket_count", Integer.toString(table.getSd().getNumBuckets()));
+    job.set("columns", "id,msg");
+    job.set("columns.types", "bigint:string");
     job.set(ValidTxnList.VALID_TXNS_KEY, txns.toString());
     InputSplit[] splits = inputFormat.getSplits(job, 1);
     assertEquals(1, splits.length);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java
Patch:
@@ -244,7 +244,7 @@ private void initializeSourceForTag(ReduceWork redWork, int tag, ObjectInspector
     boolean vectorizedRecordSource = (tag == bigTablePosition) && redWork.getVectorMode();
     sources[tag].init(jconf, redWork.getReducer(), vectorizedRecordSource, keyTableDesc,
         valueTableDesc, reader, tag == bigTablePosition, (byte) tag,
-        redWork.getVectorScratchColumnTypeMap());
+        redWork.getVectorizedRowBatchCtx());
     ois[tag] = sources[tag].getObjectInspector();
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
Patch:
@@ -813,7 +813,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
           outputFieldNames, objectInspectors);
       if (isVectorOutput) {
         vrbCtx = new VectorizedRowBatchCtx();
-        vrbCtx.init(vOutContext.getScratchColumnTypeMap(), (StructObjectInspector) outputObjInspector);
+        vrbCtx.init((StructObjectInspector) outputObjInspector, vOutContext.getScratchColumnTypeNames());
         outputBatch = vrbCtx.createVectorizedRowBatch();
         vectorAssignRowSameBatch = new VectorAssignRowSameBatch();
         vectorAssignRowSameBatch.init((StructObjectInspector) outputObjInspector, vOutContext.getProjectedColumns());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapJoinBaseOperator.java
Patch:
@@ -90,7 +90,7 @@ public void initializeOp(Configuration hconf) throws HiveException {
     super.initializeOp(hconf);
 
     vrbCtx = new VectorizedRowBatchCtx();
-    vrbCtx.init(vOutContext.getScratchColumnTypeMap(), (StructObjectInspector) this.outputObjInspector);
+    vrbCtx.init((StructObjectInspector) this.outputObjInspector, vOutContext.getScratchColumnTypeNames());
 
     outputBatch = vrbCtx.createVectorizedRowBatch();
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java
Patch:
@@ -146,7 +146,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     super.initializeOp(hconf);
 
     vrbCtx = new VectorizedRowBatchCtx();
-    vrbCtx.init(vOutContext.getScratchColumnTypeMap(), (StructObjectInspector) this.outputObjInspector);
+    vrbCtx.init((StructObjectInspector) this.outputObjInspector, vOutContext.getScratchColumnTypeNames());
 
     outputBatch = vrbCtx.createVectorizedRowBatch();
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java
Patch:
@@ -48,6 +48,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
 
 /**
@@ -413,8 +414,8 @@ protected void generateHashMapResultRepeatedAll(VectorizedRowBatch batch,
 
   private void setupSpillSerDe(VectorizedRowBatch batch) throws HiveException {
 
-    PrimitiveTypeInfo[] inputObjInspectorsTypeInfos =
-        VectorizedBatchUtil.primitiveTypeInfosFromStructObjectInspector(
+    TypeInfo[] inputObjInspectorsTypeInfos =
+        VectorizedBatchUtil.typeInfosFromStructObjectInspector(
                (StructObjectInspector) inputObjInspectors[posBigTable]);
 
     List<Integer> projectedColumns = vContext.getProjectedColumns();

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFile.java
Patch:
@@ -402,7 +402,7 @@ public WriterOptions compress(CompressionKind value) {
     public WriterOptions inspector(ObjectInspector value) {
       this.inspector = value;
       if (!explicitSchema) {
-        schema = OrcOutputFormat.convertTypeInfo(
+        schema = OrcUtils.convertTypeInfo(
             TypeInfoUtils.getTypeInfoFromObjectInspector(value));
       }
       return this;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
Patch:
@@ -370,6 +370,7 @@ public final void setFiltered(boolean filtered) {
 
     private FetchWork convertToWork() throws HiveException {
       inputs.clear();
+      Utilities.addSchemaEvolutionToTableScanOperator(table, scanOp);
       TableDesc tableDesc = Utilities.getTableDesc(table);
       if (!table.isPartitioned()) {
         inputs.add(new ReadEntity(table, parent, !table.isView() && parent == null));

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorRowObject.java
Patch:
@@ -24,6 +24,7 @@
 
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 
 import junit.framework.TestCase;
 
@@ -50,13 +51,13 @@ void examineBatch(VectorizedRowBatch batch, VectorExtractRowSameBatch vectorExtr
 
   void testVectorRowObject(int caseNum, Random r) throws HiveException {
 
-    Map<Integer, String> emptyScratchMap = new HashMap<Integer, String>();
+    String[] emptyScratchTypeNames = new String[0];
 
     RandomRowObjectSource source = new RandomRowObjectSource();
     source.init(r);
 
     VectorizedRowBatchCtx batchContext = new VectorizedRowBatchCtx();
-    batchContext.init(emptyScratchMap, source.rowStructObjectInspector());
+    batchContext.init(source.rowStructObjectInspector(), emptyScratchTypeNames);
     VectorizedRowBatch batch = batchContext.createVectorizedRowBatch();
 
     VectorAssignRowSameBatch vectorAssignRow = new VectorAssignRowSameBatch();

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -544,9 +544,9 @@ public static enum ConfVars {
     METASTORE_VALIDATE_CONSTRAINTS("datanucleus.validateConstraints", false,
         "validates existing schema against code. turn this on if you want to verify existing schema"),
     METASTORE_STORE_MANAGER_TYPE("datanucleus.storeManagerType", "rdbms", "metadata store type"),
-    METASTORE_AUTO_CREATE_SCHEMA("datanucleus.autoCreateSchema", true,
+    METASTORE_AUTO_CREATE_SCHEMA("datanucleus.autoCreateSchema", false,
         "creates necessary schema on a startup if one doesn't exist. set this to false, after creating it once"),
-    METASTORE_FIXED_DATASTORE("datanucleus.fixedDatastore", false, ""),
+    METASTORE_FIXED_DATASTORE("datanucleus.fixedDatastore", true, "Dictates whether to allow updates to schema or not."),
     METASTORE_SCHEMA_VERIFICATION("hive.metastore.schema.verification", false,
         "Enforce metastore schema version consistency.\n" +
         "True: Verify that version information stored in metastore matches with one from Hive jars.  Also disable automatic\n" +

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveCalciteUtil.java
Patch:
@@ -234,8 +234,8 @@ public static RexNode projectNonColumnEquiConditions(ProjectFactory factory, Rel
       leftKeys.add(origLeftInputSize + i);
       rightKeys.add(origRightInputSize + i);
       RexNode cond = rexBuilder.makeCall(SqlStdOperatorTable.EQUALS,
-          rexBuilder.makeInputRef(newLeftFields.get(i).getType(), newLeftOffset + i),
-          rexBuilder.makeInputRef(newLeftFields.get(i).getType(), newRightOffset + i));
+          rexBuilder.makeInputRef(newLeftFields.get(origLeftInputSize + i).getType(), newLeftOffset + i),
+          rexBuilder.makeInputRef(newRightFields.get(origRightInputSize + i).getType(), newRightOffset + i));
       if (outJoinCond == null) {
         outJoinCond = cond;
       } else {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -637,7 +637,7 @@ public static enum ConfVars {
         "as nulls, so we should set this parameter if we wish to reverse that behaviour. For others, " +
         "pruning is the correct behaviour"),
     METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES(
-        "hive.metastore.disallow.incompatible.col.type.changes", false,
+        "hive.metastore.disallow.incompatible.col.type.changes", true,
         "If true (default is false), ALTER TABLE operations which change the type of a\n" +
         "column (say STRING) to an incompatible type (say MAP) are disallowed.\n" +
         "RCFile default SerDe (ColumnarSerDe) serializes the values in such a way that the\n" +

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/TestSemanticAnalysis.java
Patch:
@@ -68,6 +68,7 @@ public void setUpHCatDriver() throws IOException {
           "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe");
       hcatConf.set(HiveConf.ConfVars.SEMANTIC_ANALYZER_HOOK.varname,
           HCatSemanticAnalyzer.class.getName());
+      hcatConf.setBoolVar(HiveConf.ConfVars.METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES, false);
       hcatDriver = new Driver(hcatConf);
       SessionState.start(new CliSessionState(hcatConf));
     }

File: hcatalog/webhcat/java-client/src/test/java/org/apache/hive/hcatalog/api/TestHCatClient.java
Patch:
@@ -565,7 +565,7 @@ public void testUpdateTableSchema() throws Exception {
       client.createTable(HCatCreateTableDesc.create(dbName, tableName, oldSchema).build());
 
       List<HCatFieldSchema> newSchema = Arrays.asList(new HCatFieldSchema("completely", Type.DOUBLE, ""),
-          new HCatFieldSchema("new", Type.FLOAT, ""),
+          new HCatFieldSchema("new", Type.STRING, ""),
           new HCatFieldSchema("fields", Type.STRING, ""));
 
       client.updateTableSchema(dbName, tableName, newSchema);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -78,6 +78,7 @@
 import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hive.common.util.DateUtils;
@@ -903,7 +904,7 @@ protected ExprNodeDesc getXpathOrFuncExprNodeDesc(ASTNode expr,
 
         if (myt.getCategory() == Category.LIST) {
           // Only allow integer index for now
-          if (!FunctionRegistry.implicitConvertible(children.get(1).getTypeInfo(),
+          if (!TypeInfoUtils.implicitConvertible(children.get(1).getTypeInfo(),
               TypeInfoFactory.intTypeInfo)) {
             throw new SemanticException(SemanticAnalyzer.generateErrorMessage(
                   expr, ErrorMsg.INVALID_ARRAYINDEX_TYPE.getMsg()));
@@ -913,7 +914,7 @@ protected ExprNodeDesc getXpathOrFuncExprNodeDesc(ASTNode expr,
           TypeInfo t = ((ListTypeInfo) myt).getListElementTypeInfo();
           desc = new ExprNodeGenericFuncDesc(t, FunctionRegistry.getGenericUDFForIndex(), children);
         } else if (myt.getCategory() == Category.MAP) {
-          if (!FunctionRegistry.implicitConvertible(children.get(1).getTypeInfo(),
+          if (!TypeInfoUtils.implicitConvertible(children.get(1).getTypeInfo(),
               ((MapTypeInfo) myt).getMapKeyTypeInfo())) {
             throw new SemanticException(ErrorMsg.INVALID_MAPINDEX_TYPE
                 .getMsg(expr));

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java
Patch:
@@ -80,7 +80,7 @@ protected void setUp() {
   }
 
   private void implicit(TypeInfo a, TypeInfo b, boolean convertible) {
-    assertEquals(convertible, FunctionRegistry.implicitConvertible(a, b));
+    assertEquals(convertible, TypeInfoUtils.implicitConvertible(a, b));
   }
 
   public void testImplicitConversion() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateString.java
Patch:
@@ -22,6 +22,7 @@
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator;
 import org.apache.hadoop.io.Text;
+import org.apache.hive.common.util.DateUtils;
 
 import java.text.SimpleDateFormat;
 import java.util.Date;
@@ -30,14 +31,13 @@
 public class VectorUDFDateString extends StringUnaryUDF {
   private static final long serialVersionUID = 1L;
 
-  private transient static SimpleDateFormat formatter = new SimpleDateFormat("yyyy-MM-dd");
-
   private static final Logger LOG = LoggerFactory.getLogger(
       VectorUDFDateString.class.getName());
 
   public VectorUDFDateString(int colNum, int outputColumn) {
     super(colNum, outputColumn, new StringUnaryUDF.IUDFUnaryString() {
       Text t = new Text();
+      final transient SimpleDateFormat formatter = DateUtils.getDateFormat();
 
       @Override
       public Text evaluate(Text s) {

File: jdbc/src/java/org/apache/hive/jdbc/ZooKeeperHiveClientHelper.java
Patch:
@@ -159,7 +159,7 @@ private static void applyConfs(String serverConfStr, JdbcConnectionParams connPa
         }
         // KERBEROS
         // If delegation token is passed from the client side, do not set the principal
-        if (matcher.group(2).equalsIgnoreCase("hive.server2.authentication.kerberos.principal")
+        if (matcher.group(1).equalsIgnoreCase("hive.server2.authentication.kerberos.principal")
             && !(connParams.getSessionVars().containsKey(JdbcConnectionParams.AUTH_TYPE) && connParams
                 .getSessionVars().get(JdbcConnectionParams.AUTH_TYPE)
                 .equalsIgnoreCase(JdbcConnectionParams.AUTH_TOKEN))

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java
Patch:
@@ -191,7 +191,7 @@ public void submitWork(SubmitWorkRequestProto request) throws IOException {
 
       Token<JobTokenIdentifier> jobToken = TokenCache.getSessionToken(credentials);
 
-      LOG.info("DEBUG: Registering request with the ShuffleHandler");
+      LOG.debug("Registering request with the ShuffleHandler");
       ShuffleHandler.get()
           .registerDag(request.getApplicationIdString(), dagIdentifier, jobToken,
               request.getUser(), localDirs);

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java
Patch:
@@ -272,7 +272,7 @@ public void killTask() {
             shouldRunTask = false;
           } else {
             // If the task hasn't started, and it is killed - report back to the AM that the task has been killed.
-            LOG.info("DBG: Reporting taskKilled for non-started fragment {}", getRequestId());
+            LOG.debug("Reporting taskKilled for non-started fragment {}", getRequestId());
             reportTaskKilled();
           }
           if (!isStarted.get()) {
@@ -398,7 +398,7 @@ public void onSuccess(TaskRunner2Result result) {
       switch(result.getEndReason()) {
         // Only the KILLED case requires a message to be sent out to the AM.
         case SUCCESS:
-          LOG.info("Successfully finished {}", requestId);
+          LOG.debug("Successfully finished {}", requestId);
           metrics.incrExecutorTotalSuccess();
           break;
         case CONTAINER_STOP_REQUESTED:

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
Patch:
@@ -168,8 +168,8 @@ public OrcEncodedDataReader(LowLevelCache lowLevelCache, Cache<OrcCacheKey> cach
 
   @Override
   public void stop() {
-    if (LOG.isInfoEnabled()) {
-      LOG.info("Encoded reader is being stopped");
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Encoded reader is being stopped");
     }
     isStopped = true;
   }

File: llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java
Patch:
@@ -359,12 +359,12 @@ public void dagComplete() {
 
   @Override
   public void blacklistNode(NodeId nodeId) {
-    LOG.info("DEBUG: BlacklistNode not supported");
+    LOG.info("BlacklistNode not supported");
   }
 
   @Override
   public void unblacklistNode(NodeId nodeId) {
-    LOG.info("DEBUG: unBlacklistNode not supported");
+    LOG.info("unBlacklistNode not supported");
   }
 
   @Override
@@ -494,7 +494,7 @@ public boolean deallocateTask(Object task, boolean taskSucceeded, TaskAttemptEnd
 
   @Override
   public Object deallocateContainer(ContainerId containerId) {
-    LOG.info("DEBUG: Ignoring deallocateContainer for containerId: " + containerId);
+    LOG.debug("Ignoring deallocateContainer for containerId: " + containerId);
     // Containers are not being tracked for re-use.
     // This is safe to ignore since a deallocate task will come in.
     return null;

File: metastore/src/java/org/apache/hadoop/hive/metastore/AggregateStatsCache.java
Patch:
@@ -171,7 +171,7 @@ public AggrColStats get(String dbName, String tblName, String colName, List<Stri
     AggrColStatsList candidateList = cacheStore.get(key);
     // No key, or no nodes in candidate list
     if ((candidateList == null) || (candidateList.nodes.size() == 0)) {
-      LOG.info("No aggregate stats cached for " + key.toString());
+      LOG.debug("No aggregate stats cached for " + key.toString());
       return null;
     }
     // Find the value object

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapredContext.java
Patch:
@@ -52,7 +52,7 @@ public static MapredContext init(boolean isMap, JobConf jobConf) {
         HiveConf.getVar(jobConf, ConfVars.HIVE_EXECUTION_ENGINE).equals("tez") ?
             new TezContext(isMap, jobConf) : new MapredContext(isMap, jobConf);
     contexts.set(context);
-    logger.info("MapredContext initialized.");
+    logger.debug("MapredContext initialized.");
     return context;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -399,11 +399,11 @@ private static BaseWork getBaseWork(Configuration conf, String name) {
         } else if (ShimLoader.getHadoopShims().isLocalMode(conf)) {
           localPath = path;
         } else {
-          LOG.info("***************non-local mode***************");
+          LOG.debug("***************non-local mode***************");
           localPath = new Path(name);
         }
         localPath = path;
-        LOG.info("local path = " + localPath);
+        LOG.debug("local path = " + localPath);
         if (HiveConf.getBoolVar(conf, ConfVars.HIVE_RPC_QUERY_PLAN)) {
           LOG.debug("Loading plan from string: "+path.toUri().getPath());
           String planString = conf.getRaw(path.toUri().getPath());
@@ -415,7 +415,7 @@ private static BaseWork getBaseWork(Configuration conf, String name) {
           in = new ByteArrayInputStream(planBytes);
           in = new InflaterInputStream(in);
         } else {
-          LOG.info("Open file to read in plan: " + localPath);
+          LOG.debug("Open file to read in plan: " + localPath);
           in = localPath.getFileSystem(conf).open(localPath);
         }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
Patch:
@@ -318,7 +318,7 @@ public void initialize(Configuration hconf) throws HiveException {
 
       mapKeysAggregationBuffers = new HashMap<KeyWrapper, VectorAggregationBufferRow>();
       computeMemoryLimits();
-      LOG.info("using hash aggregation processing mode");
+      LOG.debug("using hash aggregation processing mode");
     }
 
     @Override

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java
Patch:
@@ -171,7 +171,7 @@ private void initIOContext(long startPos, boolean isBlockPointer,
     ioCxtRef.setCurrentBlockStart(startPos);
     ioCxtRef.setBlockPointer(isBlockPointer);
     ioCxtRef.setInputPath(inputPath);
-    LOG.info("Processing file " + inputPath);
+    LOG.debug("Processing file " + inputPath); // Logged at INFO in multiple other places.
     initDone = true;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
Patch:
@@ -210,7 +210,9 @@ public static InputFormat<WritableComparable, Writable> wrapForLlap(
       LOG.info("Not using llap for " + inputFormat + ": " + isSupported + ", " + isVector);
       return inputFormat;
     }
-    LOG.info("Wrapping " + inputFormat);
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Wrapping " + inputFormat);
+    }
     @SuppressWarnings("unchecked")
     LlapIo<VectorizedRowBatch> llapIo = LlapIoProxy.getIo();
     if (llapIo == null) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java
Patch:
@@ -172,7 +172,7 @@ public int compare(MapWork o1, MapWork o2) {
         return null;
       }
 
-      LOG.info("Looking for table scans where optimization is applicable");
+      LOG.debug("Looking for table scans where optimization is applicable");
 
       // The dispatcher fires the processor corresponding to the closest
       // matching rule and passes the context along
@@ -196,7 +196,7 @@ public int compare(MapWork o1, MapWork o2) {
 
       ogw.startWalking(topNodes, null);
 
-      LOG.info(String.format("Found %d null table scans",
+      LOG.debug(String.format("Found %d null table scans",
           walkerCtx.getMetadataOnlyTableScans().size()));
       if (walkerCtx.getMetadataOnlyTableScans().size() > 0)
         processAlias(mapWork, walkerCtx.getMetadataOnlyTableScans());

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestDBTokenStore.java
Patch:
@@ -37,7 +37,7 @@ public class TestDBTokenStore extends TestCase{
   public void testDBTokenStore() throws TokenStoreException, MetaException, IOException {
 
     DelegationTokenStore ts = new DBTokenStore();
-    ts.init(new HMSHandler("Test handler").getMS(), null);
+    ts.init(new HMSHandler("Test handler"), null);
     assertEquals(0, ts.getMasterKeys().length);
     assertEquals(false,ts.removeMasterKey(-1));
     try{

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -5974,7 +5974,7 @@ public static void startMetaStore(int port, HadoopThriftAuthBridge bridge,
             conf.getVar(HiveConf.ConfVars.METASTORE_KERBEROS_KEYTAB_FILE),
             conf.getVar(HiveConf.ConfVars.METASTORE_KERBEROS_PRINCIPAL));
         // start delegation token manager
-        saslServer.startDelegationTokenSecretManager(conf, baseHandler.getMS(), ServerMode.METASTORE);
+        saslServer.startDelegationTokenSecretManager(conf, baseHandler, ServerMode.METASTORE);
         transFactory = saslServer.createTransportFactory(
                 MetaStoreUtils.getMetaStoreSaslProperties(conf));
         processor = saslServer.wrapProcessor(

File: shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java
Patch:
@@ -425,8 +425,8 @@ protected DelegationTokenStore getTokenStore(Configuration conf)
     }
 
 
-    public void startDelegationTokenSecretManager(Configuration conf, Object rawStore, ServerMode smode)
-        throws IOException{
+    public void startDelegationTokenSecretManager(Configuration conf, Object hms, ServerMode smode)
+        throws IOException {
       long secretKeyInterval =
           conf.getLong(DELEGATION_KEY_UPDATE_INTERVAL_KEY,
               DELEGATION_KEY_UPDATE_INTERVAL_DEFAULT);
@@ -440,7 +440,7 @@ public void startDelegationTokenSecretManager(Configuration conf, Object rawStor
           DELEGATION_TOKEN_GC_INTERVAL_DEFAULT);
 
       DelegationTokenStore dts = getTokenStore(conf);
-      dts.init(rawStore, smode);
+      dts.init(hms, smode);
       secretManager = new TokenStoreDelegationTokenSecretManager(secretKeyInterval,
           tokenMaxLifetime,
           tokenRenewInterval,

File: shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java
Patch:
@@ -429,7 +429,7 @@ public void close() throws IOException {
   }
 
   @Override
-  public void init(Object objectStore, ServerMode smode) {
+  public void init(Object hmsHandler, ServerMode smode) {
     this.serverMode = smode;
     zkConnectString =
         conf.get(HadoopThriftAuthBridge.Server.DELEGATION_TOKEN_STORE_ZK_CONNECT_STR, null);

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/InvalidTable.java
Patch:
@@ -29,10 +29,10 @@ public InvalidTable(String db, String table) {
   }
 
   public InvalidTable(String db, String table, String msg) {
-    super(msg);
+    super(makeMsg(db, table) + ": " + msg, null);
   }
 
   public InvalidTable(String db, String table, Exception inner) {
-    super(inner.getMessage(), inner);
+    super(makeMsg(db, table) + ": " + inner.getMessage(), inner);
   }
 }

File: metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -79,6 +79,7 @@
 import org.apache.hadoop.hive.metastore.api.CurrentNotificationEventId;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.FileMetadataExprType;
 import org.apache.hadoop.hive.metastore.api.Function;
 import org.apache.hadoop.hive.metastore.api.FunctionType;
 import org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege;
@@ -7666,7 +7667,7 @@ public boolean isFileMetadataSupported() {
   }
 
   @Override
-  public void getFileMetadataByExpr(List<Long> fileIds, byte[] expr,
+  public void getFileMetadataByExpr(List<Long> fileIds, FileMetadataExprType type, byte[] expr,
       ByteBuffer[] metadatas, ByteBuffer[] stripeBitsets, boolean[] eliminated) {
     throw new UnsupportedOperationException();
   }

File: metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java
Patch:
@@ -1747,7 +1747,7 @@ ByteBuffer[] getFileMetadata(List<Long> fileIds) throws IOException {
    * @param fileIds file ID list.
    * @return Serialized file metadata.
    */
-  void getFileMetadata(List<Long> fileIds, ByteBuffer[] result) throws IOException {
+  public void getFileMetadata(List<Long> fileIds, ByteBuffer[] result) throws IOException {
     byte[][] keys = new byte[fileIds.size()][];
     for (int i = 0; i < fileIds.size(); ++i) {
       keys[i] = HBaseUtils.makeLongKey(fileIds.get(i));

File: metastore/src/test/org/apache/hadoop/hive/metastore/DummyRawStoreControlledCommit.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hive.metastore.api.ColumnStatistics;
 import org.apache.hadoop.hive.metastore.api.CurrentNotificationEventId;
 import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.FileMetadataExprType;
 import org.apache.hadoop.hive.metastore.api.Function;
 import org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege;
 import org.apache.hadoop.hive.metastore.api.Index;
@@ -777,7 +778,7 @@ public boolean isFileMetadataSupported() {
 
 
   @Override
-  public void getFileMetadataByExpr(List<Long> fileIds, byte[] expr,
+  public void getFileMetadataByExpr(List<Long> fileIds, FileMetadataExprType type, byte[] expr,
       ByteBuffer[] metadatas, ByteBuffer[] stripeBitsets, boolean[] eliminated) {
   }
 }

File: metastore/src/test/org/apache/hadoop/hive/metastore/DummyRawStoreForJdoConnection.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.metastore.api.ColumnStatistics;
 import org.apache.hadoop.hive.metastore.api.CurrentNotificationEventId;
 import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.FileMetadataExprType;
 import org.apache.hadoop.hive.metastore.api.Function;
 import org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege;
 import org.apache.hadoop.hive.metastore.api.Index;
@@ -793,7 +794,7 @@ public boolean isFileMetadataSupported() {
   }
 
   @Override
-  public void getFileMetadataByExpr(List<Long> fileIds, byte[] expr,
+  public void getFileMetadataByExpr(List<Long> fileIds, FileMetadataExprType type, byte[] expr,
       ByteBuffer[] metadatas, ByteBuffer[] stripeBitsets, boolean[] eliminated) {
   }
 }

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/AbortTxnRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class AbortTxnRequest implements org.apache.thrift.TBase<AbortTxnRequest, AbortTxnRequest._Fields>, java.io.Serializable, Cloneable, Comparable<AbortTxnRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("AbortTxnRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/AddDynamicPartitions.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class AddDynamicPartitions implements org.apache.thrift.TBase<AddDynamicPartitions, AddDynamicPartitions._Fields>, java.io.Serializable, Cloneable, Comparable<AddDynamicPartitions> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("AddDynamicPartitions");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/AddPartitionsRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class AddPartitionsRequest implements org.apache.thrift.TBase<AddPartitionsRequest, AddPartitionsRequest._Fields>, java.io.Serializable, Cloneable, Comparable<AddPartitionsRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("AddPartitionsRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/AddPartitionsResult.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class AddPartitionsResult implements org.apache.thrift.TBase<AddPartitionsResult, AddPartitionsResult._Fields>, java.io.Serializable, Cloneable, Comparable<AddPartitionsResult> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("AddPartitionsResult");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/AggrStats.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class AggrStats implements org.apache.thrift.TBase<AggrStats, AggrStats._Fields>, java.io.Serializable, Cloneable, Comparable<AggrStats> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("AggrStats");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/AlreadyExistsException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class AlreadyExistsException extends TException implements org.apache.thrift.TBase<AlreadyExistsException, AlreadyExistsException._Fields>, java.io.Serializable, Cloneable, Comparable<AlreadyExistsException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("AlreadyExistsException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/BinaryColumnStatsData.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class BinaryColumnStatsData implements org.apache.thrift.TBase<BinaryColumnStatsData, BinaryColumnStatsData._Fields>, java.io.Serializable, Cloneable, Comparable<BinaryColumnStatsData> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("BinaryColumnStatsData");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/BooleanColumnStatsData.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class BooleanColumnStatsData implements org.apache.thrift.TBase<BooleanColumnStatsData, BooleanColumnStatsData._Fields>, java.io.Serializable, Cloneable, Comparable<BooleanColumnStatsData> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("BooleanColumnStatsData");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/CheckLockRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class CheckLockRequest implements org.apache.thrift.TBase<CheckLockRequest, CheckLockRequest._Fields>, java.io.Serializable, Cloneable, Comparable<CheckLockRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("CheckLockRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ClearFileMetadataRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ClearFileMetadataRequest implements org.apache.thrift.TBase<ClearFileMetadataRequest, ClearFileMetadataRequest._Fields>, java.io.Serializable, Cloneable, Comparable<ClearFileMetadataRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ClearFileMetadataRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ClearFileMetadataResult.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ClearFileMetadataResult implements org.apache.thrift.TBase<ClearFileMetadataResult, ClearFileMetadataResult._Fields>, java.io.Serializable, Cloneable, Comparable<ClearFileMetadataResult> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ClearFileMetadataResult");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ColumnStatistics.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ColumnStatistics implements org.apache.thrift.TBase<ColumnStatistics, ColumnStatistics._Fields>, java.io.Serializable, Cloneable, Comparable<ColumnStatistics> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ColumnStatistics");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ColumnStatisticsDesc.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ColumnStatisticsDesc implements org.apache.thrift.TBase<ColumnStatisticsDesc, ColumnStatisticsDesc._Fields>, java.io.Serializable, Cloneable, Comparable<ColumnStatisticsDesc> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ColumnStatisticsDesc");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ColumnStatisticsObj.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ColumnStatisticsObj implements org.apache.thrift.TBase<ColumnStatisticsObj, ColumnStatisticsObj._Fields>, java.io.Serializable, Cloneable, Comparable<ColumnStatisticsObj> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ColumnStatisticsObj");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/CommitTxnRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class CommitTxnRequest implements org.apache.thrift.TBase<CommitTxnRequest, CommitTxnRequest._Fields>, java.io.Serializable, Cloneable, Comparable<CommitTxnRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("CommitTxnRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/CompactionRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class CompactionRequest implements org.apache.thrift.TBase<CompactionRequest, CompactionRequest._Fields>, java.io.Serializable, Cloneable, Comparable<CompactionRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("CompactionRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ConfigValSecurityException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ConfigValSecurityException extends TException implements org.apache.thrift.TBase<ConfigValSecurityException, ConfigValSecurityException._Fields>, java.io.Serializable, Cloneable, Comparable<ConfigValSecurityException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ConfigValSecurityException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/CurrentNotificationEventId.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class CurrentNotificationEventId implements org.apache.thrift.TBase<CurrentNotificationEventId, CurrentNotificationEventId._Fields>, java.io.Serializable, Cloneable, Comparable<CurrentNotificationEventId> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("CurrentNotificationEventId");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Database.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Database implements org.apache.thrift.TBase<Database, Database._Fields>, java.io.Serializable, Cloneable, Comparable<Database> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Database");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Date.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Date implements org.apache.thrift.TBase<Date, Date._Fields>, java.io.Serializable, Cloneable, Comparable<Date> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Date");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DateColumnStatsData.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class DateColumnStatsData implements org.apache.thrift.TBase<DateColumnStatsData, DateColumnStatsData._Fields>, java.io.Serializable, Cloneable, Comparable<DateColumnStatsData> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("DateColumnStatsData");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Decimal.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Decimal implements org.apache.thrift.TBase<Decimal, Decimal._Fields>, java.io.Serializable, Cloneable, Comparable<Decimal> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Decimal");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DecimalColumnStatsData.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class DecimalColumnStatsData implements org.apache.thrift.TBase<DecimalColumnStatsData, DecimalColumnStatsData._Fields>, java.io.Serializable, Cloneable, Comparable<DecimalColumnStatsData> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("DecimalColumnStatsData");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DoubleColumnStatsData.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class DoubleColumnStatsData implements org.apache.thrift.TBase<DoubleColumnStatsData, DoubleColumnStatsData._Fields>, java.io.Serializable, Cloneable, Comparable<DoubleColumnStatsData> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("DoubleColumnStatsData");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DropPartitionsExpr.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class DropPartitionsExpr implements org.apache.thrift.TBase<DropPartitionsExpr, DropPartitionsExpr._Fields>, java.io.Serializable, Cloneable, Comparable<DropPartitionsExpr> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("DropPartitionsExpr");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DropPartitionsRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class DropPartitionsRequest implements org.apache.thrift.TBase<DropPartitionsRequest, DropPartitionsRequest._Fields>, java.io.Serializable, Cloneable, Comparable<DropPartitionsRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("DropPartitionsRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/DropPartitionsResult.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class DropPartitionsResult implements org.apache.thrift.TBase<DropPartitionsResult, DropPartitionsResult._Fields>, java.io.Serializable, Cloneable, Comparable<DropPartitionsResult> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("DropPartitionsResult");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/EnvironmentContext.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class EnvironmentContext implements org.apache.thrift.TBase<EnvironmentContext, EnvironmentContext._Fields>, java.io.Serializable, Cloneable, Comparable<EnvironmentContext> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("EnvironmentContext");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/FieldSchema.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class FieldSchema implements org.apache.thrift.TBase<FieldSchema, FieldSchema._Fields>, java.io.Serializable, Cloneable, Comparable<FieldSchema> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("FieldSchema");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/FireEventRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class FireEventRequest implements org.apache.thrift.TBase<FireEventRequest, FireEventRequest._Fields>, java.io.Serializable, Cloneable, Comparable<FireEventRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("FireEventRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/FireEventResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class FireEventResponse implements org.apache.thrift.TBase<FireEventResponse, FireEventResponse._Fields>, java.io.Serializable, Cloneable, Comparable<FireEventResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("FireEventResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Function.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Function implements org.apache.thrift.TBase<Function, Function._Fields>, java.io.Serializable, Cloneable, Comparable<Function> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Function");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GetAllFunctionsResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GetAllFunctionsResponse implements org.apache.thrift.TBase<GetAllFunctionsResponse, GetAllFunctionsResponse._Fields>, java.io.Serializable, Cloneable, Comparable<GetAllFunctionsResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GetAllFunctionsResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GetFileMetadataByExprResult.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GetFileMetadataByExprResult implements org.apache.thrift.TBase<GetFileMetadataByExprResult, GetFileMetadataByExprResult._Fields>, java.io.Serializable, Cloneable, Comparable<GetFileMetadataByExprResult> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GetFileMetadataByExprResult");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GetFileMetadataRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GetFileMetadataRequest implements org.apache.thrift.TBase<GetFileMetadataRequest, GetFileMetadataRequest._Fields>, java.io.Serializable, Cloneable, Comparable<GetFileMetadataRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GetFileMetadataRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GetFileMetadataResult.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GetFileMetadataResult implements org.apache.thrift.TBase<GetFileMetadataResult, GetFileMetadataResult._Fields>, java.io.Serializable, Cloneable, Comparable<GetFileMetadataResult> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GetFileMetadataResult");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GetOpenTxnsInfoResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GetOpenTxnsInfoResponse implements org.apache.thrift.TBase<GetOpenTxnsInfoResponse, GetOpenTxnsInfoResponse._Fields>, java.io.Serializable, Cloneable, Comparable<GetOpenTxnsInfoResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GetOpenTxnsInfoResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GetOpenTxnsResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GetOpenTxnsResponse implements org.apache.thrift.TBase<GetOpenTxnsResponse, GetOpenTxnsResponse._Fields>, java.io.Serializable, Cloneable, Comparable<GetOpenTxnsResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GetOpenTxnsResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GetPrincipalsInRoleRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GetPrincipalsInRoleRequest implements org.apache.thrift.TBase<GetPrincipalsInRoleRequest, GetPrincipalsInRoleRequest._Fields>, java.io.Serializable, Cloneable, Comparable<GetPrincipalsInRoleRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GetPrincipalsInRoleRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GetPrincipalsInRoleResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GetPrincipalsInRoleResponse implements org.apache.thrift.TBase<GetPrincipalsInRoleResponse, GetPrincipalsInRoleResponse._Fields>, java.io.Serializable, Cloneable, Comparable<GetPrincipalsInRoleResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GetPrincipalsInRoleResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GetRoleGrantsForPrincipalRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GetRoleGrantsForPrincipalRequest implements org.apache.thrift.TBase<GetRoleGrantsForPrincipalRequest, GetRoleGrantsForPrincipalRequest._Fields>, java.io.Serializable, Cloneable, Comparable<GetRoleGrantsForPrincipalRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GetRoleGrantsForPrincipalRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GetRoleGrantsForPrincipalResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GetRoleGrantsForPrincipalResponse implements org.apache.thrift.TBase<GetRoleGrantsForPrincipalResponse, GetRoleGrantsForPrincipalResponse._Fields>, java.io.Serializable, Cloneable, Comparable<GetRoleGrantsForPrincipalResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GetRoleGrantsForPrincipalResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GrantRevokePrivilegeRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GrantRevokePrivilegeRequest implements org.apache.thrift.TBase<GrantRevokePrivilegeRequest, GrantRevokePrivilegeRequest._Fields>, java.io.Serializable, Cloneable, Comparable<GrantRevokePrivilegeRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GrantRevokePrivilegeRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GrantRevokePrivilegeResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GrantRevokePrivilegeResponse implements org.apache.thrift.TBase<GrantRevokePrivilegeResponse, GrantRevokePrivilegeResponse._Fields>, java.io.Serializable, Cloneable, Comparable<GrantRevokePrivilegeResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GrantRevokePrivilegeResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GrantRevokeRoleRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GrantRevokeRoleRequest implements org.apache.thrift.TBase<GrantRevokeRoleRequest, GrantRevokeRoleRequest._Fields>, java.io.Serializable, Cloneable, Comparable<GrantRevokeRoleRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GrantRevokeRoleRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GrantRevokeRoleResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class GrantRevokeRoleResponse implements org.apache.thrift.TBase<GrantRevokeRoleResponse, GrantRevokeRoleResponse._Fields>, java.io.Serializable, Cloneable, Comparable<GrantRevokeRoleResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GrantRevokeRoleResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/HeartbeatRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class HeartbeatRequest implements org.apache.thrift.TBase<HeartbeatRequest, HeartbeatRequest._Fields>, java.io.Serializable, Cloneable, Comparable<HeartbeatRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("HeartbeatRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/HeartbeatTxnRangeRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class HeartbeatTxnRangeRequest implements org.apache.thrift.TBase<HeartbeatTxnRangeRequest, HeartbeatTxnRangeRequest._Fields>, java.io.Serializable, Cloneable, Comparable<HeartbeatTxnRangeRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("HeartbeatTxnRangeRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/HeartbeatTxnRangeResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class HeartbeatTxnRangeResponse implements org.apache.thrift.TBase<HeartbeatTxnRangeResponse, HeartbeatTxnRangeResponse._Fields>, java.io.Serializable, Cloneable, Comparable<HeartbeatTxnRangeResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("HeartbeatTxnRangeResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/HiveObjectPrivilege.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class HiveObjectPrivilege implements org.apache.thrift.TBase<HiveObjectPrivilege, HiveObjectPrivilege._Fields>, java.io.Serializable, Cloneable, Comparable<HiveObjectPrivilege> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("HiveObjectPrivilege");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/HiveObjectRef.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class HiveObjectRef implements org.apache.thrift.TBase<HiveObjectRef, HiveObjectRef._Fields>, java.io.Serializable, Cloneable, Comparable<HiveObjectRef> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("HiveObjectRef");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Index.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Index implements org.apache.thrift.TBase<Index, Index._Fields>, java.io.Serializable, Cloneable, Comparable<Index> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Index");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/IndexAlreadyExistsException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class IndexAlreadyExistsException extends TException implements org.apache.thrift.TBase<IndexAlreadyExistsException, IndexAlreadyExistsException._Fields>, java.io.Serializable, Cloneable, Comparable<IndexAlreadyExistsException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("IndexAlreadyExistsException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/InsertEventRequestData.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class InsertEventRequestData implements org.apache.thrift.TBase<InsertEventRequestData, InsertEventRequestData._Fields>, java.io.Serializable, Cloneable, Comparable<InsertEventRequestData> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("InsertEventRequestData");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/InvalidInputException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class InvalidInputException extends TException implements org.apache.thrift.TBase<InvalidInputException, InvalidInputException._Fields>, java.io.Serializable, Cloneable, Comparable<InvalidInputException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("InvalidInputException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/InvalidObjectException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class InvalidObjectException extends TException implements org.apache.thrift.TBase<InvalidObjectException, InvalidObjectException._Fields>, java.io.Serializable, Cloneable, Comparable<InvalidObjectException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("InvalidObjectException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/InvalidOperationException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class InvalidOperationException extends TException implements org.apache.thrift.TBase<InvalidOperationException, InvalidOperationException._Fields>, java.io.Serializable, Cloneable, Comparable<InvalidOperationException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("InvalidOperationException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/InvalidPartitionException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class InvalidPartitionException extends TException implements org.apache.thrift.TBase<InvalidPartitionException, InvalidPartitionException._Fields>, java.io.Serializable, Cloneable, Comparable<InvalidPartitionException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("InvalidPartitionException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LockComponent.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class LockComponent implements org.apache.thrift.TBase<LockComponent, LockComponent._Fields>, java.io.Serializable, Cloneable, Comparable<LockComponent> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("LockComponent");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LockRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class LockRequest implements org.apache.thrift.TBase<LockRequest, LockRequest._Fields>, java.io.Serializable, Cloneable, Comparable<LockRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("LockRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LockResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class LockResponse implements org.apache.thrift.TBase<LockResponse, LockResponse._Fields>, java.io.Serializable, Cloneable, Comparable<LockResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("LockResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/LongColumnStatsData.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class LongColumnStatsData implements org.apache.thrift.TBase<LongColumnStatsData, LongColumnStatsData._Fields>, java.io.Serializable, Cloneable, Comparable<LongColumnStatsData> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("LongColumnStatsData");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/MetaException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class MetaException extends TException implements org.apache.thrift.TBase<MetaException, MetaException._Fields>, java.io.Serializable, Cloneable, Comparable<MetaException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("MetaException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/MetadataPpdResult.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class MetadataPpdResult implements org.apache.thrift.TBase<MetadataPpdResult, MetadataPpdResult._Fields>, java.io.Serializable, Cloneable, Comparable<MetadataPpdResult> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("MetadataPpdResult");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/NoSuchLockException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class NoSuchLockException extends TException implements org.apache.thrift.TBase<NoSuchLockException, NoSuchLockException._Fields>, java.io.Serializable, Cloneable, Comparable<NoSuchLockException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("NoSuchLockException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/NoSuchObjectException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class NoSuchObjectException extends TException implements org.apache.thrift.TBase<NoSuchObjectException, NoSuchObjectException._Fields>, java.io.Serializable, Cloneable, Comparable<NoSuchObjectException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("NoSuchObjectException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/NoSuchTxnException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class NoSuchTxnException extends TException implements org.apache.thrift.TBase<NoSuchTxnException, NoSuchTxnException._Fields>, java.io.Serializable, Cloneable, Comparable<NoSuchTxnException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("NoSuchTxnException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/NotificationEvent.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class NotificationEvent implements org.apache.thrift.TBase<NotificationEvent, NotificationEvent._Fields>, java.io.Serializable, Cloneable, Comparable<NotificationEvent> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("NotificationEvent");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/NotificationEventRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class NotificationEventRequest implements org.apache.thrift.TBase<NotificationEventRequest, NotificationEventRequest._Fields>, java.io.Serializable, Cloneable, Comparable<NotificationEventRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("NotificationEventRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/NotificationEventResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class NotificationEventResponse implements org.apache.thrift.TBase<NotificationEventResponse, NotificationEventResponse._Fields>, java.io.Serializable, Cloneable, Comparable<NotificationEventResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("NotificationEventResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/OpenTxnRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class OpenTxnRequest implements org.apache.thrift.TBase<OpenTxnRequest, OpenTxnRequest._Fields>, java.io.Serializable, Cloneable, Comparable<OpenTxnRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("OpenTxnRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/OpenTxnsResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class OpenTxnsResponse implements org.apache.thrift.TBase<OpenTxnsResponse, OpenTxnsResponse._Fields>, java.io.Serializable, Cloneable, Comparable<OpenTxnsResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("OpenTxnsResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Order.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Order implements org.apache.thrift.TBase<Order, Order._Fields>, java.io.Serializable, Cloneable, Comparable<Order> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Order");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Partition.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Partition implements org.apache.thrift.TBase<Partition, Partition._Fields>, java.io.Serializable, Cloneable, Comparable<Partition> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Partition");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PartitionListComposingSpec.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PartitionListComposingSpec implements org.apache.thrift.TBase<PartitionListComposingSpec, PartitionListComposingSpec._Fields>, java.io.Serializable, Cloneable, Comparable<PartitionListComposingSpec> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PartitionListComposingSpec");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PartitionSpec.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PartitionSpec implements org.apache.thrift.TBase<PartitionSpec, PartitionSpec._Fields>, java.io.Serializable, Cloneable, Comparable<PartitionSpec> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PartitionSpec");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PartitionSpecWithSharedSD.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PartitionSpecWithSharedSD implements org.apache.thrift.TBase<PartitionSpecWithSharedSD, PartitionSpecWithSharedSD._Fields>, java.io.Serializable, Cloneable, Comparable<PartitionSpecWithSharedSD> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PartitionSpecWithSharedSD");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PartitionWithoutSD.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PartitionWithoutSD implements org.apache.thrift.TBase<PartitionWithoutSD, PartitionWithoutSD._Fields>, java.io.Serializable, Cloneable, Comparable<PartitionWithoutSD> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PartitionWithoutSD");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PartitionsByExprRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PartitionsByExprRequest implements org.apache.thrift.TBase<PartitionsByExprRequest, PartitionsByExprRequest._Fields>, java.io.Serializable, Cloneable, Comparable<PartitionsByExprRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PartitionsByExprRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PartitionsByExprResult.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PartitionsByExprResult implements org.apache.thrift.TBase<PartitionsByExprResult, PartitionsByExprResult._Fields>, java.io.Serializable, Cloneable, Comparable<PartitionsByExprResult> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PartitionsByExprResult");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PartitionsStatsRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PartitionsStatsRequest implements org.apache.thrift.TBase<PartitionsStatsRequest, PartitionsStatsRequest._Fields>, java.io.Serializable, Cloneable, Comparable<PartitionsStatsRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PartitionsStatsRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PartitionsStatsResult.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PartitionsStatsResult implements org.apache.thrift.TBase<PartitionsStatsResult, PartitionsStatsResult._Fields>, java.io.Serializable, Cloneable, Comparable<PartitionsStatsResult> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PartitionsStatsResult");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PrincipalPrivilegeSet.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PrincipalPrivilegeSet implements org.apache.thrift.TBase<PrincipalPrivilegeSet, PrincipalPrivilegeSet._Fields>, java.io.Serializable, Cloneable, Comparable<PrincipalPrivilegeSet> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PrincipalPrivilegeSet");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PrivilegeBag.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PrivilegeBag implements org.apache.thrift.TBase<PrivilegeBag, PrivilegeBag._Fields>, java.io.Serializable, Cloneable, Comparable<PrivilegeBag> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PrivilegeBag");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PrivilegeGrantInfo.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PrivilegeGrantInfo implements org.apache.thrift.TBase<PrivilegeGrantInfo, PrivilegeGrantInfo._Fields>, java.io.Serializable, Cloneable, Comparable<PrivilegeGrantInfo> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PrivilegeGrantInfo");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PutFileMetadataRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PutFileMetadataRequest implements org.apache.thrift.TBase<PutFileMetadataRequest, PutFileMetadataRequest._Fields>, java.io.Serializable, Cloneable, Comparable<PutFileMetadataRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PutFileMetadataRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/PutFileMetadataResult.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class PutFileMetadataResult implements org.apache.thrift.TBase<PutFileMetadataResult, PutFileMetadataResult._Fields>, java.io.Serializable, Cloneable, Comparable<PutFileMetadataResult> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("PutFileMetadataResult");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ResourceUri.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ResourceUri implements org.apache.thrift.TBase<ResourceUri, ResourceUri._Fields>, java.io.Serializable, Cloneable, Comparable<ResourceUri> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ResourceUri");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Role.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Role implements org.apache.thrift.TBase<Role, Role._Fields>, java.io.Serializable, Cloneable, Comparable<Role> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Role");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/RolePrincipalGrant.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class RolePrincipalGrant implements org.apache.thrift.TBase<RolePrincipalGrant, RolePrincipalGrant._Fields>, java.io.Serializable, Cloneable, Comparable<RolePrincipalGrant> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("RolePrincipalGrant");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Schema.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Schema implements org.apache.thrift.TBase<Schema, Schema._Fields>, java.io.Serializable, Cloneable, Comparable<Schema> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Schema");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/SerDeInfo.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class SerDeInfo implements org.apache.thrift.TBase<SerDeInfo, SerDeInfo._Fields>, java.io.Serializable, Cloneable, Comparable<SerDeInfo> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("SerDeInfo");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/SetPartitionsStatsRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class SetPartitionsStatsRequest implements org.apache.thrift.TBase<SetPartitionsStatsRequest, SetPartitionsStatsRequest._Fields>, java.io.Serializable, Cloneable, Comparable<SetPartitionsStatsRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("SetPartitionsStatsRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ShowCompactRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ShowCompactRequest implements org.apache.thrift.TBase<ShowCompactRequest, ShowCompactRequest._Fields>, java.io.Serializable, Cloneable, Comparable<ShowCompactRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ShowCompactRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ShowCompactResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ShowCompactResponse implements org.apache.thrift.TBase<ShowCompactResponse, ShowCompactResponse._Fields>, java.io.Serializable, Cloneable, Comparable<ShowCompactResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ShowCompactResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ShowCompactResponseElement.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ShowCompactResponseElement implements org.apache.thrift.TBase<ShowCompactResponseElement, ShowCompactResponseElement._Fields>, java.io.Serializable, Cloneable, Comparable<ShowCompactResponseElement> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ShowCompactResponseElement");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ShowLocksRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ShowLocksRequest implements org.apache.thrift.TBase<ShowLocksRequest, ShowLocksRequest._Fields>, java.io.Serializable, Cloneable, Comparable<ShowLocksRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ShowLocksRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ShowLocksResponse.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ShowLocksResponse implements org.apache.thrift.TBase<ShowLocksResponse, ShowLocksResponse._Fields>, java.io.Serializable, Cloneable, Comparable<ShowLocksResponse> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ShowLocksResponse");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ShowLocksResponseElement.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ShowLocksResponseElement implements org.apache.thrift.TBase<ShowLocksResponseElement, ShowLocksResponseElement._Fields>, java.io.Serializable, Cloneable, Comparable<ShowLocksResponseElement> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ShowLocksResponseElement");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/SkewedInfo.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class SkewedInfo implements org.apache.thrift.TBase<SkewedInfo, SkewedInfo._Fields>, java.io.Serializable, Cloneable, Comparable<SkewedInfo> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("SkewedInfo");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/StorageDescriptor.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class StorageDescriptor implements org.apache.thrift.TBase<StorageDescriptor, StorageDescriptor._Fields>, java.io.Serializable, Cloneable, Comparable<StorageDescriptor> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("StorageDescriptor");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/StringColumnStatsData.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class StringColumnStatsData implements org.apache.thrift.TBase<StringColumnStatsData, StringColumnStatsData._Fields>, java.io.Serializable, Cloneable, Comparable<StringColumnStatsData> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("StringColumnStatsData");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Table implements org.apache.thrift.TBase<Table, Table._Fields>, java.io.Serializable, Cloneable, Comparable<Table> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Table");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/TableStatsRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TableStatsRequest implements org.apache.thrift.TBase<TableStatsRequest, TableStatsRequest._Fields>, java.io.Serializable, Cloneable, Comparable<TableStatsRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TableStatsRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/TableStatsResult.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TableStatsResult implements org.apache.thrift.TBase<TableStatsResult, TableStatsResult._Fields>, java.io.Serializable, Cloneable, Comparable<TableStatsResult> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TableStatsResult");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ThriftHiveMetastore {
 
   /**

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/TxnAbortedException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TxnAbortedException extends TException implements org.apache.thrift.TBase<TxnAbortedException, TxnAbortedException._Fields>, java.io.Serializable, Cloneable, Comparable<TxnAbortedException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TxnAbortedException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/TxnInfo.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TxnInfo implements org.apache.thrift.TBase<TxnInfo, TxnInfo._Fields>, java.io.Serializable, Cloneable, Comparable<TxnInfo> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TxnInfo");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/TxnOpenException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TxnOpenException extends TException implements org.apache.thrift.TBase<TxnOpenException, TxnOpenException._Fields>, java.io.Serializable, Cloneable, Comparable<TxnOpenException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TxnOpenException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Type.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Type implements org.apache.thrift.TBase<Type, Type._Fields>, java.io.Serializable, Cloneable, Comparable<Type> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Type");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/UnknownDBException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class UnknownDBException extends TException implements org.apache.thrift.TBase<UnknownDBException, UnknownDBException._Fields>, java.io.Serializable, Cloneable, Comparable<UnknownDBException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("UnknownDBException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/UnknownPartitionException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class UnknownPartitionException extends TException implements org.apache.thrift.TBase<UnknownPartitionException, UnknownPartitionException._Fields>, java.io.Serializable, Cloneable, Comparable<UnknownPartitionException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("UnknownPartitionException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/UnknownTableException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class UnknownTableException extends TException implements org.apache.thrift.TBase<UnknownTableException, UnknownTableException._Fields>, java.io.Serializable, Cloneable, Comparable<UnknownTableException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("UnknownTableException");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/UnlockRequest.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class UnlockRequest implements org.apache.thrift.TBase<UnlockRequest, UnlockRequest._Fields>, java.io.Serializable, Cloneable, Comparable<UnlockRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("UnlockRequest");
 

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Version.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Version implements org.apache.thrift.TBase<Version, Version._Fields>, java.io.Serializable, Cloneable, Comparable<Version> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Version");
 

File: metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -79,6 +79,7 @@
 import org.apache.hadoop.hive.metastore.api.CurrentNotificationEventId;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.FileMetadataExprType;
 import org.apache.hadoop.hive.metastore.api.Function;
 import org.apache.hadoop.hive.metastore.api.FunctionType;
 import org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege;
@@ -7666,7 +7667,7 @@ public boolean isFileMetadataSupported() {
   }
 
   @Override
-  public void getFileMetadataByExpr(List<Long> fileIds, byte[] expr,
+  public void getFileMetadataByExpr(List<Long> fileIds, FileMetadataExprType type, byte[] expr,
       ByteBuffer[] metadatas, ByteBuffer[] stripeBitsets, boolean[] eliminated) {
     throw new UnsupportedOperationException();
   }

File: metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java
Patch:
@@ -1747,7 +1747,7 @@ ByteBuffer[] getFileMetadata(List<Long> fileIds) throws IOException {
    * @param fileIds file ID list.
    * @return Serialized file metadata.
    */
-  void getFileMetadata(List<Long> fileIds, ByteBuffer[] result) throws IOException {
+  public void getFileMetadata(List<Long> fileIds, ByteBuffer[] result) throws IOException {
     byte[][] keys = new byte[fileIds.size()][];
     for (int i = 0; i < fileIds.size(); ++i) {
       keys[i] = HBaseUtils.makeLongKey(fileIds.get(i));

File: metastore/src/test/org/apache/hadoop/hive/metastore/DummyRawStoreControlledCommit.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hive.metastore.api.ColumnStatistics;
 import org.apache.hadoop.hive.metastore.api.CurrentNotificationEventId;
 import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.FileMetadataExprType;
 import org.apache.hadoop.hive.metastore.api.Function;
 import org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege;
 import org.apache.hadoop.hive.metastore.api.Index;
@@ -777,7 +778,7 @@ public boolean isFileMetadataSupported() {
 
 
   @Override
-  public void getFileMetadataByExpr(List<Long> fileIds, byte[] expr,
+  public void getFileMetadataByExpr(List<Long> fileIds, FileMetadataExprType type, byte[] expr,
       ByteBuffer[] metadatas, ByteBuffer[] stripeBitsets, boolean[] eliminated) {
   }
 }

File: metastore/src/test/org/apache/hadoop/hive/metastore/DummyRawStoreForJdoConnection.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.metastore.api.ColumnStatistics;
 import org.apache.hadoop.hive.metastore.api.CurrentNotificationEventId;
 import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.FileMetadataExprType;
 import org.apache.hadoop.hive.metastore.api.Function;
 import org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege;
 import org.apache.hadoop.hive.metastore.api.Index;
@@ -793,7 +794,7 @@ public boolean isFileMetadataSupported() {
   }
 
   @Override
-  public void getFileMetadataByExpr(List<Long> fileIds, byte[] expr,
+  public void getFileMetadataByExpr(List<Long> fileIds, FileMetadataExprType type, byte[] expr,
       ByteBuffer[] metadatas, ByteBuffer[] stripeBitsets, boolean[] eliminated) {
   }
 }

File: ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Adjacency.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Adjacency implements org.apache.thrift.TBase<Adjacency, Adjacency._Fields>, java.io.Serializable, Cloneable, Comparable<Adjacency> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Adjacency");
 

File: ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Graph.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Graph implements org.apache.thrift.TBase<Graph, Graph._Fields>, java.io.Serializable, Cloneable, Comparable<Graph> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Graph");
 

File: ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Operator.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Operator implements org.apache.thrift.TBase<Operator, Operator._Fields>, java.io.Serializable, Cloneable, Comparable<Operator> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Operator");
 

File: ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Query.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Query implements org.apache.thrift.TBase<Query, Query._Fields>, java.io.Serializable, Cloneable, Comparable<Query> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Query");
 

File: ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/QueryPlan.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class QueryPlan implements org.apache.thrift.TBase<QueryPlan, QueryPlan._Fields>, java.io.Serializable, Cloneable, Comparable<QueryPlan> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("QueryPlan");
 

File: ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Stage.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Stage implements org.apache.thrift.TBase<Stage, Stage._Fields>, java.io.Serializable, Cloneable, Comparable<Stage> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Stage");
 

File: ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Task.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Task implements org.apache.thrift.TBase<Task, Task._Fields>, java.io.Serializable, Cloneable, Comparable<Task> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Task");
 

File: serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde/test/InnerStruct.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class InnerStruct implements org.apache.thrift.TBase<InnerStruct, InnerStruct._Fields>, java.io.Serializable, Cloneable, Comparable<InnerStruct> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("InnerStruct");
 

File: serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde/test/ThriftTestObj.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ThriftTestObj implements org.apache.thrift.TBase<ThriftTestObj, ThriftTestObj._Fields>, java.io.Serializable, Cloneable, Comparable<ThriftTestObj> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ThriftTestObj");
 

File: serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/Complex.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class Complex implements org.apache.thrift.TBase<Complex, Complex._Fields>, java.io.Serializable, Cloneable, Comparable<Complex> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Complex");
 

File: serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/IntString.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class IntString implements org.apache.thrift.TBase<IntString, IntString._Fields>, java.io.Serializable, Cloneable, Comparable<IntString> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("IntString");
 

File: serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MegaStruct.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class MegaStruct implements org.apache.thrift.TBase<MegaStruct, MegaStruct._Fields>, java.io.Serializable, Cloneable, Comparable<MegaStruct> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("MegaStruct");
 

File: serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/MiniStruct.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class MiniStruct implements org.apache.thrift.TBase<MiniStruct, MiniStruct._Fields>, java.io.Serializable, Cloneable, Comparable<MiniStruct> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("MiniStruct");
 

File: serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde2/thrift/test/SetIntString.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class SetIntString implements org.apache.thrift.TBase<SetIntString, SetIntString._Fields>, java.io.Serializable, Cloneable, Comparable<SetIntString> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("SetIntString");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/service/HiveClusterStatus.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class HiveClusterStatus implements org.apache.thrift.TBase<HiveClusterStatus, HiveClusterStatus._Fields>, java.io.Serializable, Cloneable, Comparable<HiveClusterStatus> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("HiveClusterStatus");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/service/HiveServerException.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class HiveServerException extends TException implements org.apache.thrift.TBase<HiveServerException, HiveServerException._Fields>, java.io.Serializable, Cloneable, Comparable<HiveServerException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("HiveServerException");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/service/ThriftHive.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class ThriftHive {
 
   public interface Iface extends org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.Iface {

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TArrayTypeEntry.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TArrayTypeEntry implements org.apache.thrift.TBase<TArrayTypeEntry, TArrayTypeEntry._Fields>, java.io.Serializable, Cloneable, Comparable<TArrayTypeEntry> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TArrayTypeEntry");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TBinaryColumn.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TBinaryColumn implements org.apache.thrift.TBase<TBinaryColumn, TBinaryColumn._Fields>, java.io.Serializable, Cloneable, Comparable<TBinaryColumn> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TBinaryColumn");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TBoolColumn.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TBoolColumn implements org.apache.thrift.TBase<TBoolColumn, TBoolColumn._Fields>, java.io.Serializable, Cloneable, Comparable<TBoolColumn> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TBoolColumn");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TBoolValue.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TBoolValue implements org.apache.thrift.TBase<TBoolValue, TBoolValue._Fields>, java.io.Serializable, Cloneable, Comparable<TBoolValue> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TBoolValue");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TByteColumn.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TByteColumn implements org.apache.thrift.TBase<TByteColumn, TByteColumn._Fields>, java.io.Serializable, Cloneable, Comparable<TByteColumn> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TByteColumn");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TByteValue.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TByteValue implements org.apache.thrift.TBase<TByteValue, TByteValue._Fields>, java.io.Serializable, Cloneable, Comparable<TByteValue> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TByteValue");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TCLIService.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TCLIService {
 
   public interface Iface {

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TCancelDelegationTokenReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TCancelDelegationTokenReq implements org.apache.thrift.TBase<TCancelDelegationTokenReq, TCancelDelegationTokenReq._Fields>, java.io.Serializable, Cloneable, Comparable<TCancelDelegationTokenReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TCancelDelegationTokenReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TCancelDelegationTokenResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TCancelDelegationTokenResp implements org.apache.thrift.TBase<TCancelDelegationTokenResp, TCancelDelegationTokenResp._Fields>, java.io.Serializable, Cloneable, Comparable<TCancelDelegationTokenResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TCancelDelegationTokenResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TCancelOperationReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TCancelOperationReq implements org.apache.thrift.TBase<TCancelOperationReq, TCancelOperationReq._Fields>, java.io.Serializable, Cloneable, Comparable<TCancelOperationReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TCancelOperationReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TCancelOperationResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TCancelOperationResp implements org.apache.thrift.TBase<TCancelOperationResp, TCancelOperationResp._Fields>, java.io.Serializable, Cloneable, Comparable<TCancelOperationResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TCancelOperationResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TCloseOperationReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TCloseOperationReq implements org.apache.thrift.TBase<TCloseOperationReq, TCloseOperationReq._Fields>, java.io.Serializable, Cloneable, Comparable<TCloseOperationReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TCloseOperationReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TCloseOperationResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TCloseOperationResp implements org.apache.thrift.TBase<TCloseOperationResp, TCloseOperationResp._Fields>, java.io.Serializable, Cloneable, Comparable<TCloseOperationResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TCloseOperationResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TCloseSessionReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TCloseSessionReq implements org.apache.thrift.TBase<TCloseSessionReq, TCloseSessionReq._Fields>, java.io.Serializable, Cloneable, Comparable<TCloseSessionReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TCloseSessionReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TCloseSessionResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TCloseSessionResp implements org.apache.thrift.TBase<TCloseSessionResp, TCloseSessionResp._Fields>, java.io.Serializable, Cloneable, Comparable<TCloseSessionResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TCloseSessionResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TColumnDesc.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TColumnDesc implements org.apache.thrift.TBase<TColumnDesc, TColumnDesc._Fields>, java.io.Serializable, Cloneable, Comparable<TColumnDesc> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TColumnDesc");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TDoubleColumn.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TDoubleColumn implements org.apache.thrift.TBase<TDoubleColumn, TDoubleColumn._Fields>, java.io.Serializable, Cloneable, Comparable<TDoubleColumn> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TDoubleColumn");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TDoubleValue.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TDoubleValue implements org.apache.thrift.TBase<TDoubleValue, TDoubleValue._Fields>, java.io.Serializable, Cloneable, Comparable<TDoubleValue> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TDoubleValue");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TExecuteStatementReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TExecuteStatementReq implements org.apache.thrift.TBase<TExecuteStatementReq, TExecuteStatementReq._Fields>, java.io.Serializable, Cloneable, Comparable<TExecuteStatementReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TExecuteStatementReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TExecuteStatementResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TExecuteStatementResp implements org.apache.thrift.TBase<TExecuteStatementResp, TExecuteStatementResp._Fields>, java.io.Serializable, Cloneable, Comparable<TExecuteStatementResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TExecuteStatementResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TFetchResultsReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TFetchResultsReq implements org.apache.thrift.TBase<TFetchResultsReq, TFetchResultsReq._Fields>, java.io.Serializable, Cloneable, Comparable<TFetchResultsReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TFetchResultsReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TFetchResultsResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TFetchResultsResp implements org.apache.thrift.TBase<TFetchResultsResp, TFetchResultsResp._Fields>, java.io.Serializable, Cloneable, Comparable<TFetchResultsResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TFetchResultsResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetCatalogsReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetCatalogsReq implements org.apache.thrift.TBase<TGetCatalogsReq, TGetCatalogsReq._Fields>, java.io.Serializable, Cloneable, Comparable<TGetCatalogsReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetCatalogsReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetCatalogsResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetCatalogsResp implements org.apache.thrift.TBase<TGetCatalogsResp, TGetCatalogsResp._Fields>, java.io.Serializable, Cloneable, Comparable<TGetCatalogsResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetCatalogsResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetColumnsReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetColumnsReq implements org.apache.thrift.TBase<TGetColumnsReq, TGetColumnsReq._Fields>, java.io.Serializable, Cloneable, Comparable<TGetColumnsReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetColumnsReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetColumnsResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetColumnsResp implements org.apache.thrift.TBase<TGetColumnsResp, TGetColumnsResp._Fields>, java.io.Serializable, Cloneable, Comparable<TGetColumnsResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetColumnsResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetDelegationTokenReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetDelegationTokenReq implements org.apache.thrift.TBase<TGetDelegationTokenReq, TGetDelegationTokenReq._Fields>, java.io.Serializable, Cloneable, Comparable<TGetDelegationTokenReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetDelegationTokenReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetDelegationTokenResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetDelegationTokenResp implements org.apache.thrift.TBase<TGetDelegationTokenResp, TGetDelegationTokenResp._Fields>, java.io.Serializable, Cloneable, Comparable<TGetDelegationTokenResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetDelegationTokenResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetFunctionsReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetFunctionsReq implements org.apache.thrift.TBase<TGetFunctionsReq, TGetFunctionsReq._Fields>, java.io.Serializable, Cloneable, Comparable<TGetFunctionsReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetFunctionsReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetFunctionsResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetFunctionsResp implements org.apache.thrift.TBase<TGetFunctionsResp, TGetFunctionsResp._Fields>, java.io.Serializable, Cloneable, Comparable<TGetFunctionsResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetFunctionsResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetInfoReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetInfoReq implements org.apache.thrift.TBase<TGetInfoReq, TGetInfoReq._Fields>, java.io.Serializable, Cloneable, Comparable<TGetInfoReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetInfoReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetInfoResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetInfoResp implements org.apache.thrift.TBase<TGetInfoResp, TGetInfoResp._Fields>, java.io.Serializable, Cloneable, Comparable<TGetInfoResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetInfoResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetOperationStatusReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetOperationStatusReq implements org.apache.thrift.TBase<TGetOperationStatusReq, TGetOperationStatusReq._Fields>, java.io.Serializable, Cloneable, Comparable<TGetOperationStatusReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetOperationStatusReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetOperationStatusResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetOperationStatusResp implements org.apache.thrift.TBase<TGetOperationStatusResp, TGetOperationStatusResp._Fields>, java.io.Serializable, Cloneable, Comparable<TGetOperationStatusResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetOperationStatusResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetResultSetMetadataReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetResultSetMetadataReq implements org.apache.thrift.TBase<TGetResultSetMetadataReq, TGetResultSetMetadataReq._Fields>, java.io.Serializable, Cloneable, Comparable<TGetResultSetMetadataReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetResultSetMetadataReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetResultSetMetadataResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetResultSetMetadataResp implements org.apache.thrift.TBase<TGetResultSetMetadataResp, TGetResultSetMetadataResp._Fields>, java.io.Serializable, Cloneable, Comparable<TGetResultSetMetadataResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetResultSetMetadataResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetSchemasReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetSchemasReq implements org.apache.thrift.TBase<TGetSchemasReq, TGetSchemasReq._Fields>, java.io.Serializable, Cloneable, Comparable<TGetSchemasReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetSchemasReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetSchemasResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetSchemasResp implements org.apache.thrift.TBase<TGetSchemasResp, TGetSchemasResp._Fields>, java.io.Serializable, Cloneable, Comparable<TGetSchemasResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetSchemasResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetTableTypesReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetTableTypesReq implements org.apache.thrift.TBase<TGetTableTypesReq, TGetTableTypesReq._Fields>, java.io.Serializable, Cloneable, Comparable<TGetTableTypesReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetTableTypesReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetTableTypesResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetTableTypesResp implements org.apache.thrift.TBase<TGetTableTypesResp, TGetTableTypesResp._Fields>, java.io.Serializable, Cloneable, Comparable<TGetTableTypesResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetTableTypesResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetTablesReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetTablesReq implements org.apache.thrift.TBase<TGetTablesReq, TGetTablesReq._Fields>, java.io.Serializable, Cloneable, Comparable<TGetTablesReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetTablesReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetTablesResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetTablesResp implements org.apache.thrift.TBase<TGetTablesResp, TGetTablesResp._Fields>, java.io.Serializable, Cloneable, Comparable<TGetTablesResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetTablesResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetTypeInfoReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetTypeInfoReq implements org.apache.thrift.TBase<TGetTypeInfoReq, TGetTypeInfoReq._Fields>, java.io.Serializable, Cloneable, Comparable<TGetTypeInfoReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetTypeInfoReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetTypeInfoResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TGetTypeInfoResp implements org.apache.thrift.TBase<TGetTypeInfoResp, TGetTypeInfoResp._Fields>, java.io.Serializable, Cloneable, Comparable<TGetTypeInfoResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TGetTypeInfoResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/THandleIdentifier.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class THandleIdentifier implements org.apache.thrift.TBase<THandleIdentifier, THandleIdentifier._Fields>, java.io.Serializable, Cloneable, Comparable<THandleIdentifier> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("THandleIdentifier");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TI16Column.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TI16Column implements org.apache.thrift.TBase<TI16Column, TI16Column._Fields>, java.io.Serializable, Cloneable, Comparable<TI16Column> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TI16Column");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TI16Value.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TI16Value implements org.apache.thrift.TBase<TI16Value, TI16Value._Fields>, java.io.Serializable, Cloneable, Comparable<TI16Value> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TI16Value");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TI32Column.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TI32Column implements org.apache.thrift.TBase<TI32Column, TI32Column._Fields>, java.io.Serializable, Cloneable, Comparable<TI32Column> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TI32Column");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TI32Value.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TI32Value implements org.apache.thrift.TBase<TI32Value, TI32Value._Fields>, java.io.Serializable, Cloneable, Comparable<TI32Value> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TI32Value");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TI64Column.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TI64Column implements org.apache.thrift.TBase<TI64Column, TI64Column._Fields>, java.io.Serializable, Cloneable, Comparable<TI64Column> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TI64Column");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TI64Value.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TI64Value implements org.apache.thrift.TBase<TI64Value, TI64Value._Fields>, java.io.Serializable, Cloneable, Comparable<TI64Value> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TI64Value");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TMapTypeEntry.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TMapTypeEntry implements org.apache.thrift.TBase<TMapTypeEntry, TMapTypeEntry._Fields>, java.io.Serializable, Cloneable, Comparable<TMapTypeEntry> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TMapTypeEntry");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TOpenSessionReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TOpenSessionReq implements org.apache.thrift.TBase<TOpenSessionReq, TOpenSessionReq._Fields>, java.io.Serializable, Cloneable, Comparable<TOpenSessionReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TOpenSessionReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TOpenSessionResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TOpenSessionResp implements org.apache.thrift.TBase<TOpenSessionResp, TOpenSessionResp._Fields>, java.io.Serializable, Cloneable, Comparable<TOpenSessionResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TOpenSessionResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TOperationHandle.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TOperationHandle implements org.apache.thrift.TBase<TOperationHandle, TOperationHandle._Fields>, java.io.Serializable, Cloneable, Comparable<TOperationHandle> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TOperationHandle");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TPrimitiveTypeEntry.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TPrimitiveTypeEntry implements org.apache.thrift.TBase<TPrimitiveTypeEntry, TPrimitiveTypeEntry._Fields>, java.io.Serializable, Cloneable, Comparable<TPrimitiveTypeEntry> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TPrimitiveTypeEntry");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TRenewDelegationTokenReq.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TRenewDelegationTokenReq implements org.apache.thrift.TBase<TRenewDelegationTokenReq, TRenewDelegationTokenReq._Fields>, java.io.Serializable, Cloneable, Comparable<TRenewDelegationTokenReq> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TRenewDelegationTokenReq");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TRenewDelegationTokenResp.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TRenewDelegationTokenResp implements org.apache.thrift.TBase<TRenewDelegationTokenResp, TRenewDelegationTokenResp._Fields>, java.io.Serializable, Cloneable, Comparable<TRenewDelegationTokenResp> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TRenewDelegationTokenResp");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TRow.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TRow implements org.apache.thrift.TBase<TRow, TRow._Fields>, java.io.Serializable, Cloneable, Comparable<TRow> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TRow");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TRowSet.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TRowSet implements org.apache.thrift.TBase<TRowSet, TRowSet._Fields>, java.io.Serializable, Cloneable, Comparable<TRowSet> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TRowSet");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TSessionHandle.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TSessionHandle implements org.apache.thrift.TBase<TSessionHandle, TSessionHandle._Fields>, java.io.Serializable, Cloneable, Comparable<TSessionHandle> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TSessionHandle");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TStatus.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TStatus implements org.apache.thrift.TBase<TStatus, TStatus._Fields>, java.io.Serializable, Cloneable, Comparable<TStatus> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TStatus");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TStringColumn.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TStringColumn implements org.apache.thrift.TBase<TStringColumn, TStringColumn._Fields>, java.io.Serializable, Cloneable, Comparable<TStringColumn> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TStringColumn");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TStringValue.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TStringValue implements org.apache.thrift.TBase<TStringValue, TStringValue._Fields>, java.io.Serializable, Cloneable, Comparable<TStringValue> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TStringValue");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TStructTypeEntry.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TStructTypeEntry implements org.apache.thrift.TBase<TStructTypeEntry, TStructTypeEntry._Fields>, java.io.Serializable, Cloneable, Comparable<TStructTypeEntry> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TStructTypeEntry");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TTableSchema.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TTableSchema implements org.apache.thrift.TBase<TTableSchema, TTableSchema._Fields>, java.io.Serializable, Cloneable, Comparable<TTableSchema> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TTableSchema");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TTypeDesc.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TTypeDesc implements org.apache.thrift.TBase<TTypeDesc, TTypeDesc._Fields>, java.io.Serializable, Cloneable, Comparable<TTypeDesc> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TTypeDesc");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TTypeQualifiers.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TTypeQualifiers implements org.apache.thrift.TBase<TTypeQualifiers, TTypeQualifiers._Fields>, java.io.Serializable, Cloneable, Comparable<TTypeQualifiers> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TTypeQualifiers");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TUnionTypeEntry.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TUnionTypeEntry implements org.apache.thrift.TBase<TUnionTypeEntry, TUnionTypeEntry._Fields>, java.io.Serializable, Cloneable, Comparable<TUnionTypeEntry> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TUnionTypeEntry");
 

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TUserDefinedTypeEntry.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-5")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-21")
 public class TUserDefinedTypeEntry implements org.apache.thrift.TBase<TUserDefinedTypeEntry, TUserDefinedTypeEntry._Fields>, java.io.Serializable, Cloneable, Comparable<TUserDefinedTypeEntry> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TUserDefinedTypeEntry");
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java
Patch:
@@ -104,7 +104,7 @@ public void initialize(
     }
 
     // limit * 64 : compensation of arrays for key/value/hashcodes
-    this.threshold = (long) (memUsage * Runtime.getRuntime().maxMemory()) - topN * 64;
+    this.threshold = (long) (memUsage * Runtime.getRuntime().freeMemory()) - topN * 64L;
     if (threshold < 0) {
       return;
     }

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java
Patch:
@@ -217,7 +217,7 @@ public LlapTaskUmbilicalProtocol run() throws Exception {
               serviceConsumerMetadata, envMap, startedInputsMap, taskReporter, executor,
               objectRegistry,
               pid,
-              executionContext, memoryAvailable);
+              executionContext, memoryAvailable, false);
         }
       }
       if (taskRunner == null) {

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -3168,7 +3168,7 @@ public AggrStats getAggrColStatsFor(String dbName, String tblName,
       return getMSC().getAggrColStatsFor(dbName, tblName, colNames, partName);
     } catch (Exception e) {
       LOG.debug(StringUtils.stringifyException(e));
-      return null;
+      return new AggrStats(new ArrayList<ColumnStatisticsObj>(),0);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ExplainWork.java
Patch:
@@ -79,7 +79,9 @@ public ExplainWork(Path resFile,
     this.fetchTask = fetchTask;
     this.astTree = astTree;
     this.analyzer = analyzer;
-    this.inputs = analyzer.getInputs();
+    if (analyzer != null) {
+      this.inputs = analyzer.getInputs();
+    }
     this.extended = extended;
     this.formatted = formatted;
     this.dependency = dependency;

File: ql/src/java/org/apache/hadoop/hive/ql/index/AggregateIndexHandler.java
Patch:
@@ -153,7 +153,6 @@ protected Task<?> getIndexBuilderMapRedTask(Set<ReadEntity> inputs,
       builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGETEZFILES, false);
       Task<?> rootTask = IndexUtils.createRootTask(builderConf, inputs, outputs,
           command, (LinkedHashMap<String, String>) partSpec, indexTableName, dbName);
-      super.setStatsDir(builderConf);
       return rootTask;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/index/bitmap/BitmapIndexHandler.java
Patch:
@@ -289,7 +289,6 @@ protected Task<?> getIndexBuilderMapRedTask(Set<ReadEntity> inputs, Set<WriteEnt
 
     Task<?> rootTask = IndexUtils.createRootTask(builderConf, inputs, outputs,
         command, partSpec, indexTableName, dbName);
-    super.setStatsDir(builderConf);
     return rootTask;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/index/compact/CompactIndexHandler.java
Patch:
@@ -150,7 +150,6 @@ protected Task<?> getIndexBuilderMapRedTask(Set<ReadEntity> inputs, Set<WriteEnt
     builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGETEZFILES, false);
     Task<?> rootTask = IndexUtils.createRootTask(builderConf, inputs, outputs,
         command, partSpec, indexTableName, dbName);
-    super.setStatsDir(builderConf);
     return rootTask;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
Patch:
@@ -64,6 +64,7 @@ public GenMRTableScan1() {
    * @param opProcCtx
    *          context
    */
+  @Override
   public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
       Object... nodeOutputs) throws SemanticException {
     TableScanOperator op = (TableScanOperator) nd;
@@ -121,6 +122,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
 
             StatsWork statsWork = new StatsWork(op.getConf().getTableMetadata().getTableSpec());
             statsWork.setAggKey(op.getConf().getStatsAggPrefix());
+            statsWork.setStatsTmpDir(op.getConf().getTmpStatsDir());
             statsWork.setSourceTask(currTask);
             statsWork.setStatsReliable(parseCtx.getConf().getBoolVar(
                 HiveConf.ConfVars.HIVE_STATS_RELIABLE));
@@ -195,6 +197,7 @@ private void handlePartialScanCommand(TableScanOperator op, GenMRProcContext ctx
     PartialScanWork scanWork = new PartialScanWork(inputPaths);
     scanWork.setMapperCannotSpanPartns(true);
     scanWork.setAggKey(aggregationKey);
+    scanWork.setStatsTmpDir(op.getConf().getTmpStatsDir(), parseCtx.getConf());
 
     // stats work
     statsWork.setPartialScanAnalyzeCommand(true);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -1416,7 +1416,7 @@ public static void addStatsTask(FileSinkOperator nd, MoveTask mvTask,
 
     statsWork.setSourceTask(currTask);
     statsWork.setStatsReliable(hconf.getBoolVar(ConfVars.HIVE_STATS_RELIABLE));
-
+    statsWork.setStatsTmpDir(nd.getConf().getStatsTmpDir());
     if (currTask.getWork() instanceof MapredWork) {
       MapredWork mrWork = (MapredWork) currTask.getWork();
       mrWork.getMapWork().setGatheringStats(true);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java
Patch:
@@ -70,7 +70,7 @@ public Object process(Node nd, Stack<Node> stack,
       throws SemanticException {
 
     GenTezProcContext context = (GenTezProcContext) procContext;
-    
+
     TableScanOperator tableScan = (TableScanOperator) nd;
 
     ParseContext parseContext = context.parseContext;
@@ -124,6 +124,7 @@ public Object process(Node nd, Stack<Node> stack,
 
       StatsWork statsWork = new StatsWork(tableScan.getConf().getTableMetadata().getTableSpec());
       statsWork.setAggKey(tableScan.getConf().getStatsAggPrefix());
+      statsWork.setStatsTmpDir(tableScan.getConf().getTmpStatsDir());
       statsWork.setSourceTask(context.currentTask);
       statsWork.setStatsReliable(parseContext.getConf().getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));
       Task<StatsWork> statsTask = TaskFactory.get(statsWork, parseContext.getConf());
@@ -181,6 +182,7 @@ private void handlePartialScanCommand(TableScanOperator tableScan, ParseContext
     PartialScanWork scanWork = new PartialScanWork(inputPaths);
     scanWork.setMapperCannotSpanPartns(true);
     scanWork.setAggKey(aggregationKey);
+    scanWork.setStatsTmpDir(tableScan.getConf().getTmpStatsDir(), parseContext.getConf());
 
     // stats work
     statsWork.setPartialScanAnalyzeCommand(true);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkProcessAnalyzeTable.java
Patch:
@@ -121,6 +121,7 @@ public Object process(Node nd, Stack<Node> stack,
 
         StatsWork statsWork = new StatsWork(tableScan.getConf().getTableMetadata().getTableSpec());
         statsWork.setAggKey(tableScan.getConf().getStatsAggPrefix());
+        statsWork.setStatsTmpDir(tableScan.getConf().getTmpStatsDir());
         statsWork.setSourceTask(context.currentTask);
         statsWork.setStatsReliable(parseContext.getConf().getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));
         Task<StatsWork> statsTask = TaskFactory.get(statsWork, parseContext.getConf());
@@ -176,6 +177,7 @@ private void handlePartialScanCommand(TableScanOperator tableScan, ParseContext
     PartialScanWork scanWork = new PartialScanWork(inputPaths);
     scanWork.setMapperCannotSpanPartns(true);
     scanWork.setAggKey(aggregationKey);
+    scanWork.setStatsTmpDir(tableScan.getConf().getTmpStatsDir(), parseContext.getConf());
 
     // stats work
     statsWork.setPartialScanAnalyzeCommand(true);

File: ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregatorSpark.java
Patch:
@@ -33,8 +33,8 @@ public class CounterStatsAggregatorSpark
 
   @SuppressWarnings("rawtypes")
   @Override
-  public boolean connect(Configuration hconf, Task sourceTask) {
-    SparkTask task = (SparkTask) sourceTask;
+  public boolean connect(StatsCollectionContext scc) {
+    SparkTask task = (SparkTask) scc.getTask();
     sparkCounters = task.getSparkCounters();
     if (sparkCounters == null) {
       return false;
@@ -52,7 +52,7 @@ public String aggregateStats(String keyPrefix, String statType) {
   }
 
   @Override
-  public boolean closeConnection() {
+  public boolean closeConnection(StatsCollectionContext scc) {
     return true;
   }
 

File: common/src/java/org/apache/hadoop/hive/ql/log/PerfLogger.java
Patch:
@@ -84,7 +84,7 @@ public class PerfLogger {
   protected static final ThreadLocal<PerfLogger> perfLogger = new ThreadLocal<PerfLogger>();
 
 
-  public PerfLogger() {
+  private PerfLogger() {
     // Use getPerfLogger to get an instance of PerfLogger
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnInfo.java
Patch:
@@ -107,7 +107,6 @@ public ColumnInfo(ColumnInfo columnInfo) {
     this.isVirtualCol = columnInfo.getIsVirtualCol();
     this.isHiddenVirtualCol = columnInfo.isHiddenVirtualCol();
     this.setType(columnInfo.getType());
-    this.typeName = columnInfo.getType().getTypeName();
   }
 
   public String getTypeName() {
@@ -133,6 +132,7 @@ public String getInternalName() {
   public void setType(TypeInfo type) {
     objectInspector =
         TypeInfoUtils.getStandardWritableObjectInspectorFromTypeInfo(type);
+    setTypeName(type.getTypeName());
   }
 
   public void setInternalName(String internalName) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -1124,8 +1124,7 @@ private RelNode genUnionLogicalPlan(String unionalias, String leftalias, RelNode
                   + " on second table"));
         }
         ColumnInfo unionColInfo = new ColumnInfo(lInfo);
-        unionColInfo.setType(FunctionRegistry.getCommonClassForUnionAll(lInfo.getType(),
-            rInfo.getType()));
+        unionColInfo.setType(commonTypeInfo);
         unionoutRR.put(unionalias, field, unionColInfo);
       }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -9026,8 +9026,7 @@ private Operator genUnionPlan(String unionalias, String leftalias,
                 + " on second table"));
       }
       ColumnInfo unionColInfo = new ColumnInfo(lInfo);
-      unionColInfo.setType(FunctionRegistry.getCommonClassForUnionAll(lInfo.getType(),
-          rInfo.getType()));
+      unionColInfo.setType(commonTypeInfo);
       unionoutRR.put(unionalias, field, unionColInfo);
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java
Patch:
@@ -159,8 +159,7 @@ private boolean update(ObjectInspector oi, boolean isUnionAll) throws UDFArgumen
       // a common base class or not.
       TypeInfo commonTypeInfo = null;
       if (isUnionAll) {
-        commonTypeInfo = FunctionRegistry.getCommonClassForUnionAll(oiTypeInfo,
-          rTypeInfo);
+        commonTypeInfo = FunctionRegistry.getCommonClassForUnionAll(rTypeInfo, oiTypeInfo);
       } else {
         commonTypeInfo = FunctionRegistry.getCommonClass(oiTypeInfo,
           rTypeInfo);

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java
Patch:
@@ -347,7 +347,7 @@ static OrcFile.WriterVersion getWriterVersion(int writerVersion) {
         return version;
       }
     }
-    return OrcFile.WriterVersion.ORIGINAL;
+    return OrcFile.WriterVersion.FUTURE;
   }
 
   /** Extracts the necessary metadata from an externally store buffer (fullFooterBuffer). */

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcRawRecordMerger.java
Patch:
@@ -896,7 +896,7 @@ synchronized void addedRow() throws IOException {
    */
   @Test
   public void testRecordReaderNewBaseAndDelta() throws Exception {
-    final int BUCKET = 10;
+    final int BUCKET = 11;
     Configuration conf = new Configuration();
     OrcOutputFormat of = new OrcOutputFormat();
     FileSystem fs = FileSystem.getLocal(conf);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -862,7 +862,7 @@ private String unparseExprForValuesClause(ASTNode expr) throws SemanticException
         return expr.getText();
 
       case HiveParser.StringLiteral:
-        return PlanUtils.stripQuotes(expr.getText());
+        return BaseSemanticAnalyzer.unescapeSQLString(expr.getText());
 
       case HiveParser.KW_FALSE:
         // UDFToBoolean casts any non-empty string to true, so set this to false

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -432,7 +432,7 @@ public enum ErrorMsg {
   UPDATE_CANNOT_UPDATE_BUCKET_VALUE(10302, "Updating values of bucketing columns is not supported.  Column {0}.", true),
   IMPORT_INTO_STRICT_REPL_TABLE(10303,"Non-repl import disallowed against table that is a destination of replication."),
   CTAS_LOCATION_NONEMPTY(10304, "CREATE-TABLE-AS-SELECT cannot create table with location to a non-empty directory."),
-
+  CTAS_CREATES_VOID_TYPE(10305, "CREATE-TABLE-AS-SELECT creates a VOID type, please use CAST to specify the type, near field: "),
   //========================== 20000 range starts here ========================//
   SCRIPT_INIT_ERROR(20000, "Unable to initialize custom script."),
   SCRIPT_IO_ERROR(20001, "An error occurred while reading or writing to your custom script. "

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
Patch:
@@ -625,8 +625,7 @@ public void processBatch(VectorizedRowBatch batch) throws HiveException {
           rowsToFlush[flushMark] = currentStreamingAggregators;
           if (keysToFlush[flushMark] == null) {
             keysToFlush[flushMark] = (VectorHashKeyWrapper) streamingKey.copyKey();
-          }
-          else {
+          } else {
             streamingKey.duplicateTo(keysToFlush[flushMark]);
           }
 
@@ -836,6 +835,8 @@ protected Collection<Future<?>> initializeOp(Configuration hconf) throws HiveExc
     } else if (conf.getVectorDesc().isReduceMergePartial()) {
       // Sorted GroupBy of vector batches where an individual batch has the same group key (e.g. reduce).
       processingMode = this.new ProcessingModeReduceMergePartialKeys();
+    } else if (conf.getVectorDesc().isReduceStreaming()) {
+      processingMode = this.new ProcessingModeUnsortedStreaming();
     } else {
       // We start in hash mode and may dynamically switch to unsorted stream mode.
       processingMode = this.new ProcessingModeHashAggregate();

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java
Patch:
@@ -27,8 +27,8 @@
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
 
-import com.google.common.annotations.VisibleForTesting;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hive.common.CallableWithNdc;
 import org.apache.hadoop.hive.llap.daemon.FragmentCompletionHandler;
 import org.apache.hadoop.hive.llap.daemon.HistoryLogger;
@@ -62,6 +62,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Stopwatch;
 import com.google.common.collect.HashMultimap;
 import com.google.common.collect.Multimap;
@@ -232,8 +233,7 @@ public LlapTaskUmbilicalProtocol run() throws Exception {
         isCompleted.set(true);
         return result;
       } finally {
-        // TODO Fix UGI and FS Handling. Closing UGI here causes some errors right now.
-        //        FileSystem.closeAllForUGI(taskUgi);
+        FileSystem.closeAllForUGI(taskUgi);
         LOG.info("ExecutionTime for Container: " + request.getContainerIdString() + "=" +
             runtimeWatch.stop().elapsedMillis());
         if (LOG.isDebugEnabled()) {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -767,6 +767,8 @@ public static enum ConfVars {
     HIVEMAPJOINUSEOPTIMIZEDTABLE("hive.mapjoin.optimized.hashtable", true,
         "Whether Hive should use memory-optimized hash table for MapJoin. Only works on Tez,\n" +
         "because memory-optimized hashtable cannot be serialized."),
+    HIVEMAPJOINOPTIMIZEDTABLEPROBEPERCENT("hive.mapjoin.optimized.hashtable.probe.percent",
+        (float) 0.5, "Probing space percentage of the optimized hashtable"),
     HIVEUSEHYBRIDGRACEHASHJOIN("hive.mapjoin.hybridgrace.hashtable", true, "Whether to use hybrid" +
         "grace hash join as the join method for mapjoin. Tez only."),
     HIVEHYBRIDGRACEHASHJOINMEMCHECKFREQ("hive.mapjoin.hybridgrace.memcheckfrequency", 1024, "For " +

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/LazyHBaseRow.java
Patch:
@@ -22,7 +22,6 @@
 import java.util.Arrays;
 import java.util.List;
 
-import com.google.common.annotations.VisibleForTesting;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping;
 import org.apache.hadoop.hive.hbase.struct.HBaseValueFactory;
@@ -36,6 +35,8 @@
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 
+import com.google.common.annotations.VisibleForTesting;
+
 /**
  * LazyObject for storing an HBase row.  The field of an HBase row can be
  * primitive or non-primitive.
@@ -148,7 +149,7 @@ private Object uncheckedGetField(int fieldID) {
         // qualifier prefix to cherry pick the qualifiers that match the prefix instead of picking
         // up everything
         ((LazyHBaseCellMap) fields[fieldID]).init(
-            result, colMap.familyNameBytes, colMap.binaryStorage, colMap.qualifierPrefixBytes);
+            result, colMap.familyNameBytes, colMap.binaryStorage, colMap.qualifierPrefixBytes, colMap.isDoPrefixCut());
         return fields[fieldID].getObject();
       }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
Patch:
@@ -398,7 +398,6 @@ public int execute(DriverContext driverContext) {
       if (pwd != null) {
         HiveConf.setVar(job, HiveConf.ConfVars.METASTOREPWD, "HIVE");
       }
-      LOG.error(job.get("mapreduce.framework.name"));
       JobClient jc = new JobClient(job);
       // make this client wait if job tracker is not behaving well.
       Throttle.checkJobTracker(job, LOG);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java
Patch:
@@ -98,7 +98,7 @@ public int execute(DriverContext driverContext) {
 
     if (work.getReloadFunctionDesc() != null) {
       try {
-        Hive.reloadFunctions();
+        Hive.get().reloadFunctions();
       } catch (Exception e) {
         setException(e);
         LOG.error(stringifyException(e));

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
Patch:
@@ -2541,6 +2541,9 @@ public void testSimpleFunction() throws Exception {
 
     try {
       cleanUp(dbName, null, null);
+      for (Function f : client.getAllFunctions().getFunctions()) {
+        client.dropFunction(f.getDbName(), f.getFunctionName());
+      }
 
       createDb(dbName);
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPruner.java
Patch:
@@ -174,10 +174,10 @@ public void walk(Node nd) throws SemanticException {
         return;
       }
       // move all the children to the front of queue
-      getToWalk().removeAll(nd.getChildren());
-      getToWalk().addAll(0, nd.getChildren());
+      toWalk.removeAll(nd.getChildren());
+      toWalk.addAll(0, nd.getChildren());
       // add self to the end of the queue
-      getToWalk().add(nd);
+      toWalk.add(nd);
       opStack.pop();
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
Patch:
@@ -637,8 +637,6 @@ private static Object getBaseObjectForComparison(PredicateLeaf.Type type, Object
           return ((BigDecimal) obj).doubleValue();
         }
         break;
-      case INTEGER:
-        // fall through
       case LONG:
         if (obj instanceof Number) {
           // widening conversion

File: ql/src/java/org/apache/hadoop/hive/ql/io/sarg/ConvertAstToSearchArg.java
Patch:
@@ -88,7 +88,6 @@ private static PredicateLeaf.Type getType(ExprNodeDesc expr) {
         case BYTE:
         case SHORT:
         case INT:
-          return PredicateLeaf.Type.INTEGER;
         case LONG:
           return PredicateLeaf.Type.LONG;
         case CHAR:
@@ -139,8 +138,6 @@ private static Object boxLiteral(ExprNodeConstantDesc constantDesc,
       return null;
     }
     switch (type) {
-      case INTEGER:
-        return ((Number) lit).intValue();
       case LONG:
         return ((Number) lit).longValue();
       case STRING:

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
Patch:
@@ -1844,7 +1844,7 @@ public void testSetSearchArgument() throws Exception {
     types.add(builder.build());
     types.add(builder.build());
     SearchArgument isNull = SearchArgumentFactory.newBuilder()
-        .startAnd().isNull("cost", PredicateLeaf.Type.INTEGER).end().build();
+        .startAnd().isNull("cost", PredicateLeaf.Type.LONG).end().build();
     conf.set(ConvertAstToSearchArg.SARG_PUSHDOWN, toKryo(isNull));
     conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR,
         "url,cost");
@@ -1889,7 +1889,7 @@ public void testSplitElimination() throws Exception {
     SearchArgument sarg =
         SearchArgumentFactory.newBuilder()
             .startAnd()
-            .lessThan("z", PredicateLeaf.Type.INTEGER, new Integer(0))
+            .lessThan("z", PredicateLeaf.Type.LONG, new Long(0))
             .end()
             .build();
     conf.set("sarg.pushdown", toKryo(sarg));

File: ql/src/test/org/apache/hadoop/hive/ql/io/parquet/read/TestParquetFilterPredicate.java
Patch:
@@ -35,9 +35,9 @@ public void testFilterColumnsThatDoNoExistOnSchema() {
     SearchArgument sarg = SearchArgumentFactory.newBuilder()
         .startNot()
         .startOr()
-        .isNull("a", PredicateLeaf.Type.INTEGER)
-        .between("y", PredicateLeaf.Type.INTEGER, 10, 20) // Column will be removed from filter
-        .in("z", PredicateLeaf.Type.INTEGER, 1, 2, 3) // Column will be removed from filter
+        .isNull("a", PredicateLeaf.Type.LONG)
+        .between("y", PredicateLeaf.Type.LONG, 10L, 20L) // Column will be removed from filter
+        .in("z", PredicateLeaf.Type.LONG, 1L, 2L, 3L) // Column will be removed from filter
         .nullSafeEquals("a", PredicateLeaf.Type.STRING, "stinger")
         .end()
         .end()

File: storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/PredicateLeaf.java
Patch:
@@ -47,8 +47,7 @@ public static enum Operator {
    * The possible types for sargs.
    */
   public static enum Type {
-    INTEGER(Integer.class), // all of the integer types except long
-    LONG(Long.class),
+    LONG(Long.class),      // all of the integer types
     FLOAT(Double.class),   // float and double
     STRING(String.class),  // string, char, varchar
     DATE(Date.class),

File: metastore/src/java/org/apache/hadoop/hive/metastore/hbase/StatsCache.java
Patch:
@@ -99,7 +99,7 @@ public AggrStats load(StatsCacheKey key) throws Exception {
                   for (ColumnStatisticsObj cso : cs.getStatsObj()) {
                     if (statsObj == null) {
                       statsObj = ColumnStatsAggregatorFactory.newColumnStaticsObj(key.colName,
-                          cso.getStatsData().getSetField());
+                          cso.getColType(), cso.getStatsData().getSetField());
                     }
                     if (aggregator == null) {
                       aggregator = ColumnStatsAggregatorFactory.getColumnStatsAggregator(

File: ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java
Patch:
@@ -737,6 +737,9 @@ public static List<ColStatistics> getTableColumnStats(
   }
 
   private static List<ColStatistics> convertColStats(List<ColumnStatisticsObj> colStats, String tabName) {
+    if (colStats==null) {
+      return new ArrayList<ColStatistics>();
+    }
     List<ColStatistics> stats = new ArrayList<ColStatistics>(colStats.size());
     for (ColumnStatisticsObj statObj : colStats) {
       ColStatistics cs = getColStatistics(statObj, tabName, statObj.getColName());

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNvl.java
Patch:
@@ -42,7 +42,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen
     returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);
     if (!(returnOIResolver.update(arguments[0]) && returnOIResolver
         .update(arguments[1]))) {
-      throw new UDFArgumentTypeException(2,
+      throw new UDFArgumentTypeException(1,
           "The first and seconds arguments of function NLV should have the same type, "
           + "but they are different: \"" + arguments[0].getTypeName()
           + "\" and \"" + arguments[1].getTypeName() + "\"");

File: llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java
Patch:
@@ -1111,6 +1111,7 @@ static class StatsPerDag {
     int numRequestedAllocations = 0;
     int numRequestsWithLocation = 0;
     int numRequestsWithoutLocation = 0;
+    int numTotalAllocations = 0;
     int numLocalAllocations = 0;
     int numNonLocalAllocations = 0;
     int numAllocationsNoLocalityRequest = 0;
@@ -1129,6 +1130,7 @@ public String toString() {
       sb.append("NumRequestsWithlocation=").append(numRequestsWithLocation).append(", ");
       sb.append("NumLocalAllocations=").append(numLocalAllocations).append(",");
       sb.append("NumNonLocalAllocations=").append(numNonLocalAllocations).append(",");
+      sb.append("NumTotalAllocations=").append(numTotalAllocations).append(",");
       sb.append("NumRequestsWithoutLocation=").append(numRequestsWithoutLocation).append(", ");
       sb.append("NumRejectedTasks=").append(numRejectedTasks).append(", ");
       sb.append("NumCommFailures=").append(numCommFailures).append(", ");
@@ -1163,6 +1165,7 @@ void registerTaskAllocated(String[] requestedHosts, String[] requestedRacks,
       } else {
         numAllocationsNoLocalityRequest++;
       }
+      numTotalAllocations++;
       _registerAllocationInHostMap(allocatedHost, numAllocationsPerHost);
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBaseNumeric.java
Patch:
@@ -245,7 +245,7 @@ protected PrimitiveTypeInfo deriveResultApproxTypeInfo() {
     }    
 
     // Use type promotion
-    PrimitiveCategory commonCat = FunctionRegistry.getCommonCategory(left, right);
+    PrimitiveCategory commonCat = FunctionRegistry.getPrimitiveCommonCategory(left, right);
     if (commonCat == PrimitiveCategory.DECIMAL) {
       // Hive 0.12 behavior where double * decimal -> decimal is gone.
       return TypeInfoFactory.doubleTypeInfo;
@@ -267,7 +267,7 @@ protected PrimitiveTypeInfo deriveResultExactTypeInfo() {
     PrimitiveTypeInfo right = (PrimitiveTypeInfo) TypeInfoUtils.getTypeInfoFromObjectInspector(rightOI);
 
     // Now we are handling exact types. Base implementation handles type promotion.
-    PrimitiveCategory commonCat = FunctionRegistry.getCommonCategory(left, right);
+    PrimitiveCategory commonCat = FunctionRegistry.getPrimitiveCommonCategory(left, right);
     if (commonCat == PrimitiveCategory.DECIMAL) {
       return deriveResultDecimalTypeInfo();
     } else {

File: beeline/src/java/org/apache/hive/beeline/BeeLineOpts.java
Patch:
@@ -228,7 +228,7 @@ public void load(InputStream fin) throws IOException {
   public void updateBeeLineOptsFromConf() {
     if (!beeLine.isBeeLine()) {
       if (conf == null) {
-        conf = beeLine.getCommands().getHiveConf(true);
+        conf = beeLine.getCommands().getHiveConf(false);
       }
       setForce(HiveConf.getBoolVar(conf, HiveConf.ConfVars.CLIIGNOREERRORS));
     }

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/SettableUnionObjectInspector.java
Patch:
@@ -29,6 +29,6 @@ public abstract class SettableUnionObjectInspector implements
   /* Create an empty object */
   public abstract Object create();
 
-  /* Add fields to the object */
-  public abstract Object addField(Object union, ObjectInspector oi);
+  /* Add field to the object */
+  public abstract Object addField(Object union, Object field);
 }

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardUnionObjectInspector.java
Patch:
@@ -124,9 +124,9 @@ public Object create() {
   }
 
   @Override
-  public Object addField(Object union, ObjectInspector oi) {
+  public Object addField(Object union, Object field) {
     ArrayList<Object> a = (ArrayList<Object>) union;
-    a.add(oi);
+    a.add(field);
     return a;
   }
 

File: jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java
Patch:
@@ -59,7 +59,7 @@ public class HiveStatement implements java.sql.Statement {
   private TOperationHandle stmtHandle = null;
   private final TSessionHandle sessHandle;
   Map<String,String> sessConf = new HashMap<String,String>();
-  private int fetchSize = 50;
+  private int fetchSize = 1000;
   private boolean isScrollableResultset = false;
   /**
    * We need to keep a reference to the result set to support the following:

File: itests/hive-unit/src/test/java/org/apache/hive/service/cli/operation/TestOperationLoggingAPIWithMr.java
Patch:
@@ -62,8 +62,6 @@ public static void setUpBeforeClass() throws Exception {
     };
     hiveConf = new HiveConf();
     hiveConf.set(ConfVars.HIVE_SERVER2_LOGGING_OPERATION_LEVEL.varname, "verbose");
-    // We need to set the below parameter to test performance level logging
-    hiveConf.set("hive.ql.log.PerfLogger.level", "INFO,DRFA");
     miniHS2 = new MiniHS2(hiveConf);
     confOverlay = new HashMap<String, String>();
     confOverlay.put(ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");

File: itests/hive-unit/src/test/java/org/apache/hive/service/cli/operation/TestOperationLoggingAPIWithTez.java
Patch:
@@ -40,8 +40,6 @@ public static void setUpBeforeClass() throws Exception {
     };
     hiveConf = new HiveConf();
     hiveConf.set(ConfVars.HIVE_SERVER2_LOGGING_OPERATION_LEVEL.varname, "verbose");
-    // We need to set the below parameter to test performance level logging
-    hiveConf.set("hive.ql.log.PerfLogger.level", "INFO,DRFA");
     // Change the engine to tez
     hiveConf.setVar(ConfVars.HIVE_EXECUTION_ENGINE, "tez");
     // Set tez execution summary to false.

File: itests/hive-unit/src/test/java/org/apache/hive/service/cli/operation/TestOperationLoggingLayout.java
Patch:
@@ -39,8 +39,6 @@ public static void setUpBeforeClass() throws Exception {
     tableName = "TestOperationLoggingLayout_table";
     hiveConf = new HiveConf();
     hiveConf.set(HiveConf.ConfVars.HIVE_SERVER2_LOGGING_OPERATION_LEVEL.varname, "execution");
-    // We need to set the below parameter to test performance level logging
-    hiveConf.set("hive.ql.log.PerfLogger.level", "INFO,DRFA");
     miniHS2 = new MiniHS2(hiveConf);
     confOverlay = new HashMap<String, String>();
     confOverlay.put(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java
Patch:
@@ -118,6 +118,8 @@ public static class HashPartition {
     public HashPartition(int threshold, float loadFactor, int wbSize, long memUsage,
                          boolean createHashMap) {
       if (createHashMap) {
+        // Hash map should be at least the size of our designated wbSize
+        memUsage = Math.max(memUsage, wbSize);
         hashMap = new BytesBytesMultiHashMap(threshold, loadFactor, wbSize, memUsage);
       } else {
         hashMapSpilledOnCreation = true;

File: llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java
Patch:
@@ -402,7 +402,7 @@ public void allocateTask(Object task, Resource capability, ContainerId container
   // This may be invoked before a container is ever assigned to a task. allocateTask... app decides
   // the task is no longer required, and asks for a de-allocation.
   @Override
-  public boolean deallocateTask(Object task, boolean taskSucceeded, TaskAttemptEndReason endReason) {
+  public boolean deallocateTask(Object task, boolean taskSucceeded, TaskAttemptEndReason endReason, String diagnostics) {
     writeLock.lock(); // Updating several local structures
     TaskInfo taskInfo;
     try {
@@ -471,11 +471,11 @@ public boolean deallocateTask(Object task, boolean taskSucceeded, TaskAttemptEnd
         } else if (!taskSucceeded) {
           nodeInfo.registerUnsuccessfulTaskEnd(false);
           if (endReason != null && EnumSet
-              .of(TaskAttemptEndReason.SERVICE_BUSY, TaskAttemptEndReason.COMMUNICATION_ERROR)
+              .of(TaskAttemptEndReason.EXECUTOR_BUSY, TaskAttemptEndReason.COMMUNICATION_ERROR)
               .contains(endReason)) {
             if (endReason == TaskAttemptEndReason.COMMUNICATION_ERROR) {
               dagStats.registerCommFailure(taskInfo.assignedInstance.getHost());
-            } else if (endReason == TaskAttemptEndReason.SERVICE_BUSY) {
+            } else if (endReason == TaskAttemptEndReason.EXECUTOR_BUSY) {
               dagStats.registerTaskRejected(taskInfo.assignedInstance.getHost());
             }
           }

File: llap-server/src/test/org/apache/tez/dag/app/rm/TestLlapTaskSchedulerService.java
Patch:
@@ -342,11 +342,11 @@ void allocateTask(Object task, String[] hosts, Priority priority, Object clientC
     }
 
     void deallocateTask(Object task, boolean succeeded, TaskAttemptEndReason endReason) {
-      ts.deallocateTask(task, succeeded, endReason);
+      ts.deallocateTask(task, succeeded, endReason, null);
     }
 
     void rejectExecution(Object task) {
-      ts.deallocateTask(task, false, TaskAttemptEndReason.SERVICE_BUSY);
+      ts.deallocateTask(task, false, TaskAttemptEndReason.EXECUTOR_BUSY, null);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
Patch:
@@ -283,7 +283,7 @@ public void setInputFormatClass(Class<? extends InputFormat> inputFormatClass) {
   public void setOutputFormatClass(Class<? extends HiveOutputFormat> outputFormatClass) {
     this.outputFormatClass = outputFormatClass;
     tPartition.getSd().setOutputFormat(HiveFileFormatUtils
-        .getOutputFormatSubstitute(outputFormatClass).toString());
+        .getOutputFormatSubstitute(outputFormatClass).getName());
   }
 
   final public Class<? extends InputFormat> getInputFormatClass()

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/ReduceSinkDeDuplication.java
Patch:
@@ -500,7 +500,7 @@ public Object process(ReduceSinkOperator cRS, ReduceSinkDeduplicateProcCtx dedup
     public Object process(ReduceSinkOperator cRS, GroupByOperator cGBY,
         ReduceSinkDeduplicateProcCtx dedupCtx)
         throws SemanticException {
-      Operator<?> start = CorrelationUtilities.getStartForGroupBy(cRS);
+      Operator<?> start = CorrelationUtilities.getStartForGroupBy(cRS, dedupCtx);
       GroupByOperator pGBY =
           CorrelationUtilities.findPossibleParent(
               start, GroupByOperator.class, dedupCtx.trustScript());
@@ -547,7 +547,7 @@ public Object process(ReduceSinkOperator cRS, ReduceSinkDeduplicateProcCtx dedup
     public Object process(ReduceSinkOperator cRS, GroupByOperator cGBY,
         ReduceSinkDeduplicateProcCtx dedupCtx)
         throws SemanticException {
-      Operator<?> start = CorrelationUtilities.getStartForGroupBy(cRS);
+      Operator<?> start = CorrelationUtilities.getStartForGroupBy(cRS, dedupCtx);
       JoinOperator pJoin =
           CorrelationUtilities.findPossibleParent(
               start, JoinOperator.class, dedupCtx.trustScript());
@@ -590,7 +590,7 @@ public Object process(ReduceSinkOperator cRS, ReduceSinkDeduplicateProcCtx dedup
     public Object process(ReduceSinkOperator cRS, GroupByOperator cGBY,
         ReduceSinkDeduplicateProcCtx dedupCtx)
         throws SemanticException {
-      Operator<?> start = CorrelationUtilities.getStartForGroupBy(cRS);
+      Operator<?> start = CorrelationUtilities.getStartForGroupBy(cRS, dedupCtx);
       ReduceSinkOperator pRS =
           CorrelationUtilities.findPossibleParent(
               start, ReduceSinkOperator.class, dedupCtx.trustScript());

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
Patch:
@@ -256,7 +256,7 @@ public String toString() {
     };
 
     private static boolean isTypeChar(char c) {
-      return Character.isLetterOrDigit(c) || c == '_' || c == '.';
+      return Character.isLetterOrDigit(c) || c == '_' || c == '.' || c == ' ';
     }
 
     /**

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -410,7 +410,7 @@ public int compile(String command, boolean resetTaskIds) {
               HiveSemanticAnalyzerHook.class);
 
       // Do semantic analysis and plan generation
-      if (saHooks != null) {
+      if (saHooks != null && !saHooks.isEmpty()) {
         HiveSemanticAnalyzerHookContext hookCtx = new HiveSemanticAnalyzerHookContextImpl();
         hookCtx.setConf(conf);
         hookCtx.setUserName(userName);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java
Patch:
@@ -605,7 +605,7 @@ private OpAttr genPTF(OpAttr inputOpAf, WindowingSpec wSpec) throws SemanticExce
     WindowingComponentizer groups = new WindowingComponentizer(wSpec);
     RowResolver rr = new RowResolver();
     for (ColumnInfo ci : input.getSchema().getSignature()) {
-      rr.put(ci.getTabAlias(), ci.getInternalName(), ci);
+      rr.put(inputOpAf.tabAlias, ci.getInternalName(), ci);
     }
 
     while (groups.hasNext()) {

File: llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapContainerLauncher.java
Patch:
@@ -25,7 +25,7 @@ public class LlapContainerLauncher extends ContainerLauncher {
   private static final Logger LOG = LoggerFactory.getLogger(LlapContainerLauncher.class);
 
   public LlapContainerLauncher(ContainerLauncherContext containerLauncherContext) {
-    super(LlapContainerLauncher.class.getName(), containerLauncherContext);
+    super(containerLauncherContext);
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierForASTConv.java
Patch:
@@ -169,7 +169,7 @@ private static void convertOpTree(RelNode rel, RelNode parent) {
     }
   }
 
-  private static RelNode renameTopLevelSelectInResultSchema(final RelNode rootRel,
+  public static RelNode renameTopLevelSelectInResultSchema(final RelNode rootRel,
       Pair<RelNode, RelNode> topSelparentPair, List<FieldSchema> resultSchema)
       throws CalciteSemanticException {
     RelNode parentOforiginalProjRel = topSelparentPair.getKey();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -641,7 +641,8 @@ Operator getOptimizedHiveOPDag() throws SemanticException {
     }
 
     RelNode modifiedOptimizedOptiqPlan = PlanModifierForReturnPath.convertOpTree(
-            introduceProjectIfNeeded(optimizedOptiqPlan), topLevelFieldSchema);
+        introduceProjectIfNeeded(optimizedOptiqPlan), topLevelFieldSchema, this.getQB()
+            .getTableDesc() != null);
 
     LOG.debug("Translating the following plan:\n" + RelOptUtil.toString(modifiedOptimizedOptiqPlan));
     Operator<?> hiveRoot = new HiveOpConverter(this, conf, unparseTranslator, topOps,

File: llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
Patch:
@@ -31,7 +31,6 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.llap.LlapNodeId;
 import org.apache.hadoop.hive.llap.configuration.LlapConfiguration;
-import org.apache.hadoop.hive.llap.daemon.QueryFailedHandler;
 import org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos;
 import org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.FragmentRuntimeInfo;
 import org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto;
@@ -55,8 +54,6 @@
 import org.apache.hadoop.yarn.api.records.LocalResource;
 import org.apache.tez.common.TezTaskUmbilicalProtocol;
 import org.apache.tez.common.security.JobTokenSecretManager;
-import org.apache.tez.dag.api.ContainerEndReason;
-import org.apache.tez.dag.api.TaskAttemptEndReason;
 import org.apache.tez.dag.api.TaskCommunicatorContext;
 import org.apache.tez.dag.api.TezConfiguration;
 import org.apache.tez.dag.api.TezException;
@@ -67,6 +64,8 @@
 import org.apache.tez.runtime.api.impl.TaskSpec;
 import org.apache.tez.runtime.api.impl.TezHeartbeatRequest;
 import org.apache.tez.runtime.api.impl.TezHeartbeatResponse;
+import org.apache.tez.serviceplugins.api.ContainerEndReason;
+import org.apache.tez.serviceplugins.api.TaskAttemptEndReason;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java
Patch:
@@ -60,7 +60,6 @@
 import org.apache.hadoop.yarn.api.records.Priority;
 import org.apache.hadoop.yarn.api.records.Resource;
 import org.apache.hadoop.yarn.util.Clock;
-import org.apache.tez.dag.api.TaskAttemptEndReason;
 import org.apache.tez.dag.app.AppContext;
 
 import com.google.common.annotations.VisibleForTesting;
@@ -69,6 +68,7 @@
 import com.google.common.util.concurrent.ListeningExecutorService;
 import com.google.common.util.concurrent.MoreExecutors;
 import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import org.apache.tez.serviceplugins.api.TaskAttemptEndReason;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: llap-server/src/test/org/apache/tez/dag/app/rm/TestLlapTaskSchedulerService.java
Patch:
@@ -41,10 +41,10 @@
 import org.apache.hadoop.yarn.api.records.Priority;
 import org.apache.hadoop.yarn.api.records.Resource;
 import org.apache.hadoop.yarn.util.SystemClock;
-import org.apache.tez.dag.api.TaskAttemptEndReason;
 import org.apache.tez.dag.app.AppContext;
 import org.apache.tez.dag.app.ControlledClock;
 import org.apache.tez.dag.app.rm.TaskSchedulerService.TaskSchedulerAppCallback;
+import org.apache.tez.serviceplugins.api.TaskAttemptEndReason;
 import org.junit.Test;
 import org.mockito.ArgumentCaptor;
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -876,6 +876,7 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu
         // 6.1. Merge join into multijoin operators (if possible)
         calciteOptimizedPlan = hepPlan(calciteOptimizedPlan, true, mdProvider.getMetadataProvider(),
                 HepMatchOrder.BOTTOM_UP, HiveJoinProjectTransposeRule.BOTH_PROJECT,
+                HiveJoinProjectTransposeRule.LEFT_PROJECT, HiveJoinProjectTransposeRule.RIGHT_PROJECT,
                 HiveJoinToMultiJoinRule.INSTANCE, HiveProjectMergeRule.INSTANCE);
         // The previous rules can pull up projections through join operators,
         // thus we run the field trimmer again to push them back down

File: metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java
Patch:
@@ -417,16 +417,16 @@ List<Partition> add_partitions(
       throws InvalidObjectException, AlreadyExistsException, MetaException, TException;
 
   /**
-   * @param tblName
    * @param dbName
+   * @param tblName
    * @param partVals
    * @return the partition object
    * @throws MetaException
    * @throws TException
    * @see org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.Iface#get_partition(java.lang.String,
    *      java.lang.String, java.util.List)
    */
-  Partition getPartition(String tblName, String dbName,
+  Partition getPartition(String dbName, String tblName,
       List<String> partVals) throws NoSuchObjectException, MetaException, TException;
 
   /**

File: jdbc/src/java/org/apache/hive/jdbc/HivePreparedStatement.java
Patch:
@@ -436,7 +436,7 @@ public void setClob(int parameterIndex, Reader reader, long length) throws SQLEx
    */
 
   public void setDate(int parameterIndex, Date x) throws SQLException {
-    this.parameters.put(parameterIndex, x.toString());
+    this.parameters.put(parameterIndex, "'" + x.toString() + "'");
   }
 
   /*

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java
Patch:
@@ -203,8 +203,9 @@ public void unregisterTask(String amLocation, int port) {
       amNodeInfo = knownAppMasters.get(amNodeId);
       if (amNodeInfo == null) {
         LOG.info(("Ignoring duplicate unregisterRequest for am at: " + amLocation + ":" + port));
+      } else {
+        amNodeInfo.decrementAndGetTaskCount();
       }
-      amNodeInfo.decrementAndGetTaskCount();
       // Not removing this here. Will be removed when taken off the queue and discovered to have 0
       // pending tasks.
     }

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -5995,7 +5995,7 @@ public static void startMetaStore(int port, HadoopThriftAuthBridge bridge,
       // Server will create new threads up to max as necessary. After an idle
       // period, it will destroy threads to keep the number of threads in the
       // pool to min.
-      int maxMessageSize = conf.getIntVar(HiveConf.ConfVars.METASTORESERVERMAXMESSAGESIZE);
+      long maxMessageSize = conf.getIntVar(HiveConf.ConfVars.METASTORESERVERMAXMESSAGESIZE);
       int minWorkerThreads = conf.getIntVar(HiveConf.ConfVars.METASTORESERVERMINTHREADS);
       int maxWorkerThreads = conf.getIntVar(HiveConf.ConfVars.METASTORESERVERMAXTHREADS);
       boolean tcpKeepAlive = conf.getBoolVar(HiveConf.ConfVars.METASTORE_TCP_KEEP_ALIVE);

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -2581,7 +2581,7 @@ public Partition exchange_partition(Map<String, String> partitionSpecs,
         pathCreated = wh.renameDir(sourcePath, destPath);
         success = ms.commitTransaction();
       } finally {
-        if (!success) {
+        if (!success || !pathCreated) {
           ms.rollbackTransaction();
           if (pathCreated) {
             wh.renameDir(destPath, sourcePath);

File: beeline/src/test/org/apache/hive/beeline/cli/TestHiveCli.java
Patch:
@@ -78,7 +78,8 @@ private void verifyCMD(String CMD, String keywords, OutputStream os, String[] op
       int retCode) {
     executeCMD(options, CMD, retCode);
     String output = os.toString();
-    Assert.assertTrue(output.contains(keywords));
+    Assert.assertTrue("The expected keyword doesn't occur in the output: " + output,
+        output.contains(keywords));
   }
 
   @Test

File: beeline/src/java/org/apache/hive/beeline/BeeLine.java
Patch:
@@ -889,7 +889,7 @@ private int executeFile(String fileName) {
     FileInputStream initStream = null;
     try {
       initStream = new FileInputStream(fileName);
-      return execute(getConsoleReader(initStream), true);
+      return execute(getConsoleReader(initStream), !getOpts().getForce());
     } catch (Throwable t) {
       handleException(t);
       return ERRNO_OTHER;

File: beeline/src/java/org/apache/hive/beeline/BeeLine.java
Patch:
@@ -792,7 +792,7 @@ private int executeFile(String fileName) {
     FileInputStream initStream = null;
     try {
       initStream = new FileInputStream(fileName);
-      return execute(getConsoleReader(initStream), true);
+      return execute(getConsoleReader(initStream), !getOpts().getForce());
     } catch (Throwable t) {
       handleException(t);
       return ERRNO_OTHER;

File: itests/hive-unit/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java
Patch:
@@ -181,8 +181,7 @@ private MiniHS2(HiveConf hiveConf, MiniClusterType miniClusterType, boolean useM
       // Initialize the execution engine based on cluster type
       switch (miniClusterType) {
       case TEZ:
-        mr = ShimLoader.getHadoopShims().getMiniTezCluster(hiveConf, 4, uriString, 1, false,
-            baseDir.toString() + "/staging");
+        mr = ShimLoader.getHadoopShims().getMiniTezCluster(hiveConf, 4, uriString, 1);
         break;
       case MR:
         mr = ShimLoader.getHadoopShims().getMiniMrCluster(hiveConf, 4, uriString, 1);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestLocationQueries.java
Patch:
@@ -88,7 +88,7 @@ public CheckResults(String outDir, String logDir, MiniClusterType miniMr,
         String hadoopVer, String locationSubdir)
       throws Exception
     {
-      super(outDir, logDir, miniMr, null, hadoopVer, "", "", null, false);
+      super(outDir, logDir, miniMr, null, hadoopVer, "", "", false);
       this.locationSubdir = locationSubdir;
     }
   }

File: itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloQTestUtil.java
Patch:
@@ -26,7 +26,7 @@ public class AccumuloQTestUtil extends QTestUtil {
   public AccumuloQTestUtil(String outDir, String logDir, MiniClusterType miniMr,
       AccumuloTestSetup setup, String initScript, String cleanupScript) throws Exception {
 
-    super(outDir, logDir, miniMr, null, initScript, cleanupScript, null);
+    super(outDir, logDir, miniMr, null, initScript, cleanupScript);
     setup.setupWithHiveConf(conf);
     super.init();
   }

File: itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseQTestUtil.java
Patch:
@@ -44,7 +44,7 @@ public HBaseQTestUtil(
     String initScript, String cleanupScript)
     throws Exception {
 
-    super(outDir, logDir, miniMr, null, initScript, cleanupScript, null);
+    super(outDir, logDir, miniMr, null, initScript, cleanupScript);
     setup.preTest(conf);
     this.conn = setup.getConnection();
     super.init();

File: shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
Patch:
@@ -231,8 +231,7 @@ public MiniMrShim getMiniMrCluster(Configuration conf, int numberOfTaskTrackers,
 
   @Override
   public MiniMrShim getMiniTezCluster(Configuration conf, int numberOfTaskTrackers,
-				      String nameNode, int numDir, boolean local,
-				      String tezDir) throws IOException {
+      String nameNode, int numDir) throws IOException {
     throw new IOException("Cannot run tez on current hadoop, Version: " + VersionInfo.getVersion());
   }
 

File: shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
Patch:
@@ -95,7 +95,7 @@ public MiniMrShim getMiniMrCluster(Configuration conf, int numberOfTaskTrackers,
       String nameNode, int numDir) throws IOException;
 
   public MiniMrShim getMiniTezCluster(Configuration conf, int numberOfTaskTrackers,
-      String nameNode, int numDir, boolean local, String tezDir) throws IOException;
+      String nameNode, int numDir) throws IOException;
 
   public MiniMrShim getMiniSparkCluster(Configuration conf, int numberOfTaskTrackers,
       String nameNode, int numDir) throws IOException;

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -490,8 +490,7 @@ public enum ErrorMsg {
       "Stats type {0} is missing from stats aggregator. If you don't want the query " +
       "to fail because of this, set hive.stats.atomic=false", true),
   STATS_SKIPPING_BY_ERROR(30017, "Skipping stats aggregation by error {0}", true),
-  ORC_CORRUPTED_READ(30018, "Corruption in ORC data encountered. To skip reading corrupted "
-      + "data, set " + HiveConf.ConfVars.HIVE_ORC_SKIP_CORRUPT_DATA + " to true"),
+
 
   INVALID_FILE_FORMAT_IN_LOAD(30019, "The file that you are trying to load does not match the" +
       " file format of the destination table.")

File: serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
Patch:
@@ -555,6 +555,6 @@ public static Text transformTextToUTF8(Text text, Charset previousCharset) {
   }
 
   public static Text transformTextFromUTF8(Text text, Charset targetCharset) {
-    return new Text(new String(text.getBytes()).getBytes(targetCharset));
+    return new Text(new String(text.getBytes(), 0, text.getLength()).getBytes(targetCharset));
   }
 }

File: beeline/src/java/org/apache/hive/beeline/Commands.java
Patch:
@@ -605,6 +605,7 @@ public boolean dbinfo(String line) {
             "" + beeLine.getReflector().invoke(beeLine.getDatabaseMetaData(),
                 m[i], new Object[0])));
       } catch (Exception e) {
+        beeLine.output(beeLine.getColorBuffer().pad(m[i], padlen), false);
         beeLine.handleException(e);
       }
     }

File: beeline/src/java/org/apache/hive/beeline/BeeLineOpts.java
Patch:
@@ -197,9 +197,9 @@ public Properties toProperties()
 
     String[] names = propertyNames();
     for (int i = 0; names != null && i < names.length; i++) {
+      Object o = beeLine.getReflector().invoke(this, "get" + names[i], new Object[0]);
       props.setProperty(PROPERTY_PREFIX + names[i],
-          beeLine.getReflector().invoke(this, "get" + names[i], new Object[0])
-              .toString());
+          o == null ? "" : o.toString());
     }
     beeLine.debug("properties: " + props.toString());
     return props;

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/tez/Attr.java
Patch:
@@ -18,9 +18,9 @@
 
 package org.apache.hadoop.hive.common.jsonexplain.tez;
 
-public class Attr implements Comparable<Attr> {
-  String name;
-  String value;
+public final class Attr implements Comparable<Attr> {
+  public final String name;
+  public final String value;
 
   public Attr(String name, String value) {
     super();

File: common/src/java/org/apache/hadoop/hive/common/jsonexplain/tez/Connection.java
Patch:
@@ -18,9 +18,9 @@
 
 package org.apache.hadoop.hive.common.jsonexplain.tez;
 
-public class Connection {
-  public String type;
-  public Vertex from;
+public final class Connection {
+  public final String type;
+  public final Vertex from;
 
   public Connection(String type, Vertex from) {
     super();

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1705,7 +1705,7 @@ public static enum ConfVars {
     HIVE_LOG_EXPLAIN_OUTPUT("hive.log.explain.output", false,
         "Whether to log explain output for every query.\n" +
         "When enabled, will log EXPLAIN EXTENDED output for the query at INFO log4j log level."),
-    HIVE_EXPLAIN_USER("hive.explain.user", false,
+    HIVE_EXPLAIN_USER("hive.explain.user", true,
         "Whether to show explain result at user level.\n" +
         "When enabled, will log EXPLAIN output for the query at user level."),
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalWork.java
Patch:
@@ -21,11 +21,13 @@
 import java.io.Serializable;
 import java.util.List;
 
+import org.apache.hadoop.hive.ql.plan.Explain.Level;
+
 /**
  * ConditionalWork.
  *
  */
-@Explain(displayName = "Conditional Operator")
+@Explain(displayName = "Conditional Operator", explainLevels = { Level.USER, Level.DEFAULT, Level.EXTENDED })
 public class ConditionalWork implements Serializable {
   private static final long serialVersionUID = 1L;
   List<? extends Serializable> listWorks;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnInfo.java
Patch:
@@ -156,7 +156,7 @@ public boolean isHiddenVirtualCol() {
    */
   @Override
   public String toString() {
-    return internalName + ": " + objectInspector.getTypeName();
+    return internalName + ": " + typeName;
   }
 
   public void setAlias(String col_alias) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -270,7 +270,9 @@ Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticExcept
             LOG.info("CBO Succeeded; optimized logical plan.");
             this.ctx.setCboInfo("Plan optimized by CBO.");
             this.ctx.setCboSucceeded(true);
-            LOG.debug(newAST.dump());
+            if (LOG.isDebugEnabled()) {
+              LOG.debug(newAST.dump());
+            }
           }
         } catch (Exception e) {
           boolean isMissingStats = noColsMissingStats.get() > 0;

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/HiveLockObject.java
Patch:
@@ -51,7 +51,7 @@ public HiveLockObjectData(String queryId,
       this.queryId = removeDelimiter(queryId);
       this.lockTime = removeDelimiter(lockTime);
       this.lockMode = removeDelimiter(lockMode);
-      this.queryStr = removeDelimiter(queryStr.trim());
+      this.queryStr = removeDelimiter(queryStr == null ? null : queryStr.trim());
     }
 
     /**

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
Patch:
@@ -255,6 +255,8 @@ private String getLockName(String parent, HiveLockMode mode) {
 
   private ZooKeeperHiveLock lock (HiveLockObject key, HiveLockMode mode,
       boolean keepAlive, boolean parentCreated) throws LockException {
+    LOG.info("Acquiring lock for " + key.getName() + " with mode " + key.getData().getLockMode());
+
     int tryNum = 0;
     ZooKeeperHiveLock ret = null;
     Set<String> conflictingLocks = new HashSet<String>();

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/TestLocationQueries.java
Patch:
@@ -88,7 +88,7 @@ public CheckResults(String outDir, String logDir, MiniClusterType miniMr,
         String hadoopVer, String locationSubdir)
       throws Exception
     {
-      super(outDir, logDir, miniMr, hadoopVer, "", "", null);
+      super(outDir, logDir, miniMr, null, hadoopVer, "", "", null, false);
       this.locationSubdir = locationSubdir;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -83,6 +83,8 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.util.ReflectionUtils;
 
+import com.google.common.collect.Lists;
+
 /**
  * File Sink operator implementation.
  **/
@@ -899,7 +901,7 @@ protected String generateListBucketingDirName(Object row) {
       /* create default directory. */
       lbDirName = FileUtils.makeDefaultListBucketingDirName(skewedCols,
           lbCtx.getDefaultDirName());
-      List<String> defaultKey = Arrays.asList(lbCtx.getDefaultKey());
+      List<String> defaultKey = Lists.newArrayList(lbCtx.getDefaultKey());
       if (!locationMap.containsKey(defaultKey)) {
         locationMap.put(defaultKey, lbDirName);
       }

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java
Patch:
@@ -195,7 +195,6 @@ private long getTotalHeapSize() {
     // in the usable space.
 
     long total = 0;
-    MemoryMXBean m = ManagementFactory.getMemoryMXBean();
     for (MemoryPoolMXBean mp : ManagementFactory.getMemoryPoolMXBeans()) {
       long sz = mp.getUsage().getMax();
       if (mp.getName().contains("Survivor")) {
@@ -205,6 +204,8 @@ private long getTotalHeapSize() {
         total += sz;
       }
     }
+    // round up to the next MB
+    total += (total % (1024*1024));
     return total;
   }
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1725,7 +1725,7 @@ public static enum ConfVars {
         "Hive metrics subsystem implementation class."),
     HIVE_METRICS_REPORTER("hive.service.metrics.reporter", "JSON_FILE, JMX",
         "Reporter type for metric class org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics, comma separated list of JMX, CONSOLE, JSON_FILE"),
-    HIVE_METRICS_JSON_FILE_LOCATION("hive.service.metrics.file.location", "file:///tmp/my-logging.properties",
+    HIVE_METRICS_JSON_FILE_LOCATION("hive.service.metrics.file.location", "file:///tmp/report.json",
         "For metric class org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics JSON_FILE reporter, the location of JSON metrics file.  " +
         "This file will get overwritten at every interval."),
     HIVE_METRICS_JSON_FILE_INTERVAL("hive.service.metrics.file.frequency", "5s",

File: common/src/test/org/apache/hadoop/hive/common/metrics/TestLegacyMetrics.java
Patch:
@@ -47,16 +47,16 @@ public class TestLegacyMetrics {
 
   @Before
   public void before() throws Exception {
-    MetricsFactory.deInit();
+    MetricsFactory.close();
     HiveConf conf = new HiveConf();
     conf.setVar(HiveConf.ConfVars.HIVE_METRICS_CLASS, LegacyMetrics.class.getCanonicalName());
     MetricsFactory.init(conf);
-    metrics = (LegacyMetrics) MetricsFactory.getMetricsInstance();
+    metrics = (LegacyMetrics) MetricsFactory.getInstance();
   }
 
   @After
   public void after() throws Exception {
-    MetricsFactory.deInit();
+    MetricsFactory.close();
   }
 
   @Test

File: service/src/java/org/apache/hive/service/server/HiveServer2.java
Patch:
@@ -308,9 +308,9 @@ public synchronized void stop() {
     HiveConf hiveConf = this.getHiveConf();
     super.stop();
     // Shutdown Metrics
-    if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_METRICS_ENABLED)) {
+    if (MetricsFactory.getInstance() != null) {
       try {
-        MetricsFactory.getMetricsInstance().deInit();
+        MetricsFactory.close();
       } catch (Exception e) {
         LOG.error("error in Metrics deinit: " + e.getClass().getName() + " "
           + e.getMessage(), e);
@@ -359,7 +359,7 @@ private static void startHiveServer2() throws Throwable {
         server.start();
 
         if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_METRICS_ENABLED)) {
-          MetricsFactory.getMetricsInstance().init(hiveConf);
+          MetricsFactory.init(hiveConf);
         }
         try {
           JvmPauseMonitor pauseMonitor = new JvmPauseMonitor(hiveConf);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -2378,9 +2378,9 @@ private Pair<RexNode, TypeInfo> genWindowingProj(QB qb, WindowExpressionSpec wEx
         WindowSpec wndSpec = ((WindowFunctionSpec) wExpSpec).getWindowSpec();
         List<RexNode> partitionKeys = getPartitionKeys(wndSpec.getPartition(), converter, inputRR);
         List<RexFieldCollation> orderKeys = getOrderKeys(wndSpec.getOrder(), converter, inputRR);
-        RexWindowBound upperBound = getBound(wndSpec.windowFrame.start, converter);
-        RexWindowBound lowerBound = getBound(wndSpec.windowFrame.end, converter);
-        boolean isRows = ((wndSpec.windowFrame.start instanceof RangeBoundarySpec) || (wndSpec.windowFrame.end instanceof RangeBoundarySpec)) ? true
+        RexWindowBound upperBound = getBound(wndSpec.getWindowFrame().start, converter);
+        RexWindowBound lowerBound = getBound(wndSpec.getWindowFrame().end, converter);
+        boolean isRows = ((wndSpec.getWindowFrame().start instanceof RangeBoundarySpec) || (wndSpec.getWindowFrame().end instanceof RangeBoundarySpec)) ? true
             : false;
 
         w = cluster.getRexBuilder().makeOver(calciteAggFnRetType, calciteAggFn, calciteAggFnArgs,

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -1173,6 +1173,9 @@ private CommandProcessorResponse runInternal(String command, boolean alreadyComp
       if (ret != 0) {
         return createProcessorResponse(ret);
       }
+    } else {
+      // Since we're reusing the compiled plan, we need to update its start time for current run
+      plan.setQueryStartTime(perfLogger.getStartTime(PerfLogger.DRIVER_RUN));
     }
 
     // the reason that we set the txn manager for the cxt here is because each

File: ql/src/java/org/apache/hadoop/hive/ql/parse/PTFTranslator.java
Patch:
@@ -610,6 +610,7 @@ private static void validateValueBoundaryExprType(ObjectInspector OI)
     case SHORT:
     case DECIMAL:
     case TIMESTAMP:
+    case DATE:
     case STRING:
       break;
     default:

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java
Patch:
@@ -110,7 +110,7 @@ private LlapIoImpl(Configuration conf) throws IOException {
     // Arbitrary thread pool. Listening is used for unhandled errors for now (TODO: remove?)
     int numThreads = HiveConf.getIntVar(conf, HiveConf.ConfVars.LLAP_IO_THREADPOOL_SIZE);
     executor = MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(numThreads,
-        new ThreadFactoryBuilder().setNameFormat("IO-Elevator-Thread-%d").build()));
+        new ThreadFactoryBuilder().setNameFormat("IO-Elevator-Thread-%d").setDaemon(true).build()));
 
     // TODO: this should depends on input format and be in a map, or something.
     this.cvp = new OrcColumnVectorProducer(metadataCache, orcCache, cache, conf, cacheMetrics,
@@ -148,5 +148,6 @@ public void close() {
       MBeans.unregister(buddyAllocatorMXBean);
       buddyAllocatorMXBean = null;
     }
+    executor.shutdownNow();
   }
 }

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreFsImpl.java
Patch:
@@ -60,7 +60,6 @@ public boolean deleteDir(FileSystem fs, Path f, boolean recursive,
     } catch (FileNotFoundException e) {
       return true; // ok even if there is not data
     } catch (Exception e) {
-      Warehouse.closeFs(fs);
       MetaStoreUtils.logAndThrowMetaException(e);
     }
     return false;

File: metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
Patch:
@@ -200,7 +200,6 @@ public boolean mkdirs(Path f, boolean inheritPermCandidate) throws MetaException
       fs = getFs(f);
       return FileUtils.mkdir(fs, f, inheritPerms, conf);
     } catch (IOException e) {
-      closeFs(fs);
       MetaStoreUtils.logAndThrowMetaException(e);
     }
     return false;
@@ -480,7 +479,6 @@ public boolean isDir(Path f) throws MetaException {
     } catch (FileNotFoundException e) {
       return false;
     } catch (IOException e) {
-      closeFs(fs);
       MetaStoreUtils.logAndThrowMetaException(e);
     }
     return true;

File: metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
Patch:
@@ -525,7 +525,7 @@ public FileStatus[] getFileStatusesForLocation(String location)
    */
   public FileStatus[] getFileStatusesForUnpartitionedTable(Database db, Table table)
       throws MetaException {
-    Path tablePath = getTablePath(db, table.getTableName());
+    Path tablePath = getDnsPath(new Path(table.getSd().getLocation()));
     try {
       FileSystem fileSys = tablePath.getFileSystem(conf);
       return HiveStatsUtils.getFileStatusRecurse(tablePath, -1, fileSys);

File: service/src/java/org/apache/hive/service/cli/Type.java
Patch:
@@ -149,7 +149,7 @@ public static Type getType(TTypeId tType) {
         return type;
       }
     }
-    throw new IllegalArgumentException("Unregonized Thrift TTypeId value: " + tType);
+    throw new IllegalArgumentException("Unrecognized Thrift TTypeId value: " + tType);
   }
 
   public static Type getType(String name) {

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/HcatDelegator.java
Patch:
@@ -766,7 +766,7 @@ public Response descOneColumn(String user, String db, String table, String colum
     Object o = res.getEntity();
     final Map fields = (o != null && (o instanceof Map)) ? (Map) o : null;
     if (fields == null)
-      throw new SimpleWebException(HttpStatus.INTERNAL_SERVER_ERROR_500, "Internal error, unable to find column "
+      throw new SimpleWebException(HttpStatus.NOT_FOUND_404, "Internal error, unable to find column "
         + column);
 
 
@@ -781,7 +781,7 @@ public Response descOneColumn(String user, String db, String table, String colum
       }
     }
     if (found == null)
-      throw new SimpleWebException(HttpStatus.INTERNAL_SERVER_ERROR_500, "unable to find column " + column,
+      throw new SimpleWebException(HttpStatus.NOT_FOUND_404, "unable to find column " + column,
         new HashMap<String, Object>() {
           {
             put("description", fields);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/RelOptHiveTable.java
Patch:
@@ -322,7 +322,7 @@ private void updateColStats(Set<Integer> projIndxLst, boolean allowNullColumnFor
             hiveColStats = new ArrayList<ColStatistics>();
             for (String c : nonPartColNamesThatRqrStats) {
               // add empty stats object for each column
-              hiveColStats.add(new ColStatistics(hiveTblMetadata.getTableName(), c, null));
+              hiveColStats.add(new ColStatistics(c, null));
             }
             colNamesFailedStats.clear();
           } else {
@@ -358,7 +358,7 @@ private void updateColStats(Set<Integer> projIndxLst, boolean allowNullColumnFor
     if (colNamesFailedStats.isEmpty() && !partColNamesThatRqrStats.isEmpty()) {
       ColStatistics cStats = null;
       for (int i = 0; i < partColNamesThatRqrStats.size(); i++) {
-        cStats = new ColStatistics(hiveTblMetadata.getTableName(), partColNamesThatRqrStats.get(i),
+        cStats = new ColStatistics(partColNamesThatRqrStats.get(i),
             hivePartitionColsMap.get(partColIndxsThatRqrStats.get(i)).getTypeName());
         cStats.setCountDistint(getDistinctCount(partitionList.getPartitions(),
             partColNamesThatRqrStats.get(i)));

File: ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractOperatorDesc.java
Patch:
@@ -27,7 +27,7 @@
 public class AbstractOperatorDesc implements OperatorDesc {
 
   protected boolean vectorMode = false;
-  protected transient Statistics statistics;
+  protected Statistics statistics;
   protected transient OpTraits opTraits;
   protected transient Map<String, String> opProps;
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -1930,6 +1930,8 @@ private void fireInsertEvent(Table tbl, Map<String, String> partitionSpec, List<
         for (Path p : newFiles) {
           insertData.addToFilesAdded(p.toString());
         }
+      } else {
+        insertData.setFilesAdded(new ArrayList<String>());
       }
       FireEventRequest rqst = new FireEventRequest(true, data);
       rqst.setDbName(tbl.getDbName());

File: metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -2758,7 +2758,7 @@ public void alterTable(String dbname, String name, Table newTable)
 
       MTable oldt = getMTable(dbname, name);
       if (oldt == null) {
-        throw new MetaException("table " + name + " doesn't exist");
+        throw new MetaException("table " + dbname + "." + name + " doesn't exist");
       }
 
       // For now only alter name, owner, parameters, cols, bucketcols are allowed

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/MetadataListStructObjectInspector.java
Patch:
@@ -49,7 +49,7 @@ public static MetadataListStructObjectInspector getInstance(
       List<String> columnNames) {
     ArrayList<List<String>> key = new ArrayList<List<String>>(1);
     key.add(columnNames);
-    MetadataListStructObjectInspector result = cached.get(columnNames);
+    MetadataListStructObjectInspector result = cached.get(key);
     if (result == null) {
       result = new MetadataListStructObjectInspector(columnNames);
       MetadataListStructObjectInspector prev = cached.putIfAbsent(key, result);

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/OutputJobInfo.java
Patch:
@@ -119,7 +119,7 @@ protected void setPosOfPartCols(List<Integer> posOfPartCols) {
     Collections.sort(posOfPartCols, new Comparator<Integer>() {
       @Override
       public int compare(Integer earlier, Integer later) {
-        return (earlier > later) ? -1 : ((earlier == later) ? 0 : 1);
+        return later.compareTo(earlier);
       }
     });
     this.posOfPartCols = posOfPartCols;

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -782,7 +782,7 @@ public void createIndex(String tableName, String indexName, String indexHandlerC
       }
 
       org.apache.hadoop.hive.metastore.api.Table baseTbl = getTable(tableName).getTTable();
-      if (baseTbl.getTableType() == TableType.VIRTUAL_VIEW.toString()) {
+      if (TableType.VIRTUAL_VIEW.toString().equals(baseTbl.getTableType())) {
         throw new HiveException("tableName="+ tableName +" is a VIRTUAL VIEW. Index on VIRTUAL VIEW is not supported.");
       }
       if (baseTbl.isTemporary()) {

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatFieldSchema.java
Patch:
@@ -218,11 +218,11 @@ public HCatFieldSchema(String fieldName, Type type, String comment) throws HCatE
   public HCatFieldSchema(String fieldName, PrimitiveTypeInfo typeInfo, String comment)
           throws HCatException {
     this.fieldName = fieldName;
-    //HCatUtil.assertNotNull(fieldName, "fieldName cannot be null");//seems sometimes it can be 
-    // null, for ARRAY types in particular (which may be a complex type)
     this.category = Category.PRIMITIVE;
     this.typeInfo = typeInfo;
-    HCatUtil.assertNotNull(typeInfo, "typeInfo cannot be null; fieldName=" + fieldName, null);
+    if (typeInfo == null) {
+      throw new IllegalArgumentException("typeInfo cannot be null; fieldName=" + fieldName);
+    }
     type = Type.getPrimitiveHType(typeInfo);
     this.comment = comment;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java
Patch:
@@ -238,7 +238,7 @@ private HybridHashTableContainer(float keyCountAdj, int threshold, float loadFac
         keyCountAdj, threshold, loadFactor, keyCount);
 
     memoryThreshold = memoryAvailable;
-    tableRowSize = estimatedTableSize / keyCount;
+    tableRowSize = estimatedTableSize / (keyCount != 0 ? keyCount : 1);
     memoryCheckFrequency = memCheckFreq;
 
     this.nwayConf = nwayConf;

File: service/src/java/org/apache/hive/service/cli/ColumnValue.java
Patch:
@@ -202,6 +202,8 @@ public static TColumnValue toTColumnValue(Type type, Object value) {
     case UNION_TYPE:
     case USER_DEFINED_TYPE:
       return stringValue((String)value);
+    case NULL_TYPE:
+      return stringValue((String)value);
     default:
       return null;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
Patch:
@@ -199,7 +199,7 @@ protected RecordReaderImpl(List<StripeInformation> stripes,
     firstRow = skippedRows;
     totalRowCount = rows;
     boolean skipCorrupt = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_ORC_SKIP_CORRUPT_DATA);
-    reader = TreeReaderFactory.createTreeReader(0, types, included, skipCorrupt);
+    reader = RecordReaderFactory.createTreeReader(0, conf, types, included, skipCorrupt);
     indexes = new OrcProto.RowIndex[types.size()];
     bloomFilterIndices = new OrcProto.BloomFilterIndex[types.size()];
     advanceToNextRow(reader, 0L, true);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
Patch:
@@ -125,7 +125,7 @@ public static enum Counter {
   protected transient Object[] cachedValues;
   protected transient List<List<Integer>> distinctColIndices;
   protected transient Random random;
-  protected transient int bucketNumber;
+  protected transient int bucketNumber = -1;
 
   /**
    * This two dimensional array holds key data and a corresponding Union object
@@ -552,6 +552,7 @@ private BytesWritable makeValueWritable(Object row) throws Exception {
     // in case of bucketed table, insert the bucket number as the last column in value
     if (bucketEval != null) {
       length -= 1;
+      assert bucketNumber >= 0;
       cachedValues[length] = new Text(String.valueOf(bucketNumber));
     }
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1657,7 +1657,8 @@ public static enum ConfVars {
         "imported on to tables that are the target of replication. If this parameter is\n" +
         "set, regular imports will check if the destination table(if it exists) has a " +
         "'repl.last.id' set on it. If so, it will fail."),
-    HIVE_REPL_TASK_FACTORY("hive.repl.task.factory","",
+    HIVE_REPL_TASK_FACTORY("hive.repl.task.factory",
+        "org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory",
         "Parameter that can be used to override which ReplicationTaskFactory will be\n" +
         "used to instantiate ReplicationTask events. Override for third party repl plugins"),
     HIVE_MAPPER_CANNOT_SPAN_MULTIPLE_PARTITIONS("hive.mapper.cannot.span.multiple.partitions", false, ""),

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCachePolicy.java
Patch:
@@ -20,10 +20,11 @@
 
 import org.apache.hadoop.hive.llap.io.api.cache.LowLevelCache.Priority;
 
-public interface LowLevelCachePolicy {
+public interface LowLevelCachePolicy extends LlapOomDebugDump {
   void cache(LlapCacheableBuffer buffer, Priority priority);
   void notifyLock(LlapCacheableBuffer buffer);
   void notifyUnlock(LlapCacheableBuffer buffer);
   long evictSomeBlocks(long memoryToReserve);
   void setEvictionListener(EvictionListener listener);
+  void setParentDebugDumper(LlapOomDebugDump dumper);
 }

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/MemoryManager.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hadoop.hive.llap.cache;
 
-public interface MemoryManager {
+public interface MemoryManager extends LlapOomDebugDump {
   boolean reserveMemory(long memoryToReserve, boolean waitForEviction);
   void releaseMemory(long memUsage);
 }

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java
Patch:
@@ -102,6 +102,7 @@ private LlapIoImpl(Configuration conf) throws IOException {
       orcCache = new LowLevelCacheImpl(cacheMetrics, cachePolicy, allocator, true);
       // And finally cache policy uses cache to notify it of eviction. The cycle is complete!
       cachePolicy.setEvictionListener(new EvictionDispatcher(orcCache, metadataCache));
+      cachePolicy.setParentDebugDumper(orcCache);
       orcCache.init();
     } else {
       cachePolicy.setEvictionListener(new EvictionDispatcher(null, metadataCache));

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatRecordReader.java
Patch:
@@ -63,7 +63,7 @@ class HCatRecordReader extends RecordReader<WritableComparable, HCatRecord> {
 
   private Deserializer deserializer;
 
-  private Map<String, String> valuesNotInDataCols;
+  private Map<String, Object> valuesNotInDataCols;
 
   private HCatSchema outputSchema = null;
   private HCatSchema dataSchema = null;
@@ -72,7 +72,7 @@ class HCatRecordReader extends RecordReader<WritableComparable, HCatRecord> {
    * Instantiates a new hcat record reader.
    */
   public HCatRecordReader(HiveStorageHandler storageHandler,
-              Map<String, String> valuesNotInDataCols) {
+              Map<String, Object> valuesNotInDataCols) {
     this.storageHandler = storageHandler;
     this.valuesNotInDataCols = valuesNotInDataCols;
   }

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatMapReduceTest.java
Patch:
@@ -381,6 +381,7 @@ List<HCatRecord> runMRRead(int readCount, String filter) throws Exception {
     readRecords.clear();
 
     Configuration conf = new Configuration();
+    conf.set(HiveConf.ConfVars.METASTORE_INTEGER_JDO_PUSHDOWN.varname,"true");
     Job job = new Job(conf, "hcat mapreduce read test");
     job.setJarByClass(this.getClass());
     job.setMapperClass(HCatMapReduceTest.MapRead.class);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java
Patch:
@@ -126,7 +126,8 @@ protected void addKey(String outputType) throws HiveException {
       doubleIndices[doubleIndicesIndex] = addIndex;
       indexLookup[addIndex].setDouble(doubleIndicesIndex);
       ++doubleIndicesIndex;
-    } else if (VectorizationContext.isStringFamily(outputType)) {
+    } else if (VectorizationContext.isStringFamily(outputType) ||
+        outputType.equalsIgnoreCase("binary")) {
       stringIndices[stringIndicesIndex]= addIndex;
       indexLookup[addIndex].setString(stringIndicesIndex);
       ++stringIndicesIndex;

File: beeline/src/java/org/apache/hive/beeline/BeeLine.java
Patch:
@@ -809,7 +809,8 @@ private int execute(ConsoleReader reader, boolean exitOnError) {
       try {
         // Execute one instruction; terminate on executing a script if there is an error
         // in silent mode, prevent the query and prompt being echoed back to terminal
-        line = getOpts().isSilent() ? reader.readLine(null, ConsoleReader.NULL_MASK) : reader.readLine(getPrompt());
+        line = (getOpts().isSilent() && getOpts().getScriptFile() != null) ?
+                 reader.readLine(null, ConsoleReader.NULL_MASK) : reader.readLine(getPrompt());
 
         if (!dispatch(line) && exitOnError) {
           return ERRNO_OTHER;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketingSortingReduceSinkOptimizer.java
Patch:
@@ -57,6 +57,7 @@
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.SMBJoinDesc;
 import org.apache.hadoop.hive.ql.plan.SelectDesc;
+import org.apache.hadoop.hive.shims.ShimLoader;
 
 /**
  * This transformation does optimization for enforcing bucketing and sorting.
@@ -216,7 +217,7 @@ private boolean checkTable(Table table,
     private void storeBucketPathMapping(TableScanOperator tsOp, FileStatus[] srcs) {
       Map<String, Integer> bucketFileNameMapping = new HashMap<String, Integer>();
       for (int pos = 0; pos < srcs.length; pos++) {
-        if(!srcs[pos].isFile()) {
+        if (ShimLoader.getHadoopShims().isDirectory(srcs[pos])) {
           throw new RuntimeException("Was expecting '" + srcs[pos].getPath() + "' to be bucket file.");
         }
         bucketFileNameMapping.put(srcs[pos].getPath().getName(), pos);

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/DateColumnStatistics.java
Patch:
@@ -17,7 +17,7 @@
  */
 package org.apache.hadoop.hive.ql.io.orc;
 
-import org.apache.hadoop.hive.serde2.io.DateWritable;
+import java.util.Date;
 
 /**
  * Statistics for DATE columns.
@@ -27,11 +27,11 @@ public interface DateColumnStatistics extends ColumnStatistics {
    * Get the minimum value for the column.
    * @return minimum value
    */
-  DateWritable getMinimum();
+  Date getMinimum();
 
   /**
    * Get the maximum value for the column.
    * @return maximum value
    */
-  DateWritable getMaximum();
+  Date getMaximum();
 }

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/MetaStoreAuthzAPIAuthorizerEmbedOnly.java
Patch:
@@ -37,7 +37,8 @@ public class MetaStoreAuthzAPIAuthorizerEmbedOnly extends HiveAuthorizationProvi
     implements HiveMetastoreAuthorizationProvider {
 
   public static final String errMsg = "Metastore Authorization api invocation for "
-      + "remote metastore is disabled in this configuration.";
+      + "remote metastore is disabled in this configuration. Run commands via jdbc/odbc clients "
+      + "via HiveServer2 that is using embedded metastore.";
 
   @Override
   public void init(Configuration conf) throws HiveException {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -147,6 +147,7 @@
 import org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter;
 import org.apache.hadoop.hive.ql.optimizer.calcite.translator.JoinCondTypeCheckProcFactory;
 import org.apache.hadoop.hive.ql.optimizer.calcite.translator.JoinTypeCheckCtx;
+import org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForReturnPath;
 import org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter;
 import org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter;
 import org.apache.hadoop.hive.ql.optimizer.calcite.translator.TypeConverter;
@@ -629,7 +630,8 @@ Operator getOptimizedHiveOPDag() throws SemanticException {
       throw new AssertionError("rethrowCalciteException didn't throw for " + e.getMessage());
     }
 
-    RelNode modifiedOptimizedOptiqPlan = introduceProjectIfNeeded(optimizedOptiqPlan);
+    RelNode modifiedOptimizedOptiqPlan = PlanModifierForReturnPath.convertOpTree(
+            introduceProjectIfNeeded(optimizedOptiqPlan), topLevelFieldSchema);
 
     LOG.debug("Translating the following plan:\n" + RelOptUtil.toString(modifiedOptimizedOptiqPlan));
     Operator<?> hiveRoot = new HiveOpConverter(this, conf, unparseTranslator, topOps,

File: ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
Patch:
@@ -530,8 +530,6 @@ public static SessionState start(SessionState startSs) {
       } catch (Exception e) {
         throw new RuntimeException(e);
       }
-    } else {
-      LOG.info("No Tez session required at this point. hive.execution.engine=mr.");
     }
     return startSs;
   }

File: llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java
Patch:
@@ -312,7 +312,7 @@ public int getClusterNodeCount() {
   }
 
   @Override
-  public void resetMatchLocalityForAllHeldContainers() {
+  public void dagComplete() {
     // This is effectively DAG completed, and can be used to reset statistics being tracked.
     LOG.info("DAG: " + dagCounter.get() + " completed. Scheduling stats: " + dagStats);
     dagCounter.incrementAndGet();

File: ql/src/java/org/apache/hadoop/hive/ql/plan/SparkWork.java
Patch:
@@ -236,13 +236,15 @@ public void remove(BaseWork work) {
     List<BaseWork> parents = getParents(work);
 
     for (BaseWork w: children) {
+      edgeProperties.remove(new ImmutablePair<BaseWork, BaseWork>(work, w));
       invertedWorkGraph.get(w).remove(work);
       if (invertedWorkGraph.get(w).size() == 0) {
         roots.add(w);
       }
     }
 
     for (BaseWork w: parents) {
+      edgeProperties.remove(new ImmutablePair<BaseWork, BaseWork>(w, work));
       workGraph.get(w).remove(work);
       if (workGraph.get(w).size() == 0) {
         leaves.add(w);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java
Patch:
@@ -46,7 +46,7 @@ public class HiveSparkClientFactory {
   protected static final transient Log LOG = LogFactory.getLog(HiveSparkClientFactory.class);
 
   private static final String SPARK_DEFAULT_CONF_FILE = "spark-defaults.conf";
-  private static final String SPARK_DEFAULT_MASTER = "local";
+  private static final String SPARK_DEFAULT_MASTER = "yarn-cluster";
   private static final String SPARK_DEFAULT_APP_NAME = "Hive on Spark";
   private static final String SPARK_DEFAULT_SERIALIZER = "org.apache.spark.serializer.KryoSerializer";
   private static final String SPARK_DEFAULT_REFERENCE_TRACKING = "false";

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/TypeConverter.java
Patch:
@@ -240,7 +240,7 @@ public static TypeInfo convert(RelDataType rType) {
     } else if (rType.getKeyType() != null) {
       return convertMapType(rType);
     } else {
-      return convertPrimtiveType(rType);
+      return convertPrimitiveType(rType);
     }
   }
 
@@ -271,7 +271,7 @@ public static TypeInfo convertListType(RelDataType rType) {
     return TypeInfoFactory.getListTypeInfo(convert(rType.getComponentType()));
   }
 
-  public static TypeInfo convertPrimtiveType(RelDataType rType) {
+  public static TypeInfo convertPrimitiveType(RelDataType rType) {
     switch (rType.getSqlTypeName()) {
     case BOOLEAN:
       return TypeInfoFactory.booleanTypeInfo;

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
Patch:
@@ -80,7 +80,7 @@
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 import javax.annotation.Nullable;
 
@@ -372,7 +372,7 @@ static public Deserializer getDeserializer(Configuration conf,
       return null;
     }
     try {
-      Deserializer deserializer = ReflectionUtils.newInstance(conf.getClassByName(lib).
+      Deserializer deserializer = ReflectionUtil.newInstance(conf.getClassByName(lib).
               asSubclass(Deserializer.class), conf);
       if (skipConfError) {
         SerDeUtils.initializeSerDeWithoutErrorCheck(deserializer, conf,
@@ -419,7 +419,7 @@ static public Deserializer getDeserializer(Configuration conf,
       org.apache.hadoop.hive.metastore.api.Table table) throws MetaException {
     String lib = part.getSd().getSerdeInfo().getSerializationLib();
     try {
-      Deserializer deserializer = ReflectionUtils.newInstance(conf.getClassByName(lib).
+      Deserializer deserializer = ReflectionUtil.newInstance(conf.getClassByName(lib).
         asSubclass(Deserializer.class), conf);
       SerDeUtils.initializeSerDe(deserializer, conf, MetaStoreUtils.getTableMetadata(table),
                                  MetaStoreUtils.getPartitionMetadata(part, table));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -207,9 +207,9 @@
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.tools.HadoopArchives;
-import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.ToolRunner;
 import org.apache.hive.common.util.AnnotationUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 import org.stringtemplate.v4.ST;
 
 /**
@@ -3778,7 +3778,7 @@ private boolean updateModifiedParameters(Map<String, String> params, HiveConf co
   private void validateSerDe(String serdeName) throws HiveException {
     try {
 
-      Deserializer d = ReflectionUtils.newInstance(conf.getClassByName(serdeName).
+      Deserializer d = ReflectionUtil.newInstance(conf.getClassByName(serdeName).
           asSubclass(Deserializer.class), conf);
       if (d != null) {
         LOG.debug("Found class for " + serdeName);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultFetchFormatter.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.hadoop.hive.serde2.SerDe;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * serialize row by user specified serde and call toString() to make string type result
@@ -50,12 +50,13 @@ public void initialize(Configuration hconf, Properties props) throws HiveExcepti
     }
   }
 
+//TODO#: THIS
   private SerDe initializeSerde(Configuration conf, Properties props) throws Exception {
     String serdeName = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEFETCHOUTPUTSERDE);
     Class<? extends SerDe> serdeClass = Class.forName(serdeName, true,
         Utilities.getSessionSpecifiedClassLoader()).asSubclass(SerDe.class);
     // cast only needed for Hadoop 0.17 compatibility
-    SerDe serde = ReflectionUtils.newInstance(serdeClass, null);
+    SerDe serde = ReflectionUtil.newInstance(serdeClass, null);
 
     Properties serdeProps = new Properties();
     if (serde instanceof DelimitedJSONSerDe) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DemuxOperator.java
Patch:
@@ -39,7 +39,7 @@
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * DemuxOperator is an operator used by MapReduce Jobs optimized by
@@ -134,12 +134,12 @@ protected Collection<Future<?>> initializeOp(Configuration hconf) throws HiveExc
         cntrs[newTag] = 0;
         nextCntrs[newTag] = 0;
         TableDesc keyTableDesc = conf.getKeysSerializeInfos().get(newTag);
-        Deserializer inputKeyDeserializer = ReflectionUtils.newInstance(keyTableDesc
+        Deserializer inputKeyDeserializer = ReflectionUtil.newInstance(keyTableDesc
             .getDeserializerClass(), null);
         SerDeUtils.initializeSerDe(inputKeyDeserializer, null, keyTableDesc.getProperties(), null);
 
         TableDesc valueTableDesc = conf.getValuesSerializeInfos().get(newTag);
-        Deserializer inputValueDeserializer = ReflectionUtils.newInstance(valueTableDesc
+        Deserializer inputValueDeserializer = ReflectionUtil.newInstance(valueTableDesc
             .getDeserializerClass(), null);
         SerDeUtils.initializeSerDe(inputValueDeserializer, null, valueTableDesc.getProperties(),
                                    null);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
Patch:
@@ -69,9 +69,9 @@
 import org.apache.hadoop.mapred.JobConfigurable;
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.hive.common.util.AnnotationUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 import com.google.common.collect.Iterators;
 
@@ -204,13 +204,13 @@ static InputFormat getInputFormatFromCache(
     Class<? extends InputFormat> inputFormatClass, JobConf conf) throws IOException {
     if (Configurable.class.isAssignableFrom(inputFormatClass) ||
         JobConfigurable.class.isAssignableFrom(inputFormatClass)) {
-      return ReflectionUtils.newInstance(inputFormatClass, conf);
+      return ReflectionUtil.newInstance(inputFormatClass, conf);
     }
     // TODO: why is this copy-pasted from HiveInputFormat?
     InputFormat format = inputFormats.get(inputFormatClass.getName());
     if (format == null) {
       try {
-        format = ReflectionUtils.newInstance(inputFormatClass, conf);
+        format = ReflectionUtil.newInstance(inputFormatClass, conf);
         inputFormats.put(inputFormatClass.getName(), format);
       } catch (Exception e) {
         throw new IOException("Cannot create an instance of InputFormat class "

File: ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java
Patch:
@@ -44,7 +44,7 @@
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.SequenceFileInputFormat;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 public class JoinUtil {
 
@@ -268,7 +268,7 @@ public static SerDe getSpillSerDe(byte alias, TableDesc[] spillTableDesc,
     if (desc == null) {
       return null;
     }
-    SerDe sd = (SerDe) ReflectionUtils.newInstance(desc.getDeserializerClass(),
+    SerDe sd = (SerDe) ReflectionUtil.newInstance(desc.getDeserializerClass(),
         null);
     try {
       SerDeUtils.initializeSerDe(sd, null, desc.getProperties(), null);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java
Patch:
@@ -51,7 +51,7 @@
 import org.apache.hadoop.io.WritableComparator;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.util.PriorityQueue;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * Sorted Merge Map Join Operator.
@@ -525,7 +525,7 @@ private void setUpFetchContexts(String alias, MergeQueue mergeQueue) throws Hive
 
     BucketMapJoinContext bucketMatcherCxt = localWork.getBucketMapjoinContext();
     Class<? extends BucketMatcher> bucketMatcherCls = bucketMatcherCxt.getBucketMatcherClass();
-    BucketMatcher bucketMatcher = ReflectionUtils.newInstance(bucketMatcherCls, null);
+    BucketMatcher bucketMatcher = ReflectionUtil.newInstance(bucketMatcherCls, null);
 
     getExecContext().setFileId(bucketMatcherCxt.createFileId(currentInputPath.toString()));
     if (isLogInfoEnabled) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/StatsNoJobTask.java
Patch:
@@ -52,8 +52,8 @@
 import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 import com.google.common.collect.Lists;
 import com.google.common.collect.MapMaker;
@@ -150,7 +150,7 @@ public void run() {
         boolean statsAvailable = false;
         for(FileStatus file: fileList) {
           if (!file.isDir()) {
-            InputFormat<?, ?> inputFormat = (InputFormat<?, ?>) ReflectionUtils.newInstance(
+            InputFormat<?, ?> inputFormat = (InputFormat<?, ?>) ReflectionUtil.newInstance(
                 partn.getInputFormatClass(), jc);
             InputSplit dummySplit = new FileSplit(file.getPath(), 0, 0,
                 new String[] { partn.getLocation() });
@@ -249,7 +249,7 @@ private int aggregateStats(ExecutorService threadPool) {
           for(FileStatus file: fileList) {
             if (!file.isDir()) {
               // TODO: do we need to wrap for Llap here? probably later when stats are cached?
-              InputFormat<?, ?> inputFormat = (InputFormat<?, ?>) ReflectionUtils.newInstance(
+              InputFormat<?, ?> inputFormat = (InputFormat<?, ?>) ReflectionUtil.newInstance(
                   table.getInputFormatClass(), jc);
               InputSplit dummySplit = new FileSplit(file.getPath(), 0, 0, new String[] { table
                   .getDataLocation().toString() });

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java
Patch:
@@ -46,7 +46,7 @@
 import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * Simple persistent container for rows.
@@ -213,8 +213,7 @@ public ROW first() throws HiveException {
         JobConf localJc = getLocalFSJobConfClone(jc);
         if (inputSplits == null) {
           if (this.inputFormat == null) {
-            // TODO: do we need to wrap here?
-            inputFormat = ReflectionUtils.newInstance(
+            inputFormat = ReflectionUtil.newInstance(
                 tblDesc.getInputFileFormatClass(), localJc);
           }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/BucketizedHiveInputSplit.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit;
 import org.apache.hadoop.mapred.FileSplit;
 import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * HiveInputSplit encapsulates an InputSplit with its corresponding
@@ -137,12 +137,11 @@ public String[] getLocations() throws IOException {
   @Override
   public void readFields(DataInput in) throws IOException {
     String inputSplitClassName = in.readUTF();
-
     int numSplits = in.readInt();
     inputSplits = new InputSplit[numSplits];
     for (int i = 0; i < numSplits; i++) {
       try {
-        inputSplits[i] = (InputSplit) ReflectionUtils.newInstance(conf
+        inputSplits[i] = (InputSplit) ReflectionUtil.newInstance(conf
             .getClassByName(inputSplitClassName), conf);
       } catch (Exception e) {
         throw new IOException(

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveFileFormatUtils.java
Patch:
@@ -59,7 +59,7 @@
 import org.apache.hadoop.mapred.TaskAttemptContext;
 import org.apache.hadoop.mapred.TextInputFormat;
 import org.apache.hadoop.util.Shell;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * An util class for various Hive file format tasks.
@@ -274,7 +274,7 @@ public static RecordWriter getRecordWriter(JobConf jc,
 
   private static HiveOutputFormat<?, ?> getHiveOutputFormat(
       Configuration conf, Class<? extends OutputFormat> outputClass) throws HiveException {
-    OutputFormat<?, ?> outputFormat = ReflectionUtils.newInstance(outputClass, conf);
+    OutputFormat<?, ?> outputFormat = ReflectionUtil.newInstance(outputClass, conf);
     if (!(outputFormat instanceof HiveOutputFormat)) {
       outputFormat = new HivePassThroughOutputFormat(outputFormat);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java
Patch:
@@ -40,8 +40,8 @@
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.OutputFormat;
-import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hive.common.util.HiveStringUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * PartitionDesc.
@@ -136,7 +136,7 @@ public String getDeserializerClassName() {
   public Deserializer getDeserializer(Configuration conf) throws Exception {
     Properties schema = getProperties();
     String clazzName = getDeserializerClassName();
-    Deserializer deserializer = ReflectionUtils.newInstance(conf.getClassByName(clazzName)
+    Deserializer deserializer = ReflectionUtil.newInstance(conf.getClassByName(clazzName)
         .asSubclass(Deserializer.class), conf);
     SerDeUtils.initializeSerDe(deserializer, conf, getTableDesc().getProperties(), schema);
     return deserializer;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/TableDesc.java
Patch:
@@ -33,8 +33,9 @@
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.OutputFormat;
-import org.apache.hadoop.util.ReflectionUtils;
+
 import org.apache.hive.common.util.HiveStringUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * TableDesc.
@@ -89,7 +90,7 @@ public Deserializer getDeserializer(Configuration conf) throws Exception {
   }
 
   public Deserializer getDeserializer(Configuration conf, boolean ignoreError) throws Exception {
-    Deserializer de = ReflectionUtils.newInstance(
+    Deserializer de = ReflectionUtil.newInstance(
         getDeserializerClass().asSubclass(Deserializer.class), conf);
     if (ignoreError) {
       SerDeUtils.initializeSerDeWithoutErrorCheck(de, conf, properties, null);

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
Patch:
@@ -80,7 +80,7 @@
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 import javax.annotation.Nullable;
 
@@ -372,7 +372,7 @@ static public Deserializer getDeserializer(Configuration conf,
       return null;
     }
     try {
-      Deserializer deserializer = ReflectionUtils.newInstance(conf.getClassByName(lib).
+      Deserializer deserializer = ReflectionUtil.newInstance(conf.getClassByName(lib).
               asSubclass(Deserializer.class), conf);
       if (skipConfError) {
         SerDeUtils.initializeSerDeWithoutErrorCheck(deserializer, conf,
@@ -419,7 +419,7 @@ static public Deserializer getDeserializer(Configuration conf,
       org.apache.hadoop.hive.metastore.api.Table table) throws MetaException {
     String lib = part.getSd().getSerdeInfo().getSerializationLib();
     try {
-      Deserializer deserializer = ReflectionUtils.newInstance(conf.getClassByName(lib).
+      Deserializer deserializer = ReflectionUtil.newInstance(conf.getClassByName(lib).
         asSubclass(Deserializer.class), conf);
       SerDeUtils.initializeSerDe(deserializer, conf, MetaStoreUtils.getTableMetadata(table),
                                  MetaStoreUtils.getPartitionMetadata(part, table));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -207,9 +207,9 @@
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.tools.HadoopArchives;
-import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.ToolRunner;
 import org.apache.hive.common.util.AnnotationUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 import org.stringtemplate.v4.ST;
 
 /**
@@ -3786,7 +3786,7 @@ private boolean updateModifiedParameters(Map<String, String> params, HiveConf co
   private void validateSerDe(String serdeName) throws HiveException {
     try {
 
-      Deserializer d = ReflectionUtils.newInstance(conf.getClassByName(serdeName).
+      Deserializer d = ReflectionUtil.newInstance(conf.getClassByName(serdeName).
           asSubclass(Deserializer.class), conf);
       if (d != null) {
         LOG.debug("Found class for " + serdeName);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultFetchFormatter.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.hadoop.hive.serde2.SerDe;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * serialize row by user specified serde and call toString() to make string type result
@@ -50,12 +50,13 @@ public void initialize(Configuration hconf, Properties props) throws HiveExcepti
     }
   }
 
+//TODO#: THIS
   private SerDe initializeSerde(Configuration conf, Properties props) throws Exception {
     String serdeName = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEFETCHOUTPUTSERDE);
     Class<? extends SerDe> serdeClass = Class.forName(serdeName, true,
         Utilities.getSessionSpecifiedClassLoader()).asSubclass(SerDe.class);
     // cast only needed for Hadoop 0.17 compatibility
-    SerDe serde = ReflectionUtils.newInstance(serdeClass, null);
+    SerDe serde = ReflectionUtil.newInstance(serdeClass, null);
 
     Properties serdeProps = new Properties();
     if (serde instanceof DelimitedJSONSerDe) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DemuxOperator.java
Patch:
@@ -39,7 +39,7 @@
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * DemuxOperator is an operator used by MapReduce Jobs optimized by
@@ -134,12 +134,12 @@ protected Collection<Future<?>> initializeOp(Configuration hconf) throws HiveExc
         cntrs[newTag] = 0;
         nextCntrs[newTag] = 0;
         TableDesc keyTableDesc = conf.getKeysSerializeInfos().get(newTag);
-        Deserializer inputKeyDeserializer = ReflectionUtils.newInstance(keyTableDesc
+        Deserializer inputKeyDeserializer = ReflectionUtil.newInstance(keyTableDesc
             .getDeserializerClass(), null);
         SerDeUtils.initializeSerDe(inputKeyDeserializer, null, keyTableDesc.getProperties(), null);
 
         TableDesc valueTableDesc = conf.getValuesSerializeInfos().get(newTag);
-        Deserializer inputValueDeserializer = ReflectionUtils.newInstance(valueTableDesc
+        Deserializer inputValueDeserializer = ReflectionUtil.newInstance(valueTableDesc
             .getDeserializerClass(), null);
         SerDeUtils.initializeSerDe(inputValueDeserializer, null, valueTableDesc.getProperties(),
                                    null);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/JoinUtil.java
Patch:
@@ -44,7 +44,7 @@
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.SequenceFileInputFormat;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 public class JoinUtil {
 
@@ -268,7 +268,7 @@ public static SerDe getSpillSerDe(byte alias, TableDesc[] spillTableDesc,
     if (desc == null) {
       return null;
     }
-    SerDe sd = (SerDe) ReflectionUtils.newInstance(desc.getDeserializerClass(),
+    SerDe sd = (SerDe) ReflectionUtil.newInstance(desc.getDeserializerClass(),
         null);
     try {
       SerDeUtils.initializeSerDe(sd, null, desc.getProperties(), null);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java
Patch:
@@ -51,7 +51,7 @@
 import org.apache.hadoop.io.WritableComparator;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.util.PriorityQueue;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * Sorted Merge Map Join Operator.
@@ -525,7 +525,7 @@ private void setUpFetchContexts(String alias, MergeQueue mergeQueue) throws Hive
 
     BucketMapJoinContext bucketMatcherCxt = localWork.getBucketMapjoinContext();
     Class<? extends BucketMatcher> bucketMatcherCls = bucketMatcherCxt.getBucketMatcherClass();
-    BucketMatcher bucketMatcher = ReflectionUtils.newInstance(bucketMatcherCls, null);
+    BucketMatcher bucketMatcher = ReflectionUtil.newInstance(bucketMatcherCls, null);
 
     getExecContext().setFileId(bucketMatcherCxt.createFileId(currentInputPath.toString()));
     if (isLogInfoEnabled) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/StatsNoJobTask.java
Patch:
@@ -52,8 +52,8 @@
 import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 import com.google.common.collect.Lists;
 import com.google.common.collect.MapMaker;
@@ -150,7 +150,7 @@ public void run() {
         boolean statsAvailable = false;
         for(FileStatus file: fileList) {
           if (!file.isDir()) {
-            InputFormat<?, ?> inputFormat = (InputFormat<?, ?>) ReflectionUtils.newInstance(
+            InputFormat<?, ?> inputFormat = (InputFormat<?, ?>) ReflectionUtil.newInstance(
                 partn.getInputFormatClass(), jc);
             InputSplit dummySplit = new FileSplit(file.getPath(), 0, 0,
                 new String[] { partn.getLocation() });
@@ -248,7 +248,7 @@ private int aggregateStats(ExecutorService threadPool) {
           boolean statsAvailable = false;
           for(FileStatus file: fileList) {
             if (!file.isDir()) {
-              InputFormat<?, ?> inputFormat = (InputFormat<?, ?>) ReflectionUtils.newInstance(
+              InputFormat<?, ?> inputFormat = (InputFormat<?, ?>) ReflectionUtil.newInstance(
                   table.getInputFormatClass(), jc);
               InputSplit dummySplit = new FileSplit(file.getPath(), 0, 0, new String[] { table
                   .getDataLocation().toString() });

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java
Patch:
@@ -46,7 +46,7 @@
 import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * Simple persistent container for rows.
@@ -213,7 +213,7 @@ public ROW first() throws HiveException {
         JobConf localJc = getLocalFSJobConfClone(jc);
         if (inputSplits == null) {
           if (this.inputFormat == null) {
-            inputFormat = ReflectionUtils.newInstance(
+            inputFormat = ReflectionUtil.newInstance(
                 tblDesc.getInputFileFormatClass(), localJc);
           }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/BucketizedHiveInputSplit.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit;
 import org.apache.hadoop.mapred.FileSplit;
 import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * HiveInputSplit encapsulates an InputSplit with its corresponding
@@ -137,12 +137,11 @@ public String[] getLocations() throws IOException {
   @Override
   public void readFields(DataInput in) throws IOException {
     String inputSplitClassName = in.readUTF();
-
     int numSplits = in.readInt();
     inputSplits = new InputSplit[numSplits];
     for (int i = 0; i < numSplits; i++) {
       try {
-        inputSplits[i] = (InputSplit) ReflectionUtils.newInstance(conf
+        inputSplits[i] = (InputSplit) ReflectionUtil.newInstance(conf
             .getClassByName(inputSplitClassName), conf);
       } catch (Exception e) {
         throw new IOException(

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveFileFormatUtils.java
Patch:
@@ -59,7 +59,7 @@
 import org.apache.hadoop.mapred.TaskAttemptContext;
 import org.apache.hadoop.mapred.TextInputFormat;
 import org.apache.hadoop.util.Shell;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * An util class for various Hive file format tasks.
@@ -274,7 +274,7 @@ public static RecordWriter getRecordWriter(JobConf jc,
 
   private static HiveOutputFormat<?, ?> getHiveOutputFormat(
       Configuration conf, Class<? extends OutputFormat> outputClass) throws HiveException {
-    OutputFormat<?, ?> outputFormat = ReflectionUtils.newInstance(outputClass, conf);
+    OutputFormat<?, ?> outputFormat = ReflectionUtil.newInstance(outputClass, conf);
     if (!(outputFormat instanceof HiveOutputFormat)) {
       outputFormat = new HivePassThroughOutputFormat(outputFormat);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PartitionDesc.java
Patch:
@@ -40,7 +40,7 @@
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.OutputFormat;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 import org.apache.hive.common.util.HiveStringUtils;
 import org.apache.hadoop.hive.ql.plan.Explain.Level;
 
@@ -138,7 +138,7 @@ public String getDeserializerClassName() {
   public Deserializer getDeserializer(Configuration conf) throws Exception {
     Properties schema = getProperties();
     String clazzName = getDeserializerClassName();
-    Deserializer deserializer = ReflectionUtils.newInstance(conf.getClassByName(clazzName)
+    Deserializer deserializer = ReflectionUtil.newInstance(conf.getClassByName(clazzName)
         .asSubclass(Deserializer.class), conf);
     SerDeUtils.initializeSerDe(deserializer, conf, getTableDesc().getProperties(), schema);
     return deserializer;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.util.ArrayList;
 import java.util.HashMap;
-import java.util.LinkedHashMap;
 import java.util.LinkedHashSet;
 import java.util.List;
 import java.util.Map;
@@ -40,7 +39,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * ReduceWork represents all the information used to run a reduce task on the cluster.
@@ -114,7 +113,7 @@ public TableDesc getKeyDesc() {
   private ObjectInspector getObjectInspector(TableDesc desc) {
     ObjectInspector objectInspector;
     try {
-      Deserializer deserializer = ReflectionUtils.newInstance(desc
+      Deserializer deserializer = ReflectionUtil.newInstance(desc
                 .getDeserializerClass(), null);
       SerDeUtils.initializeSerDe(deserializer, null, desc.getProperties(), null);
       objectInspector = deserializer.getObjectInspector();

File: ql/src/java/org/apache/hadoop/hive/ql/plan/TableDesc.java
Patch:
@@ -34,8 +34,9 @@
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.OutputFormat;
-import org.apache.hadoop.util.ReflectionUtils;
+
 import org.apache.hive.common.util.HiveStringUtils;
+import org.apache.hive.common.util.ReflectionUtil;
 
 /**
  * TableDesc.
@@ -90,7 +91,7 @@ public Deserializer getDeserializer(Configuration conf) throws Exception {
   }
 
   public Deserializer getDeserializer(Configuration conf, boolean ignoreError) throws Exception {
-    Deserializer de = ReflectionUtils.newInstance(
+    Deserializer de = ReflectionUtil.newInstance(
         getDeserializerClass().asSubclass(Deserializer.class), conf);
     if (ignoreError) {
       SerDeUtils.initializeSerDeWithoutErrorCheck(de, conf, properties, null);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveSortExchange.java
Patch:
@@ -1,6 +1,5 @@
 package org.apache.hadoop.hive.ql.optimizer.calcite.reloperators;
 
-import org.apache.calcite.plan.Convention;
 import org.apache.calcite.plan.RelOptCluster;
 import org.apache.calcite.plan.RelTraitSet;
 import org.apache.calcite.rel.RelCollation;
@@ -33,9 +32,8 @@ public static HiveSortExchange create(RelNode input,
       RelDistribution distribution, RelCollation collation) {
     RelOptCluster cluster = input.getCluster();
     distribution = RelDistributionTraitDef.INSTANCE.canonize(distribution);
-    RelTraitSet traitSet =
-        input.getTraitSet().replace(Convention.NONE).replace(distribution);
     collation = RelCollationTraitDef.INSTANCE.canonize(collation);
+    RelTraitSet traitSet = RelTraitSet.createEmpty().plus(distribution).plus(collation);
     return new HiveSortExchange(cluster, traitSet, input, distribution, collation);
   }
 

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -5487,7 +5487,7 @@ public Function get_function(String dbName, String funcName)
         ex = e;
         throw newMetaException(e);
       } finally {
-        endFunction("get_database", func != null, ex);
+        endFunction("get_function", func != null, ex);
       }
 
       return func;

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcStripeMetadata.java
Patch:
@@ -69,7 +69,7 @@ public OrcStripeMetadata(OrcBatchKey stripeKey, MetadataReader mr, StripeInforma
     estimatedMemUsage = SIZE_ESTIMATOR.estimate(this, SIZE_ESTIMATORS);
   }
 
-  public OrcStripeMetadata(long id) {
+  private OrcStripeMetadata(long id) {
     stripeKey = new OrcBatchKey(id, 0, 0);
     encodings = new ArrayList<>();
     streams = new ArrayList<>();

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java
Patch:
@@ -147,7 +147,6 @@ public void readFields(DataInput in) throws IOException {
     if (hasFileId) {
       fileId = in.readLong();
     }
-    LOG.error("TODO# Got file ID " + fileId + " for " + getPath());
   }
 
   FileMetaInfo getFileMetaInfo(){

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/Reader.java
Patch:
@@ -23,9 +23,9 @@
 import java.util.List;
 
 import org.apache.hadoop.hive.llap.Consumer;
-import org.apache.hadoop.hive.llap.io.api.EncodedColumnBatch;
 import org.apache.hadoop.hive.llap.io.api.cache.LowLevelCache;
 import org.apache.hadoop.hive.llap.io.api.orc.OrcBatchKey;
+import org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.OrcEncodedColumnBatch;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 
@@ -318,7 +318,7 @@ RecordReader rows(long offset, long length,
   MetadataReader metadata() throws IOException;
 
   EncodedReader encodedReader(long fileId, LowLevelCache lowLevelCache,
-      Consumer<EncodedColumnBatch<OrcBatchKey>> consumer) throws IOException;
+      Consumer<OrcEncodedColumnBatch> consumer) throws IOException;
 
   List<Integer> getVersionList();
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java
Patch:
@@ -38,9 +38,9 @@
 import org.apache.hadoop.hive.common.DiskRange;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.llap.Consumer;
-import org.apache.hadoop.hive.llap.io.api.EncodedColumnBatch;
 import org.apache.hadoop.hive.llap.io.api.cache.LowLevelCache;
 import org.apache.hadoop.hive.llap.io.api.orc.OrcBatchKey;
+import org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.OrcEncodedColumnBatch;
 import org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterVersion;
 import org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer;
 import org.apache.hadoop.hive.ql.io.orc.OrcProto.Type;
@@ -716,7 +716,7 @@ public MetadataReader metadata() throws IOException {
 
   @Override
   public EncodedReader encodedReader(long fileId, LowLevelCache lowLevelCache,
-      Consumer<EncodedColumnBatch<OrcBatchKey>> consumer) throws IOException {
+      Consumer<OrcEncodedColumnBatch> consumer) throws IOException {
     boolean useZeroCopy = (conf != null) && (HiveConf.getBoolVar(conf, HIVE_ORC_ZEROCOPY));
     return new EncodedReaderImpl(fileSystem, path, fileId, useZeroCopy, types,
         codec, bufferSize, rowIndexStride, lowLevelCache, consumer);

File: llap-client/src/java/org/apache/hadoop/hive/llap/configuration/LlapConfiguration.java
Patch:
@@ -45,8 +45,9 @@ public LlapConfiguration() {
   public static final String LLAP_DAEMON_SHUFFLE_DIR_WATCHER_ENABLED = LLAP_DAEMON_PREFIX + "shuffle.dir-watcher.enabled";
   public static final boolean LLAP_DAEMON_SHUFFLE_DIR_WATCHER_ENABLED_DEFAULT = false;
 
+  // This needs to be kept below the task timeout interval, but otherwise as high as possible to avoid unnecessary traffic.
   public static final String LLAP_DAEMON_LIVENESS_HEARTBEAT_INTERVAL_MS = LLAP_DAEMON_PREFIX + "liveness.heartbeat.interval-ms";
-  public static final long LLAP_DAEMON_LIVENESS_HEARTBEAT_INTERVAL_MS_DEFAULT = 5000l;
+  public static final long LLAP_DAEMON_LIVENESS_HEARTBEAT_INTERVAL_MS_DEFAULT = 10000l;
 
 
   // Section for configs used in AM and executors

File: llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
Patch:
@@ -458,7 +458,7 @@ void nodePinged(String hostname, int port) {
         }
       } else {
         if (System.currentTimeMillis() > nodeNotFoundLogTime.get() + 5000l) {
-          LOG.warn("Recevied ping from unknown node: " + hostname + ":" + port +
+          LOG.warn("Received ping from unknown node: " + hostname + ":" + port +
               ". Could be caused by pre-emption by the AM," +
               " or a mismatched hostname. Enable debug logging for mismatched host names");
           nodeNotFoundLogTime.set(System.currentTimeMillis());

File: cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
Patch:
@@ -102,6 +102,9 @@ public CliDriver() {
     SessionState ss = SessionState.get();
     conf = (ss != null) ? ss.getConf() : new Configuration();
     Log LOG = LogFactory.getLog("CliDriver");
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("CliDriver inited with classpath " + System.getProperty("java.class.path"));
+    }
     console = new LogHelper(LOG);
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java
Patch:
@@ -147,6 +147,7 @@ public void readFields(DataInput in) throws IOException {
     if (hasFileId) {
       fileId = in.readLong();
     }
+    LOG.error("TODO# Got file ID " + fileId + " for " + getPath());
   }
 
   FileMetaInfo getFileMetaInfo(){

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
Patch:
@@ -385,10 +385,10 @@ static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object min,
         result = evaluatePredicateBloomFilter(predicate, predObj, bloomFilter, hasNull);
       }
       // in case failed conversion, return the default YES_NO_NULL truth value
-    } catch (NumberFormatException nfe) {
+    } catch (Exception e) {
       if (LOG.isWarnEnabled()) {
-        LOG.warn("NumberFormatException when type matching predicate object" +
-            " and statistics object. Exception: " + ExceptionUtils.getStackTrace(nfe));
+        LOG.warn("Exception when evaluating predicate. Skipping ORC PPD." +
+            " Exception: " + ExceptionUtils.getStackTrace(e));
       }
       result = hasNull ? TruthValue.YES_NO_NULL : TruthValue.YES_NO;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSize.java
Patch:
@@ -98,6 +98,8 @@ public List<Double> averageColumnSizes(HiveJoin rel) {
     return ImmutableNullableList.copyOf(sizes);
   }
 
+  // TODO: remove when averageTypeValueSize method RelMdSize
+  //       supports all types
   public Double averageTypeValueSize(RelDataType type) {
     switch (type.getSqlTypeName()) {
     case BOOLEAN:
@@ -108,6 +110,7 @@ public Double averageTypeValueSize(RelDataType type) {
     case INTEGER:
     case FLOAT:
     case REAL:
+    case DECIMAL:
     case DATE:
     case TIME:
       return 4d;

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java
Patch:
@@ -18,7 +18,6 @@
 import parquet.io.api.GroupConverter;
 import parquet.io.api.RecordMaterializer;
 import parquet.schema.GroupType;
-import parquet.schema.MessageType;
 import parquet.schema.MessageTypeParser;
 
 import java.util.Map;
@@ -34,7 +33,7 @@ public class DataWritableRecordConverter extends RecordMaterializer<ArrayWritabl
 
   public DataWritableRecordConverter(final GroupType requestedSchema, final Map<String, String> metadata) {
     this.root = new HiveStructConverter(requestedSchema,
-      MessageTypeParser.parseMessageType(metadata.get(DataWritableReadSupport.HIVE_SCHEMA_KEY)), metadata);
+      MessageTypeParser.parseMessageType(metadata.get(DataWritableReadSupport.HIVE_TABLE_AS_PARQUET_SCHEMA)), metadata);
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
Patch:
@@ -246,7 +246,7 @@ protected ParquetInputSplit getSplit(
       final ReadContext readContext = new DataWritableReadSupport()
           .init(jobConf, fileMetaData.getKeyValueMetaData(), fileMetaData.getSchema());
       schemaSize = MessageTypeParser.parseMessageType(readContext.getReadSupportMetadata()
-          .get(DataWritableReadSupport.HIVE_SCHEMA_KEY)).getFieldCount();
+          .get(DataWritableReadSupport.HIVE_TABLE_AS_PARQUET_SCHEMA)).getFieldCount();
       final List<BlockMetaData> splitGroup = new ArrayList<BlockMetaData>();
       final long splitStart = ((FileSplit) oldSplit).getStart();
       final long splitLength = ((FileSplit) oldSplit).getLength();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java
Patch:
@@ -34,7 +34,6 @@
 import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;
 import org.apache.hadoop.hive.ql.parse.HiveParser;
 import org.apache.hadoop.hive.ql.parse.ParseDriver;
-import org.apache.hive.common.util.DateTimeMath;
 
 class ASTBuilder {
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPDTIMinus.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.util.DateTimeMath;
 import org.apache.hadoop.hive.serde2.io.DateWritable;
 import org.apache.hadoop.hive.serde2.io.HiveIntervalDayTimeWritable;
 import org.apache.hadoop.hive.serde2.io.HiveIntervalYearMonthWritable;
@@ -44,7 +45,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hive.common.util.DateTimeMath;
 
 @Description(name = "-", value = "a _FUNC_ b - Returns the difference a-b")
 public class GenericUDFOPDTIMinus extends GenericUDFBaseDTI {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPDTIPlus.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.util.DateTimeMath;
 import org.apache.hadoop.hive.serde2.io.DateWritable;
 import org.apache.hadoop.hive.serde2.io.HiveIntervalDayTimeWritable;
 import org.apache.hadoop.hive.serde2.io.HiveIntervalYearMonthWritable;
@@ -44,7 +45,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hive.common.util.DateTimeMath;
 
 @Description(name = "+", value = "a _FUNC_ b - Returns a+b")
 public class GenericUDFOPDTIPlus extends GenericUDFBaseDTI {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToIntervalDayTime.java
Patch:
@@ -21,6 +21,8 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToIntervalDayTime;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
@@ -39,6 +41,7 @@
 */
 @Description(name = "interval_day_time",
   value = "CAST(<string> AS INTERVAL DAY TO SECOND) - Returns the day-time interval represented by the string")
+@VectorizedExpressions({CastStringToIntervalDayTime.class})
 public class GenericUDFToIntervalDayTime extends GenericUDF {
 
   private transient PrimitiveObjectInspector argumentOI;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToIntervalYearMonth.java
Patch:
@@ -21,6 +21,8 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToIntervalYearMonth;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
@@ -39,6 +41,7 @@
 */
 @Description(name = "interval_year_month",
   value = "CAST(<string> AS INTERVAL YEAR TO MONTH) - Returns the year-month interval represented by the string")
+@VectorizedExpressions({CastStringToIntervalYearMonth.class})
 public class GenericUDFToIntervalYearMonth extends GenericUDF {
 
   private transient PrimitiveObjectInspector argumentOI;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizationContext.java
Patch:
@@ -151,7 +151,7 @@ public void testVectorExpressionDescriptor() {
     VectorUDFUnixTimeStampLong v1 = new VectorUDFUnixTimeStampLong();
     VectorExpressionDescriptor.Builder builder1 = new VectorExpressionDescriptor.Builder();
     VectorExpressionDescriptor.Descriptor d1 = builder1.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
-        .setNumArguments(1).setArgumentTypes(VectorExpressionDescriptor.ArgumentType.INT_DATETIME_FAMILY)
+        .setNumArguments(1).setArgumentTypes(VectorExpressionDescriptor.ArgumentType.INT_DATETIME_INTERVAL_FAMILY)
         .setInputExpressionTypes(VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
     assertTrue(d1.matches(v1.getDescriptor()));
 

File: ql/src/test/org/apache/hadoop/hive/ql/util/TestDateTimeMath.java
Patch:
@@ -15,14 +15,15 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hive.common.util;
+package org.apache.hadoop.hive.ql.util;
 
 import java.sql.Date;
 import java.sql.Timestamp;
 import java.util.TimeZone;
 
 import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
+import org.apache.hadoop.hive.ql.util.DateTimeMath;
 import org.junit.*;
 
 import static org.junit.Assert.*;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -2041,7 +2041,9 @@ public static enum ConfVars {
     SPARK_RPC_SASL_MECHANISM("hive.spark.client.rpc.sasl.mechanisms", "DIGEST-MD5",
       "Name of the SASL mechanism to use for authentication."),
     NWAYJOINREORDER("hive.reorder.nway.joins", true,
-      "Runs reordering of tables within single n-way join (i.e.: picks streamtable)");
+      "Runs reordering of tables within single n-way join (i.e.: picks streamtable)"),
+    HIVE_LOG_N_RECORDS("hive.log.every.n.records", 0L, new RangeValidator(0L, null),
+      "If value is greater than 0 logs in fixed intervals of size n rather than exponentially.");
 
     public final String varname;
     private final String defaultExpr;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCount.java
Patch:
@@ -88,8 +88,9 @@ public static class GenericUDAFCountEvaluator extends GenericUDAFEvaluator {
     public ObjectInspector init(Mode m, ObjectInspector[] parameters)
     throws HiveException {
       super.init(m, parameters);
-      partialCountAggOI =
-        PrimitiveObjectInspectorFactory.writableLongObjectInspector;
+      if (mode == Mode.PARTIAL2 || mode == Mode.FINAL) {
+        partialCountAggOI = (LongObjectInspector)parameters[0];
+      }
       result = new LongWritable(0);
       return PrimitiveObjectInspectorFactory.writableLongObjectInspector;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEWAHBitmap.java
Patch:
@@ -91,12 +91,12 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters)
             .getStandardListObjectInspector(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
       } else if (m == Mode.PARTIAL2 || m == Mode.FINAL) {
         internalMergeOI = (StandardListObjectInspector) parameters[0];
-        inputOI = PrimitiveObjectInspectorFactory.writableByteObjectInspector;
+        inputOI = (PrimitiveObjectInspector)internalMergeOI.getListElementObjectInspector();
         loi = (StandardListObjectInspector) ObjectInspectorFactory
             .getStandardListObjectInspector(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
         return loi;
       } else { // Mode.COMPLETE, ie. no map-side aggregation, requires ordering
-        inputOI = PrimitiveObjectInspectorFactory.writableByteObjectInspector;
+        inputOI = (PrimitiveObjectInspector)parameters[0];
         loi = (StandardListObjectInspector) ObjectInspectorFactory
             .getStandardListObjectInspector(PrimitiveObjectInspectorFactory.writableLongObjectInspector);
         return loi;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCase.java
Patch:
@@ -101,7 +101,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
       PrimitiveObjectInspector caseOI = (PrimitiveObjectInspector) caseOIResolver.get();
       if (PrimitiveObjectInspectorUtils.comparePrimitiveObjects(
             caseOIResolver.convertIfNecessary(exprValue, argumentOIs[0]), caseOI,
-            caseOIResolver.convertIfNecessary(caseKey, argumentOIs[i]), caseOI)) {
+            caseOIResolver.convertIfNecessary(caseKey, argumentOIs[i], false), caseOI)) {
         Object caseValue = arguments[i + 1].get();
         return returnOIResolver.convertIfNecessary(caseValue, argumentOIs[i + 1]);
       }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIn.java
Patch:
@@ -181,7 +181,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
             conversionHelper.convertIfNecessary(
                 arguments[0].get(), argumentOIs[0]), compareOI,
             conversionHelper.convertIfNecessary(
-                arguments[i].get(), argumentOIs[i]), compareOI) == 0) {
+                arguments[i].get(), argumentOIs[i], false), compareOI) == 0) {
           bw.set(true);
           return bw;
         }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NGramEstimator.java
Patch:
@@ -208,7 +208,7 @@ public int compare(Map.Entry<ArrayList<String>,Double> o1,
    *
    * @param other A serialized n-gram object created by the serialize() method
    */
-  public void merge(List<Text> other) throws HiveException {
+  public void merge(List other) throws HiveException {
     if(other == null) {
       return;
     }
@@ -240,8 +240,7 @@ public void merge(List<Text> other) throws HiveException {
     for(int i = 3; i < other.size(); i++) {
       ArrayList<String> key = new ArrayList<String>();
       for(int j = 0; j < n; j++) {
-        Text word = other.get(i+j);
-        key.add(word.toString());
+        key.add(other.get(i+j).toString());
       }
       i += n;
       double val = Double.parseDouble( other.get(i).toString() );

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CuckooSetBytes.java
Patch:
@@ -206,7 +206,7 @@ private void rehash() {
     // Save original values
     if (prev1 == null) {
       prev1 = t1;
-      prev1 = t2;
+      prev2 = t2;
     }
     t1 = new byte[n][];
     t2 = new byte[n][];

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CuckooSetDouble.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import java.util.Arrays;
-import java.util.Random;
 
 /**
  * A high-performance set implementation used to support fast set membership testing,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CuckooSetLong.java
Patch:
@@ -244,7 +244,7 @@ private void rehash() {
     // Save original values
     if (prev1 == null) {
       prev1 = t1;
-      prev1 = t2;
+      prev2 = t2;
     }
     t1 = new long[n];
     t2 = new long[n];

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -337,6 +337,8 @@ public final class FunctionRegistry {
 
     system.registerGenericUDF(serdeConstants.DATE_TYPE_NAME, GenericUDFToDate.class);
     system.registerGenericUDF(serdeConstants.TIMESTAMP_TYPE_NAME, GenericUDFTimestamp.class);
+    system.registerGenericUDF(serdeConstants.INTERVAL_YEAR_MONTH_TYPE_NAME, GenericUDFToIntervalYearMonth.class);
+    system.registerGenericUDF(serdeConstants.INTERVAL_DAY_TIME_TYPE_NAME, GenericUDFToIntervalDayTime.class);
     system.registerGenericUDF(serdeConstants.BINARY_TYPE_NAME, GenericUDFToBinary.class);
     system.registerGenericUDF(serdeConstants.DECIMAL_TYPE_NAME, GenericUDFToDecimal.class);
     system.registerGenericUDF(serdeConstants.VARCHAR_TYPE_NAME, GenericUDFToVarchar.class);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java
Patch:
@@ -82,6 +82,8 @@ public static MapJoinKey read(Output output, MapJoinObjectSerDeContext context,
     SUPPORTED_PRIMITIVES.add(PrimitiveCategory.STRING);
     SUPPORTED_PRIMITIVES.add(PrimitiveCategory.DATE);
     SUPPORTED_PRIMITIVES.add(PrimitiveCategory.TIMESTAMP);
+    SUPPORTED_PRIMITIVES.add(PrimitiveCategory.INTERVAL_YEAR_MONTH);
+    SUPPORTED_PRIMITIVES.add(PrimitiveCategory.INTERVAL_DAY_TIME);
     SUPPORTED_PRIMITIVES.add(PrimitiveCategory.BINARY);
     SUPPORTED_PRIMITIVES.add(PrimitiveCategory.VARCHAR);
     SUPPORTED_PRIMITIVES.add(PrimitiveCategory.CHAR);

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPMinus.java
Patch:
@@ -46,7 +46,6 @@ protected GenericUDFBaseNumeric instantiateNumericUDF() {
 
   @Override
   protected GenericUDF instantiateDTIUDF() {
-    // TODO: implement date-time/interval version of UDF
-    return new GenericUDFOPNumericMinus();
+    return new GenericUDFOPDTIMinus();
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPPlus.java
Patch:
@@ -52,7 +52,6 @@ protected GenericUDFBaseNumeric instantiateNumericUDF() {
 
   @Override
   protected GenericUDF instantiateDTIUDF() {
-    // TODO: implement date-time/interval version of UDF
-    return new GenericUDFOPNumericPlus();
+    return new GenericUDFOPDTIPlus();
   }
 }

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/PrimitiveObjectInspector.java
Patch:
@@ -31,7 +31,8 @@ public interface PrimitiveObjectInspector extends ObjectInspector {
    */
   public static enum PrimitiveCategory {
     VOID, BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, STRING,
-    DATE, TIMESTAMP, BINARY, DECIMAL, VARCHAR, CHAR, UNKNOWN
+    DATE, TIMESTAMP, BINARY, DECIMAL, VARCHAR, CHAR, INTERVAL_YEAR_MONTH, INTERVAL_DAY_TIME,
+    UNKNOWN
   };
 
   public PrimitiveTypeInfo getTypeInfo();

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveSchemaConverter.java
Patch:
@@ -98,7 +98,8 @@ private static Type convertType(final String name, final TypeInfo typeInfo,
         return Types.optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY).length(bytes).as(OriginalType.DECIMAL).
             scale(scale).precision(prec).named(name);
       } else if (typeInfo.equals(TypeInfoFactory.dateTypeInfo)) {
-        return new PrimitiveType(repetition, PrimitiveTypeName.INT32, name, OriginalType.DATE);
+        return Types.primitive(PrimitiveTypeName.INT32, repetition).as(OriginalType.DATE).named
+            (name);
       } else if (typeInfo.equals(TypeInfoFactory.unknownTypeInfo)) {
         throw new UnsupportedOperationException("Unknown type not implemented");
       } else {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java
Patch:
@@ -1190,7 +1190,7 @@ public boolean isEqual(Object v1, Object v2) {
           (PrimitiveObjectInspector) expressionDef.getOI());
       String s2 = PrimitiveObjectInspectorUtils.getString(v2,
           (PrimitiveObjectInspector) expressionDef.getOI());
-      return (s1 == null && s2 == null) || s1.equals(s2);
+      return (s1 == null && s2 == null) || (s1 != null && s1.equals(s2));
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultUDAFEvaluatorResolver.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.lang.reflect.Method;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.List;
 
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
@@ -92,12 +91,12 @@ public Class<? extends UDAFEvaluator> getEvaluatorClass(
         if (found == -1) {
           found = i;
         } else {
-          throw new AmbiguousMethodException(udafClass, null, null); 
+          throw new AmbiguousMethodException(udafClass, argClasses, mList);
         }
       }
     }
     assert (found != -1);
-    
+
     return cList.get(found);
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java
Patch:
@@ -149,7 +149,7 @@ public static String makeQueryId() {
 
     return userid
         + "_"
-        + String.format("%1$4d%2$02d%3$02d%4$02d%5$02d%5$02d", gc
+        + String.format("%1$4d%2$02d%3$02d%4$02d%5$02d%6$02d", gc
         .get(Calendar.YEAR), gc.get(Calendar.MONTH) + 1, gc
         .get(Calendar.DAY_OF_MONTH), gc.get(Calendar.HOUR_OF_DAY), gc
         .get(Calendar.MINUTE), gc.get(Calendar.SECOND))
@@ -435,7 +435,7 @@ public String getJSONKeyValue(Object key, Object value) {
     return "\"" + key + "\":" + getJSONValue(value) + ",";
   }
 
-  @SuppressWarnings("unchecked")
+  @SuppressWarnings("rawtypes")
   private String getJSONList(List list) {
     if (list == null) {
       return "null";
@@ -451,7 +451,7 @@ private String getJSONList(List list) {
     return sb.toString();
   }
 
-  @SuppressWarnings("unchecked")
+  @SuppressWarnings("rawtypes")
   public String getJSONMap(Map map) {
     if (map == null) {
       return "null";

File: ql/src/java/org/apache/hadoop/hive/ql/plan/SparkWork.java
Patch:
@@ -236,13 +236,15 @@ public void remove(BaseWork work) {
     List<BaseWork> parents = getParents(work);
 
     for (BaseWork w: children) {
+      edgeProperties.remove(new ImmutablePair<BaseWork, BaseWork>(work, w));
       invertedWorkGraph.get(w).remove(work);
       if (invertedWorkGraph.get(w).size() == 0) {
         roots.add(w);
       }
     }
 
     for (BaseWork w: parents) {
+      edgeProperties.remove(new ImmutablePair<BaseWork, BaseWork>(w, work));
       workGraph.get(w).remove(work);
       if (workGraph.get(w).size() == 0) {
         leaves.add(w);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java
Patch:
@@ -46,7 +46,7 @@ public class HiveSparkClientFactory {
   protected static final transient Log LOG = LogFactory.getLog(HiveSparkClientFactory.class);
 
   private static final String SPARK_DEFAULT_CONF_FILE = "spark-defaults.conf";
-  private static final String SPARK_DEFAULT_MASTER = "local";
+  private static final String SPARK_DEFAULT_MASTER = "yarn-cluster";
   private static final String SPARK_DEFAULT_APP_NAME = "Hive on Spark";
   private static final String SPARK_DEFAULT_SERIALIZER = "org.apache.spark.serializer.KryoSerializer";
   private static final String SPARK_DEFAULT_REFERENCE_TRACKING = "false";

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -2896,7 +2896,7 @@ public static String generatePath(Path baseURI, String filename) {
 
   public static String now() {
     Calendar cal = Calendar.getInstance();
-    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss");
+    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
     return sdf.format(cal.getTime());
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java
Patch:
@@ -64,8 +64,6 @@
 import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.shims.HadoopShims;
-import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.hive.shims.Utils;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.security.UserGroupInformation;
@@ -123,7 +121,7 @@ public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext driverC
 
   public static String now() {
     Calendar cal = Calendar.getInstance();
-    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-mm-dd hh:mm:ss");
+    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
     return sdf.format(cal.getTime());
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java
Patch:
@@ -213,7 +213,7 @@ static ASTNode literal(RexLiteral literal, boolean useTypeQualInLiteral) {
     case TIMESTAMP: {
       val = literal.getValue();
       type = HiveParser.TOK_TIMESTAMPLITERAL;
-      DateFormat df = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss");
+      DateFormat df = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
       val = df.format(((Calendar) val).getTime());
       val = "'" + val + "'";
     }

File: llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java
Patch:
@@ -212,7 +212,7 @@ static class ContainerRunnerCallable extends CallableWithNdc<ContainerExecutionR
 
     @Override
     protected ContainerExecutionResult callInternal() throws Exception {
-      Stopwatch sw = new Stopwatch().start();s
+      Stopwatch sw = new Stopwatch().start();
       tezChild =
           new TezChild(conf, request.getAmHost(), request.getAmPort(),
               request.getContainerIdString(),

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LlapCacheableBuffer.java
Patch:
@@ -108,7 +108,8 @@ int decRef() {
 
   @Override
   public String toString() {
-    return "0x" + Integer.toHexString(System.identityHashCode(this));
+    int refCount = this.refCount.get();
+    return "0x" + Integer.toHexString(System.identityHashCode(this)) + "(" + refCount + ")";
   }
 
   /**

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataProducer.java
Patch:
@@ -414,6 +414,7 @@ private void ensureMetadataReader() throws IOException {
 
     @Override
     public void returnData(StreamBuffer data) {
+      if (data.decRef() != 0) return;
       if (DebugUtils.isTraceLockingEnabled()) {
         for (LlapMemoryBuffer buf : data.cacheBuffers) {
           LlapIoImpl.LOG.info("Unlocking " + buf + " at the end of processing");

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
Patch:
@@ -274,7 +274,6 @@ private static int align64(int number) {
     return ((number + 63) & ~63);
   }
 
-
   @Override
   public void releaseBuffer(LlapMemoryBuffer buffer) {
     releaseBufferInternal((LlapCacheableBuffer)buffer);

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java
Patch:
@@ -219,13 +219,14 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
     // 2. Now, read all of the ranges from cache or disk.
     DiskRangeListMutateHelper toRead = new DiskRangeListMutateHelper(listToRead.get());
     if (DebugUtils.isTraceOrcEnabled()) {
-      LOG.info("Resulting disk ranges to read: " + RecordReaderUtils.stringifyDiskRanges(toRead));
+      LOG.info("Resulting disk ranges to read: "
+          + RecordReaderUtils.stringifyDiskRanges(toRead.next));
     }
     if (cache != null) {
       cache.getFileData(fileName, toRead.next, stripeOffset);
       if (DebugUtils.isTraceOrcEnabled()) {
         LOG.info("Disk ranges after cache (base offset " + stripeOffset
-            + "): " + RecordReaderUtils.stringifyDiskRanges(toRead));
+            + "): " + RecordReaderUtils.stringifyDiskRanges(toRead.next));
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java
Patch:
@@ -565,9 +565,10 @@ public ProcCacheChunk(long cbStartOffset, long cbEndOffset, boolean isCompressed
    */
   public static DiskRangeList uncompressStream(String fileName, long baseOffset,
       DiskRangeList start, long cOffset, long endCOffset, ZeroCopyReaderShim zcr,
-      CompressionCodec codec,int bufferSize, LowLevelCache cache, StreamBuffer streamBuffer)
+      CompressionCodec codec, int bufferSize, LowLevelCache cache, StreamBuffer streamBuffer)
           throws IOException {
     streamBuffer.cacheBuffers = new ArrayList<LlapMemoryBuffer>();
+    if (cOffset == endCOffset) return null;
     List<ProcCacheChunk> toDecompress = null;
     List<ByteBuffer> toRelease = null;
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderUtils.java
Patch:
@@ -302,6 +302,7 @@ static DiskRangeList readDiskRanges(FSDataInputStream file,
   static List<DiskRange> getStreamBuffers(DiskRangeList range, long offset, long length) {
     // This assumes sorted ranges (as do many other parts of ORC code.
     ArrayList<DiskRange> buffers = new ArrayList<DiskRange>();
+    if (length == 0) return buffers;
     long streamEnd = offset + length;
     boolean inRange = false;
     while (range != null) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderUtils.java
Patch:
@@ -285,7 +285,8 @@ static DiskRangeList readDiskRanges(FSDataInputStream file,
           file.readFully(buffer, 0, buffer.length);
           directBuf.put(buffer);
         }
-        directBuf.position(0);
+        directBuf.position(startPos);
+        directBuf.limit(startPos + len);
         range = range.replaceSelfWith(new BufferChunk(directBuf, range.offset));
       } else {
         byte[] buffer = new byte[len];

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java
Patch:
@@ -234,6 +234,7 @@ private int allocateWithSplit(int arenaIx, int freeListIx,
       FreeList freeList = freeLists[freeListIx];
       int remaining = -1;
       freeList.lock.lock();
+      // TODO: write some comments for this method
       try {
         ix = allocateFromFreeListUnderLock(
             arenaIx, freeList, freeListIx, dest, ix, allocationSize);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java
Patch:
@@ -523,6 +523,9 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,
       for (ExprNodeDesc key : keys) {
         colLists = Utilities.mergeUniqElems(colLists, key.getCols());
       }
+      for (ExprNodeDesc key : conf.getPartitionCols()) {
+        colLists = Utilities.mergeUniqElems(colLists, key.getCols());
+      }
 
       assert op.getNumChild() == 1;
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -472,6 +472,9 @@ public static enum ConfVars {
         "      schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures\n" +
         "      proper metastore schema migration. (Default)\n" +
         "False: Warn if the version information stored in metastore doesn't match with one from in Hive jars."),
+    METASTORE_SCHEMA_VERIFICATION_RECORD_VERSION("hive.metastore.schema.verification.record.version", true,
+      "When true the current MS version is recorded in the VERSION table. If this is disabled and verification is\n" +
+      " enabled the MS will be unusable."),
     METASTORE_AUTO_START_MECHANISM_MODE("datanucleus.autoStartMechanismMode", "checked",
         "throw exception if metadata tables are incorrect"),
     METASTORE_TRANSACTION_ISOLATION("datanucleus.transactionIsolation", "read-committed",

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java
Patch:
@@ -730,7 +730,7 @@ private static DiskRangeList addOneCompressionBuffer(BufferChunk current, ZeroCo
       }
       return next;
     }
-    if (current.end < cbEndOffset && current.next == null) {
+    if (current.end < cbEndOffset && !current.hasContiguousNext()) {
       return null; // This is impossible to read from this chunk.
     }
 
@@ -782,11 +782,10 @@ private static DiskRangeList addOneCompressionBuffer(BufferChunk current, ZeroCo
       if (DebugUtils.isTraceOrcEnabled()) {
         LOG.info("Removing " + tmp + " from ranges");
       }
-      next = next.next;
+      next = next.hasContiguousNext() ? next.next : null;
       tmp.removeSelf();
     }
     return null; // This is impossible to read from this chunk.
-    // TODO: dbl check this is valid; we just did a bunch of changes to the list.
   }
 
   /**

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/orc/stream/readers/CharacterStreamReader.java
Patch:
@@ -64,7 +64,7 @@ public void seek(PositionProvider index) throws IOException {
       if (isFileCompressed) {
         index.getNext();
       }
-      present.seek(index);
+      reader.present.seek(index);
     }
 
     if (isDictionaryEncoding) {

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/orc/stream/readers/StringStreamReader.java
Patch:
@@ -53,7 +53,7 @@ public void seek(PositionProvider index) throws IOException {
       if (isFileCompressed) {
         index.getNext();
       }
-      present.seek(index);
+      reader.present.seek(index);
     }
 
     if (isDictionaryEncoding) {

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataProducer.java
Patch:
@@ -126,7 +126,7 @@ public Void call() throws IOException {
         metadata = getOrReadFileMetadata();
         int bufferSize = metadata.getCompressionBufferSize();
         int minAllocSize = HiveConf.getIntVar(conf, HiveConf.ConfVars.LLAP_ORC_CACHE_MIN_ALLOC);
-        if (bufferSize != minAllocSize) {
+        if (bufferSize < minAllocSize) {
           throw new IOException("ORC compression buffer size (" + bufferSize + ") is smaller than" +
               " LLAP low-level cache minimum allocation size (" + minAllocSize + "). Decrease the" +
               " value for " + HiveConf.ConfVars.LLAP_ORC_CACHE_MIN_ALLOC.toString());

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
Patch:
@@ -262,7 +262,7 @@ public long getNext() {
 
   public abstract static class TreeReader {
     protected final int columnId;
-    protected BitFieldReader present = null;
+    public BitFieldReader present = null;
     protected boolean valuePresent = false;
 
     public TreeReader(int columnId) throws IOException {

File: llap-client/src/java/org/apache/hadoop/hive/llap/io/api/cache/LowLevelCache.java
Patch:
@@ -18,19 +18,20 @@
 
 package org.apache.hadoop.hive.llap.io.api.cache;
 
-import java.util.LinkedList;
 import java.util.List;
 
 import org.apache.hadoop.hive.common.DiskRange;
+import org.apache.hadoop.hive.common.DiskRangeList;
 
 
 public interface LowLevelCache {
   /**
    * Gets file data for particular offsets. Null entries mean no data.
    * @param file File name; MUST be interned.
    * @param base base offset for the ranges (stripe offset in case of ORC).
+   * @return 
    */
-  void getFileData(String fileName, LinkedList<DiskRange> ranges, long base);
+  DiskRangeList getFileData(String fileName, DiskRangeList range, long baseOffset);
 
   /**
    * Puts file data into cache.

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/TraitsUtil.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.calcite.plan.RelOptCluster;
 import org.apache.calcite.plan.RelTraitSet;
 import org.apache.calcite.rel.RelCollation;
-import org.apache.calcite.rel.RelCollationImpl;
+import org.apache.calcite.rel.RelCollations;
 import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveRelNode;
 
 public class TraitsUtil {
@@ -32,6 +32,6 @@ public static RelTraitSet getSortTraitSet(RelOptCluster cluster, RelTraitSet tra
   }
 
   public static RelTraitSet getDefaultTraitSet(RelOptCluster cluster) {
-    return cluster.traitSetOf(HiveRelNode.CONVENTION, RelCollationImpl.EMPTY);
+    return cluster.traitSetOf(HiveRelNode.CONVENTION, RelCollations.EMPTY);
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
Patch:
@@ -50,6 +50,7 @@
 import org.apache.calcite.rel.InvalidRelException;
 import org.apache.calcite.rel.RelCollation;
 import org.apache.calcite.rel.RelCollationImpl;
+import org.apache.calcite.rel.RelCollations;
 import org.apache.calcite.rel.RelFieldCollation;
 import org.apache.calcite.rel.RelNode;
 import org.apache.calcite.rel.core.Aggregate;
@@ -756,7 +757,7 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu
       List<RelMetadataProvider> list = Lists.newArrayList();
       list.add(HiveDefaultRelMetadataProvider.INSTANCE);
       RelTraitSet desiredTraits = cluster
-          .traitSetOf(HiveRelNode.CONVENTION, RelCollationImpl.EMPTY);
+          .traitSetOf(HiveRelNode.CONVENTION, RelCollations.EMPTY);
 
       HepProgram hepPgm = null;
       HepProgramBuilder hepPgmBldr = new HepProgramBuilder().addMatchOrder(HepMatchOrder.BOTTOM_UP)
@@ -2084,7 +2085,7 @@ private RelNode genLimitLogicalPlan(QB qb, RelNode srcRel) throws SemanticExcept
       if (limit != null) {
         RexNode fetch = cluster.getRexBuilder().makeExactLiteral(BigDecimal.valueOf(limit));
         RelTraitSet traitSet = cluster.traitSetOf(HiveRelNode.CONVENTION);
-        RelCollation canonizedCollation = traitSet.canonize(RelCollationImpl.EMPTY);
+        RelCollation canonizedCollation = traitSet.canonize(RelCollations.EMPTY);
         sortRel = new HiveSort(cluster, traitSet, srcRel, canonizedCollation, null, fetch);
 
         RowResolver outputRR = new RowResolver();

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java
Patch:
@@ -129,9 +129,10 @@ private static void printMetaData(List<String> files, Configuration conf,
         OrcProto.StripeFooter footer = rows.readStripeFooter(stripe);
         long sectionStart = stripeStart;
         for(OrcProto.Stream section: footer.getStreamsList()) {
+          String kind = section.hasKind() ? section.getKind().name() : "UNKNOWN";
           System.out.println("    Stream: column " + section.getColumn() +
-            " section " + section.getKind() + " start: " + sectionStart +
-            " length " + section.getLength());
+              " section " + kind + " start: " + sectionStart +
+              " length " + section.getLength());
           sectionStart += section.getLength();
         }
         for (int i = 0; i < footer.getColumnsCount(); ++i) {

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
Patch:
@@ -172,7 +172,6 @@ public long[] putFileData(
     try {
       for (int i = 0; i < ranges.length; ++i) {
         LlapCacheableBuffer buffer = (LlapCacheableBuffer)buffers[i];
-        assert !buffer.isLocked(); // TODO: is this always true? does put happen before reuse?
         buffer.incRef();
         long offset = ranges[i].offset + baseOffset;
         buffer.declaredLength = ranges[i].getLength();
@@ -414,7 +413,6 @@ public LlapMemoryBuffer createUnallocated() {
 
   @Override
   public void notifyReused(LlapMemoryBuffer buffer) {
-    int newVal = ((LlapCacheableBuffer)buffer).incRef();
-    assert newVal > 1;
+    ((LlapCacheableBuffer)buffer).incRef();
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java
Patch:
@@ -295,6 +295,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,
         consumer.consumeData(ecb);
       }
     }
+    // TODO: WE NEED TO DECREF ALL THE CACHE BUFFERS ONCE
   }
 
   /**

File: llap-client/src/java/org/apache/hadoop/hive/llap/io/api/cache/LlapMemoryBuffer.java
Patch:
@@ -28,5 +28,7 @@ protected void initialize(ByteBuffer byteBuffer, int offset, int length) {
     this.byteBuffer.position(offset);
     this.byteBuffer.limit(offset + length);
   }
+  /** Note - position/limit of this should NOT be modified after it's in cache.
+      We could add a wrapper to enforce that, but for now it's shared and should be duplicated. */
   public ByteBuffer byteBuffer;
 }
\ No newline at end of file

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1896,7 +1896,7 @@ public static enum ConfVars {
         "and use it to run queries."),
 
     // Vectorization enabled
-    HIVE_VECTORIZATION_ENABLED("hive.vectorized.execution.enabled", true,
+    HIVE_VECTORIZATION_ENABLED("hive.vectorized.execution.enabled", false,
         "This flag should be set to true to enable vectorized mode of query execution.\n" +
         "The default value is false."),
     HIVE_VECTORIZATION_REDUCE_ENABLED("hive.vectorized.execution.reduce.enabled", true,
@@ -2003,7 +2003,7 @@ public static enum ConfVars {
         "hive.tez.exec.inplace.progress",
         true,
         "Updates tez job execution progress in-place in the terminal."),
-    LLAP_IO_ENABLED("hive.llap.io.enabled", true, ""),
+    LLAP_IO_ENABLED("hive.llap.io.enabled", false, ""),
     LLAP_LOW_LEVEL_CACHE("hive.llap.io.use.lowlevel.cache", true, ""),
     LLAP_ORC_CACHE_MIN_ALLOC("hive.llap.io.cache.orc.alloc.min", 128 * 1024, ""),
     LLAP_ORC_CACHE_MAX_ALLOC("hive.llap.io.cache.orc.alloc.max", 16 * 1024 * 1024, ""),

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java
Patch:
@@ -89,7 +89,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen
     if (arguments[1].getCategory() != ObjectInspector.Category.PRIMITIVE) {
       throw new UDFArgumentTypeException(1,
         "Only primitive type arguments are accepted but "
-        + arguments[2].getTypeName() + " is passed. as second arguments");
+        + arguments[1].getTypeName() + " is passed. as second arguments");
     }
 
     inputType1 = ((PrimitiveObjectInspector) arguments[0]).getPrimitiveCategory();

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java
Patch:
@@ -89,7 +89,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen
     if (arguments[1].getCategory() != ObjectInspector.Category.PRIMITIVE) {
       throw new UDFArgumentTypeException(1,
           "Only primitive type arguments are accepted but "
-              + arguments[2].getTypeName() + " is passed. as second arguments");
+              + arguments[1].getTypeName() + " is passed. as second arguments");
     }
 
     inputType1 = ((PrimitiveObjectInspector) arguments[0]).getPrimitiveCategory();

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFEncode.java
Patch:
@@ -35,13 +35,10 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
 import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.Text;
 
 @Description(name = "encode",
 value = "_FUNC_(str, str) - Encode the first argument using the second argument character set",

File: common/src/java/org/apache/hadoop/hive/common/DiskRange.java
Patch:
@@ -70,8 +70,8 @@ public ByteBuffer getData() {
     throw new UnsupportedOperationException();
   }
 
-  public void shiftBy(long baseOffset) {
-    offset -= baseOffset;
-    end -= baseOffset;
+  public void shiftBy(long offsetDelta) {
+    offset += offsetDelta;
+    end += offsetDelta;
   }
 }
\ No newline at end of file

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1896,7 +1896,7 @@ public static enum ConfVars {
         "and use it to run queries."),
 
     // Vectorization enabled
-    HIVE_VECTORIZATION_ENABLED("hive.vectorized.execution.enabled", false,
+    HIVE_VECTORIZATION_ENABLED("hive.vectorized.execution.enabled", true,
         "This flag should be set to true to enable vectorized mode of query execution.\n" +
         "The default value is false."),
     HIVE_VECTORIZATION_REDUCE_ENABLED("hive.vectorized.execution.reduce.enabled", true,
@@ -2009,7 +2009,7 @@ public static enum ConfVars {
         new TimeValidator(TimeUnit.SECONDS),
         "remote spark client JobHandle future timeout value in seconds."),
 
-    LLAP_IO_ENABLED("hive.llap.io.enabled", false, ""),
+    LLAP_IO_ENABLED("hive.llap.io.enabled", true, ""),
     LLAP_LOW_LEVEL_CACHE("hive.llap.io.use.lowlevel.cache", true, ""),
     LLAP_ORC_CACHE_MIN_ALLOC("hive.llap.io.cache.orc.alloc.min", 128 * 1024, ""),
     LLAP_ORC_CACHE_MAX_ALLOC("hive.llap.io.cache.orc.alloc.max", 16 * 1024 * 1024, ""),

File: llap-client/src/java/org/apache/hadoop/hive/llap/io/api/cache/LowLevelCache.java
Patch:
@@ -28,16 +28,17 @@ public interface LowLevelCache {
   /**
    * Gets file data for particular offsets. Null entries mean no data.
    * @param file File name; MUST be interned.
+   * @param base base offset for the ranges (stripe offset in case of ORC).
    */
-  void getFileData(String fileName, LinkedList<DiskRange> ranges);
+  void getFileData(String fileName, LinkedList<DiskRange> ranges, long base);
 
   /**
    * Puts file data into cache.
    * @param file File name; MUST be interned.
    * @return null if all data was put; bitmask indicating which chunks were not put otherwise;
    *         the replacement chunks from cache are updated directly in the array.
    */
-  long[] putFileData(String file, DiskRange[] ranges, LlapMemoryBuffer[] chunks);
+  long[] putFileData(String file, DiskRange[] ranges, LlapMemoryBuffer[] chunks, long base);
 
   /**
    * Releases the buffer returned by getFileData or allocateMultiple.

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/ColumnVectorProducer.java
Patch:
@@ -106,7 +106,6 @@ public void consumeData(EncodedColumnBatch<BatchKey> data) {
         return;
       }
 
-      int colsRemaining = -1;
       synchronized (targetBatch) {
         // Check if we are stopped and the batch was already cleaned.
         localIsStopped = (targetBatch.columnData == null);
@@ -128,7 +127,7 @@ public void consumeData(EncodedColumnBatch<BatchKey> data) {
         returnProcessed(data.columnData);
         return;
       }
-      if (0 == colsRemaining) {
+      if (0 == targetBatch.colsRemaining) {
         ColumnVectorProducer.this.decodeBatch(targetBatch, downstreamConsumer);
         // Batch has been decoded; unlock the buffers in cache
         returnProcessed(targetBatch.columnData);

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcStripeMetadata.java
Patch:
@@ -40,9 +40,9 @@ public OrcStripeMetadata(RecordReader reader, boolean[] includes) throws IOExcep
 
   public boolean hasAllIndexes(boolean[] includes) {
     for (int i = 0; i < includes.length; ++i) {
-      if (includes[i] && rowIndexes[i] == null) return true;
+      if (includes[i] && rowIndexes[i] == null) return false;
     }
-    return false;
+    return true;
   }
 
   public void loadMissingIndexes(RecordReader reader, boolean[] includes) throws IOException {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReader.java
Patch:
@@ -86,7 +86,8 @@ public interface RecordReader {
 
   void prepareEncodedColumnRead() throws IOException;
 
-  // TODO: maybe all of this should be moved to LLAP-specific class
+  // TODO: maybe all of this should be moved to LLAP-specific class.
+  //       See also comment in RecordReaderImpl; class doesn't even have to be RecordReader 
   /**
    *  TODO: assumes the reader is for one stripe, otherwise the signature makes no sense.
    *        Also has no columns passed, because that is in ctor.

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/VectorReader.java
Patch:
@@ -30,8 +30,8 @@
 import org.apache.hadoop.mapred.InputSplit;
 
 /** This used to be the main LLAP interface (next and close). However, since inputformat
- * is used instead at present time, this class is just an extra layer. It could be merged
- * into RecordReader of LlapInputFormat. */
+ * is used instead at present time, this class is just an extra layer.
+ * TODO: It could be merged into RecordReader of LlapInputFormat. */
 public class VectorReader implements Consumer<ColumnVectorBatch> {
   private final InputSplit split;
   private final List<Integer> columnIds;

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/EncodedDataProducer.java
Patch:
@@ -26,6 +26,6 @@
 import org.apache.hadoop.mapred.InputSplit;
 
 public interface EncodedDataProducer<BatchKey> {
-  EncodedDataReader<BatchKey> getReader(InputSplit split, List<Integer> columnIds,
+  EncodedDataReader<BatchKey> createReader(InputSplit split, List<Integer> columnIds,
       SearchArgument sarg, String[] columnNames, Consumer<EncodedColumnBatch<BatchKey>> consumer);
 }

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReader.java
Patch:
@@ -100,7 +100,7 @@ public interface RecordReader {
   void readEncodedColumns(int stripeIx, boolean[] stripeIncludes, boolean[][] colRgs,
       LowLevelCache cache, Consumer<EncodedColumnBatch<OrcBatchKey>> consumer) throws IOException;
 
-  RowIndex[] getCurrentRowIndexEntries(boolean[] included) throws IOException;
+  void getCurrentRowIndexEntries(boolean[] included, RowIndex[] indexes) throws IOException;
 
   List<ColumnEncoding> getCurrentColumnEncodings() throws IOException;
 

File: llap-client/src/java/org/apache/hadoop/hive/llap/io/api/LlapIoProxy.java
Patch:
@@ -62,6 +62,7 @@ private static LlapIo createIoImpl(Configuration conf) throws IOException {
       @SuppressWarnings("unchecked")
       Class<? extends LlapIo> clazz = (Class<? extends LlapIo>)Class.forName(IMPL_CLASS);
       Constructor<? extends LlapIo> ctor = clazz.getDeclaredConstructor(Configuration.class);
+      ctor.setAccessible(true);
       return ctor.newInstance(conf);
     } catch (Exception e) {
       throw new RuntimeException("Failed to create impl class", e);

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
Patch:
@@ -167,9 +167,10 @@ public long[] putFileData(String fileName, DiskRange[] ranges, LlapMemoryBuffer[
     try {
       for (int i = 0; i < ranges.length; ++i) {
         LlapCacheableBuffer buffer = (LlapCacheableBuffer)buffers[i];
+        assert !buffer.isLocked(); // TODO: is this always true? does put happen before reuse?
+        buffer.incRef();
         long offset = ranges[i].offset;
         buffer.declaredLength = ranges[i].getLength();
-        assert buffer.isLocked();
         while (true) { // Overwhelmingly executes once, or maybe twice (replacing stale value).
           LlapCacheableBuffer oldVal = subCache.cache.putIfAbsent(offset, buffer);
           if (oldVal == null) {

File: llap-client/src/java/org/apache/hadoop/hive/llap/io/api/cache/LowLevelCache.java
Patch:
@@ -53,4 +53,5 @@ public interface LowLevelCache {
 
   LlapMemoryBuffer createUnallocated();
 
+  void notifyReused(LlapMemoryBuffer buffer);
 }

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/Cache.java
Patch:
@@ -18,10 +18,10 @@
 
 package org.apache.hadoop.hive.llap.cache;
 
-import org.apache.hadoop.hive.llap.io.api.EncodedColumn.StreamBuffer;
+import org.apache.hadoop.hive.llap.io.api.EncodedColumnBatch.StreamBuffer;
 
 /** Dummy interface for now, might be different. */
 public interface Cache<CacheKey> {
-  public StreamBuffer cacheOrGet(CacheKey key, StreamBuffer value);
-  public StreamBuffer get(CacheKey key);
+  public StreamBuffer[] cacheOrGet(CacheKey key, StreamBuffer[] value);
+  public StreamBuffer[] get(CacheKey key);
 }

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/NoopCache.java
Patch:
@@ -18,16 +18,16 @@
 
 package org.apache.hadoop.hive.llap.cache;
 
-import org.apache.hadoop.hive.llap.io.api.EncodedColumn.StreamBuffer;
+import org.apache.hadoop.hive.llap.io.api.EncodedColumnBatch.StreamBuffer;
 
 public class NoopCache<CacheKey> implements Cache<CacheKey> {
   @Override
-  public StreamBuffer cacheOrGet(CacheKey key, StreamBuffer value) {
+  public StreamBuffer[] cacheOrGet(CacheKey key, StreamBuffer[] value) {
     return value;
   }
 
   @Override
-  public StreamBuffer get(CacheKey key) {
+  public StreamBuffer[] get(CacheKey key) {
     return null;  // TODO: ensure real implementation increases refcount
   }
 }

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java
Patch:
@@ -22,6 +22,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.llap.Consumer;
+import org.apache.hadoop.hive.llap.io.api.EncodedColumnBatch;
 import org.apache.hadoop.hive.llap.io.api.impl.ColumnVectorBatch;
 import org.apache.hadoop.hive.llap.io.api.orc.OrcBatchKey;
 import org.apache.hadoop.hive.llap.io.encoded.EncodedDataProducer;
@@ -42,8 +43,8 @@ protected EncodedDataProducer<OrcBatchKey> getEncodedDataProducer() {
   }
 
   @Override
-  protected void decodeBatch(OrcBatchKey batchKey,
-      EncodedColumnBatch batch, Consumer<ColumnVectorBatch> downstreamConsumer) {
+  protected void decodeBatch(EncodedColumnBatch<OrcBatchKey> batch,
+      Consumer<ColumnVectorBatch> downstreamConsumer) {
     throw new UnsupportedOperationException("not implemented");
     // TODO:  HERE decode EncodedColumn-s into ColumnVector-s
     //        sarg columns first, apply sarg, then decode others if needed; can cols skip values?

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/EncodedDataProducer.java
Patch:
@@ -21,12 +21,11 @@
 import java.util.List;
 
 import org.apache.hadoop.hive.llap.Consumer;
-import org.apache.hadoop.hive.llap.io.api.EncodedColumn;
-import org.apache.hadoop.hive.llap.io.api.orc.OrcBatchKey;
+import org.apache.hadoop.hive.llap.io.api.EncodedColumnBatch;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
 import org.apache.hadoop.mapred.InputSplit;
 
 public interface EncodedDataProducer<BatchKey> {
   EncodedDataReader<BatchKey> getReader(InputSplit split, List<Integer> columnIds,
-      SearchArgument sarg, String[] columnNames, Consumer<EncodedColumn<BatchKey>> consumer);
+      SearchArgument sarg, String[] columnNames, Consumer<EncodedColumnBatch<BatchKey>> consumer);
 }

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/EncodedDataReader.java
Patch:
@@ -21,7 +21,7 @@
 import java.util.concurrent.Callable;
 
 import org.apache.hadoop.hive.llap.ConsumerFeedback;
-import org.apache.hadoop.hive.llap.io.api.EncodedColumn.StreamBuffer;
+import org.apache.hadoop.hive.llap.io.api.EncodedColumnBatch.StreamBuffer;
 
 /**
  * Interface for encoded data readers to implement.

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/orc/LLAPRecordReaderImpl.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.llap.Consumer;
-import org.apache.hadoop.hive.llap.io.api.EncodedColumn;
+import org.apache.hadoop.hive.llap.io.api.EncodedColumnBatch;
 import org.apache.hadoop.hive.llap.io.api.cache.LowLevelCache;
 import org.apache.hadoop.hive.llap.io.api.orc.OrcBatchKey;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java
Patch:
@@ -148,7 +148,7 @@ private static void printMetaData(List<String> files, Configuration conf,
           System.out.println(buf);
         }
         if (rowIndexCols != null) {
-          RowIndex[] indices = rows.readRowIndex(stripeIx);
+          RowIndex[] indices = rows.readRowIndex(stripeIx, null);
           for (int col : rowIndexCols) {
             StringBuilder buf = new StringBuilder();
             buf.append("    Row group index column ").append(col).append(":");

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java
Patch:
@@ -672,7 +672,7 @@ public void testStripeLevelStats() throws Exception {
     assertEquals(5000, ((StringColumnStatistics)ss3.getColumnStatistics()[2]).getSum());
 
     RecordReaderImpl recordReader = (RecordReaderImpl) reader.rows();
-    OrcProto.RowIndex[] index = recordReader.readRowIndex(0);
+    OrcProto.RowIndex[] index = recordReader.readRowIndex(0, null);
     assertEquals(3, index.length);
     List<OrcProto.RowIndexEntry> items = index[1].getEntryList();
     assertEquals(1, items.size());
@@ -682,7 +682,7 @@ public void testStripeLevelStats() throws Exception {
     assertEquals(0, items.get(0).getPositions(2));
     assertEquals(1, 
                  items.get(0).getStatistics().getIntStatistics().getMinimum());
-    index = recordReader.readRowIndex(1);
+    index = recordReader.readRowIndex(1, null);
     assertEquals(3, index.length);
     items = index[1].getEntryList();
     assertEquals(2, 

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java
Patch:
@@ -53,7 +53,7 @@ protected static HiveGroupConverter getConverterFromDescription(GroupType type,
     OriginalType annotation = type.getOriginalType();
     if (annotation == OriginalType.LIST) {
       return HiveCollectionConverter.forList(type, parent, index);
-    } else if (annotation == OriginalType.MAP) {
+    } else if (annotation == OriginalType.MAP || annotation == OriginalType.MAP_KEY_VALUE) {
       return HiveCollectionConverter.forMap(type, parent, index);
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java
Patch:
@@ -233,6 +233,7 @@ private JobConf cloneJobConf(BaseWork work) throws Exception {
       throw new IllegalArgumentException(msg, e);
     }
     if (work instanceof MapWork) {
+      cloned.setBoolean("mapred.task.is.map", true);
       List<Path> inputPaths = Utilities.getInputPaths(cloned, (MapWork) work,
           scratchDir, context, false);
       Utilities.setInputPaths(cloned, inputPaths);
@@ -250,6 +251,7 @@ private JobConf cloneJobConf(BaseWork work) throws Exception {
       // remember the JobConf cloned for each MapWork, so we won't clone for it again
       workToJobConf.put(work, cloned);
     } else if (work instanceof ReduceWork) {
+      cloned.setBoolean("mapred.task.is.map", false);
       Utilities.setReduceWork(cloned, (ReduceWork) work, scratchDir, false);
       Utilities.createTmpDirs(cloned, (ReduceWork) work);
       cloned.set(Utilities.MAPRED_REDUCER_CLASS, ExecReducer.class.getName());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ObjectCacheFactory.java
Patch:
@@ -35,7 +35,8 @@ private ObjectCacheFactory() {
    * Returns the appropriate cache
    */
   public static ObjectCache getCache(Configuration conf) {
-    if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez")) {
+    if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez") &&
+	HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_MODE).equals("container")) {
       return new org.apache.hadoop.hive.ql.exec.tez.ObjectCache();
     } else {
       return new org.apache.hadoop.hive.ql.exec.mr.ObjectCache();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -365,7 +365,7 @@ private static BaseWork getBaseWork(Configuration conf, String name) {
       LOG.info("PLAN PATH = " + path);
       assert path != null;
       if (!gWorkMap.containsKey(path)
-        || HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("spark")) {
+        || !HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("mr")) {
         Path localPath;
         if (conf.getBoolean("mapreduce.task.uberized", false) && name.equals(REDUCE_PLAN_NAME)) {
           localPath = new Path(name);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java
Patch:
@@ -68,7 +68,7 @@ public class ReduceRecordSource implements RecordSource {
 
   private boolean abort = false;
 
-  private static Deserializer inputKeyDeserializer;
+  private Deserializer inputKeyDeserializer;
 
   // Input value serde needs to be an array to support different SerDe
   // for different tags

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java
Patch:
@@ -190,7 +190,7 @@ public long[] putFileData(String fileName, DiskRange[] ranges, LlapMemoryBuffer[
                   + oldVal.declaredLength + " vs " + buffer.declaredLength + " @" + offset);
             }
             // We found an old, valid block for this key in the cache.
-s            releaseBufferInternal(buffer);
+            releaseBufferInternal(buffer);
             buffers[i] = oldVal;
             if (result == null) {
               result = new long[align64(buffers.length) >>> 6];

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
Patch:
@@ -202,7 +202,7 @@ public void configure(JobConf job) {
 
   public static InputFormat<WritableComparable, Writable> wrapForLlap(
       InputFormat<WritableComparable, Writable> inputFormat, Configuration conf) {
-    if (!HiveConf.getBoolVar(conf, ConfVars.LLAP_ENABLED)) {
+    if (!HiveConf.getBoolVar(conf, ConfVars.LLAP_IO_ENABLED)) {
       return inputFormat; // LLAP not enabled, no-op.
     }
     boolean isSupported = inputFormat instanceof LlapWrappableInputFormatInterface,

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
Patch:
@@ -2490,7 +2490,8 @@ static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object min,
           return hasNull ? TruthValue.YES_NO_NULL : TruthValue.YES_NO;
         }
       case IS_NULL:
-        return hasNull ? TruthValue.YES : TruthValue.NO;
+        // min = null condition above handles the all-nulls YES case
+        return hasNull ? TruthValue.YES_NO : TruthValue.NO;
       default:
         return hasNull ? TruthValue.YES_NO_NULL : TruthValue.YES_NO;
       }

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java
Patch:
@@ -750,7 +750,7 @@ public void testIsNullWithNullInStats() throws Exception {
     PredicateLeaf pred = TestSearchArgumentImpl.createPredicateLeaf
         (PredicateLeaf.Operator.IS_NULL, PredicateLeaf.Type.STRING,
             "x", null, null);
-    assertEquals(TruthValue.YES,
+    assertEquals(TruthValue.YES_NO,
         RecordReaderImpl.evaluatePredicate(createStringStats("c", "d", true), pred));
     assertEquals(TruthValue.NO,
         RecordReaderImpl.evaluatePredicate(createStringStats("c", "d", false), pred));

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java
Patch:
@@ -31,9 +31,10 @@
  *
  * A Parquet InputFormat for Hive (with the deprecated package mapred)
  *
+ * NOTE: With HIVE-9235 we removed "implements VectorizedParquetInputFormat" since all data types
+ *       are not currently supported.  Removing the interface turns off vectorization.
  */
-public class MapredParquetInputFormat extends FileInputFormat<Void, ArrayWritable>
-    implements VectorizedInputFormatInterface {
+public class MapredParquetInputFormat extends FileInputFormat<Void, ArrayWritable> {
 
   private static final Log LOG = LogFactory.getLog(MapredParquetInputFormat.class);
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
Patch:
@@ -129,7 +129,7 @@ public boolean fetch(List res) throws IOException, CommandNeedRetryException {
       rowsRet = work.getLimit() >= 0 ? Math.min(work.getLimit() - totalRows, maxRows) : maxRows;
     }
     try {
-      if (rowsRet <= 0) {
+      if (rowsRet <= 0 || work.getLimit() == totalRows) {
         fetch.clearFetchContext();
         return false;
       }

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java
Patch:
@@ -24,8 +24,6 @@
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hive.llap.io.api.VectorReader;
-import org.apache.hadoop.hive.llap.io.api.VectorReader.ColumnVectorBatch;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedInputFormatInterface;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java
Patch:
@@ -36,7 +36,6 @@
 import org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy;
 import org.apache.hadoop.hive.llap.cache.NoopCache;
 import org.apache.hadoop.hive.llap.io.api.LlapIo;
-import org.apache.hadoop.hive.llap.io.api.VectorReader;
 import org.apache.hadoop.hive.llap.io.api.orc.OrcCacheKey;
 import org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer;
 import org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataProducer;
@@ -72,7 +71,7 @@ private LlapIoImpl(Configuration conf) throws IOException {
   }
 
   VectorReader getReader(InputSplit split, List<Integer> columnIds, SearchArgument sarg) {
-    return new VectorReaderImpl(split, columnIds, sarg, cvp);
+    return new VectorReader(split, columnIds, sarg, cvp);
   }
 
   @Override

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/ColumnVectorProducer.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hadoop.hive.llap.ConsumerFeedback;
 import org.apache.hadoop.hive.llap.io.api.EncodedColumn;
 import org.apache.hadoop.hive.llap.io.api.EncodedColumn.ColumnBuffer;
-import org.apache.hadoop.hive.llap.io.api.VectorReader.ColumnVectorBatch;
+import org.apache.hadoop.hive.llap.io.api.impl.ColumnVectorBatch;
 import org.apache.hadoop.hive.llap.io.encoded.EncodedDataProducer;
 import org.apache.hadoop.hive.llap.io.encoded.EncodedDataReader;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java
Patch:
@@ -22,7 +22,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.llap.Consumer;
-import org.apache.hadoop.hive.llap.io.api.VectorReader.ColumnVectorBatch;
+import org.apache.hadoop.hive.llap.io.api.impl.ColumnVectorBatch;
 import org.apache.hadoop.hive.llap.io.api.orc.OrcBatchKey;
 import org.apache.hadoop.hive.llap.io.encoded.EncodedDataProducer;
 import org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataProducer;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFLower.java
Patch:
@@ -51,7 +51,7 @@ public class GenericUDFLower extends GenericUDF {
 
   @Override
   public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
-    if (arguments.length < 0) {
+    if (arguments.length != 1) {
       throw new UDFArgumentLengthException(
           "LOWER requires 1 argument, got " + arguments.length);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUpper.java
Patch:
@@ -33,7 +33,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;
 
 /**
  * UDFUpper.
@@ -52,7 +51,7 @@ public class GenericUDFUpper extends GenericUDF {
 
   @Override
   public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
-    if (arguments.length < 0) {
+    if (arguments.length != 1) {
       throw new UDFArgumentLengthException(
           "UPPER requires 1 argument, got " + arguments.length);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDayOfMonth.java
Patch:
@@ -42,7 +42,7 @@
     extended = "date is a string in the format of 'yyyy-MM-dd HH:mm:ss' or "
     + "'yyyy-MM-dd'.\n"
     + "Example:\n "
-    + "  > SELECT _FUNC_('2009-30-07', 1) FROM src LIMIT 1;\n" + "  30")
+    + "  > SELECT _FUNC_('2009-07-30') FROM src LIMIT 1;\n" + "  30")
 @VectorizedExpressions({VectorUDFDayOfMonthLong.class, VectorUDFDayOfMonthString.class})
 public class UDFDayOfMonth extends UDF {
   private final SimpleDateFormat formatter = new SimpleDateFormat("yyyy-MM-dd");

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFMonth.java
Patch:
@@ -40,7 +40,7 @@
 @Description(name = "month",
     value = "_FUNC_(date) - Returns the month of date",
     extended = "Example:\n"
-    + "  > SELECT _FUNC_('2009-30-07') FROM src LIMIT 1;\n" + "  7")
+    + "  > SELECT _FUNC_('2009-07-30') FROM src LIMIT 1;\n" + "  7")
 @VectorizedExpressions({VectorUDFMonthLong.class, VectorUDFMonthString.class})
 public class UDFMonth extends UDF {
   private final SimpleDateFormat formatter = new SimpleDateFormat("yyyy-MM-dd");

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFYear.java
Patch:
@@ -42,7 +42,7 @@
     extended = "date is a string in the format of 'yyyy-MM-dd HH:mm:ss' or "
     + "'yyyy-MM-dd'.\n"
     + "Example:\n "
-    + "  > SELECT _FUNC_('2009-30-07', 1) FROM src LIMIT 1;\n" + "  2009")
+    + "  > SELECT _FUNC_('2009-07-30') FROM src LIMIT 1;\n" + "  2009")
 @VectorizedExpressions({VectorUDFYearLong.class, VectorUDFYearString.class})
 public class UDFYear extends UDF {
   private final SimpleDateFormat formatter = new SimpleDateFormat("yyyy-MM-dd");

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java
Patch:
@@ -61,8 +61,8 @@
         + " 'yyyy-MM-dd'. num_days is a number. The time part of start_date is "
         + "ignored.\n"
         + "Example:\n "
-        + "  > SELECT _FUNC_('2009-30-07', 1) FROM src LIMIT 1;\n"
-        + "  '2009-31-07'")
+        + "  > SELECT _FUNC_('2009-07-30', 1) FROM src LIMIT 1;\n"
+        + "  '2009-07-31'")
 @VectorizedExpressions({VectorUDFDateAddColScalar.class, VectorUDFDateAddScalarCol.class, VectorUDFDateAddColCol.class})
 public class GenericUDFDateAdd extends GenericUDF {
   private transient SimpleDateFormat formatter = new SimpleDateFormat("yyyy-MM-dd");

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateDiff.java
Patch:
@@ -57,7 +57,7 @@
         + "'yyyy-MM-dd HH:mm:ss' or 'yyyy-MM-dd'. The time parts are ignored."
         + "If date1 is earlier than date2, the result is negative.\n"
         + "Example:\n "
-        + "  > SELECT _FUNC_('2009-30-07', '2009-31-07') FROM src LIMIT 1;\n"
+        + "  > SELECT _FUNC_('2009-07-30', '2009-07-31') FROM src LIMIT 1;\n"
         + "  1")
 @VectorizedExpressions({VectorUDFDateDiffColScalar.class, VectorUDFDateDiffColCol.class, VectorUDFDateDiffScalarCol.class})
 public class GenericUDFDateDiff extends GenericUDF {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java
Patch:
@@ -61,8 +61,8 @@
         + " 'yyyy-MM-dd'. num_days is a number. The time part of start_date is "
         + "ignored.\n"
         + "Example:\n "
-        + "  > SELECT _FUNC_('2009-30-07', 1) FROM src LIMIT 1;\n"
-        + "  '2009-29-07'")
+        + "  > SELECT _FUNC_('2009-07-30', 1) FROM src LIMIT 1;\n"
+        + "  '2009-07-29'")
 @VectorizedExpressions({VectorUDFDateSubColScalar.class, VectorUDFDateSubScalarCol.class, VectorUDFDateSubColCol.class})
 public class GenericUDFDateSub extends GenericUDF {
   private transient SimpleDateFormat formatter = new SimpleDateFormat("yyyy-MM-dd");

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/BucketingSortingInferenceOptimizer.java
Patch:
@@ -103,7 +103,7 @@ private void inferBucketingSorting(List<ExecDriver> mapRedTasks) throws Semantic
       Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();
       opRules.put(new RuleRegExp("R1", SelectOperator.getOperatorName() + "%"),
           BucketingSortingOpProcFactory.getSelProc());
-      // Matches only GroupByOpeartors which are reducers, rather than map group by operators,
+      // Matches only GroupByOperators which are reducers, rather than map group by operators,
       // or multi group by optimization specific operators
       opRules.put(new RuleExactMatch("R2", GroupByOperator.getOperatorName() + "%"),
           BucketingSortingOpProcFactory.getGroupByProc());

File: serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarSerDe.java
Patch:
@@ -113,7 +113,7 @@ public void initialize(Configuration conf, Properties tbl) throws SerDeException
         cachedObjectInspector, notSkipIDs, serdeParams.getNullSequence());
 
     super.initialize(size);
-    LOG.info("ColumnarSerDe initialized with: columnNames="
+    LOG.debug("ColumnarSerDe initialized with: columnNames="
         + serdeParams.getColumnNames() + " columnTypes="
         + serdeParams.getColumnTypes() + " separator="
         + Arrays.asList(serdeParams.getSeparators()) + " nullstring="

File: beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java
Patch:
@@ -178,7 +178,8 @@ public void verifySchemaVersion() throws HiveMetaException {
         getConnectionToMetastore(false));
     // verify that the new version is added to schema
     if (!MetaStoreSchemaInfo.getHiveSchemaVersion().equalsIgnoreCase(newSchemaVersion)) {
-      throw new HiveMetaException("Found unexpected schema version " + newSchemaVersion);
+      throw new HiveMetaException("Expected schema version " + MetaStoreSchemaInfo.getHiveSchemaVersion() +
+        ", found version " + newSchemaVersion);
     }
   }
 

File: llap-client/src/java/org/apache/hadoop/hive/llap/io/api/cache/LlapMemoryBuffer.java
Patch:
@@ -24,13 +24,14 @@ public abstract class LlapMemoryBuffer {
   protected LlapMemoryBuffer(ByteBuffer byteBuffer, int offset, int length) {
     initialize(byteBuffer, offset, length);
   }
-  public void initialize(ByteBuffer byteBuffer, int offset, int length) {
+  protected LlapMemoryBuffer() {
+  }
+  protected void initialize(ByteBuffer byteBuffer, int offset, int length) {
     this.byteBuffer = byteBuffer;
     this.offset = offset;
     this.length = length;
   }
   public ByteBuffer byteBuffer;
   public int offset;
   public int length;
-
 }
\ No newline at end of file

File: llap-client/src/java/org/apache/hadoop/hive/llap/io/api/cache/LowLevelCache.java
Patch:
@@ -23,11 +23,13 @@ public interface LowLevelCache {
 
   /**
    * Gets file data for particular offsets. Null entries mean no data.
+   * @param file File name; MUST be interned.
    */
   LlapMemoryBuffer[] getFileData(String fileName, long[] offsets);
 
   /**
    * Puts file data into cache.
+   * @param file File name; MUST be interned.
    * @return null if all data was put; bitmask indicating which chunks were not put otherwise;
    *         the replacement chunks from cache are updated directly in the array.
    */

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCachePolicy.java
Patch:
@@ -22,5 +22,4 @@ public interface LowLevelCachePolicy {
   void cache(LlapCacheableBuffer buffer);
   void notifyLock(LlapCacheableBuffer buffer);
   void notifyUnlock(LlapCacheableBuffer buffer);
-  boolean reserveMemory(long memoryToReserve, boolean oneEviction);
 }

File: llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCachePolicyBase.java
Patch:
@@ -20,7 +20,7 @@
 
 import java.util.concurrent.atomic.AtomicLong;
 
-public abstract class LowLevelCachePolicyBase implements LowLevelCachePolicy {
+public abstract class LowLevelCachePolicyBase implements LowLevelCachePolicy, MemoryManager {
   private final AtomicLong usedMemory;
   private final long maxSize;
   private EvictionListener evictionListener;

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.llap.cache.Cache;
-import org.apache.hadoop.hive.llap.cache.LowLevelBuddyCache;
+import org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl;
 import org.apache.hadoop.hive.llap.cache.NoopCache;
 import org.apache.hadoop.hive.llap.io.api.LlapIo;
 import org.apache.hadoop.hive.llap.io.api.VectorReader;
@@ -56,7 +56,7 @@ private LlapIoImpl(Configuration conf) throws IOException {
     boolean useLowLevelCache = HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_LOW_LEVEL_CACHE);
     // High-level cache not supported yet.
     Cache<OrcCacheKey> cache = useLowLevelCache ? null : new NoopCache<OrcCacheKey>();
-    LowLevelBuddyCache orcCache = useLowLevelCache ? new LowLevelBuddyCache(conf) : null;
+    LowLevelCacheImpl orcCache = useLowLevelCache ? new LowLevelCacheImpl(conf) : null;
     this.edp = new OrcEncodedDataProducer(orcCache, cache, conf);
     this.cvp = new OrcColumnVectorProducer(edp, conf);
   }

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -223,7 +223,7 @@ public void normalizeNames(File path) throws Exception {
 
   public QTestUtil(String outDir, String logDir, String initScript, String cleanupScript) throws
       Exception {
-    this(outDir, logDir, MiniClusterType.spark, null, "0.20", initScript, cleanupScript);
+    this(outDir, logDir, MiniClusterType.none, null, "0.20", initScript, cleanupScript);
   }
 
   public String getOutputDirectory() {
@@ -1545,7 +1545,7 @@ public static QTestUtil[] queryListRunnerSetup(File[] qfiles, String resDir,
   {
     QTestUtil[] qt = new QTestUtil[qfiles.length];
     for (int i = 0; i < qfiles.length; i++) {
-      qt[i] = new QTestUtil(resDir, logDir, MiniClusterType.spark, null, "0.20", "", "");
+      qt[i] = new QTestUtil(resDir, logDir, MiniClusterType.none, null, "0.20", "", "");
       qt[i].addFile(qfiles[i]);
       qt[i].clearTestSideEffects();
     }

File: llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcMetadataCache.java
Patch:
@@ -16,17 +16,16 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.llap.cache;
+package org.apache.hadoop.hive.llap.io.encoded;
 
 import java.util.List;
 import java.util.concurrent.ConcurrentHashMap;
 
 import org.apache.hadoop.hive.ql.io.orc.StripeInformation;
 import org.apache.hadoop.hive.ql.io.orc.OrcProto.Type;
 
-// TODO: write unit tests if this class becomes less primitive.
 /** ORC-specific metadata cache. */
-public class MetadataCache {
+public class OrcMetadataCache {
   private final ConcurrentHashMap<String, List<StripeInformation>> metadataCacheStripes =
       new ConcurrentHashMap<String, List<StripeInformation>>();
   private final ConcurrentHashMap<String, List<Type>> metadataCacheTypes =

File: llap-server/src/java/org/apache/hadoop/hive/llap/old/BufferInProgress.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.llap.loader;
+package org.apache.hadoop.hive.llap.old;
 
-import org.apache.hadoop.hive.llap.cache.BufferPool.WeakBuffer;
-import org.apache.hadoop.hive.llap.loader.ChunkPool.Chunk;
+import org.apache.hadoop.hive.llap.old.BufferPool.WeakBuffer;
+import org.apache.hadoop.hive.llap.old.ChunkPool.Chunk;
 
 /**
  * Helper struct that is used by loaders (e.g. OrcLoader) and chunk writer to write chunks.

File: llap-server/src/java/org/apache/hadoop/hive/llap/old/CachePolicy.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.llap.cache;
+package org.apache.hadoop.hive.llap.old;
 
-import org.apache.hadoop.hive.llap.cache.BufferPool.WeakBuffer;
+import org.apache.hadoop.hive.llap.old.BufferPool.WeakBuffer;
 
 public interface CachePolicy {
   public static final WeakBuffer CANNOT_EVICT = BufferPool.allocateFake();

File: llap-server/src/java/org/apache/hadoop/hive/llap/old/FifoCachePolicy.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.llap.cache;
+package org.apache.hadoop.hive.llap.old;
 
 import java.util.Iterator;
 import java.util.LinkedHashSet;
 
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 
-import org.apache.hadoop.hive.llap.cache.BufferPool.WeakBuffer;
+import org.apache.hadoop.hive.llap.old.BufferPool.WeakBuffer;
 
 public class FifoCachePolicy implements CachePolicy {
   private final Lock lock = new ReentrantLock();

File: llap-server/src/test/org/apache/hadoop/hive/llap/old/TestLrfuCachePolicy.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hadoop.hive.llap.cache;
+package org.apache.hadoop.hive.llap.old;
 
 import java.util.ArrayList;
 import java.util.Collections;
@@ -24,7 +24,7 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.llap.cache.BufferPool.WeakBuffer;
+import org.apache.hadoop.hive.llap.old.BufferPool.WeakBuffer;
 import org.junit.Assume;
 import org.junit.Test;
 import static org.junit.Assert.*;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
Patch:
@@ -203,8 +203,7 @@ static InputFormat getInputFormatFromCache(
     Class<? extends InputFormat> inputFormatClass, JobConf conf) throws IOException {
     if (Configurable.class.isAssignableFrom(inputFormatClass) ||
         JobConfigurable.class.isAssignableFrom(inputFormatClass)) {
-      return HiveInputFormat.wrapForLlap((InputFormat<WritableComparable, Writable>) ReflectionUtils
-          .newInstance(inputFormatClass, conf), conf);
+      return ReflectionUtils.newInstance(inputFormatClass, conf);
     }
     // TODO: why is this copy-pasted from HiveInputFormat?
     InputFormat format = inputFormats.get(inputFormatClass.getName());
@@ -217,7 +216,7 @@ static InputFormat getInputFormatFromCache(
             + inputFormatClass.getName() + " as specified in mapredWork!", e);
       }
     }
-    return HiveInputFormat.wrapForLlap(format, conf);
+    return format;
   }
 
   private StructObjectInspector getPartitionKeyOI(TableDesc tableDesc) throws Exception {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/IntegerReader.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.io.IOException;
 
-import org.apache.hadoop.hive.llap.chunk.ChunkWriter;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 
 /**
@@ -63,6 +62,4 @@ interface IntegerReader {
    */
    void nextVector(LongColumnVector previous, long previousLen)
       throws IOException;
-
-   int nextChunk(ChunkWriter writer, BitFieldReader present, long rowsLeft) throws IOException;
 }

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
Patch:
@@ -53,7 +53,6 @@
 import org.apache.hadoop.hive.ql.io.AcidUtils;
 import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
 import org.apache.hadoop.hive.ql.io.InputFormatChecker;
-import org.apache.hadoop.hive.ql.io.LlapWrappableInputFormatInterface;
 import org.apache.hadoop.hive.ql.io.RecordIdentifier;
 import org.apache.hadoop.hive.ql.io.StatsProvidingRecordReader;
 import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;
@@ -107,8 +106,7 @@
 public class OrcInputFormat  implements InputFormat<NullWritable, OrcStruct>,
   InputFormatChecker, VectorizedInputFormatInterface,
     AcidInputFormat<NullWritable, OrcStruct>, 
-    CombineHiveInputFormat.AvoidSplitCombination,
-    LlapWrappableInputFormatInterface {
+    CombineHiveInputFormat.AvoidSplitCombination {
 
   private static final Log LOG = LogFactory.getLog(OrcInputFormat.class);
   static final HadoopShims SHIMS = ShimLoader.getHadoopShims();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/MapReduceCompiler.java
Patch:
@@ -280,10 +280,10 @@ protected void generateTaskTree(List<Task<? extends Serializable>> rootTasks, Pa
 
     // generate map reduce plans
     ParseContext tempParseContext = getParseContext(pCtx, rootTasks);
-
     GenMRProcContext procCtx = new GenMRProcContext(
         conf,
-        new HashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>>(),
+        // Must be deterministic order map for consistent q-test output across Java versions
+        new LinkedHashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>>(),
         tempParseContext, mvTask, rootTasks,
         new LinkedHashMap<Operator<? extends OperatorDesc>, GenMapRedCtx>(),
         inputs, outputs);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -303,7 +303,8 @@ public SemanticAnalyzer(HiveConf conf) throws SemanticException {
     opParseCtx = new LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext>();
     joinContext = new HashMap<JoinOperator, QBJoinTree>();
     smbMapJoinContext = new HashMap<SMBMapJoinOperator, QBJoinTree>();
-    topToTable = new HashMap<TableScanOperator, Table>();
+    // Must be deterministic order map for consistent q-test output across Java versions
+    topToTable = new LinkedHashMap<TableScanOperator, Table>();
     fsopToTable = new HashMap<FileSinkOperator, Table>();
     reduceSinkOperatorsAddedByEnforceBucketingSorting = new ArrayList<ReduceSinkOperator>();
     topToTableProps = new HashMap<TableScanOperator, Map<String, String>>();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TableAccessInfo.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hadoop.hive.ql.parse;
 
-import java.util.HashMap;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 
@@ -33,8 +33,9 @@ public class TableAccessInfo {
     Map<String, List<String>>> operatorToTableAccessMap;
 
   public TableAccessInfo() {
+    // Must be deterministic order map for consistent q-test output across Java versions
     operatorToTableAccessMap =
-      new HashMap<Operator<? extends OperatorDesc>, Map<String, List<String>>>();
+      new LinkedHashMap<Operator<? extends OperatorDesc>, Map<String, List<String>>>();
   }
 
   public void add(Operator<? extends OperatorDesc> op,

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java
Patch:
@@ -24,6 +24,7 @@
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
@@ -391,8 +392,9 @@ public Task<? extends Serializable> processCurrentTask(MapRedTask currTask,
     List<Task<? extends Serializable>> listTasks = new ArrayList<Task<? extends Serializable>>();
 
     // create task to aliases mapping and alias to input file mapping for resolver
+    // Must be deterministic order map for consistent q-test output across Java versions
     HashMap<Task<? extends Serializable>, Set<String>> taskToAliases =
-        new HashMap<Task<? extends Serializable>, Set<String>>();
+        new LinkedHashMap<Task<? extends Serializable>, Set<String>>();
     HashMap<String, ArrayList<String>> pathToAliases = currWork.getPathToAliases();
     Map<String, Operator<? extends OperatorDesc>> aliasToWork = currWork.getAliasToWork();
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/MapJoinResolver.java
Patch:
@@ -200,8 +200,9 @@ private void processCurrentTask(Task<? extends Serializable> currTask,
                   .getResolverCtx();
               HashMap<Task<? extends Serializable>, Set<String>> taskToAliases = context.getTaskToAliases();
               // to avoid concurrent modify the hashmap
+              // Must be deterministic order map for consistent q-test output across Java versions
               HashMap<Task<? extends Serializable>, Set<String>> newTaskToAliases =
-                  new HashMap<Task<? extends Serializable>, Set<String>>();
+                  new LinkedHashMap<Task<? extends Serializable>, Set<String>>();
               // reset the resolver
               for (Map.Entry<Task<? extends Serializable>, Set<String>> entry : taskToAliases.entrySet()) {
                 Task<? extends Serializable> task = entry.getKey();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SortMergeJoinTaskDispatcher.java
Patch:
@@ -21,6 +21,7 @@
 import java.io.UnsupportedEncodingException;
 import java.util.ArrayList;
 import java.util.HashMap;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -265,8 +266,9 @@ public Task<? extends Serializable> processCurrentTask(MapRedTask currTask,
     List<Task<? extends Serializable>> listTasks = new ArrayList<Task<? extends Serializable>>();
 
     // create task to aliases mapping and alias to input file mapping for resolver
+    // Must be deterministic order map for consistent q-test output across Java versions
     HashMap<Task<? extends Serializable>, Set<String>> taskToAliases =
-        new HashMap<Task<? extends Serializable>, Set<String>>();
+        new LinkedHashMap<Task<? extends Serializable>, Set<String>>();
     // Note that pathToAlias will behave as if the original plan was a join plan
     HashMap<String, ArrayList<String>> pathToAliases = currJoinWork.getMapWork().getPathToAliases();
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -1692,7 +1692,8 @@ private void analyzeAlterTableCompact(ASTNode ast, String tableName,
   }
 
   static HashMap<String, String> getProps(ASTNode prop) {
-    HashMap<String, String> mapProp = new HashMap<String, String>();
+    // Must be deterministic order map for consistent q-test output across Java versions
+    HashMap<String, String> mapProp = new LinkedHashMap<String, String>();
     readProps(prop, mapProp);
     return mapProp;
   }

File: ql/src/test/org/apache/hadoop/hive/ql/plan/TestConditionalResolverCommonJoin.java
Patch:
@@ -29,6 +29,7 @@
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
+import java.util.LinkedHashMap;
 import java.util.Set;
 
 public class TestConditionalResolverCommonJoin {
@@ -52,8 +53,9 @@ public void testResolvingDriverAlias() throws Exception {
     task2.setId("alias3");
 
     // joins alias1, alias2, alias3 (alias1 was not eligible for big pos)
+    // Must be deterministic order map for consistent q-test output across Java versions
     HashMap<Task<? extends Serializable>, Set<String>> taskToAliases =
-        new HashMap<Task<? extends Serializable>, Set<String>>();
+        new LinkedHashMap<Task<? extends Serializable>, Set<String>>();
     taskToAliases.put(task1, new HashSet<String>(Arrays.asList("alias2")));
     taskToAliases.put(task2, new HashSet<String>(Arrays.asList("alias3")));
 

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyVoidObjectInspector.java
Patch:
@@ -39,6 +39,6 @@ public Object copyObject(Object o) {
 
   @Override
   public Object getPrimitiveJavaObject(Object o) {
-    throw new RuntimeException("Internal error: cannot create Void object.");
+    return null;
   }
 }

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
Patch:
@@ -1000,6 +1000,9 @@ public static boolean compareTypes(ObjectInspector o1, ObjectInspector o2) {
   }
 
   public static ConstantObjectInspector getConstantObjectInspector(ObjectInspector oi, Object value) {
+    if (oi instanceof ConstantObjectInspector) {
+      return (ConstantObjectInspector) oi;  
+    }
     ObjectInspector writableOI = getStandardObjectInspector(oi, ObjectInspectorCopyOption.WRITABLE);
     Object writableValue =
       ObjectInspectorConverters.getConverter(oi, writableOI).convert(value);

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableVoidObjectInspector.java
Patch:
@@ -44,6 +44,6 @@ public Object getWritableConstantValue() {
 
   @Override
   public Object getPrimitiveJavaObject(Object o) {
-    throw new RuntimeException("Internal error: cannot create Void object.");
+    return null;
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/session/SparkSessionManagerImpl.java
Patch:
@@ -139,10 +139,11 @@ public SparkSession getSession(SparkSession existingSession, HiveConf conf,
    */
   private boolean canReuseSession(SparkSession existingSession, HiveConf conf) throws HiveException {
     try {
-      UserGroupInformation newUgi = Utils.getUGIForConf(conf);
+      UserGroupInformation newUgi = Utils.getUGI();
       String newUserName = newUgi.getShortUserName();
 
-      UserGroupInformation ugiInSession = Utils.getUGIForConf(existingSession.getConf());
+      // TODOD this we need to store the session username somewhere else as getUGIForConf never used the conf
+      UserGroupInformation ugiInSession = Utils.getUGI();
       String userNameInSession = ugiInSession.getShortUserName();
 
       return newUserName.equals(userNameInSession);

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MapBuilder.java
Patch:
@@ -18,14 +18,15 @@
 package org.apache.hadoop.hive.ql.metadata.formatting;
 
 import java.util.HashMap;
+import java.util.LinkedHashMap;
 import java.util.Map;
 
 /**
  * Helper class to build Maps consumed by the JSON formatter.  Only
  * add non-null entries to the Map.
  */
 public class MapBuilder {
-    private Map<String, Object> map = new HashMap<String, Object>();
+    private Map<String, Object> map = new LinkedHashMap<String, Object>();
 
     private MapBuilder() {}
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/SkewJoinOptimizer.java
Patch:
@@ -588,8 +588,10 @@ private ExprNodeConstantDesc createConstDesc(
 
     private Map<String, Operator<? extends OperatorDesc>> getTopOps(
       Operator<? extends OperatorDesc> op) {
+      // Must be deterministic order map for consistent q-test output across
+      // Java versions
       Map<String, Operator<? extends OperatorDesc>> topOps =
-        new HashMap<String, Operator<? extends OperatorDesc>>();
+        new LinkedHashMap<String, Operator<? extends OperatorDesc>>();
       if (op.getParentOperators() == null || op.getParentOperators().size() == 0) {
         topOps.put(((TableScanOperator)op).getConf().getAlias(), op);
       } else {

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java
Patch:
@@ -21,6 +21,7 @@
 import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.HashSet;
+import java.util.LinkedHashSet;
 import java.util.List;
 import java.util.Set;
 
@@ -54,7 +55,8 @@ public class ReadEntity extends Entity implements Serializable {
   private boolean isUpdateOrDelete = false;
 
   // For views, the entities can be nested - by default, entities are at the top level
-  private final Set<ReadEntity> parents = new HashSet<ReadEntity>();
+  // Must be deterministic order set for consistent q-test output across Java versions
+  private final Set<ReadEntity> parents = new LinkedHashSet<ReadEntity>();
 
   // The accessed columns of query
   private final List<String> accessedColumns = new ArrayList<String>();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -25,6 +25,7 @@
 import java.util.Collections;
 import java.util.EnumMap;
 import java.util.HashSet;
+import java.util.LinkedHashSet;
 import java.util.Iterator;
 import java.util.LinkedHashMap;
 import java.util.List;
@@ -778,7 +779,8 @@ public static Set<String> getFunctionNamesByLikePattern(String funcPatternStr) {
    * @return Set of synonyms for funcName
    */
   public static Set<String> getFunctionSynonyms(String funcName) {
-    Set<String> synonyms = new HashSet<String>();
+    // Must be deterministic order map for consistent q-test output across Java versions - see HIVE-9161
+    Set<String> synonyms = new LinkedHashSet<String>();
 
     FunctionInfo funcInfo;
     try {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStringToMap.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hadoop.hive.ql.udf.generic;
 
-import java.util.HashMap;
+import java.util.LinkedHashMap;
 
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
@@ -46,7 +46,8 @@
     + " second delimiter sperates key and value. If only one parameter is given, default"
     + " delimiters are used: ',' as delimiter1 and '=' as delimiter2.")
 public class GenericUDFStringToMap extends GenericUDF {
-  private final HashMap<Object, Object> ret = new HashMap<Object, Object>();
+  // Must be deterministic order map for consistent q-test output across Java versions - see HIVE-9161
+  private final LinkedHashMap<Object, Object> ret = new LinkedHashMap<Object, Object>();
   private transient Converter soi_text, soi_de1 = null, soi_de2 = null;
   final static String default_de1 = ",";
   final static String default_de2 = ":";

File: spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java
Patch:
@@ -346,6 +346,7 @@ <T extends Serializable> JobHandleImpl<T> submit(Job<T> job) {
       jobs.put(jobId, handle);
 
       final io.netty.util.concurrent.Future<Void> rpc = driverRpc.call(new JobRequest(jobId, job));
+      LOG.debug("Send JobRequest[{}].", jobId);
 
       // Link the RPC and the promise so that events from one are propagated to the other as
       // needed.

File: spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcDispatcher.java
Patch:
@@ -139,6 +139,7 @@ private void handleReply(ChannelHandlerContext ctx, Object msg, OutstandingRpc r
   private void handleError(ChannelHandlerContext ctx, Object msg, OutstandingRpc rpc)
       throws Exception {
     if (msg instanceof String) {
+      LOG.warn("Received error message:{}.", msg);
       rpc.future.setFailure(new RpcException((String) msg));
     } else {
       String error = String.format("Received error with unexpected payload (%s).",

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/NonBlockingOpDeDupProc.java
Patch:
@@ -186,7 +186,9 @@ private boolean checkReferences(ExprNodeDesc expr, Set<String> funcOutputs, Set<
    * @param pSEL parent operator
    */
   private void fixContextReferences(SelectOperator cSEL, SelectOperator pSEL) {
-    Collection<QBJoinTree> qbJoinTrees = pctx.getJoinContext().values();
+    Collection<QBJoinTree> qbJoinTrees = new ArrayList<QBJoinTree>();
+    qbJoinTrees.addAll(pctx.getJoinContext().values());
+    qbJoinTrees.addAll(pctx.getMapJoinContext().values());
     for (QBJoinTree qbJoinTree : qbJoinTrees) {
       Map<String, Operator<? extends OperatorDesc>> aliasToOpInfo = qbJoinTree.getAliasToOpInfo();
       for (Map.Entry<String, Operator<? extends OperatorDesc>> entry : aliasToOpInfo.entrySet()) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/NonBlockingOpDeDupProc.java
Patch:
@@ -186,7 +186,9 @@ private boolean checkReferences(ExprNodeDesc expr, Set<String> funcOutputs, Set<
    * @param pSEL parent operator
    */
   private void fixContextReferences(SelectOperator cSEL, SelectOperator pSEL) {
-    Collection<QBJoinTree> qbJoinTrees = pctx.getJoinContext().values();
+    Collection<QBJoinTree> qbJoinTrees = new ArrayList<QBJoinTree>();
+    qbJoinTrees.addAll(pctx.getJoinContext().values());
+    qbJoinTrees.addAll(pctx.getMapJoinContext().values());
     for (QBJoinTree qbJoinTree : qbJoinTrees) {
       Map<String, Operator<? extends OperatorDesc>> aliasToOpInfo = qbJoinTree.getAliasToOpInfo();
       for (Map.Entry<String, Operator<? extends OperatorDesc>> entry : aliasToOpInfo.entrySet()) {

File: ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsAggregator.java
Patch:
@@ -134,7 +134,7 @@ public ResultSet run(PreparedStatement stmt) throws SQLException {
       }
     };
 
-    fileID = JDBCStatsUtils.truncateRowId(fileID);
+    JDBCStatsUtils.validateRowId(fileID);
     String keyPrefix = Utilities.escapeSqlLike(fileID) + "%";
     for (int failures = 0;; failures++) {
       try {
@@ -218,7 +218,7 @@ public Void run(PreparedStatement stmt) throws SQLException {
     };
     try {
 
-      rowID = JDBCStatsUtils.truncateRowId(rowID);
+      JDBCStatsUtils.validateRowId(rowID);
       String keyPrefix = Utilities.escapeSqlLike(rowID) + "%";
 
       PreparedStatement delStmt = Utilities.prepareWithRetry(conn,

File: ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsSetupConstants.java
Patch:
@@ -34,7 +34,6 @@ public final class JDBCStatsSetupConstants {
 
   public static final String PART_STAT_RAW_DATA_SIZE_COLUMN_NAME = "RAW_DATA_SIZE";
 
-  // 255 is an old value that we will keep for now; it can be increased to 4000; limits are
   // MySQL - 65535, SQL Server - 8000, Oracle - 4000, Derby - 32762, Postgres - large.
-  public static final int ID_COLUMN_VARCHAR_SIZE = 255;
+  public static final int ID_COLUMN_VARCHAR_SIZE = 4000;
 }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -675,7 +675,7 @@ public static enum ConfVars {
         "How many rows in the joining tables (except the streaming table) should be cached in memory."),
 
     // CBO related
-    HIVE_CBO_ENABLED("hive.cbo.enable", false, "Flag to control enabling Cost Based Optimizations using Calcite framework."),
+    HIVE_CBO_ENABLED("hive.cbo.enable", true, "Flag to control enabling Cost Based Optimizations using Calcite framework."),
 
     // hive.mapjoin.bucket.cache.size has been replaced by hive.smbjoin.cache.row,
     // need to remove by hive .13. Also, do not change default (see SMB operator)

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
Patch:
@@ -1857,7 +1857,7 @@ public void testBuiltInUDFCol() throws SQLException {
         + " where c1=1");
     ResultSetMetaData md = res.getMetaData();
     assertEquals(md.getColumnCount(), 2); // only one result column
-    assertEquals(md.getColumnLabel(2), "_c1" ); // verify the system generated column name
+    assertEquals(md.getColumnLabel(2), "c1" ); // verify the system generated column name
     assertTrue(res.next());
     assertEquals(res.getLong(1), 1);
     assertEquals(res.getString(2), "1");

File: common/src/java/org/apache/hadoop/hive/common/FileUtils.java
Patch:
@@ -448,7 +448,7 @@ public static boolean isActionPermittedForFileHierarchy(FileSystem fs, FileStatu
    */
   public static boolean isLocalFile(HiveConf conf, String fileName) {
     try {
-      // do best effor to determine if this is a local file
+      // do best effort to determine if this is a local file
       return isLocalFile(conf, new URI(fileName));
     } catch (URISyntaxException e) {
       LOG.warn("Unable to create URI from " + fileName, e);
@@ -464,7 +464,7 @@ public static boolean isLocalFile(HiveConf conf, String fileName) {
    */
   public static boolean isLocalFile(HiveConf conf, URI fileUri) {
     try {
-      // do best effor to determine if this is a local file
+      // do best effort to determine if this is a local file
       FileSystem fsForFile = FileSystem.get(fileUri, conf);
       return LocalFileSystem.class.isInstance(fsForFile);
     } catch (IOException e) {

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/ReadEntity.java
Patch:
@@ -128,7 +128,7 @@ public ReadEntity(Partition p, ReadEntity parent, boolean isDirect) {
    *          Flag to decide whether this directory is local or in dfs.
    */
   public ReadEntity(Path d, boolean islocal) {
-    super(d.toString(), islocal, true);
+    super(d, islocal, true);
   }
 
   public Set<ReadEntity> getParents() {

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/WriteEntity.java
Patch:
@@ -133,7 +133,7 @@ public WriteEntity(Path d, boolean islocal) {
    *          True if this is a temporary location such as scratch dir
    */
   public WriteEntity(Path d, boolean islocal, boolean isTemp) {
-    super(d.toString(), islocal, true);
+    super(d, islocal, true);
     this.isTempURI = isTemp;
     this.writeType = WriteType.PATH_WRITE;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -10950,7 +10950,7 @@ private ASTNode analyzeCreateTable(
       case HiveParser.TOK_TABLELOCATION:
         location = unescapeSQLString(child.getChild(0).getText());
         location = EximUtil.relativeToAbsolutePath(conf, location);
-        inputs.add(new ReadEntity(new Path(location), FileUtils.isLocalFile(conf, location)));
+        inputs.add(toReadEntity(location));
         break;
       case HiveParser.TOK_TABLEPROPERTIES:
         tblProps = DDLSemanticAnalyzer.getProps((ASTNode) child.getChild(0));

File: spark-client/src/main/java/org/apache/hive/spark/client/rpc/KryoMessageCodec.java
Patch:
@@ -105,18 +105,18 @@ protected void encode(ChannelHandlerContext ctx, Object msg, ByteBuf buf)
     kryoOut.flush();
 
     byte[] msgData = bytes.toByteArray();
+    LOG.debug("Encoded message of type {} ({} bytes)", msg.getClass().getName(), msgData.length);
     checkSize(msgData.length);
 
     buf.ensureWritable(msgData.length + 4);
     buf.writeInt(msgData.length);
     buf.writeBytes(msgData);
-    LOG.debug("Encoded message of type {} ({} bytes)", msg.getClass().getName(), msgData.length);
   }
 
   private void checkSize(int msgSize) {
-    Preconditions.checkArgument(msgSize > 0, "Message size must be positive.");
+    Preconditions.checkArgument(msgSize > 0, "Message size (%s bytes) must be positive.", msgSize);
     Preconditions.checkArgument(maxMessageSize <= 0 || msgSize <= maxMessageSize,
-        "Message exceeds maximum allowed size (%s bytes).", maxMessageSize);
+        "Message (%s bytes) exceeds maximum allowed size (%s bytes).", msgSize, maxMessageSize);
   }
 
 }

File: spark-client/src/main/java/org/apache/hive/spark/client/rpc/Rpc.java
Patch:
@@ -268,8 +268,10 @@ public <T> Future<T> call(Object msg, Class<T> retType) {
           @Override
           public void operationComplete(ChannelFuture cf) {
             if (!cf.isSuccess() && !promise.isDone()) {
+              LOG.warn("Failed to send RPC, closing connection.", cf.cause());
               promise.setFailure(cf.cause());
               dispatcher.get().discardRpc(id);
+              close();
             }
           }
       };

File: spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcConfiguration.java
Patch:
@@ -65,7 +65,7 @@ public final class RpcConfiguration {
 
   /** Maximum message size. Default = 10MB. */
   public final static String RPC_MAX_MESSAGE_SIZE_KEY = "hive.spark.client.rpc.max.size";
-  public final static int RPC_MAX_MESSAGE_SIZE_DEFAULT = 10 * 1024 * 1024;
+  public final static int RPC_MAX_MESSAGE_SIZE_DEFAULT = 50 * 1024 * 1024;
 
   /** Channel logging level. */
   public final static String RPC_CHANNEL_LOG_LEVEL_KEY = "hive.spark.client.channel.log.level";

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
Patch:
@@ -309,9 +309,7 @@ protected RexNode convert(ExprNodeConstantDesc literal) throws CalciteSemanticEx
       calciteLiteral = rexBuilder.makeLiteral(((Boolean) value).booleanValue());
       break;
     case BYTE:
-      byte[] byteArray = new byte[] { (Byte) value };
-      ByteString bs = new ByteString(byteArray);
-      calciteLiteral = rexBuilder.makeBinaryLiteral(bs);
+      calciteLiteral = rexBuilder.makeExactLiteral(new BigDecimal((Byte) value), calciteDataType);
       break;
     case SHORT:
       calciteLiteral = rexBuilder.makeExactLiteral(new BigDecimal((Short) value), calciteDataType);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java
Patch:
@@ -38,6 +38,7 @@
 import java.util.Map.Entry;
 import java.util.Set;
 import java.util.TreeMap;
+import java.util.LinkedHashMap;
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
@@ -95,7 +96,7 @@ private static JSONObject getJSONDependencies(ExplainWork work)
       switch (input.getType()) {
         case TABLE:
           Table table = input.getTable();
-          Map<String, String> tableInfo = new HashMap<String, String>();
+          Map<String, String> tableInfo = new LinkedHashMap<String, String>();
           tableInfo.put("tablename", table.getCompleteName());
           tableInfo.put("tabletype", table.getTableType().toString());
           if ((input.getParents() != null) && (!input.getParents().isEmpty())) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
Patch:
@@ -28,6 +28,7 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashMap;
+import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.TreeMap;
@@ -2132,9 +2133,9 @@ Object next(Object previous) throws IOException {
       Map<Object, Object> result = null;
       if (valuePresent) {
         if (previous == null) {
-          result = new HashMap<Object, Object>();
+          result = new LinkedHashMap<Object, Object>();
         } else {
-          result = (HashMap<Object, Object>) previous;
+          result = (LinkedHashMap<Object, Object>) previous;
         }
         // for now just clear and create new objects
         result.clear();

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java
Patch:
@@ -14,6 +14,7 @@
 package org.apache.hadoop.hive.ql.io.parquet.serde;
 
 import java.util.HashMap;
+import java.util.LinkedHashMap;
 import java.util.Map;
 
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -65,7 +66,7 @@ public ObjectInspector getMapValueObjectInspector() {
       }
 
       final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
-      final Map<Writable, Writable> map = new HashMap<Writable, Writable>();
+      final Map<Writable, Writable> map = new LinkedHashMap<Writable, Writable>();
 
       for (final Writable obj : mapArray) {
         final ArrayWritable mapObj = (ArrayWritable) obj;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
Patch:
@@ -24,6 +24,7 @@
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.LinkedHashSet;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -269,7 +270,7 @@ private class FetchData {
     private final Table table;
     private final SplitSample splitSample;
     private final PrunedPartitionList partsList;
-    private final HashSet<ReadEntity> inputs = new HashSet<ReadEntity>();
+    private final LinkedHashSet<ReadEntity> inputs = new LinkedHashSet<ReadEntity>();
     private final boolean onlyPruningFilter;
 
     // source table scan

File: ql/src/java/org/apache/hadoop/hive/ql/parse/MacroSemanticAnalyzer.java
Patch:
@@ -26,6 +26,7 @@
 import java.util.List;
 import java.util.Set;
 import java.util.Stack;
+import java.util.LinkedHashSet;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -122,7 +123,7 @@ public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
       macroColNames.add(argument.getName());
       macroColTypes.add(colType);
     }
-    Set<String> expectedColumnNames = new HashSet<String>(macroColNames);
+    Set<String> expectedColumnNames = new LinkedHashSet<String>(macroColNames);
     if(!expectedColumnNames.equals(actualColumnNames)) {
       throw new SemanticException("Expected columns " + expectedColumnNames + " but found "
           + actualColumnNames);

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMkCollectionEvaluator.java
Patch:
@@ -21,7 +21,7 @@
 import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.HashSet;
+import java.util.LinkedHashSet;
 import java.util.List;
 
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -92,7 +92,7 @@ public MkArrayAggregationBuffer() {
       if (bufferType == BufferType.LIST){
         container = new ArrayList<Object>();
       } else if(bufferType == BufferType.SET){
-        container = new HashSet<Object>();
+        container = new LinkedHashSet<Object>();
       } else {
         throw new RuntimeException("Buffer type unknown");
       }

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyMap.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.hadoop.hive.serde2.lazy;
 
 import java.util.Arrays;
-import java.util.HashSet;
+import java.util.LinkedHashSet;
 import java.util.LinkedHashMap;
 import java.util.Map;
 import java.util.Set;
@@ -154,7 +154,7 @@ private void parse() {
     int keyValueSeparatorPosition = -1;
     int elementByteEnd = start;
     byte[] bytes = this.bytes.getData();
-    Set<Object> keySet = new HashSet<Object>();
+    Set<Object> keySet = new LinkedHashSet<Object>();
 
     // Go through all bytes in the byte[]
     while (elementByteEnd <= arrayByteEnd) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
Patch:
@@ -1757,7 +1757,8 @@ private VectorExpression getCustomUDFExpression(ExprNodeGenericFuncDesc expr)
   }
 
   public static boolean isStringFamily(String resultType) {
-    return resultType.equalsIgnoreCase("string") || charVarcharTypePattern.matcher(resultType).matches();
+    return resultType.equalsIgnoreCase("string") || charVarcharTypePattern.matcher(resultType).matches() ||
+           resultType.equalsIgnoreCase("string_family");
   }
 
   public static boolean isDatetimeFamily(String resultType) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringGroupColConcatStringScalar.java
Patch:
@@ -125,7 +125,7 @@ public int getOutputColumn() {
 
   @Override
   public String getOutputType() {
-    return "StringGroup";
+    return "String_Family";
   }
 
   public int getColNum() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringGroupConcatColCol.java
Patch:
@@ -416,7 +416,7 @@ public int getOutputColumn() {
 
   @Override
   public String getOutputType() {
-    return "StringGroup";
+    return "String_Family";
   }
 
   public int getColNum1() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringScalarConcatStringGroupCol.java
Patch:
@@ -125,7 +125,7 @@ public int getOutputColumn() {
 
   @Override
   public String getOutputType() {
-    return "StringGroup";
+    return "String_Family";
   }
 
   public int getColNum() {

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -388,6 +388,8 @@ public enum ErrorMsg {
   TXN_NO_SUCH_TRANSACTION(10262, "No record of transaction could be found, " +
       "may have timed out"),
   TXN_ABORTED(10263, "Transaction manager has aborted the transaction."),
+  DBTXNMGR_REQUIRES_CONCURRENCY(10264,
+      "To use DbTxnManager you must set hive.support.concurrency=true"),
 
   LOCK_NO_SUCH_LOCK(10270, "No record of lock could be found, " +
       "may have timed out"),

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java
Patch:
@@ -216,7 +216,7 @@ public List<StructField> getAllStructFieldRefs() {
     @Override
     public StructField getStructFieldRef(String s) {
       for(StructField field: fields) {
-        if (field.getFieldName().equals(s)) {
+        if (field.getFieldName().equalsIgnoreCase(s)) {
           return field;
         }
       }
@@ -304,7 +304,7 @@ public boolean equals(Object o) {
         for(int i = 0; i < fields.size(); ++i) {
           StructField left = other.get(i);
           StructField right = fields.get(i);
-          if (!(left.getFieldName().equals(right.getFieldName()) &&
+          if (!(left.getFieldName().equalsIgnoreCase(right.getFieldName()) &&
                 left.getFieldObjectInspector().equals
                     (right.getFieldObjectInspector()))) {
             return false;

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -219,7 +219,7 @@ public enum ErrorMsg {
       "Dynamic partitions do not support IF NOT EXISTS. Specified partitions with value :"),
   UDAF_INVALID_LOCATION(10128, "Not yet supported place for UDAF"),
   DROP_PARTITION_NON_STRING_PARTCOLS_NONEQUALITY(10129,
-    "Drop partitions for a non string partition columns is not allowed using non-equality"),
+    "Drop partitions for a non-string partition column is only allowed using equality"),
   ALTER_COMMAND_FOR_VIEWS(10131, "To alter a view you need to use the ALTER VIEW command."),
   ALTER_COMMAND_FOR_TABLES(10132, "To alter a base table you need to use the ALTER TABLE command."),
   ALTER_VIEW_DISALLOWED_OP(10133, "Cannot use this form of ALTER on a view"),

File: spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java
Patch:
@@ -234,6 +234,7 @@ public void call(JavaFutureAction<?> future, SparkCounters sparkCounters) {
         // Catch throwables in a best-effort to report job status back to the client. It's
         // re-thrown so that the executor can destroy the affected thread (or the JVM can
         // die or whatever would happen if the throwable bubbled up).
+        LOG.info("Failed to run job " + req.id, t);
         client.tell(new Protocol.JobResult(req.id, null, t, null), actor);
         throw new ExecutionException(t);
       } finally {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/ASTBuilder.java
Patch:
@@ -213,7 +213,7 @@ static ASTNode literal(RexLiteral literal, boolean useTypeQualInLiteral) {
     case TIME:
     case TIMESTAMP: {
       val = literal.getValue();
-      type = HiveParser.TOK_TIMESTAMP;
+      type = HiveParser.TOK_TIMESTAMPLITERAL;
       DateFormat df = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss");
       val = df.format(((Calendar) val).getTime());
       val = "'" + val + "'";

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -1123,9 +1123,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
 
       // Create all children
       int childrenBegin = (isFunction ? 1 : 0);
-      ArrayList<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>(expr
-          .getChildCount()
-          - childrenBegin);
+      ArrayList<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>(
+          expr.getChildCount() - childrenBegin);
       for (int ci = childrenBegin; ci < expr.getChildCount(); ci++) {
         if (nodeOutputs[ci] instanceof ExprNodeColumnListDesc) {
           children.addAll(((ExprNodeColumnListDesc)nodeOutputs[ci]).getChildren());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java
Patch:
@@ -156,9 +156,6 @@ private Class getInputFormat(JobConf jobConf, MapWork mWork) throws HiveExceptio
     }
     String inpFormat = HiveConf.getVar(jobConf,
         HiveConf.ConfVars.HIVEINPUTFORMAT);
-    if ((inpFormat == null) || (StringUtils.isBlank(inpFormat))) {
-      inpFormat = ShimLoader.getHadoopShims().getInputFormatClassName();
-    }
 
     if (mWork.isUseBucketizedHiveInputFormat()) {
       inpFormat = BucketizedHiveInputFormat.class.getName();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -8289,7 +8289,7 @@ private ObjectPair<Integer, int[]> findMergePos(QBJoinTree node, QBJoinTree targ
       }
     }
 
-    if ( targetCondn == null ) {
+    if ( targetCondn == null || (nodeCondn.size() != targetCondn.size())) {
       return new ObjectPair(-1, null);
     }
 

File: metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -1023,7 +1023,8 @@ private <T> List<T> convertList(List<T> dnList) {
 
   /** Makes shallow copy of a map to avoid DataNucleus mucking with our objects. */
   private <K, V> Map<K, V> convertMap(Map<K, V> dnMap) {
-    return (dnMap == null) ? null : Maps.newHashMap(dnMap);
+    // Must be deterministic order map - see HIVE-8707
+    return (dnMap == null) ? null : Maps.newLinkedHashMap(dnMap);
   }
 
   private Table convertToTable(MTable mtbl) throws MetaException {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -375,9 +375,9 @@ public static enum ConfVars {
     METASTORECONNECTURLKEY("javax.jdo.option.ConnectionURL",
         "jdbc:derby:;databaseName=metastore_db;create=true",
         "JDBC connect string for a JDBC metastore"),
-    HMSHANDLERATTEMPTS("hive.hmshandler.retry.attempts", 1,
+    HMSHANDLERATTEMPTS("hive.hmshandler.retry.attempts", 10,
         "The number of times to retry a HMSHandler call if there were a connection error."),
-    HMSHANDLERINTERVAL("hive.hmshandler.retry.interval", "1000ms",
+    HMSHANDLERINTERVAL("hive.hmshandler.retry.interval", "2000ms",
         new TimeValidator(TimeUnit.MILLISECONDS), "The time between HMSHandler retry attempts on failure."),
     HMSHANDLERFORCERELOADCONF("hive.hmshandler.force.reload.conf", false,
         "Whether to force reloading of the HMSHandler configuration (including\n" +

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java
Patch:
@@ -65,7 +65,7 @@ public void init(ExecMapperContext context, Configuration hconf, MapJoinOperator
   @Override
   public void load(
       MapJoinTableContainer[] mapJoinTables,
-      MapJoinTableContainerSerDe[] mapJoinTableSerdes) throws HiveException {
+      MapJoinTableContainerSerDe[] mapJoinTableSerdes, long memUsage) throws HiveException {
 
     String currentInputPath = context.getCurrentInputPath().toString();
     LOG.info("******* Load from HashTable for input file: " + currentInputPath);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java
Patch:
@@ -76,7 +76,7 @@ public Writable getWritableObject(int index) {
   public void fill(Decimal128 value) {
     noNulls = true;
     isRepeating = true;
-    vector[0] = value;
+    // TODO#: vector[0] = value;
   }
 
   // Fill the column vector with nulls

File: ql/src/java/org/apache/hadoop/hive/ql/io/LlapInputFormat.java
Patch:
@@ -308,7 +308,7 @@ public void visit(DecimalColumnVector c) throws IOException {
         } else {
           c.reset();
           c.noNulls = !hasNulls;
-          currentVectorSlice.copyDecimals(c.vector, hasNulls ? c.isNull : null, 0);
+          // TODO#: currentVectorSlice.copyDecimals(c.vector, hasNulls ? c.isNull : null, 0);
           if (hasNulls) {
             NullUtil.setNullDataEntriesDecimal(c, false, null, rowCount);
           }

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
Patch:
@@ -951,9 +951,9 @@ public static boolean compareTypes(ObjectInspector o1, ObjectInspector o2) {
 
       if (childFieldsList1 == null && childFieldsList2 == null) {
         return true;
-      }
-
-      if (childFieldsList1.size() != childFieldsList2.size()) {
+      } else if (childFieldsList1 == null || childFieldsList2 == null) {
+        return false;
+      } else if (childFieldsList1.size() != childFieldsList2.size()) {
         return false;
       }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinTableContainerSerDe.java
Patch:
@@ -118,7 +118,7 @@ public MapJoinPersistableTableContainer load(
 
       for (FileStatus fileStatus: fs.listStatus(folder)) {
         Path filePath = fileStatus.getPath();
-        if (!fileStatus.isFile()) {
+        if (fileStatus.isDir()) {
           throw new HiveException("Error, not a file: " + filePath);
         }
         InputStream is = null;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/counter/SparkCounterGroup.java
Patch:
@@ -21,14 +21,14 @@
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.hadoop.mapreduce.util.ResourceBundles;
+import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.spark.api.java.JavaSparkContext;
 
 /**
  * We use group to fold all the same kind of counters.
  */
 public class SparkCounterGroup implements Serializable {
-
+  private static final long serialVersionUID = 1L;
   private String groupName;
   private String groupDisplayName;
   private Map<String, SparkCounter> sparkCounters;
@@ -47,7 +47,7 @@ public SparkCounterGroup(
   }
 
   public void createCounter(String name, long initValue) {
-    String displayName = ResourceBundles.getCounterGroupName(name, name);
+    String displayName = ShimLoader.getHadoopShims().getCounterGroupName(groupName, groupName);
     SparkCounter counter = new SparkCounter(name, displayName, groupName, initValue, javaSparkContext);
     sparkCounters.put(name, counter);
   }

File: ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestHiveKVResultCache.java
Patch:
@@ -121,8 +121,8 @@ private static void scanAndVerify(
     HashSet<Long> primaryRowKeys = new HashSet<Long>();
     HashSet<Long> separateRowKeys = new HashSet<Long>();
     for (Tuple2<HiveKey, BytesWritable> item: output) {
-      String key = new String(item._1.copyBytes());
-      String value = new String(item._2.copyBytes());
+      String key = new String(item._1.getBytes());
+      String value = new String(item._2.getBytes());
       String prefix = key.substring(0, key.indexOf('_'));
       Long id = Long.valueOf(key.substring(5 + prefix.length()));
       if (prefix.equals(prefix1)) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/SqlFunctionConverter.java
Patch:
@@ -278,6 +278,7 @@ private static class StaticBlockBuilder {
       registerFunction(">=", SqlStdOperatorTable.GREATER_THAN_OR_EQUAL,
           hToken(HiveParser.GREATERTHANOREQUALTO, ">="));
       registerFunction("!", SqlStdOperatorTable.NOT, hToken(HiveParser.KW_NOT, "not"));
+      registerFunction("<>", SqlStdOperatorTable.NOT_EQUALS, hToken(HiveParser.NOTEQUAL, "<>"));
     }
 
     private void registerFunction(String name, SqlOperator optiqFn, HiveToken hiveToken) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java
Patch:
@@ -187,7 +187,6 @@ public void open(HiveConf conf, String[] additionalFiles)
     LOG.info("Opening new Tez Session (id: " + sessionId
         + ", scratch dir: " + tezScratchDir + ")");
 
-    TezJobMonitor.initShutdownHook();
     session.start();
 
     if (HiveConf.getBoolVar(conf, ConfVars.HIVE_PREWARM_ENABLED)) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/SqlFunctionConverter.java
Patch:
@@ -278,7 +278,6 @@ private static class StaticBlockBuilder {
       registerFunction(">=", SqlStdOperatorTable.GREATER_THAN_OR_EQUAL,
           hToken(HiveParser.GREATERTHANOREQUALTO, ">="));
       registerFunction("!", SqlStdOperatorTable.NOT, hToken(HiveParser.KW_NOT, "not"));
-      registerFunction("<>", SqlStdOperatorTable.NOT_EQUALS, hToken(HiveParser.NOTEQUAL, "<>"));
     }
 
     private void registerFunction(String name, SqlOperator optiqFn, HiveToken hiveToken) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java
Patch:
@@ -187,6 +187,7 @@ public void open(HiveConf conf, String[] additionalFiles)
     LOG.info("Opening new Tez Session (id: " + sessionId
         + ", scratch dir: " + tezScratchDir + ")");
 
+    TezJobMonitor.initShutdownHook();
     session.start();
 
     if (HiveConf.getBoolVar(conf, ConfVars.HIVE_PREWARM_ENABLED)) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/SqlFunctionConverter.java
Patch:
@@ -278,6 +278,7 @@ private static class StaticBlockBuilder {
       registerFunction(">=", SqlStdOperatorTable.GREATER_THAN_OR_EQUAL,
           hToken(HiveParser.GREATERTHANOREQUALTO, ">="));
       registerFunction("!", SqlStdOperatorTable.NOT, hToken(HiveParser.KW_NOT, "not"));
+      registerFunction("<>", SqlStdOperatorTable.NOT_EQUALS, hToken(HiveParser.NOTEQUAL, "<>"));
     }
 
     private void registerFunction(String name, SqlOperator optiqFn, HiveToken hiveToken) {

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java
Patch:
@@ -84,8 +84,9 @@ public class HCatUtil {
   private static volatile HiveClientCache hiveClientCache;
 
   public static boolean checkJobContextIfRunningFromBackend(JobContext j) {
-    if (j.getConfiguration().get("mapred.task.id", "").equals("") &&
-        !("true".equals(j.getConfiguration().get("pig.illustrating")))) {
+    if (j.getConfiguration().get("pig.job.converted.fetch", "").equals("") &&
+          j.getConfiguration().get("mapred.task.id", "").equals("") &&
+          !("true".equals(j.getConfiguration().get("pig.illustrating")))) {
       return false;
     }
     return true;

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestAuthorizationApiAuthorizer.java
Patch:
@@ -156,7 +156,7 @@ public void testCreateRole() throws Exception {
     FunctionInvoker invoker = new FunctionInvoker() {
       @Override
       public void invoke() throws Exception {
-        msc.create_role(new Role());
+        msc.create_role(new Role("role1", 0, "owner"));
       }
     };
     testFunction(invoker);

File: itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestJdbcWithMiniKdc.java
Patch:
@@ -171,8 +171,8 @@ public void testNegativeTokenAuth() throws Exception {
           MiniHiveKdc.HIVE_TEST_USER_2);
     } catch (SQLException e) {
       // Expected error
-      assertTrue(e.getMessage().contains("Failed to validate proxy privilege"));
-      assertTrue(e.getCause().getCause().getMessage().contains("Failed to validate proxy privilege"));
+      assertTrue(e.getMessage().contains("Error retrieving delegation token for user"));
+      assertTrue(e.getCause().getCause().getMessage().contains("is not allowed to impersonate"));
     } finally {
       hs2Conn.close();
     }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -386,7 +386,7 @@ public static enum ConfVars {
         "testing only."),
     METASTORESERVERMINTHREADS("hive.metastore.server.min.threads", 200,
         "Minimum number of worker threads in the Thrift server's pool."),
-    METASTORESERVERMAXTHREADS("hive.metastore.server.max.threads", 100000,
+    METASTORESERVERMAXTHREADS("hive.metastore.server.max.threads", 1000,
         "Maximum number of worker threads in the Thrift server's pool."),
     METASTORE_TCP_KEEP_ALIVE("hive.metastore.server.tcp.keepalive", true,
         "Whether to enable TCP keepalive for the metastore server. Keepalive will prevent accumulation of half-open connections."),

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/TestUseDatabase.java
Patch:
@@ -64,7 +64,7 @@ public void testAlterTablePass() throws IOException, CommandNeedRetryException {
 
     String tmpDir = System.getProperty("test.tmp.dir");
     File dir = new File(tmpDir + "/hive-junit-" + System.nanoTime());
-    response = hcatDriver.run("alter table " + tblName + " add partition (b='2') location '" + dir.getAbsolutePath() + "'");
+    response = hcatDriver.run("alter table " + tblName + " add partition (b='2') location '" + dir.toURI().getPath() + "'");
     assertEquals(0, response.getResponseCode());
     assertNull(response.getErrorMessage());
 

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderStorer.java
Patch:
@@ -64,7 +64,7 @@ public void testReadWrite() throws Exception {
     TestHCatLoader.executeStatementOnDriver("create external table " + tblName +
       " (my_small_int smallint, my_tiny_int tinyint)" +
       " row format delimited fields terminated by '\t' stored as textfile location '" +
-      dataDir + "'", driver);
+      dataDir.toURI().getPath() + "'", driver);
     TestHCatLoader.dropTable(tblName2, driver);
     TestHCatLoader.createTable(tblName2, "my_small_int smallint, my_tiny_int tinyint", null, driver,
       "textfile");

File: itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestPasswordWithCredentialProvider.java
Patch:
@@ -81,7 +81,7 @@ public void testPassword() throws Exception {
     conf.set("hadoop.security.credential.clear-text-fallback", "true");
 
     // Set up CredentialProvider
-    conf.set("hadoop.security.credential.provider.path", "jceks://file/" + tmpDir + "/test.jks");
+    conf.set("hadoop.security.credential.provider.path", "jceks://file/" + tmpDir.toURI().getPath() + "/test.jks");
 
     // CredentialProvider/CredentialProviderFactory may not exist, depending on the version of
     // hadoop-2 being used to build Hive. Use reflection to do the following lines

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java
Patch:
@@ -140,7 +140,7 @@ public void testStatsAfterCompactionPartTbl() throws Exception {
     executeStatementOnDriver("CREATE EXTERNAL TABLE " + tblNameStg + "(a INT, b STRING)" +
       " ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' LINES TERMINATED BY '\\n'" +
       " STORED AS TEXTFILE" +
-      " LOCATION '" + stagingFolder.newFolder() + "'", driver);
+      " LOCATION '" + stagingFolder.newFolder().toURI().getPath() + "'", driver);
 
     executeStatementOnDriver("load data local inpath '" + BASIC_FILE_NAME +
       "' overwrite into table " + tblNameStg, driver);

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
Patch:
@@ -1805,6 +1805,7 @@ public void testDuplicateColumnNameOrder() throws SQLException {
     ResultSet rs = stmt.executeQuery("SELECT 1 AS a, 2 AS a from " + tableName);
     assertTrue(rs.next());
     assertEquals(1, rs.getInt("a"));
+    rs.close();
   }
 
 

File: serde/src/test/org/apache/hadoop/hive/serde2/avro/TestTypeInfoToSchema.java
Patch:
@@ -80,6 +80,7 @@ public class TestTypeInfoToSchema {
 
   private TypeInfoToSchema typeInfoToSchema;
 
+  private final String lineSeparator = System.getProperty("line.separator");
 
   private String getAvroSchemaString(TypeInfo columnType) {
     return typeInfoToSchema.convert(
@@ -383,7 +384,7 @@ public void createAvroStructSchema() throws IOException {
     LOGGER.info("structTypeInfo is " + structTypeInfo);
 
     final String specificSchema = IOUtils.toString(Resources.getResource("avro-struct.avsc")
-        .openStream()).replace("\n", "");
+        .openStream()).replace(lineSeparator, "");
     String expectedSchema = genSchema(
         specificSchema);
 
@@ -414,7 +415,7 @@ public void createAvroNestedStructSchema() throws IOException {
     superStructTypeInfo.setAllStructFieldTypeInfos(superTypeInfos);
 
     final String specificSchema = IOUtils.toString(Resources.getResource("avro-nested-struct.avsc")
-        .openStream()).replace("\n", "");
+        .openStream()).replace(lineSeparator, "");
     String expectedSchema = genSchema(
         specificSchema);
     Assert.assertEquals("Test for nested struct's avro schema failed",

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/HiveOptiqUtil.java
Patch:
@@ -75,7 +75,7 @@ public static List<Integer> getVirtualCols(List<? extends RexNode> exps) {
     return vCols;
   }
 
-  public static boolean validateASTForCBO(ASTNode ast) {
+  public static boolean validateASTForUnsupportedTokens(ASTNode ast) {
     String astTree = ast.toStringTree();
     // if any of following tokens are present in AST, bail out
     String[] tokens = { "TOK_CHARSETLITERAL","TOK_TABLESPLITSAMPLE" };

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -10241,13 +10241,15 @@ private boolean canHandleAstForCbo(ASTNode ast, QB qb, PreCboCtx cboCtx) {
      //        be supported and would require additional checks similar to IsQuery?
      boolean isSupportedType =
          qb.getIsQuery() || qb.isCTAS() || cboCtx.type == PreCboCtx.Type.INSERT;
-     boolean result = isSupportedRoot && isSupportedType && createVwDesc == null;
+     boolean noBadTokens = HiveOptiqUtil.validateASTForUnsupportedTokens(ast);
+     boolean result = isSupportedRoot && isSupportedType && createVwDesc == null && noBadTokens;
      if (!result) {
        if (needToLogMessage) {
          String msg = "";
          if (!isSupportedRoot) msg += "doesn't have QUERY or EXPLAIN as root and not a CTAS; ";
          if (!isSupportedType) msg += "is not a query, CTAS, or insert; ";
          if (createVwDesc != null) msg += "has create view; ";
+         if (!noBadTokens) msg += "has unsupported tokens; ";
 
          if (msg.isEmpty()) msg += "has some unspecified limitations; ";
          LOG.info("Not invoking CBO because the statement " + msg.substring(0, msg.length() - 2));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkMapRecordHandler.java
Patch:
@@ -197,7 +197,7 @@ public void close() {
         logCloseInfo();
       }
 
-      ReportStats rps = new ReportStats(rp);
+      ReportStats rps = new ReportStats(rp, jc);
       mo.preorderMap(rps);
       return;
     } catch (Exception e) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkReduceRecordHandler.java
Patch:
@@ -410,7 +410,7 @@ public void close() {
       }
 
       reducer.close(abort);
-      ReportStats rps = new ReportStats(rp);
+      ReportStats rps = new ReportStats(rp, jc);
       reducer.preorderMap(rps);
 
     } catch (Exception e) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java
Patch:
@@ -85,7 +85,6 @@ public Object process(Node nd, Stack<Node> stack,
             maxReducers, false);
         LOG.info("Set parallelism for reduce sink " + sink + " to: " + numReducers);
         desc.setNumReducers(numReducers);
-        desc.setAutoParallel(true);
       }
     } else {
       LOG.info("Number of reducers determined to be: " + desc.getNumReducers());

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/ASTBuilder.java
Patch:
@@ -167,6 +167,9 @@ static ASTNode literal(RexLiteral literal, boolean useTypeQualInLiteral) {
       type = HiveParser.SmallintLiteral;
       break;
     case INTEGER:
+      val = literal.getValue3();
+      type = HiveParser.BigintLiteral;
+      break;
     case BIGINT:
       if (useTypeQualInLiteral) {
         val = literal.getValue3() + "L";

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/RexNodeConverter.java
Patch:
@@ -314,7 +314,7 @@ protected RexNode convert(ExprNodeConstantDesc literal) throws OptiqSemanticExce
       optiqLiteral = rexBuilder.makeBinaryLiteral(bs);
       break;
     case SHORT:
-      optiqLiteral = rexBuilder.makeExactLiteral(new BigDecimal((Short) value));
+      optiqLiteral = rexBuilder.makeExactLiteral(new BigDecimal((Short) value), optiqDataType);
       break;
     case INT:
       optiqLiteral = rexBuilder.makeExactLiteral(new BigDecimal((Integer) value));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java
Patch:
@@ -80,7 +80,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     for (int p = 0; p < parents; p++) {
       assert (parentFields[p].size() == columns);
       for (int c = 0; c < columns; c++) {
-        if (!columnTypeResolvers[c].update(parentFields[p].get(c)
+        if (!columnTypeResolvers[c].updateForUnionAll(parentFields[p].get(c)
             .getFieldObjectInspector())) {
           // checked in SemanticAnalyzer. Should not happen
           throw new HiveException("Incompatible types for union operator");

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1317,10 +1317,11 @@ public static enum ConfVars {
 
     HIVEOUTERJOINSUPPORTSFILTERS("hive.outerjoin.supports.filters", true, ""),
 
-    HIVEFETCHTASKCONVERSION("hive.fetch.task.conversion", "more", new StringSet("minimal", "more"),
+    HIVEFETCHTASKCONVERSION("hive.fetch.task.conversion", "more", new StringSet("none", "minimal", "more"),
         "Some select queries can be converted to single FETCH task minimizing latency.\n" +
         "Currently the query should be single sourced not having any subquery and should not have\n" +
         "any aggregations or distincts (which incurs RS), lateral views and joins.\n" +
+        "0. none : disable hive.fetch.task.conversion\n" +
         "1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only\n" +
         "2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)"
     ),

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -3355,7 +3355,7 @@ public List<FieldSchema> get_fields(String db, String tableName)
           ret = tbl.getSd().getCols();
         } else {
           try {
-            Deserializer s = MetaStoreUtils.getDeserializer(hiveConf, tbl);
+            Deserializer s = MetaStoreUtils.getDeserializer(hiveConf, tbl, false);
             ret = MetaStoreUtils.getFieldsFromDeserializer(tableName, s);
           } catch (SerDeException e) {
             StringUtils.stringifyException(e);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
Patch:
@@ -239,7 +239,7 @@ static InputFormat<WritableComparable, Writable> getInputFormatFromCache(Class i
 
   private StructObjectInspector getRowInspectorFromTable(TableDesc table) throws Exception {
     Deserializer serde = table.getDeserializerClass().newInstance();
-    SerDeUtils.initializeSerDe(serde, job, table.getProperties(), null);
+    SerDeUtils.initializeSerDeWithoutErrorCheck(serde, job, table.getProperties(), null);
     return createRowInspector(getStructOIFrom(serde.getObjectInspector()));
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlan.java
Patch:
@@ -38,7 +38,7 @@ public class SparkPlan {
   private final Map<SparkTran, List<SparkTran>> transGraph = new HashMap<SparkTran, List<SparkTran>>();
   private final Map<SparkTran, List<SparkTran>> invertedTransGraph = new HashMap<SparkTran, List<SparkTran>>();
 
-  public void execute() throws IllegalStateException {
+  public JavaPairRDD<HiveKey, BytesWritable> generateGraph() throws IllegalStateException {
     Map<SparkTran, JavaPairRDD<HiveKey, BytesWritable>> tranToOutputRDDMap
         = new HashMap<SparkTran, JavaPairRDD<HiveKey, BytesWritable>>();
     for (SparkTran tran : getAllTrans()) {
@@ -74,7 +74,7 @@ public void execute() throws IllegalStateException {
       }
     }
 
-    finalRDD.foreach(HiveVoidFunction.getInstance());
+    return finalRDD;
   }
 
   public void addTran(SparkTran tran) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -12423,7 +12423,7 @@ private RelNode genUnionLogicalPlan(String unionalias, String leftalias, RelNode
                   + " does not have the field " + field));
         }
         if (!lInfo.getInternalName().equals(rInfo.getInternalName())) {
-          throw new SemanticException(generateErrorMessage(tabref,
+          throw new OptiqSemanticException(generateErrorMessage(tabref,
               "Schema of both sides of union should match: field " + field + ":"
                   + " appears on the left side of the UNION at column position: "
                   + getPositionFromInternalName(lInfo.getInternalName())
@@ -12435,7 +12435,7 @@ private RelNode genUnionLogicalPlan(String unionalias, String leftalias, RelNode
         TypeInfo commonTypeInfo = FunctionRegistry.getCommonClassForUnionAll(lInfo.getType(),
             rInfo.getType());
         if (commonTypeInfo == null) {
-          throw new SemanticException(generateErrorMessage(tabref,
+          throw new OptiqSemanticException(generateErrorMessage(tabref,
               "Schema of both sides of union should match: Column " + field + " is of type "
                   + lInfo.getType().getTypeName() + " on first table and type "
                   + rInfo.getType().getTypeName() + " on second table"));
@@ -13293,7 +13293,7 @@ private RelNode genGBLogicalPlan(QB qb, RelNode srcRel) throws SemanticException
                 grpbyExpr, new TypeCheckCtx(groupByInputRowResolver));
             ExprNodeDesc grpbyExprNDesc = astToExprNDescMap.get(grpbyExpr);
             if (grpbyExprNDesc == null)
-              throw new RuntimeException("Invalid Column Reference: " + grpbyExpr.dump());
+              throw new OptiqSemanticException("Invalid Column Reference: " + grpbyExpr.dump());
 
             addToGBExpr(groupByOutputRowResolver, groupByInputRowResolver, grpbyExpr,
                 grpbyExprNDesc, gbExprNDescLst, outputColumnNames);

File: jdbc/src/java/org/apache/hive/jdbc/ZooKeeperHiveClientHelper.java
Patch:
@@ -53,6 +53,9 @@ static String getNextServerUriFromZooKeeper(JdbcConnectionParams connParams)
     String zooKeeperEnsemble = connParams.getZooKeeperEnsemble();
     String zooKeeperNamespace =
         connParams.getSessionVars().get(JdbcConnectionParams.ZOOKEEPER_NAMESPACE);
+    if ((zooKeeperNamespace == null) || (zooKeeperNamespace.isEmpty())) {
+      zooKeeperNamespace = JdbcConnectionParams.ZOOKEEPER_DEFAULT_NAMESPACE;
+    }
     List<String> serverHosts;
     Random randomizer = new Random();
     String serverNode;

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/CharTypeInfo.java
Patch:
@@ -25,6 +25,7 @@ public class CharTypeInfo  extends BaseCharTypeInfo {
 
   // no-arg constructor to make kyro happy.
   public CharTypeInfo() {
+    super(serdeConstants.CHAR_TYPE_NAME);
   }
 
   public CharTypeInfo(int length) {

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/VarcharTypeInfo.java
Patch:
@@ -25,6 +25,7 @@ public class VarcharTypeInfo extends BaseCharTypeInfo {
 
   // no-arg constructor to make kyro happy.
   public VarcharTypeInfo() {
+    super(serdeConstants.VARCHAR_TYPE_NAME);
   }
 
   public VarcharTypeInfo(int length) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
Patch:
@@ -971,7 +971,7 @@ public LocalResource localizeResource(Path src, Path dest, Configuration conf)
   public JobConf createConfiguration(HiveConf hiveConf) throws IOException {
     hiveConf.setBoolean("mapred.mapper.new-api", false);
 
-    JobConf conf = new JobConf(hiveConf);
+    JobConf conf = new JobConf(new TezConfiguration(hiveConf));
 
     conf.set("mapred.output.committer.class", NullOutputCommitter.class.getName());
 

File: serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java
Patch:
@@ -40,7 +40,6 @@
 import org.apache.avro.io.BinaryEncoder;
 import org.apache.avro.io.DecoderFactory;
 import org.apache.avro.io.EncoderFactory;
-import org.apache.avro.util.Utf8;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.common.type.HiveChar;
@@ -370,10 +369,10 @@ private Object deserializeMap(Object datum, Schema fileSchema, Schema mapSchema,
     // Avro only allows maps with Strings for keys, so we only have to worry
     // about deserializing the values
     Map<String, Object> map = new HashMap<String, Object>();
-    Map<Utf8, Object> mapDatum = (Map)datum;
+    Map<CharSequence, Object> mapDatum = (Map)datum;
     Schema valueSchema = mapSchema.getValueType();
     TypeInfo valueTypeInfo = columnType.getMapValueTypeInfo();
-    for (Utf8 key : mapDatum.keySet()) {
+    for (CharSequence key : mapDatum.keySet()) {
       Object value = mapDatum.get(key);
       map.put(key.toString(), worker(value, fileSchema == null ? null : fileSchema.getValueType(),
           valueSchema, valueTypeInfo));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
Patch:
@@ -76,7 +76,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
       statsMap.put(Counter.FILTERED, filtered_count);
       statsMap.put(Counter.PASSED, passed_count);
       conditionInspector = null;
-      ioContext = IOContext.get(hconf.get(Utilities.INPUT_NAME));
+      ioContext = IOContext.get(hconf);
     } catch (Throwable e) {
       throw new HiveException(e);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
Patch:
@@ -339,7 +339,7 @@ else if (partRawRowObjectInspector.equals(tblRawRowObjectInspector)) {
   }
 
   public void setChildren(Configuration hconf) throws HiveException {
-    Path fpath = IOContext.get(hconf.get(Utilities.INPUT_NAME)).getInputPath();
+    Path fpath = IOContext.get(hconf).getInputPath();
 
     boolean schemeless = fpath.toUri().getScheme() == null;
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java
Patch:
@@ -63,7 +63,7 @@ public void setCurrentBigBucketFile(String currentBigBucketFile) {
 
   public ExecMapperContext(JobConf jc) {
     this.jc = jc;
-    ioCxt = IOContext.get(jc.get(Utilities.INPUT_NAME));
+    ioCxt = IOContext.get(jc);
   }
 
   public void clear() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkMapRecordHandler.java
Patch:
@@ -63,8 +63,7 @@ public class SparkMapRecordHandler extends SparkRecordHandler {
 
   private MapredLocalWork localWork = null;
   private boolean isLogInfoEnabled = false;
-
-  private final ExecMapperContext execContext = new ExecMapperContext();
+  private ExecMapperContext execContext;
 
   public void init(JobConf job, OutputCollector output, Reporter reporter) {
     super.init(job, output, reporter);
@@ -74,7 +73,7 @@ public void init(JobConf job, OutputCollector output, Reporter reporter) {
 
     try {
       jc = job;
-      execContext.setJc(jc);
+      execContext = new ExecMapperContext(jc);
       // create map and fetch operators
       MapWork mrwork = (MapWork) cache.retrieve(PLAN_KEY);
       if (mrwork == null) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkReduceRecordHandler.java
Patch:
@@ -186,7 +186,7 @@ public void init(JobConf job, OutputCollector output, Reporter reporter) {
 	} else {
 	  ois.add(keyObjectInspector);
 	  ois.add(valueObjectInspector[tag]);
-	  reducer.setGroupKeyObjectInspector(keyObjectInspector);
+	  //reducer.setGroupKeyObjectInspector(keyObjectInspector);
 	  rowObjectInspector[tag] = ObjectInspectorFactory.getStandardStructObjectInspector(
 	      Utilities.reduceFieldNameList, ois);
 	}

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java
Patch:
@@ -161,7 +161,7 @@ protected void updateIOContext()
   }
 
   public IOContext getIOContext() {
-    return IOContext.get(jobConf.get(Utilities.INPUT_NAME));
+    return IOContext.get(jobConf);
   }
 
   private void initIOContext(long startPos, boolean isBlockPointer,

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java
Patch:
@@ -53,6 +53,7 @@ public class Optimizer {
   public void initialize(HiveConf hiveConf) {
 
     boolean isTezExecEngine = HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez");
+    boolean isSparkExecEngine = HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("spark");
     boolean bucketMapJoinOptimizer = false;
 
     transformations = new ArrayList<Transform>();
@@ -134,7 +135,7 @@ public void initialize(HiveConf hiveConf) {
     if(HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTIMIZEMETADATAQUERIES)) {
       transformations.add(new StatsOptimizer());
     }
-    if (pctx.getContext().getExplain() && !isTezExecEngine) {
+    if (pctx.getContext().getExplain() && !isSparkExecEngine && !isTezExecEngine) {
       transformations.add(new AnnotateWithStatistics());
       transformations.add(new AnnotateWithOpTraits());
     }

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
Patch:
@@ -331,7 +331,7 @@ public void testMapOperator() throws Throwable {
       Configuration hconf = new JobConf(TestOperators.class);
       HiveConf.setVar(hconf, HiveConf.ConfVars.HADOOPMAPFILENAME,
           "hdfs:///testDir/testFile");
-      IOContext.get(hconf.get(Utilities.INPUT_NAME)).setInputPath(
+      IOContext.get(hconf).setInputPath(
           new Path("hdfs:///testDir/testFile"));
 
       // initialize pathToAliases

File: ql/src/test/org/apache/hadoop/hive/ql/io/TestHiveBinarySearchRecordReader.java
Patch:
@@ -116,7 +116,7 @@ public void doClose() throws IOException {
 
   private void resetIOContext() {
     conf.set(Utilities.INPUT_NAME, "TestHiveBinarySearchRecordReader");
-    ioContext = IOContext.get(conf.get(Utilities.INPUT_NAME));
+    ioContext = IOContext.get(conf);
     ioContext.setUseSorted(false);
     ioContext.setIsBinarySearching(false);
     ioContext.setEndBinarySearch(false);

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -430,7 +430,8 @@ public int compile(String command, boolean resetTaskIds) {
       sem.validate();
       perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.ANALYZE);
 
-      plan = new QueryPlan(command, sem, perfLogger.getStartTime(PerfLogger.DRIVER_RUN), queryId);
+      plan = new QueryPlan(command, sem, perfLogger.getStartTime(PerfLogger.DRIVER_RUN), queryId,
+        SessionState.get().getCommandType());
 
       String queryStr = plan.getQueryStr();
       conf.setVar(HiveConf.ConfVars.HIVEQUERYSTRING, queryStr);

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/HookContext.java
Patch:
@@ -147,7 +147,7 @@ public String getIpAddress() {
  }
 
   public String getOperationName() {
-    return SessionState.get().getHiveOperation().name();
+    return queryPlan.getOperationName();
   }
 
   public String getUserName() {

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestUpdateDeleteSemanticAnalyzer.java
Patch:
@@ -283,7 +283,7 @@ private ReturnInfo parseAndAnalyze(String query, String testName)
     // validate the plan
     sem.validate();
 
-    QueryPlan plan = new QueryPlan(query, sem, 0L, testName);
+    QueryPlan plan = new QueryPlan(query, sem, 0L, testName, null);
 
     return new ReturnInfo(tree, sem, plan);
   }

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java
Patch:
@@ -47,7 +47,6 @@
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.security.UserGroupInformation.AuthenticationMethod;
 import org.apache.hadoop.security.authorize.AuthorizationException;
-import org.apache.hadoop.security.authorize.DefaultImpersonationProvider;
 import org.apache.hadoop.security.authorize.ProxyUsers;
 import org.apache.hadoop.security.token.SecretManager.InvalidToken;
 import org.apache.hadoop.security.token.Token;
@@ -130,7 +129,7 @@ private void configureSuperUserIPAddresses(Configuration conf,
     }
     builder.append("127.0.1.1,");
     builder.append(InetAddress.getLocalHost().getCanonicalHostName());
-    conf.setStrings(DefaultImpersonationProvider.getProxySuperuserIpConfKey(superUserShortName),
+    conf.setStrings(ProxyUsers.getProxySuperuserIpConfKey(superUserShortName),
         builder.toString());
   }
 
@@ -293,7 +292,7 @@ public String run() throws Exception {
   private void setGroupsInConf(String[] groupNames, String proxyUserName)
   throws IOException {
    conf.set(
-       DefaultImpersonationProvider.getProxySuperuserGroupConfKey(proxyUserName),
+      ProxyUsers.getProxySuperuserGroupConfKey(proxyUserName),
       StringUtils.join(",", Arrays.asList(groupNames)));
     configureSuperUserIPAddresses(conf, proxyUserName);
     ProxyUsers.refreshSuperUserGroupsConfiguration(conf);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/AppMasterEventOperator.java
Patch:
@@ -60,6 +60,9 @@ public void initializeOp(Configuration hconf) throws HiveException {
   protected void initDataBuffer(boolean skipPruning) throws HiveException {
     buffer = new DataOutputBuffer();
     try {
+      // where does this go to?
+      buffer.writeUTF(((TezContext) TezContext.get()).getTezProcessorContext().getTaskVertexName());
+
       // add any other header info
       getConf().writeEventHeader(buffer);
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java
Patch:
@@ -34,7 +34,6 @@
 import org.eigenbase.relopt.RelOptCluster;
 import org.eigenbase.relopt.RelOptCost;
 import org.eigenbase.relopt.RelOptPlanner;
-import org.eigenbase.relopt.RelOptRule;
 import org.eigenbase.relopt.RelTraitSet;
 import org.eigenbase.reltype.RelDataType;
 import org.eigenbase.reltype.RelDataTypeField;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -8028,7 +8028,7 @@ private void mergeJoins(QB qb, QBJoinTree node, QBJoinTree target, int pos, int[
       List<ASTNode> nodeConds = node.getExpressions().get(i + 1);
       ArrayList<ASTNode> reordereNodeConds = new ArrayList<ASTNode>();
       for(int k=0; k < tgtToNodeExprMap.length; k++) {
-        reordereNodeConds.add(nodeConds.get(k));
+        reordereNodeConds.add(nodeConds.get(tgtToNodeExprMap[k]));
       }
       expr.add(reordereNodeConds);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -417,6 +417,8 @@ public enum ErrorMsg {
       "that implements AcidOutputFormat while transaction manager that supports ACID is in use"),
   VALUES_TABLE_CONSTRUCTOR_NOT_SUPPORTED(10296,
       "Values clause with table constructor not yet supported"),
+  ACID_OP_ON_NONACID_TABLE(10297, "Attempt to do update or delete on table {0} that does not use " +
+      "an AcidOutputFormat", true),
 
   //========================== 20000 range starts here ========================//
   SCRIPT_INIT_ERROR(20000, "Unable to initialize custom script."),

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestRemoteHiveMetaStoreIpAddress.java
Patch:
@@ -21,6 +21,7 @@
 import junit.framework.TestCase;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.util.StringUtils;
@@ -49,6 +50,8 @@ protected void setUp() throws Exception {
 
     int port = MetaStoreUtils.findFreePort();
     System.out.println("Starting MetaStore Server on port " + port);
+    System.setProperty(ConfVars.METASTORE_EVENT_LISTENERS.varname,
+        IpAddressListener.class.getName());
     MetaStoreUtils.startMetaStore(port, ShimLoader.getHadoopThriftAuthBridge());
     isServerStarted = true;
 

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/authorization/TestHS2AuthzContext.java
Patch:
@@ -120,9 +120,10 @@ private void verifyContextContents(final String cmd, String ctxCmd) throws SQLEx
     HiveAuthzContext context = contextCapturer.getValue();
 
     assertEquals("Command ", ctxCmd, context.getCommandString());
-    assertTrue("ip address pattern check", context.getIpAddress().contains("."));
+    assertTrue("ip address pattern check", context.getIpAddress().matches("[.:a-fA-F0-9]+"));
     // ip address size check - check for something better than non zero
     assertTrue("ip address size check", context.getIpAddress().length() > 7);
+
   }
 
   private Connection getConnection(String userName) throws SQLException {

File: metastore/src/java/org/apache/hadoop/hive/metastore/TSetIpAddressProcessor.java
Patch:
@@ -57,6 +57,6 @@ protected void setIpAddress(final TProtocol in) {
   }
 
   protected void setIpAddress(final Socket inSocket) {
-    HMSHandler.setIpAddress(inSocket.getInetAddress().toString());
+    HMSHandler.setIpAddress(inSocket.getInetAddress().getHostAddress());
   }
 }

File: service/src/java/org/apache/hive/service/auth/TSetIpAddressProcessor.java
Patch:
@@ -75,7 +75,7 @@ protected void setIpAddress(final TProtocol in) {
     if (tSocket == null) {
       LOGGER.warn("Unknown Transport, cannot determine ipAddress");
     } else {
-      THREAD_LOCAL_IP_ADDRESS.set(tSocket.getSocket().getInetAddress().toString());
+      THREAD_LOCAL_IP_ADDRESS.set(tSocket.getSocket().getInetAddress().getHostAddress());
     }
   }
 

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveDriver.java
Patch:
@@ -102,8 +102,9 @@ public boolean acceptsURL(String url) throws SQLException {
     return Pattern.matches(URL_PREFIX + ".*", url);
   }
 
+  @Override
   public Connection connect(String url, Properties info) throws SQLException {
-    return new HiveConnection(url, info);
+    return acceptsURL(url) ? new HiveConnection(url, info) : null;
   }
 
   /**

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1712,6 +1712,9 @@ public static enum ConfVars {
     HIVE_VECTORIZATION_REDUCE_ENABLED("hive.vectorized.execution.reduce.enabled", true,
             "This flag should be set to true to enable vectorized mode of the reduce-side of query execution.\n" +
             "The default value is true."),
+    HIVE_VECTORIZATION_REDUCE_GROUPBY_ENABLED("hive.vectorized.execution.reduce.groupby.enabled", true,
+            "This flag should be set to true to enable vectorized mode of the reduce-side GROUP BY query execution.\n" +
+            "The default value is true."),
     HIVE_VECTORIZATION_GROUPBY_CHECKINTERVAL("hive.vectorized.groupby.checkinterval", 100000,
         "Number of entries added to the group by aggregation hash before a recomputation of average entry size is performed."),
     HIVE_VECTORIZATION_GROUPBY_MAXENTRIES("hive.vectorized.groupby.maxentries", 1000000,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExpressionDescriptor.java
Patch:
@@ -67,6 +67,7 @@ public enum ArgumentType {
     DATE                    (0x040),
     TIMESTAMP               (0x080),
     DATETIME_FAMILY         (DATE.value | TIMESTAMP.value),
+    INT_TIMESTAMP_FAMILY    (INT_FAMILY.value | TIMESTAMP.value),
     INT_DATETIME_FAMILY     (INT_FAMILY.value | DATETIME_FAMILY.value),
     STRING_DATETIME_FAMILY  (STRING_FAMILY.value | DATETIME_FAMILY.value),
     ALL_FAMILY              (0xFFF);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/FolderPermissionBase.java
Patch:
@@ -52,7 +52,7 @@ public abstract class FolderPermissionBase {
   protected static Path warehouseDir;
   protected static Path baseDfsDir;
 
-  public static final PathFilter hiddenFileFilter = new PathFilter(){
+  protected static final PathFilter hiddenFileFilter = new PathFilter(){
     public boolean accept(Path p){
       String name = p.getName();
       return !name.startsWith("_") && !name.startsWith(".");
@@ -591,7 +591,7 @@ private void assertExistence(String locn) throws Exception {
 
   private List<String> listStatus(String locn) throws Exception {
     List<String> results = new ArrayList<String>();
-    FileStatus[] listStatus = fs.listStatus(new Path(locn));
+    FileStatus[] listStatus = fs.listStatus(new Path(locn), hiddenFileFilter);
     for (FileStatus status : listStatus) {
       results.add(status.getPath().toString());
     }

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizerFactoryForTest.java
Patch:
@@ -37,7 +37,7 @@ public HiveAuthorizer createHiveAuthorizer(HiveMetastoreClientFactory metastoreC
     return new HiveAuthorizerImpl(
         privilegeManager,
         new SQLStdHiveAuthorizationValidatorForTest(metastoreClientFactory, conf, authenticator,
-            privilegeManager)
+            privilegeManager, ctx)
         );
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizerFactory.java
Patch:
@@ -37,7 +37,7 @@ public HiveAuthorizer createHiveAuthorizer(HiveMetastoreClientFactory metastoreC
     return new HiveAuthorizerImpl(
         privilegeManager,
         new SQLStdHiveAuthorizationValidator(metastoreClientFactory, conf, authenticator,
-            privilegeManager)
+            privilegeManager, ctx)
         );
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -115,6 +115,7 @@
 import org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext;
 import org.apache.hadoop.hive.ql.optimizer.optiq.HiveDefaultRelMetadataProvider;
 import org.apache.hadoop.hive.ql.optimizer.optiq.HiveOptiqUtil;
+import org.apache.hadoop.hive.ql.optimizer.optiq.HiveTypeSystemImpl;
 import org.apache.hadoop.hive.ql.optimizer.optiq.OptiqSemanticException;
 import org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable;
 import org.apache.hadoop.hive.ql.optimizer.optiq.TraitsUtil;
@@ -12177,7 +12178,8 @@ private ASTNode getOptimizedAST(Map<String, PrunedPartitionList> partitionCache)
       this.partitionCache = partitionCache;
 
       try {
-        optimizedOptiqPlan = Frameworks.withPlanner(this);
+        optimizedOptiqPlan = Frameworks.withPlanner(this,
+            Frameworks.newConfigBuilder().typeSystem(new HiveTypeSystemImpl()).build());
       } catch (Exception e) {
         if (semanticException != null)
           throw semanticException;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java
Patch:
@@ -129,7 +129,9 @@ private void reparseAndSuperAnalyze(ASTNode tree) throws SemanticException {
     try {
       mTable = db.getTable(tableName[0], tableName[1]);
     } catch (HiveException e) {
-      throw new SemanticException(ErrorMsg.UPDATEDELETE_PARSE_ERROR.getMsg(), e);
+      LOG.error("Failed to find table " + getDotName(tableName) + " got exception " +
+          e.getMessage());
+      throw new SemanticException(ErrorMsg.INVALID_TABLE, getDotName(tableName));
     }
     List<FieldSchema> partCols = mTable.getPartCols();
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/CompressionCodec.java
Patch:
@@ -21,6 +21,8 @@
 import java.nio.ByteBuffer;
 import java.util.EnumSet;
 
+import javax.annotation.Nullable;
+
 interface CompressionCodec {
 
   public enum Modifier {
@@ -62,6 +64,6 @@ boolean compress(ByteBuffer in, ByteBuffer out, ByteBuffer overflow
    * @param modifiers compression modifiers
    * @return codec for use after optional modification
    */
-  CompressionCodec modify(EnumSet<Modifier> modifiers);
+  CompressionCodec modify(@Nullable EnumSet<Modifier> modifiers);
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
Patch:
@@ -485,6 +485,7 @@ public OutStream createStream(int column,
         modifiers = EnumSet.of(Modifier.FASTEST, Modifier.BINARY);
         break;
       default:
+        LOG.warn("Missing ORC compression modifiers for " + kind);
         modifiers = null;
         break;
       }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java
Patch:
@@ -131,6 +131,9 @@ public double getRowCount() {
       }
     }
 
+    if (rowCount == -1)
+      noColsMissingStats.getAndIncrement();
+
     return rowCount;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java
Patch:
@@ -86,7 +86,7 @@ public static HiveProjectRel create(RelNode child, List<? extends RexNode> exps,
     RelOptCluster cluster = child.getCluster();
 
     // 1 Ensure columnNames are unique - OPTIQ-411
-    if (!Util.isDistinct(fieldNames)) {
+    if (fieldNames != null && !Util.isDistinct(fieldNames)) {
       String msg = "Select list contains multiple expressions with the same name." + fieldNames;
       throw new OptiqSemanticException(msg);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -84,7 +84,8 @@ public enum ErrorMsg {
   INVALID_PATH(10027, "Invalid path"),
   ILLEGAL_PATH(10028, "Path is not legal"),
   INVALID_NUMERICAL_CONSTANT(10029, "Invalid numerical constant"),
-  INVALID_ARRAYINDEX_CONSTANT(10030, "Non-constant expressions for array indexes not supported"),
+  INVALID_ARRAYINDEX_TYPE(10030,
+      "Not proper type for index of ARRAY. Currently, only integer type is supported"),
   INVALID_MAPINDEX_CONSTANT(10031, "Non-constant expression for map indexes not supported"),
   INVALID_MAPINDEX_TYPE(10032, "MAP key type does not match index expression type"),
   NON_COLLECTION_TYPE(10033, "[] not valid on non-collection types"),

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.lang.reflect.Method;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.LinkedList;
 import java.util.List;
 
@@ -80,7 +79,7 @@ protected void setUp() {
   }
 
   private void implicit(TypeInfo a, TypeInfo b, boolean convertible) {
-    assertEquals(convertible, FunctionRegistry.implicitConvertable(a,b));
+    assertEquals(convertible, FunctionRegistry.implicitConvertible(a, b));
   }
 
   public void testImplicitConversion() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
Patch:
@@ -225,6 +225,7 @@ public boolean isEmptyTable() {
   @SuppressWarnings("unchecked")
   static InputFormat<WritableComparable, Writable> getInputFormatFromCache(Class inputFormatClass,
       Configuration conf) throws IOException {
+    // TODO: why is this copy-pasted from HiveInputFormat?
     if (!inputFormats.containsKey(inputFormatClass)) {
       try {
         InputFormat<WritableComparable, Writable> newInstance = (InputFormat<WritableComparable, Writable>) ReflectionUtils
@@ -235,7 +236,7 @@ static InputFormat<WritableComparable, Writable> getInputFormatFromCache(Class i
             + inputFormatClass.getName() + " as specified in mapredWork!", e);
       }
     }
-    return inputFormats.get(inputFormatClass);
+    return HiveInputFormat.wrapForLlap(inputFormats.get(inputFormatClass), conf);
   }
 
   private StructObjectInspector getRowInspectorFromTable(TableDesc table) throws Exception {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/StatsNoJobTask.java
Patch:
@@ -239,6 +239,7 @@ private int aggregateStats(ExecutorService threadPool) {
           boolean statsAvailable = false;
           for(FileStatus file: fileList) {
             if (!file.isDir()) {
+              // TODO: do we need to wrap for Llap here? probably later when stats are cached?
               InputFormat<?, ?> inputFormat = (InputFormat<?, ?>) ReflectionUtils.newInstance(
                   table.getInputFormatClass(), jc);
               InputSplit dummySplit = new FileSplit(file.getPath(), 0, 0, new String[] { table

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java
Patch:
@@ -213,6 +213,7 @@ public ROW first() throws HiveException {
         JobConf localJc = getLocalFSJobConfClone(jc);
         if (inputSplits == null) {
           if (this.inputFormat == null) {
+            // TODO: do we need to wrap here?
             inputFormat = ReflectionUtils.newInstance(
                 tblDesc.getInputFileFormatClass(), localJc);
           }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java
Patch:
@@ -132,6 +132,8 @@ public void run(Map<String, LogicalInput> inputs, Map<String, LogicalOutput> out
         rproc = new MapRecordProcessor(jobConf);
         MRInputLegacy mrInput = getMRInput(inputs);
         try {
+          // TODO: This might create oldInputFormat in MRInput.
+          //       We are assuming we don't need to wrap it for Llap.
           mrInput.init();
         } catch (IOException e) {
           throw new RuntimeException("Failed while initializing MRInput", e);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/ColumnVector.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.ql.exec.vector;
 
+import java.io.IOException;
 import java.util.Arrays;
 
 import org.apache.hadoop.io.Writable;
@@ -156,5 +157,7 @@ protected void flattenPush() {
     public void init() {
       // Do nothing by default
     }
+
+    public abstract void visit(ColumnVectorVisitor v) throws IOException;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java
Patch:
@@ -29,8 +29,6 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex;
 import org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry;
-import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;
-import org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue;
 
 /**
  * A tool for printing out the file structure of ORC files.

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java
Patch:
@@ -34,8 +34,8 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.ql.io.orc.OrcProto.Type;
-import org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
+import org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.io.Text;

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
Patch:
@@ -1589,7 +1589,7 @@ public void testSetSearchArgument() throws Exception {
     types.add(builder.build());
     SearchArgument isNull = SearchArgumentFactory.newBuilder()
         .startAnd().isNull("cost").end().build();
-    conf.set(OrcInputFormat.SARG_PUSHDOWN, isNull.toKryo());
+    conf.set(SearchArgumentFactory.SARG_PUSHDOWN, isNull.toKryo());
     conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR,
         "url,cost");
     options.include(new boolean[]{true, true, false, true, false});

File: serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorUtils.java
Patch:
@@ -46,7 +46,7 @@ public void testObjectInspectorUtils() throws Throwable {
       StructObjectInspector soi = (StructObjectInspector) ObjectInspectorUtils
           .getStandardObjectInspector(oi1);
       List<? extends StructField> fields = soi.getAllStructFieldRefs();
-      assertEquals(6, fields.size());
+      assertEquals(10, fields.size());
       assertEquals(fields.get(0), soi.getStructFieldRef("aint"));
 
       // null
@@ -75,7 +75,7 @@ public void testObjectInspectorUtils() throws Throwable {
       assertEquals(c4, soi.getStructFieldData(c, fields.get(4)));
       assertNull(soi.getStructFieldData(c, fields.get(5)));
       ArrayList<Object> cfields = new ArrayList<Object>();
-      for (int i = 0; i < 6; i++) {
+      for (int i = 0; i < 10; i++) {
         cfields.add(soi.getStructFieldData(c, fields.get(i)));
       }
       assertEquals(cfields, soi.getStructFieldsDataAsList(c));

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorFactory.java
Patch:
@@ -26,6 +26,7 @@
 import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 import java.util.concurrent.ConcurrentHashMap;
 
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
@@ -111,7 +112,8 @@ private static ObjectInspector getReflectionObjectInspectorNoCache(Type t,
     if (t instanceof ParameterizedType) {
       ParameterizedType pt = (ParameterizedType) t;
       // List?
-      if (List.class.isAssignableFrom((Class<?>) pt.getRawType())) {
+      if (List.class.isAssignableFrom((Class<?>) pt.getRawType()) ||
+          Set.class.isAssignableFrom((Class<?>) pt.getRawType())) {
         return getStandardListObjectInspector(getReflectionObjectInspector(pt
             .getActualTypeArguments()[0], options));
       }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -11982,7 +11982,8 @@ public RelNode applyPreCBOTransforms(RelNode basePlan, RelMetadataProvider mdPro
 
       basePlan = hepPlan(basePlan, false, mdProvider, new TransitivePredicatesOnJoinRule(
           JoinRelBase.class, HiveFilterRel.DEFAULT_FILTER_FACTORY),
-          RemoveTrivialProjectRule.INSTANCE,
+          // TODO: Enable it after OPTIQ-407 is fixed
+          //RemoveTrivialProjectRule.INSTANCE,
           new HivePartitionPrunerRule(SemanticAnalyzer.this.conf));
 
       RelFieldTrimmer fieldTrimmer = new RelFieldTrimmer(null, HiveProjectRel.DEFAULT_PROJECT_FACTORY,

File: metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStorePartitionSpecs.java
Patch:
@@ -11,7 +11,6 @@
 import org.apache.hadoop.hive.metastore.partition.spec.CompositePartitionSpecProxy;
 import org.apache.hadoop.hive.metastore.partition.spec.PartitionSpecProxy;
 import org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe;
-import org.apache.hadoop.util.ExitUtil;
 import org.junit.AfterClass;
 import org.junit.Assert;
 import org.junit.BeforeClass;
@@ -52,7 +51,7 @@ public void checkPermission(Permission perm, Object context) {
     public void checkExit(int status) {
 
       super.checkExit(status);
-      throw new ExitUtil.ExitException(status, "System.exit() was called. Raising exception. ");
+      throw new RuntimeException("System.exit() was called. Raising exception. ");
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionEdge.java
Patch:
@@ -26,7 +26,6 @@
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.io.DataInputByteBuffer;
 import org.apache.tez.dag.api.EdgeManagerPlugin;
 import org.apache.tez.dag.api.EdgeManagerPluginContext;
 import org.apache.tez.runtime.api.events.DataMovementEvent;

File: ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java
Patch:
@@ -311,7 +311,7 @@ public static boolean isAcid(Path directory,
       String filename = file.getPath().getName();
       if (filename.startsWith(BASE_PREFIX) ||
           filename.startsWith(DELTA_PREFIX)) {
-        if (file.isDirectory()) {
+        if (file.isDir()) {
           return true;
         }
       }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/GroupByShuffler.java
Patch:
@@ -1,13 +1,14 @@
 package org.apache.hadoop.hive.ql.exec.spark;
 
+import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.spark.api.java.JavaPairRDD;
 
 public class GroupByShuffler implements SparkShuffler {
 
   @Override
-  public JavaPairRDD<BytesWritable, Iterable<BytesWritable>> shuffle(
-      JavaPairRDD<BytesWritable, BytesWritable> input, int numPartitions) {
+  public JavaPairRDD<HiveKey, Iterable<BytesWritable>> shuffle(
+      JavaPairRDD<HiveKey, BytesWritable> input, int numPartitions) {
     if (numPartitions > 0) {
       return input.groupByKey(numPartitions);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.util.Iterator;
 
+import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
@@ -28,7 +29,7 @@
 import scala.Tuple2;
 
 public class HiveMapFunction implements PairFlatMapFunction<Iterator<Tuple2<BytesWritable, BytesWritable>>,
-BytesWritable, BytesWritable> {
+    HiveKey, BytesWritable> {
   private static final long serialVersionUID = 1L;
 
   private transient JobConf jobConf;
@@ -40,7 +41,7 @@ public HiveMapFunction(byte[] buffer) {
   }
 
   @Override
-  public Iterable<Tuple2<BytesWritable, BytesWritable>> 
+  public Iterable<Tuple2<HiveKey, BytesWritable>>
   call(Iterator<Tuple2<BytesWritable, BytesWritable>> it) throws Exception {
     if (jobConf == null) {
       jobConf = KryoSerializer.deserializeJobConf(this.buffer);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveVoidFunction.java
Patch:
@@ -18,16 +18,16 @@
 
 package org.apache.hadoop.hive.ql.exec.spark;
 
+import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.spark.api.java.function.VoidFunction;
 
 import scala.Tuple2;
 
 /**
  * Implementation of a voidFunction that does nothing.
- *
  */
-public class HiveVoidFunction implements VoidFunction<Tuple2<BytesWritable, BytesWritable>> {
+public class HiveVoidFunction implements VoidFunction<Tuple2<HiveKey, BytesWritable>> {
   private static final long serialVersionUID = 1L;
 
   private static HiveVoidFunction instance = new HiveVoidFunction();
@@ -40,7 +40,7 @@ private HiveVoidFunction() {
   }
 
   @Override
-  public void call(Tuple2<BytesWritable, BytesWritable> t) throws Exception {
+  public void call(Tuple2<HiveKey, BytesWritable> t) throws Exception {
   }
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/MapTran.java
Patch:
@@ -18,14 +18,15 @@
 
 package org.apache.hadoop.hive.ql.exec.spark;
 
+import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.spark.api.java.JavaPairRDD;
 
-public class MapTran implements SparkTran {
+public class MapTran implements SparkTran<BytesWritable,HiveKey> {
   private HiveMapFunction mapFunc;
 
   @Override
-  public JavaPairRDD<BytesWritable, BytesWritable> transform(
+  public JavaPairRDD<HiveKey, BytesWritable> transform(
       JavaPairRDD<BytesWritable, BytesWritable> input) {
     return input.mapPartitionsToPair(mapFunc);
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkRecordHandler.java
Patch:
@@ -33,7 +33,7 @@
 import java.util.Iterator;
 
 public abstract class SparkRecordHandler {
-  private static final Log LOG = LogFactory.getLog(SparkRecordHandler.class);
+  private final Log LOG = LogFactory.getLog(this.getClass());
 
   // used to log memory usage periodically
   protected final MemoryMXBean memoryMXBean = ManagementFactory.getMemoryMXBean();
@@ -83,7 +83,7 @@ protected void logMemoryInfo() {
     rowNumber++;
     if (rowNumber == nextLogThreshold) {
       long used_memory = memoryMXBean.getHeapMemoryUsage().getUsed();
-      LOG.info("ExecReducer: processing " + rowNumber
+      LOG.info("processing " + rowNumber
         + " rows: used memory = " + used_memory);
       nextLogThreshold = getNextLogThreshold(rowNumber);
     }
@@ -96,7 +96,7 @@ protected void logMemoryInfo() {
    */
   protected void logCloseInfo() {
     long used_memory = memoryMXBean.getHeapMemoryUsage().getUsed();
-    LOG.info("ExecMapper: processed " + rowNumber + " rows: used memory = "
+    LOG.info("processed " + rowNumber + " rows: used memory = "
       + used_memory);
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkShuffler.java
Patch:
@@ -18,12 +18,13 @@
 
 package org.apache.hadoop.hive.ql.exec.spark;
 
+import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.spark.api.java.JavaPairRDD;
 
 public interface SparkShuffler {
 
-  JavaPairRDD<BytesWritable, Iterable<BytesWritable>> shuffle(
-      JavaPairRDD<BytesWritable, BytesWritable> input, int numPartitions);
+  JavaPairRDD<HiveKey, Iterable<BytesWritable>> shuffle(
+      JavaPairRDD<HiveKey, BytesWritable> input, int numPartitions);
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTran.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.spark.api.java.JavaPairRDD;
 
-public interface SparkTran {
-  JavaPairRDD<BytesWritable, BytesWritable> transform(
-      JavaPairRDD<BytesWritable, BytesWritable> input);
+public interface SparkTran<KI extends BytesWritable, KO extends BytesWritable> {
+  JavaPairRDD<KO, BytesWritable> transform(
+      JavaPairRDD<KI, BytesWritable> input);
 }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1797,7 +1797,7 @@ public static enum ConfVars {
         "vertices to the tez application master. These events will be used to prune unnecessary partitions."),
     TEZ_DYNAMIC_PARTITION_PRUNING_MAX_EVENT_SIZE("hive.tez.dynamic.partition.pruning.max.event.size", 1*1024*1024L,
         "Maximum size of events sent by processors in dynamic pruning. If this size is crossed no pruning will take place."),
-    TEZ_DYNAMIC_PARTITION_PRUNING_MAX_DATA_SIZE("hive.tez.dynamic.parition.pruning.max.data.size", 100*1024*1024L,
+    TEZ_DYNAMIC_PARTITION_PRUNING_MAX_DATA_SIZE("hive.tez.dynamic.partition.pruning.max.data.size", 100*1024*1024L,
         "Maximum total data size of events in dynamic pruning.")
     ;
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/RexNodeConverter.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
+import org.apache.hadoop.hive.ql.optimizer.optiq.OptiqSemanticException;
 import org.apache.hadoop.hive.ql.parse.ParseUtils;
 import org.apache.hadoop.hive.ql.parse.RowResolver;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
@@ -211,7 +212,8 @@ private boolean castExprUsingUDFBridge(GenericUDF gUDF) {
     return castExpr;
   }
 
-  private RexNode handleExplicitCast(ExprNodeGenericFuncDesc func, List<RexNode> childRexNodeLst) {
+  private RexNode handleExplicitCast(ExprNodeGenericFuncDesc func,
+    List<RexNode> childRexNodeLst) throws OptiqSemanticException {
     RexNode castExpr = null;
 
     if (childRexNodeLst != null && childRexNodeLst.size() == 1) {

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedClientSideAuthorizationProvider.java
Patch:
@@ -85,7 +85,7 @@ private void setPermissions(String locn, String permissions) throws Exception {
   protected void assertNoPrivileges(CommandProcessorResponse ret){
     assertNotNull(ret);
     assertFalse(0 == ret.getResponseCode());
-    assertTrue(ret.getErrorMessage().indexOf("not permitted") != -1);
+    assertTrue(ret.getErrorMessage().indexOf("AccessControlException") != -1);
   }
 
 

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryFactory.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryListObjectInspector;
 import org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector;
 import org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector;
+import org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryUnionObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
@@ -106,6 +107,8 @@ public static LazyBinaryObject createLazyBinaryObject(ObjectInspector oi) {
       return new LazyBinaryArray((LazyBinaryListObjectInspector) oi);
     case STRUCT:
       return new LazyBinaryStruct((LazyBinaryStructObjectInspector) oi);
+    case UNION:
+      return new LazyBinaryUnion((LazyBinaryUnionObjectInspector) oi);
     }
 
     throw new RuntimeException("Hive LazyBinarySerDe Internal error.");

File: ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
Patch:
@@ -624,10 +624,10 @@ private void setupAuth() {
         authorizerV2 = authorizerFactory.createHiveAuthorizer(new HiveMetastoreClientFactoryImpl(),
             conf, authenticator, authzContextBuilder.build());
 
-        authorizerV2.applyAuthorizationConfigPolicy(conf);
-        // create the create table grants with new config
-        createTableGrants = CreateTableAutomaticGrant.create(conf);
+        authorizerV2.applyAuthorizationConfigPolicy(conf); 
       }
+      // create the create table grants with new config
+      createTableGrants = CreateTableAutomaticGrant.create(conf);
 
     } catch (HiveException e) {
       throw new RuntimeException(e);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/stats/FilterSelectivityEstimator.java
Patch:
@@ -5,7 +5,6 @@
 import org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable;
 import org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveTableScanRel;
 import org.eigenbase.rel.FilterRelBase;
-import org.eigenbase.rel.ProjectRel;
 import org.eigenbase.rel.ProjectRelBase;
 import org.eigenbase.rel.RelNode;
 import org.eigenbase.rel.metadata.RelMetadataQuery;
@@ -14,7 +13,6 @@
 import org.eigenbase.rex.RexCall;
 import org.eigenbase.rex.RexInputRef;
 import org.eigenbase.rex.RexNode;
-import org.eigenbase.rex.RexUtil;
 import org.eigenbase.rex.RexVisitorImpl;
 import org.eigenbase.sql.SqlKind;
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/ExprNodeConverter.java
Patch:
@@ -27,7 +27,6 @@
 import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
-import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.eigenbase.reltype.RelDataType;
 import org.eigenbase.reltype.RelDataTypeField;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java
Patch:
@@ -127,9 +127,8 @@ public void initialize(HiveConf hiveConf) {
     if(HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTIMIZEMETADATAQUERIES)) {
       transformations.add(new StatsOptimizer());
     }
-    if (pctx.getContext().getExplain()
-        && !HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez")
-        && !HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("spark")) {
+    String execEngine = HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE);
+    if ((pctx.getContext().getExplain() || "spark".equals(execEngine)) && !"tez".equals(execEngine)) {
       transformations.add(new AnnotateWithStatistics());
       transformations.add(new AnnotateWithOpTraits());
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java
Patch:
@@ -171,13 +171,14 @@ public void removeUnionOperators(Configuration conf, GenSparkProcContext context
       BaseWork work)
     throws SemanticException {
 
-    Set<Operator<?>> roots = work.getAllRootOperators();
+    List<Operator<?>> roots = new ArrayList<Operator<?>>();
+    roots.addAll(work.getAllRootOperators());
     if (work.getDummyOps() != null) {
       roots.addAll(work.getDummyOps());
     }
 
     // need to clone the plan.
-    Set<Operator<?>> newRoots = Utilities.cloneOperatorTree(conf, roots);
+    List<Operator<?>> newRoots = Utilities.cloneOperatorTree(conf, roots);
 
     // we're cloning the operator plan but we're retaining the original work. That means
     // that root operators have to be replaced with the cloned ops. The replacement map

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java
Patch:
@@ -840,6 +840,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
 
           Map<String, ColStatistics> joinedColStats = Maps.newHashMap();
           Map<Integer, List<String>> joinKeys = Maps.newHashMap();
+          List<Long> rowCounts = Lists.newArrayList();
 
           // get the join keys from parent ReduceSink operators
           for (int pos = 0; pos < parents.size(); pos++) {
@@ -859,6 +860,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
             for (String tabAlias : tableAliases) {
               rowCountParents.put(tabAlias, parentStats.getNumRows());
             }
+            rowCounts.add(parentStats.getNumRows());
 
             // multi-attribute join key
             if (keyExprs.size() > 1) {
@@ -959,8 +961,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
 
           // update join statistics
           stats.setColumnStats(outColStats);
-          long newRowCount = computeNewRowCount(
-              Lists.newArrayList(rowCountParents.values()), denom);
+          long newRowCount = computeNewRowCount(rowCounts, denom);
 
           updateStatsForJoinType(stats, newRowCount, jop, rowCountParents,
               outInTabAlias);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/AbstractFilterStringColLikeStringScalar.java
Patch:
@@ -420,8 +420,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("string"),
-            VectorExpressionDescriptor.ArgumentType.getType("string"))
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToDecimal.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastLongToDate.java
Patch:
@@ -122,6 +122,8 @@ public void evaluate(VectorizedRowBatch batch) {
       case DATE:
         inV.copySelected(batch.selectedInUse, batch.selected, batch.size, outV);
         break;
+      default:
+        throw new Error("Unsupported input type " + inputTypes[0].name());
     }
   }
 
@@ -153,7 +155,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.DATETIME_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastStringToDate.java
Patch:
@@ -154,7 +154,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.STRING)
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastStringToDecimal.java
Patch:
@@ -159,7 +159,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.STRING)
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalToStringUnaryUDF.java
Patch:
@@ -130,7 +130,7 @@ public void setInputColumn(int inputColumn) {
 
   @Override
   public String getOutputType() {
-    return "Decimal";
+    return "String";
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterColAndScalar.java
Patch:
@@ -79,8 +79,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterColOrScalar.java
Patch:
@@ -79,8 +79,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterExprAndExpr.java
Patch:
@@ -55,8 +55,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterExprOrExpr.java
Patch:
@@ -128,8 +128,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterScalarAndColumn.java
Patch:
@@ -79,8 +79,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.SCALAR,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterScalarOrColumn.java
Patch:
@@ -79,8 +79,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.SCALAR,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncDoubleToDecimal.java
Patch:
@@ -137,7 +137,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.DOUBLE)
+            VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncLogWithBaseDoubleToDouble.java
Patch:
@@ -61,8 +61,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.DOUBLE,
-            VectorExpressionDescriptor.ArgumentType.DOUBLE)
+            VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.SCALAR,
             VectorExpressionDescriptor.InputExpressionType.COLUMN);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncLogWithBaseLongToDouble.java
Patch:
@@ -62,8 +62,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.DOUBLE,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.SCALAR,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncLongToDecimal.java
Patch:
@@ -137,7 +137,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncLongToString.java
Patch:
@@ -149,6 +149,6 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     return (new VectorExpressionDescriptor.Builder()).setMode(
         VectorExpressionDescriptor.Mode.PROJECTION).setNumArguments(1).setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN).setArgumentTypes(
-                VectorExpressionDescriptor.ArgumentType.LONG).build();
+                VectorExpressionDescriptor.ArgumentType.INT_FAMILY).build();
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncPowerDoubleToDouble.java
Patch:
@@ -70,8 +70,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.DOUBLE,
-            VectorExpressionDescriptor.ArgumentType.DOUBLE)
+            VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncPowerLongToDouble.java
Patch:
@@ -71,8 +71,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.DOUBLE)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncRand.java
Patch:
@@ -110,7 +110,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncRoundWithNumDigitsDecimalToDecimal.java
Patch:
@@ -149,7 +149,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
         .setNumArguments(2)
         .setArgumentTypes(
             VectorExpressionDescriptor.ArgumentType.DECIMAL,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IsNotNull.java
Patch:
@@ -113,7 +113,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.ANY)
+            VectorExpressionDescriptor.ArgumentType.ALL_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IsNull.java
Patch:
@@ -110,7 +110,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.ANY)
+            VectorExpressionDescriptor.ArgumentType.ALL_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/LongColDivideLongColumn.java
Patch:
@@ -180,8 +180,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/LongColDivideLongScalar.java
Patch:
@@ -147,8 +147,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/LongScalarDivideLongColumn.java
Patch:
@@ -159,8 +159,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.SCALAR,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/LongToStringUnaryUDF.java
Patch:
@@ -140,7 +140,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/NotCol.java
Patch:
@@ -128,7 +128,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/PosModDoubleToDouble.java
Patch:
@@ -61,8 +61,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.DOUBLE)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/PosModLongToLong.java
Patch:
@@ -61,8 +61,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/RoundWithNumDigitsDoubleToDouble.java
Patch:
@@ -64,8 +64,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.DOUBLE,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/SelectColumnIsFalse.java
Patch:
@@ -144,7 +144,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/SelectColumnIsNotNull.java
Patch:
@@ -113,7 +113,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.ANY)
+            VectorExpressionDescriptor.ArgumentType.ALL_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/SelectColumnIsNull.java
Patch:
@@ -111,7 +111,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.ANY)
+            VectorExpressionDescriptor.ArgumentType.ALL_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/SelectColumnIsTrue.java
Patch:
@@ -143,7 +143,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringLength.java
Patch:
@@ -162,7 +162,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.STRING)
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringSubstrColStart.java
Patch:
@@ -250,8 +250,8 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.STRING,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringSubstrColStartLen.java
Patch:
@@ -277,9 +277,9 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(3)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.STRING,
-            VectorExpressionDescriptor.ArgumentType.LONG,
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY,
+            VectorExpressionDescriptor.ArgumentType.INT_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR,

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringUnaryUDF.java
Patch:
@@ -200,7 +200,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.STRING)
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringUnaryUDFDirect.java
Patch:
@@ -142,7 +142,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.STRING)
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFTimestampFieldLong.java
Patch:
@@ -179,6 +179,8 @@ public void evaluate(VectorizedRowBatch batch) {
           }
         }
         break;
+      default:
+        throw new Error("Unsupported input type " + inputTypes[0].name());
     }
   }
 
@@ -218,7 +220,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.LONG)
+            VectorExpressionDescriptor.ArgumentType.DATETIME_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFTimestampFieldString.java
Patch:
@@ -181,7 +181,7 @@ public VectorExpressionDescriptor.Descriptor getDescriptor() {
     b.setMode(VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(1)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.STRING)
+            VectorExpressionDescriptor.ArgumentType.STRING_FAMILY)
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN);
     return b.build();

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToBoolean.java
Patch:
@@ -25,6 +25,8 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToBoolean;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.CastDoubleToBooleanViaDoubleToLong;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.CastLongToBooleanViaLongToLong;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.CastDateToBooleanViaLongToLong;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.CastTimestampToBooleanViaLongToLong;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DateWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@@ -43,6 +45,7 @@
  *
  */
 @VectorizedExpressions({CastLongToBooleanViaLongToLong.class,
+  CastDateToBooleanViaLongToLong.class, CastTimestampToBooleanViaLongToLong.class,
   CastDoubleToBooleanViaDoubleToLong.class, CastDecimalToBoolean.class})
 public class UDFToBoolean extends UDF {
   private final BooleanWritable booleanWritable = new BooleanWritable();

File: common/src/java/org/apache/hadoop/hive/common/ServerUtils.java
Patch:
@@ -25,9 +25,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 
 /**
- *
- * ServerUtils.
- *
+ * ServerUtils (specific to HiveServer version 1)
  */
 public class ServerUtils {
 

File: service/src/java/org/apache/hive/service/cli/session/HiveSessionBase.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hive.service.cli.session;
 
+import java.util.Map;
+
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hive.service.cli.SessionHandle;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java
Patch:
@@ -69,6 +69,7 @@ public class TezProcessor extends AbstractLogicalIOProcessor {
 
   public TezProcessor(ProcessorContext context) {
     super(context);
+    ObjectCache.setupObjectRegistry(context.getObjectRegistry());
   }
 
   @Override

File: beeline/src/java/org/apache/hive/beeline/ReflectiveCommandHandler.java
Patch:
@@ -40,13 +40,15 @@ public ReflectiveCommandHandler(BeeLine beeLine, String[] cmds, Completor[] comp
   }
 
   public boolean execute(String line) {
+    lastException = null;
     try {
       Object ob = beeLine.getCommands().getClass().getMethod(getName(),
           new Class[] {String.class})
           .invoke(beeLine.getCommands(), new Object[] {line});
       return ob != null && ob instanceof Boolean
           && ((Boolean) ob).booleanValue();
     } catch (Throwable e) {
+      lastException = e;
       return beeLine.error(e);
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java
Patch:
@@ -120,7 +120,7 @@ void run(HiveConf conf, String jobName, Table t, StorageDescriptor sd,
     job.setBoolean(IS_MAJOR, isMajor);
     job.setBoolean(IS_COMPRESSED, sd.isCompressed());
     job.set(TABLE_PROPS, new StringableMap(t.getParameters()).toString());
-    job.setInt(NUM_BUCKETS, sd.getBucketColsSize());
+    job.setInt(NUM_BUCKETS, sd.getNumBuckets());
     job.set(ValidTxnList.VALID_TXNS_KEY, txns.toString());
     setColumnTypes(job, sd.getCols());
 

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/DataType.java
Patch:
@@ -224,7 +224,7 @@ private static int compareByteArray(byte[] o1, byte[] o2) {
       if (o1[i] == o2[i]) {
         continue;
       }
-      if (o1[i] > o1[i]) {
+      if (o1[i] > o2[i]) {
         return 1;
       } else {
         return -1;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/RelNodeConverter.java
Patch:
@@ -644,7 +644,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
       }
       RelDataType rowType = TypeConverter.getType(ctx.cluster, rr, neededCols);
       RelOptHiveTable optTable = new RelOptHiveTable(ctx.schema, tableScanOp.getConf().getAlias(),
-          rowType, ctx.sA.getTable(tableScanOp), null, null, null, null);
+          rowType, ctx.sA.getTable(tableScanOp), null, null, null, null, null);
       TableAccessRelBase tableRel = new HiveTableScanRel(ctx.cluster,
           ctx.cluster.traitSetOf(HiveRel.CONVENTION), optTable, rowType);
       ctx.buildColumnMap(tableScanOp, tableRel);

File: common/src/java/org/apache/hadoop/hive/ant/GenHiveTemplate.java
Patch:
@@ -108,7 +108,7 @@ private Document generateTemplate() throws Exception {
         continue;
       }
       Element property = appendElement(root, "property", null);
-      appendElement(property, "key", confVars.varname);
+      appendElement(property, "name", confVars.varname);
       appendElement(property, "value", confVars.getDefaultExpr());
       appendElement(property, "description", normalize(confVars.getDescription()));
       // wish to add new line here.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java
Patch:
@@ -130,7 +130,7 @@ public void run(Map<String, LogicalInput> inputs, Map<String, LogicalOutput> out
       LOG.info("Running task: " + getContext().getUniqueIdentifier());
 
       if (isMap) {
-        rproc = new MapRecordProcessor();
+        rproc = new MapRecordProcessor(jobConf);
         MRInputLegacy mrInput = getMRInput(inputs);
         try {
           mrInput.init();
@@ -201,6 +201,7 @@ void initialize() throws Exception {
       this.writer = (KeyValueWriter) output.getWriter();
     }
 
+    @Override
     public void collect(Object key, Object value) throws IOException {
       writer.write(key, value);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
Patch:
@@ -134,7 +134,7 @@ public int execute(DriverContext driverContext) {
       }
 
       List<LocalResource> additionalLr = session.getLocalizedResources();
-      
+
       // log which resources we're adding (apart from the hive exec)
       if (LOG.isDebugEnabled()) {
         if (additionalLr == null || additionalLr.size() == 0) {
@@ -165,7 +165,7 @@ public int execute(DriverContext driverContext) {
       counters = client.getDAGStatus(statusGetOpts).getDAGCounters();
       TezSessionPoolManager.getInstance().returnSession(session);
 
-      if (LOG.isInfoEnabled()) {
+      if (LOG.isInfoEnabled() && counters != null) {
         for (CounterGroup group: counters) {
           LOG.info(group.getDisplayName() +":");
           for (TezCounter counter: group) {

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java
Patch:
@@ -249,6 +249,8 @@ private void createTempTable(org.apache.hadoop.hive.metastore.api.Table tbl,
               + " is not a directory or unable to create one");
         }
       }
+      // Make sure location string is in proper format
+      tbl.getSd().setLocation(tblPath.toString());
     }
 
     // Add temp table info to current session

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java
Patch:
@@ -45,7 +45,6 @@ public HiveReduceFunction(byte[] buffer) {
   call(Iterator<Tuple2<BytesWritable,Iterable<BytesWritable>>> it) throws Exception {
     if (jobConf == null) {
       jobConf = KryoSerializer.deserializeJobConf(this.buffer);
-      jobConf.set("mapred.reducer.class", ExecReducer.class.getName());
     }
 
     SparkReduceRecordHandler reducerRecordhandler = new SparkReduceRecordHandler();

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFStreamingEvaluator.java
Patch:
@@ -179,6 +179,7 @@ public Object terminate(AggregationBuffer agg) throws HiveException {
 
       for (int i = 0; i < ss.numFollowing; i++) {
         ss.results.add(getNextResult(ss));
+        ss.numRows++;
       }
       return o;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -143,6 +143,7 @@ public Vectorizer() {
     patternBuilder.append("|short");
     patternBuilder.append("|timestamp");
     patternBuilder.append("|boolean");
+    patternBuilder.append("|binary");
     patternBuilder.append("|string");
     patternBuilder.append("|byte");
     patternBuilder.append("|float");

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/AnnotateWithStatistics.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.ql.exec.GroupByOperator;
 import org.apache.hadoop.hive.ql.exec.LimitOperator;
 import org.apache.hadoop.hive.ql.exec.MapJoinOperator;
+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
 import org.apache.hadoop.hive.ql.exec.SelectOperator;
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
 import org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher;
@@ -62,6 +63,8 @@ public ParseContext transform(ParseContext pctx) throws SemanticException {
         + MapJoinOperator.getOperatorName() + "%"), StatsRulesProcFactory.getJoinRule());
     opRules.put(new RuleRegExp("LIM", LimitOperator.getOperatorName() + "%"),
         StatsRulesProcFactory.getLimitRule());
+    opRules.put(new RuleRegExp("RS", ReduceSinkOperator.getOperatorName() + "%"),
+        StatsRulesProcFactory.getReduceSinkRule());
 
     // The dispatcher fires the processor corresponding to the closest matching
     // rule and passes the context along

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1098,7 +1098,9 @@ public static enum ConfVars {
         "Whether queries will fail because stats cannot be collected completely accurately. \n" +
         "If this is set to true, reading/writing from/into a partition may fail because the stats\n" +
         "could not be computed accurately."),
-
+    HIVE_STATS_COLLECT_PART_LEVEL_STATS("hive.analyze.stmt.collect.partlevel.stats", true,
+        "analyze table T compute statistics for columns. Queries like these should compute partition"
+        + "level stats for partitioned table even when no part spec is specified."),
     HIVE_STATS_GATHER_NUM_THREADS("hive.stats.gather.num.threads", 10,
         "Number of threads used by partialscan/noscan analyze command for partitioned tables.\n" +
         "This is applicable only for file formats that implement StatsProvidingRecordReader (like ORC)."),

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -4008,7 +4008,7 @@ public boolean drop_role(final String roleName)
       incrementCounter("drop_role");
       firePreEvent(new PreAuthorizationCallEvent(this));
       if (ADMIN.equals(roleName) || PUBLIC.equals(roleName)) {
-        throw new MetaException(PUBLIC + "/" + ADMIN +" role can't be dropped.");
+        throw new MetaException(PUBLIC + "," + ADMIN + " roles can't be dropped.");
       }
       Boolean ret = null;
       try {
@@ -4078,6 +4078,7 @@ private boolean revoke_role(final String roleName, final String userName,
       return ret;
     }
 
+    @Override
     public GrantRevokeRoleResponse grant_revoke_role(GrantRevokeRoleRequest request)
         throws MetaException, org.apache.thrift.TException {
       GrantRevokeRoleResponse response = new GrantRevokeRoleResponse();

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java
Patch:
@@ -412,5 +412,8 @@ public static void assertNoDeniedPermissions(HivePrincipal hivePrincipal,
     }
   }
 
+  static HiveAuthzPluginException getPluginException(String prefix, Exception e) {
+    return new HiveAuthzPluginException(prefix + ": " + e.getMessage(), e);
+  }
 
 }

File: contrib/src/java/org/apache/hadoop/hive/contrib/metastore/hooks/TestURLHook.java
Patch:
@@ -28,7 +28,8 @@
  */
 public class TestURLHook implements JDOConnectionURLHook {
 
-  static String originalUrl = null;
+  private String originalUrl;
+
   @Override
   public String getJdoConnectionUrl(Configuration conf) throws Exception {
     if (originalUrl == null) {

File: itests/hive-unit/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java
Patch:
@@ -178,7 +178,6 @@ private MiniHS2(HiveConf hiveConf, boolean useMiniMR, boolean useMiniKdc, String
     hiveConf.setVar(ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST, getHost());
     hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_PORT, getBinaryPort());
     hiveConf.setIntVar(ConfVars.HIVE_SERVER2_THRIFT_HTTP_PORT, getHttpPort());
-    HiveMetaStore.HMSHandler.resetDefaultDBFlag();
 
     Path scratchDir = new Path(baseDfsDir, "scratch");
     fs.mkdirs(scratchDir);

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java
Patch:
@@ -42,9 +42,9 @@ public class TestMetastoreVersion extends TestCase {
   @Override
   protected void setUp() throws Exception {
     super.setUp();
-    Field defDb = HiveMetaStore.HMSHandler.class.getDeclaredField("createDefaultDB");
+    Field defDb = HiveMetaStore.HMSHandler.class.getDeclaredField("currentUrl");
     defDb.setAccessible(true);
-    defDb.setBoolean(null, false);
+    defDb.set(null, null);
     hiveConf = new HiveConf(this.getClass());
     System.setProperty("hive.metastore.event.listeners",
         DummyListener.class.getName());

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/DefaultHiveAuthorizationProvider.java
Patch:
@@ -19,15 +19,14 @@
 package org.apache.hadoop.hive.ql.security.authorization;
 
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 
 public class DefaultHiveAuthorizationProvider extends
     BitSetCheckedAuthorizationProvider {
 
   public void init(Configuration conf) throws HiveException {
-    hive_db = new HiveProxy(Hive.get(new HiveConf(conf, HiveAuthorizationProvider.class)));
+    hive_db = new HiveProxy(Hive.get(conf, HiveAuthorizationProvider.class));
   }
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java
Patch:
@@ -35,7 +35,6 @@
 import org.apache.hadoop.fs.permission.FsAction;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.hdfs.DFSConfigKeys;
-import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler;
 import org.apache.hadoop.hive.metastore.Warehouse;
 import org.apache.hadoop.hive.metastore.api.Database;
@@ -83,7 +82,7 @@ private void initWh() throws MetaException, HiveException {
         // till we explicitly initialize it as being from the client side. So, we have a
         // chicken-and-egg problem. So, we now track whether or not we're running from client-side
         // in the SBAP itself.
-        hive_db = new HiveProxy(Hive.get(new HiveConf(getConf(), StorageBasedAuthorizationProvider.class)));
+        hive_db = new HiveProxy(Hive.get(getConf(), StorageBasedAuthorizationProvider.class));
         this.wh = new Warehouse(getConf());
         if (this.wh == null){
           // If wh is still null after just having initialized it, bail out - something's very wrong.
@@ -117,7 +116,7 @@ public void authorize(Privilege[] readRequiredPriv, Privilege[] writeRequiredPri
 
     // Update to previous comment: there does seem to be one place that uses this
     // and that is to authorize "show databases" in hcat commandline, which is used
-    // by webhcat. And user-level auth seems to be a resonable default in this case.
+    // by webhcat. And user-level auth seems to be a reasonable default in this case.
     // The now deprecated HdfsAuthorizationProvider in hcatalog approached this in
     // another way, and that was to see if the user had said above appropriate requested
     // privileges for the hive root warehouse directory. That seems to be the best

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveRoleGrant.java
Patch:
@@ -123,5 +123,7 @@ public int compareTo(HiveRoleGrant other) {
 
   }
 
-
+  public String toString() {
+    return roleName + "[" + principalName + ":" + principalType + (grantOption ? ":WITH GRANT]" : "]");
+  }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java
Patch:
@@ -105,6 +105,7 @@ private void initUserRoles() throws HiveAuthzPluginException {
     }
     this.currentUserName = newUserName;
     this.currentRoles = getRolesFromMS();
+    LOG.info("Current user : " + currentUserName + ", Current Roles : " + currentRoles);
   }
 
   private List<HiveRoleGrant> getRolesFromMS() throws HiveAuthzPluginException {
@@ -532,6 +533,7 @@ public void setCurrentRole(String roleName) throws HiveAccessControlException,
       currentRoles.add(adminRole);
       return;
     }
+    LOG.info("Current user : " + currentUserName + ", Current Roles : " + currentRoles);
     // If we are here it means, user is requesting a role he doesn't belong to.
     throw new HiveAccessControlException(currentUserName +" doesn't belong to role "
       +roleName);

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHive.java
Patch:
@@ -509,6 +509,7 @@ public void testIndex() throws Throwable {
 
   public void testHiveRefreshOnConfChange() throws Throwable{
     Hive prevHiveObj = Hive.get();
+    prevHiveObj.getDatabaseCurrent();
     Hive newHiveObj;
 
     //if HiveConf has not changed, same object should be returned
@@ -522,6 +523,7 @@ public void testHiveRefreshOnConfChange() throws Throwable{
 
     //if HiveConf has changed, new object should be returned
     prevHiveObj = Hive.get();
+    prevHiveObj.getDatabaseCurrent();
     //change value of a metavar config param in new hive conf
     newHconf = new HiveConf(hiveConf);
     newHconf.setIntVar(ConfVars.METASTORETHRIFTCONNECTIONRETRIES,

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/DerivedTableInjector.java
Patch:
@@ -5,7 +5,6 @@
 
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveAggregateRel;
-import org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveJoinRel;
 import org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveProjectRel;
 import org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveSortRel;
 import org.eigenbase.rel.AggregateRelBase;
@@ -48,7 +47,7 @@ private static void convertOpTree(RelNode rel, RelNode parent) {
       // TODO: replace with null scan
     } else if (rel instanceof HepRelVertex) {
       // TODO: is this relevant?
-    } else if (rel instanceof HiveJoinRel) {
+    } else if (rel instanceof JoinRelBase) {
       if (!validJoinParent(rel, parent)) {
         introduceDerivedTable(rel, parent);
       }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -229,6 +229,7 @@
 import org.eigenbase.rel.JoinRelType;
 import org.eigenbase.rel.RelCollation;
 import org.eigenbase.rel.RelCollationImpl;
+import org.eigenbase.rel.RelFactories;
 import org.eigenbase.rel.RelFieldCollation;
 import org.eigenbase.rel.RelNode;
 import org.eigenbase.rel.metadata.CachingRelMetadataProvider;
@@ -11869,7 +11870,7 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu
       } else {
         final HepProgram hepPgm = new HepProgramBuilder().addMatchOrder(HepMatchOrder.BOTTOM_UP)
             .addRuleInstance(new ConvertMultiJoinRule(HiveJoinRel.class))
-            .addRuleInstance(LoptOptimizeJoinRule.INSTANCE).build();
+            .addRuleInstance(new LoptOptimizeJoinRule(HiveJoinRel.HIVE_JOIN_FACTORY)).build();
 
         HepPlanner hepPlanner = new HepPlanner(hepPgm);
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java
Patch:
@@ -31,7 +31,7 @@
 public class HiveMapFunction implements PairFlatMapFunction<Iterator<Tuple2<BytesWritable, BytesWritable>>,
 BytesWritable, BytesWritable> {
   private static final long serialVersionUID = 1L;
-  
+
   private transient ExecMapper mapper;
   private transient SparkCollector collector;
   private transient JobConf jobConf;
@@ -58,9 +58,9 @@ public HiveMapFunction(byte[] buffer) {
       System.out.println("mapper input: " + input._1() + ", " + input._2());
       mapper.map(input._1(), input._2(), collector, Reporter.NULL);
     }
-    
+
     mapper.close();
     return collector.getResult();
   }
-  
+
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveVoidFunction.java
Patch:
@@ -29,13 +29,13 @@
  */
 public class HiveVoidFunction implements VoidFunction<Tuple2<BytesWritable, BytesWritable>> {
   private static final long serialVersionUID = 1L;
-  
+
   private static HiveVoidFunction instance = new HiveVoidFunction();
 
   public static HiveVoidFunction getInstance() {
     return instance;
   }
-  
+
   private HiveVoidFunction() {
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/KryoSerializer.java
Patch:
@@ -73,7 +73,7 @@ public static byte[] serializeJobConf(JobConf jobConf) {
     return out.toByteArray();
 
   }
-  
+
   public static JobConf deserializeJobConf(byte[] buffer) {
     JobConf conf = new JobConf();
     try {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkCollector.java
Patch:
@@ -32,19 +32,19 @@ public class SparkCollector implements OutputCollector<BytesWritable, BytesWrita
   private static final long serialVersionUID = 1L;
 
   private List<Tuple2<BytesWritable, BytesWritable>> result = new ArrayList<Tuple2<BytesWritable, BytesWritable>>();
-  
+
   @Override
   public void collect(BytesWritable key, BytesWritable value) throws IOException {
     result.add(new Tuple2<BytesWritable, BytesWritable>(copyBytesWritable(key), copyBytesWritable(value)));
   }
-  
+
   // TODO: Move this to a utility class.
   public static BytesWritable copyBytesWritable(BytesWritable bw) {
     BytesWritable copy = new BytesWritable();
     copy.set(bw);
     return copy;
   }
-  
+
   public void clear() {
     result.clear();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java
Patch:
@@ -46,7 +46,7 @@ public int execute(DriverContext driverContext) {
     }
     return rc;
   }
-  
+
   /**
    * close will move the temp files into the right place for the fetch
    * task. If the job has failed it will clean up the files.
@@ -64,7 +64,7 @@ private int close(int rc) {
       if (rc == 0) {
         rc = 3;
         String mesg = "Job Commit failed with exception '"
-          + Utilities.getNameMessage(e) + "'";
+            + Utilities.getNameMessage(e) + "'";
         console.printError(mesg, "\n" + StringUtils.stringifyException(e));
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
Patch:
@@ -104,9 +104,9 @@ public ParquetRecordReaderWrapper(
     } else {
       realReader = null;
       eof = true;
-      if (valueObj == null) { // Should initialize the value for createValue
-        valueObj = new ArrayWritable(Writable.class, new Writable[schemaSize]);
-      }
+    }
+    if (valueObj == null) { // Should initialize the value for createValue
+      valueObj = new ArrayWritable(Writable.class, new Writable[schemaSize]);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java
Patch:
@@ -162,7 +162,7 @@ public int execute(DriverContext driverContext, SparkWork sparkWork) {
 
     List<Path> inputPaths;
     try {
-      inputPaths = Utilities.getInputPaths(jobConf, mapWork, emptyScratchDir, ctx);
+      inputPaths = Utilities.getInputPaths(jobConf, mapWork, emptyScratchDir, ctx, false);
     } catch (Exception e2) {
       e2.printStackTrace();
       return -1;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1261,14 +1261,14 @@ public static enum ConfVars {
 
     HIVEOUTERJOINSUPPORTSFILTERS("hive.outerjoin.supports.filters", true, ""),
 
-    HIVEFETCHTASKCONVERSION("hive.fetch.task.conversion", "minimal", new StringSet("minimal", "more"),
+    HIVEFETCHTASKCONVERSION("hive.fetch.task.conversion", "more", new StringSet("minimal", "more"),
         "Some select queries can be converted to single FETCH task minimizing latency.\n" +
         "Currently the query should be single sourced not having any subquery and should not have\n" +
         "any aggregations or distincts (which incurs RS), lateral views and joins.\n" +
         "1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only\n" +
         "2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)\n"
     ),
-    HIVEFETCHTASKCONVERSIONTHRESHOLD("hive.fetch.task.conversion.threshold", -1l,
+    HIVEFETCHTASKCONVERSIONTHRESHOLD("hive.fetch.task.conversion.threshold", 1073741824L,
         "Input threshold for applying hive.fetch.task.conversion. If target table is native, input length\n" +
         "is calculated by summation of file lengths. If it's not native, storage handler for the table\n" +
         "can optionally implement org.apache.hadoop.hive.ql.metadata.InputEstimator interface."),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -198,6 +198,7 @@
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.ToolRunner;
+import org.apache.hive.common.util.AnnotationUtils;
 import org.stringtemplate.v4.ST;
 
 /**
@@ -3214,7 +3215,7 @@ private int describeFunction(DescFunctionDesc descFunc) throws HiveException {
         funcClass = functionInfo.getFunctionClass();
       }
       if (funcClass != null) {
-        desc = funcClass.getAnnotation(Description.class);
+        desc = AnnotationUtils.getAnnotation(funcClass, Description.class);
       }
       if (desc != null) {
         outStream.writeBytes(desc.value().replace("_FUNC_", funcName));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/WindowFunctionInfo.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFResolver;
+import org.apache.hive.common.util.AnnotationUtils;
 
 @SuppressWarnings("deprecation")
 public class WindowFunctionInfo implements CommonFunctionInfo
@@ -33,7 +34,8 @@ public class WindowFunctionInfo implements CommonFunctionInfo
 		assert fInfo.isGenericUDAF();
 		this.fInfo = fInfo;
 		Class<? extends GenericUDAFResolver> wfnCls = fInfo.getGenericUDAFResolver().getClass();
-		WindowFunctionDescription def = wfnCls.getAnnotation(WindowFunctionDescription.class);
+		WindowFunctionDescription def =
+          AnnotationUtils.getAnnotation(wfnCls, WindowFunctionDescription.class);
 		if ( def != null)
 		{
 			supportsWindow = def.supportsWindow();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExpressionDescriptor.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hive.common.util.AnnotationUtils;
 
 /**
  * Describes a vector expression and encapsulates the {@link Mode}, number of arguments,
@@ -219,7 +220,8 @@ public String toString() {
   }
 
   public Class<?> getVectorExpressionClass(Class<?> udf, Descriptor descriptor) throws HiveException {
-    VectorizedExpressions annotation = udf.getAnnotation(VectorizedExpressions.class);
+    VectorizedExpressions annotation =
+        AnnotationUtils.getAnnotation(udf, VectorizedExpressions.class);
     if (annotation == null || annotation.value() == null) {
       return null;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java
Patch:
@@ -72,6 +72,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
 import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hive.common.util.AnnotationUtils;
 import org.apache.thrift.TException;
 
 import com.google.common.collect.Lists;
@@ -229,7 +230,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
             // our stats for NDV is approx, not accurate.
             return null;
           }
-          if (aggr.getGenericUDAFName().equals(GenericUDAFSum.class.getAnnotation(
+          if (aggr.getGenericUDAFName().equals(AnnotationUtils.getAnnotation(GenericUDAFSum.class,
               Description.class).name())) {
             if(!(aggr.getParameters().get(0) instanceof ExprNodeConstantDesc)){
               return null;
@@ -243,7 +244,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
             ois.add(PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(
                 PrimitiveCategory.DECIMAL));
           }
-          else if (aggr.getGenericUDAFName().equals(GenericUDAFCount.class.getAnnotation(
+          else if (aggr.getGenericUDAFName().equals(AnnotationUtils.getAnnotation(GenericUDAFCount.class,
               Description.class).name())) {
             Long rowCnt = 0L;
             if ((aggr.getParameters().isEmpty() || aggr.getParameters().get(0) instanceof

File: ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java
Patch:
@@ -23,6 +23,7 @@
 
 import org.apache.hadoop.hive.ql.udf.UDFType;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
+import org.apache.hive.common.util.AnnotationUtils;
 
 /**
  * GroupByDesc.
@@ -228,7 +229,7 @@ public boolean isDistinctLike() {
     for (AggregationDesc ad : aggregators) {
       if (!ad.getDistinct()) {
         GenericUDAFEvaluator udafEval = ad.getGenericUDAFEvaluator();
-        UDFType annot = udafEval.getClass().getAnnotation(UDFType.class);
+        UDFType annot = AnnotationUtils.getAnnotation(udafEval.getClass(), UDFType.class);
         if (annot == null || !annot.distinctLike()) {
           return false;
         }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEvaluator.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef;
 import org.apache.hadoop.hive.ql.udf.UDFType;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hive.common.util.AnnotationUtils;
 
 /**
  * A Generic User-defined aggregation function (GenericUDAF) for the use with
@@ -49,7 +50,7 @@ public abstract class GenericUDAFEvaluator implements Closeable {
   public static boolean isEstimable(AggregationBuffer buffer) {
     if (buffer instanceof AbstractAggregationBuffer) {
       Class<? extends AggregationBuffer> clazz = buffer.getClass();
-      AggregationType annotation = clazz.getAnnotation(AggregationType.class);
+      AggregationType annotation = AnnotationUtils.getAnnotation(clazz, AggregationType.class);
       return annotation != null && annotation.estimable();
     }
     return false;
@@ -94,7 +95,7 @@ public GenericUDAFEvaluator() {
    * Additionally setup GenericUDAFEvaluator with MapredContext before initializing.
    * This is only called in runtime of MapRedTask.
    *
-   * @param context context
+   * @param mapredContext context
    */
   public void configure(MapredContext mapredContext) {
   }

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException;
+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveMetastoreClientFactory;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType;
@@ -43,15 +44,15 @@ public SQLStdHiveAuthorizationValidatorForTest(HiveMetastoreClientFactory metast
 
   @Override
   public void checkPrivileges(HiveOperationType hiveOpType, List<HivePrivilegeObject> inputHObjs,
-      List<HivePrivilegeObject> outputHObjs) throws HiveAuthzPluginException,
+      List<HivePrivilegeObject> outputHObjs, HiveAuthzContext context) throws HiveAuthzPluginException,
       HiveAccessControlException {
     switch (hiveOpType) {
     case DFS:
     case SET:
       // allow SET and DFS commands to be used during testing
       return;
     default:
-      super.checkPrivileges(hiveOpType, inputHObjs, outputHObjs);
+      super.checkPrivileges(hiveOpType, inputHObjs, outputHObjs, context);
     }
 
   }

File: ql/src/java/org/apache/hadoop/hive/ql/processors/CommandUtil.java
Patch:
@@ -68,7 +68,7 @@ static CommandProcessorResponse authorizeCommand(SessionState ss, HiveOperationT
   static void authorizeCommandThrowEx(SessionState ss, HiveOperationType type,
       List<String> command) throws HiveAuthzPluginException, HiveAccessControlException {
     HivePrivilegeObject commandObj = HivePrivilegeObject.createHivePrivilegeObject(command);
-    ss.getAuthorizerV2().checkPrivileges(type, Arrays.asList(commandObj), null);
+    ss.getAuthorizerV2().checkPrivileges(type, Arrays.asList(commandObj), null, null);
   }
 
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
Patch:
@@ -1938,7 +1938,7 @@ private void flushStripe() throws IOException {
       if (availRatio < paddingTolerance && addBlockPadding) {
         long padding = blockSize - (start % blockSize);
         byte[] pad = new byte[(int) Math.min(HDFS_BUFFER_SIZE, padding)];
-        LOG.info(String.format("Padding ORC by %d bytes (<=  %0.2f * %d)", 
+        LOG.info(String.format("Padding ORC by %d bytes (<=  %.2f * %d)", 
             padding, availRatio, defaultStripeSize));
         start += padding;
         while (padding > 0) {

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java
Patch:
@@ -18,7 +18,6 @@
 package org.apache.hadoop.hive.conf;
 
 import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.util.Shell;
 import org.apache.hive.common.util.HiveTestUtils;
@@ -50,7 +49,7 @@ private void checkHadoopConf(String name, String expectedHadoopVal) throws Excep
   }
 
   private void checkConfVar(ConfVars var, String expectedConfVarVal) throws Exception {
-    Assert.assertEquals(expectedConfVarVal, var.defaultVal);
+    Assert.assertEquals(expectedConfVarVal, var.getDefaultValue());
   }
 
   private void checkHiveConf(String name, String expectedHiveVal) throws Exception {
@@ -87,7 +86,7 @@ public void testConfProperties() throws Exception {
     checkHiveConf("test.property1", "hive-site.xml");
 
     // Test HiveConf property variable substitution in hive-site.xml
-    checkHiveConf("test.var.hiveconf.property", ConfVars.DEFAULTPARTITIONNAME.defaultVal);
+    checkHiveConf("test.var.hiveconf.property", ConfVars.DEFAULTPARTITIONNAME.getDefaultValue());
   }
 
   @Test

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveConfRestrictList.java
Patch:
@@ -19,7 +19,6 @@
 
 import junit.framework.TestCase;
 
-import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.junit.Test;
 

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveLogging.java
Patch:
@@ -18,15 +18,13 @@
 package org.apache.hadoop.hive.conf;
 
 import java.io.BufferedReader;
-import java.io.IOException;
 import java.io.InputStreamReader;
 
 import junit.framework.TestCase;
 
 import org.apache.hadoop.hive.common.LogUtils;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hive.common.util.HiveTestUtils;
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 
 /**
  * TestHiveLogging

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.jdbc;
 
 import static org.apache.hadoop.hive.ql.exec.ExplainTask.EXPL_COLUMN_NAME;
-import static org.apache.hadoop.hive.ql.processors.SetProcessor.SET_COLUMN_NAME;
+import static org.apache.hadoop.hive.conf.SystemVariables.SET_COLUMN_NAME;
 
 import java.sql.Connection;
 import java.sql.DatabaseMetaData;

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hive.jdbc;
 
 import static org.apache.hadoop.hive.ql.exec.ExplainTask.EXPL_COLUMN_NAME;
-import static org.apache.hadoop.hive.ql.processors.SetProcessor.SET_COLUMN_NAME;
+import static org.apache.hadoop.hive.conf.SystemVariables.SET_COLUMN_NAME;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotNull;
@@ -1888,7 +1888,7 @@ public void testFetchFirstNonMR() throws Exception {
    */
   @Test
   public void testFetchFirstSetCmds() throws Exception {
-    execFetchFirst("set -v", SetProcessor.SET_COLUMN_NAME, false);
+    execFetchFirst("set -v", SET_COLUMN_NAME, false);
   }
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java
Patch:
@@ -69,6 +69,7 @@ public final class SemanticAnalyzerFactory {
     commandType.put(HiveParser.TOK_SHOWPARTITIONS, HiveOperation.SHOWPARTITIONS);
     commandType.put(HiveParser.TOK_SHOWLOCKS, HiveOperation.SHOWLOCKS);
     commandType.put(HiveParser.TOK_SHOWDBLOCKS, HiveOperation.SHOWLOCKS);
+    commandType.put(HiveParser.TOK_SHOWCONF, HiveOperation.SHOWCONF);
     commandType.put(HiveParser.TOK_CREATEFUNCTION, HiveOperation.CREATEFUNCTION);
     commandType.put(HiveParser.TOK_DROPFUNCTION, HiveOperation.DROPFUNCTION);
     commandType.put(HiveParser.TOK_CREATEMACRO, HiveOperation.CREATEMACRO);
@@ -203,6 +204,7 @@ public static BaseSemanticAnalyzer get(HiveConf conf, ASTNode tree)
       case HiveParser.TOK_SHOWDBLOCKS:
       case HiveParser.TOK_SHOW_COMPACTIONS:
       case HiveParser.TOK_SHOW_TRANSACTIONS:
+      case HiveParser.TOK_SHOWCONF:
       case HiveParser.TOK_CREATEINDEX:
       case HiveParser.TOK_DROPINDEX:
       case HiveParser.TOK_ALTERTABLE_CLUSTER_SORT:

File: ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java
Patch:
@@ -67,6 +67,7 @@ public enum HiveOperation {
   SHOWINDEXES("SHOWINDEXES", null, null),
   SHOWPARTITIONS("SHOWPARTITIONS", null, null),
   SHOWLOCKS("SHOWLOCKS", null, null),
+  SHOWCONF("SHOWCONF", null, null),
   CREATEFUNCTION("CREATEFUNCTION", null, null),
   DROPFUNCTION("DROPFUNCTION", null, null),
   CREATEMACRO("CREATEMACRO", null, null),

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveOperationType.java
Patch:
@@ -69,6 +69,7 @@ public enum HiveOperationType {
   SHOWINDEXES,
   SHOWPARTITIONS,
   SHOWLOCKS,
+  SHOWCONF,
   CREATEFUNCTION,
   DROPFUNCTION,
   CREATEMACRO,

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/Operation2Privilege.java
Patch:
@@ -46,7 +46,7 @@ private static class PrivRequirement {
 
 
     private PrivRequirement(SQLPrivTypeGrant[] privs, IOType ioType) {
-      this(privs, ioType, (HivePrivObjectActionType) null);
+      this(privs, ioType, null);
     }
 
     private PrivRequirement(SQLPrivTypeGrant[] privs, IOType ioType,
@@ -291,6 +291,8 @@ private HivePrivObjectActionType getActionType() {
 (null, null));
     op2Priv.put(HiveOperationType.SHOW_TRANSACTIONS, PrivRequirement.newIOPrivRequirement
 (null, null));
+    op2Priv.put(HiveOperationType.SHOWCONF, PrivRequirement.newIOPrivRequirement
+(null, null));
 
     op2Priv.put(HiveOperationType.LOCKTABLE, PrivRequirement.newIOPrivRequirement
 (null, null));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java
Patch:
@@ -56,7 +56,7 @@ public class SparkClient implements Serializable {
 
   private static String sparkHome = "/home/xzhang/apache/spark";
   
-  private static int reducerCount = 5;
+  private static int reducerCount = 1;
   
   private static String execMem = "1g";
   private static String execJvmOpts = "";

File: ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkCollector.java
Patch:
@@ -35,7 +35,7 @@ public class SparkCollector implements OutputCollector<BytesWritable, BytesWrita
   
   @Override
   public void collect(BytesWritable key, BytesWritable value) throws IOException {
-    result.add(new Tuple2<BytesWritable, BytesWritable>(key, value));
+    result.add(new Tuple2<BytesWritable, BytesWritable>(new BytesWritable(key.copyBytes()), new BytesWritable(value.copyBytes())));
   }
   
   public void clear() {

File: itests/hive-unit-hadoop2/src/test/java/org/apache/hadoop/hive/ql/security/TestExtendedAcls.java
Patch:
@@ -24,7 +24,6 @@
 
 import java.util.List;
 
-import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.permission.AclEntry;
 import org.apache.hadoop.fs.permission.AclEntryScope;
@@ -34,7 +33,6 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.junit.Assert;
 import org.junit.BeforeClass;
-import org.junit.Test;
 
 import com.google.common.collect.Lists;
 

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/FolderPermissionBase.java
Patch:
@@ -28,7 +28,6 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.fs.permission.AclStatus;
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
Patch:
@@ -17,7 +17,8 @@
 import java.sql.Timestamp;
 import java.util.ArrayList;
 
-import org.apache.hadoop.hive.ql.io.parquet.utils.NanoTimeUtils;
+import org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime;
+import org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTimeUtils;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
@@ -30,7 +31,6 @@
 import org.apache.hadoop.io.Writable;
 
 import parquet.column.Dictionary;
-import parquet.example.data.simple.NanoTime;
 import parquet.io.api.Binary;
 import parquet.io.api.Converter;
 import parquet.io.api.PrimitiveConverter;

File: ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java
Patch:
@@ -15,7 +15,8 @@
 
 import java.sql.Timestamp;
 
-import org.apache.hadoop.hive.ql.io.parquet.utils.NanoTimeUtils;
+import org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime;
+import org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTimeUtils;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
@@ -29,7 +30,6 @@
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Writable;
 
-import parquet.example.data.simple.NanoTime;
 import parquet.io.ParquetEncodingException;
 import parquet.io.api.Binary;
 import parquet.io.api.RecordConsumer;

File: ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetTimestampUtils.java
Patch:
@@ -21,9 +21,10 @@
 import junit.framework.Assert;
 import junit.framework.TestCase;
 
-import org.apache.hadoop.hive.ql.io.parquet.utils.NanoTimeUtils;
+import org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime;
+import org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTimeUtils;
+
 
-import parquet.example.data.simple.NanoTime;
 
 /**
  * Tests util-libraries used for parquet-timestamp.

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java
Patch:
@@ -17,7 +17,7 @@
  */
 package org.apache.hadoop.hive.conf;
 
-import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.util.Shell;
@@ -36,7 +36,7 @@ public class TestHiveConf {
   @Test
   public void testHiveSitePath() throws Exception {
     String expectedPath = HiveTestUtils.getFileFromClasspath("hive-site.xml");
-    String hiveSiteLocation = new HiveConf().getHiveSiteLocation().getPath();
+    String hiveSiteLocation = HiveConf.getHiveSiteLocation().getPath();
     if (Shell.WINDOWS) {
       // Do case-insensitive comparison on Windows, as drive letter can have different case.
       expectedPath = expectedPath.toLowerCase();
@@ -46,7 +46,7 @@ public void testHiveSitePath() throws Exception {
   }
 
   private void checkHadoopConf(String name, String expectedHadoopVal) throws Exception {
-    Assert.assertEquals(expectedHadoopVal, new Configuration().get(name));
+    Assert.assertEquals(expectedHadoopVal, new JobConf(HiveConf.class).get(name));
   }
 
   private void checkConfVar(ConfVars var, String expectedConfVarVal) throws Exception {

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.hadoop.util.StringUtils;
 import org.apache.hadoop.util.VersionInfo;
 import org.apache.hive.hcatalog.templeton.tool.JobState;
+import org.apache.hive.hcatalog.templeton.tool.TempletonUtils;
 import org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup;
 import org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage;
 
@@ -313,7 +314,7 @@ public Collection<String> hiveProps() {
     //since raw data was (possibly) escaped to make split work,
     //now need to remove escape chars so they don't interfere with downstream processing
     for(int i = 0; i < props.length; i++) {
-      props[i] = StringUtils.unEscapeString(props[i]);
+      props[i] = TempletonUtils.unEscapeString(props[i]);
     }
     return Arrays.asList(props);
   }

File: hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/tool/TestTempletonUtils.java
Patch:
@@ -306,7 +306,7 @@ public void testPropertiesParsing() throws Exception {
     String[] newProps = StringUtils.split(input.toString());
     for(int i = 0; i < newProps.length; i++) {
       Assert.assertEquals("Pre/post split values don't match",
-        StringUtils.unEscapeString(props[i]), StringUtils.unEscapeString(newProps[i]));
+        TempletonUtils.unEscapeString(props[i]), TempletonUtils.unEscapeString(newProps[i]));
     }
   }
 }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -275,7 +275,7 @@ public static enum ConfVars {
     // Number of seconds the client should wait between connection attempts
     METASTORE_CLIENT_CONNECT_RETRY_DELAY("hive.metastore.client.connect.retry.delay", 1),
     // Socket timeout for the client connection (in seconds)
-    METASTORE_CLIENT_SOCKET_TIMEOUT("hive.metastore.client.socket.timeout", 20),
+    METASTORE_CLIENT_SOCKET_TIMEOUT("hive.metastore.client.socket.timeout", 600),
     METASTOREPWD("javax.jdo.option.ConnectionPassword", "mine"),
     // Class name of JDO connection url hook
     METASTORECONNECTURLHOOK("hive.metastore.ds.connection.url.hook", ""),

File: serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java
Patch:
@@ -199,7 +199,7 @@ private int getBufferIndex(long offset) {
 
   private void nextBufferToWrite() {
     if (currentWriteBufferIndex == (writeBuffers.size() - 1)) {
-      if ((1 + writeBuffers.size()) * wbSize > maxSize) {
+      if ((1 + writeBuffers.size()) * ((long)wbSize) > maxSize) {
         // We could verify precisely at write time, but just do approximate at allocation time.
         throw new RuntimeException("Too much memory used by write buffers");
       }
@@ -283,11 +283,11 @@ public void clear() {
   }
 
   public long getWritePoint() {
-    return (currentWriteBufferIndex * wbSize) + currentWriteOffset;
+    return (currentWriteBufferIndex * (long)wbSize) + currentWriteOffset;
   }
 
   public long getReadPoint() {
-    return (currentReadBufferIndex * wbSize) + currentReadOffset;
+    return (currentReadBufferIndex * (long)wbSize) + currentReadOffset;
   }
 
   public void writeVLong(long value) {

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/LaunchMapper.java
Patch:
@@ -81,7 +81,7 @@ private static void handlePigEnvVars(Configuration conf, Map<String, String> env
     if(conf.get(PigConstants.PIG_OPTS) != null) {
       StringBuilder pigOpts = new StringBuilder();
       for(String prop : StringUtils.split(conf.get(PigConstants.PIG_OPTS))) {
-        pigOpts.append("-D").append(TempletonUtils.unEscape(prop)).append(" ");
+        pigOpts.append("-D").append(StringUtils.unEscapeString(prop)).append(" ");
       }
       env.put(PigConstants.PIG_OPTS, pigOpts.toString());
     }

File: hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/tool/TestTempletonUtils.java
Patch:
@@ -306,7 +306,7 @@ public void testPropertiesParsing() throws Exception {
     String[] newProps = StringUtils.split(input.toString());
     for(int i = 0; i < newProps.length; i++) {
       Assert.assertEquals("Pre/post split values don't match",
-        TempletonUtils.unEscape(props[i]), TempletonUtils.unEscape(newProps[i]));
+        StringUtils.unEscapeString(props[i]), StringUtils.unEscapeString(newProps[i]));
     }
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java
Patch:
@@ -271,7 +271,7 @@ public TezSessionState getSession(TezSessionState session, HiveConf conf,
   public void closeAndOpen(TezSessionState sessionState, HiveConf conf)
       throws Exception {
     HiveConf sessionConf = sessionState.getConf();
-    if (sessionConf.get("tez.queue.name") != null) {
+    if (sessionConf != null && sessionConf.get("tez.queue.name") != null) {
       conf.set("tez.queue.name", sessionConf.get("tez.queue.name"));
     }
     close(sessionState);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java
Patch:
@@ -203,7 +203,7 @@ public void testSubmit() throws Exception {
     task.submit(conf, dag, path, appLr, sessionState);
     // validate close/reopen
     verify(sessionState, times(1)).open(any(HiveConf.class));
-    verify(sessionState, times(1)).close(eq(true));
+    verify(sessionState, times(1)).close(eq(false));  // now uses pool after HIVE-7043
     verify(session, times(2)).submitDAG(any(DAG.class));
   }
 

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java
Patch:
@@ -500,8 +500,8 @@ public String toString() {
       if (txnIds==null || txnIds.isEmpty()) {
         return "{}";
       }
-      return "TxnIds=[" + txnIds.get(0) + "src/gen/thrift" + txnIds.get(txnIds.size()-1)
-              + "] on endPoint= " + endPt;
+      return "TxnIds=[" + txnIds.get(0) + "..." + txnIds.get(txnIds.size()-1)
+              + "] on endPoint = " + endPt;
     }
 
     /**

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
Patch:
@@ -2649,6 +2649,8 @@ static int getIndexPosition(OrcProto.ColumnEncoding.Kind encoding,
       case LIST:
       case UNION:
         return base;
+      case CHAR:
+      case VARCHAR:
       case STRING:
         if (encoding == OrcProto.ColumnEncoding.Kind.DICTIONARY ||
             encoding == OrcProto.ColumnEncoding.Kind.DICTIONARY_V2) {

File: shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
Patch:
@@ -27,7 +27,6 @@
 import java.nio.ByteBuffer;
 import java.security.PrivilegedExceptionAction;
 import java.util.Comparator;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
@@ -631,4 +630,7 @@ public interface DirectDecompressorShim {
    * Get configuration from JobContext
    */
   public Configuration getConfiguration(JobContext context);
+
+  public FileSystem getNonCachedFileSystem(URI uri, Configuration conf) throws IOException;
+
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java
Patch:
@@ -207,6 +207,9 @@ void close(){
 
     // detecting failed executions by exceptions thrown by the operator tree
     try {
+      if (mapOp == null || mapWork == null) {
+        return;
+      }
       mapOp.close(abort);
 
       // Need to close the dummyOps as well. The operator pipeline
@@ -236,5 +239,4 @@ void close(){
       MapredContext.close();
     }
   }
-
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobMonitor.java
Patch:
@@ -78,7 +78,7 @@ public void run() {
         try {
           for (TezSessionState s: TezSessionState.getOpenSessions()) {
             System.err.println("Shutting down tez session.");
-            s.close(false);
+            TezSessionPoolManager.getInstance().close(s);
           }
         } catch (Exception e) {
           // ignore

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -42,6 +42,7 @@
 import java.util.Map.Entry;
 import java.util.Set;
 import java.util.SortedSet;
+import java.util.TreeMap;
 import java.util.TreeSet;
 
 import org.apache.commons.lang.ArrayUtils;
@@ -2370,7 +2371,7 @@ else if (sortCol.getOrder() == BaseSemanticAnalyzer.HIVE_COLUMN_ORDER_DESC) {
 
       // Table properties
       String tbl_properties = "";
-      Map<String, String> properties = tbl.getParameters();
+      Map<String, String> properties = new TreeMap<String, String>(tbl.getParameters());
       if (properties.size() > 0) {
         List<String> realProps = new ArrayList<String>();
         for (String key : properties.keySet()) {
@@ -3310,7 +3311,7 @@ private int showTableProperties(Hive db, ShowTblPropertiesDesc showTblPrpt) thro
         }
       }
       else {
-        Map<String, String> properties = tbl.getParameters();
+        Map<String, String> properties = new TreeMap<String, String>(tbl.getParameters());
         for (Entry<String, String> entry : properties.entrySet()) {
           appendNonNull(builder, entry.getKey(), true);
           appendNonNull(builder, entry.getValue());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java
Patch:
@@ -44,7 +44,6 @@
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.history.HiveHistory.Keys;
 import org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager;
-import org.apache.hadoop.hive.ql.lockmgr.LockException;
 import org.apache.hadoop.hive.ql.plan.ReducerTimeStatsPerJob;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
@@ -550,7 +549,7 @@ public int progress(RunningJob rj, JobClient jc, HiveTxnManager txnMgr) throws I
     // Not always there is a SessionState. Sometimes ExeDriver is directly invoked
     // for special modes. In that case, SessionState.get() is empty.
     if (SessionState.get() != null) {
-      SessionState.get().getLastMapRedStatsList().add(mapRedStats);
+      SessionState.get().getMapRedStats().put(getId(), mapRedStats);
 
       // Computes the skew for all the MapReduce irrespective
       // of Success or Failure

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java
Patch:
@@ -120,7 +120,6 @@ public class AppConfig extends Configuration {
   public static final String PIG_ARCHIVE_NAME    = "templeton.pig.archive";
   public static final String PIG_PATH_NAME       = "templeton.pig.path";
   public static final String STREAMING_JAR_NAME  = "templeton.streaming.jar";
-  public static final String TEMPLETON_JAR_NAME  = "templeton.jar";
   public static final String OVERRIDE_JARS_NAME  = "templeton.override.jars";
   public static final String OVERRIDE_JARS_ENABLED = "templeton.override.enabled";
   public static final String TEMPLETON_CONTROLLER_MR_CHILD_OPTS 
@@ -241,7 +240,6 @@ private boolean loadOneClasspathConfig(String fname) {
     return false;
   }
 
-  public String templetonJar()     { return get(TEMPLETON_JAR_NAME); }
   public String libJars()          { return get(LIB_JARS_NAME); }
   public String hadoopQueueName()  { return get(HADOOP_QUEUE_NAME); }
   public String clusterHadoop()    { return get(HADOOP_NAME); }

File: ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroGenericRecordReader.java
Patch:
@@ -31,7 +31,6 @@
 import org.apache.avro.mapred.FsInput;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.plan.MapWork;
@@ -94,7 +93,6 @@ public AvroGenericRecordReader(JobConf job, FileSplit split, Reporter reporter)
    * @throws AvroSerdeException
    */
   private Schema getSchema(JobConf job, FileSplit split) throws AvroSerdeException, IOException {
-    FileSystem fs = split.getPath().getFileSystem(job);
     // Inside of a MR job, we can pull out the actual properties
     if(AvroSerdeUtils.insideMRJob(job)) {
       MapWork mapWork = Utilities.getMapWork(job);
@@ -155,6 +153,7 @@ public boolean next(NullWritable nullWritable, AvroGenericRecordWritable record)
     GenericData.Record r = (GenericData.Record)reader.next();
     record.setRecord(r);
     record.setRecordReaderID(recordReaderID);
+    record.setFileSchema(reader.getSchema());
 
     return true;
   }

File: serde/src/test/org/apache/hadoop/hive/serde2/avro/TestGenericAvroRecordWritable.java
Patch:
@@ -60,13 +60,15 @@ public void writableContractIsImplementedCorrectly() throws IOException {
     assertEquals("Doctor", gr.get("last"));
 
     AvroGenericRecordWritable garw = new AvroGenericRecordWritable(gr);
+    garw.setFileSchema(gr.getSchema());
     garw.setRecordReaderID(new UID());
 
     ByteArrayOutputStream baos = new ByteArrayOutputStream();
     DataOutputStream daos = new DataOutputStream(baos);
     garw.write(daos);
 
     AvroGenericRecordWritable garw2 = new AvroGenericRecordWritable(gr);
+    garw.setFileSchema(gr.getSchema());
     garw2.setRecordReaderID(new UID());
 
     ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());

File: serde/src/test/org/apache/hadoop/hive/serde2/avro/Utils.java
Patch:
@@ -33,6 +33,8 @@ class Utils {
   serializeAndDeserializeRecord(GenericData.Record record) throws IOException {
     AvroGenericRecordWritable garw = new AvroGenericRecordWritable(record);
     garw.setRecordReaderID(new UID());
+    // Assuming file schema is the same as record schema for testing purpose.
+    garw.setFileSchema(record.getSchema());
     ByteArrayOutputStream baos = new ByteArrayOutputStream();
     DataOutputStream daos = new DataOutputStream(baos);
     garw.write(daos);

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
Patch:
@@ -139,7 +139,7 @@ public void setTTable(org.apache.hadoop.hive.metastore.api.Table tTable) {
   /**
    * Initialize an emtpy table.
    */
-  static org.apache.hadoop.hive.metastore.api.Table
+  public static org.apache.hadoop.hive.metastore.api.Table
   getEmptyTable(String databaseName, String tableName) {
     StorageDescriptor sd = new StorageDescriptor();
     {

File: ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java
Patch:
@@ -442,7 +442,7 @@ public void execute() throws BuildException {
       }
       ctx.put("logDir", relativePath(hiveRootDir, logDir));
       ctx.put("clusterMode", clusterMode);
-      ctx.put("hiveConfDir", hiveConfDir);
+      ctx.put("hiveConfDir", escapePath(hiveConfDir));
       ctx.put("hadoopVersion", hadoopVersion);
 
       File outFile = new File(outDir, className + ".java");

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -350,7 +350,7 @@ public static enum ConfVars {
     METASTORE_AUTHORIZATION_STORAGE_AUTH_CHECKS("hive.metastore.authorization.storage.checks", false),
     METASTORE_EVENT_CLEAN_FREQ("hive.metastore.event.clean.freq",0L),
     METASTORE_EVENT_EXPIRY_DURATION("hive.metastore.event.expiry.duration",0L),
-    METASTORE_EXECUTE_SET_UGI("hive.metastore.execute.setugi", false),
+    METASTORE_EXECUTE_SET_UGI("hive.metastore.execute.setugi", true),
     METASTORE_PARTITION_NAME_WHITELIST_PATTERN(
         "hive.metastore.partition.name.whitelist.pattern", ""),
     // Whether to enable integral JDO pushdown. For partition columns storing integers

File: hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java
Patch:
@@ -61,7 +61,7 @@ protected AbstractRecordWriter(HiveEndPoint endPoint, HiveConf conf)
     this.conf = conf!=null ? conf
                 : HiveEndPoint.createHiveConf(DelimitedInputWriter.class, endPoint.metaStoreUri);
     try {
-      msClient = new HiveMetaStoreClient(conf);
+      msClient = new HiveMetaStoreClient(this.conf);
       this.tbl = msClient.getTable(endPoint.database, endPoint.table);
       this.partitionPath = getPathForEndPoint(msClient, endPoint);
       this.totalBuckets = tbl.getSd().getNumBuckets();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GroupByOptimizer.java
Patch:
@@ -210,7 +210,7 @@ else if (!HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEGROUPBYSKEW)) {
       if (removeReduceSink) {
         convertGroupByMapSideSortedGroupBy(hiveConf, groupByOp, depth);
       }
-      else if (optimizeDistincts) {
+      else if (optimizeDistincts && !HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED)) {
         // In test mode, dont change the query plan. However, setup a query property
         pGraphContext.getQueryProperties().setHasMapGroupBy(true);
         if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_MAP_GROUPBY_SORT_TESTMODE)) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IsNull.java
Patch:
@@ -84,7 +84,7 @@ public void evaluate(VectorizedRowBatch batch) {
 
   @Override
   public int getOutputColumn() {
-    return -1;
+    return outputColumn;
   }
 
   @Override

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizationContext.java
Patch:
@@ -545,6 +545,7 @@ public void testNullExpressions() throws HiveException {
     ve = vc.getVectorExpression(isNullExpr, VectorExpressionDescriptor.Mode.PROJECTION);
     assertEquals(ve.getClass(), IsNull.class);
     assertEquals(2, ((IsNull) ve).getColNum());
+    assertEquals(3, ve.getOutputColumn());
     assertEquals(ve.getChildExpressions()[0].getClass(), LongColGreaterLongScalar.class);
   }
 

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -287,9 +287,6 @@ private void convertPathsFromWindowsToHdfs() {
     String orgTestTempDir = System.getProperty("test.tmp.dir");
     System.setProperty("test.tmp.dir", getHdfsUriString(orgTestTempDir));
 
-    String orgTestDataDir = System.getProperty("test.src.data.dir");
-    System.setProperty("test.src.data.dir", getHdfsUriString(orgTestDataDir));
-
     String orgScratchDir = conf.getVar(HiveConf.ConfVars.SCRATCHDIR);
     conf.setVar(HiveConf.ConfVars.SCRATCHDIR, getHdfsUriString(orgScratchDir));
 

File: ql/src/test/org/apache/hadoop/hive/ql/WindowsPathUtil.java
Patch:
@@ -30,9 +30,6 @@ public static void convertPathsFromWindowsToHdfs(HiveConf conf){
     String orgTestTempDir = System.getProperty("test.tmp.dir");
     System.setProperty("test.tmp.dir", getHdfsUriString(orgTestTempDir));
 
-    String orgTestDataDir = System.getProperty("test.src.data.dir");
-    System.setProperty("test.src.data.dir", getHdfsUriString(orgTestDataDir));
-
     String orgScratchDir = conf.getVar(HiveConf.ConfVars.SCRATCHDIR);
     conf.setVar(HiveConf.ConfVars.SCRATCHDIR, getHdfsUriString(orgScratchDir));
   }

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hive.hcatalog.templeton;
 
 import java.io.File;
-import java.io.StringBufferInputStream;
 import java.net.URL;
 import java.util.ArrayList;
 import java.util.Collections;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnAssignFactory.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.sql.Timestamp;
 import java.util.Arrays;
-import java.util.Date;
 import java.util.List;
 import java.util.Map;
 
@@ -92,7 +91,7 @@ public VectorColumnAssign init(VectorizedRowBatch out, T cv) {
     }
 
     protected void assignNull(int index) {
-      VectorizedBatchUtil.SetNullColIsNullValue(outCol, index);
+      VectorizedBatchUtil.setNullColIsNullValue(outCol, index);
     }
 
     @Override

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
Patch:
@@ -389,7 +389,7 @@ protected static PartitionDesc getPartitionDescFromPath(
     }
     if (partDesc == null) {
       throw new IOException("cannot find dir = " + dir.toString()
-          + " in partToPartitionInfo!");
+          + " in " + pathToPartitionInfo);
     }
 
     return partDesc;

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcOutputFormat.java
Patch:
@@ -305,6 +305,7 @@ public FSRecordWriter getRawRecordWriter(Path path,
       public void write(Writable w) throws IOException {
         OrcStruct orc = (OrcStruct) w;
         watcher.addKey(
+            ((IntWritable) orc.getFieldValue(OrcRecordUpdater.OPERATION)).get(),
             ((LongWritable)
                 orc.getFieldValue(OrcRecordUpdater.ORIGINAL_TRANSACTION)).get(),
             ((IntWritable) orc.getFieldValue(OrcRecordUpdater.BUCKET)).get(),

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
Patch:
@@ -2069,6 +2069,9 @@ public synchronized long writeIntermediateFooter() throws IOException {
     flushStripe();
     // write a footer
     if (stripesAtLastFlush != stripes.size()) {
+      if (callback != null) {
+        callback.preFooterWrite(callbackContext);
+      }
       int metaLength = writeMetadata(rawWriter.getPos());
       int footLength = writeFooter(rawWriter.getPos() - metaLength);
       rawWriter.writeByte(writePostScript(footLength, metaLength));

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java
Patch:
@@ -71,7 +71,7 @@ public class ExprNodeGenericFuncDesc extends ExprNodeDesc implements
   //Is this an expression that should perform a comparison for sorted searches
   private boolean isSortedExpr;
 
-  public ExprNodeGenericFuncDesc() {
+  public ExprNodeGenericFuncDesc() {;
   }
 
   /* If the function has an explicit name like func(args) then call a

File: common/src/java/org/apache/hadoop/hive/common/FileUtils.java
Patch:
@@ -441,6 +441,4 @@ public static boolean isOwnerOfFileHierarchy(FileSystem fs, FileStatus fileStatu
     }
     return true;
   }
-
-
 }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -1020,7 +1020,8 @@ public static enum ConfVars {
     // Check if a plan contains a Cross Product.
     // If there is one, output a warning to the Session's console.
     HIVE_CHECK_CROSS_PRODUCT("hive.exec.check.crossproducts", true),
-
+    HIVE_LOCALIZE_RESOURCE_WAIT_INTERVAL("hive.localize.resource.wait.interval", 5000L), // in ms
+    HIVE_LOCALIZE_RESOURCE_NUM_WAIT_ATTEMPTS("hive.localize.resource.num.wait.attempts", 5),
     ;
 
     public final String varname;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java
Patch:
@@ -118,6 +118,7 @@ private void setupMRLegacyConfigs(TezProcessorContext processorContext) {
     String taskAttemptIdStr = taskAttemptIdBuilder.toString();
     this.jobConf.set("mapred.task.id", taskAttemptIdStr);
     this.jobConf.set("mapreduce.task.attempt.id", taskAttemptIdStr);
+    this.jobConf.setInt("mapred.task.partition",processorContext.getTaskIndex());
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java
Patch:
@@ -133,7 +133,7 @@ public final class SemanticAnalyzerFactory {
     tablePartitionCommandType.put(HiveParser.TOK_ALTERTABLE_RENAMEPART,
         new HiveOperation[] {null, HiveOperation.ALTERTABLE_RENAMEPART});
     tablePartitionCommandType.put(HiveParser.TOK_COMPACT,
-        new HiveOperation[] {null, HiveOperation.ALTERTABLE_COMPACT});
+        new HiveOperation[] {HiveOperation.ALTERTABLE_COMPACT, HiveOperation.ALTERTABLE_COMPACT});
     tablePartitionCommandType.put(HiveParser.TOK_ALTERTBLPART_SKEWED_LOCATION,
         new HiveOperation[] {HiveOperation.ALTERTBLPART_SKEWED_LOCATION,
             HiveOperation.ALTERTBLPART_SKEWED_LOCATION });

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -548,7 +548,7 @@ public static enum ConfVars {
     HIVECONVERTJOINNOCONDITIONALTASK("hive.auto.convert.join.noconditionaltask", true),
     HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD("hive.auto.convert.join.noconditionaltask.size",
         10000000L),
-    HIVECONVERTJOINUSENONSTAGED("hive.auto.convert.join.use.nonstaged", true),
+    HIVECONVERTJOINUSENONSTAGED("hive.auto.convert.join.use.nonstaged", false),
     HIVESKEWJOINKEY("hive.skewjoin.key", 100000),
     HIVESKEWJOINMAPJOINNUMMAPTASK("hive.skewjoin.mapjoin.map.tasks", 10000),
     HIVESKEWJOINMAPJOINMINSPLIT("hive.skewjoin.mapjoin.min.split", 33554432L), //32M

File: ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
Patch:
@@ -387,7 +387,7 @@ private void setupAuth() {
         authorizerV2 = authorizerFactory.createHiveAuthorizer(new HiveMetastoreClientFactoryImpl(),
             getConf(), authenticator);
         // grant all privileges for table to its owner
-        getConf().setVar(ConfVars.HIVE_AUTHORIZATION_TABLE_OWNER_GRANTS, "insert,select,update,delete");
+        getConf().setVar(ConfVars.HIVE_AUTHORIZATION_TABLE_OWNER_GRANTS, "INSERT,SELECT,UPDATE,DELETE");
         String hooks = getConf().getVar(ConfVars.PREEXECHOOKS).trim();
         if (hooks.isEmpty()) {
           hooks = DisallowTransformHook.class.getName();

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
Patch:
@@ -754,7 +754,7 @@ public void run() {
               // eliminate stripes that doesn't satisfy the predicate condition
               includeStripe = new boolean[stripes.size()];
               for(int i=0; i < stripes.size(); ++i) {
-                includeStripe[i] = (i > stripeStats.size()) ||
+                includeStripe[i] = (i >= stripeStats.size()) ||
                     isStripeSatisfyPredicate(stripeStats.get(i), sarg,
                                              filterColumns);
                 if (LOG.isDebugEnabled() && !includeStripe[i]) {

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java
Patch:
@@ -106,7 +106,7 @@ private static ObjectInspector getObjectInspector(TypeInfo type) throws IOExcept
     case PRIMITIVE:
       PrimitiveTypeInfo primitiveType = (PrimitiveTypeInfo) type;
       return PrimitiveObjectInspectorFactory.
-        getPrimitiveJavaObjectInspector(primitiveType.getPrimitiveCategory());
+        getPrimitiveJavaObjectInspector(primitiveType);
 
     case MAP:
       MapTypeInfo mapType = (MapTypeInfo) type;

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/hive_metastoreConstants.java
Patch:
@@ -66,7 +66,7 @@ public class hive_metastoreConstants {
   public static final String META_TABLE_SERDE = "serde";
 
   public static final String META_TABLE_PARTITION_COLUMNS = "partition_columns";
-  
+
   public static final String META_TABLE_PARTITION_COLUMN_TYPES = "partition_columns.types";
 
   public static final String FILE_INPUT_FORMAT = "file.inputformat";

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapper.java
Patch:
@@ -222,7 +222,8 @@ public void assignNullString(int index) {
   }
 
   public void assignDecimal(int index, Decimal128 value) {
-      decimalValues[index].update(value);
+    decimalValues[index].update(value);
+    isNull[longValues.length + doubleValues.length + byteValues.length + index] = false;
   }
 
   public void assignNullDecimal(int index) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -356,7 +356,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
         Object... nodeOutputs) throws SemanticException {
       for (Node n : stack) {
         Operator<? extends OperatorDesc> op = (Operator<? extends OperatorDesc>) n;
-        if (op.getType().equals(OperatorType.REDUCESINK) &&
+        if ((op.getType().equals(OperatorType.REDUCESINK) || op.getType().equals(OperatorType.FILESINK)) &&
             op.getParentOperators().get(0).getType().equals(OperatorType.GROUPBY)) {
           return new Boolean(true);
         }
@@ -450,7 +450,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
 
       assert vContext != null;
 
-      if (op.getType().equals(OperatorType.REDUCESINK) &&
+      if ((op.getType().equals(OperatorType.REDUCESINK) || op.getType().equals(OperatorType.FILESINK)) &&
           op.getParentOperators().get(0).getType().equals(OperatorType.GROUPBY)) {
         // No need to vectorize
         if (!opsDone.contains(op)) {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizationContext.java
Patch:
@@ -965,12 +965,12 @@ public void testBetweenFilters() throws HiveException {
     children1.set(2, new ExprNodeConstantDesc("2013-11-05 00:00:00.000"));
     children1.set(3, new ExprNodeConstantDesc("2013-11-06 00:00:00.000"));
     ve = vc.getVectorExpression(exprDesc, VectorExpressionDescriptor.Mode.FILTER);
-    assertTrue(ve instanceof FilterLongColumnBetween);
+    assertEquals(FilterStringColumnBetween.class, ve.getClass());
 
     // timestamp NOT BETWEEN
     children1.set(0, new ExprNodeConstantDesc(new Boolean(true)));
     ve = vc.getVectorExpression(exprDesc, VectorExpressionDescriptor.Mode.FILTER);
-    assertTrue(ve instanceof FilterLongColumnNotBetween);
+    assertEquals(FilterStringColumnNotBetween.class, ve.getClass());
   }
 
   // Test translation of both IN filters and boolean-valued IN expressions (non-filters).

File: metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java
Patch:
@@ -205,7 +205,7 @@ public OpenTxnsResponse openTxns(OpenTxnRequest rqst) throws MetaException {
 
       Statement stmt = dbConn.createStatement();
       LOG.debug("Going to execute query <select ntxn_next from NEXT_TXN_ID " +
-          "for update>");
+          " for update>");
       ResultSet rs =
           stmt.executeQuery("select ntxn_next from NEXT_TXN_ID for update");
       if (!rs.next()) {
@@ -806,7 +806,7 @@ private LockResponse lock(Connection dbConn, LockRequest rqst, boolean wait)
       if (txnid > 0) {
         // We need to check whether this transaction is valid and open
         String s = "select txn_state from TXNS where txn_id = " +
-            txnid + "for update";
+            txnid + " for update";
         LOG.debug("Going to execute query <" + s + ">");
         ResultSet rs = stmt.executeQuery(s);
         if (!rs.next()) {
@@ -1157,7 +1157,7 @@ private void heartbeatTxn(Connection dbConn, long txnid)
       long now = System.currentTimeMillis();
       // We need to check whether this transaction is valid and open
       String s = "select txn_state from TXNS where txn_id = " +
-          txnid + "for update";
+          txnid + " for update";
       LOG.debug("Going to execute query <" + s + ">");
       ResultSet rs = stmt.executeQuery(s);
       if (!rs.next()) {

File: beeline/src/java/org/apache/hive/beeline/BeeLine.java
Patch:
@@ -567,7 +567,7 @@ boolean initArgs(String[] args) {
       } else if (args[i].equals("-f")) {
         getOpts().setScriptFile(args[i++ + 1]);
       } else {
-        files.add(args[i]);
+        return error(loc("unrecognized-argument", args[i]));
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
Patch:
@@ -1068,7 +1068,7 @@ Object nextVector(Object previousVector, long batchSize) throws IOException {
 
     private static int parseNanos(long serialized) {
       int zeros = 7 & (int) serialized;
-      int result = (int) serialized >>> 3;
+      int result = (int) (serialized >>> 3);
       if (zeros != 0) {
         for(int i =0; i <= zeros; ++i) {
           result *= 10;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -933,7 +933,7 @@ public static enum ConfVars {
 
     HIVE_EXECUTION_ENGINE("hive.execution.engine", "mr",
         new StringsValidator("mr", "tez")),
-    HIVE_JAR_DIRECTORY("hive.jar.directory", "hdfs:///user/hive/"),
+    HIVE_JAR_DIRECTORY("hive.jar.directory", null),
     HIVE_USER_INSTALL_DIR("hive.user.install.directory", "hdfs:///user/"),
 
     // Vectorization enabled

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -914,11 +914,10 @@ private void publishStats() throws HiveException {
       String lbSpec = split[1];
 
       String prefix;
-      String postfix;
+      String postfix=null;
       if (taskIndependent) {
         // key = "database.table/SP/DP/"LB/
         prefix = conf.getTableInfo().getTableName();
-        postfix = Utilities.join(lbSpec);
       } else {
         // key = "prefix/SP/DP/"LB/taskID/
         prefix = conf.getStatsAggPrefix();

File: common/src/test/org/apache/hadoop/hive/common/type/TestUnsignedInt128.java
Patch:
@@ -552,8 +552,6 @@ public void testDivideDestructiveUnsignedInt128Again() {
   public void testBigIntConversion() {
     BigInteger bigInteger = BigInteger.valueOf(0x1ABCDEF0123456L);
     UnsignedInt128 uInt128 = new UnsignedInt128(bigInteger);
-    System.out.println("Out = "+uInt128.toString());
-    System.out.println("Out = "+bigInteger.toString());
     assertEquals(bigInteger, uInt128.toBigIntegerSlow());
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java
Patch:
@@ -38,7 +38,7 @@ public CastDecimalToString(int inputColumn, int outputColumn) {
 
   @Override
   protected void func(BytesColumnVector outV, DecimalColumnVector inV, int i) {
-    String s = inV.vector[i].toFormalString();
+    String s = inV.vector[i].getHiveDecimalString();
     byte[] b = null;
     try {
       b = s.getBytes("UTF-8");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToTimestamp.java
Patch:
@@ -53,7 +53,8 @@ public CastDecimalToTimestamp() {
   @Override
   protected void func(LongColumnVector outV, DecimalColumnVector inV,  int i) {
     tmp.update(inV.vector[i]);
-    tmp.multiplyDestructive(tenE9, (short) 0);
+    int newScale = inV.scale > 9 ? (inV.scale - 9) : 0;
+    tmp.multiplyDestructive(tenE9, (short) newScale);
 
     // set output
     outV.vector[i] = tmp.longValue();

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java
Patch:
@@ -322,17 +322,17 @@ public void testCastDecimalToString() {
     expr.evaluate(b);
     BytesColumnVector r = (BytesColumnVector) b.cols[1];
 
-    byte[] v = toBytes("1.10");
+    byte[] v = toBytes("1.1");
     Assert.assertEquals(0,
         StringExpr.compare(v, 0, v.length,
             r.vector[0], r.start[0], r.length[0]));
 
-    v = toBytes("-2.20");
+    v = toBytes("-2.2");
     Assert.assertEquals(0,
         StringExpr.compare(v, 0, v.length,
             r.vector[1], r.start[1], r.length[1]));
 
-    v = toBytes("9999999999999999.00");
+    v = toBytes("9999999999999999");
     Assert.assertEquals(0,
         StringExpr.compare(v, 0, v.length,
             r.vector[2], r.start[2], r.length[2]));

File: service/src/java/org/apache/hive/service/cli/CLIService.java
Patch:
@@ -423,8 +423,8 @@ private void setupStagingDir(String dirPath, boolean isLocal) throws IOException
     }
     if (!fs.exists(scratchDir)) {
       fs.mkdirs(scratchDir);
-      FsPermission fsPermission = new FsPermission((short)0777);
-      fs.setPermission(scratchDir, fsPermission);
     }
+    FsPermission fsPermission = new FsPermission((short)0777);
+    fs.setPermission(scratchDir, fsPermission);
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java
Patch:
@@ -1045,7 +1045,7 @@ public VectorExpressionWriterSetter init(
     @Override
     public Object writeValue(ColumnVector column, int row)
         throws HiveException {
-      throw new HiveException("Should never reach here");
+      return baseWriter.writeValue(column, row);
     }
 
     @Override

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
Patch:
@@ -647,7 +647,7 @@ private boolean validateExprNodeDescRecursive(ExprNodeDesc desc) {
     boolean ret = validateDataType(typeName);
     if (!ret) {
       if (LOG.isDebugEnabled()) {
-        LOG.debug("Cannot vectorize " + desc.getExprString() + " of type " + typeName);
+        LOG.debug("Cannot vectorize " + desc.toString() + " of type " + typeName);
       }
       return false;
     }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -534,6 +534,9 @@ public static enum ConfVars {
 
     HIVE_ORC_ZEROCOPY("hive.exec.orc.zerocopy", false),
 
+    // Whether extended literal set is allowed for LazySimpleSerde.
+    HIVE_LAZYSIMPLE_EXTENDED_BOOLEAN_LITERAL("hive.lazysimple.extended_boolean_literal", false),
+
     HIVESKEWJOIN("hive.optimize.skewjoin", false),
     HIVECONVERTJOIN("hive.auto.convert.join", true),
     HIVECONVERTJOINNOCONDITIONALTASK("hive.auto.convert.join.noconditionaltask", true),

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -512,9 +512,6 @@ private void createDefaultDB() throws MetaException {
         throw new MetaException(e.getMessage());
       } catch (MetaException e) {
         throw e;
-      } catch (Exception e) {
-        assert (e instanceof RuntimeException);
-        throw (RuntimeException) e;
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFYearLong.java
Patch:
@@ -45,7 +45,7 @@ public final class VectorUDFYearLong extends VectorUDFTimestampFieldLong {
   }
 
   @Override
-  protected long getField(long time) {
+  protected long getTimestampField(long time) {
     /* binarySearch is faster than a loop doing a[i] (no array out of bounds checks) */
     int year = Arrays.binarySearch(YEAR_BOUNDARIES, time);
     if(year >= 0) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
Patch:
@@ -2543,6 +2543,7 @@ static int getIndexPosition(OrcProto.ColumnEncoding.Kind encoding,
       case LONG:
       case FLOAT:
       case DOUBLE:
+      case DATE:
       case STRUCT:
       case MAP:
       case LIST:

File: ql/src/java/org/apache/hadoop/hive/ql/parse/FileSinkProcessor.java
Patch:
@@ -51,7 +51,7 @@ public Object process(Node nd, Stack<Node> stack,
 
     GenTezProcContext context = (GenTezProcContext) procCtx;
     FileSinkOperator fileSink = (FileSinkOperator) nd;
-    
+
     // just remember it for later processing
     context.fileSinkSet.add(fileSink);
     return true;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -3144,8 +3144,10 @@ public static void setInputPaths(JobConf job, List<Path> pathsToAdd) {
    * Set hive input format, and input format file if necessary.
    */
   public static void setInputAttributes(Configuration conf, MapWork mWork) {
+    HiveConf.ConfVars var = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez") ?
+      HiveConf.ConfVars.HIVETEZINPUTFORMAT : HiveConf.ConfVars.HIVEINPUTFORMAT;
     if (mWork.getInputformat() != null) {
-      HiveConf.setVar(conf, HiveConf.ConfVars.HIVEINPUTFORMAT, mWork.getInputformat());
+      HiveConf.setVar(conf, var, mWork.getInputformat());
     }
     if (mWork.getIndexIntermediateFile() != null) {
       conf.set("hive.index.compact.file", mWork.getIndexIntermediateFile());

File: ql/src/java/org/apache/hadoop/hive/ql/index/AggregateIndexHandler.java
Patch:
@@ -152,6 +152,7 @@ protected Task<?> getIndexBuilderMapRedTask(Set<ReadEntity> inputs,
       HiveConf builderConf = new HiveConf(getConf(), AggregateIndexHandler.class);
       builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGEMAPFILES, false);
       builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGEMAPREDFILES, false);
+      builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGETEZFILES, false);
       Task<?> rootTask = IndexUtils.createRootTask(builderConf, inputs, outputs,
           command, (LinkedHashMap<String, String>) partSpec, indexTableName, dbName);
 

File: ql/src/java/org/apache/hadoop/hive/ql/index/compact/CompactIndexHandler.java
Patch:
@@ -144,6 +144,7 @@ protected Task<?> getIndexBuilderMapRedTask(Set<ReadEntity> inputs, Set<WriteEnt
     HiveConf builderConf = new HiveConf(getConf(), CompactIndexHandler.class);
     builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGEMAPFILES, false);
     builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGEMAPREDFILES, false);
+    builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGETEZFILES, false);
     Task<?> rootTask = IndexUtils.createRootTask(builderConf, inputs, outputs,
         command, partSpec, indexTableName, dbName);
     return rootTask;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -643,7 +643,9 @@ private String processTable(QB qb, ASTNode tabref) throws SemanticException {
   }
 
   private void assertCombineInputFormat(Tree numerator, String message) throws SemanticException {
-    String inputFormat = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEINPUTFORMAT);
+    String inputFormat = conf.getVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez") ?
+      HiveConf.getVar(conf, HiveConf.ConfVars.HIVETEZINPUTFORMAT):
+      HiveConf.getVar(conf, HiveConf.ConfVars.HIVEINPUTFORMAT);
     if (!inputFormat.equals(CombineHiveInputFormat.class.getName())) {
       throw new SemanticException(generateErrorMessage((ASTNode) numerator,
           message + " sampling is not supported in " + inputFormat));

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFSign.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDF;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncSignDecimalToLong;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncSignDoubleToDouble;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncSignLongToDouble;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@@ -36,7 +37,7 @@
     		"  > SELECT _FUNC_(40) FROM src LIMIT 1;\n" +
     		"  1"
     )
-@VectorizedExpressions({FuncSignLongToDouble.class, FuncSignDoubleToDouble.class})
+@VectorizedExpressions({FuncSignLongToDouble.class, FuncSignDoubleToDouble.class, FuncSignDecimalToLong.class})
 public class UDFSign extends UDF {
 
   @SuppressWarnings("unused")

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAbs.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncAbsDecimalToDecimal;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncAbsDoubleToDouble;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncAbsLongToLong;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -48,7 +49,7 @@
         + "  > SELECT _FUNC_(0) FROM src LIMIT 1;\n"
         + "  0\n"
         + "  > SELECT _FUNC_(-5) FROM src LIMIT 1;\n" + "  5")
-@VectorizedExpressions({FuncAbsLongToLong.class, FuncAbsDoubleToDouble.class})
+@VectorizedExpressions({FuncAbsLongToLong.class, FuncAbsDoubleToDouble.class, FuncAbsDecimalToDecimal.class})
 public class GenericUDFAbs extends GenericUDF {
   private transient PrimitiveCategory inputType;
   private final DoubleWritable resultDouble = new DoubleWritable();

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCeil.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncCeilDecimalToDecimal;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncCeilDoubleToLong;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncCeilLongToLong;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@@ -33,7 +34,7 @@
     + "  > SELECT _FUNC_(-0.1) FROM src LIMIT 1;\n"
     + "  0\n"
     + "  > SELECT _FUNC_(5) FROM src LIMIT 1;\n" + "  5")
-@VectorizedExpressions({FuncCeilLongToLong.class, FuncCeilDoubleToLong.class})
+@VectorizedExpressions({FuncCeilLongToLong.class, FuncCeilDoubleToLong.class, FuncCeilDecimalToDecimal.class})
 public final class GenericUDFCeil extends GenericUDFFloorCeilBase {
 
   public GenericUDFCeil() {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFloor.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncFloorDecimalToDecimal;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncFloorDoubleToLong;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncFloorLongToLong;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@@ -33,7 +34,7 @@
     + "  > SELECT _FUNC_(-0.1) FROM src LIMIT 1;\n"
     + "  -1\n"
     + "  > SELECT _FUNC_(5) FROM src LIMIT 1;\n" + "  5")
-@VectorizedExpressions({FuncFloorLongToLong.class, FuncFloorDoubleToLong.class})
+@VectorizedExpressions({FuncFloorLongToLong.class, FuncFloorDoubleToLong.class, FuncFloorDecimalToDecimal.class})
 public final class GenericUDFFloor extends GenericUDFFloorCeilBase {
 
   public GenericUDFFloor() {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNegative.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedExpressions;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColUnaryMinus;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncNegateDecimalToDecimal;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColUnaryMinus;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
@@ -33,7 +34,7 @@
 import org.apache.hadoop.io.LongWritable;
 
 @Description(name = "-", value = "_FUNC_ a - Returns -a")
-@VectorizedExpressions({LongColUnaryMinus.class, DoubleColUnaryMinus.class})
+@VectorizedExpressions({LongColUnaryMinus.class, DoubleColUnaryMinus.class, FuncNegateDecimalToDecimal.class})
 public class GenericUDFOPNegative extends GenericUDFBaseUnary {
 
   public GenericUDFOPNegative() {

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java
Patch:
@@ -18,7 +18,6 @@
 package org.apache.hadoop.hive.conf;
 
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hive.common.util.HiveTestUtils;
 import org.junit.Assert;
@@ -43,7 +42,7 @@ private void checkHadoopConf(String name, String expectedHadoopVal) throws Excep
   }
 
   private void checkConfVar(ConfVars var, String expectedConfVarVal) throws Exception {
-    Assert.assertEquals(expectedConfVarVal, var.defaultVal);
+    Assert.assertEquals(expectedConfVarVal, var.getDefaultValue());
   }
 
   private void checkHiveConf(String name, String expectedHiveVal) throws Exception {
@@ -80,7 +79,7 @@ public void testConfProperties() throws Exception {
     checkHiveConf("test.property1", "hive-site.xml");
 
     // Test HiveConf property variable substitution in hive-site.xml
-    checkHiveConf("test.var.hiveconf.property", ConfVars.DEFAULTPARTITIONNAME.defaultVal);
+    checkHiveConf("test.var.hiveconf.property", ConfVars.DEFAULTPARTITIONNAME.getDefaultValue());
   }
 
   @Test

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveConfRestrictList.java
Patch:
@@ -19,7 +19,6 @@
 
 import junit.framework.TestCase;
 
-import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.junit.Test;
 

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveLogging.java
Patch:
@@ -18,15 +18,13 @@
 package org.apache.hadoop.hive.conf;
 
 import java.io.BufferedReader;
-import java.io.IOException;
 import java.io.InputStreamReader;
 
 import junit.framework.TestCase;
 
 import org.apache.hadoop.hive.common.LogUtils;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hive.common.util.HiveTestUtils;
-import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 
 /**
  * TestHiveLogging

File: itests/hive-unit/src/test/java/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.jdbc;
 
 import static org.apache.hadoop.hive.ql.exec.ExplainTask.EXPL_COLUMN_NAME;
-import static org.apache.hadoop.hive.ql.processors.SetProcessor.SET_COLUMN_NAME;
+import static org.apache.hive.common.util.SystemVariables.SET_COLUMN_NAME;
 
 import java.sql.Connection;
 import java.sql.DatabaseMetaData;

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hive.jdbc;
 
 import static org.apache.hadoop.hive.ql.exec.ExplainTask.EXPL_COLUMN_NAME;
-import static org.apache.hadoop.hive.ql.processors.SetProcessor.SET_COLUMN_NAME;
+import static org.apache.hive.common.util.SystemVariables.SET_COLUMN_NAME;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotNull;
@@ -1855,7 +1855,7 @@ public void testFetchFirstNonMR() throws Exception {
    * @throws Exception
    */
   public void testFetchFirstSetCmds() throws Exception {
-    execFetchFirst("set -v", SetProcessor.SET_COLUMN_NAME, false);
+    execFetchFirst("set -v", SET_COLUMN_NAME, false);
   }
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -72,7 +72,6 @@
 import java.util.UUID;
 import java.util.zip.Deflater;
 import java.util.zip.DeflaterOutputStream;
-import java.util.zip.Inflater;
 import java.util.zip.InflaterInputStream;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ExecutionException;

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveUtils.java
Patch:
@@ -29,7 +29,6 @@
 import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;
 import org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
 import org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;
-import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerFactory;
 import org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;
 import org.apache.hadoop.io.Text;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java
Patch:
@@ -69,6 +69,7 @@ public final class SemanticAnalyzerFactory {
     commandType.put(HiveParser.TOK_SHOWPARTITIONS, HiveOperation.SHOWPARTITIONS);
     commandType.put(HiveParser.TOK_SHOWLOCKS, HiveOperation.SHOWLOCKS);
     commandType.put(HiveParser.TOK_SHOWDBLOCKS, HiveOperation.SHOWLOCKS);
+    commandType.put(HiveParser.TOK_SHOWCONF, HiveOperation.SHOWCONF);
     commandType.put(HiveParser.TOK_CREATEFUNCTION, HiveOperation.CREATEFUNCTION);
     commandType.put(HiveParser.TOK_DROPFUNCTION, HiveOperation.DROPFUNCTION);
     commandType.put(HiveParser.TOK_CREATEMACRO, HiveOperation.CREATEMACRO);
@@ -193,6 +194,7 @@ public static BaseSemanticAnalyzer get(HiveConf conf, ASTNode tree)
       case HiveParser.TOK_SHOWINDEXES:
       case HiveParser.TOK_SHOWLOCKS:
       case HiveParser.TOK_SHOWDBLOCKS:
+      case HiveParser.TOK_SHOWCONF:
       case HiveParser.TOK_CREATEINDEX:
       case HiveParser.TOK_DROPINDEX:
       case HiveParser.TOK_ALTERTABLE_CLUSTER_SORT:

File: ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java
Patch:
@@ -66,6 +66,7 @@ public enum HiveOperation {
   SHOWINDEXES("SHOWINDEXES", null, null),
   SHOWPARTITIONS("SHOWPARTITIONS", null, null),
   SHOWLOCKS("SHOWLOCKS", null, null),
+  SHOWCONF("SHOWCONF", null, null),
   CREATEFUNCTION("CREATEFUNCTION", null, null),
   DROPFUNCTION("DROPFUNCTION", null, null),
   CREATEMACRO("CREATEMACRO", null, null),

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveOperationType.java
Patch:
@@ -69,6 +69,7 @@ public enum HiveOperationType {
   SHOWINDEXES,
   SHOWPARTITIONS,
   SHOWLOCKS,
+  SHOWCONF,
   CREATEFUNCTION,
   DROPFUNCTION,
   CREATEMACRO,

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/Operation2Privilege.java
Patch:
@@ -160,6 +160,7 @@ private SQLPrivTypeGrant[] getOutputPrivs() {
     op2Priv.put(HiveOperationType.SHOWINDEXES, new InOutPrivs(null, null));
     op2Priv.put(HiveOperationType.SHOWPARTITIONS, new InOutPrivs(null, null));
     op2Priv.put(HiveOperationType.SHOWLOCKS, new InOutPrivs(null, null));
+    op2Priv.put(HiveOperationType.SHOWCONF, new InOutPrivs(null, null));
     op2Priv.put(HiveOperationType.CREATEFUNCTION, new InOutPrivs(null, null));
     op2Priv.put(HiveOperationType.DROPFUNCTION, new InOutPrivs(null, null));
     op2Priv.put(HiveOperationType.CREATEMACRO, new InOutPrivs(null, null));

File: ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLPrivTypeGrant.java
Patch:
@@ -29,8 +29,8 @@ public enum SQLPrivTypeGrant {
   UPDATE_WGRANT(SQLPrivilegeType.UPDATE, true),
   DELETE_NOGRANT(SQLPrivilegeType.DELETE, false),
   DELETE_WGRANT(SQLPrivilegeType.DELETE, true),
-  OWNER_PRIV("Object ownership"),
-  ADMIN_PRIV("Admin privilege"); // This one can be used to deny permission for performing the operation
+  OWNER_PRIV("OBJECT OWNERSHIP"),
+  ADMIN_PRIV("ADMIN PRIVILEGE"); // This one can be used to deny permission for performing the operation
 
   private final SQLPrivilegeType privType;
   private final boolean withGrant;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
Patch:
@@ -326,7 +326,7 @@ static void configureDebugVariablesForChildJVM(Map<String, String> environmentVa
     if (environmentVariables.get(HIVE_DEBUG_RECURSIVE).equals("y")) {
       // swap debug options in HADOOP_CLIENT_OPTS to those that the child JVM should have
       assert environmentVariables.containsKey(HIVE_CHILD_CLIENT_DEBUG_OPTS)
-          && environmentVariables.get(HIVE_MAIN_CLIENT_DEBUG_OPTS) != null : HIVE_CHILD_CLIENT_DEBUG_OPTS
+          && environmentVariables.get(HIVE_CHILD_CLIENT_DEBUG_OPTS) != null : HIVE_CHILD_CLIENT_DEBUG_OPTS
           + " environment variable must be set when JVM in debug mode";
       String newHadoopClientOpts = hadoopClientOpts.replace(
           environmentVariables.get(HIVE_MAIN_CLIENT_DEBUG_OPTS),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -183,6 +183,7 @@
 import com.esotericsoftware.kryo.io.Input;
 import com.esotericsoftware.kryo.io.Output;
 import com.esotericsoftware.kryo.serializers.FieldSerializer;
+import com.esotericsoftware.shaded.org.objenesis.strategy.StdInstantiatorStrategy;
 
 /**
  * Utilities.
@@ -920,6 +921,7 @@ protected synchronized Kryo initialValue() {
       kryo.register(java.sql.Date.class, new SqlDateSerializer());
       kryo.register(java.sql.Timestamp.class, new TimestampSerializer());
       kryo.register(Path.class, new PathSerializer());
+      kryo.setInstantiatorStrategy(new StdInstantiatorStrategy());
       removeField(kryo, Operator.class, "colExprMap");
       removeField(kryo, ColumnInfo.class, "objectInspector");
       removeField(kryo, MapWork.class, "opParseCtxMap");
@@ -942,6 +944,7 @@ protected synchronized Kryo initialValue() {
       kryo.register(java.sql.Date.class, new SqlDateSerializer());
       kryo.register(java.sql.Timestamp.class, new TimestampSerializer());
       kryo.register(Path.class, new PathSerializer());
+      kryo.setInstantiatorStrategy(new StdInstantiatorStrategy());
       return kryo;
     };
   };

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoaderStorer.java
Patch:
@@ -134,7 +134,7 @@ private void smallTinyIntBoundsCheckHelper(String data, ExecJob.JOB_STATUS expec
     server.registerQuery("data = load '" + data +
       "' using PigStorage('\t') as (my_small_int:int, my_tiny_int:int);");
     server.registerQuery(
-      "store data into 'test_tbl' using org.apache.hive.hcatalog.pig.HCatStorer();");
+      "store data into 'test_tbl' using org.apache.hive.hcatalog.pig.HCatStorer('','','-onOutOfRangeValue Throw');");
     List<ExecJob> jobs = server.executeBatch();
     Assert.assertEquals(expectedStatus, jobs.get(0).getStatus());
   }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java
Patch:
@@ -988,9 +988,6 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
             if (limit <= parentStats.getNumRows()) {
               long numRows = limit;
               long avgRowSize = parentStats.getAvgRowSize();
-              if (avgRowSize <= 0) {
-                avgRowSize = HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVE_STATS_AVG_ROW_SIZE);
-              }
               long dataSize = avgRowSize * limit;
               wcStats.setNumRows(numRows);
               wcStats.setDataSize(dataSize);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExpressionDescriptor.java
Patch:
@@ -34,6 +34,7 @@ public enum ArgumentType {
     LONG(1),
     DOUBLE(2),
     STRING(3),
+    DECIMAL(4),
     ANY(7);
 
     private final int value;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -795,7 +795,7 @@ public static enum ConfVars {
     // Number of async threads
     HIVE_SERVER2_ASYNC_EXEC_THREADS("hive.server2.async.exec.threads", 100),
     // Number of seconds HiveServer2 shutdown will wait for async threads to terminate
-    HIVE_SERVER2_ASYNC_EXEC_SHUTDOWN_TIMEOUT("hive.server2.async.exec.shutdown.timeout", 10L),
+    HIVE_SERVER2_ASYNC_EXEC_SHUTDOWN_TIMEOUT("hive.server2.async.exec.shutdown.timeout", 10),
     // Size of the wait queue for async thread pool in HiveServer2.
     // After hitting this limit, the async thread pool will reject new requests.
     HIVE_SERVER2_ASYNC_EXEC_WAIT_QUEUE_SIZE("hive.server2.async.exec.wait.queue.size", 100),

File: service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java
Patch:
@@ -70,7 +70,7 @@ public void Authenticate(String user, String  password)
       DirContext ctx = new InitialDirContext(env);
       ctx.close();
     } catch (NamingException e) {
-      throw new AuthenticationException("Error validating LDAP user");
+      throw new AuthenticationException("Error validating LDAP user", e);
     }
   return;
   }

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFLTrim.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hadoop.hive.ql.udf;
+package org.apache.hadoop.hive.ql.udf.generic;
 
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFLpad.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hadoop.hive.ql.udf;
+package org.apache.hadoop.hive.ql.udf.generic;
 
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFRTrim.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hadoop.hive.ql.udf;
+package org.apache.hadoop.hive.ql.udf.generic;
 
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFRpad.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hadoop.hive.ql.udf;
+package org.apache.hadoop.hive.ql.udf.generic;
 
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFTrim.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.hadoop.hive.ql.udf;
+package org.apache.hadoop.hive.ql.udf.generic;
 
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TTypeQualifiers.java
Patch:
@@ -360,7 +360,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, TTypeQualifiers str
                 for (int _i1 = 0; _i1 < _map0.size; ++_i1)
                 {
                   String _key2; // required
-                  TTypeQualifierValue _val3; // optional
+                  TTypeQualifierValue _val3; // required
                   _key2 = iprot.readString();
                   _val3 = new TTypeQualifierValue();
                   _val3.read(iprot);
@@ -435,7 +435,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, TTypeQualifiers stru
         for (int _i7 = 0; _i7 < _map6.size; ++_i7)
         {
           String _key8; // required
-          TTypeQualifierValue _val9; // optional
+          TTypeQualifierValue _val9; // required
           _key8 = iprot.readString();
           _val9 = new TTypeQualifierValue();
           _val9.read(iprot);

File: service/src/java/org/apache/hive/service/cli/HiveSQLException.java
Patch:
@@ -118,6 +118,7 @@ public static TStatus toTStatus(Exception e) {
       return ((HiveSQLException)e).toTStatus();
     }
     TStatus tStatus = new TStatus(TStatusCode.ERROR_STATUS);
+    tStatus.setErrorMessage(e.getMessage());
     return tStatus;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java
Patch:
@@ -147,6 +147,7 @@ public void open(String sessionId, HiveConf conf)
   /**
    * Close a tez session. Will cleanup any tez/am related resources. After closing a session
    * no further DAGs can be executed against it.
+   * @param keepTmpDir whether or not to remove the scratch dir at the same time.
    * @throws IOException
    * @throws TezException
    */
@@ -212,7 +213,6 @@ private Path createTezDir(String sessionId)
   /**
    * Returns a local resource representing the hive-exec jar. This resource will
    * be used to execute the plan on the cluster.
-   * @param conf
    * @return LocalResource corresponding to the localized hive exec resource.
    * @throws IOException when any file system related call fails.
    * @throws LoginException when we are unable to determine the user.

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java
Patch:
@@ -39,7 +39,7 @@
 import org.apache.hadoop.hive.ql.plan.Statistics;
 
 /**
- * ConvertJoinMapJoin is an optimization that replaces a commone join
+ * ConvertJoinMapJoin is an optimization that replaces a common join
  * (aka shuffle join) with a map join (aka broadcast or fragment replicate
  * join when possible. Map joins have restrictions on which joins can be
  * converted (e.g.: full outer joins cannot be handled as map joins) as well

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java
Patch:
@@ -34,8 +34,8 @@ public class ReduceSinkMapJoinProc implements NodeProcessor {
 
   /* (non-Javadoc)
    * This processor addresses the RS-MJ case that occurs in tez on the small/hash
-   * table side of things. The connection between the work that RS will be a part of
-   * must be connected to the MJ work via be a broadcast edge.
+   * table side of things. The work that RS will be a part of must be connected 
+   * to the MJ work via be a broadcast edge.
    * We should not walk down the tree when we encounter this pattern because:
    * the type of work (map work or reduce work) needs to be determined
    * on the basis of the big table side because it may be a mapwork (no need for shuffle)

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
Patch:
@@ -419,7 +419,7 @@ public static TableDesc getMapJoinKeyTableDesc(Configuration conf,
   }
 
   /**
-   * Generate the table descriptor for Map-side join key.
+   * Generate the table descriptor for Map-side join value.
    */
   public static TableDesc getMapJoinValueTableDesc(
       List<FieldSchema> fieldSchemas) {

File: itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/TestHiveServer2.java
Patch:
@@ -64,6 +64,6 @@ public void testConnection() throws Exception {
     serviceClient.executeStatement(sessHandle, "CREATE TABLE " + tabName + " (id INT)", confOverlay);
     OperationHandle opHandle = serviceClient.executeStatement(sessHandle, "SHOW TABLES", confOverlay);
     RowSet rowSet = serviceClient.fetchResults(opHandle);
-    assertFalse(rowSet.getSize() == 0);
+    assertFalse(rowSet.numRows() == 0);
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ListSinkOperator.java
Patch:
@@ -36,6 +36,7 @@
 public class ListSinkOperator extends Operator<ListSinkDesc> {
 
   public static final String OUTPUT_FORMATTER = "output.formatter";
+  public static final String OUTPUT_PROTOCOL = "output.protocol";
 
   private transient List res;
   private transient FetchFormatter fetcher;

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TTypeQualifiers.java
Patch:
@@ -360,7 +360,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, TTypeQualifiers str
                 for (int _i1 = 0; _i1 < _map0.size; ++_i1)
                 {
                   String _key2; // required
-                  TTypeQualifierValue _val3; // required
+                  TTypeQualifierValue _val3; // optional
                   _key2 = iprot.readString();
                   _val3 = new TTypeQualifierValue();
                   _val3.read(iprot);
@@ -435,7 +435,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, TTypeQualifiers stru
         for (int _i7 = 0; _i7 < _map6.size; ++_i7)
         {
           String _key8; // required
-          TTypeQualifierValue _val9; // required
+          TTypeQualifierValue _val9; // optional
           _key8 = iprot.readString();
           _val9 = new TTypeQualifierValue();
           _val9.read(iprot);

File: service/src/java/org/apache/hive/service/cli/operation/GetCatalogsOperation.java
Patch:
@@ -18,13 +18,12 @@
 
 package org.apache.hive.service.cli.operation;
 
-import java.util.EnumSet;
-
 import org.apache.hive.service.cli.FetchOrientation;
 import org.apache.hive.service.cli.HiveSQLException;
 import org.apache.hive.service.cli.OperationState;
 import org.apache.hive.service.cli.OperationType;
 import org.apache.hive.service.cli.RowSet;
+import org.apache.hive.service.cli.RowSetFactory;
 import org.apache.hive.service.cli.TableSchema;
 import org.apache.hive.service.cli.session.HiveSession;
 
@@ -36,10 +35,11 @@ public class GetCatalogsOperation extends MetadataOperation {
   private static final TableSchema RESULT_SET_SCHEMA = new TableSchema()
   .addStringColumn("TABLE_CAT", "Catalog name. NULL if not applicable.");
 
-  private final RowSet rowSet = new RowSet();
+  private final RowSet rowSet;
 
   protected GetCatalogsOperation(HiveSession parentSession) {
     super(parentSession, OperationType.GET_CATALOGS);
+    rowSet = RowSetFactory.create(RESULT_SET_SCHEMA, getProtocolVersion());
   }
 
   /* (non-Javadoc)

File: service/src/java/org/apache/hive/service/cli/operation/HiveCommandOperation.java
Patch:
@@ -27,7 +27,6 @@
 import java.io.PrintStream;
 import java.io.UnsupportedEncodingException;
 import java.util.ArrayList;
-import java.util.EnumSet;
 import java.util.List;
 import java.util.Map;
 
@@ -40,6 +39,7 @@
 import org.apache.hive.service.cli.HiveSQLException;
 import org.apache.hive.service.cli.OperationState;
 import org.apache.hive.service.cli.RowSet;
+import org.apache.hive.service.cli.RowSetFactory;
 import org.apache.hive.service.cli.TableSchema;
 import org.apache.hive.service.cli.session.HiveSession;
 
@@ -158,10 +158,10 @@ public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws H
       resetResultReader();
     }
     List<String> rows = readResults((int) maxRows);
-    RowSet rowSet = new RowSet();
+    RowSet rowSet = RowSetFactory.create(resultSchema, getProtocolVersion());
 
     for (String row : rows) {
-      rowSet.addRow(resultSchema, new String[] {row});
+      rowSet.addRow(new String[] {row});
     }
     return rowSet;
   }

File: service/src/test/org/apache/hive/service/cli/thrift/ThriftCLIServiceTest.java
Patch:
@@ -357,8 +357,9 @@ public void testDoAs() throws HiveSQLException, LoginException, IOException {
     cliService.init(hconf);
     ThriftCLIService tcliService = new ThriftBinaryCLIService(cliService);
     TOpenSessionReq req = new TOpenSessionReq();
+    TOpenSessionResp res = new TOpenSessionResp();
     req.setUsername("testuser1");
-    SessionHandle sHandle = tcliService.getSessionHandle(req );
+    SessionHandle sHandle = tcliService.getSessionHandle(req, res);
     SessionManager sManager = getSessionManager(cliService.getServices());
     HiveSession session = sManager.getSession(sHandle);
 

File: hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/HCatBaseTest.java
Patch:
@@ -41,8 +41,7 @@
  */
 public class HCatBaseTest {
   protected static final Logger LOG = LoggerFactory.getLogger(HCatBaseTest.class);
-  protected static final String TEST_DATA_DIR = System.getProperty("user.dir") +
-      "/build/test/data/" + HCatBaseTest.class.getCanonicalName();
+  protected static final String TEST_DATA_DIR = org.apache.hive.hcatalog.mapreduce.HCatBaseTest.TEST_DATA_DIR;
   protected static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
 
   protected HiveConf hiveConf = null;

File: hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatPartitionPublish.java
Patch:
@@ -79,12 +79,12 @@ public class TestHCatPartitionPublish {
 
   @BeforeClass
   public static void setup() throws Exception {
+    File workDir = org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.handleWorkDir();
     conf.set("yarn.scheduler.capacity.root.queues", "default");
     conf.set("yarn.scheduler.capacity.root.default.capacity", "100");
 
     fs = FileSystem.get(conf);
-    System.setProperty("hadoop.log.dir", new File(fs.getWorkingDirectory()
-        .toString(), "/logs").getAbsolutePath());
+    System.setProperty("hadoop.log.dir", new File(workDir, "/logs").getAbsolutePath());
     // LocalJobRunner does not work with mapreduce OutputCommitter. So need
     // to use MiniMRCluster. MAPREDUCE-2350
     mrCluster = new MiniMRCluster(1, fs.getUri().toString(), 1, null, null,
@@ -99,6 +99,7 @@ public static void setup() throws Exception {
 
     MetaStoreUtils.startMetaStore(msPort, ShimLoader
         .getHadoopThriftAuthBridge());
+    Thread.sleep(10000);
     isServerRunning = true;
     securityManager = System.getSecurityManager();
     System.setSecurityManager(new NoExitSecurityManager());

File: hcatalog/core/src/test/java/org/apache/hive/hcatalog/mapreduce/HCatBaseTest.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hive.hcatalog.common.HCatUtil;
 import org.apache.pig.PigServer;
 import org.junit.Assert;
 import org.junit.Before;
@@ -40,8 +41,8 @@
  */
 public class HCatBaseTest {
   protected static final Logger LOG = LoggerFactory.getLogger(HCatBaseTest.class);
-  protected static final String TEST_DATA_DIR =
-      "/tmp/build/test/data/" + HCatBaseTest.class.getCanonicalName();
+  public static final String TEST_DATA_DIR = HCatUtil.makePathASafeFileName(System.getProperty("user.dir") +
+          "/build/test/data/" + HCatBaseTest.class.getCanonicalName() + "-" + System.currentTimeMillis());
   protected static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
 
   protected HiveConf hiveConf = null;

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoaderStorer.java
Patch:
@@ -69,7 +69,7 @@ public void testSmallTinyInt() throws Exception {
       " (my_small_int smallint, my_tiny_int tinyint)" +
       " row format delimited fields terminated by '\t' stored as textfile").getResponseCode());
     Assert.assertEquals(0, driver.run("load data local inpath '" +
-      dataDir.getAbsolutePath() + "' into table " + readTblName).getResponseCode());
+      dataDir.getPath().replaceAll("\\\\", "/") + "' into table " + readTblName).getResponseCode());
 
     PigServer server = new PigServer(ExecType.LOCAL);
     server.registerQuery(
@@ -104,7 +104,7 @@ public void testSmallTinyInt() throws Exception {
       String.format("%d\t%d", Short.MIN_VALUE, Byte.MIN_VALUE),
       String.format("%d\t%d", Short.MAX_VALUE, Byte.MAX_VALUE)
     });
-    smallTinyIntBoundsCheckHelper(writeDataFile.getAbsolutePath(), ExecJob.JOB_STATUS.COMPLETED);
+    smallTinyIntBoundsCheckHelper(writeDataFile.getPath().replaceAll("\\\\", "/"), ExecJob.JOB_STATUS.COMPLETED);
 
     // Values outside the column type bounds will fail at runtime.
     HcatTestUtils.createTestDataFile(TEST_DATA_DIR + "/shortTooSmall.tsv", new String[]{

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatStorerMulti.java
Patch:
@@ -41,8 +41,7 @@
  * @deprecated Use/modify {@link org.apache.hive.hcatalog.pig.TestHCatStorerMulti} instead
  */
 public class TestHCatStorerMulti extends TestCase {
-  private static final String TEST_DATA_DIR = System.getProperty("user.dir") +
-    "/build/test/data/" + TestHCatStorerMulti.class.getCanonicalName();
+  private static final String TEST_DATA_DIR = org.apache.hive.hcatalog.pig.TestHCatStorerMulti.TEST_DATA_DIR;
   private static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
   private static final String INPUT_FILE_NAME = TEST_DATA_DIR + "/input.data";
 

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatStorerWrapper.java
Patch:
@@ -48,7 +48,7 @@ public class TestHCatStorerWrapper extends HCatBaseTest {
   @Test
   public void testStoreExternalTableWithExternalDir() throws IOException, CommandNeedRetryException{
 
-    File tmpExternalDir = new File(SystemUtils.getJavaIoTmpDir(), UUID.randomUUID().toString());
+    File tmpExternalDir = new File(TEST_DATA_DIR, UUID.randomUUID().toString());
     tmpExternalDir.deleteOnExit();
 
     String part_val = "100";
@@ -71,11 +71,11 @@ public void testStoreExternalTableWithExternalDir() throws IOException, CommandN
     server.setBatchOn();
     logAndRegister(server, "A = load '"+INPUT_FILE_NAME+"' as (a:int, b:chararray);");
     logAndRegister(server, "store A into 'default.junit_external' using " + HCatStorerWrapper.class.getName()
-        + "('c=" + part_val + "','" + tmpExternalDir.getAbsolutePath() + "');");
+        + "('c=" + part_val + "','" + tmpExternalDir.getPath().replaceAll("\\\\", "/") + "');");
     server.executeBatch();
 
     Assert.assertTrue(tmpExternalDir.exists());
-    Assert.assertTrue(new File(tmpExternalDir.getAbsoluteFile() + "/" + "part-m-00000").exists());
+    Assert.assertTrue(new File(tmpExternalDir.getPath().replaceAll("\\\\", "/") + "/" + "part-m-00000").exists());
 
     driver.run("select * from junit_external");
     ArrayList<String> res = new ArrayList<String>();

File: hcatalog/storage-handlers/hbase/src/test/org/apache/hcatalog/hbase/TestHCatHBaseInputFormat.java
Patch:
@@ -64,7 +64,7 @@
 import org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer;
 import org.apache.hcatalog.common.HCatConstants;
 import org.apache.hcatalog.common.HCatException;
-import org.apache.hcatalog.common.HCatUtil;
+import org.apache.hive.hcatalog.common.HCatUtil;
 import org.apache.hcatalog.data.HCatRecord;
 import org.apache.hcatalog.data.schema.HCatFieldSchema;
 import org.apache.hcatalog.data.schema.HCatSchema;
@@ -188,7 +188,7 @@ public void TestHBaseTableReadMR() throws Exception {
     String databaseName = newTableName("MyDatabase");
     //Table name will be lower case unless specified by hbase.table.name property
     String hbaseTableName = (databaseName + "." + tableName).toLowerCase();
-    String db_dir = getTestDir() + "/hbasedb";
+    String db_dir = HCatUtil.makePathASafeFileName(getTestDir() + "/hbasedb");
 
     String dbquery = "CREATE DATABASE IF NOT EXISTS " + databaseName + " LOCATION '"
         + db_dir + "'";

File: hcatalog/webhcat/java-client/src/test/java/org/apache/hcatalog/api/TestHCatClient.java
Patch:
@@ -128,7 +128,8 @@ public void testBasicDDLCommands() throws Exception {
     assertTrue(testDb.getProperties().size() == 0);
     String warehouseDir = System
       .getProperty("test.warehouse.dir", "/user/hive/warehouse");
-    String expectedDir = warehouseDir.replaceAll("\\\\", "/").replaceFirst("pfile:///", "pfile:/");
+    String expectedDir = org.apache.hive.hcatalog.api.TestHCatClient.fixPath(warehouseDir).
+            replaceFirst("pfile:///", "pfile:/");
     assertEquals(expectedDir + "/" + db + ".db", testDb.getLocation());
     ArrayList<HCatFieldSchema> cols = new ArrayList<HCatFieldSchema>();
     cols.add(new HCatFieldSchema("id", Type.INT, "id comment"));

File: itests/hcatalog-unit/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java
Patch:
@@ -43,7 +43,7 @@
 import org.apache.hcatalog.HcatTestUtils;
 import org.apache.hcatalog.common.HCatConstants;
 import org.apache.hcatalog.common.HCatException;
-import org.apache.hcatalog.common.HCatUtil;
+import org.apache.hive.hcatalog.common.HCatUtil;
 import org.apache.hcatalog.data.DefaultHCatRecord;
 import org.apache.hcatalog.data.schema.HCatFieldSchema;
 import org.apache.hcatalog.data.schema.HCatSchema;
@@ -72,8 +72,8 @@ public void setup() throws Exception {
     dataDir = new File(System.getProperty("java.io.tmpdir") + File.separator +
         TestSequenceFileReadWrite.class.getCanonicalName() + "-" + System.currentTimeMillis());
     hiveConf = new HiveConf(this.getClass());
-    warehouseDir = new File(dataDir, "warehouse").getAbsolutePath();
-    inputFileName = new File(dataDir, "input.data").getAbsolutePath();
+    warehouseDir = HCatUtil.makePathASafeFileName(dataDir + File.separator + "warehouse");
+    inputFileName = HCatUtil.makePathASafeFileName(dataDir + File.separator + "input.data");
     hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");

File: itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestSequenceFileReadWrite.java
Patch:
@@ -69,8 +69,8 @@ public void setup() throws Exception {
     dataDir = new File(System.getProperty("java.io.tmpdir") + File.separator +
         TestSequenceFileReadWrite.class.getCanonicalName() + "-" + System.currentTimeMillis());
     hiveConf = new HiveConf(this.getClass());
-    warehouseDir = new File(dataDir, "warehouse").getAbsolutePath();
-    inputFileName = new File(dataDir, "input.data").getAbsolutePath();
+    warehouseDir = HCatUtil.makePathASafeFileName(dataDir + File.separator + "warehouse");
+    inputFileName = HCatUtil.makePathASafeFileName(dataDir + File.separator + "input.data");
     hiveConf = new HiveConf(this.getClass());
     hiveConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
     hiveConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
Patch:
@@ -53,7 +53,7 @@
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
-import org.apache.hadoop.hive.shims.Hadoop20Shims.NullOutputCommitter;
+import org.apache.hadoop.hive.shims.HadoopShimsSecure.NullOutputCommitter;
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.mapred.InputFormat;

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -366,9 +366,9 @@ public enum ErrorMsg {
   UNSUPPORTED_SUBQUERY_EXPRESSION(10249, "Unsupported SubQuery Expression"),
   INVALID_SUBQUERY_EXPRESSION(10250, "Invalid SubQuery expression"),
 
-  INVALID_HDFS_URI(10248, "{0} is not a hdfs uri", true),
-  INVALID_DIR(10249, "{0} is not a directory", true),
-  NO_VALID_LOCATIONS(10250, "Could not find any valid location to place the jars. " +
+  INVALID_HDFS_URI(10251, "{0} is not a hdfs uri", true),
+  INVALID_DIR(10252, "{0} is not a directory", true),
+  NO_VALID_LOCATIONS(10253, "Could not find any valid location to place the jars. " +
   "Please update hive.jar.directory or hive.user.install.directory with a valid location", false),
 
   SCRIPT_INIT_ERROR(20000, "Unable to initialize custom script."),

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.io.IOContext;
 import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
 import org.apache.hadoop.hive.ql.plan.CollectDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;
@@ -313,6 +314,7 @@ public void testMapOperator() throws Throwable {
       Configuration hconf = new JobConf(TestOperators.class);
       HiveConf.setVar(hconf, HiveConf.ConfVars.HADOOPMAPFILENAME,
           "hdfs:///testDir/testFile");
+      IOContext.get().setInputPath(new Path("hdfs:///testDir/testFile"));
 
       // initialize pathToAliases
       ArrayList<String> aliases = new ArrayList<String>();

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
Patch:
@@ -19,6 +19,7 @@
 
 import static org.junit.Assert.assertArrayEquals;
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
 
 import java.io.DataInput;
 import java.io.DataOutput;
@@ -562,7 +563,6 @@ public void testInOutFormat() throws Exception {
     IntObjectInspector intInspector =
         (IntObjectInspector) fields.get(0).getFieldObjectInspector();
     assertEquals(0.0, reader.getProgress(), 0.00001);
-    assertEquals(3, reader.getPos());
     while (reader.next(key, value)) {
       assertEquals(++rowNum, intInspector.get(inspector.
           getStructFieldData(serde.deserialize(value), fields.get(0))));
@@ -697,7 +697,7 @@ public void testEmptyFile() throws Exception {
     InputFormat<?,?> in = new OrcInputFormat();
     FileInputFormat.setInputPaths(conf, testFilePath.toString());
     InputSplit[] splits = in.getSplits(conf, 1);
-    assertEquals(0, splits.length);
+    assertTrue(1 == splits.length);
     assertEquals(null, serde.getSerDeStats());
   }
 

File: jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java
Patch:
@@ -137,6 +137,7 @@ public HiveConnection(String uri, Properties info) throws SQLException {
     supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V2);
     supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V3);
     supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V4);
+    supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V5);
 
     // open client session
     openSession();
@@ -145,9 +146,7 @@ public HiveConnection(String uri, Properties info) throws SQLException {
   }
 
   private void openTransport() throws SQLException {
-    transport = isHttpTransportMode() ?
-        createHttpTransport() :
-          createBinaryTransport();
+    transport = isHttpTransportMode() ? createHttpTransport() : createBinaryTransport();
     TProtocol protocol = new TBinaryProtocol(transport);
     client = new TCLIService.Client(protocol);
     try {

File: jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java
Patch:
@@ -238,6 +238,7 @@ public boolean execute(String sql) throws SQLException {
             case UKNOWN_STATE:
               throw new SQLException("Unknown query", "HY000");
             case INITIALIZED_STATE:
+            case PENDING_STATE:
             case RUNNING_STATE:
               break;
             }

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TGetTablesReq.java
Patch:
@@ -715,7 +715,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, TGetTablesReq struc
                 struct.tableTypes = new ArrayList<String>(_list164.size);
                 for (int _i165 = 0; _i165 < _list164.size; ++_i165)
                 {
-                  String _elem166; // optional
+                  String _elem166; // required
                   _elem166 = iprot.readString();
                   struct.tableTypes.add(_elem166);
                 }
@@ -856,7 +856,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, TGetTablesReq struct
           struct.tableTypes = new ArrayList<String>(_list169.size);
           for (int _i170 = 0; _i170 < _list169.size; ++_i170)
           {
-            String _elem171; // optional
+            String _elem171; // required
             _elem171 = iprot.readString();
             struct.tableTypes.add(_elem171);
           }

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TOpenSessionReq.java
Patch:
@@ -141,7 +141,7 @@ public String getFieldName() {
   }
 
   public TOpenSessionReq() {
-    this.client_protocol = org.apache.hive.service.cli.thrift.TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V3;
+    this.client_protocol = org.apache.hive.service.cli.thrift.TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V5;
 
   }
 
@@ -188,7 +188,7 @@ public TOpenSessionReq deepCopy() {
 
   @Override
   public void clear() {
-    this.client_protocol = org.apache.hive.service.cli.thrift.TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V3;
+    this.client_protocol = org.apache.hive.service.cli.thrift.TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V5;
 
     this.username = null;
     this.password = null;

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TOpenSessionResp.java
Patch:
@@ -141,7 +141,7 @@ public String getFieldName() {
   }
 
   public TOpenSessionResp() {
-    this.serverProtocolVersion = org.apache.hive.service.cli.thrift.TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V4;
+    this.serverProtocolVersion = org.apache.hive.service.cli.thrift.TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V5;
 
   }
 
@@ -191,7 +191,7 @@ public TOpenSessionResp deepCopy() {
   @Override
   public void clear() {
     this.status = null;
-    this.serverProtocolVersion = org.apache.hive.service.cli.thrift.TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V4;
+    this.serverProtocolVersion = org.apache.hive.service.cli.thrift.TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V5;
 
     this.sessionHandle = null;
     this.configuration = null;

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TRow.java
Patch:
@@ -354,7 +354,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, TRow struct) throws
                 struct.colVals = new ArrayList<TColumnValue>(_list102.size);
                 for (int _i103 = 0; _i103 < _list102.size; ++_i103)
                 {
-                  TColumnValue _elem104; // optional
+                  TColumnValue _elem104; // required
                   _elem104 = new TColumnValue();
                   _elem104.read(iprot);
                   struct.colVals.add(_elem104);
@@ -425,7 +425,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, TRow struct) throws
         struct.colVals = new ArrayList<TColumnValue>(_list107.size);
         for (int _i108 = 0; _i108 < _list107.size; ++_i108)
         {
-          TColumnValue _elem109; // optional
+          TColumnValue _elem109; // required
           _elem109 = new TColumnValue();
           _elem109.read(iprot);
           struct.colVals.add(_elem109);

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TStatus.java
Patch:
@@ -698,7 +698,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, TStatus struct) thr
                 struct.infoMessages = new ArrayList<String>(_list126.size);
                 for (int _i127 = 0; _i127 < _list126.size; ++_i127)
                 {
-                  String _elem128; // optional
+                  String _elem128; // required
                   _elem128 = iprot.readString();
                   struct.infoMessages.add(_elem128);
                 }
@@ -848,7 +848,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, TStatus struct) thro
           struct.infoMessages = new ArrayList<String>(_list131.size);
           for (int _i132 = 0; _i132 < _list131.size; ++_i132)
           {
-            String _elem133; // optional
+            String _elem133; // required
             _elem133 = iprot.readString();
             struct.infoMessages.add(_elem133);
           }

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TTableSchema.java
Patch:
@@ -354,7 +354,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, TTableSchema struct
                 struct.columns = new ArrayList<TColumnDesc>(_list38.size);
                 for (int _i39 = 0; _i39 < _list38.size; ++_i39)
                 {
-                  TColumnDesc _elem40; // optional
+                  TColumnDesc _elem40; // required
                   _elem40 = new TColumnDesc();
                   _elem40.read(iprot);
                   struct.columns.add(_elem40);
@@ -425,7 +425,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, TTableSchema struct)
         struct.columns = new ArrayList<TColumnDesc>(_list43.size);
         for (int _i44 = 0; _i44 < _list43.size; ++_i44)
         {
-          TColumnDesc _elem45; // optional
+          TColumnDesc _elem45; // required
           _elem45 = new TColumnDesc();
           _elem45.read(iprot);
           struct.columns.add(_elem45);

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TTypeDesc.java
Patch:
@@ -354,7 +354,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, TTypeDesc struct) t
                 struct.types = new ArrayList<TTypeEntry>(_list30.size);
                 for (int _i31 = 0; _i31 < _list30.size; ++_i31)
                 {
-                  TTypeEntry _elem32; // optional
+                  TTypeEntry _elem32; // required
                   _elem32 = new TTypeEntry();
                   _elem32.read(iprot);
                   struct.types.add(_elem32);
@@ -425,7 +425,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, TTypeDesc struct) th
         struct.types = new ArrayList<TTypeEntry>(_list35.size);
         for (int _i36 = 0; _i36 < _list35.size; ++_i36)
         {
-          TTypeEntry _elem37; // optional
+          TTypeEntry _elem37; // required
           _elem37 = new TTypeEntry();
           _elem37.read(iprot);
           struct.types.add(_elem37);

File: service/src/java/org/apache/hive/service/cli/EmbeddedCLIServiceClient.java
Patch:
@@ -143,15 +143,15 @@ public OperationHandle getColumns(SessionHandle sessionHandle, String catalogNam
   @Override
   public OperationHandle getFunctions(SessionHandle sessionHandle,
       String catalogName, String schemaName, String functionName)
-      throws HiveSQLException {
+          throws HiveSQLException {
     return cliService.getFunctions(sessionHandle, catalogName, schemaName, functionName);
   }
 
   /* (non-Javadoc)
    * @see org.apache.hive.service.cli.CLIServiceClient#getOperationStatus(org.apache.hive.service.cli.OperationHandle)
    */
   @Override
-  public OperationState getOperationStatus(OperationHandle opHandle) throws HiveSQLException {
+  public OperationStatus getOperationStatus(OperationHandle opHandle) throws HiveSQLException {
     return cliService.getOperationStatus(opHandle);
   }
 

File: service/src/java/org/apache/hive/service/cli/OperationState.java
Patch:
@@ -94,7 +94,7 @@ public static void validateTransition(OperationState oldState, OperationState ne
   }
 
   public void validateTransition(OperationState newState)
-  throws HiveSQLException {
+      throws HiveSQLException {
     validateTransition(this, newState);
   }
 

File: serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java
Patch:
@@ -58,7 +58,7 @@ public static void setReadColumnIDs(Configuration conf, List<Integer> ids) {
    * @deprecated for backwards compatibility with <= 0.12, use appendReadColumns
    */
   @Deprecated
-  public static void appendReadColumnID(Configuration conf, List<Integer> ids) {
+  public static void appendReadColumnIDs(Configuration conf, List<Integer> ids) {
     appendReadColumns(conf, ids);
   }
 

File: serde/src/test/org/apache/hadoop/hive/serde2/TestColumnProjectionUtils.java
Patch:
@@ -105,7 +105,7 @@ public void testDeprecatedMethods() {
     Collections.sort(actual);
     assertEquals(columnIds, actual);
     columnIds.add(2);
-    ColumnProjectionUtils.appendReadColumnID(conf, Collections.singletonList(2));
+    ColumnProjectionUtils.appendReadColumnIDs(conf, Collections.singletonList(2));
     actual = ColumnProjectionUtils.getReadColumnIDs(conf);
     Collections.sort(actual);
     assertEquals(columnIds, actual);

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java
Patch:
@@ -413,6 +413,9 @@ public static boolean serialize(Output byteStream, Object obj,
       case DECIMAL: {
         HiveDecimalObjectInspector bdoi = (HiveDecimalObjectInspector) poi;
         HiveDecimalWritable t = bdoi.getPrimitiveWritableObject(obj);
+        if (t == null) {
+          return warnedOnceNullMapKey;
+        }
         t.writeToByteStream(byteStream);
         return warnedOnceNullMapKey;
       }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -2351,7 +2351,7 @@ public static void setColumnTypeList(JobConf jobConf, Operator op, boolean exclu
     jobConf.set(serdeConstants.LIST_COLUMN_TYPES, columnTypesString);
   }
 
-  public static void validatePartSpec(Table tbl, Map<String, String> partSpec)
+  public static void validatePartSpecColumnNames(Table tbl, Map<String, String> partSpec)
       throws SemanticException {
 
     List<FieldSchema> parts = tbl.getPartitionKeys();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -5267,7 +5267,7 @@ private Operator genFileSinkPlan(String dest, QB qb, Operator input)
         }
         dpCtx = qbm.getDPCtx(dest);
         if (dpCtx == null) {
-          Utilities.validatePartSpec(dest_tab, partSpec);
+          Utilities.validatePartSpecColumnNames(dest_tab, partSpec);
           dpCtx = new DynamicPartitionCtx(dest_tab, partSpec,
               conf.getVar(HiveConf.ConfVars.DEFAULTPARTITIONNAME),
               conf.getIntVar(HiveConf.ConfVars.DYNAMICPARTITIONMAXPARTSPERNODE));

File: testutils/ptest2/src/test/java/org/apache/hive/ptest/execution/TestHostExecutor.java
Patch:
@@ -203,7 +203,7 @@ public void testShutdownBeforeExec()
     executor.submitTests(parallelWorkQueue, isolatedWorkQueue, failedTestResults).get();
     Assert.assertEquals(Collections.emptySet(),  failedTestResults);
     Assert.assertEquals(parallelWorkQueue.toString(), 1, parallelWorkQueue.size());
-    Approvals.verify(getExecutedCommands());
+    Approvals.verify("EMPTY\n" + getExecutedCommands());
     Assert.assertTrue(executor.isShutdown());
   }
   @Test

File: testutils/ptest2/src/test/java/org/apache/hive/ptest/execution/TestScripts.java
Patch:
@@ -63,6 +63,7 @@ public void testBatch() throws Throwable {
     templateVariables.put("workingDir", "/some/working/dir");
     templateVariables.put("buildTool", "maven");
     templateVariables.put("antArgs", "-Dant=arg1");
+    templateVariables.put("mavenArgs", "-Dant=arg1");
     templateVariables.put("testClass", "TestCliDriver");
     templateVariables.put("buildTag", "build-1");
     templateVariables.put("logDir", "/some/log/dir");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java
Patch:
@@ -64,9 +64,9 @@ public class MapRecordProcessor  extends RecordProcessor{
 
   @Override
   void init(JobConf jconf, MRTaskReporter mrReporter, Map<String, LogicalInput> inputs,
-      OutputCollector out){
+      Map<String, OutputCollector> outMap){
     perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);
-    super.init(jconf, mrReporter, inputs, out);
+    super.init(jconf, mrReporter, inputs, outMap);
 
     //Update JobConf using MRInput, info like filename comes via this
     MRInputLegacy mrInput = getMRInput(inputs);
@@ -124,7 +124,7 @@ void init(JobConf jconf, MRTaskReporter mrReporter, Map<String, LogicalInput> in
         }
       }
 
-      OperatorUtils.setChildrenCollector(mapOp.getChildOperators(), out);
+      OperatorUtils.setChildrenCollector(mapOp.getChildOperators(), outMap);
       mapOp.setReporter(reporter);
       MapredContext.get().setReporter(reporter);
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/RecordProcessor.java
Patch:
@@ -39,7 +39,7 @@ public abstract class RecordProcessor  {
 
   protected JobConf jconf;
   protected Map<String, LogicalInput> inputs;
-  protected OutputCollector out;
+  protected Map<String, OutputCollector> outMap;
 
   public static final Log l4j = LogFactory.getLog(RecordProcessor.class);
 
@@ -63,11 +63,11 @@ public abstract class RecordProcessor  {
    * @param out
    */
   void init(JobConf jconf, MRTaskReporter mrReporter, Map<String, LogicalInput> inputs,
-      OutputCollector out){
+      Map<String, OutputCollector> outMap){
     this.jconf = jconf;
     this.reporter = mrReporter;
     this.inputs = inputs;
-    this.out = out;
+    this.outMap = outMap;
 
     // Allocate the bean at the beginning -
     memoryMXBean = ManagementFactory.getMemoryMXBean();

File: jdbc/src/java/org/apache/hive/jdbc/HiveQueryResultSet.java
Patch:
@@ -193,8 +193,8 @@ private static JdbcColumnAttributes getColumnAttributes(
         case DECIMAL_TYPE:
           TTypeQualifierValue prec = tq.getQualifiers().get(TCLIServiceConstants.PRECISION);
           TTypeQualifierValue scale = tq.getQualifiers().get(TCLIServiceConstants.SCALE);
-          ret = new JdbcColumnAttributes(prec == null ? HiveDecimal.DEFAULT_PRECISION : prec.getI32Value(),
-              scale == null ? HiveDecimal.DEFAULT_SCALE : scale.getI32Value());
+          ret = new JdbcColumnAttributes(prec == null ? HiveDecimal.USER_DEFAULT_PRECISION : prec.getI32Value(),
+              scale == null ? HiveDecimal.USER_DEFAULT_SCALE : scale.getI32Value());
           break;
         default:
           break;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -2870,8 +2870,8 @@ private int describeTable(Hive db, DescTableDesc descTbl) throws HiveException {
   private static void fixDecimalColumnTypeName(List<FieldSchema> cols) {
     for (FieldSchema col : cols) {
       if (serdeConstants.DECIMAL_TYPE_NAME.equals(col.getType())) {
-        col.setType(DecimalTypeInfo.getQualifiedName(HiveDecimal.DEFAULT_PRECISION,
-            HiveDecimal.DEFAULT_SCALE));
+        col.setType(DecimalTypeInfo.getQualifiedName(HiveDecimal.USER_DEFAULT_PRECISION,
+            HiveDecimal.USER_DEFAULT_SCALE));
       }
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java
Patch:
@@ -545,8 +545,8 @@ static ObjectInspector createObjectInspector(int columnId,
       case DATE:
         return PrimitiveObjectInspectorFactory.javaDateObjectInspector;
       case DECIMAL:
-        int precision = type.hasPrecision() ? type.getPrecision() : HiveDecimal.MAX_PRECISION;
-        int scale =  type.hasScale()? type.getScale() : HiveDecimal.MAX_SCALE;
+        int precision = type.hasPrecision() ? type.getPrecision() : HiveDecimal.SYSTEM_DEFAULT_PRECISION;
+        int scale =  type.hasScale()? type.getScale() : HiveDecimal.SYSTEM_DEFAULT_SCALE;
         return PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(
             TypeInfoFactory.getDecimalTypeInfo(precision, scale));
       case STRUCT:

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
Patch:
@@ -1899,8 +1899,8 @@ private static TreeReader createTreeReader(Path path,
       case DATE:
         return new DateTreeReader(path, columnId);
       case DECIMAL:
-        int precision = type.hasPrecision() ? type.getPrecision() : HiveDecimal.MAX_PRECISION;
-        int scale =  type.hasScale()? type.getScale() : HiveDecimal.MAX_SCALE;
+        int precision = type.hasPrecision() ? type.getPrecision() : HiveDecimal.SYSTEM_DEFAULT_PRECISION;
+        int scale =  type.hasScale()? type.getScale() : HiveDecimal.SYSTEM_DEFAULT_SCALE;
         return new DecimalTreeReader(path, columnId, precision, scale);
       case STRUCT:
         return new StructTreeReader(path, columnId, types, included);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java
Patch:
@@ -200,8 +200,8 @@ public static DecimalTypeInfo getDecimalTypeTypeInfo(ASTNode node)
         throw new SemanticException("Bad params for type decimal");
       }
 
-      int precision = HiveDecimal.DEFAULT_PRECISION;
-      int scale = HiveDecimal.DEFAULT_SCALE;
+      int precision = HiveDecimal.USER_DEFAULT_PRECISION;
+      int scale = HiveDecimal.USER_DEFAULT_SCALE;
 
       if (node.getChildCount() >= 1) {
         String precStr = node.getChild(0).getText();

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBridge.java
Patch:
@@ -185,7 +185,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
     // If the returned value is HiveDecimal, we assume maximum precision/scale.
     if (result != null && result instanceof HiveDecimalWritable) {
       result = HiveDecimalUtils.enforcePrecisionScale((HiveDecimalWritable) result,
-          HiveDecimal.MAX_PRECISION, HiveDecimal.MAX_SCALE);
+          HiveDecimal.SYSTEM_DEFAULT_PRECISION, HiveDecimal.SYSTEM_DEFAULT_SCALE);
     }
 
     return result;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java
Patch:
@@ -222,7 +222,7 @@ public void testCommonClass() {
     common(TypeInfoFactory.stringTypeInfo, TypeInfoFactory.decimalTypeInfo,
            TypeInfoFactory.stringTypeInfo);
     common(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.decimalTypeInfo,
-           TypeInfoFactory.getDecimalTypeInfo(65, 30));
+           TypeInfoFactory.decimalTypeInfo);
     common(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.stringTypeInfo,
            TypeInfoFactory.stringTypeInfo);
 
@@ -244,7 +244,7 @@ public void testCommonClassComparison() {
     comparison(TypeInfoFactory.stringTypeInfo, TypeInfoFactory.decimalTypeInfo,
                TypeInfoFactory.decimalTypeInfo);
     comparison(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.decimalTypeInfo,
-               TypeInfoFactory.getDecimalTypeInfo(65, 30));
+               TypeInfoFactory.decimalTypeInfo);
     comparison(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.stringTypeInfo,
                TypeInfoFactory.doubleTypeInfo);
 
@@ -319,7 +319,7 @@ public void testCommonClassUnionAll() {
     unionAll(TypeInfoFactory.stringTypeInfo, TypeInfoFactory.decimalTypeInfo,
         TypeInfoFactory.decimalTypeInfo);
     unionAll(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.decimalTypeInfo,
-        TypeInfoFactory.getDecimalTypeInfo(65, 30));
+        TypeInfoFactory.decimalTypeInfo);
     unionAll(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.stringTypeInfo,
         TypeInfoFactory.stringTypeInfo);
 

File: ql/src/test/org/apache/hadoop/hive/ql/parse/TestHiveDecimalParse.java
Patch:
@@ -61,7 +61,7 @@ public void testDecimalType3() throws ParseException {
     int rc = driver.compile(query);
     Assert.assertTrue("Got " + rc + ", expected not zero", rc != 0);
     Assert.assertTrue(driver.getErrorMsg(),
-        driver.getErrorMsg().contains("Decimal precision out of allowed range [1,65]"));
+        driver.getErrorMsg().contains("Decimal precision out of allowed range [1,38]"));
   }
 
   @Test
@@ -72,7 +72,7 @@ public void testDecimalType4() throws ParseException {
     int rc = driver.compile(query);
     Assert.assertTrue("Got " + rc + ", expected not zero", rc != 0);
     Assert.assertTrue(driver.getErrorMsg(),
-        driver.getErrorMsg().contains("Decimal precision out of allowed range [1,65]"));
+        driver.getErrorMsg().contains("Decimal precision out of allowed range [1,38]"));
   }
 
   @Test
@@ -83,7 +83,7 @@ public void testDecimalType5() throws ParseException {
     int rc = driver.compile(query);
     Assert.assertTrue("Got " + rc + ", expected not zero", rc != 0);
     Assert.assertTrue(driver.getErrorMsg(),
-        driver.getErrorMsg().contains("Decimal scale out of allowed range [0,30]"));
+        driver.getErrorMsg().contains("Decimal scale must be less than or equal to precision"));
   }
 
   @Test

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java
Patch:
@@ -64,8 +64,8 @@ private TypeInfoFactory() {
   /**
    * A DecimalTypeInfo instance that has max precision and max scale.
    */
-  public static final DecimalTypeInfo decimalTypeInfo = new DecimalTypeInfo(HiveDecimal.MAX_PRECISION,
-      HiveDecimal.MAX_SCALE);
+  public static final DecimalTypeInfo decimalTypeInfo = new DecimalTypeInfo(HiveDecimal.SYSTEM_DEFAULT_PRECISION,
+      HiveDecimal.SYSTEM_DEFAULT_SCALE);
 
   public static final PrimitiveTypeInfo unknownTypeInfo = new PrimitiveTypeInfo("unknown");
 

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
Patch:
@@ -420,8 +420,8 @@ private TypeInfo parseType() {
                 params.length + " is seen");
           }
         case DECIMAL:
-          int precision = HiveDecimal.DEFAULT_PRECISION;
-          int scale = HiveDecimal.DEFAULT_SCALE;
+          int precision = HiveDecimal.USER_DEFAULT_PRECISION;
+          int scale = HiveDecimal.USER_DEFAULT_SCALE;
           if (params == null || params.length == 0) {
             // It's possible that old metadata still refers to "decimal" as a column type w/o
             // precision/scale. In this case, the default (10,0) is assumed. Thus, do nothing here.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
Patch:
@@ -377,7 +377,7 @@ private void updateStats(String[] statsList, Statistics stats,
         if (work.getLoadTableDesc() != null &&
             !work.getLoadTableDesc().getReplace()) {
           String originalValue = parameters.get(statType);
-          if (originalValue != null) {
+          if (originalValue != null && !originalValue.equals("-1")) {
             longValue += Long.parseLong(originalValue);
           }
         }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -1138,8 +1138,8 @@ private static void getPartExprNodeDesc(ASTNode astNode,
       Map<ASTNode, ExprNodeDesc> astExprNodeMap)
           throws SemanticException, HiveException {
 
-    if ((astNode == null) || (astNode.getChildren() == null) ||
-        (astNode.getChildren().size() <= 1)) {
+    if ((astNode == null) || (astNode.getChildren() == null) || 
+        (astNode.getChildren().size() == 0)) {
       return;
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java
Patch:
@@ -201,7 +201,9 @@ public ASTNode parse(String command, Context ctx) throws ParseException {
       throw new ParseException(parser.errors);
     }
 
-    return (ASTNode) r.getTree();
+    ASTNode tree = (ASTNode) r.getTree();
+    tree.setUnknownTokenBoundaries();
+    return tree;
   }
 
 

File: itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -1533,8 +1533,8 @@ public static boolean queryListRunnerMultiThreaded(File[] qfiles, QTestUtil[] qt
   }
 
   public static void outputTestFailureHelpMessage() {
-    System.err.println("See build/ql/tmp/hive.log, "
-        + "or try \"ant test ... -Dtest.silent=false\" to get more logs.");
+    System.err.println("See ./ql/target/tmp/log/hive.log or ./itests/qtest/target/tmp/log/hive.log, "
+        + "or check ./ql/target/surefire-reports or ./itests/qtest/target/surefire-reports/ for specific test cases logs.");
     System.err.flush();
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java
Patch:
@@ -32,7 +32,7 @@
 public class ASTNode extends CommonTree implements Node,Serializable {
   private static final long serialVersionUID = 1L;
 
-  private ASTNodeOrigin origin;
+  private transient ASTNodeOrigin origin;
 
   public ASTNode() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java
Patch:
@@ -83,6 +83,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case DOUBLE:
       return new GenericUDAFDoubleStatsEvaluator();
     case STRING:
+    case CHAR:
     case VARCHAR:
       return new GenericUDAFStringStatsEvaluator();
     case BINARY:

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDateObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDoubleObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyFloatObjectInspector;
+import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveCharObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveVarcharObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector;
@@ -113,6 +114,8 @@ public final class LazyFactory {
       return new LazyDouble((LazyDoubleObjectInspector) oi);
     case STRING:
       return new LazyString((LazyStringObjectInspector) oi);
+    case CHAR:
+      return new LazyHiveChar((LazyHiveCharObjectInspector) oi);
     case VARCHAR:
       return new LazyHiveVarchar((LazyHiveVarcharObjectInspector) oi);
     case DATE:

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyHiveVarcharObjectInspector.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.serde2.lazy.LazyHiveVarchar;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveVarcharObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.VarcharUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.BaseCharUtils;
 
 public class LazyHiveVarcharObjectInspector
     extends AbstractPrimitiveLazyObjectInspector<HiveVarcharWritable>
@@ -55,7 +55,7 @@ public HiveVarchar getPrimitiveJavaObject(Object o) {
     }
 
     HiveVarchar ret = ((LazyHiveVarchar) o).getWritableObject().getHiveVarchar();
-    if (!VarcharUtils.doesPrimitiveMatchTypeParams(
+    if (!BaseCharUtils.doesPrimitiveMatchTypeParams(
         ret, (VarcharTypeInfo)typeInfo)) {
       HiveVarchar newValue = new HiveVarchar(ret, ((VarcharTypeInfo)typeInfo).getLength());
       return newValue;

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryFactory.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector;
@@ -72,6 +73,8 @@ public final class LazyBinaryFactory {
       return new LazyBinaryDouble((WritableDoubleObjectInspector) oi);
     case STRING:
       return new LazyBinaryString((WritableStringObjectInspector) oi);
+    case CHAR:
+      return new LazyBinaryHiveChar((WritableHiveCharObjectInspector) oi);
     case VARCHAR:
       return new LazyBinaryHiveVarchar((WritableHiveVarcharObjectInspector) oi);
     case VOID: // for NULL

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryUtils.java
Patch:
@@ -195,7 +195,7 @@ public static void checkObjectByteInfo(ObjectInspector objectInspector,
         recordInfo.elementOffset = vInt.length;
         recordInfo.elementSize = vInt.value;
         break;
-
+      case CHAR:
       case VARCHAR:
         LazyBinaryUtils.readVInt(bytes, offset, vInt);
         recordInfo.elementOffset = vInt.length;

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/PrimitiveObjectInspector.java
Patch:
@@ -31,7 +31,7 @@ public interface PrimitiveObjectInspector extends ObjectInspector {
    */
   public static enum PrimitiveCategory {
     VOID, BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, STRING,
-    DATE, TIMESTAMP, BINARY, DECIMAL, VARCHAR, UNKNOWN
+    DATE, TIMESTAMP, BINARY, DECIMAL, VARCHAR, CHAR, UNKNOWN
   };
 
   public PrimitiveTypeInfo getTypeInfo();

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaHiveVarcharObjectInspector.java
Patch:
@@ -20,7 +20,7 @@
 import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.serde2.io.HiveVarcharWritable;
 import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.VarcharUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.BaseCharUtils;
 
 public class JavaHiveVarcharObjectInspector extends AbstractPrimitiveJavaObjectInspector
 implements SettableHiveVarcharObjectInspector {
@@ -39,7 +39,7 @@ public HiveVarchar getPrimitiveJavaObject(Object o) {
       return null;
     }
     HiveVarchar value = (HiveVarchar)o;
-    if (VarcharUtils.doesPrimitiveMatchTypeParams(
+    if (BaseCharUtils.doesPrimitiveMatchTypeParams(
         value, (VarcharTypeInfo)typeInfo)) {
       return value;
     }
@@ -69,7 +69,7 @@ private HiveVarcharWritable getWritableWithParams(HiveVarchar val) {
   @Override
   public Object set(Object o, HiveVarchar value) {
     HiveVarchar setValue = (HiveVarchar)o;
-    if (VarcharUtils.doesPrimitiveMatchTypeParams(
+    if (BaseCharUtils.doesPrimitiveMatchTypeParams(
         value, (VarcharTypeInfo)typeInfo)) {
       setValue.setValue(value);
     } else {

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableHiveVarcharObjectInspector.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.serde2.io.HiveVarcharWritable;
 import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;
-import org.apache.hadoop.hive.serde2.typeinfo.VarcharUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.BaseCharUtils;
 
 public class WritableHiveVarcharObjectInspector extends AbstractPrimitiveWritableObjectInspector
 implements SettableHiveVarcharObjectInspector {
@@ -78,7 +78,7 @@ private HiveVarcharWritable getWritableWithParams(HiveVarcharWritable val) {
   }
 
   private boolean doesWritableMatchTypeParams(HiveVarcharWritable writable) {
-    return VarcharUtils.doesWritableMatchTypeParams(
+    return BaseCharUtils.doesWritableMatchTypeParams(
         writable, (VarcharTypeInfo)typeInfo);
   }
 

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/VarcharTypeInfo.java
Patch:
@@ -29,7 +29,7 @@ public VarcharTypeInfo() {
 
   public VarcharTypeInfo(int length) {
     super(serdeConstants.VARCHAR_TYPE_NAME, length);
-    VarcharUtils.validateParameter(length);
+    BaseCharUtils.validateVarcharParameter(length);
   }
 
   @Override

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -507,6 +507,9 @@ public static enum ConfVars {
     HIVE_ORC_FILE_MEMORY_POOL("hive.exec.orc.memory.pool", 0.5f), // 50%
     // Define the version of the file to write
     HIVE_ORC_WRITE_FORMAT("hive.exec.orc.write.format", null),
+    // Define the default ORC stripe size
+    HIVE_ORC_DEFAULT_STRIPE_SIZE("hive.exec.orc.default.stripe.size",
+        256L * 1024 * 1024),
 
     HIVE_ORC_DICTIONARY_KEY_SIZE_THRESHOLD("hive.exec.orc.dictionary.key.size.threshold", 0.8f),
 

File: testutils/ptest2/src/main/java/org/apache/hive/ptest/api/server/ExecutionController.java
Patch:
@@ -150,7 +150,7 @@ private TestStartResponse doStartTest(TestStartRequest startRequest, BindingResu
     String testHandle = stopRequest.getTestHandle();
     Test test = mTests.get(testHandle);
     if(result.hasErrors() ||
-        Strings.emptyToNull(stopRequest.getTestHandle()).trim().isEmpty() ||
+        Strings.nullToEmpty(stopRequest.getTestHandle()).trim().isEmpty() ||
         test == null) {
       return new TestStopResponse(Status.illegalArgument());
     }
@@ -164,7 +164,7 @@ private TestStartResponse doStartTest(TestStartRequest startRequest, BindingResu
     String testHandle = stopRequest.getTestHandle();
     Test test = mTests.get(testHandle);
     if(result.hasErrors() ||
-        Strings.emptyToNull(stopRequest.getTestHandle()).trim().isEmpty() ||
+        Strings.nullToEmpty(stopRequest.getTestHandle()).trim().isEmpty() ||
         test == null) {
       return new TestStatusResponse(Status.illegalArgument());
     }
@@ -177,7 +177,7 @@ private TestStartResponse doStartTest(TestStartRequest startRequest, BindingResu
     String testHandle = logsRequest.getTestHandle();
     Test testExecution = mTests.get(testHandle);
     if(result.hasErrors() ||
-        Strings.emptyToNull(logsRequest.getTestHandle()).trim().isEmpty() ||
+        Strings.nullToEmpty(logsRequest.getTestHandle()).trim().isEmpty() ||
         testExecution == null ||
         logsRequest.getLength() > MAX_READ_SIZE) {
       return new TestLogResponse(Status.illegalArgument());

File: testutils/ptest2/src/main/java/org/apache/hive/ptest/execution/LocalCommandFactory.java
Patch:
@@ -24,7 +24,6 @@
 
 public class LocalCommandFactory {
 
-
   private final Logger mLogger;
 
   public LocalCommandFactory(Logger logger) {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeConstantDesc.java
Patch:
@@ -59,11 +59,11 @@ public Object getValue() {
 
   @Override
   public ConstantObjectInspector getWritableObjectInspector() {
-    PrimitiveCategory pc = ((PrimitiveTypeInfo)getTypeInfo())
-        .getPrimitiveCategory();
+    PrimitiveTypeInfo pti = (PrimitiveTypeInfo) getTypeInfo();
+    PrimitiveCategory pc = pti.getPrimitiveCategory();
     // Convert from Java to Writable
     Object writableValue = PrimitiveObjectInspectorFactory
-        .getPrimitiveJavaObjectInspector(pc).getPrimitiveWritableObject(
+        .getPrimitiveJavaObjectInspector(pti).getPrimitiveWritableObject(
           getValue());
     return PrimitiveObjectInspectorFactory
         .getPrimitiveWritableConstantObjectInspector((PrimitiveTypeInfo) getTypeInfo(), writableValue);

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java
Patch:
@@ -24,6 +24,7 @@
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
@@ -48,6 +49,7 @@ private TypeInfoFactory() {
   public static final PrimitiveTypeInfo intTypeInfo = new PrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME);
   public static final PrimitiveTypeInfo longTypeInfo = new PrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME);
   public static final PrimitiveTypeInfo stringTypeInfo = new PrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME);
+  public static final PrimitiveTypeInfo varcharTypeInfo = new VarcharTypeInfo(HiveVarchar.MAX_VARCHAR_LENGTH);
   public static final PrimitiveTypeInfo floatTypeInfo = new PrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME);
   public static final PrimitiveTypeInfo doubleTypeInfo = new PrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME);
   public static final PrimitiveTypeInfo byteTypeInfo = new PrimitiveTypeInfo(serdeConstants.TINYINT_TYPE_NAME);
@@ -74,6 +76,7 @@ private TypeInfoFactory() {
     cachedPrimitiveTypeInfo.put(serdeConstants.INT_TYPE_NAME, intTypeInfo);
     cachedPrimitiveTypeInfo.put(serdeConstants.BIGINT_TYPE_NAME, longTypeInfo);
     cachedPrimitiveTypeInfo.put(serdeConstants.STRING_TYPE_NAME, stringTypeInfo);
+    cachedPrimitiveTypeInfo.put(varcharTypeInfo.getQualifiedName(), varcharTypeInfo);
     cachedPrimitiveTypeInfo.put(serdeConstants.FLOAT_TYPE_NAME, floatTypeInfo);
     cachedPrimitiveTypeInfo.put(serdeConstants.DOUBLE_TYPE_NAME, doubleTypeInfo);
     cachedPrimitiveTypeInfo.put(serdeConstants.TINYINT_TYPE_NAME, byteTypeInfo);

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveResultSetMetaData.java
Patch:
@@ -114,7 +114,7 @@ public String getColumnTypeName(int column) throws SQLException {
       return serdeConstants.DATE_TYPE_NAME;
     } else if ("timestamp".equalsIgnoreCase(type)) {
       return serdeConstants.TIMESTAMP_TYPE_NAME;
-    } else if ("decimal".equalsIgnoreCase(type)) {
+    } else if (type.startsWith("decimal")) {
       return serdeConstants.DECIMAL_TYPE_NAME;
     } else if (type.startsWith("map<")) {
       return serdeConstants.STRING_TYPE_NAME;

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/Utils.java
Patch:
@@ -50,7 +50,7 @@ public static int hiveTypeToSqlType(String type) throws SQLException {
       return Types.DATE;
     } else if ("timestamp".equalsIgnoreCase(type)) {
       return Types.TIMESTAMP;
-    } else if ("decimal".equalsIgnoreCase(type)) {
+    } else if (type.startsWith("decimal")) {
       return Types.DECIMAL;
     } else if (type.startsWith("map<")) {
       return Types.VARCHAR;

File: jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
Patch:
@@ -150,7 +150,7 @@ protected void setUp() throws Exception {
         + " c15 struct<r:int,s:struct<a:int,b:string>>,"
         + " c16 array<struct<m:map<string,string>,n:int>>,"
         + " c17 timestamp, "
-        + " c18 decimal,"
+        + " c18 decimal(16,7),"
         + " c19 binary,"
         + " c20 date) comment'" + dataTypeTableComment
             +"' partitioned by (dt STRING)");

File: jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java
Patch:
@@ -180,7 +180,7 @@ public void setUp() throws Exception {
         + " c15 struct<r:int,s:struct<a:int,b:string>>,"
         + " c16 array<struct<m:map<string,string>,n:int>>,"
         + " c17 timestamp, "
-        + " c18 decimal, "
+        + " c18 decimal(16,7), "
         + " c19 binary, "
         + " c20 date,"
         + " c21 varchar(20)"

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToVarchar.java
Patch:
@@ -90,8 +90,8 @@ public String getDisplayString(String[] children) {
     StringBuilder sb = new StringBuilder();
     sb.append("CAST( ");
     sb.append(children[0]);
-    sb.append(" AS VARCHAR(");
-    sb.append("" + typeInfo.getLength());
+    sb.append(" AS ");
+    sb.append(typeInfo.getQualifiedName());
     sb.append(")");
     return sb.toString();
   }

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java
Patch:
@@ -208,7 +208,7 @@ public void testCommonClass() {
     common(TypeInfoFactory.stringTypeInfo, TypeInfoFactory.decimalTypeInfo,
            TypeInfoFactory.stringTypeInfo);
     common(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.decimalTypeInfo,
-           TypeInfoFactory.decimalTypeInfo);
+           TypeInfoFactory.getDecimalTypeInfo(65, 30));
     common(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.stringTypeInfo,
            TypeInfoFactory.stringTypeInfo);
 
@@ -226,7 +226,7 @@ public void testCommonClassComparison() {
     comparison(TypeInfoFactory.stringTypeInfo, TypeInfoFactory.decimalTypeInfo,
                TypeInfoFactory.decimalTypeInfo);
     comparison(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.decimalTypeInfo,
-               TypeInfoFactory.decimalTypeInfo);
+               TypeInfoFactory.getDecimalTypeInfo(65, 30));
     comparison(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.stringTypeInfo,
                TypeInfoFactory.doubleTypeInfo);
 
@@ -296,7 +296,7 @@ public void testCommonClassUnionAll() {
     unionAll(TypeInfoFactory.stringTypeInfo, TypeInfoFactory.decimalTypeInfo,
         TypeInfoFactory.decimalTypeInfo);
     unionAll(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.decimalTypeInfo,
-        TypeInfoFactory.decimalTypeInfo);
+        TypeInfoFactory.getDecimalTypeInfo(65, 30));
     unionAll(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.stringTypeInfo,
         TypeInfoFactory.stringTypeInfo);
 

File: serde/src/java/org/apache/hadoop/hive/serde2/RegexSerDe.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
@@ -231,7 +232,7 @@ public Object deserialize(Writable blob) throws SerDeException {
           Date d;
           d = Date.valueOf(t);
           row.set(c, d);
-        } else if (typeName.equals(serdeConstants.DECIMAL_TYPE_NAME)) {
+        } else if (typeInfo instanceof DecimalTypeInfo) {
           HiveDecimal bd = HiveDecimal.create(t);
           row.set(c, bd);
         } else if (typeInfo instanceof VarcharTypeInfo) {

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/VarcharTypeInfo.java
Patch:
@@ -45,15 +45,15 @@ public boolean equals(Object other) {
 
     VarcharTypeInfo pti = (VarcharTypeInfo) other;
 
-    return this.typeName.equals(pti.typeName) && this.getLength() == pti.getLength();
+    return this.getLength() == pti.getLength();
   }
 
   /**
    * Generate the hashCode for this TypeInfo.
    */
   @Override
   public int hashCode() {
-    return getQualifiedName().hashCode();
+    return getLength();
   }
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
Patch:
@@ -354,7 +354,7 @@ public void initialize(Configuration hconf, ObjectInspector[] inputOIs)
     // initialize structure to maintain child op info. operator tree changes
     // while
     // initializing so this need to be done here instead of initialize() method
-    if (childOperators != null) {
+    if (childOperators != null && !childOperators.isEmpty()) {
       childOperatorsArray = new Operator[childOperators.size()];
       for (int i = 0; i < childOperatorsArray.length; i++) {
         childOperatorsArray[i] = childOperators.get(i);
@@ -411,7 +411,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
   protected void initializeChildren(Configuration hconf) throws HiveException {
     state = State.INIT;
     LOG.info("Operator " + id + " " + getName() + " initialized");
-    if (childOperators == null) {
+    if (childOperators == null || childOperators.isEmpty()) {
       return;
     }
     LOG.info("Initializing children of " + id + " " + getName());
@@ -780,7 +780,7 @@ public boolean removeChildren(int depth) {
     Operator<? extends OperatorDesc> currOp = this;
     for (int i = 0; i < depth; i++) {
       // If there are more than 1 children at any level, don't do anything
-      if ((currOp.getChildOperators() == null) ||
+      if ((currOp.getChildOperators() == null) || (currOp.getChildOperators().isEmpty()) || 
           (currOp.getChildOperators().size() > 1)) {
         return false;
       }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java
Patch:
@@ -162,7 +162,7 @@ public Object process(Node nd, Stack<Node> stack,
     ParseContext parseContext = context.parseContext;
     MapJoinOperator mapJoinOp = MapJoinProcessor.
       convertJoinOpMapJoinOp(context.conf, parseContext.getOpParseCtx(),
-      joinOp, parseContext.getJoinContext().get(joinOp), bigTablePosition, true, false);
+      joinOp, parseContext.getJoinContext().get(joinOp), bigTablePosition, true);
 
     Operator<? extends OperatorDesc> parentBigTableOp
       = mapJoinOp.getParentOperators().get(bigTablePosition);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GroupByOptimizer.java
Patch:
@@ -286,7 +286,7 @@ protected GroupByOptimizerSortMatch checkSortGroupBy(Stack<Node> stack,
       currOp = currOp.getParentOperators().get(0);
 
       while (true) {
-        if (currOp.getParentOperators() == null) {
+        if ((currOp.getParentOperators() == null) || (currOp.getParentOperators().isEmpty())) {
           break;
         }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/SkewJoinOptimizer.java
Patch:
@@ -398,7 +398,8 @@ private Table getTable(
             return parseContext.getTopToTable().get(tsOp);
           }
         }
-        if ((op.getParentOperators() == null) || (op.getParentOperators().size() > 1)) {
+        if ((op.getParentOperators() == null) || (op.getParentOperators().isEmpty()) || 
+            (op.getParentOperators().size() > 1)) {
           return null;
         }
         op = op.getParentOperators().get(0);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationOptimizer.java
Patch:
@@ -346,7 +346,7 @@ private LinkedHashSet<ReduceSinkOperator> findCorrelatedReduceSinkOperators(
             "involved in this operator");
         return correlatedReduceSinkOperators;
       }
-      if (current.getParentOperators() == null) {
+      if ((current.getParentOperators() == null) || (current.getParentOperators().isEmpty())) {
         return correlatedReduceSinkOperators;
       }
       if (current instanceof PTFOperator) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SortMergeJoinTaskDispatcher.java
Patch:
@@ -204,7 +204,7 @@ private boolean isEligibleForOptimization(SMBMapJoinOperator originalSMBJoinOp)
 
     Operator<? extends OperatorDesc> currOp = originalSMBJoinOp;
     while (true) {
-      if (currOp.getChildOperators() == null) {
+      if ((currOp.getChildOperators() == null) || (currOp.getChildOperators().isEmpty())) {
         if (currOp instanceof FileSinkOperator) {
           FileSinkOperator fsOp = (FileSinkOperator)currOp;
           // The query has enforced that a sort-merge join should be performed.

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TableAccessAnalyzer.java
Patch:
@@ -239,7 +239,7 @@ public static TableScanOperator genRootTableScan(
     // and filters.
     while (true) {
       parentOps = currOp.getParentOperators();
-      if (parentOps == null) {
+      if ((parentOps == null) || (parentOps.isEmpty())) {
         return (TableScanOperator) currOp;
       }
 

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InputJobInfo.java
Patch:
@@ -36,7 +36,7 @@
  * Container for metadata read from the metadata server.
  * Prior to release 0.5, InputJobInfo was a key part of the public API, exposed directly
  * to end-users as an argument to
- * {@link HCatInputFormat#setInput(org.apache.hadoop.mapreduce.Job, InputJobInfo)}.
+ * HCatInputFormat#setInput(org.apache.hadoop.mapreduce.Job, InputJobInfo).
  * Going forward, we plan on treating InputJobInfo as an implementation detail and no longer
  * expose to end-users. Should you have a need to use InputJobInfo outside HCatalog itself,
  * please contact the developer mailing list before depending on this class.

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/PigDelegator.java
Patch:
@@ -67,7 +67,7 @@ public EnqueueBean run(String user, Map<String, Object> userArgs,
    * @param usehcatalog whether the command uses hcatalog/needs to connect
    *         to hive metastore server
    * @param completedUrl call back url
-   * @return
+   * @return list of arguments
    * @throws BadParam
    * @throws IOException
    * @throws InterruptedException

File: metastore/src/test/org/apache/hadoop/hive/metastore/TestMetastoreVersion.java
Patch:
@@ -89,6 +89,7 @@ public void testVersionRestriction () throws Exception {
     // session creation should fail since the schema didn't get created
     try {
       SessionState.start(new CliSessionState(hiveConf));
+      fail("Expected exception");
     } catch (RuntimeException re) {
       assertTrue(re.getCause().getCause() instanceof MetaException);
     }

File: shims/src/common/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java
Patch:
@@ -87,7 +87,7 @@ public TUGIContainingTransport getTransport(TTransport trans) {
         tugiTrans = new TUGIContainingTransport(trans);
         TUGIContainingTransport prev = transMap.putIfAbsent(trans, tugiTrans);
         if (prev != null) {
-          return prev
+          return prev;
         }
       }
       return tugiTrans;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
Patch:
@@ -227,7 +227,9 @@ public void processOp(Object row, int tag) throws HiveException {
 
   @Override
   public void closeOp(boolean abort) throws HiveException {
-    if (mapJoinTables != null) {
+    if ((this.getExecContext().getLocalWork() != null
+        && this.getExecContext().getLocalWork().getInputFileChangeSensitive())
+        && mapJoinTables != null) {
       for (MapJoinTableContainer tableContainer : mapJoinTables) {
         if (tableContainer != null) {
           tableContainer.clear();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobMonitor.java
Patch:
@@ -102,6 +102,9 @@ public int monitorExecution(DAGClient dagClient) throws InterruptedException {
             if (!running) {
               perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_SUBMIT_TO_RUNNING);
               console.printInfo("Status: Running\n");
+              for (String s: progressMap.keySet()) {
+                perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_RUN_VERTEX + s);
+              }
               running = true;
             }
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverCommonJoin.java
Patch:
@@ -135,7 +135,7 @@ public List<Task<? extends Serializable>> getTasks(HiveConf conf, Object objCtx)
     return resTsks;
   }
 
-  class AliasFileSizePair implements Comparable<AliasFileSizePair> {
+  static class AliasFileSizePair implements Comparable<AliasFileSizePair> {
     String alias;
     long size;
     AliasFileSizePair(String alias, long size) {
@@ -148,7 +148,7 @@ public int compareTo(AliasFileSizePair o) {
       if (o == null) {
         return 1;
       }
-      return (int)(size - o.size);
+      return (size < o.size) ? -1 : ((size > o.size) ? 1 : 0);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeConstantDesc.java
Patch:
@@ -66,8 +66,7 @@ public ConstantObjectInspector getWritableObjectInspector() {
         .getPrimitiveJavaObjectInspector(pc).getPrimitiveWritableObject(
           getValue());
     return PrimitiveObjectInspectorFactory
-        .getPrimitiveWritableConstantObjectInspector(
-            (PrimitiveTypeInfo) getTypeInfo(), writableValue);
+        .getPrimitiveWritableConstantObjectInspector((PrimitiveTypeInfo) getTypeInfo(), writableValue);
   }
 
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/SettableUDF.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 
 /**
  * THIS INTERFACE IS UNSTABLE AND SHOULD NOT BE USED BY 3RD PARTY UDFS.
@@ -31,8 +32,8 @@ public interface SettableUDF {
    * An exception may be thrown if the UDF doesn't know what to do with this data.
    * @param params UDF-specific data to add to the UDF
    */
-  void setParams(Object params) throws UDFArgumentException;
+  void setTypeInfo(TypeInfo typeInfo) throws UDFArgumentException;
 
-  Object getParams();
+  TypeInfo getTypeInfo();
 
 }

File: ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFMacro.java
Patch:
@@ -62,10 +62,10 @@ public void setup() throws Exception {
     inspectors = new ObjectInspector[] {
         PrimitiveObjectInspectorFactory.
           getPrimitiveWritableConstantObjectInspector(
-            PrimitiveObjectInspector.PrimitiveCategory.INT, x),
+              TypeInfoFactory.intTypeInfo, x),
         PrimitiveObjectInspectorFactory.
           getPrimitiveWritableConstantObjectInspector(
-            PrimitiveObjectInspector.PrimitiveCategory.INT, y),
+              TypeInfoFactory.intTypeInfo, y),
     };
     arguments = new DeferredObject[] {
         new DeferredJavaObject(x),

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
Patch:
@@ -54,7 +54,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
-import org.apache.hadoop.hive.serde2.typeinfo.BaseTypeParams;
 import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
@@ -219,7 +218,6 @@ public static ObjectInspector createLazyObjectInspector(TypeInfo typeInfo,
     ObjectInspector.Category c = typeInfo.getCategory();
     switch (c) {
     case PRIMITIVE:
-      BaseTypeParams typeParams = ((PrimitiveTypeInfo)typeInfo).getTypeParams();
       return LazyPrimitiveObjectInspectorFactory.getLazyObjectInspector(
           (PrimitiveTypeInfo) typeInfo, escaped, escapeChar);
     case MAP:

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DateObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector;
@@ -45,7 +46,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.DateObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 import org.apache.hadoop.io.BytesWritable;

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/AbstractPrimitiveLazyObjectInspector.java
Patch:
@@ -19,7 +19,7 @@
 
 import org.apache.hadoop.hive.serde2.lazy.LazyPrimitive;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveTypeEntry;
+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.io.Writable;
 
 /**
@@ -31,8 +31,8 @@ public abstract class AbstractPrimitiveLazyObjectInspector<T extends Writable>
   protected AbstractPrimitiveLazyObjectInspector() {
     super();
   }
-  protected AbstractPrimitiveLazyObjectInspector(PrimitiveTypeEntry typeEntry) {
-    super(typeEntry);
+  protected AbstractPrimitiveLazyObjectInspector(PrimitiveTypeInfo typeInfo) {
+    super(typeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyBinaryObjectInspector.java
Patch:
@@ -21,15 +21,15 @@
 import org.apache.hadoop.hive.serde2.lazy.LazyBinary;
 import org.apache.hadoop.hive.serde2.lazy.LazyUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.BytesWritable;
 
 public class LazyBinaryObjectInspector extends
   AbstractPrimitiveLazyObjectInspector<BytesWritable> implements
     BinaryObjectInspector {
 
   public LazyBinaryObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.binaryTypeEntry);
+    super(TypeInfoFactory.binaryTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyBooleanObjectInspector.java
Patch:
@@ -19,7 +19,7 @@
 
 import org.apache.hadoop.hive.serde2.lazy.LazyBoolean;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.BooleanWritable;
 
 /**
@@ -30,7 +30,7 @@ public class LazyBooleanObjectInspector extends
     BooleanObjectInspector {
 
   LazyBooleanObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.booleanTypeEntry);
+    super(TypeInfoFactory.booleanTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyByteObjectInspector.java
Patch:
@@ -20,7 +20,7 @@
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.lazy.LazyByte;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A WritableByteObjectInspector inspects a ByteWritable Object.
@@ -30,7 +30,7 @@ public class LazyByteObjectInspector extends
     ByteObjectInspector {
 
   LazyByteObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.byteTypeEntry);
+    super(TypeInfoFactory.byteTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyDateObjectInspector.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hadoop.hive.serde2.io.DateWritable;
 import org.apache.hadoop.hive.serde2.lazy.LazyDate;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.DateObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A WritableDateObjectInspector inspects a DateWritable Object.
@@ -32,7 +32,7 @@ public class LazyDateObjectInspector
     implements DateObjectInspector {
 
   protected LazyDateObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.dateTypeEntry);
+    super(TypeInfoFactory.dateTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyDoubleObjectInspector.java
Patch:
@@ -20,7 +20,7 @@
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.lazy.LazyDouble;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A WritableDoubleObjectInspector inspects a DoubleWritable Object.
@@ -30,7 +30,7 @@ public class LazyDoubleObjectInspector extends
     DoubleObjectInspector {
 
   LazyDoubleObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.doubleTypeEntry);
+    super(TypeInfoFactory.doubleTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyFloatObjectInspector.java
Patch:
@@ -19,7 +19,7 @@
 
 import org.apache.hadoop.hive.serde2.lazy.LazyFloat;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.FloatWritable;
 
 /**
@@ -30,7 +30,7 @@ public class LazyFloatObjectInspector extends
     FloatObjectInspector {
 
   LazyFloatObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.floatTypeEntry);
+    super(TypeInfoFactory.floatTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyHiveDecimalObjectInspector.java
Patch:
@@ -22,14 +22,14 @@
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 public class LazyHiveDecimalObjectInspector
     extends AbstractPrimitiveLazyObjectInspector<HiveDecimalWritable>
     implements HiveDecimalObjectInspector {
 
   protected LazyHiveDecimalObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.decimalTypeEntry);
+    super(TypeInfoFactory.decimalTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyIntObjectInspector.java
Patch:
@@ -19,7 +19,7 @@
 
 import org.apache.hadoop.hive.serde2.lazy.LazyInteger;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.IntWritable;
 
 /**
@@ -30,7 +30,7 @@ public class LazyIntObjectInspector extends
     IntObjectInspector {
 
   LazyIntObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.intTypeEntry);
+    super(TypeInfoFactory.intTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyLongObjectInspector.java
Patch:
@@ -19,7 +19,7 @@
 
 import org.apache.hadoop.hive.serde2.lazy.LazyLong;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.LongWritable;
 
 /**
@@ -30,7 +30,7 @@ public class LazyLongObjectInspector extends
     LongObjectInspector {
 
   LazyLongObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.longTypeEntry);
+    super(TypeInfoFactory.longTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyShortObjectInspector.java
Patch:
@@ -19,8 +19,8 @@
 
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.lazy.LazyShort;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A WritableShortObjectInspector inspects a ShortWritable Object.
@@ -30,7 +30,7 @@ public class LazyShortObjectInspector extends
     ShortObjectInspector {
 
   LazyShortObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.shortTypeEntry);
+    super(TypeInfoFactory.shortTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyStringObjectInspector.java
Patch:
@@ -18,8 +18,8 @@
 package org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive;
 
 import org.apache.hadoop.hive.serde2.lazy.LazyString;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.Text;
 
 /**
@@ -36,7 +36,7 @@ protected LazyStringObjectInspector() {
   }
 
   LazyStringObjectInspector(boolean escaped, byte escapeChar) {
-    super(PrimitiveObjectInspectorUtils.stringTypeEntry);
+    super(TypeInfoFactory.stringTypeInfo);
     this.escaped = escaped;
     this.escapeChar = escapeChar;
   }

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyTimestampObjectInspector.java
Patch:
@@ -21,15 +21,15 @@
 
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.lazy.LazyTimestamp;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 public class LazyTimestampObjectInspector
     extends AbstractPrimitiveLazyObjectInspector<TimestampWritable>
     implements TimestampObjectInspector {
 
   protected LazyTimestampObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.timestampTypeEntry);
+    super(TypeInfoFactory.timestampTypeInfo);
   }
 
   public Object copyObject(Object o) {

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyVoidObjectInspector.java
Patch:
@@ -17,8 +17,8 @@
  */
 package org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive;
 
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.VoidObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.NullWritable;
 
 /**
@@ -29,7 +29,7 @@ public class LazyVoidObjectInspector extends
     VoidObjectInspector {
 
   LazyVoidObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.voidTypeEntry);
+    super(TypeInfoFactory.voidTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
Patch:
@@ -39,6 +39,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableTimestampObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.VoidObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 
 /**
  * ObjectInspectorConverters.
@@ -222,7 +223,8 @@ private static ObjectInspector getConvertedOI(
     case PRIMITIVE:
       // Create a writable object inspector for primitive type and return it.
       PrimitiveObjectInspector primOutputOI = (PrimitiveObjectInspector) outputOI;
-      return PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(primOutputOI);
+      return PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(
+          (PrimitiveTypeInfo)primOutputOI.getTypeInfo());
     case STRUCT:
       StructObjectInspector structOutputOI = (StructObjectInspector) outputOI;
       // create a standard settable struct object inspector.

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/AbstractPrimitiveJavaObjectInspector.java
Patch:
@@ -17,7 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveTypeEntry;
+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 
 /**
  * An AbstractJavaPrimitiveObjectInspector for a Java object.
@@ -28,8 +28,8 @@ public abstract class AbstractPrimitiveJavaObjectInspector extends
   protected AbstractPrimitiveJavaObjectInspector() {
     super();
   }
-  protected AbstractPrimitiveJavaObjectInspector(PrimitiveTypeEntry typeEntry) {
-    super(typeEntry);
+  protected AbstractPrimitiveJavaObjectInspector(PrimitiveTypeInfo typeInfo) {
+    super(typeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/AbstractPrimitiveWritableObjectInspector.java
Patch:
@@ -17,7 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveTypeEntry;
+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 
 /**
  * An AbstractWritablePrimitiveObjectInspector for a Writable object.
@@ -29,8 +29,8 @@ protected AbstractPrimitiveWritableObjectInspector() {
     super();
   }
   protected AbstractPrimitiveWritableObjectInspector(
-      PrimitiveTypeEntry typeEntry) {
-    super(typeEntry);
+      PrimitiveTypeInfo typeInfo) {
+    super(typeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaBinaryObjectInspector.java
Patch:
@@ -3,6 +3,7 @@
 import java.util.Arrays;
 
 import org.apache.hadoop.hive.serde2.lazy.LazyUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.BytesWritable;
 
 /**
@@ -27,7 +28,7 @@ public class JavaBinaryObjectInspector extends AbstractPrimitiveJavaObjectInspec
     SettableBinaryObjectInspector {
 
   JavaBinaryObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.binaryTypeEntry);
+    super(TypeInfoFactory.binaryTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaBooleanObjectInspector.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.BooleanWritable;
 
 /**
@@ -27,7 +28,7 @@ public class JavaBooleanObjectInspector extends
     SettableBooleanObjectInspector {
 
   JavaBooleanObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.booleanTypeEntry);
+    super(TypeInfoFactory.booleanTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaByteObjectInspector.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A JavaByteObjectInspector inspects a Java Byte Object.
@@ -26,7 +27,7 @@ public class JavaByteObjectInspector extends
     AbstractPrimitiveJavaObjectInspector implements SettableByteObjectInspector {
 
   JavaByteObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.byteTypeEntry);
+    super(TypeInfoFactory.byteTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaDateObjectInspector.java
Patch:
@@ -20,6 +20,7 @@
 import java.sql.Date;
 
 import org.apache.hadoop.hive.serde2.io.DateWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A JavaDateObjectInspector inspects a Java Date Object.
@@ -29,7 +30,7 @@ public class JavaDateObjectInspector
     implements SettableDateObjectInspector {
 
   protected JavaDateObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.dateTypeEntry);
+    super(TypeInfoFactory.dateTypeInfo);
   }
 
   public DateWritable getPrimitiveWritableObject(Object o) {

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaDoubleObjectInspector.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A JavaDoubleObjectInspector inspects a Java Double Object.
@@ -27,7 +28,7 @@ public class JavaDoubleObjectInspector extends
     SettableDoubleObjectInspector {
 
   JavaDoubleObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.doubleTypeEntry);
+    super(TypeInfoFactory.doubleTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaFloatObjectInspector.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.FloatWritable;
 
 /**
@@ -27,7 +28,7 @@ public class JavaFloatObjectInspector extends
     SettableFloatObjectInspector {
 
   JavaFloatObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.floatTypeEntry);
+    super(TypeInfoFactory.floatTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaHiveDecimalObjectInspector.java
Patch:
@@ -21,13 +21,14 @@
 
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 public class JavaHiveDecimalObjectInspector
     extends AbstractPrimitiveJavaObjectInspector
     implements SettableHiveDecimalObjectInspector {
 
   protected JavaHiveDecimalObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.decimalTypeEntry);
+    super(TypeInfoFactory.decimalTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaIntObjectInspector.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.IntWritable;
 
 /**
@@ -26,7 +27,7 @@ public class JavaIntObjectInspector extends
     AbstractPrimitiveJavaObjectInspector implements SettableIntObjectInspector {
 
   JavaIntObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.intTypeEntry);
+    super(TypeInfoFactory.intTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaLongObjectInspector.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.LongWritable;
 
 /**
@@ -26,7 +27,7 @@ public class JavaLongObjectInspector extends
     AbstractPrimitiveJavaObjectInspector implements SettableLongObjectInspector {
 
   JavaLongObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.longTypeEntry);
+    super(TypeInfoFactory.longTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaShortObjectInspector.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A JavaShortObjectInspector inspects a Java Short Object.
@@ -27,7 +28,7 @@ public class JavaShortObjectInspector extends
     SettableShortObjectInspector {
 
   JavaShortObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.shortTypeEntry);
+    super(TypeInfoFactory.shortTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaStringObjectInspector.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.Text;
 
 /**
@@ -27,7 +28,7 @@ public class JavaStringObjectInspector extends
     SettableStringObjectInspector {
 
   JavaStringObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.stringTypeEntry);
+    super(TypeInfoFactory.stringTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaTimestampObjectInspector.java
Patch:
@@ -20,13 +20,14 @@
 import java.sql.Timestamp;
 
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 public class JavaTimestampObjectInspector
     extends AbstractPrimitiveJavaObjectInspector
     implements SettableTimestampObjectInspector {
 
   protected JavaTimestampObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.timestampTypeEntry);
+    super(TypeInfoFactory.timestampTypeInfo);
   }
 
   public TimestampWritable getPrimitiveWritableObject(Object o) {

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaVoidObjectInspector.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.NullWritable;
 
 /**
@@ -26,7 +27,7 @@ public class JavaVoidObjectInspector extends
     AbstractPrimitiveJavaObjectInspector implements VoidObjectInspector {
 
   JavaVoidObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.voidTypeEntry);
+    super(TypeInfoFactory.voidTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java
Patch:
@@ -29,7 +29,6 @@
 import org.apache.hadoop.hive.serde2.lazy.LazyLong;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeParams;
 import org.apache.hadoop.io.Text;
 
 /**
@@ -452,7 +451,6 @@ public HiveVarcharConverter(PrimitiveObjectInspector inputOI,
         SettableHiveVarcharObjectInspector outputOI) {
       this.inputOI = inputOI;
       this.outputOI = outputOI;
-      VarcharTypeParams typeParams = (VarcharTypeParams) outputOI.getTypeParams();
 
       // unfortunately we seem to get instances of varchar object inspectors without params
       // when an old-style UDF has an evaluate() method with varchar arguments.

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableBinaryObjectInspector.java
Patch:
@@ -21,6 +21,7 @@
 import java.util.Arrays;
 
 import org.apache.hadoop.hive.serde2.lazy.LazyUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.BytesWritable;
 /**
  * A WritableBinaryObjectInspector inspects a BytesWritable Object.
@@ -29,7 +30,7 @@ public class WritableBinaryObjectInspector extends AbstractPrimitiveWritableObje
     implements SettableBinaryObjectInspector {
 
   WritableBinaryObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.binaryTypeEntry);
+    super(TypeInfoFactory.binaryTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableBooleanObjectInspector.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.BooleanWritable;
 
 /**
@@ -27,7 +28,7 @@ public class WritableBooleanObjectInspector extends
     SettableBooleanObjectInspector {
 
   WritableBooleanObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.booleanTypeEntry);
+    super(TypeInfoFactory.booleanTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableByteObjectInspector.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A WritableByteObjectInspector inspects a ByteWritable Object.
@@ -26,8 +27,8 @@ public class WritableByteObjectInspector extends
     AbstractPrimitiveWritableObjectInspector implements
     SettableByteObjectInspector {
 
-  WritableByteObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.byteTypeEntry);
+  public WritableByteObjectInspector() {
+    super(TypeInfoFactory.byteTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableDateObjectInspector.java
Patch:
@@ -20,6 +20,7 @@
 import java.sql.Date;
 
 import org.apache.hadoop.hive.serde2.io.DateWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A WritableDateObjectInspector inspects a DateWritable Object.
@@ -29,7 +30,7 @@ public class WritableDateObjectInspector extends
     SettableDateObjectInspector {
 
   public WritableDateObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.dateTypeEntry);
+    super(TypeInfoFactory.dateTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableDoubleObjectInspector.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A WritableDoubleObjectInspector inspects a DoubleWritable Object.
@@ -27,7 +28,7 @@ public class WritableDoubleObjectInspector extends
     SettableDoubleObjectInspector {
 
   WritableDoubleObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.doubleTypeEntry);
+    super(TypeInfoFactory.doubleTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableFloatObjectInspector.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.FloatWritable;
 
 /**
@@ -27,7 +28,7 @@ public class WritableFloatObjectInspector extends
     SettableFloatObjectInspector {
 
   WritableFloatObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.floatTypeEntry);
+    super(TypeInfoFactory.floatTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableHiveDecimalObjectInspector.java
Patch:
@@ -20,13 +20,14 @@
 
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 public class WritableHiveDecimalObjectInspector
     extends AbstractPrimitiveWritableObjectInspector
     implements SettableHiveDecimalObjectInspector {
 
   protected WritableHiveDecimalObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.decimalTypeEntry);
+    super(TypeInfoFactory.decimalTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableIntObjectInspector.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.IntWritable;
 
 /**
@@ -27,7 +28,7 @@ public class WritableIntObjectInspector extends
     SettableIntObjectInspector {
 
   WritableIntObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.intTypeEntry);
+    super(TypeInfoFactory.intTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableLongObjectInspector.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.LongWritable;
 
 /**
@@ -27,7 +28,7 @@ public class WritableLongObjectInspector extends
     SettableLongObjectInspector {
 
   WritableLongObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.longTypeEntry);
+    super(TypeInfoFactory.longTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableShortObjectInspector.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A WritableShortObjectInspector inspects a ShortWritable Object.
@@ -27,7 +28,7 @@ public class WritableShortObjectInspector extends
     SettableShortObjectInspector {
 
   WritableShortObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.shortTypeEntry);
+    super(TypeInfoFactory.shortTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableStringObjectInspector.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.Text;
 
 /**
@@ -27,7 +28,7 @@ public class WritableStringObjectInspector extends
     SettableStringObjectInspector {
 
   WritableStringObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.stringTypeEntry);
+    super(TypeInfoFactory.stringTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableTimestampObjectInspector.java
Patch:
@@ -20,13 +20,14 @@
 import java.sql.Timestamp;
 
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 public class WritableTimestampObjectInspector extends
     AbstractPrimitiveWritableObjectInspector implements
     SettableTimestampObjectInspector {
 
   public WritableTimestampObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.timestampTypeEntry);
+    super(TypeInfoFactory.timestampTypeInfo);
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableVoidObjectInspector.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
 import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * A WritableVoidObjectInspector inspects a NullWritable Object.
@@ -28,7 +29,7 @@ public class WritableVoidObjectInspector extends
     VoidObjectInspector, ConstantObjectInspector {
 
   WritableVoidObjectInspector() {
-    super(PrimitiveObjectInspectorUtils.voidTypeEntry);
+    super(TypeInfoFactory.voidTypeInfo);
   }
 
   @Override

File: serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinarySerDe.java
Patch:
@@ -54,6 +54,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.io.BytesWritable;
 
 /**
@@ -603,7 +604,7 @@ public void testLazyBinaryObjectInspector() throws Throwable {
     inpBARef.setData(inpBArray);
 
     AbstractPrimitiveLazyObjectInspector<?> binInspector = LazyPrimitiveObjectInspectorFactory
-    .getLazyObjectInspector(PrimitiveCategory.BINARY, false, (byte)0);
+    .getLazyObjectInspector(TypeInfoFactory.binaryTypeInfo, false, (byte)0);
 
     //create LazyBinary initialed with inputBA
     LazyBinary lazyBin = (LazyBinary) LazyFactory.createLazyObject(binInspector);

File: serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorConverters.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeParams;
+import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;
 import org.apache.hadoop.io.BooleanWritable;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.FloatWritable;
@@ -189,7 +189,7 @@ public void testGetConvertedOI() throws Throwable {
     // output OI should have varchar type params
     PrimitiveObjectInspector poi = (PrimitiveObjectInspector)
         ObjectInspectorConverters.getConvertedOI(varchar10OI, varchar5OI);
-    VarcharTypeParams vcParams = (VarcharTypeParams) poi.getTypeParams();
-    assertEquals("varchar length doesn't match", 5, vcParams.length);
+    VarcharTypeInfo vcParams = (VarcharTypeInfo) poi.getTypeInfo();
+    assertEquals("varchar length doesn't match", 5, vcParams.getLength());
   }
 }

File: beeline/src/java/org/apache/hive/beeline/HiveSchemaHelper.java
Patch:
@@ -22,7 +22,7 @@
 public class HiveSchemaHelper {
   public static final String DB_DERBY = "derby";
   public static final String DB_MYSQL = "mysql";
-  public static final String DB_POSTGRACE = "postgrace";
+  public static final String DB_POSTGRACE = "postgres";
   public static final String DB_ORACLE = "oracle";
 
   public interface NestedScriptParser {
@@ -225,8 +225,8 @@ public String getScriptName(String dbCommand) throws IllegalArgumentException {
       if (!isNestedScript(dbCommand)) {
         throw new IllegalArgumentException("Not a nested script format " + dbCommand);
       }
-      // remove ending ';'
-      return dbCommand.replace(";", "");
+      // remove ending ';' and starting '@'
+      return dbCommand.replace(";", "").replace(ORACLE_NESTING_TOKEN, "");
     }
 
     @Override

File: beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java
Patch:
@@ -366,6 +366,7 @@ public void runBeeLine(String sqlScriptFile) throws IOException {
       beeLine.getOpts().setSilent(true);
     }
     beeLine.getOpts().setAllowMultiLineCommand(false);
+    beeLine.getOpts().setIsolation("TRANSACTION_READ_COMMITTED");
     int status = beeLine.begin(argList.toArray(new String[0]), null);
     if (status != 0) {
       throw new IOException("Schema script failed, errorcode " + status);

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java
Patch:
@@ -87,6 +87,7 @@ public class AppConfig extends Configuration {
   public static final String HADOOP_NAME         = "templeton.hadoop";
   public static final String HADOOP_CONF_DIR     = "templeton.hadoop.conf.dir";
   public static final String HCAT_NAME           = "templeton.hcat";
+  public static final String PYTHON_NAME         = "templeton.python";
   public static final String HIVE_ARCHIVE_NAME   = "templeton.hive.archive";
   public static final String HIVE_PATH_NAME      = "templeton.hive.path";
   public static final String HIVE_PROPS_NAME     = "templeton.hive.properties";
@@ -181,6 +182,7 @@ private boolean loadOneClasspathConfig(String fname) {
   public String hadoopQueueName()  { return get(HADOOP_QUEUE_NAME); }
   public String clusterHadoop()    { return get(HADOOP_NAME); }
   public String clusterHcat()      { return get(HCAT_NAME); }
+  public String clusterPython()    { return get(PYTHON_NAME); }
   public String pigPath()          { return get(PIG_PATH_NAME); }
   public String pigArchive()       { return get(PIG_ARCHIVE_NAME); }
   public String hivePath()         { return get(HIVE_PATH_NAME); }

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/JarDelegator.java
Patch:
@@ -68,6 +68,7 @@ private List<String> makeArgs(String jar, String mainClass,
       args.addAll(makeLauncherArgs(appConf, statusdir,
         completedUrl, allFiles, enablelog, jobType));
       args.add("--");
+      TempletonUtils.addCmdForWindows(args);
       args.add(appConf.clusterHadoop());
       args.add("jar");
       args.add(TempletonUtils.hadoopFsPath(jar, appConf, runAs).getName());

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/HDFSCleanup.java
Patch:
@@ -98,7 +98,7 @@ public void run() {
         // cycle fails, it'll try again on the next cycle.
         try {
           if (fs == null) {
-            fs = FileSystem.get(appConf);
+            fs = new Path(storage_root).getFileSystem(appConf);
           }
           checkFiles(fs);
         } catch (Exception e) {

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/HDFSStorage.java
Patch:
@@ -210,7 +210,7 @@ public List<String> getAllForTypeAndKey(Type type, String key, String value) {
   public void openStorage(Configuration config) throws IOException {
     storage_root = config.get(TempletonStorage.STORAGE_ROOT);
     if (fs == null) {
-      fs = FileSystem.get(config);
+      fs = new Path(storage_root).getFileSystem(config);
     }
   }
 

File: hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestServer.java
Patch:
@@ -31,7 +31,7 @@ public class TestServer extends TestCase {
   MockServer server;
 
   public void setUp() {
-    new Main(null);         // Initialize the config
+    new Main(new String[]{});         // Initialize the config
     server = new MockServer();
   }
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -813,6 +813,8 @@ public static enum ConfVars {
 
     // Whether to show the unquoted partition names in query results.
     HIVE_DECODE_PARTITION_NAME("hive.decode.partition.name", false),
+
+    HIVE_TYPE_CHECK_ON_INSERT("hive.typecheck.on.insert", true),
     ;
 
     public final String varname;

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -362,6 +362,7 @@ public enum ErrorMsg {
   UNSUPPORTED_ALTER_TBL_OP(10245, "{0} alter table options is not supported"),
   INVALID_BIGTABLE_MAPJOIN(10246, "{0} table chosen for streaming is not valid", true),
   MISSING_OVER_CLAUSE(10247, "Missing over clause for function : "),
+  PARTITION_SPEC_TYPE_MISMATCH(10248, "Cannot add partition column {0} of type {1} as it cannot be converted to type {2}", true),
 
   SCRIPT_INIT_ERROR(20000, "Unable to initialize custom script."),
   SCRIPT_IO_ERROR(20001, "An error occurred while reading or writing to your custom script. "

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -2613,6 +2613,7 @@ private void analyzeAlterTableAddParts(CommonTree ast, boolean expectView)
           currentLocation = null;
         }
         currentPart = getPartSpec(child);
+        validatePartSpec(tab, currentPart, (ASTNode)child, conf);
         break;
       case HiveParser.TOK_PARTITIONLOCATION:
         // if location specified, set in partition

File: ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingExprNodeEvaluatorFactory.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.parse.PTFTranslator.LeadLagInfo;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDesc.java
Patch:
@@ -26,9 +26,9 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.PTFUtils;
+import org.apache.hadoop.hive.ql.parse.LeadLagInfo;
 import org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.Order;
 import org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PTFQueryInputType;
-import org.apache.hadoop.hive.ql.parse.PTFTranslator.LeadLagInfo;
 import org.apache.hadoop.hive.ql.parse.RowResolver;
 import org.apache.hadoop.hive.ql.parse.TypeCheckCtx;
 import org.apache.hadoop.hive.ql.parse.WindowingSpec.Direction;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PTFDeserializer.java
Patch:
@@ -29,7 +29,7 @@
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.PTFPartition;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.parse.PTFTranslator.LeadLagInfo;
+import org.apache.hadoop.hive.ql.parse.LeadLagInfo;
 import org.apache.hadoop.hive.ql.parse.WindowingExprNodeEvaluatorFactory;
 import org.apache.hadoop.hive.ql.plan.PTFDesc.BoundaryDef;
 import org.apache.hadoop.hive.ql.plan.PTFDesc.PTFExpressionDef;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
Patch:
@@ -67,8 +67,8 @@
 import org.apache.tez.dag.api.OutputDescriptor;
 import org.apache.tez.dag.api.ProcessorDescriptor;
 import org.apache.tez.dag.api.Vertex;
-import org.apache.tez.engine.lib.input.ShuffledMergedInput;
-import org.apache.tez.engine.lib.output.OnFileSortedOutput;
+import org.apache.tez.runtime.library.input.ShuffledMergedInput;
+import org.apache.tez.runtime.library.output.OnFileSortedOutput;
 import org.apache.tez.mapreduce.hadoop.InputSplitInfo;
 import org.apache.tez.mapreduce.hadoop.MRHelpers;
 import org.apache.tez.mapreduce.hadoop.MRJobConfig;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ObjectCache.java
Patch:
@@ -20,9 +20,9 @@
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.tez.engine.common.objectregistry.ObjectLifeCycle;
-import org.apache.tez.engine.common.objectregistry.ObjectRegistry;
-import org.apache.tez.engine.common.objectregistry.ObjectRegistryFactory;
+import org.apache.tez.runtime.common.objectregistry.ObjectLifeCycle;
+import org.apache.tez.runtime.common.objectregistry.ObjectRegistry;
+import org.apache.tez.runtime.common.objectregistry.ObjectRegistryFactory;
 
 
 /**

File: hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java
Patch:
@@ -483,9 +483,9 @@ public static Pair<String, String> getDbAndTableName(String tableName) throws IO
                   OutputJobInfo outputJobInfo) {
     //TODO replace IgnoreKeyTextOutputFormat with a
     //HiveOutputFormatWrapper in StorageHandler
-   Properties props = outputJobInfo.getTableInfo().getStorerInfo().getProperties();
-   props.put(serdeConstants.SERIALIZATION_LIB,storageHandler.getSerDeClass().getName());
-   TableDesc tableDesc = new TableDesc(storageHandler.getInputFormatClass(),
+    Properties props = outputJobInfo.getTableInfo().getStorerInfo().getProperties();
+    props.put(serdeConstants.SERIALIZATION_LIB,storageHandler.getSerDeClass().getName());
+    TableDesc tableDesc = new TableDesc(storageHandler.getInputFormatClass(),
       IgnoreKeyTextOutputFormat.class,props);
     if (tableDesc.getJobProperties() == null)
       tableDesc.setJobProperties(new HashMap<String, String>());

File: serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/primitive/TestPrimitiveObjectInspectorUtils.java
Patch:
@@ -39,7 +39,7 @@ public void testGetPrimitiveGrouping() {
 
     assertEquals(PrimitiveGrouping.UNKNOWN_GROUP,
         PrimitiveObjectInspectorUtils.getPrimitiveGrouping(PrimitiveCategory.UNKNOWN));
-    assertEquals(PrimitiveGrouping.UNKNOWN_GROUP,
+    assertEquals(PrimitiveGrouping.VOID_GROUP,
         PrimitiveObjectInspectorUtils.getPrimitiveGrouping(PrimitiveCategory.VOID));
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
Patch:
@@ -277,6 +277,8 @@ private Map<TableDesc, StructObjectInspector> getConvertedOI(Configuration hconf
         new HashMap<TableDesc, StructObjectInspector>();
     Set<TableDesc> identityConverterTableDesc = new HashSet<TableDesc>();
     try {
+      Map<ObjectInspector, Boolean> oiSettableProperties = new HashMap<ObjectInspector, Boolean>();
+
       for (String onefile : conf.getPathToAliases().keySet()) {
         PartitionDesc pd = conf.getPathToPartitionInfo().get(onefile);
         TableDesc tableDesc = pd.getTableDesc();
@@ -310,7 +312,7 @@ private Map<TableDesc, StructObjectInspector> getConvertedOI(Configuration hconf
           tblRawRowObjectInspector =
               (StructObjectInspector) ObjectInspectorConverters.getConvertedOI(
                   partRawRowObjectInspector,
-                  tblDeserializer.getObjectInspector(), true);
+                  tblDeserializer.getObjectInspector(), oiSettableProperties);
 
           if (identityConverterTableDesc.contains(tableDesc)) {
             if (!partRawRowObjectInspector.equals(tblRawRowObjectInspector)) {

File: ql/src/java/org/apache/hadoop/hive/ql/Context.java
Patch:
@@ -206,7 +206,7 @@ private String getScratchDir(String scheme, String authority,
             throw new RuntimeException("Cannot make directory: "
                                        + dirPath.toString());
           } else {
-            FsPermission fsPermission = new FsPermission(Short.parseShort(scratchDirPermission.trim()));
+            FsPermission fsPermission = new FsPermission(Short.parseShort(scratchDirPermission.trim(), 8));
             fs.setPermission(dirPath, fsPermission);
           }
           if (isHDFSCleanup) {

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
Patch:
@@ -52,7 +52,6 @@
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
-import org.apache.hadoop.hive.metastore.api.InvalidOperationException;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.SerDeException;

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
Patch:
@@ -102,15 +102,13 @@ class RecordReaderImpl implements RecordReader {
     this.included = included;
     this.sarg = sarg;
     if (sarg != null) {
-      System.out.println("DEBUG XXXX SARG is not null");
       sargLeaves = sarg.getLeaves();
       filterColumns = new int[sargLeaves.size()];
       for(int i=0; i < filterColumns.length; ++i) {
         String colName = sargLeaves.get(i).getColumnName();
         filterColumns[i] = findColumns(columnNames, colName);
       }
     } else {
-      System.out.println("DEBUG XXXX SARG is null");
       sargLeaves = null;
       filterColumns = null;
     }

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -107,8 +107,7 @@ public class QTestUtil {
   public static final HashSet<String> srcTables = new HashSet<String>
     (Arrays.asList(new String [] {
         "src", "src1", "srcbucket", "srcbucket2", "src_json", "src_thrift",
-        "src_sequencefile", "srcpart",
-        AllVectorTypesRecord.TABLE_NAME
+        "src_sequencefile", "srcpart", "alltypesorc"
       }));
 
   private ParseDriver pd;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -758,7 +758,7 @@ public static enum ConfVars {
     HIVE_SERVER2_PLAIN_LDAP_DOMAIN("hive.server2.authentication.ldap.Domain", null),
     HIVE_SERVER2_CUSTOM_AUTHENTICATION_CLASS("hive.server2.custom.authentication.class", null),
     HIVE_SERVER2_ENABLE_DOAS("hive.server2.enable.doAs", true),
-    HIVE_SERVER2_TABLE_TYPE_MAPPING("hive.server2.table.type.mapping", "HIVE"),
+    HIVE_SERVER2_TABLE_TYPE_MAPPING("hive.server2.table.type.mapping", "CLASSIC"),
     HIVE_SERVER2_SESSION_HOOK("hive.server2.session.hook", ""),
 
     HIVE_CONF_RESTRICTED_LIST("hive.conf.restricted.list", null),

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java
Patch:
@@ -714,6 +714,7 @@ static ColumnStatisticsImpl create(ObjectInspector inspector) {
           case DOUBLE:
             return new DoubleStatisticsImpl();
           case STRING:
+          case VARCHAR:
             return new StringStatisticsImpl();
           case DECIMAL:
             return new DecimalStatisticsImpl();

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java
Patch:
@@ -229,8 +229,9 @@ public static void writePrimitiveUTF8(OutputStream out, Object o,
 
     case VARCHAR: {
       HiveVarcharWritable hc = ((HiveVarcharObjectInspector)oi).getPrimitiveWritableObject(o);
-      ByteBuffer b = Text.encode(hc.toString());
-      writeEscaped(out, b.array(), 0, b.limit(), escaped, escapeChar, needsEscape);
+      Text t = hc.getTextValue();
+      writeEscaped(out, t.getBytes(), 0, t.getLength(), escaped, escapeChar,
+          needsEscape);
       break;
     }
     case BINARY: {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -628,7 +628,7 @@ protected static String getTypeStringFromAST(ASTNode typeNode)
     case HiveParser.TOK_UNIONTYPE:
       return getUnionTypeStringFromAST(typeNode);
     default:
-      return DDLSemanticAnalyzer.getTypeName(typeNode.getType());
+      return DDLSemanticAnalyzer.getTypeName(typeNode);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeConstantDesc.java
Patch:
@@ -66,7 +66,8 @@ public ConstantObjectInspector getWritableObjectInspector() {
         .getPrimitiveJavaObjectInspector(pc).getPrimitiveWritableObject(
           getValue());
     return PrimitiveObjectInspectorFactory
-        .getPrimitiveWritableConstantObjectInspector(pc, writableValue);
+        .getPrimitiveWritableConstantObjectInspector(
+            (PrimitiveTypeInfo) getTypeInfo(), writableValue);
   }
 
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToString.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hadoop.hive.ql.exec.UDF;
 import org.apache.hadoop.hive.serde2.ByteStream;
+import org.apache.hadoop.hive.serde2.io.HiveVarcharWritable;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;

File: serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde/test/ThriftTestObj.java
Patch:
@@ -528,7 +528,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, ThriftTestObj struc
                 struct.field3 = new ArrayList<InnerStruct>(_list0.size);
                 for (int _i1 = 0; _i1 < _list0.size; ++_i1)
                 {
-                  InnerStruct _elem2; // required
+                  InnerStruct _elem2; // optional
                   _elem2 = new InnerStruct();
                   _elem2.read(iprot);
                   struct.field3.add(_elem2);
@@ -636,7 +636,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, ThriftTestObj struct
           struct.field3 = new ArrayList<InnerStruct>(_list5.size);
           for (int _i6 = 0; _i6 < _list5.size; ++_i6)
           {
-            InnerStruct _elem7; // required
+            InnerStruct _elem7; // optional
             _elem7 = new InnerStruct();
             _elem7.read(iprot);
             struct.field3.add(_elem7);

File: serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroObjectInspectorGenerator.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.ParameterizedPrimitiveTypeUtils;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
@@ -91,8 +92,7 @@ private ObjectInspector createObjectInspectorWorker(TypeInfo ti) throws SerDeExc
     switch(ti.getCategory()) {
       case PRIMITIVE:
         PrimitiveTypeInfo pti = (PrimitiveTypeInfo)ti;
-        result = PrimitiveObjectInspectorFactory
-                .getPrimitiveJavaObjectInspector(pti.getPrimitiveCategory());
+        result = PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(pti);
         break;
       case STRUCT:
         StructTypeInfo sti = (StructTypeInfo)ti;

File: serde/src/java/org/apache/hadoop/hive/serde2/io/DoubleWritable.java
Patch:
@@ -17,7 +17,7 @@
  */
 
 /**
- * This file is back-ported from hadoop-0.19, to make sure hive can run 
+ * This file is back-ported from hadoop-0.19, to make sure hive can run
  * with hadoop-0.17.
  */
 package org.apache.hadoop.hive.serde2.io;

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDoubleObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyFloatObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveVarcharObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyLongObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory;
@@ -113,6 +114,8 @@ public final class LazyFactory {
       return new LazyDouble((LazyDoubleObjectInspector) oi);
     case STRING:
       return new LazyString((LazyStringObjectInspector) oi);
+    case VARCHAR:
+      return new LazyHiveVarchar((LazyHiveVarcharObjectInspector) oi);
     case DATE:
       return new LazyDate((LazyDateObjectInspector) oi);
     case TIMESTAMP:

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryFactory.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector;
@@ -71,6 +72,8 @@ public final class LazyBinaryFactory {
       return new LazyBinaryDouble((WritableDoubleObjectInspector) oi);
     case STRING:
       return new LazyBinaryString((WritableStringObjectInspector) oi);
+    case VARCHAR:
+      return new LazyBinaryHiveVarchar((WritableHiveVarcharObjectInspector) oi);
     case VOID: // for NULL
       return new LazyBinaryVoid((WritableVoidObjectInspector) oi);
     case DATE:

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/PrimitiveObjectInspector.java
Patch:
@@ -31,7 +31,7 @@ public interface PrimitiveObjectInspector extends ObjectInspector, PrimitiveType
    */
   public static enum PrimitiveCategory {
     VOID, BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, STRING,
-    DATE, TIMESTAMP, BINARY, DECIMAL, UNKNOWN
+    DATE, TIMESTAMP, BINARY, DECIMAL, VARCHAR, UNKNOWN
   };
 
   /**

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java
Patch:
@@ -158,7 +158,7 @@ private boolean loadOneFileConfig(String dir, String fname) {
       File f = new File(dir, fname);
       if (f.exists()) {
         addResource(new Path(f.getAbsolutePath()));
-        LOG.debug("loaded config file " + f.getAbsolutePath());
+        LOG.info("loaded config file " + f.getAbsolutePath());
         return true;
       }
     }

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/CompleteDelegator.java
Patch:
@@ -60,12 +60,12 @@ public CompleteBean run(String id)
     try {
       state = new JobState(id, Main.getAppConfigInstance());
       if (state.getCompleteStatus() == null)
-        failed("Job not yet complete", null);
+        failed("Job not yet complete. jobId=" + id, null);
 
       Long notified = state.getNotifiedTime();
       if (notified != null)
-        return acceptWithError("Callback already run on "
-          + new Date(notified.longValue()));
+        return acceptWithError("Callback already run for jobId=" + id +
+                " at " + new Date(notified));
 
       String callback = state.getCallback();
       if (callback == null)

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/Server.java
Patch:
@@ -727,7 +727,7 @@ public List<String> showQueueList(@QueryParam("showall") boolean showall)
   }
 
   /**
-   * Notify on a completed job.
+   * Notify on a completed job.  Called by JobTracker.
    */
   @GET
   @Path("internal/complete/{jobid}")

File: hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/HDFSStorage.java
Patch:
@@ -102,7 +102,9 @@ public String getField(Type type, String id, String key) {
       }
       return val;
     } catch (Exception e) {
-      LOG.info("Couldn't find " + p + ": " + e.getMessage(), e);
+      //don't print stack trace since clients poll for 'exitValue', 'completed',
+      //files which are not there until job completes
+      LOG.info("Couldn't find " + p + ": " + e.getMessage());
     } finally {
       close(in);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
Patch:
@@ -130,7 +130,7 @@ private void initialize() {
       List<ObjectInspector> inspectors = new ArrayList<ObjectInspector>(vcCols.size());
       for (VirtualColumn vc : vcCols) {
         inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(
-                vc.getTypeInfo().getPrimitiveCategory()));
+                vc.getTypeInfo()));
         names.add(vc.getName());
       }
       vcsOI = ObjectInspectorFactory.getStandardStructObjectInspector(names, inspectors);

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/VirtualColumn.java
Patch:
@@ -138,7 +138,7 @@ public static StructObjectInspector getVCSObjectInspector(List<VirtualColumn> vc
     for (VirtualColumn vc : vcs) {
       names.add(vc.getName());
       inspectors.add(PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(
-          vc.getTypeInfo().getPrimitiveCategory()));
+          vc.getTypeInfo()));
     }
     return ObjectInspectorFactory.getStandardStructObjectInspector(names, inspectors);
   }

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryUtils.java
Patch:
@@ -425,8 +425,7 @@ public static ObjectInspector getLazyBinaryObjectInspectorFromTypeInfo(
       switch (typeInfo.getCategory()) {
       case PRIMITIVE: {
         result = PrimitiveObjectInspectorFactory
-            .getPrimitiveWritableObjectInspector(((PrimitiveTypeInfo) typeInfo)
-            .getPrimitiveCategory());
+            .getPrimitiveWritableObjectInspector(((PrimitiveTypeInfo) typeInfo));
         break;
       }
       case LIST: {

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
Patch:
@@ -172,8 +172,7 @@ public static ObjectInspector getConvertedOI(
     switch (outputOI.getCategory()) {
     case PRIMITIVE:
       PrimitiveObjectInspector primInputOI = (PrimitiveObjectInspector) inputOI;
-      return PrimitiveObjectInspectorFactory.
-          getPrimitiveWritableObjectInspector(primInputOI.getPrimitiveCategory());
+      return PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(primInputOI);
     case STRUCT:
       StructObjectInspector structOutputOI = (StructObjectInspector) outputOI;
       if (structOutputOI.isSettable()) {

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -403,6 +403,8 @@ public enum ErrorMsg {
   COLUMNSTATSCOLLECTOR_PARSE_ERROR(30009, "Encountered parse error while parsing rewritten query"),
   COLUMNSTATSCOLLECTOR_IO_ERROR(30010, "Encountered I/O exception while parsing rewritten query"),
   DROP_COMMAND_NOT_ALLOWED_FOR_PARTITION(30011, "Partition protected from being dropped"),
+  COLUMNSTATSCOLLECTOR_INVALID_COLUMN(30012, "Column statistics are not supported "
+      + "for partition columns"),
     ;
 
   private int errorCode;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
Patch:
@@ -402,7 +402,7 @@ private RecordReader<WritableComparable, Writable> getRecordReader() throws Exce
 
       ObjectInspector outputOI = ObjectInspectorConverters.getConvertedOI(
           serde.getObjectInspector(),
-          partitionedTableOI == null ? tblSerde.getObjectInspector() : partitionedTableOI);
+          partitionedTableOI == null ? tblSerde.getObjectInspector() : partitionedTableOI, true);
 
       partTblObjectInspectorConverter = ObjectInspectorConverters.getConverter(
           serde.getObjectInspector(), outputOI);
@@ -628,7 +628,7 @@ public ObjectInspector getOutputObjectInspector() throws HiveException {
         partSerde.initialize(job, listPart.getOverlayedProperties());
 
         partitionedTableOI = ObjectInspectorConverters.getConvertedOI(
-            partSerde.getObjectInspector(), tableOI);
+            partSerde.getObjectInspector(), tableOI, true);
         if (!partitionedTableOI.equals(tableOI)) {
           break;
         }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
Patch:
@@ -32,8 +32,8 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
+import org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.VirtualColumn;
 import org.apache.hadoop.hive.ql.plan.MapWork;
@@ -310,7 +310,7 @@ private Map<TableDesc, StructObjectInspector> getConvertedOI(Configuration hconf
           tblRawRowObjectInspector =
               (StructObjectInspector) ObjectInspectorConverters.getConvertedOI(
                   partRawRowObjectInspector,
-                  tblDeserializer.getObjectInspector());
+                  tblDeserializer.getObjectInspector(), true);
 
           if (identityConverterTableDesc.contains(tableDesc)) {
             if (!partRawRowObjectInspector.equals(tblRawRowObjectInspector)) {

File: hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/CreateTableHook.java
Patch:
@@ -205,7 +205,7 @@ public void postAnalyze(HiveSemanticAnalyzerHookContext context,
                         desc.getSerName(),
                         desc.getInputFormat(),
                         desc.getOutputFormat());
-                //Authorization checks are performed by the storageHandler.getAuthorizationProvider(), if  
+                //Authorization checks are performed by the storageHandler.getAuthorizationProvider(), if
                 //StorageDelegationAuthorizationProvider is used.
             } catch (IOException e) {
                 throw new SemanticException(e);

File: hcatalog/core/src/main/java/org/apache/hcatalog/common/HCatContext.java
Patch:
@@ -43,7 +43,7 @@
  * <p>HCatalog <em>developers</em> should enable optional functionality by checking properties
  * from {@link #getConf()}. Since users are not obligated to set a configuration, optional
  * functionality must provide a sensible default.</p>
- * 
+ *
  * @deprecated Use/modify {@link org.apache.hive.hcatalog.common.HCatContext} instead
  */
 @InterfaceAudience.Public

File: hcatalog/core/src/main/java/org/apache/hcatalog/common/HCatUtil.java
Patch:
@@ -619,7 +619,7 @@ public static void copyJobPropertiesToJobConf(
             jobConf.set(entry.getKey(), entry.getValue());
         }
     }
-    
+
 
     public static boolean isHadoop23() {
         String version = org.apache.hadoop.util.VersionInfo.getVersion();

File: hcatalog/core/src/main/java/org/apache/hcatalog/data/transfer/EntityBase.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.Map;
 
 /**
- * This is a base class for 
+ * This is a base class for
  * {@link ReadEntity.Builder} / {@link WriteEntity.Builder}.
  * Many fields in them are common, so this class
  * contains the common fields.

File: hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/FileOutputCommitterContainer.java
Patch:
@@ -469,7 +469,7 @@ private void moveTaskOutputs(FileSystem fs,
                         // Create the directory
                         Path placeholder = new Path(parentDir, "_placeholder");
                         if (fs.mkdirs(parentDir)) {
-                            // It is weired but we need a placeholder, 
+                            // It is weired but we need a placeholder,
                             // otherwise rename cannot move file to the right place
                             fs.create(placeholder).close();
                         }

File: hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/FileRecordWriterContainer.java
Patch:
@@ -221,7 +221,7 @@ public void write(WritableComparable<?> key, HCatRecord value) throws IOExceptio
 
                 Path parentDir = new Path(currTaskContext.getConfiguration().get("mapred.work.output.dir"));
                 Path childPath = new Path(parentDir,FileOutputFormat.getUniqueFile(currTaskContext, "part", ""));
-                
+
                 org.apache.hadoop.mapred.RecordWriter baseRecordWriter =
                     baseOF.getRecordWriter(
                         parentDir.getFileSystem(currTaskContext.getConfiguration()),

File: hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/FosterStorageHandler.java
Patch:
@@ -185,7 +185,7 @@ public HiveAuthorizationProvider getAuthorizationProvider()
     @Override
     public void configureJobConf(TableDesc tableDesc, JobConf jobConf) {
         //do nothing by default
-        //EK: added the same (no-op) implementation as in 
+        //EK: added the same (no-op) implementation as in
         // org.apache.hive.hcatalog.DefaultStorageHandler (hive 0.12)
         // this is needed to get 0.11 API compat layer to work
         // see HIVE-4896

File: hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/HCatOutputFormat.java
Patch:
@@ -231,7 +231,7 @@ public static void setSchema(final Configuration conf, final HCatSchema schema)
     }
 
     /**
-     * Get the record writer for the job. This uses the StorageHandler's default 
+     * Get the record writer for the job. This uses the StorageHandler's default
      * OutputFormat to get the record writer.
      * @param context the information about the current task
      * @return a RecordWriter to write the output for the job

File: hcatalog/core/src/test/java/org/apache/hcatalog/data/schema/TestHCatSchemaUtils.java
Patch:
@@ -29,7 +29,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 /**
- * @deprecated Use/modify {@link org.apache.hive.hcatalog.data.schema.TestHCatSchema} instead
+ * @deprecated Use/modify {@link org.apache.hive.hcatalog.data.schema.TestHCatSchemaUtils} instead
  */
 public class TestHCatSchemaUtils extends TestCase {
 

File: hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestHCatDynamicPartitioned.java
Patch:
@@ -46,7 +46,7 @@
 import static junit.framework.Assert.assertTrue;
 
 /**
- * @deprecated Use/modify {@link org.apache.hive.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned} instead
+ * @deprecated Use/modify {@link org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned} instead
  */
 public class TestHCatDynamicPartitioned extends HCatMapReduceTest {
 

File: hcatalog/server-extensions/src/main/java/org/apache/hcatalog/messaging/json/JSONAddPartitionMessage.java
Patch:
@@ -27,7 +27,7 @@
 
 /**
  * JSON implementation of AddPartitionMessage.
- * @deprecated Use/modify {@link org.apache.hive.hcatalog.messaging.jms.MessagingUtils} instead
+ * @deprecated Use/modify {@link org.apache.hcatalog.messaging.json.JSONAddPartitionMessage} instead
  */
 public class JSONAddPartitionMessage extends AddPartitionMessage {
 

File: hcatalog/src/test/e2e/hcatalog/udfs/java/org/apache/hive/hcatalog/utils/HCatTestDriver.java
Patch:
@@ -57,4 +57,3 @@ public static void main(String argv[]) {
         System.exit(exitCode);
     }
 }
-	

File: hcatalog/core/src/main/java/org/apache/hcatalog/data/transfer/impl/HCatInputFormatReader.java
Patch:
@@ -38,7 +38,7 @@
 import org.apache.hcatalog.data.transfer.ReaderContext;
 import org.apache.hcatalog.data.transfer.state.StateProvider;
 import org.apache.hcatalog.mapreduce.HCatInputFormat;
-import org.apache.hcatalog.shims.HCatHadoopShims;
+import org.apache.hadoop.hive.shims.ShimLoader;
 
 /**
  * This reader reads via {@link HCatInputFormat}
@@ -66,7 +66,7 @@ public ReaderContext prepareRead() throws HCatException {
                 job, re.getDbName(), re.getTableName()).setFilter(re.getFilterString());
             ReaderContext cntxt = new ReaderContext();
             cntxt.setInputSplits(hcif.getSplits(
-                HCatHadoopShims.Instance.get().createJobContext(job.getConfiguration(), null)));
+                ShimLoader.getHadoopShims().getHCatShim().createJobContext(job.getConfiguration(), null)));
             cntxt.setConf(job.getConfiguration());
             return cntxt;
         } catch (IOException e) {
@@ -82,7 +82,7 @@ public Iterator<HCatRecord> read() throws HCatException {
         HCatInputFormat inpFmt = new HCatInputFormat();
         RecordReader<WritableComparable, HCatRecord> rr;
         try {
-            TaskAttemptContext cntxt = HCatHadoopShims.Instance.get().createTaskAttemptContext(conf, new TaskAttemptID());
+            TaskAttemptContext cntxt = ShimLoader.getHadoopShims().getHCatShim().createTaskAttemptContext(conf, new TaskAttemptID());
             rr = inpFmt.createRecordReader(split, cntxt);
             rr.initialize(split, cntxt);
         } catch (IOException e) {

File: hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/DefaultOutputCommitterContainer.java
Patch:
@@ -23,7 +23,7 @@
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
-import org.apache.hadoop.mapred.HCatMapRedUtil;
+import org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil;
 import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.JobStatus.State;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;

File: hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/FileRecordWriterContainer.java
Patch:
@@ -33,7 +33,7 @@
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.mapred.HCatMapRedUtil;
+import org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.OutputCommitter;

File: hcatalog/core/src/main/java/org/apache/hcatalog/mapreduce/Security.java
Patch:
@@ -39,7 +39,7 @@
 import org.apache.hadoop.security.token.TokenSelector;
 import org.apache.hcatalog.common.HCatConstants;
 import org.apache.hcatalog.common.HCatUtil;
-import org.apache.hcatalog.shims.HCatHadoopShims;
+import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.thrift.TException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -142,7 +142,7 @@ void handleSecurity(
                     TokenSelector<? extends TokenIdentifier> jtTokenSelector =
                         new org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSelector();
                     Token jtToken = jtTokenSelector.selectToken(org.apache.hadoop.security.SecurityUtil.buildTokenService(
-                        HCatHadoopShims.Instance.get().getResourceManagerAddress(conf)), ugi.getTokens());
+                        ShimLoader.getHadoopShims().getHCatShim().getResourceManagerAddress(conf)), ugi.getTokens());
                     if (jtToken == null) {
                         //we don't need to cancel this token as the TokenRenewer for JT tokens
                         //takes care of cancelling them

File: hcatalog/core/src/test/java/org/apache/hcatalog/cli/TestSemanticAnalysis.java
Patch:
@@ -207,7 +207,7 @@ public void testChangeColumns() throws CommandNeedRetryException {
     public void testAddReplaceCols() throws IOException, MetaException, TException, NoSuchObjectException, CommandNeedRetryException {
 
         hcatDriver.run("drop table junit_sem_analysis");
-        hcatDriver.run("create table junit_sem_analysis (a int, c string) partitioned by (b string) stored as RCFILE");
+        hcatDriver.run("create table junit_sem_analysis (a int, c string) partitioned by (b string)");
         CommandProcessorResponse response = hcatDriver.run("alter table junit_sem_analysis replace columns (a1 tinyint)");
         assertEquals(0, response.getResponseCode());
 

File: hcatalog/core/src/test/java/org/apache/hcatalog/rcfile/TestRCFileMapReduceInputFormat.java
Patch:
@@ -43,7 +43,7 @@
 import org.apache.hadoop.mapreduce.RecordReader;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.apache.hcatalog.shims.HCatHadoopShims;
+import org.apache.hadoop.hive.shims.ShimLoader;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -233,7 +233,7 @@ private void writeThenReadByRecordReader(int intervalRecordCount,
         assertEquals("splits length should be " + splitNumber, splits.size(), splitNumber);
         int readCount = 0;
         for (int i = 0; i < splits.size(); i++) {
-            TaskAttemptContext tac = HCatHadoopShims.Instance.get().createTaskAttemptContext(jonconf, new TaskAttemptID());
+            TaskAttemptContext tac = ShimLoader.getHadoopShims().getHCatShim().createTaskAttemptContext(jonconf, new TaskAttemptID());
             RecordReader<LongWritable, BytesRefArrayWritable> rr = inputFormat.createRecordReader(splits.get(i), tac);
             rr.initialize(splits.get(i), tac);
             while (rr.nextKeyValue()) {

File: hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hcatalog/pig/HCatStorer.java
Patch:
@@ -36,7 +36,7 @@
 import org.apache.hcatalog.data.schema.HCatSchema;
 import org.apache.hcatalog.mapreduce.HCatOutputFormat;
 import org.apache.hcatalog.mapreduce.OutputJobInfo;
-import org.apache.hcatalog.shims.HCatHadoopShims;
+import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.pig.PigException;
 import org.apache.pig.ResourceSchema;
 import org.apache.pig.impl.logicalLayer.FrontendException;
@@ -157,11 +157,11 @@ public void setStoreLocation(String location, Job job) throws IOException {
 
     @Override
     public void storeSchema(ResourceSchema schema, String arg1, Job job) throws IOException {
-        HCatHadoopShims.Instance.get().commitJob(getOutputFormat(), job);
+        ShimLoader.getHadoopShims().getHCatShim().commitJob(getOutputFormat(), job);
     }
 
     @Override
     public void cleanupOnFailure(String location, Job job) throws IOException {
-        HCatHadoopShims.Instance.get().abortJob(getOutputFormat(), job);
+        ShimLoader.getHadoopShims().getHCatShim().abortJob(getOutputFormat(), job);
     }
 }

File: hcatalog/core/src/test/java/org/apache/hcatalog/cli/TestPermsGrp.java
Patch:
@@ -90,6 +90,7 @@ protected void setUp() throws Exception {
         hcatConf.set(HiveConf.ConfVars.PREEXECHOOKS.varname, "");
         hcatConf.set(HiveConf.ConfVars.POSTEXECHOOKS.varname, "");
         hcatConf.set(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY.varname, "false");
+        hcatConf.set(HiveConf.ConfVars.METASTORE_CLIENT_SOCKET_TIMEOUT.varname, "60");
         clientWH = new Warehouse(hcatConf);
         msc = new HiveMetaStoreClient(hcatConf, null);
         System.setProperty(HiveConf.ConfVars.PREEXECHOOKS.varname, " ");

File: hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/HCatBaseTest.java
Patch:
@@ -40,8 +40,8 @@
  */
 public class HCatBaseTest {
     protected static final Logger LOG = LoggerFactory.getLogger(HCatBaseTest.class);
-    protected static final String TEST_DATA_DIR = System.getProperty("user.dir") +
-            "/build/test/data/" + HCatBaseTest.class.getCanonicalName();
+    protected static final String TEST_DATA_DIR =
+            "/tmp/build/test/data/" + HCatBaseTest.class.getCanonicalName();
     protected static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
 
     protected HiveConf hiveConf = null;

File: hcatalog/core/src/test/java/org/apache/hcatalog/mapreduce/TestSequenceFileReadWrite.java
Patch:
@@ -54,8 +54,8 @@
 import org.junit.Test;
 
 public class TestSequenceFileReadWrite extends TestCase {
-    private static final String TEST_DATA_DIR = System.getProperty("user.dir") +
-            "/build/test/data/" + TestSequenceFileReadWrite.class.getCanonicalName();
+    private static final String TEST_DATA_DIR =
+            "/tmp/build/test/data/" + TestSequenceFileReadWrite.class.getCanonicalName();
     private static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
     private static final String INPUT_FILE_NAME = TEST_DATA_DIR + "/input.data";
 

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoader.java
Patch:
@@ -50,8 +50,8 @@
 import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
 
 public class TestHCatLoader extends TestCase {
-    private static final String TEST_DATA_DIR = System.getProperty("user.dir") +
-        "/build/test/data/" + TestHCatLoader.class.getCanonicalName();
+    private static final String TEST_DATA_DIR =
+        "/tmp/build/test/data/" + TestHCatLoader.class.getCanonicalName();
     private static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
     private static final String BASIC_FILE_NAME = TEST_DATA_DIR + "/basic.input.data";
     private static final String COMPLEX_FILE_NAME = TEST_DATA_DIR + "/complex.input.data";
@@ -420,7 +420,7 @@ public void testConvertBooleanToInt() throws Exception {
         assertEquals(0, driver.run("create external table " + tbl +
             " (a string, b boolean) row format delimited fields terminated by '\t'" +
             " stored as textfile location 'file://" +
-            inputDataDir.getAbsolutePath() + "'").getResponseCode());
+            inputDataDir.getPath().replaceAll("\\\\", "/") + "'").getResponseCode());
 
         Properties properties = new Properties();
         properties.setProperty(HCatConstants.HCAT_DATA_CONVERT_BOOLEAN_TO_INTEGER, "true");

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatLoaderStorer.java
Patch:
@@ -68,7 +68,7 @@ public void testSmallTinyInt() throws Exception {
             " (my_small_int smallint, my_tiny_int tinyint)" +
             " row format delimited fields terminated by '\t' stored as textfile").getResponseCode());
         Assert.assertEquals(0, driver.run("load data local inpath '" +
-            dataDir.getAbsolutePath() + "' into table " + readTblName).getResponseCode());
+            dataDir.getPath().replaceAll("\\\\", "/") + "' into table " + readTblName).getResponseCode());
 
         PigServer server = new PigServer(ExecType.LOCAL);
         server.registerQuery(
@@ -103,7 +103,7 @@ public void testSmallTinyInt() throws Exception {
             String.format("%d\t%d", Short.MIN_VALUE, Byte.MIN_VALUE),
             String.format("%d\t%d", Short.MAX_VALUE, Byte.MAX_VALUE)
         });
-        smallTinyIntBoundsCheckHelper(writeDataFile.getAbsolutePath(), ExecJob.JOB_STATUS.COMPLETED);
+        smallTinyIntBoundsCheckHelper(writeDataFile.getPath().replaceAll("\\\\", "/"), ExecJob.JOB_STATUS.COMPLETED);
 
         // Values outside the column type bounds will fail at runtime.
         HcatTestUtils.createTestDataFile(TEST_DATA_DIR + "/shortTooSmall.tsv", new String[]{

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatStorerMulti.java
Patch:
@@ -38,8 +38,8 @@
 import org.apache.pig.PigServer;
 
 public class TestHCatStorerMulti extends TestCase {
-    private static final String TEST_DATA_DIR = System.getProperty("user.dir") +
-        "/build/test/data/" + TestHCatStorerMulti.class.getCanonicalName();
+    private static final String TEST_DATA_DIR =
+        "/tmp/build/test/data/" + TestHCatStorerMulti.class.getCanonicalName();
     private static final String TEST_WAREHOUSE_DIR = TEST_DATA_DIR + "/warehouse";
     private static final String INPUT_FILE_NAME = TEST_DATA_DIR + "/input.data";
 

File: hcatalog/storage-handlers/hbase/src/test/org/apache/hcatalog/hbase/ManyMiniCluster.java
Patch:
@@ -247,7 +247,8 @@ private void setupHBaseCluster() {
         final int numRegionServers = 1;
 
         try {
-            hbaseDir = new File(workDir, "hbase").getAbsolutePath();
+            hbaseDir = new File(workDir, "hbase").toString();
+            hbaseDir = hbaseDir.replaceAll("\\\\", "/");
             hbaseRoot = "file://" + hbaseDir;
 
             if (hbaseConf == null)

File: hcatalog/storage-handlers/hbase/src/test/org/apache/hcatalog/hbase/TestHBaseInputFormat.java
Patch:
@@ -181,7 +181,7 @@ public void TestHBaseTableReadMR() throws Exception {
         String databaseName = newTableName("MyDatabase");
         //Table name will be lower case unless specified by hbase.table.name property
         String hbaseTableName = (databaseName + "." + tableName).toLowerCase();
-        String db_dir = getTestDir() + "/hbasedb";
+        String db_dir = new Path(getTestDir(), "hbasedb").toString();
 
         String dbquery = "CREATE DATABASE IF NOT EXISTS " + databaseName + " LOCATION '"
             + db_dir + "'";

File: hcatalog/storage-handlers/hbase/src/test/org/apache/hcatalog/hbase/TestSnapshots.java
Patch:
@@ -76,7 +76,7 @@ public void TestSnapshotConversion() throws Exception {
         String tableName = newTableName("mytableOne");
         String databaseName = newTableName("mydatabase");
         String fullyQualTableName = databaseName + "." + tableName;
-        String db_dir = getTestDir() + "/hbasedb";
+        String db_dir = new Path(getTestDir(), "hbasedb").toString();
         String dbquery = "CREATE DATABASE IF NOT EXISTS " + databaseName + " LOCATION '"
             + db_dir + "'";
         String tableQuery = "CREATE TABLE " + fullyQualTableName

File: hcatalog/webhcat/svr/src/test/java/org/apache/hcatalog/templeton/TestWebHCatE2e.java
Patch:
@@ -102,7 +102,7 @@ public void listDataBases() throws IOException {
     @Test
     public void invalidPath() throws IOException {
         MethodCallRetVal p = doHttpCall(templetonBaseUrl + "/no_such_mapping/database", HTTP_METHOD_TYPE.GET);
-        Assert.assertEquals(p.getAssertMsg(), HttpStatus.INTERNAL_SERVER_ERROR_500, p.httpStatusCode);
+        Assert.assertEquals(p.getAssertMsg(), HttpStatus.NOT_FOUND_404, p.httpStatusCode);
     }
     /**
      * tries to drop table in a DB that doesn't exist

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
Patch:
@@ -88,8 +88,6 @@ public ParseContext transform(ParseContext pctx) throws SemanticException {
     /* Move logic to PrunerUtils.walkOperatorTree() so that it can be reused. */
     PrunerUtils.walkOperatorTree(pctx, opWalkerCtx, OpProcFactory.getFilterProc(),
         OpProcFactory.getDefaultProc());
-    pctx.setHasNonPartCols(opWalkerCtx.getHasNonPartCols());
-
     return pctx;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -1162,7 +1162,8 @@ public int execute() throws CommandNeedRetryException {
       }
 
 
-      int jobs = Utilities.getMRTasks(plan.getRootTasks()).size();
+      int jobs = Utilities.getMRTasks(plan.getRootTasks()).size()
+        + Utilities.getTezTasks(plan.getRootTasks()).size();
       if (jobs > 0) {
         console.printInfo("Total MapReduce jobs = " + jobs);
       }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -176,7 +176,8 @@ public static Map<ASTNode, ExprNodeDesc> genExprNode(ASTNode expr,
   private static Map<ASTNode, ExprNodeDesc> convert(Map<Node, Object> outputs) {
     Map<ASTNode, ExprNodeDesc> converted = new LinkedHashMap<ASTNode, ExprNodeDesc>();
     for (Map.Entry<Node, Object> entry : outputs.entrySet()) {
-      if (entry.getKey() instanceof ASTNode && entry.getValue() instanceof ExprNodeDesc) {
+      if (entry.getKey() instanceof ASTNode &&
+          (entry.getValue() == null || entry.getValue() instanceof ExprNodeDesc)) {
         converted.put((ASTNode)entry.getKey(), (ExprNodeDesc)entry.getValue());
       } else {
         LOG.warn("Invalid type entry " + entry);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -384,6 +384,9 @@ public static enum ConfVars {
     // whether session is running in silent mode or not
     HIVESESSIONSILENT("hive.session.silent", false),
 
+    // Whether to enable history for this session
+    HIVE_SESSION_HISTORY_ENABLED("hive.session.history.enabled", false),
+
     // query being executed (multiple per session)
     HIVEQUERYSTRING("hive.query.string", ""),
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
Patch:
@@ -191,7 +191,8 @@ private static Vertex createVertex(JobConf conf, MapWork mapWork, int seqNo,
         mrScratchDir.toUri().toString(), ctx);
     Utilities.setInputPaths(conf, inputPaths);
 
-    InputSplitInfo inputSplitInfo = MRHelpers.generateInputSplits(conf, tezDir);
+    InputSplitInfo inputSplitInfo = MRHelpers.generateInputSplits(conf, 
+        new Path(tezDir, ""+seqNo));
 
     // create the directories FileSinkOperators need
     Utilities.createTmpDirs(conf, mapWork);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
Patch:
@@ -83,7 +83,6 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Partitioner;
 import org.apache.hadoop.mapred.RunningJob;
-import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
 import org.apache.log4j.Appender;
 import org.apache.log4j.BasicConfigurator;
 import org.apache.log4j.FileAppender;
@@ -505,7 +504,7 @@ private void handleSampling(DriverContext context, MapWork mWork, JobConf job, H
     String tmpPath = context.getCtx().getExternalTmpFileURI(onePath.toUri());
 
     Path partitionFile = new Path(tmpPath, ".partitions");
-    TotalOrderPartitioner.setPartitionFile(job, partitionFile);
+    ShimLoader.getHadoopShims().setTotalOrderPartitionFile(job, partitionFile);
 
     PartitionKeySampler sampler = new PartitionKeySampler();
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -738,6 +738,7 @@ public static enum ConfVars {
     HIVE_SERVER2_PLAIN_LDAP_DOMAIN("hive.server2.authentication.ldap.Domain", null),
     HIVE_SERVER2_CUSTOM_AUTHENTICATION_CLASS("hive.server2.custom.authentication.class", null),
     HIVE_SERVER2_ENABLE_DOAS("hive.server2.enable.doAs", true),
+    HIVE_SERVER2_TABLE_TYPE_MAPPING("hive.server2.table.type.mapping", "HIVE"),
 
     HIVE_CONF_RESTRICTED_LIST("hive.conf.restricted.list", null),
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
Patch:
@@ -110,7 +110,7 @@ public class VectorGroupByOperator extends Operator<GroupByDesc> implements Seri
   private transient int avgVariableSize;
 
   /**
-   * Current number of entries in the hash table
+   * Current number of entries in the hash table.
    */
   private transient int numEntriesHashTable;
 
@@ -130,7 +130,7 @@ public class VectorGroupByOperator extends Operator<GroupByDesc> implements Seri
   private static final int FLUSH_CHECK_THRESHOLD = 10000;
 
   /**
-   * Percent of entries to flush when memory threshold exceeded
+   * Percent of entries to flush when memory threshold exceeded.
    */
   private static final float PERCENT_ENTRIES_TO_FLUSH = 0.1f;
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java
Patch:
@@ -23,6 +23,9 @@
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 
+/**
+ * Constant is represented as a vector with repeating values.
+ */
 public class ConstantVectorExpression extends VectorExpression {
 
   private static enum Type {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IsNotNull.java
Patch:
@@ -26,8 +26,8 @@
  * The boolean output is stored in the specified output column.
  */
 public class IsNotNull extends VectorExpression {
-  int colNum;
-  int outputColumn;
+  private final int colNum;
+  private final int outputColumn;
 
   public IsNotNull(int colNum, int outputColumn) {
     this.colNum = colNum;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IsNull.java
Patch:
@@ -26,8 +26,8 @@
  * The boolean output is stored in the specified output column.
  */
 public class IsNull extends VectorExpression {
-  int colNum;
-  int outputColumn;
+  private final int colNum;
+  private final int outputColumn;
 
   public IsNull(int colNum, int outputColumn) {
     this.colNum = colNum;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/DoubleScalarAddDoubleColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/DoubleScalarAddLongColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/DoubleScalarDivideDoubleColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/DoubleScalarDivideLongColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/DoubleScalarModuloDoubleColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/DoubleScalarModuloLongColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/DoubleScalarMultiplyDoubleColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/DoubleScalarMultiplyLongColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/DoubleScalarSubtractDoubleColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/DoubleScalarSubtractLongColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/LongScalarAddDoubleColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/LongScalarAddLongColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/LongScalarDivideDoubleColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/LongScalarModuloDoubleColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/LongScalarModuloLongColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/LongScalarMultiplyDoubleColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/LongScalarMultiplyLongColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/LongScalarSubtractDoubleColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen/LongScalarSubtractLongColumn.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 
 /**
+ * Generated from template ScalarArithmeticColumn.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorSelectOperator.java
Patch:
@@ -39,6 +39,9 @@
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.junit.Test;
 
+/**
+ * Unit tests for vectorized select operator.
+ */
 public class TestVectorSelectOperator {
 
   static class ValidatorVectorSelectOperator extends VectorSelectOperator {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestConstantVectorExpression.java
Patch:
@@ -30,6 +30,9 @@
 import org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil;
 import org.junit.Test;
 
+/**
+ * Test vector expressions with constants.
+ */
 public class TestConstantVectorExpression {
 
   @Test

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java
Patch:
@@ -46,6 +46,9 @@
 import org.apache.hadoop.io.Writable;
 import org.junit.Test;
 
+/**
+ * Unit tests for vector expression writers.
+ */
 public class TestVectorExpressionWriters {
 
   private final int vectorSize = 5;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorLogicalExpressions.java
Patch:
@@ -116,7 +116,7 @@ private VectorizedRowBatch getBatchThreeBooleanCols() {
     i = 5; v0.vector[i] = 0; v0.isNull[i] = true; v1.vector[i] = 1; v1.isNull[i] = false;  // NULL 1
     i = 6; v0.vector[i] = 0; v0.isNull[i] = false; v1.vector[i] = 0; v1.isNull[i] = true;  // 0 NULL
     i = 7; v0.vector[i] = 1; v0.isNull[i] = false; v1.vector[i] = 1; v1.isNull[i] = true;  // 1 NULL
-    i = 8; v0.vector[i] = 1; v0.isNull[i] = true; v1.vector[i] = 1; v1.isNull[i] = true;  // NULL NULL
+    i = 8; v0.vector[i] = 1; v0.isNull[i] = true; v1.vector[i] = 1; v1.isNull[i] = true; // NULL NULL
 
     v0.noNulls = false;
     v1.noNulls = false;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
Patch:
@@ -201,7 +201,7 @@ public static PrunedPartitionList prune(Table tab, ExprNodeDesc prunerExpr,
             // are on non-partition columns.
             unkn_parts.addAll(Hive.get().getPartitions(tab));
           } else {
-            String message = Utilities.checkJDOPushDown(tab, compactExpr);
+            String message = Utilities.checkJDOPushDown(tab, compactExpr, null);
             if (message == null) {
               String filter = compactExpr.getExprString();
               String oldFilter = prunerExpr.getExprString();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -7698,7 +7698,7 @@ private Operator genTablePlan(String alias, QB qb) throws SemanticException {
       for (FieldSchema part_col : tab.getPartCols()) {
         LOG.trace("Adding partition col: " + part_col);
         // TODO: use the right type by calling part_col.getType() instead of
-        // String.class
+        // String.class. See HIVE-3059.
         rwsch.put(alias, part_col.getName(), new ColumnInfo(part_col.getName(),
             TypeInfoFactory.stringTypeInfo, alias, true));
       }

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HivePreparedStatement.java
Patch:
@@ -496,8 +496,7 @@ public void setClob(int parameterIndex, Reader reader, long length) throws SQLEx
    */
 
   public void setDate(int parameterIndex, Date x) throws SQLException {
-    // TODO Auto-generated method stub
-    throw new SQLException("Method not supported");
+    this.parameters.put(parameterIndex, x.toString());
   }
 
   /*

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveResultSetMetaData.java
Patch:
@@ -110,6 +110,8 @@ public String getColumnTypeName(int column) throws SQLException {
       return serdeConstants.INT_TYPE_NAME;
     } else if ("bigint".equalsIgnoreCase(type)) {
       return serdeConstants.BIGINT_TYPE_NAME;
+    } else if ("date".equalsIgnoreCase(type)) {
+      return serdeConstants.DATE_TYPE_NAME;
     } else if ("timestamp".equalsIgnoreCase(type)) {
       return serdeConstants.TIMESTAMP_TYPE_NAME;
     } else if ("decimal".equalsIgnoreCase(type)) {

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/Utils.java
Patch:
@@ -46,6 +46,8 @@ public static int hiveTypeToSqlType(String type) throws SQLException {
       return Types.INTEGER;
     } else if ("bigint".equalsIgnoreCase(type)) {
       return Types.BIGINT;
+    } else if ("date".equalsIgnoreCase(type)) {
+      return Types.DATE;
     } else if ("timestamp".equalsIgnoreCase(type)) {
       return Types.TIMESTAMP;
     } else if ("decimal".equalsIgnoreCase(type)) {

File: jdbc/src/java/org/apache/hive/jdbc/HivePreparedStatement.java
Patch:
@@ -43,10 +43,10 @@
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.hive.service.cli.thrift.TCLIService;
 import org.apache.hive.service.cli.thrift.TExecuteStatementReq;
 import org.apache.hive.service.cli.thrift.TExecuteStatementResp;
 import org.apache.hive.service.cli.thrift.TOperationHandle;
-import org.apache.hive.service.cli.thrift.TCLIService;
 import org.apache.hive.service.cli.thrift.TSessionHandle;
 
 /**
@@ -510,8 +510,7 @@ public void setClob(int parameterIndex, Reader reader, long length) throws SQLEx
    */
 
   public void setDate(int parameterIndex, Date x) throws SQLException {
-    // TODO Auto-generated method stub
-    throw new SQLException("Method not supported");
+    this.parameters.put(parameterIndex, x.toString());
   }
 
   /*

File: jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java
Patch:
@@ -105,6 +105,8 @@ public String getColumnTypeName(int column) throws SQLException {
       return serdeConstants.BIGINT_TYPE_NAME;
     } else if ("timestamp".equalsIgnoreCase(type)) {
       return serdeConstants.TIMESTAMP_TYPE_NAME;
+    } else if ("date".equalsIgnoreCase(type)) {
+      return serdeConstants.DATE_TYPE_NAME;
     } else if ("decimal".equalsIgnoreCase(type)) {
       return serdeConstants.DECIMAL_TYPE_NAME;
     } else if ("binary".equalsIgnoreCase(type)) {

File: jdbc/src/java/org/apache/hive/jdbc/Utils.java
Patch:
@@ -128,6 +128,8 @@ public static int hiveTypeToSqlType(String type) throws SQLException {
       return Types.INTEGER;
     } else if ("bigint".equalsIgnoreCase(type)) {
       return Types.BIGINT;
+    } else if ("date".equalsIgnoreCase(type)) {
+      return Types.DATE;
     } else if ("timestamp".equalsIgnoreCase(type)) {
       return Types.TIMESTAMP;
     } else if ("decimal".equalsIgnoreCase(type)) {

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -172,8 +172,8 @@ public enum ErrorMsg {
   DYNAMIC_PARTITION_STRICT_MODE(10096, "Dynamic partition strict mode requires at least one "
       + "static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict"),
   NONEXISTPARTCOL(10098, "Non-Partition column appears in the partition specification: "),
-  UNSUPPORTED_TYPE(10099, "DATE and DATETIME types aren't supported yet. Please use "
-      + "TIMESTAMP instead"),
+  UNSUPPORTED_TYPE(10099, "DATETIME type isn't supported yet. Please use "
+      + "DATE or TIMESTAMP instead"),
   CREATE_NON_NATIVE_AS(10100, "CREATE TABLE AS SELECT cannot be used for a non-native table"),
   LOAD_INTO_NON_NATIVE(10101, "A non-native table cannot be used as target for LOAD"),
   LOCKMGR_NOT_SPECIFIED(10102, "Lock manager not specified correctly, set hive.lock.manager"),

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -156,8 +156,8 @@ public class DDLSemanticAnalyzer extends BaseSemanticAnalyzer {
   }
 
   public static String getTypeName(int token) throws SemanticException {
-    // date and datetime types aren't currently supported
-    if (token == HiveParser.TOK_DATE || token == HiveParser.TOK_DATETIME) {
+    // datetime type isn't currently supported
+    if (token == HiveParser.TOK_DATETIME) {
       throw new SemanticException(ErrorMsg.UNSUPPORTED_TYPE.getMsg());
     }
     return TokenToTypeName.get(token);

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToByte.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.lazy.LazyByte;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToDouble.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.io.BooleanWritable;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToFloat.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.io.BooleanWritable;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToInteger.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.lazy.LazyInteger;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToLong.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.lazy.LazyLong;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToShort.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.lazy.LazyShort;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java
Patch:
@@ -80,6 +80,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case DECIMAL:
       return new GenericUDAFAverageEvaluatorDecimal();
     case BOOLEAN:
+    case DATE:
     default:
       throw new UDFArgumentTypeException(0,
           "Only numeric or string type arguments are accepted but "

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java
Patch:
@@ -116,13 +116,15 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
         return new GenericUDAFCorrelationEvaluator();
       case STRING:
       case BOOLEAN:
+      case DATE:
       default:
         throw new UDFArgumentTypeException(1,
             "Only numeric type arguments are accepted but "
             + parameters[1].getTypeName() + " is passed.");
       }
     case STRING:
     case BOOLEAN:
+    case DATE:
     default:
       throw new UDFArgumentTypeException(0,
           "Only numeric type arguments are accepted but "

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCovariance.java
Patch:
@@ -106,13 +106,15 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
         return new GenericUDAFCovarianceEvaluator();
       case STRING:
       case BOOLEAN:
+      case DATE:
       default:
         throw new UDFArgumentTypeException(1,
             "Only numeric or string type arguments are accepted but "
             + parameters[1].getTypeName() + " is passed.");
       }
     case STRING:
     case BOOLEAN:
+    case DATE:
     default:
       throw new UDFArgumentTypeException(0,
           "Only numeric or string type arguments are accepted but "

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCovarianceSample.java
Patch:
@@ -80,13 +80,15 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
         return new GenericUDAFCovarianceSampleEvaluator();
       case STRING:
       case BOOLEAN:
+      case DATE:
       default:
         throw new UDFArgumentTypeException(1,
             "Only numeric or string type arguments are accepted but "
             + parameters[1].getTypeName() + " is passed.");
       }
     case STRING:
     case BOOLEAN:
+    case DATE:
     default:
       throw new UDFArgumentTypeException(0,
           "Only numeric or string type arguments are accepted but "

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFHistogramNumeric.java
Patch:
@@ -85,6 +85,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
       break;
     case STRING:
     case BOOLEAN:
+    case DATE:
     default:
       throw new UDFArgumentTypeException(0,
           "Only numeric type arguments are accepted but "

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFPercentileApprox.java
Patch:
@@ -90,6 +90,7 @@ public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws S
     case TIMESTAMP:
     case DECIMAL:
       break;
+    case DATE:
     default:
       throw new UDFArgumentTypeException(0,
           "Only numeric type arguments are accepted but "

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFStd.java
Patch:
@@ -59,6 +59,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case DECIMAL:
       return new GenericUDAFStdEvaluator();
     case BOOLEAN:
+    case DATE:
     default:
       throw new UDFArgumentTypeException(0,
           "Only numeric or string type arguments are accepted but "

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFStdSample.java
Patch:
@@ -58,6 +58,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case DECIMAL:
       return new GenericUDAFStdSampleEvaluator();
     case BOOLEAN:
+    case DATE:
     default:
       throw new UDFArgumentTypeException(0,
           "Only numeric or string type arguments are accepted but "

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java
Patch:
@@ -72,6 +72,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case DECIMAL:
       return new GenericUDAFSumHiveDecimal();
     case BOOLEAN:
+    case DATE:
     default:
       throw new UDFArgumentTypeException(0,
           "Only numeric or string type arguments are accepted but "

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFVariance.java
Patch:
@@ -76,6 +76,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case DECIMAL:
       return new GenericUDAFVarianceEvaluator();
     case BOOLEAN:
+    case DATE:
     default:
       throw new UDFArgumentTypeException(0,
           "Only numeric or string type arguments are accepted but "

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFVarianceSample.java
Patch:
@@ -59,6 +59,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case DECIMAL:
       return new GenericUDAFVarianceSampleEvaluator();
     case BOOLEAN:
+    case DATE:
     default:
       throw new UDFArgumentTypeException(0,
           "Only numeric or string type arguments are accepted but "

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
Patch:
@@ -38,6 +38,7 @@
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyShortObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector;
+import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDateObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyTimestampObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyVoidObjectInspector;
 import org.apache.hadoop.hive.serde2.lazydio.LazyDioBoolean;
@@ -111,6 +112,8 @@ public final class LazyFactory {
       return new LazyDouble((LazyDoubleObjectInspector) oi);
     case STRING:
       return new LazyString((LazyStringObjectInspector) oi);
+    case DATE:
+      return new LazyDate((LazyDateObjectInspector) oi);
     case TIMESTAMP:
       return new LazyTimestamp((LazyTimestampObjectInspector) oi);
     case BINARY:

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryFactory.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableVoidObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
@@ -72,6 +73,8 @@ public final class LazyBinaryFactory {
       return new LazyBinaryString((WritableStringObjectInspector) oi);
     case VOID: // for NULL
       return new LazyBinaryVoid((WritableVoidObjectInspector) oi);
+    case DATE:
+      return new LazyBinaryDate((WritableDateObjectInspector) oi);
     case TIMESTAMP:
       return new LazyBinaryTimestamp((WritableTimestampObjectInspector) oi);
     case BINARY:

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/PrimitiveObjectInspector.java
Patch:
@@ -27,7 +27,8 @@ public interface PrimitiveObjectInspector extends ObjectInspector {
    * The primitive types supported by Hive.
    */
   public static enum PrimitiveCategory {
-    VOID, BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, STRING, TIMESTAMP, BINARY, DECIMAL, UNKNOWN
+    VOID, BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, STRING,
+    DATE, TIMESTAMP, BINARY, DECIMAL, UNKNOWN
   };
 
   /**

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java
Patch:
@@ -62,6 +62,7 @@ public static TypeInfo getPrimitiveTypeInfo(String typeName) {
   public static final TypeInfo doubleTypeInfo = getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME);
   public static final TypeInfo byteTypeInfo = getPrimitiveTypeInfo(serdeConstants.TINYINT_TYPE_NAME);
   public static final TypeInfo shortTypeInfo = getPrimitiveTypeInfo(serdeConstants.SMALLINT_TYPE_NAME);
+  public static final TypeInfo dateTypeInfo = getPrimitiveTypeInfo(serdeConstants.DATE_TYPE_NAME);
   public static final TypeInfo timestampTypeInfo = getPrimitiveTypeInfo(serdeConstants.TIMESTAMP_TYPE_NAME);
   public static final TypeInfo binaryTypeInfo = getPrimitiveTypeInfo(serdeConstants.BINARY_TYPE_NAME);
   public static final TypeInfo decimalTypeInfo = getPrimitiveTypeInfo(serdeConstants.DECIMAL_TYPE_NAME);

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TCLIServiceConstants.java
Patch:
@@ -47,6 +47,7 @@ public class TCLIServiceConstants {
     PRIMITIVE_TYPES.add(org.apache.hive.service.cli.thrift.TTypeId.BINARY_TYPE);
     PRIMITIVE_TYPES.add(org.apache.hive.service.cli.thrift.TTypeId.DECIMAL_TYPE);
     PRIMITIVE_TYPES.add(org.apache.hive.service.cli.thrift.TTypeId.NULL_TYPE);
+    PRIMITIVE_TYPES.add(org.apache.hive.service.cli.thrift.TTypeId.DATE_TYPE);
   }
 
   public static final Set<TTypeId> COMPLEX_TYPES = new HashSet<TTypeId>();
@@ -82,6 +83,7 @@ public class TCLIServiceConstants {
     TYPE_NAMES.put(org.apache.hive.service.cli.thrift.TTypeId.UNION_TYPE, "UNIONTYPE");
     TYPE_NAMES.put(org.apache.hive.service.cli.thrift.TTypeId.DECIMAL_TYPE, "DECIMAL");
     TYPE_NAMES.put(org.apache.hive.service.cli.thrift.TTypeId.NULL_TYPE, "NULL");
+    TYPE_NAMES.put(org.apache.hive.service.cli.thrift.TTypeId.DATE_TYPE, "DATE");
   }
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFAvgDouble.java
Patch:
@@ -308,7 +308,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch) thro
         double[] vector = inputVector.vector;
         
         if (inputVector.isRepeating) {
-          if (inputVector.noNulls || !inputVector.isNull[0]) {
+          if (inputVector.noNulls) {
             if (myagg.isNull) {
               myagg.isNull = false;
               myagg.sum = 0;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFAvgLong.java
Patch:
@@ -308,7 +308,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch) thro
         long[] vector = inputVector.vector;
         
         if (inputVector.isRepeating) {
-          if (inputVector.noNulls || !inputVector.isNull[0]) {
+          if (inputVector.noNulls) {
             if (myagg.isNull) {
               myagg.isNull = false;
               myagg.sum = 0;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMaxDouble.java
Patch:
@@ -97,7 +97,7 @@ private Aggregation getCurrentAggregationBuffer(
       return myagg;
     }
     
-@Override
+    @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
       int aggregrateIndex, 
@@ -297,8 +297,8 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
         double[] vector = inputVector.vector;
         
         if (inputVector.isRepeating) {
-          if ((inputVector.noNulls || !inputVector.isNull[0]) &&
-            myagg.isNull || vector[0] < myagg.value) {
+          if (inputVector.noNulls &&
+            (myagg.isNull || (vector[0] > myagg.value))) {
             myagg.isNull = false;
             myagg.value = vector[0];
           }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMaxLong.java
Patch:
@@ -97,7 +97,7 @@ private Aggregation getCurrentAggregationBuffer(
       return myagg;
     }
     
-@Override
+    @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
       int aggregrateIndex, 
@@ -297,8 +297,8 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
         long[] vector = inputVector.vector;
         
         if (inputVector.isRepeating) {
-          if ((inputVector.noNulls || !inputVector.isNull[0]) &&
-            myagg.isNull || vector[0] < myagg.value) {
+          if (inputVector.noNulls &&
+            (myagg.isNull || (vector[0] > myagg.value))) {
             myagg.isNull = false;
             myagg.value = vector[0];
           }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMinDouble.java
Patch:
@@ -97,7 +97,7 @@ private Aggregation getCurrentAggregationBuffer(
       return myagg;
     }
     
-@Override
+    @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
       int aggregrateIndex, 
@@ -297,8 +297,8 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
         double[] vector = inputVector.vector;
         
         if (inputVector.isRepeating) {
-          if ((inputVector.noNulls || !inputVector.isNull[0]) &&
-            myagg.isNull || vector[0] < myagg.value) {
+          if (inputVector.noNulls &&
+            (myagg.isNull || (vector[0] < myagg.value))) {
             myagg.isNull = false;
             myagg.value = vector[0];
           }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMinLong.java
Patch:
@@ -97,7 +97,7 @@ private Aggregation getCurrentAggregationBuffer(
       return myagg;
     }
     
-@Override
+    @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
       int aggregrateIndex, 
@@ -297,8 +297,8 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
         long[] vector = inputVector.vector;
         
         if (inputVector.isRepeating) {
-          if ((inputVector.noNulls || !inputVector.isNull[0]) &&
-            myagg.isNull || vector[0] < myagg.value) {
+          if (inputVector.noNulls &&
+            (myagg.isNull || (vector[0] < myagg.value))) {
             myagg.isNull = false;
             myagg.value = vector[0];
           }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFStdPopDouble.java
Patch:
@@ -310,7 +310,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
       double[] vector = inputVector.vector;
       
       if (inputVector.isRepeating) {
-        if (inputVector.noNulls || !inputVector.isNull[0]) {
+        if (inputVector.noNulls) {
           iterateRepeatingNoNulls(myagg, vector[0], batchSize);
         }
       } 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFStdPopLong.java
Patch:
@@ -310,7 +310,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
       long[] vector = inputVector.vector;
       
       if (inputVector.isRepeating) {
-        if (inputVector.noNulls || !inputVector.isNull[0]) {
+        if (inputVector.noNulls) {
           iterateRepeatingNoNulls(myagg, vector[0], batchSize);
         }
       } 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFStdSampDouble.java
Patch:
@@ -310,7 +310,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
       double[] vector = inputVector.vector;
       
       if (inputVector.isRepeating) {
-        if (inputVector.noNulls || !inputVector.isNull[0]) {
+        if (inputVector.noNulls) {
           iterateRepeatingNoNulls(myagg, vector[0], batchSize);
         }
       } 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFStdSampLong.java
Patch:
@@ -310,7 +310,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
       long[] vector = inputVector.vector;
       
       if (inputVector.isRepeating) {
-        if (inputVector.noNulls || !inputVector.isNull[0]) {
+        if (inputVector.noNulls) {
           iterateRepeatingNoNulls(myagg, vector[0], batchSize);
         }
       } 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFSumDouble.java
Patch:
@@ -290,7 +290,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
       double[] vector = inputVector.vector;
       
       if (inputVector.isRepeating) {
-        if (inputVector.noNulls || !inputVector.isNull[0]) {
+        if (inputVector.noNulls) {
         if (myagg.isNull) {
           myagg.isNull = false;
           myagg.sum = 0;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFSumLong.java
Patch:
@@ -290,7 +290,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
       long[] vector = inputVector.vector;
       
       if (inputVector.isRepeating) {
-        if (inputVector.noNulls || !inputVector.isNull[0]) {
+        if (inputVector.noNulls) {
         if (myagg.isNull) {
           myagg.isNull = false;
           myagg.sum = 0;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFVarPopDouble.java
Patch:
@@ -310,7 +310,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
       double[] vector = inputVector.vector;
       
       if (inputVector.isRepeating) {
-        if (inputVector.noNulls || !inputVector.isNull[0]) {
+        if (inputVector.noNulls) {
           iterateRepeatingNoNulls(myagg, vector[0], batchSize);
         }
       } 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFVarPopLong.java
Patch:
@@ -310,7 +310,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
       long[] vector = inputVector.vector;
       
       if (inputVector.isRepeating) {
-        if (inputVector.noNulls || !inputVector.isNull[0]) {
+        if (inputVector.noNulls) {
           iterateRepeatingNoNulls(myagg, vector[0], batchSize);
         }
       } 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFVarSampDouble.java
Patch:
@@ -310,7 +310,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
       double[] vector = inputVector.vector;
       
       if (inputVector.isRepeating) {
-        if (inputVector.noNulls || !inputVector.isNull[0]) {
+        if (inputVector.noNulls) {
           iterateRepeatingNoNulls(myagg, vector[0], batchSize);
         }
       } 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFVarSampLong.java
Patch:
@@ -310,7 +310,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
       long[] vector = inputVector.vector;
       
       if (inputVector.isRepeating) {
-        if (inputVector.noNulls || !inputVector.isNull[0]) {
+        if (inputVector.noNulls) {
           iterateRepeatingNoNulls(myagg, vector[0], batchSize);
         }
       } 

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java
Patch:
@@ -39,9 +39,9 @@
 import org.apache.hadoop.hive.hbase.HBaseSerDe.ColumnMapping;
 import org.apache.hadoop.hive.metastore.HiveMetaHook;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
-import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
 import org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer;
 import org.apache.hadoop.hive.ql.index.IndexSearchCondition;
 import org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler;
@@ -279,6 +279,8 @@ public void configureTableJobProperties(
     jobProperties.put(
       HBaseSerDe.HBASE_COLUMNS_MAPPING,
       tableProperties.getProperty(HBaseSerDe.HBASE_COLUMNS_MAPPING));
+    jobProperties.put(HBaseSerDe.HBASE_COLUMNS_REGEX_MATCHING,
+        tableProperties.getProperty(HBaseSerDe.HBASE_COLUMNS_REGEX_MATCHING, "true"));
     jobProperties.put(HBaseSerDe.HBASE_TABLE_DEFAULT_STORAGE_TYPE,
       tableProperties.getProperty(HBaseSerDe.HBASE_TABLE_DEFAULT_STORAGE_TYPE,"string"));
     String scanCache = tableProperties.getProperty(HBaseSerDe.HBASE_SCAN_CACHE);

File: hcatalog/shims/src/23/java/org/apache/hadoop/mapred/TempletonJobTracker.java
Patch:
@@ -19,8 +19,6 @@
 package org.apache.hadoop.mapred;
 
 import java.io.IOException;
-import java.net.InetSocketAddress;
-
 import org.apache.hadoop.conf.Configuration;
 
 /*

File: hcatalog/shims/src/20/java/org/apache/hadoop/mapred/TempletonJobTracker.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hadoop.mapred;
 
 import java.io.IOException;
-import java.net.InetSocketAddress;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.ipc.RPC;

File: hcatalog/webhcat/svr/src/main/java/org/apache/hcatalog/templeton/DeleteDelegator.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hadoop.mapred.JobID;
 import org.apache.hadoop.mapred.TempletonJobTracker;
 import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hcatalog.shims.HCatHadoopShims;
 import org.apache.hcatalog.templeton.tool.JobState;
 
 /**

File: hcatalog/webhcat/svr/src/main/java/org/apache/hcatalog/templeton/ListDelegator.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hadoop.mapred.JobStatus;
 import org.apache.hadoop.mapred.TempletonJobTracker;
 import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hcatalog.shims.HCatHadoopShims;
 import org.apache.hcatalog.templeton.tool.JobState;
 
 /**

File: hcatalog/webhcat/svr/src/main/java/org/apache/hcatalog/templeton/StatusDelegator.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hadoop.mapred.JobID;
 import org.apache.hadoop.mapred.JobProfile;
 import org.apache.hadoop.mapred.JobStatus;
-import org.apache.hcatalog.shims.HCatHadoopShims;
 import org.apache.hadoop.mapred.TempletonJobTracker;
 import org.apache.hcatalog.templeton.tool.JobState;
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -569,6 +569,7 @@ public static enum ConfVars {
     // It creates sub-directories in the final output, so should not be turned on in systems
     // where MAPREDUCE-1501 is not present
     HIVE_OPTIMIZE_UNION_REMOVE("hive.optimize.union.remove", false),
+    HIVEOPTCORRELATION("hive.optimize.correlation", false), // exploit intra-query correlations
 
     // whether hadoop map-reduce supports sub-directories. It was added by MAPREDUCE-1501.
     // Some optimizations can only be performed if the version of hadoop being used supports

File: ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
Patch:
@@ -331,6 +331,7 @@ public void startGroup() throws HiveException {
     for (AbstractRowContainer<ArrayList<Object>> alw : storage) {
       alw.clear();
     }
+    super.startGroup();
   }
 
   protected int getNextSize(int sz) {

File: hcatalog/webhcat/svr/src/main/java/org/apache/hcatalog/templeton/AppConfig.java
Patch:
@@ -116,6 +116,7 @@ public class AppConfig extends Configuration {
     public static final String HADOOP_SPECULATIVE_NAME
         = "mapred.map.tasks.speculative.execution";
     public static final String HADOOP_CHILD_JAVA_OPTS = "mapred.child.java.opts";
+    public static final String UNIT_TEST_MODE     = "templeton.unit.test.mode";
 
     
     private static final Log LOG = LogFactory.getLog(AppConfig.class);

File: hcatalog/webhcat/svr/src/main/java/org/apache/hcatalog/templeton/BadParam.java
Patch:
@@ -18,11 +18,13 @@
  */
 package org.apache.hcatalog.templeton;
 
+import org.eclipse.jetty.http.HttpStatus;
+
 /**
  * Missing required or badly configured paramater.
  */
 public class BadParam extends SimpleWebException {
     public BadParam(String msg) {
-        super(400, msg);
+        super(HttpStatus.BAD_REQUEST_400, msg);
     }
 }

File: hcatalog/webhcat/svr/src/main/java/org/apache/hcatalog/templeton/BusyException.java
Patch:
@@ -18,11 +18,13 @@
  */
 package org.apache.hcatalog.templeton;
 
+import org.eclipse.jetty.http.HttpStatus;
+
 /**
  * Simple "we are busy, try again" exception.
  */
 public class BusyException extends SimpleWebException {
     public BusyException() {
-        super(503, "Busy, please retry");
+        super(HttpStatus.SERVICE_UNAVAILABLE_503, "Busy, please retry");
     }
 }

File: hcatalog/webhcat/svr/src/main/java/org/apache/hcatalog/templeton/CallbackFailedException.java
Patch:
@@ -18,11 +18,13 @@
  */
 package org.apache.hcatalog.templeton;
 
+import org.eclipse.jetty.http.HttpStatus;
+
 /**
  * The callback failed when it tried to reach the callback URL.
  */
 public class CallbackFailedException extends SimpleWebException {
     public CallbackFailedException(String msg) {
-        super(400, msg);
+        super(HttpStatus.BAD_REQUEST_400, msg);
     }
 }

File: hcatalog/webhcat/svr/src/main/java/org/apache/hcatalog/templeton/CatchallExceptionMapper.java
Patch:
@@ -24,6 +24,7 @@
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.eclipse.jetty.http.HttpStatus;
 
 /**
  * Map all exceptions to the Jersey response.  This lets us have nice
@@ -36,6 +37,6 @@ public class CatchallExceptionMapper
 
     public Response toResponse(Exception e) {
         LOG.error(e.getMessage(), e);
-        return SimpleWebException.buildMessage(500, null, e.getMessage());
+        return SimpleWebException.buildMessage(HttpStatus.INTERNAL_SERVER_ERROR_500, null, e.getMessage());
     }
 }

File: hcatalog/webhcat/svr/src/main/java/org/apache/hcatalog/templeton/HcatException.java
Patch:
@@ -18,6 +18,8 @@
  */
 package org.apache.hcatalog.templeton;
 
+import org.eclipse.jetty.http.HttpStatus;
+
 import java.util.HashMap;
 
 /**
@@ -28,7 +30,7 @@ public class HcatException extends SimpleWebException {
     public String statement;
 
     public HcatException(String msg, final ExecBean bean, final String statement) {
-        super(500, msg, new HashMap<String, Object>() {
+        super(HttpStatus.INTERNAL_SERVER_ERROR_500, msg, new HashMap<String, Object>() {
             {
                 put("exec", bean);
                 put("statement", statement);

File: hcatalog/webhcat/svr/src/main/java/org/apache/hcatalog/templeton/NotAuthorizedException.java
Patch:
@@ -18,11 +18,13 @@
  */
 package org.apache.hcatalog.templeton;
 
+import org.eclipse.jetty.http.HttpStatus;
+
 /**
  * Simple "user not found" type exception.
  */
 public class NotAuthorizedException extends SimpleWebException {
     public NotAuthorizedException(String msg) {
-        super(401, msg);
+        super(HttpStatus.UNAUTHORIZED_401, msg);
     }
 }

File: hcatalog/webhcat/svr/src/main/java/org/apache/hcatalog/templeton/QueueException.java
Patch:
@@ -18,12 +18,14 @@
  */
 package org.apache.hcatalog.templeton;
 
+import org.eclipse.jetty.http.HttpStatus;
+
 /**
  * Unable to queue the job
  */
 public class QueueException extends SimpleWebException {
     public QueueException(String msg) {
-        super(500, msg);
+        super(HttpStatus.INTERNAL_SERVER_ERROR_500, msg);
     }
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TaskRunner.java
Patch:
@@ -66,7 +66,7 @@ public void runSequential() {
     } catch (Throwable t) {
       t.printStackTrace();
     }
-    result.setExitVal(exitVal);
+    result.setExitVal(exitVal, tsk.getException());
   }
 
   public static long getTaskRunnerID () {

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -951,7 +951,7 @@ public Table getTable(final String dbName, final String tableName,
     } catch (NoSuchObjectException e) {
       if (throwException) {
         LOG.error(StringUtils.stringifyException(e));
-        throw new InvalidTableException("Table " + tableName + " not found ", tableName);
+        throw new InvalidTableException(tableName);
       }
       return null;
     } catch (Exception e) {

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -1007,7 +1007,6 @@ public int checkCliDriverResults(String tname) throws Exception {
         ".*LOCATION '.*",
         ".*transient_lastDdlTime.*",
         ".*last_modified_.*",
-        ".*java.lang.RuntimeException.*",
         ".*at org.*",
         ".*at sun.*",
         ".*at java.*",

File: ql/src/java/org/apache/hadoop/hive/ql/exec/errors/DataCorruptErrorHeuristic.java
Patch:
@@ -36,7 +36,7 @@
 
 public class DataCorruptErrorHeuristic extends RegexErrorHeuristic {
 
-  private static final String SPLIT_REGEX = "split:.*";
+  private static final String SPLIT_REGEX = "split:\\s*([^\\s]+)";
   private static final String EXCEPTION_REGEX = "EOFException";
 
   public DataCorruptErrorHeuristic() {
@@ -55,14 +55,13 @@ public ErrorAndSolution getErrorAndSolution() {
           rll.get(SPLIT_REGEX).size() > 0) {
 
         // There should only be a single split line...
-        assert(rll.get(SPLIT_REGEX).size()==1);
         String splitLogLine = rll.get(SPLIT_REGEX).get(0);
 
         // Extract only 'split: hdfs://...'
         Pattern p = Pattern.compile(SPLIT_REGEX, Pattern.CASE_INSENSITIVE);
         Matcher m = p.matcher(splitLogLine);
         m.find();
-        String splitStr = m.group();
+        String splitStr = m.group(1);
 
         es = new ErrorAndSolution(
             "Data file " + splitStr + " is corrupted.",

File: ql/src/java/org/apache/hadoop/hive/ql/exec/errors/ErrorAndSolution.java
Patch:
@@ -23,8 +23,8 @@
  */
 public class ErrorAndSolution {
 
-  private String error = null;
-  private String solution = null;
+  private final String error;
+  private final String solution;
 
   ErrorAndSolution(String error, String solution) {
     this.error = error;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFAvgLong.java
Patch:
@@ -51,7 +51,7 @@ public class VectorUDAFAvgLong extends VectorAggregateExpression {
     
     /** class for storing the current aggregate value. */
     static class Aggregation implements AggregationBuffer {
-      long sum;
+      double sum;
       long count;
       boolean isNull;
       

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCount.java
Patch:
@@ -29,7 +29,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 
 
 /**
@@ -239,7 +238,7 @@ public void reset(AggregationBuffer agg) throws HiveException {
     public Object evaluateOutput(AggregationBuffer agg) throws HiveException {
     Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         result.set (myagg.value);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFCountStar.java
Patch:
@@ -28,7 +28,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 
 /**
 * VectorUDAFCountStar. Vectorized implementation for COUNT(*) aggregates.
@@ -116,7 +115,7 @@ public void reset(AggregationBuffer agg) throws HiveException {
     public Object evaluateOutput(AggregationBuffer agg) throws HiveException {
     Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         result.set (myagg.value);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFAvgDouble.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -429,7 +428,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         assert(0 < myagg.count);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFAvgLong.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -429,7 +428,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         assert(0 < myagg.count);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMaxDouble.java
Patch:
@@ -35,7 +35,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -414,7 +413,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
     Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         return resultWriter.writeValue(myagg.value);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMaxLong.java
Patch:
@@ -35,7 +35,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -414,7 +413,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
     Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         return resultWriter.writeValue(myagg.value);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMaxString.java
Patch:
@@ -40,7 +40,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.NullWritable;
 
 /**
 * VectorUDAFMaxString. Vectorized implementation for MIN/MAX aggregates. 
@@ -356,7 +355,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
     Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         result.set(myagg.bytes, 0, myagg.length);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMinDouble.java
Patch:
@@ -35,7 +35,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -414,7 +413,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
     Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         return resultWriter.writeValue(myagg.value);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMinLong.java
Patch:
@@ -35,7 +35,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -414,7 +413,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
     Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         return resultWriter.writeValue(myagg.value);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMinString.java
Patch:
@@ -40,7 +40,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.NullWritable;
 
 /**
 * VectorUDAFMinString. Vectorized implementation for MIN/MAX aggregates. 
@@ -356,7 +355,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
     Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         result.set(myagg.bytes, 0, myagg.length);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFStdPopDouble.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -476,7 +475,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         assert(0 < myagg.count);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFStdPopLong.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -476,7 +475,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         assert(0 < myagg.count);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFStdSampDouble.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -476,7 +475,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         assert(0 < myagg.count);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFStdSampLong.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -476,7 +475,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         assert(0 < myagg.count);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFSumDouble.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -400,7 +399,7 @@ public void reset(AggregationBuffer agg) throws HiveException {
     public Object evaluateOutput(AggregationBuffer agg) throws HiveException {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         result.set(myagg.sum);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFSumLong.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -400,7 +399,7 @@ public void reset(AggregationBuffer agg) throws HiveException {
     public Object evaluateOutput(AggregationBuffer agg) throws HiveException {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         result.set(myagg.sum);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFVarPopDouble.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -476,7 +475,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         assert(0 < myagg.count);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFVarPopLong.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -476,7 +475,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         assert(0 < myagg.count);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFVarSampDouble.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -476,7 +475,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         assert(0 < myagg.count);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFVarSampLong.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -476,7 +475,7 @@ public Object evaluateOutput(
         AggregationBuffer agg) throws HiveException {
       Aggregation myagg = (Aggregation) agg;
       if (myagg.isNull) {
-        return NullWritable.get();
+        return null;
       }
       else {
         assert(0 < myagg.count);

File: jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java
Patch:
@@ -499,6 +499,9 @@ private Object getColumnValue(int columnIndex) throws SQLException {
       return getTimestampValue(tColumnValue.getStringVal());
     case DECIMAL_TYPE:
       return getBigDecimalValue(tColumnValue.getStringVal());
+    case NULL_TYPE:
+      wasNull = true;
+      return null;
     default:
       throw new SQLException("Unrecognized column type:" + columnType);
     }

File: jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java
Patch:
@@ -116,6 +116,8 @@ public String getColumnTypeName(int column) throws SQLException {
       return serdeConstants.DECIMAL_TYPE_NAME;
     } else if ("binary".equalsIgnoreCase(type)) {
       return serdeConstants.BINARY_TYPE_NAME;
+    } else if ("void".equalsIgnoreCase(type)) {
+      return serdeConstants.VOID_TYPE_NAME;
     } else if (type.startsWith("map<")) {
       return serdeConstants.STRING_TYPE_NAME;
     } else if (type.startsWith("array<")) {

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/SkewedInfo.java
Patch:
@@ -125,7 +125,7 @@ public String getFieldName() {
                 new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING)))));
     tmpMap.put(_Fields.SKEWED_COL_VALUE_LOCATION_MAPS, new org.apache.thrift.meta_data.FieldMetaData("skewedColValueLocationMaps", org.apache.thrift.TFieldRequirementType.DEFAULT, 
         new org.apache.thrift.meta_data.MapMetaData(org.apache.thrift.protocol.TType.MAP, 
-            new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, SkewedValueList.class),
+            new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, SkewedValueList.class), 
             new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING))));
     metaDataMap = Collections.unmodifiableMap(tmpMap);
     org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(SkewedInfo.class, metaDataMap);

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
Patch:
@@ -38,6 +38,7 @@
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyShortObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyTimestampObjectInspector;
+import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyVoidObjectInspector;
 import org.apache.hadoop.hive.serde2.lazydio.LazyDioBoolean;
 import org.apache.hadoop.hive.serde2.lazydio.LazyDioByte;
 import org.apache.hadoop.hive.serde2.lazydio.LazyDioDouble;
@@ -115,6 +116,8 @@ public final class LazyFactory {
       return new LazyBinary((LazyBinaryObjectInspector) oi);
     case DECIMAL:
       return new LazyHiveDecimal((LazyHiveDecimalObjectInspector) oi);
+    case VOID:
+      return new LazyVoid((LazyVoidObjectInspector) oi);
     default:
       throw new RuntimeException("Internal error: no LazyObject for " + p);
     }

File: service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TCLIServiceConstants.java
Patch:
@@ -46,6 +46,7 @@ public class TCLIServiceConstants {
     PRIMITIVE_TYPES.add(org.apache.hive.service.cli.thrift.TTypeId.TIMESTAMP_TYPE);
     PRIMITIVE_TYPES.add(org.apache.hive.service.cli.thrift.TTypeId.BINARY_TYPE);
     PRIMITIVE_TYPES.add(org.apache.hive.service.cli.thrift.TTypeId.DECIMAL_TYPE);
+    PRIMITIVE_TYPES.add(org.apache.hive.service.cli.thrift.TTypeId.NULL_TYPE);
   }
 
   public static final Set<TTypeId> COMPLEX_TYPES = new HashSet<TTypeId>();
@@ -80,6 +81,7 @@ public class TCLIServiceConstants {
     TYPE_NAMES.put(org.apache.hive.service.cli.thrift.TTypeId.STRUCT_TYPE, "STRUCT");
     TYPE_NAMES.put(org.apache.hive.service.cli.thrift.TTypeId.UNION_TYPE, "UNIONTYPE");
     TYPE_NAMES.put(org.apache.hive.service.cli.thrift.TTypeId.DECIMAL_TYPE, "DECIMAL");
+    TYPE_NAMES.put(org.apache.hive.service.cli.thrift.TTypeId.NULL_TYPE, "NULL");
   }
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapper.java
Patch:
@@ -142,7 +142,7 @@ protected Object clone() {
     clone.byteLengths = byteLengths.clone();
     for (int i = 0; i < byteValues.length; ++i) {
       // avoid allocation/copy of nulls, because it potentially expensive. branch instead.
-      if (!isNull[i]) {
+      if (!isNull[longValues.length + doubleValues.length + i]) {
         clone.byteValues[i] = Arrays.copyOfRange(
             byteValues[i],
             byteStarts[i],

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapperBatch.java
Patch:
@@ -471,7 +471,7 @@ public static VectorHashKeyWrapperBatch compileKeyWrapperBatch(VectorExpression[
       } else if (outputType.equalsIgnoreCase("string")) {
         indexLookup[i].longIndex = -1;
         indexLookup[i].doubleIndex = -1;
-        stringIndices[i]= stringIndicesIndex;
+        stringIndices[stringIndicesIndex]= i;
         ++stringIndicesIndex;
       }
       else {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorGroupByOperator.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator;
 import org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromConcat;
 import org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromLongIterables;
+import org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables;
 import org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromRepeats;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -50,7 +50,7 @@
 import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;
 import org.apache.hadoop.hive.ql.io.RCFileInputFormat;
 import org.apache.hadoop.hive.ql.io.RCFileOutputFormat;
-import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
+import org.apache.hadoop.hive.ql.io.orc.CommonOrcInputFormat;
 import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;
 import org.apache.hadoop.hive.ql.io.orc.OrcSerde;
 import org.apache.hadoop.hive.ql.lib.Node;
@@ -114,7 +114,7 @@ public abstract class BaseSemanticAnalyzer {
       .getName();
   protected static final String RCFILE_OUTPUT = RCFileOutputFormat.class
       .getName();
-  protected static final String ORCFILE_INPUT = OrcInputFormat.class
+  protected static final String ORCFILE_INPUT = CommonOrcInputFormat.class
       .getName();
   protected static final String ORCFILE_OUTPUT = OrcOutputFormat.class
       .getName();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFAvgDouble.java
Patch:
@@ -23,15 +23,14 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.
-  VectorAggregateExpression.AggregationBuffer;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFAvgLong.java
Patch:
@@ -23,15 +23,14 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.
-  VectorAggregateExpression.AggregationBuffer;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMaxDouble.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMaxLong.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMinDouble.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMinLong.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFStdPopDouble.java
Patch:
@@ -23,15 +23,14 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates
-    .VectorAggregateExpression.AggregationBuffer;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFStdPopLong.java
Patch:
@@ -23,15 +23,14 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates
-    .VectorAggregateExpression.AggregationBuffer;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFStdSampDouble.java
Patch:
@@ -23,15 +23,14 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates
-    .VectorAggregateExpression.AggregationBuffer;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFStdSampLong.java
Patch:
@@ -23,15 +23,14 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates
-    .VectorAggregateExpression.AggregationBuffer;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFSumDouble.java
Patch:
@@ -23,15 +23,14 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.
-    VectorAggregateExpression.AggregationBuffer;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFSumLong.java
Patch:
@@ -23,15 +23,14 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.
-    VectorAggregateExpression.AggregationBuffer;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFVarPopDouble.java
Patch:
@@ -23,15 +23,14 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates
-    .VectorAggregateExpression.AggregationBuffer;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFVarPopLong.java
Patch:
@@ -23,15 +23,14 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates
-    .VectorAggregateExpression.AggregationBuffer;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFVarSampDouble.java
Patch:
@@ -23,15 +23,14 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates
-    .VectorAggregateExpression.AggregationBuffer;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFVarSampLong.java
Patch:
@@ -23,15 +23,14 @@
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates
-    .VectorAggregateExpression.AggregationBuffer;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.AggregationBuffer;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFAvgLong.java
Patch:
@@ -117,7 +117,7 @@ public void aggregateInputSelection(
       
       inputExpression.evaluate(batch);
       
-      LongColumnVector inputVector = (LongColumnVector)batch.
+       LongColumnVector inputVector = ( LongColumnVector)batch.
         cols[this.inputExpression.getOutputColumn()];
       long[] vector = inputVector.vector;
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMaxDouble.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMaxLong.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMinDouble.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFMinLong.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen/VectorUDAFSumLong.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/DoubleColumnVector.java
Patch:
@@ -35,6 +35,7 @@
 public class DoubleColumnVector extends ColumnVector {
   public double[] vector;
   private final DoubleWritable writableObj = new DoubleWritable();
+  public static final double NULL_VALUE = Double.NaN;
 
   /**
    * Use this constructor by default. All column vectors

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/LongColumnVector.java
Patch:
@@ -35,6 +35,7 @@
 public class LongColumnVector extends ColumnVector {
   public long[] vector;
   private final LongWritable writableObj = new LongWritable();
+  public static final long NULL_VALUE = 1;
 
   /**
    * Use this constructor by default. All column vectors

File: beeline/src/java/org/apache/hive/beeline/Commands.java
Patch:
@@ -771,9 +771,6 @@ private boolean execute(String line, boolean call) {
           beeLine.info(beeLine.loc("rows-affected", count)
               + " " + beeLine.locElapsedTime(end - start));
         }
-      } catch (Exception e) {
-        beeLine.error(e);
-        throw e;
       } finally {
         if (stmnt != null) {
           stmnt.close();

File: jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java
Patch:
@@ -108,7 +108,7 @@ public void cancel() throws SQLException {
     } catch (SQLException e) {
       throw e;
     } catch (Exception e) {
-      throw new SQLException(e.toString(), "08S01");
+      throw new SQLException(e.toString(), "08S01", e);
     }
   }
 
@@ -143,7 +143,7 @@ private void closeClientOperation() throws SQLException {
     } catch (SQLException e) {
       throw e;
     } catch (Exception e) {
-      throw new SQLException(e.toString(), "08S01");
+      throw new SQLException(e.toString(), "08S01", e);
     }
     stmtHandle = null;
   }
@@ -184,7 +184,7 @@ public boolean execute(String sql) throws SQLException {
     } catch (SQLException eS) {
       throw eS;
     } catch (Exception ex) {
-      throw new SQLException(ex.toString(), "08S01");
+      throw new SQLException(ex.toString(), "08S01", ex);
     }
 
     if (!stmtHandle.isHasResultSet()) {

File: service/src/java/org/apache/hive/service/cli/operation/HiveCommandOperation.java
Patch:
@@ -79,6 +79,7 @@ private void setupSessionIO(SessionState sessionState) {
         sessionState.out = new PrintStream(System.out, true, "UTF-8");
         sessionState.err = new PrintStream(System.err, true, "UTF-8");
       } catch (UnsupportedEncodingException ee) {
+        LOG.error("Error creating PrintStream", e);
         ee.printStackTrace();
         sessionState.out = null;
         sessionState.err = null;
@@ -117,7 +118,7 @@ public void run() throws HiveSQLException {
       }
     } catch (Exception e) {
       setState(OperationState.ERROR);
-      throw new HiveSQLException("Error running query: " + e.toString());
+      throw new HiveSQLException("Error running query: " + e.toString(), e);
     }
     setState(OperationState.FINISHED);
   }

File: service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
Patch:
@@ -110,7 +110,7 @@ public void run() throws HiveSQLException {
       throw e;
     } catch (Exception e) {
       setState(OperationState.ERROR);
-      throw new HiveSQLException("Error running query: " + e.toString());
+      throw new HiveSQLException("Error running query: " + e.toString(), e);
     }
     setState(OperationState.FINISHED);
   }
@@ -208,7 +208,7 @@ private static Object convertLazyToJava(Object o, ObjectInspector oi) {
     // for now, expose non-primitive as a string
     // TODO: expose non-primitive as a structured object while maintaining JDBC compliance
     if (oi.getCategory() != ObjectInspector.Category.PRIMITIVE) {
-      return SerDeUtils.getJSONString(o, oi); 
+      return SerDeUtils.getJSONString(o, oi);
     }
     return obj;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/DoubleColumnVector.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.ql.exec.vector;
 
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.io.Writable;
 
 /**
@@ -59,7 +60,7 @@ public Writable getWritableObject(int index) {
       index = 0;
     }
     if (!noNulls && isNull[index]) {
-      return null;
+      return NullWritable.get();
     } else {
       writableObj.set(vector[index]);
       return writableObj;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/vector/LongColumnVector.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.ql.exec.vector;
 
 import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.io.Writable;
 
 /**
@@ -59,7 +60,7 @@ public Writable getWritableObject(int index) {
       index = 0;
     }
     if (!noNulls && isNull[index]) {
-      return null;
+      return NullWritable.get();
     } else {
       writableObj.set(vector[index]);
       return writableObj;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -1350,7 +1350,7 @@ private void analyzeAlterTableFileFormat(ASTNode ast, String tableName,
     case HiveParser.TOK_TBLRCFILE:
       inputFormat = RCFILE_INPUT;
       outputFormat = RCFILE_OUTPUT;
-      serde = COLUMNAR_SERDE;
+      serde = conf.getVar(HiveConf.ConfVars.HIVEDEFAULTRCFILESERDE);
       break;
     case HiveParser.TOK_TBLORCFILE:
       inputFormat = ORCFILE_INPUT;

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -1017,7 +1017,8 @@ public int checkCliDriverResults(String tname) throws Exception {
         ".*LOCK_TIME:.*",
         ".*grantTime.*",
         ".*[.][.][.] [0-9]* more.*",
-        ".*job_[0-9]*_[0-9]*.*",
+        ".*job_[0-9_]*.*",
+        ".*job_local[0-9_]*.*",
         ".*USING 'java -cp.*",
         "^Deleted.*",
     };

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorSelectOperator.java
Patch:
@@ -22,8 +22,8 @@
 
 import org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorizedRowGroupGenUtil;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddLongColumn;
+import org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.SelectDesc;
 import org.junit.Test;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestUnaryMinus.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColUnaryMinus;
+import org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil;
 import org.junit.Test;
 
 public class TestUnaryMinus {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorArithmeticExpressions.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddLongColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddLongScalar;
+import org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil;
 import org.junit.Test;
 
 public class TestVectorArithmeticExpressions {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorFilterExpressions.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColGreaterLongScalar;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColLessLongColumn;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddLongScalar;
+import org.apache.hadoop.hive.ql.exec.vector.util.*;
 import org.junit.Assert;
 import org.junit.Test;
 

File: ql/src/test/org/apache/hadoop/hive/ql/exec/vector/util/VectorizedRowGroupGenUtil.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.exec.vector.expressions;
+package org.apache.hadoop.hive.ql.exec.vector.util;
 
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;

File: service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hive.common.util.HiveVersionInfo;
 import org.apache.hive.service.cli.FetchOrientation;
 import org.apache.hive.service.cli.GetInfoType;
 import org.apache.hive.service.cli.GetInfoValue;
@@ -144,7 +145,7 @@ public GetInfoValue getInfo(GetInfoType getInfoType)
       case CLI_DBMS_NAME:
         return new GetInfoValue("Apache Hive");
       case CLI_DBMS_VER:
-        return new GetInfoValue("0.10.0");
+        return new GetInfoValue(HiveVersionInfo.getVersion());
       case CLI_MAX_COLUMN_NAME_LEN:
         return new GetInfoValue(128);
       case CLI_MAX_SCHEMA_NAME_LEN:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFBridge.java
Patch:
@@ -147,7 +147,7 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc
     }
 
     /** class for storing UDAFEvaluator value. */
-    static class UDAFAgg implements AggregationBuffer {
+    static class UDAFAgg extends AbstractAggregationBuffer {
       UDAFEvaluator ueObject;
 
       UDAFAgg(UDAFEvaluator ueObject) {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCollectSet.java
Patch:
@@ -99,7 +99,7 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters)
       }
     }
     
-    static class MkArrayAggregationBuffer implements AggregationBuffer {
+    static class MkArrayAggregationBuffer extends AbstractAggregationBuffer {
       Set<Object> container;
     }
     

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFContextNGrams.java
Patch:
@@ -400,7 +400,7 @@ public Object terminate(AggregationBuffer agg) throws HiveException {
 
 
     // Aggregation buffer methods. 
-    static class NGramAggBuf implements AggregationBuffer {
+    static class NGramAggBuf extends AbstractAggregationBuffer {
       ArrayList<String> context;
       NGramEstimator nge;
     };

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMax.java
Patch:
@@ -71,7 +71,7 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters)
     }
 
     /** class for storing the current max value */
-    static class MaxAgg implements AggregationBuffer {
+    static class MaxAgg extends AbstractAggregationBuffer {
       Object o;
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMin.java
Patch:
@@ -71,7 +71,7 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters)
     }
 
     /** class for storing the current max value */
-    static class MinAgg implements AggregationBuffer {
+    static class MinAgg extends AbstractAggregationBuffer {
       Object o;
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFnGrams.java
Patch:
@@ -338,7 +338,7 @@ public Object terminate(AggregationBuffer agg) throws HiveException {
     }
 
     // Aggregation buffer methods. 
-    static class NGramAggBuf implements AggregationBuffer {
+    static class NGramAggBuf extends AbstractAggregationBuffer {
       NGramEstimator nge;
       int n;
     };

File: hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hcatalog/pig/TestHCatStorer.java
Patch:
@@ -429,9 +429,9 @@ public void testStoreFuncAllSimpleTypes() throws IOException, CommandNeedRetryEx
         driver.getResults(res);
 
         Iterator<String> itr = res.iterator();
-        Assert.assertEquals("0\tNULL\tNULL\tNULL\tNULL\tnull\tnull", itr.next());
-        Assert.assertEquals("NULL\t4.2\t2.2\t4\tlets hcat\tbinary-data\tnull", itr.next());
-        Assert.assertEquals("3\t6.2999997\t3.3000000000000003\t6\tlets hcat\tbinary-data\tnull", itr.next());
+        Assert.assertEquals("0\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL", itr.next());
+        Assert.assertEquals("NULL\t4.2\t2.2\t4\tlets hcat\tbinary-data\tNULL", itr.next());
+        Assert.assertEquals("3\t6.2999997\t3.3000000000000003\t6\tlets hcat\tbinary-data\tNULL", itr.next()); 
         Assert.assertFalse(itr.hasNext());
 
         server.registerQuery("B = load 'junit_unparted' using " + HCatLoader.class.getName() + ";");

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/AvgPartitionSizeBasedBigTableSelectorForAutoSMJ.java
Patch:
@@ -57,6 +57,9 @@ public int getBigTablePosition(ParseContext parseCtx, JoinOperator joinOp)
       getListTopOps(joinOp, topOps);
       int currentPos = 0;
       for (TableScanOperator topOp : topOps) {
+        if (topOp == null) {
+          return -1;
+        }
         int numPartitions = 1; // in case the sizes match, preference is
                                // given to the table with fewer partitions
         Table table = parseCtx.getTopToTable().get(topOp);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/TableSizeBasedBigTableSelectorForAutoSMJ.java
Patch:
@@ -49,6 +49,9 @@ public int getBigTablePosition(ParseContext parseCtx, JoinOperator joinOp)
       getListTopOps(joinOp, topOps);
       int currentPos = 0;
       for (TableScanOperator topOp : topOps) {
+        if (topOp == null) {
+          return -1;
+        }
         Table table = parseCtx.getTopToTable().get(topOp);
         long currentSize = 0;
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -523,7 +523,7 @@ public static enum ConfVars {
     HIVE_AUTO_SORTMERGE_JOIN("hive.auto.convert.sortmerge.join", false),
     HIVE_AUTO_SORTMERGE_JOIN_BIGTABLE_SELECTOR(
         "hive.auto.convert.sortmerge.join.bigtable.selection.policy",
-        "org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ"),
+        "org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ"),
 
     HIVESCRIPTOPERATORTRUST("hive.exec.script.trust", false),
     HIVEROWOFFSET("hive.exec.rowoffset", false),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
Patch:
@@ -385,7 +385,7 @@ protected int getNextSize(int sz) {
   // all evaluation should be processed here for valid aliasFilterTags
   //
   // for MapJoin, filter tag is pre-calculated in MapredLocalTask and stored with value.
-  // when reading the hashtable, MapJoinObjectValue calcuates alias filter and provide it to join
+  // when reading the hashtable, MapJoinObjectValue calculates alias filter and provide it to join
   protected ArrayList<Object> getFilteredValue(byte alias, Object row) throws HiveException {
     boolean hasFilter = hasFilter(alias);
     ArrayList<Object> nr = JoinUtil.computeValues(row, joinValues[alias],

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericOp.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver;
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -49,7 +49,7 @@ public UDFBaseNumericOp() {
   protected LongWritable longWritable = new LongWritable();
   protected FloatWritable floatWritable = new FloatWritable();
   protected DoubleWritable doubleWritable = new DoubleWritable();
-  protected BigDecimalWritable bigDecimalWritable = new BigDecimalWritable();
+  protected HiveDecimalWritable decimalWritable = new HiveDecimalWritable();
 
   public abstract ByteWritable evaluate(ByteWritable a, ByteWritable b);
 
@@ -63,5 +63,5 @@ public UDFBaseNumericOp() {
 
   public abstract DoubleWritable evaluate(DoubleWritable a, DoubleWritable b);
 
-  public abstract BigDecimalWritable evaluate(BigDecimalWritable a, BigDecimalWritable b);
+  public abstract HiveDecimalWritable evaluate(HiveDecimalWritable a, HiveDecimalWritable b);
 }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericUnaryOp.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -46,7 +46,7 @@ public UDFBaseNumericUnaryOp() {
   protected LongWritable longWritable = new LongWritable();
   protected FloatWritable floatWritable = new FloatWritable();
   protected DoubleWritable doubleWritable = new DoubleWritable();
-  protected BigDecimalWritable bigDecimalWritable = new BigDecimalWritable();
+  protected HiveDecimalWritable decimalWritable = new HiveDecimalWritable();
 
   public abstract ByteWritable evaluate(ByteWritable a);
 
@@ -60,5 +60,5 @@ public UDFBaseNumericUnaryOp() {
 
   public abstract DoubleWritable evaluate(DoubleWritable a);
 
-  public abstract BigDecimalWritable evaluate(BigDecimalWritable a);
+  public abstract HiveDecimalWritable evaluate(HiveDecimalWritable a);
 }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPositive.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -68,7 +68,7 @@ public DoubleWritable evaluate(DoubleWritable a) {
   }
 
   @Override
-  public BigDecimalWritable evaluate(BigDecimalWritable a) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable a) {
     return a;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToByte.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -182,11 +182,11 @@ public ByteWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public ByteWritable evaluate(BigDecimalWritable i) {
+  public ByteWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      byteWritable.set(i.getBigDecimal().byteValue());
+      byteWritable.set(i.getHiveDecimal().byteValue());
       return byteWritable;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToDouble.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -184,11 +184,11 @@ public DoubleWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public DoubleWritable evaluate(BigDecimalWritable i) {
+  public DoubleWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      doubleWritable.set(i.getBigDecimal().doubleValue());
+      doubleWritable.set(i.getHiveDecimal().doubleValue());
       return doubleWritable;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToFloat.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -185,11 +185,11 @@ public FloatWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public FloatWritable evaluate(BigDecimalWritable i) {
+  public FloatWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      floatWritable.set(i.getBigDecimal().floatValue());
+      floatWritable.set(i.getHiveDecimal().floatValue());
       return floatWritable;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToInteger.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -189,11 +189,11 @@ public IntWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public IntWritable evaluate(BigDecimalWritable i) {
+  public IntWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      intWritable.set(i.getBigDecimal().intValue());
+      intWritable.set(i.getHiveDecimal().intValue());
       return intWritable;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToLong.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -193,11 +193,11 @@ public LongWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public LongWritable evaluate(BigDecimalWritable i) {
+  public LongWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      longWritable.set(i.getBigDecimal().longValue());
+      longWritable.set(i.getHiveDecimal().longValue());
       return longWritable;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToShort.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -182,11 +182,11 @@ public ShortWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public ShortWritable evaluate(BigDecimalWritable i) {
+  public ShortWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      shortWritable.set(i.getBigDecimal().shortValue());
+      shortWritable.set(i.getHiveDecimal().shortValue());
       return shortWritable;
     }
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToString.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hadoop.hive.ql.exec.UDF;
 import org.apache.hadoop.hive.serde2.ByteStream;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -142,7 +142,7 @@ public Text evaluate(TimestampWritable i) {
     }
   }
 
-  public Text evaluate(BigDecimalWritable i) {
+  public Text evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFReflect2.java
Patch:
@@ -20,16 +20,16 @@
 
 import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
-import java.math.BigDecimal;
 import java.sql.Timestamp;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.udf.UDFType;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -166,7 +166,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
         ((BytesWritable)returnObj).set((byte[])result, 0, ((byte[]) result).length);
         return returnObj;
       case DECIMAL:
-        ((BigDecimalWritable)returnObj).set((BigDecimal)result);
+        ((HiveDecimalWritable)returnObj).set((HiveDecimal)result);
         return returnObj;
     }
     throw new HiveException("Invalid type " + returnOI.getPrimitiveCategory());

File: serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
Patch:
@@ -34,7 +34,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
@@ -298,7 +298,7 @@ static void buildJSONString(StringBuilder sb, Object o, ObjectInspector oi, Stri
           break;
         }
         case DECIMAL: {
-          sb.append(((BigDecimalObjectInspector) oi).getPrimitiveJavaObject(o));
+          sb.append(((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o));
           break;
         }
         default:

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector;
-import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyByteObjectInspector;
@@ -114,7 +114,7 @@ public final class LazyFactory {
     case BINARY:
       return new LazyBinary((LazyBinaryObjectInspector) oi);
     case DECIMAL:
-      return new LazyBigDecimal((LazyBigDecimalObjectInspector) oi);
+      return new LazyHiveDecimal((LazyHiveDecimalObjectInspector) oi);
     default:
       throw new RuntimeException("Internal error: no LazyObject for " + p);
     }

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java
Patch:
@@ -21,19 +21,19 @@
 import java.io.DataOutputStream;
 import java.io.IOException;
 import java.io.OutputStream;
-import java.math.BigDecimal;
 import java.nio.ByteBuffer;
 import java.nio.charset.CharacterCodingException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Properties;
 
 import org.apache.commons.codec.binary.Base64;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.SerDeParameters;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
@@ -238,7 +238,7 @@ public static void writePrimitiveUTF8(OutputStream out, Object o,
       break;
     }
     case DECIMAL: {
-      BigDecimal bd = ((BigDecimalObjectInspector) oi).getPrimitiveJavaObject(o);
+      HiveDecimal bd = ((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o);
       ByteBuffer b = Text.encode(bd.toString());
       out.write(b.array(), 0, b.limit());
       break;

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyPrimitiveObjectInspectorFactory.java
Patch:
@@ -57,8 +57,8 @@ public final class LazyPrimitiveObjectInspectorFactory {
       new LazyTimestampObjectInspector();
   public static final LazyBinaryObjectInspector LAZY_BINARY_OBJECT_INSPECTOR =
       new LazyBinaryObjectInspector();
-  public static final LazyBigDecimalObjectInspector LAZY_BIG_DECIMAL_OBJECT_INSPECTOR =
-      new LazyBigDecimalObjectInspector();
+  public static final LazyHiveDecimalObjectInspector LAZY_BIG_DECIMAL_OBJECT_INSPECTOR =
+      new LazyHiveDecimalObjectInspector();
 
   static HashMap<ArrayList<Object>, LazyStringObjectInspector> cachedLazyStringObjectInspector =
       new HashMap<ArrayList<Object>, LazyStringObjectInspector>();

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryFactory.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector;
@@ -77,7 +77,7 @@ public final class LazyBinaryFactory {
     case BINARY:
       return new LazyBinaryBinary((WritableBinaryObjectInspector) oi);
     case DECIMAL:
-      return new LazyBinaryBigDecimal((WritableBigDecimalObjectInspector) oi);
+      return new LazyBinaryHiveDecimal((WritableHiveDecimalObjectInspector) oi);
     default:
       throw new RuntimeException("Internal error: no LazyBinaryObject for " + p);
     }

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableHiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableByteObjectInspector;
@@ -109,9 +109,9 @@ private static Converter getConverter(PrimitiveObjectInspector inputOI,
           inputOI,
           (SettableBinaryObjectInspector)outputOI);
     case DECIMAL:
-      return new PrimitiveObjectInspectorConverter.BigDecimalConverter(
+      return new PrimitiveObjectInspectorConverter.HiveDecimalConverter(
           (PrimitiveObjectInspector) inputOI,
-          (SettableBigDecimalObjectInspector) outputOI);
+          (SettableHiveDecimalObjectInspector) outputOI);
     default:
       throw new RuntimeException("Hive internal error: conversion of "
           + inputOI.getTypeName() + " to " + outputOI.getTypeName()

File: serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.serde2;
 
-import java.math.BigDecimal;
 import java.math.BigInteger;
 import java.util.List;
 import java.util.Properties;
@@ -27,6 +26,7 @@
 import junit.framework.TestCase;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestClass;
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct;
@@ -114,7 +114,7 @@ public void testLazyBinarySerDe() throws Throwable {
         Double d = randField > 5 ? null : Double.valueOf(r.nextDouble());
         String st = randField > 6 ? null : TestBinarySortableSerDe
             .getRandString(r);
-	BigDecimal bd = randField > 8 ? null : TestBinarySortableSerDe.getRandBigDecimal(r);
+	HiveDecimal bd = randField > 8 ? null : TestBinarySortableSerDe.getRandHiveDecimal(r);
         MyTestInnerStruct is = randField > 9 ? null : new MyTestInnerStruct(r
             .nextInt(5) - 2, r.nextInt(5) - 2);
         List<Integer> li = randField > 10 ? null : TestBinarySortableSerDe

File: serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/MyTestClassBigger.java
Patch:
@@ -17,10 +17,10 @@
  */
 package org.apache.hadoop.hive.serde2.lazybinary;
 
-import java.math.BigDecimal;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct;
 
 /**
@@ -35,7 +35,7 @@ public class MyTestClassBigger {
     Float myFloat;
     Double myDouble;
     String myString;
-    BigDecimal myDecimal;
+    HiveDecimal myDecimal;
     MyTestInnerStruct myStruct;
     List<Integer> myList;
     byte[] myBA;
@@ -45,7 +45,7 @@ public MyTestClassBigger() {
     }
 
     public MyTestClassBigger(Byte b, Short s, Integer i, Long l, Float f,
-			     Double d, String st, BigDecimal bd, MyTestInnerStruct is, List<Integer> li,
+			     Double d, String st, HiveDecimal bd, MyTestInnerStruct is, List<Integer> li,
 			     byte[] ba, Map<String, List<MyTestInnerStruct>> mp) {
 	myByte = b;
 	myShort = s;

File: serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/MyTestClassSmaller.java
Patch:
@@ -17,7 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.lazybinary;
 
-import java.math.BigDecimal;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct;
 
 public class MyTestClassSmaller {
@@ -28,14 +28,14 @@ public class MyTestClassSmaller {
     Float myFloat;
     Double myDouble;
     String myString;
-    BigDecimal myDecimal;
+    HiveDecimal myDecimal;
     MyTestInnerStruct myStruct;
 
     public MyTestClassSmaller() {
     }
     
     public MyTestClassSmaller(Byte b, Short s, Integer i, Long l, Float f,
-			      Double d, String st, BigDecimal bd, MyTestInnerStruct is) {
+			      Double d, String st, HiveDecimal bd, MyTestInnerStruct is) {
 	myByte = b;
 	myShort = s;
 	myInt = i;

File: serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorConverters.java
Patch:
@@ -19,8 +19,8 @@
 
 import junit.framework.TestCase;
 
-import java.math.BigDecimal;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -129,10 +129,10 @@ public void testObjectInspectorConverters() throws Throwable {
       assertEquals("TextConverter", new Text("hive"), textConverter
 	  .convert(new String("hive")));
       textConverter = ObjectInspectorConverters.getConverter(
-          PrimitiveObjectInspectorFactory.javaBigDecimalObjectInspector,
+          PrimitiveObjectInspectorFactory.javaHiveDecimalObjectInspector,
           PrimitiveObjectInspectorFactory.writableStringObjectInspector);
       assertEquals("TextConverter", new Text("100.001"), textConverter
-	  .convert(new BigDecimal("100.001")));
+	  .convert(new HiveDecimal("100.001")));
 
       // Binary
       Converter baConverter = ObjectInspectorConverters.getConverter(

File: ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcUnion.java
Patch:
@@ -116,17 +116,17 @@ public Object getField(Object obj) {
 
     @Override
     public String getTypeName() {
-      StringBuilder builder = new StringBuilder("union{");
+      StringBuilder builder = new StringBuilder("uniontype<");
       boolean first = true;
       for(ObjectInspector child: children) {
         if (first) {
           first = false;
         } else {
-          builder.append(", ");
+          builder.append(",");
         }
         builder.append(child.getTypeName());
       }
-      builder.append("}");
+      builder.append(">");
       return builder.toString();
     }
 

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java
Patch:
@@ -656,7 +656,7 @@ public void testUnionAndTimestamp() throws Exception {
     assertEquals(true, rows.hasNext());
     row = (OrcStruct) rows.next(null);
     inspector = reader.getObjectInspector();
-    assertEquals("struct<time:timestamp,union:union{int, string}>",
+    assertEquals("struct<time:timestamp,union:uniontype<int,string>>",
         inspector.getTypeName());
     assertEquals(Timestamp.valueOf("2000-03-12 15:00:00"),
         row.getFieldValue(0));

File: ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcStruct.java
Patch:
@@ -77,7 +77,7 @@ public void testInspectorFromTypeInfo() throws Exception {
         OrcStruct.createObjectInspector(typeInfo);
     assertEquals("struct<c1:boolean,c2:tinyint,c3:smallint,c4:int,c5:" +
         "bigint,c6:float,c7:double,c8:binary,c9:string,c10:struct<" +
-        "c1:int>,c11:map<int,int>,c12:union{int},c13:array<timestamp>>",
+        "c1:int>,c11:map<int,int>,c12:uniontype<int>,c13:array<timestamp>>",
         inspector.getTypeName());
     assertEquals(null,
         inspector.getAllStructFieldRefs().get(0).getFieldComment());

File: service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java
Patch:
@@ -35,6 +35,7 @@
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.SerDe;
+import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -199,17 +200,16 @@ private static Object convertLazyToJava(Object o, ObjectInspector oi) {
     Object obj = ObjectInspectorUtils.copyToStandardObject(o, oi, ObjectInspectorCopyOption.JAVA);
 
     if (obj == null) {
-      return obj;
+      return null;
     }
     if(oi.getTypeName().equals(serdeConstants.BINARY_TYPE_NAME)) {
       return new String((byte[])obj);
     }
     // for now, expose non-primitive as a string
     // TODO: expose non-primitive as a structured object while maintaining JDBC compliance
     if (oi.getCategory() != ObjectInspector.Category.PRIMITIVE) {
-      return obj.toString();
+      return SerDeUtils.getJSONString(o, oi); 
     }
-
     return obj;
   }
 

File: jdbc/src/java/org/apache/hive/jdbc/HiveResultSetMetaData.java
Patch:
@@ -114,6 +114,8 @@ public String getColumnTypeName(int column) throws SQLException {
       return serdeConstants.TIMESTAMP_TYPE_NAME;
     } else if ("decimal".equalsIgnoreCase(type)) {
       return serdeConstants.DECIMAL_TYPE_NAME;
+    } else if ("binary".equalsIgnoreCase(type)) {
+      return serdeConstants.BINARY_TYPE_NAME;
     } else if (type.startsWith("map<")) {
       return serdeConstants.STRING_TYPE_NAME;
     } else if (type.startsWith("array<")) {

File: jdbc/src/java/org/apache/hive/jdbc/Utils.java
Patch:
@@ -132,6 +132,8 @@ public static int hiveTypeToSqlType(String type) throws SQLException {
       return Types.TIMESTAMP;
     } else if ("decimal".equalsIgnoreCase(type)) {
       return Types.DECIMAL;
+    } else if ("binary".equalsIgnoreCase(type)) {
+      return Types.BINARY;
     } else if (type.startsWith("map<")) {
       return Types.VARCHAR;
     } else if (type.startsWith("array<")) {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFHour.java
Patch:
@@ -87,7 +87,7 @@ public IntWritable evaluate(TimestampWritable t) {
     }
 
     calendar.setTime(t.getTimestamp());
-    result.set(calendar.get(Calendar.HOUR));
+    result.set(calendar.get(Calendar.HOUR_OF_DAY));
     return result;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -2994,6 +2994,9 @@ private boolean isConstant(ASTNode node) {
       case HiveParser.TinyintLiteral:
         result = true;
         break;
+      case HiveParser.DecimalLiteral:
+        result = true;
+        break;
       case HiveParser.CharSetName:
         result = true;
         break;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -484,7 +484,7 @@ public static enum ConfVars {
 
     HIVESKEWJOIN("hive.optimize.skewjoin", false),
     HIVECONVERTJOIN("hive.auto.convert.join", true),
-    HIVECONVERTJOINNOCONDITIONALTASK("hive.auto.convert.join.noconditionaltask", false),
+    HIVECONVERTJOINNOCONDITIONALTASK("hive.auto.convert.join.noconditionaltask", true),
     HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD("hive.auto.convert.join.noconditionaltask.size",
         10000000L),
     HIVESKEWJOINKEY("hive.skewjoin.key", 100000),

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -470,7 +470,7 @@ public static enum ConfVars {
     HIVEUSEEXPLICITRCFILEHEADER("hive.exec.rcfile.use.explicit.header", true),
 
     HIVESKEWJOIN("hive.optimize.skewjoin", false),
-    HIVECONVERTJOIN("hive.auto.convert.join", false),
+    HIVECONVERTJOIN("hive.auto.convert.join", true),
     HIVECONVERTJOINNOCONDITIONALTASK("hive.auto.convert.join.noconditionaltask", false),
     HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD("hive.auto.convert.join.noconditionaltask.size",
         10000000L),

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
Patch:
@@ -425,8 +425,9 @@ private boolean checkPartialPartKeysEqual(List<FieldSchema> oldPartKeys,
     while (oldPartKeysIter.hasNext()) {
       oldFs = oldPartKeysIter.next();
       newFs = newPartKeysIter.next();
-      if (!oldFs.getName().equals(newFs.getName()) ||
-          !oldFs.getType().equals(newFs.getType())) {
+      // Alter table can change the type of partition key now.
+      // So check the column name only.
+      if (!oldFs.getName().equals(newFs.getName())) {
         return false;
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java
Patch:
@@ -188,6 +188,7 @@ public static BaseSemanticAnalyzer get(HiveConf conf, ASTNode tree)
       case HiveParser.TOK_ALTERTABLE_TOUCH:
       case HiveParser.TOK_ALTERTABLE_ARCHIVE:
       case HiveParser.TOK_ALTERTABLE_UNARCHIVE:
+      case HiveParser.TOK_ALTERTABLE_ALTERPARTS:
       case HiveParser.TOK_LOCKTABLE:
       case HiveParser.TOK_UNLOCKTABLE:
       case HiveParser.TOK_CREATEROLE:

File: ql/src/java/org/apache/hadoop/hive/ql/plan/AlterTableDesc.java
Patch:
@@ -48,7 +48,7 @@ public static enum AlterTableTypes {
     ADDFILEFORMAT, ADDCLUSTERSORTCOLUMN, RENAMECOLUMN, ADDPARTITION,
     TOUCH, ARCHIVE, UNARCHIVE, ALTERPROTECTMODE, ALTERPARTITIONPROTECTMODE,
     ALTERLOCATION, DROPPARTITION, RENAMEPARTITION, ADDSKEWEDBY, ALTERSKEWEDLOCATION,
-    ALTERBUCKETNUM
+    ALTERBUCKETNUM, ALTERPARTITION
   }
 
   public static enum ProtectModeType {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -128,6 +128,7 @@ public class HiveConf extends Configuration {
       HiveConf.ConfVars.HMSHANDLERATTEMPTS,
       HiveConf.ConfVars.HMSHANDLERINTERVAL,
       HiveConf.ConfVars.HMSHANDLERFORCERELOADCONF,
+      HiveConf.ConfVars.METASTORE_PARTITION_NAME_WHITELIST_PATTERN
       };
 
   /**

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Database.java
Patch:
@@ -708,7 +708,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, Database struct) th
                 for (int _i79 = 0; _i79 < _map78.size; ++_i79)
                 {
                   String _key80; // required
-                  String _val81; // required
+                  String _val81; // optional
                   _key80 = iprot.readString();
                   _val81 = iprot.readString();
                   struct.parameters.put(_key80, _val81);
@@ -858,7 +858,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, Database struct) thr
           for (int _i85 = 0; _i85 < _map84.size; ++_i85)
           {
             String _key86; // required
-            String _val87; // required
+            String _val87; // optional
             _key86 = iprot.readString();
             _val87 = iprot.readString();
             struct.parameters.put(_key86, _val87);

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/EnvironmentContext.java
Patch:
@@ -356,7 +356,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, EnvironmentContext
                 for (int _i247 = 0; _i247 < _map246.size; ++_i247)
                 {
                   String _key248; // required
-                  String _val249; // required
+                  String _val249; // optional
                   _key248 = iprot.readString();
                   _val249 = iprot.readString();
                   struct.properties.put(_key248, _val249);
@@ -439,7 +439,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, EnvironmentContext s
           for (int _i253 = 0; _i253 < _map252.size; ++_i253)
           {
             String _key254; // required
-            String _val255; // required
+            String _val255; // optional
             _key254 = iprot.readString();
             _val255 = iprot.readString();
             struct.properties.put(_key254, _val255);

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Index.java
Patch:
@@ -1145,7 +1145,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, Index struct) throw
                 for (int _i211 = 0; _i211 < _map210.size; ++_i211)
                 {
                   String _key212; // required
-                  String _val213; // required
+                  String _val213; // optional
                   _key212 = iprot.readString();
                   _val213 = iprot.readString();
                   struct.parameters.put(_key212, _val213);
@@ -1362,7 +1362,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, Index struct) throws
           for (int _i217 = 0; _i217 < _map216.size; ++_i217)
           {
             String _key218; // required
-            String _val219; // required
+            String _val219; // optional
             _key218 = iprot.readString();
             _val219 = iprot.readString();
             struct.parameters.put(_key218, _val219);

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Partition.java
Patch:
@@ -1005,7 +1005,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, Partition struct) t
                 for (int _i196 = 0; _i196 < _map195.size; ++_i196)
                 {
                   String _key197; // required
-                  String _val198; // required
+                  String _val198; // optional
                   _key197 = iprot.readString();
                   _val198 = iprot.readString();
                   struct.parameters.put(_key197, _val198);
@@ -1219,7 +1219,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, Partition struct) th
           for (int _i207 = 0; _i207 < _map206.size; ++_i207)
           {
             String _key208; // required
-            String _val209; // required
+            String _val209; // optional
             _key208 = iprot.readString();
             _val209 = iprot.readString();
             struct.parameters.put(_key208, _val209);

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Schema.java
Patch:
@@ -476,7 +476,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, Schema struct) thro
                 for (int _i232 = 0; _i232 < _map231.size; ++_i232)
                 {
                   String _key233; // required
-                  String _val234; // required
+                  String _val234; // optional
                   _key233 = iprot.readString();
                   _val234 = iprot.readString();
                   struct.properties.put(_key233, _val234);
@@ -597,7 +597,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, Schema struct) throw
           for (int _i243 = 0; _i243 < _map242.size; ++_i243)
           {
             String _key244; // required
-            String _val245; // required
+            String _val245; // optional
             _key244 = iprot.readString();
             _val245 = iprot.readString();
             struct.properties.put(_key244, _val245);

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/SerDeInfo.java
Patch:
@@ -534,7 +534,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, SerDeInfo struct) t
                 for (int _i89 = 0; _i89 < _map88.size; ++_i89)
                 {
                   String _key90; // required
-                  String _val91; // required
+                  String _val91; // optional
                   _key90 = iprot.readString();
                   _val91 = iprot.readString();
                   struct.parameters.put(_key90, _val91);
@@ -647,7 +647,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, SerDeInfo struct) th
           for (int _i95 = 0; _i95 < _map94.size; ++_i95)
           {
             String _key96; // required
-            String _val97; // required
+            String _val97; // optional
             _key96 = iprot.readString();
             _val97 = iprot.readString();
             struct.parameters.put(_key96, _val97);

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/SkewedInfo.java
Patch:
@@ -613,7 +613,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, SkewedInfo struct)
                 for (int _i108 = 0; _i108 < _map107.size; ++_i108)
                 {
                   List<String> _key109; // required
-                  String _val110; // required
+                  String _val110; // optional
                   {
                     org.apache.thrift.protocol.TList _list111 = iprot.readListBegin();
                     _key109 = new ArrayList<String>(_list111.size);
@@ -815,7 +815,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, SkewedInfo struct) t
           for (int _i134 = 0; _i134 < _map133.size; ++_i134)
           {
             List<String> _key135; // required
-            String _val136; // required
+            String _val136; // optional
             {
               org.apache.thrift.protocol.TList _list137 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
               _key135 = new ArrayList<String>(_list137.size);

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/StorageDescriptor.java
Patch:
@@ -1410,7 +1410,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, StorageDescriptor s
                 for (int _i150 = 0; _i150 < _map149.size; ++_i150)
                 {
                   String _key151; // required
-                  String _val152; // required
+                  String _val152; // optional
                   _key151 = iprot.readString();
                   _val152 = iprot.readString();
                   struct.parameters.put(_key151, _val152);
@@ -1734,7 +1734,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, StorageDescriptor st
           for (int _i171 = 0; _i171 < _map170.size; ++_i171)
           {
             String _key172; // required
-            String _val173; // required
+            String _val173; // optional
             _key172 = iprot.readString();
             _val173 = iprot.readString();
             struct.parameters.put(_key172, _val173);

File: metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java
Patch:
@@ -1423,7 +1423,7 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, Table struct) throw
                 for (int _i178 = 0; _i178 < _map177.size; ++_i178)
                 {
                   String _key179; // required
-                  String _val180; // required
+                  String _val180; // optional
                   _key179 = iprot.readString();
                   _val180 = iprot.readString();
                   struct.parameters.put(_key179, _val180);
@@ -1723,7 +1723,7 @@ public void read(org.apache.thrift.protocol.TProtocol prot, Table struct) throws
           for (int _i189 = 0; _i189 < _map188.size; ++_i189)
           {
             String _key190; // required
-            String _val191; // required
+            String _val191; // optional
             _key190 = iprot.readString();
             _val191 = iprot.readString();
             struct.parameters.put(_key190, _val191);

File: metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
Patch:
@@ -2369,6 +2369,8 @@ public void testConcurrentMetastores() throws Exception {
     }
   }
 
+
+
   /**
    * This method simulates another Hive metastore renaming a table, by accessing the db and
    * updating the name.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -46,6 +46,7 @@
 import org.apache.hadoop.hive.ql.lockmgr.HiveLock;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockManager;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockObj;
+import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
@@ -393,6 +394,8 @@ public int execute(DriverContext driverContext) {
             }
             dc = null; // reset data container to prevent it being added again.
           } else { // static partitions
+            List<String> partVals = Hive.getPvals(table.getPartCols(), tbd.getPartitionSpec());
+            db.validatePartitionNameCharacters(partVals);
             db.loadPartition(new Path(tbd.getSourceDir()), tbd.getTable().getTableName(),
                 tbd.getPartitionSpec(), tbd.getReplace(), tbd.getHoldDDLTime(),
                 tbd.getInheritTableSpecs(), isSkewedStoredAsDirs(tbd));

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
Patch:
@@ -64,7 +64,7 @@ public void alterTable(RawStore msdb, Warehouse wh, String dbname,
     }
 
     if (!MetaStoreUtils.validateName(newt.getTableName())
-        || !MetaStoreUtils.validateColNames(newt.getSd().getCols())) {
+        || !MetaStoreUtils.validateTblColumns(newt.getSd().getCols())) {
       throw new InvalidOperationException(newt.getTableName()
           + " is not a valid object name");
     }

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -995,9 +995,9 @@ private void create_table_core(final RawStore ms, final Table tbl,
         InvalidObjectException, NoSuchObjectException {
 
       if (!MetaStoreUtils.validateName(tbl.getTableName())
-          || !MetaStoreUtils.validateColNames(tbl.getSd().getCols())
+          || !MetaStoreUtils.validateTblColumns(tbl.getSd().getCols())
           || (tbl.getPartitionKeys() != null && !MetaStoreUtils
-              .validateColNames(tbl.getPartitionKeys()))
+              .validateTblColumns(tbl.getPartitionKeys()))
           || !MetaStoreUtils.validateSkewedColNames(
               (null == tbl.getSd().getSkewedInfo()) ?
                   null : tbl.getSd().getSkewedInfo().getSkewedColNames())

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveResultSetMetaData.java
Patch:
@@ -112,6 +112,8 @@ public String getColumnTypeName(int column) throws SQLException {
       return serdeConstants.BIGINT_TYPE_NAME;
     } else if ("timestamp".equalsIgnoreCase(type)) {
       return serdeConstants.TIMESTAMP_TYPE_NAME;
+    } else if ("decimal".equalsIgnoreCase(type)) {
+      return serdeConstants.DECIMAL_TYPE_NAME;
     } else if (type.startsWith("map<")) {
       return serdeConstants.STRING_TYPE_NAME;
     } else if (type.startsWith("array<")) {

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/Utils.java
Patch:
@@ -48,6 +48,8 @@ public static int hiveTypeToSqlType(String type) throws SQLException {
       return Types.BIGINT;
     } else if ("timestamp".equalsIgnoreCase(type)) {
       return Types.TIMESTAMP;
+    } else if ("decimal".equalsIgnoreCase(type)) {
+      return Types.DECIMAL;
     } else if (type.startsWith("map<")) {
       return Types.VARCHAR;
     } else if (type.startsWith("array<")) {

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
Patch:
@@ -395,7 +395,7 @@ public static void setSerdeParam(SerDeInfo sdi, Properties schema,
         org.apache.hadoop.hive.serde.serdeConstants.STRING_TYPE_NAME, "string");
     typeToThriftTypeMap.put(
         org.apache.hadoop.hive.serde.serdeConstants.BINARY_TYPE_NAME, "binary");
-    // These 3 types are not supported yet.
+    // These 4 types are not supported yet.
     // We should define a complex type date in thrift that contains a single int
     // member, and DynamicSerDe
     // should convert it to date type at runtime.
@@ -406,6 +406,8 @@ public static void setSerdeParam(SerDeInfo sdi, Properties schema,
     typeToThriftTypeMap
         .put(org.apache.hadoop.hive.serde.serdeConstants.TIMESTAMP_TYPE_NAME,
             "timestamp");
+    typeToThriftTypeMap.put(
+        org.apache.hadoop.hive.serde.serdeConstants.DECIMAL_TYPE_NAME, "decimal");
   }
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -149,6 +149,7 @@ public class DDLSemanticAnalyzer extends BaseSemanticAnalyzer {
     TokenToTypeName.put(HiveParser.TOK_DATE, serdeConstants.DATE_TYPE_NAME);
     TokenToTypeName.put(HiveParser.TOK_DATETIME, serdeConstants.DATETIME_TYPE_NAME);
     TokenToTypeName.put(HiveParser.TOK_TIMESTAMP, serdeConstants.TIMESTAMP_TYPE_NAME);
+    TokenToTypeName.put(HiveParser.TOK_DECIMAL, serdeConstants.DECIMAL_TYPE_NAME);
   }
 
   public static String getTypeName(int token) throws SemanticException {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -501,6 +501,8 @@ public static class DefaultExprProcessor implements NodeProcessor {
           serdeConstants.BINARY_TYPE_NAME);
       conversionFunctionTextHashMap.put(HiveParser.TOK_TIMESTAMP,
           serdeConstants.TIMESTAMP_TYPE_NAME);
+      conversionFunctionTextHashMap.put(HiveParser.TOK_DECIMAL,
+          serdeConstants.DECIMAL_TYPE_NAME);
     }
 
     public static boolean isRedundantConversionFunction(ASTNode expr,

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericOp.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver;
 import org.apache.hadoop.hive.ql.exec.UDF;
+import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -48,6 +49,7 @@ public UDFBaseNumericOp() {
   protected LongWritable longWritable = new LongWritable();
   protected FloatWritable floatWritable = new FloatWritable();
   protected DoubleWritable doubleWritable = new DoubleWritable();
+  protected BigDecimalWritable bigDecimalWritable = new BigDecimalWritable();
 
   public abstract ByteWritable evaluate(ByteWritable a, ByteWritable b);
 
@@ -61,4 +63,5 @@ public UDFBaseNumericOp() {
 
   public abstract DoubleWritable evaluate(DoubleWritable a, DoubleWritable b);
 
+  public abstract BigDecimalWritable evaluate(BigDecimalWritable a, BigDecimalWritable b);
 }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericUnaryOp.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
+import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -45,6 +46,7 @@ public UDFBaseNumericUnaryOp() {
   protected LongWritable longWritable = new LongWritable();
   protected FloatWritable floatWritable = new FloatWritable();
   protected DoubleWritable doubleWritable = new DoubleWritable();
+  protected BigDecimalWritable bigDecimalWritable = new BigDecimalWritable();
 
   public abstract ByteWritable evaluate(ByteWritable a);
 
@@ -58,4 +60,5 @@ public UDFBaseNumericUnaryOp() {
 
   public abstract DoubleWritable evaluate(DoubleWritable a);
 
+  public abstract BigDecimalWritable evaluate(BigDecimalWritable a);
 }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java
Patch:
@@ -102,6 +102,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case FLOAT:
     case DOUBLE:
     case TIMESTAMP:
+    case DECIMAL:
       switch (((PrimitiveTypeInfo) parameters[1]).getPrimitiveCategory()) {
       case BYTE:
       case SHORT:
@@ -110,6 +111,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
       case FLOAT:
       case DOUBLE:
       case TIMESTAMP:
+      case DECIMAL:
         return new GenericUDAFCorrelationEvaluator();
       case STRING:
       case BOOLEAN:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCovariance.java
Patch:
@@ -93,6 +93,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case FLOAT:
     case DOUBLE:
     case TIMESTAMP:
+    case DECIMAL:
       switch (((PrimitiveTypeInfo) parameters[1]).getPrimitiveCategory()) {
       case BYTE:
       case SHORT:
@@ -101,6 +102,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
       case FLOAT:
       case DOUBLE:
       case TIMESTAMP:
+      case DECIMAL:
         return new GenericUDAFCovarianceEvaluator();
       case STRING:
       case BOOLEAN:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCovarianceSample.java
Patch:
@@ -67,6 +67,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case FLOAT:
     case DOUBLE:
     case TIMESTAMP:
+    case DECIMAL:
       switch (((PrimitiveTypeInfo) parameters[1]).getPrimitiveCategory()) {
       case BYTE:
       case SHORT:
@@ -75,6 +76,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
       case FLOAT:
       case DOUBLE:
       case TIMESTAMP:
+      case DECIMAL:
         return new GenericUDAFCovarianceSampleEvaluator();
       case STRING:
       case BOOLEAN:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFStd.java
Patch:
@@ -28,7 +28,7 @@
 /**
  * Compute the standard deviation by extending GenericUDAFVariance and
  * overriding the terminate() method of the evaluator.
- * 
+ *
  */
 @Description(name = "std,stddev,stddev_pop",
     value = "_FUNC_(x) - Returns the standard deviation of a set of numbers")
@@ -56,6 +56,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case DOUBLE:
     case STRING:
     case TIMESTAMP:
+    case DECIMAL:
       return new GenericUDAFStdEvaluator();
     case BOOLEAN:
     default:
@@ -68,7 +69,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
   /**
    * Compute the standard deviation by extending GenericUDAFVarianceEvaluator
    * and overriding the terminate() method of the evaluator.
-   * 
+   *
    */
   public static class GenericUDAFStdEvaluator extends
       GenericUDAFVarianceEvaluator {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFStdSample.java
Patch:
@@ -28,7 +28,7 @@
 /**
  * Compute the sample standard deviation by extending GenericUDAFVariance and
  * overriding the terminate() method of the evaluator.
- * 
+ *
  */
 @Description(name = "stddev_samp",
     value = "_FUNC_(x) - Returns the sample standard deviation of a set of numbers")
@@ -55,6 +55,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case DOUBLE:
     case STRING:
     case TIMESTAMP:
+    case DECIMAL:
       return new GenericUDAFStdSampleEvaluator();
     case BOOLEAN:
     default:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFVarianceSample.java
Patch:
@@ -28,7 +28,7 @@
 /**
  * Compute the sample variance by extending GenericUDAFVariance and overriding
  * the terminate() method of the evaluator.
- * 
+ *
  */
 @Description(name = "var_samp",
     value = "_FUNC_(x) - Returns the sample variance of a set of numbers")
@@ -56,6 +56,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case DOUBLE:
     case STRING:
     case TIMESTAMP:
+    case DECIMAL:
       return new GenericUDAFVarianceSampleEvaluator();
     case BOOLEAN:
     default:

File: serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde/serdeConstants.java
Patch:
@@ -85,6 +85,8 @@ public class serdeConstants {
 
   public static final String TIMESTAMP_TYPE_NAME = "timestamp";
 
+  public static final String DECIMAL_TYPE_NAME = "decimal";
+
   public static final String BINARY_TYPE_NAME = "binary";
 
   public static final String LIST_TYPE_NAME = "array";
@@ -113,6 +115,7 @@ public class serdeConstants {
     PrimitiveTypes.add("date");
     PrimitiveTypes.add("datetime");
     PrimitiveTypes.add("timestamp");
+    PrimitiveTypes.add("decimal");
     PrimitiveTypes.add("binary");
   }
 

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryFactory.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBigDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector;
@@ -75,6 +76,8 @@ public final class LazyBinaryFactory {
       return new LazyBinaryTimestamp((WritableTimestampObjectInspector) oi);
     case BINARY:
       return new LazyBinaryBinary((WritableBinaryObjectInspector) oi);
+    case DECIMAL:
+      return new LazyBinaryBigDecimal((WritableBigDecimalObjectInspector) oi);
     default:
       throw new RuntimeException("Internal error: no LazyBinaryObject for " + p);
     }

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/PrimitiveObjectInspector.java
Patch:
@@ -27,7 +27,7 @@ public interface PrimitiveObjectInspector extends ObjectInspector {
    * The primitive types supported by Hive.
    */
   public static enum PrimitiveCategory {
-    VOID, BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, STRING, TIMESTAMP, BINARY, UNKNOWN
+    VOID, BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, STRING, TIMESTAMP, BINARY, DECIMAL, UNKNOWN
   };
 
   /**

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java
Patch:
@@ -64,6 +64,7 @@ public static TypeInfo getPrimitiveTypeInfo(String typeName) {
   public static final TypeInfo shortTypeInfo = getPrimitiveTypeInfo(serdeConstants.SMALLINT_TYPE_NAME);
   public static final TypeInfo timestampTypeInfo = getPrimitiveTypeInfo(serdeConstants.TIMESTAMP_TYPE_NAME);
   public static final TypeInfo binaryTypeInfo = getPrimitiveTypeInfo(serdeConstants.BINARY_TYPE_NAME);
+  public static final TypeInfo decimalTypeInfo = getPrimitiveTypeInfo(serdeConstants.DECIMAL_TYPE_NAME);
 
   public static final TypeInfo unknownTypeInfo = getPrimitiveTypeInfo("unknown");
 

File: ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
Patch:
@@ -113,7 +113,7 @@ public enum ErrorMsg {
   NO_VALID_PARTN(10056, "The query does not reference any valid partition. "
       + "To run this query, set hive.mapred.mode=nonstrict"),
   NO_OUTER_MAPJOIN(10057, "MAPJOIN cannot be performed with OUTER JOIN"),
-  INVALID_MAPJOIN_HINT(10058, "Neither table specified as map-table"),
+  INVALID_MAPJOIN_HINT(10058, "All tables are specified as map-table for join"),
   INVALID_MAPJOIN_TABLE(10059, "Result of a union cannot be a map table"),
   NON_BUCKETED_TABLE(10060, "Sampling expression needed for non-bucketed table"),
   BUCKETED_NUMERATOR_BIGGER_DENOMINATOR(10061, "Numerator should not be bigger than "

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.optimizer;
 
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
@@ -660,8 +661,8 @@ private int mapSideJoin(JoinOperator op, QBJoinTree joinTree) throws SemanticExc
       // support this by randomly
       // leaving some table from the list of tables to be cached
       if (mapJoinPos == -1) {
-        throw new SemanticException(ErrorMsg.INVALID_MAPJOIN_HINT.getMsg(pGraphContext.getQB()
-            .getParseInfo().getHints()));
+        throw new SemanticException(ErrorMsg.INVALID_MAPJOIN_HINT.getMsg(
+            Arrays.toString(joinTree.getBaseSrc())));
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveFileFormatUtils.java
Patch:
@@ -405,7 +405,7 @@ private static String getMatchingPath(Map<String, ArrayList<String>> pathToAlias
   }
 
   /**
-   * Get the list of operatators from the opeerator tree that are needed for the path
+   * Get the list of operators from the operator tree that are needed for the path
    * @param pathToAliases  mapping from path to aliases
    * @param aliasToWork    The operator tree to be invoked for a given alias
    * @param dir            The path to look for

File: service/src/test/org/apache/hadoop/hive/service/TestHiveServerSessions.java
Patch:
@@ -52,7 +52,7 @@ public void run() {
       }
     });
     server.start();
-    Thread.sleep(1000);
+    Thread.sleep(5000);
 
     for (int i = 0; i < transports.length ; i++) {
       TSocket transport = new TSocket("localhost", port);

File: ql/src/test/org/apache/hadoop/hive/ql/io/TestSymlinkTextInputFormat.java
Patch:
@@ -154,6 +154,7 @@ public void testCombine() throws Exception {
             + " failed with exit code= " + ecode);
       }
 
+      tblCreated = true;
       String loadFileCommand = "LOAD DATA LOCAL INPATH '" + 
         new Path(symlinkDir, "symlink_file").toString() + "' INTO TABLE " + tblName;
       
@@ -263,6 +264,7 @@ public void testAccuracy1() throws IOException {
       while (reader.next(key, value)) {
         received.add(value.toString());
       }
+      reader.close();
     }
 
     List<String> expected = new ArrayList<String>();
@@ -307,6 +309,7 @@ public void testAccuracy2() throws IOException {
       while (reader.next(key, value)) {
         received.add(value.toString());
       }
+      reader.close();
     }
 
     List<String> expected = new ArrayList<String>();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -4350,6 +4350,9 @@ private Operator genBucketingSortingDest(String dest, Operator input, QB qb, Tab
 
     if (enforceBucketing || enforceSorting) {
       int maxReducers = conf.getIntVar(HiveConf.ConfVars.MAXREDUCERS);
+      if (conf.getIntVar(HiveConf.ConfVars.HADOOPNUMREDUCERS) > 0) {
+        maxReducers = conf.getIntVar(HiveConf.ConfVars.HADOOPNUMREDUCERS);
+      }
       int numBuckets  = dest_tab.getNumBuckets();
       if (numBuckets > maxReducers) {
         multiFileSpray = true;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -433,7 +433,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
             ctx.setError(ErrorMsg.NON_KEY_EXPR_IN_GROUPBY.getMsg(exprNode), expr);
             return null;
           } else {
-            List<String> possibleColumnNames = input.getNonHiddenColumnNames(-1);
+            List<String> possibleColumnNames = input.getReferenceableColumnAliases(tableOrCol, -1);
             String reason = String.format("(possible column names are: %s)",
                 StringUtils.join(possibleColumnNames, ", "));
             ctx.setError(ErrorMsg.INVALID_TABLE_OR_COLUMN.getMsg(expr.getChild(0), reason),

File: shims/src/common-secure/test/org/apache/hadoop/hive/thrift/TestHadoop20SAuthBridge.java
Patch:
@@ -28,6 +28,7 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Enumeration;
+import java.util.List;
 
 import junit.framework.TestCase;
 
@@ -96,7 +97,7 @@ protected DelegationTokenStore getTokenStore(Configuration conf) throws IOExcept
 
   private void configureSuperUserIPAddresses(Configuration conf,
       String superUserShortName) throws IOException {
-    ArrayList<String> ipList = new ArrayList<String>();
+    List<String> ipList = new ArrayList<String>();
     Enumeration<NetworkInterface> netInterfaceList = NetworkInterface
         .getNetworkInterfaces();
     while (netInterfaceList.hasMoreElements()) {

File: common/src/test/org/apache/hadoop/hive/conf/TestHiveConf.java
Patch:
@@ -20,6 +20,7 @@
 import junit.framework.TestCase;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 
 
@@ -32,7 +33,8 @@
 public class TestHiveConf extends TestCase {
 
   public void testHiveSitePath() throws Exception {
-    String expectedPath = System.getProperty("test.build.resources") + "/hive-site.xml";
+    String expectedPath =
+        new Path(System.getProperty("test.build.resources") + "/hive-site.xml").toUri().getPath();
     assertEquals(expectedPath, new HiveConf().getHiveSitePath());
   }
 

File: ql/src/test/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
Patch:
@@ -86,8 +86,7 @@ protected void setUp() {
       int i = 0;
       Path[] hadoopDataFile = new Path[2];
       String[] testFiles = {"kv1.txt", "kv2.txt"};
-      String testFileDir = "file://"
-          + conf.get("test.data.files").replace('\\', '/').replace("c:", "");
+      String testFileDir = new Path(conf.get("test.data.files")).toUri().getPath();
       for (String oneFile : testFiles) {
         Path localDataFile = new Path(testFileDir, oneFile);
         hadoopDataFile[i] = new Path(tmppath, oneFile);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/errors/TaskLogProcessor.java
Patch:
@@ -31,6 +31,7 @@
 import java.util.regex.Pattern;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.mapred.JobConf;
 
 /**
@@ -201,7 +202,8 @@ public List<List<String>> getStackTraces() {
         Pattern endStackTracePattern =
             Pattern.compile("^\t... [0-9]+ more.*", Pattern.CASE_INSENSITIVE);
 
-        while ((inputLine = in.readLine()) != null) {
+        while ((inputLine =
+          ShimLoader.getHadoopShims().unquoteHtmlChars(in.readLine())) != null) {
 
           if (stackTracePattern.matcher(inputLine).matches() ||
               endStackTracePattern.matcher(inputLine).matches()) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
Patch:
@@ -46,7 +46,7 @@
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.common.FileUtils;
+import org.apache.hadoop.hive.common.CompressionUtils;
 import org.apache.hadoop.hive.common.LogUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
@@ -387,7 +387,7 @@ public int execute(DriverContext driverContext) {
           String archiveFileName = Utilities.generateTarFileName(stageId);
           localwork.setStageID(stageId);
 
-          FileUtils.tar(parentDir, fileNames,archiveFileName);
+          CompressionUtils.tar(parentDir, fileNames,archiveFileName);
           Path archivePath = new Path(archiveFileURI);
           LOG.info("Archive "+ hashtableFiles.length+" hash table files to " + archiveFileURI);
 

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyBinaryObjectInspector.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive;
 
-import org.apache.hadoop.hive.serde2.lazy.ByteArrayRef;
 import org.apache.hadoop.hive.serde2.lazy.LazyBinary;
 import org.apache.hadoop.hive.serde2.lazy.LazyUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
@@ -39,11 +38,11 @@ public Object copyObject(Object o) {
   }
 
   @Override
-  public ByteArrayRef getPrimitiveJavaObject(Object o) {
+  public byte[] getPrimitiveJavaObject(Object o) {
     if (null == o) {
       return null;
     }
-    return LazyUtils.createByteArrayRef(((LazyBinary) o).getWritableObject());
+    return LazyUtils.createByteArray(((LazyBinary) o).getWritableObject());
   }
 
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/BinaryObjectInspector.java
Patch:
@@ -18,14 +18,13 @@
 
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
-import org.apache.hadoop.hive.serde2.lazy.ByteArrayRef;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.io.BytesWritable;
 
 public interface BinaryObjectInspector extends PrimitiveObjectInspector {
 
   @Override
-  ByteArrayRef getPrimitiveJavaObject(Object o);
+  byte[] getPrimitiveJavaObject(Object o);
 
   @Override
   BytesWritable getPrimitiveWritableObject(Object o);

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java
Patch:
@@ -30,7 +30,6 @@
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
-import org.apache.hadoop.hive.serde2.lazy.ByteArrayRef;
 import org.apache.hadoop.hive.serde2.lazy.LazyInteger;
 import org.apache.hadoop.hive.serde2.lazy.LazyLong;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -146,7 +145,7 @@ static void registerType(PrimitiveTypeEntry t) {
 
   public static final PrimitiveTypeEntry binaryTypeEntry = new PrimitiveTypeEntry(
       PrimitiveCategory.BINARY, Constants.BINARY_TYPE_NAME, byte[].class,
-      ByteArrayRef.class, BytesWritable.class);
+      byte[].class, BytesWritable.class);
   public static final PrimitiveTypeEntry stringTypeEntry = new PrimitiveTypeEntry(
       PrimitiveCategory.STRING, Constants.STRING_TYPE_NAME, null, String.class,
       Text.class);

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantBinaryObjectInspector.java
Patch:
@@ -35,6 +35,9 @@ public WritableConstantBinaryObjectInspector(BytesWritable value) {
     this.value = value;
   }
 
+  /*
+   * {@inheritDoc}
+   */
   @Override
   public BytesWritable getWritableConstantValue() {
     return value;

File: serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable;
 import org.apache.hadoop.hive.serde2.columnar.BytesRefWritable;
 import org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe;
-import org.apache.hadoop.hive.serde2.lazy.ByteArrayRef;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 import org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -117,7 +116,7 @@ public void testLazyBinarySerDe() throws Throwable {
             .nextInt(5) - 2, r.nextInt(5) - 2);
         List<Integer> li = randField > 8 ? null : TestBinarySortableSerDe
             .getRandIntegerArray(r);
-        ByteArrayRef ba = TestBinarySortableSerDe.getRandBA(r, i);
+        byte[] ba = TestBinarySortableSerDe.getRandBA(r, i);
         MyTestClass t = new MyTestClass(b, s, n, l, f, d, st, is, li,ba);
         rows[i] = t;
       }

File: serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/MyTestClassBigger.java
Patch:
@@ -21,7 +21,6 @@
 import java.util.Map;
 
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct;
-import org.apache.hadoop.hive.serde2.lazy.ByteArrayRef;
 
 /**
  * MyTestClassBigger.
@@ -37,15 +36,15 @@ public class MyTestClassBigger {
   String myString;
   MyTestInnerStruct myStruct;
   List<Integer> myList;
-  ByteArrayRef myBA;
+  byte[] myBA;
   Map<String, List<MyTestInnerStruct>> myMap;
 
   public MyTestClassBigger() {
   }
 
   public MyTestClassBigger(Byte b, Short s, Integer i, Long l, Float f,
       Double d, String st, MyTestInnerStruct is, List<Integer> li,
-      ByteArrayRef ba, Map<String, List<MyTestInnerStruct>> mp) {
+      byte[] ba, Map<String, List<MyTestInnerStruct>> mp) {
     myByte = b;
     myShort = s;
     myInt = i;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
Patch:
@@ -694,7 +694,7 @@ public static void main(String[] args) throws IOException, HiveException {
     }
 
     if (ret != 0) {
-      System.exit(2);
+      System.exit(ret);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -92,6 +92,7 @@
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.QueryPlan;
 import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;
 import org.apache.hadoop.hive.ql.io.ContentSummaryInputFormat;
@@ -105,7 +106,6 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
Patch:
@@ -39,7 +39,7 @@
 import java.util.regex.Matcher;
 import org.apache.zookeeper.KeeperException;
 
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockManager;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockManagerCtx;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLock;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 import org.apache.hadoop.hive.ql.exec.ConditionalTask;
@@ -50,7 +51,6 @@
 import org.apache.hadoop.hive.ql.lib.NodeProcessor;
 import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
 import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRMapJoinCtx;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.RowResolver;
 import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java
Patch:
@@ -26,6 +26,7 @@
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.OperatorFactory;
@@ -39,7 +40,6 @@
 import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRMapJoinCtx;
 import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx;
 import org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.FileSinkDesc;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 import org.apache.hadoop.hive.ql.exec.GroupByOperator;
@@ -53,7 +54,6 @@
 import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
 import org.apache.hadoop.hive.ql.lib.Rule;
 import org.apache.hadoop.hive.ql.lib.RuleRegExp;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.GenMapRedWalker;
 import org.apache.hadoop.hive.ql.parse.OpParseContext;
 import org.apache.hadoop.hive.ql.parse.ParseContext;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.Utilities;
@@ -50,7 +51,6 @@
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.optimizer.Transform;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
 import org.apache.hadoop.hive.ql.parse.SemanticException;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.QueryProperties;
 import org.apache.hadoop.hive.ql.exec.FetchTask;
 import org.apache.hadoop.hive.ql.exec.Task;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -50,6 +50,7 @@
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.metastore.api.PrincipalType;
 import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.ArchiveUtils;
 import org.apache.hadoop.hive.ql.exec.FetchTask;
 import org.apache.hadoop.hive.ql.exec.Task;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java
Patch:
@@ -42,6 +42,7 @@
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.thrift.TDeserializer;
 import org.apache.thrift.TException;
 import org.apache.thrift.TSerializer;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.hooks.ReadEntity;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.plan.CreateFunctionDesc;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
Patch:
@@ -42,6 +42,7 @@
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.exec.Utilities;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.exec.Utilities;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -48,6 +48,7 @@
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.QueryProperties;
 import org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.ArchiveUtils;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 import org.apache.hadoop.hive.ql.exec.FunctionInfo;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java
Patch:
@@ -26,10 +26,10 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.Utilities;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java
Patch:
@@ -41,8 +41,7 @@ public void testHookLoading() throws Exception{
 
     driver.run("drop table testDL");
     CommandProcessorResponse resp = driver.run("create table testDL (a int) as select * from tbl2");
-    assertEquals(10, resp.getResponseCode());
-    assertTrue(resp.getErrorMessage().contains("CTAS not supported."));
+    assertEquals(40000, resp.getResponseCode());
 
     resp = driver.run("create table testDL (a int)");
     assertEquals(0, resp.getResponseCode());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
Patch:
@@ -130,6 +130,7 @@ public enum ErrorMsg {
   UDTF_NO_DISTRIBUTE_BY("DISTRUBTE BY is not supported with a UDTF in the SELECT clause"),
   UDTF_INVALID_LOCATION("UDTF's are not supported outside the SELECT clause, nor nested "
       + "in expressions"),
+  UDAF_INVALID_LOCATION("Not yet supported place for UDAF"),
   UDTF_LATERAL_VIEW("UDTF's cannot be in a select expression when there is a lateral view"),
   UDTF_ALIAS_MISMATCH("The number of aliases supplied in the AS clause does not match the "
       + "number of columns output by the UDTF"),

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPlus.java
Patch:
@@ -7,7 +7,7 @@
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE+2.0
+ *     http://www.apache.org/licenses/LICENSE-2.0
  *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
@@ -31,7 +31,7 @@
  * better performance and type checking (so we know int + int is still an int
  * instead of a double); otherwise a single method that takes (Number a, Number
  * b) and use a.doubleValue() == b.doubleValue() is enough.
- * 
+ *
  * The case of int + double will be handled by implicit type casting using
  * UDFRegistry.implicitConvertable method.
  */

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -268,6 +268,7 @@ public static enum ConfVars {
     METASTORE_KERBEROS_PRINCIPAL("hive.metastore.kerberos.principal",
         "hive-metastore/_HOST@EXAMPLE.COM"),
     METASTORE_USE_THRIFT_SASL("hive.metastore.sasl.enabled", false),
+    METASTORE_USE_THRIFT_FRAMED_TRANSPORT("hive.metastore.thrift.framed.transport.enabled", false),
     METASTORE_CLUSTER_DELEGATION_TOKEN_STORE_CLS(
         "hive.cluster.delegation.token.store.class",
         "org.apache.hadoop.hive.thrift.MemoryTokenStore"),

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -122,6 +122,7 @@ public class HiveConf extends Configuration {
       HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL,
       HiveConf.ConfVars.METASTORE_END_FUNCTION_LISTENERS,
       HiveConf.ConfVars.METASTORE_PART_INHERIT_TBL_PROPS,
+      HiveConf.ConfVars.METASTORE_BATCH_RETRIEVE_TABLE_PARTITION_MAX,
       HiveConf.ConfVars.METASTORE_PRE_EVENT_LISTENERS,
       };
 
@@ -290,6 +291,8 @@ public static enum ConfVars {
     METASTORE_IDENTIFIER_FACTORY("datanucleus.identifierFactory", "datanucleus"),
     METASTORE_PLUGIN_REGISTRY_BUNDLE_CHECK("datanucleus.plugin.pluginRegistryBundleCheck", "LOG"),
     METASTORE_BATCH_RETRIEVE_MAX("hive.metastore.batch.retrieve.max", 300),
+    METASTORE_BATCH_RETRIEVE_TABLE_PARTITION_MAX(
+      "hive.metastore.batch.retrieve.table.partition.max", 1000),
     METASTORE_PRE_EVENT_LISTENERS("hive.metastore.pre.event.listeners", ""),
     METASTORE_EVENT_LISTENERS("hive.metastore.event.listeners", ""),
     // should we do checks against the storage (usually hdfs) for operations like drop_partition

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
Patch:
@@ -161,7 +161,7 @@ public void alterTable(RawStore msdb, Warehouse wh, String dbname,
         }
 
         // also the location field in partition
-        List<Partition> parts = msdb.getPartitions(dbname, name, 0);
+        List<Partition> parts = msdb.getPartitions(dbname, name, -1);
         for (Partition part : parts) {
           String oldPartLoc = part.getSd().getLocation();
           Path oldPartLocPath = new Path(oldPartLoc);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -582,6 +582,7 @@ public static enum ConfVars {
     // A comma separated list of hooks which implement HiveDriverRunHook and will be run at the
     // beginning and end of Driver.run, these will be run in the order specified
     HIVE_DRIVER_RUN_HOOKS("hive.exec.driver.run.hooks", ""),
+    HIVE_DDL_OUTPUT_FORMAT("hive.ddl.output.format", null),
     ;
 
     public final String varname;

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.metadata;
+package org.apache.hadoop.hive.ql.metadata.formatting;
 
 import java.util.ArrayList;
 import java.util.Collections;
@@ -30,6 +30,8 @@
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.ql.index.HiveIndex;
 import org.apache.hadoop.hive.ql.index.HiveIndex.IndexType;
+import org.apache.hadoop.hive.ql.metadata.Partition;
+import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.plan.DescTableDesc;
 import org.apache.hadoop.hive.ql.plan.ShowIndexesDesc;
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java
Patch:
@@ -291,7 +291,6 @@ public static void main(String[] args) {
 
     FileSystem fs = null;
     JobConf conf = new JobConf(BlockMergeTask.class);
-    HiveConf hiveConf = new HiveConf(conf, BlockMergeTask.class);
     for (String path : paths) {
       try {
         Path pathObj = new Path(path);
@@ -329,6 +328,7 @@ public static void main(String[] args) {
         }
       }
     }
+    HiveConf hiveConf = new HiveConf(conf, BlockMergeTask.class);
 
     Log LOG = LogFactory.getLog(BlockMergeTask.class.getName());
     boolean isSilent = HiveConf.getBoolVar(conf,

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -1100,6 +1100,7 @@ public int execute() throws CommandNeedRetryException {
       DriverContext driverCxt = new DriverContext(runnable, ctx);
 
       SessionState.get().setLastMapRedStatsList(new ArrayList<MapRedStats>());
+      SessionState.get().setLocalMapRedErrors(new HashMap<String, List<String>>());
 
       // Add root Tasks to runnable
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -174,7 +174,7 @@ public static enum ConfVars {
     EXECPARALLETHREADNUMBER("hive.exec.parallel.thread.number", 8),
     HIVESPECULATIVEEXECREDUCERS("hive.mapred.reduce.tasks.speculative.execution", true),
     HIVECOUNTERSPULLINTERVAL("hive.exec.counters.pull.interval", 1000L),
-    DYNAMICPARTITIONING("hive.exec.dynamic.partition", false),
+    DYNAMICPARTITIONING("hive.exec.dynamic.partition", true),
     DYNAMICPARTITIONINGMODE("hive.exec.dynamic.partition.mode", "strict"),
     DYNAMICPARTITIONMAXPARTS("hive.exec.max.dynamic.partitions", 1000),
     DYNAMICPARTITIONMAXPARTSPERNODE("hive.exec.max.dynamic.partitions.pernode", 100),

File: common/src/java/org/apache/hadoop/hive/common/FileUtils.java
Patch:
@@ -144,7 +144,7 @@ public static String makePartName(List<String> partCols, List<String> vals,
         '\u0013', '\u0014', '\u0015', '\u0016', '\u0017', '\u0018', '\u0019',
         '\u001A', '\u001B', '\u001C', '\u001D', '\u001E', '\u001F',
         '"', '#', '%', '\'', '*', '/', ':', '=', '?', '\\', '\u007F', '{',
-        '[', ']'};
+        '[', ']', '^'};
     for (char c : clist) {
       charToEscape.set(c);
     }

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -53,6 +53,7 @@
 import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;
 import org.apache.hadoop.hive.cli.CliDriver;
 import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.common.io.CachingPrintStream;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.api.Index;
@@ -586,7 +587,7 @@ public void cliInit(String tname, boolean recreate) throws Exception {
     outf = new File(outf, qf.getName().concat(".out"));
     FileOutputStream fo = new FileOutputStream(outf);
     ss.out = new PrintStream(fo, true, "UTF-8");
-    ss.err = ss.out;
+    ss.err = new CachingPrintStream(fo, true, "UTF-8");
     ss.setIsSilent(true);
     SessionState oldSs = SessionState.get();
     if (oldSs != null && oldSs.out != null && oldSs.out != System.out) {

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStatsAggregator.java
Patch:
@@ -49,8 +49,8 @@ public class HBaseStatsAggregator implements StatsAggregator {
   public boolean connect(Configuration hiveconf) {
 
     try {
-      HBaseConfiguration hbaseConf = new HBaseConfiguration(hiveconf);
-      htable = new HTable(hbaseConf, HBaseStatsSetupConstants.PART_STAT_TABLE_NAME);
+      htable = new HTable(HBaseConfiguration.create(hiveconf),
+        HBaseStatsSetupConstants.PART_STAT_TABLE_NAME);
 
       return true;
     } catch (IOException e) {

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableOutputFormat.java
Patch:
@@ -78,7 +78,7 @@ public RecordWriter getHiveRecordWriter(
     jc.set(TableOutputFormat.OUTPUT_TABLE, hbaseTableName);
     final boolean walEnabled = HiveConf.getBoolVar(
         jc, HiveConf.ConfVars.HIVE_HBASE_WAL_ENABLED);
-    final HTable table = new HTable(new HBaseConfiguration(jc), hbaseTableName);
+    final HTable table = new HTable(HBaseConfiguration.create(jc), hbaseTableName);
     table.setAutoFlush(false);
 
     return new RecordWriter() {

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyObject.java
Patch:
@@ -21,17 +21,17 @@
 
 /**
  * LazyObject stores an object in a range of bytes in a byte[].
- * 
+ *
  * A LazyObject can represent any primitive object or hierarchical object like
  * array, map or struct.
  */
 public abstract class LazyObject<OI extends ObjectInspector> extends LazyObjectBase {
 
-  OI oi;
+  protected OI oi;
 
   /**
    * Create a LazyObject.
-   * 
+   *
    * @param oi
    *          Derived classes can access meta information about this Lazy Object
    *          (e.g, separator, nullSequence, escaper) from it.

File: cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
Patch:
@@ -346,7 +346,7 @@ public int processLine(String line) {
    * @param allowInterupting
    *          When true the function will handle SIG_INT (Ctrl+C) by interrupting the processing and
    *          returning -1
-   * @return
+   * @return 0 if ok
    */
   public int processLine(String line, boolean allowInterupting) {
     SignalHandler oldSignal = null;

File: common/src/java/org/apache/hadoop/hive/common/metrics/MetricsMBean.java
Patch:
@@ -43,7 +43,7 @@ public interface MetricsMBean extends DynamicMBean {
     /**
      *
      * @param name
-     * @return
+     * @return value associated with the key
      * @throws Exception
      */
     public abstract Object get(String name) throws IOException;

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveDatabaseMetaData.java
Patch:
@@ -389,7 +389,6 @@ public int getMaxCharLiteralLength() throws SQLException {
   /**
    *  Returns the value of maxColumnNameLength.
    *
-   *  @param int
    */
   public int getMaxColumnNameLength() throws SQLException {
     return maxColumnNameLength;
@@ -654,7 +653,7 @@ public int compare(JdbcTable o1, JdbcTable o2) {
   /**
    * Translate hive table types into jdbc table types.
    * @param hivetabletype
-   * @return
+   * @return the type of the table
    */
   public static String toJdbcTableType(String hivetabletype) {
     if (hivetabletype==null) {

File: metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java
Patch:
@@ -69,7 +69,7 @@ public abstract void alterTable(RawStore msdb, Warehouse wh, String dbname,
    *          original values of the partition being altered
    * @param new_part
    *          new partition object
-   * @return
+   * @return the altered partition
    * @throws InvalidOperationException
    * @throws InvalidObjectException
    * @throws AlreadyExistsException

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -290,7 +290,7 @@ private Configuration getConf() {
     /**
      * Get a cached RawStore.
      *
-     * @return
+     * @return the cached RawStore
      * @throws MetaException
      */
     @InterfaceAudience.LimitedPrivate({"HCATALOG"})
@@ -2873,7 +2873,7 @@ public static void startMetaStore(int port, HadoopThriftAuthBridge bridge)
    *
    * @param port
    * @param bridge
-   * @param hiveconf
+   * @param conf
    *          configuration overrides
    * @throws Throwable
    */

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreFS.java
Patch:
@@ -30,10 +30,10 @@ public interface MetaStoreFS {
 
   /**
    * delete a directory
-   * 
+   *
    * @param f
    * @param recursive
-   * @return
+   * @return true on success
    * @throws MetaException
    */
   public boolean deleteDir(FileSystem fs, Path f, boolean recursive,

File: metastore/src/java/org/apache/hadoop/hive/metastore/RawStore.java
Patch:
@@ -139,8 +139,6 @@ public List<Table> getTableObjectsByName(String dbname, List<String> tableNames)
    * Gets a list of tables based on a filter string and filter type.
    * @param dbName
    *          The name of the database from which you will retrieve the table names
-   * @param filterType
-   *          The type of filter
    * @param filter
    *          The filter string
    * @param max_tables

File: metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
Patch:
@@ -283,7 +283,7 @@ public static String makePartPath(Map<String, String> spec)
    * Makes a partition name from a specification
    * @param spec
    * @param addTrailingSeperator if true, adds a trailing separator e.g. 'ds=1/'
-   * @return
+   * @return partition name
    * @throws MetaException
    */
   public static String makePartName(Map<String, String> spec,

File: metastore/src/java/org/apache/hadoop/hive/metastore/hooks/JDOConnectionURLHook.java
Patch:
@@ -33,7 +33,7 @@ public interface JDOConnectionURLHook {
    * attempt.
    *
    * @param conf The configuration used to initialize this instance of the HMS
-   * @return
+   * @return the connection URL
    * @throws Exception
    */
   public String getJdoConnectionUrl(Configuration conf)

File: metastore/src/model/org/apache/hadoop/hive/metastore/model/MPartitionEvent.java
Patch:
@@ -64,7 +64,7 @@ public void setPartName(String partName) {
   }
 
   /**
-   * @param eventTime the eventTime to set
+   * @param createTime the eventTime to set
    */
   public void setEventTime(long createTime) {
     this.eventTime = createTime;

File: metastore/src/model/org/apache/hadoop/hive/metastore/model/MRegionStorageDescriptor.java
Patch:
@@ -48,7 +48,7 @@ public String getRegionName() {
   }
 
   /**
-   * @param region
+   * @param regionName
    */
   public void setRegionName(String regionName) {
     this.regionName = regionName;

File: metastore/src/test/org/apache/hadoop/hive/metastore/DummyListener.java
Patch:
@@ -35,7 +35,7 @@
 import org.apache.hadoop.hive.metastore.events.LoadPartitionDoneEvent;
 
 /** A dummy implementation for
- * {@link org.apache.hadoop.hive.metastore.hadooorg.apache.hadoop.hive.metastore.MetaStoreEventListener}
+ * {@link org.apache.hadoop.hive.metastore.MetaStoreEventListener}
  * for testing purposes.
  */
 public class DummyListener extends MetaStoreEventListener{

File: metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreEventListener.java
Patch:
@@ -47,7 +47,7 @@
 
 /**
  * TestMetaStoreEventListener. Test case for
- * {@link org.apache.hadoop.hive.metastore.hadooorg.apache.hadoop.hive.metastore.MetaStoreEventListener}
+ * {@link org.apache.hadoop.hive.metastore.MetaStoreEventListener}
  */
 public class TestMetaStoreEventListener extends TestCase {
   private static final String msPort = "20001";

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapperContext.java
Patch:
@@ -74,7 +74,7 @@ public ExecMapperContext() {
    * after the input file changed. This is first introduced to process bucket
    * map join.
    *
-   * @return
+   * @return is the input file changed?
    */
   public boolean inputFileChanged() {
     if (!inputFileChecked) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeGenericFuncEvaluator.java
Patch:
@@ -170,7 +170,7 @@ public Object evaluate(Object row) throws HiveException {
    * If the genericUDF is not a base comparison, or there is an error executing the comparison, it
    * returns null.
    * @param row
-   * @return
+   * @return the compare results
    * @throws HiveException
    */
   public Integer compare(Object row) throws HiveException {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -716,7 +716,9 @@ public static boolean implicitConvertable(TypeInfo from, TypeInfo to) {
    *
    * @param name
    *          the name of the UDAF
-   * @param argumentTypeInfos
+   * @param argumentOIs
+   * @param isDistinct
+   * @param isAllColumns
    * @return The UDAF evaluator
    */
   @SuppressWarnings("deprecation")

File: ql/src/java/org/apache/hadoop/hive/ql/exec/HadoopJobExecHelper.java
Patch:
@@ -100,7 +100,7 @@ private static String getJobStartMsg(String jobId) {
    * this msg pattern is used to track when a job is successfully done.
    *
    * @param jobId
-   * @return
+   * @return the job end message
    */
   public static String getJobEndMsg(String jobId) {
     return "Ended Job = " + jobId;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectKey.java
Patch:
@@ -42,7 +42,6 @@ public MapJoinObjectKey() {
   }
 
   /**
-   * @param metadataTag
    * @param obj
    */
   public MapJoinObjectKey(Object[] obj) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinSingleKey.java
Patch:
@@ -39,7 +39,6 @@ public MapJoinSingleKey() {
   }
 
   /**
-   * @param metadataTag
    * @param obj
    */
   public MapJoinSingleKey(Object obj) {

File: ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java
Patch:
@@ -352,8 +352,9 @@ public void setTaskProperty(String queryId, String taskId, Keys propName,
   /**
    * Serialize the task counters and set as a task property.
    *
+   * @param queryId
    * @param taskId
-   * @param rj
+   * @param ctrs
    */
   public void setTaskCounters(String queryId, String taskId, Counters ctrs) {
     String id = queryId + ":" + taskId;

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/LineageInfo.java
Patch:
@@ -350,7 +350,7 @@ public List<BaseColumnInfo> getBaseCols() {
     }
 
     /**
-     * @param basecols the baseCols to set
+     * @param baseCols the baseCols to set
      */
     public void setBaseCols(List<BaseColumnInfo> baseCols) {
       this.baseCols = baseCols;

File: ql/src/java/org/apache/hadoop/hive/ql/index/IndexSearchCondition.java
Patch:
@@ -44,7 +44,7 @@ public class IndexSearchCondition
    *
    * @param constantDesc constant value to search for
    *
-   * @Param comparisonExpr the original comparison expression
+   * @param comparisonExpr the original comparison expression
    */
   public IndexSearchCondition(
     ExprNodeColumnDesc columnDesc,
@@ -61,7 +61,7 @@ public IndexSearchCondition(
   public void setColumnDesc(ExprNodeColumnDesc columnDesc) {
     this.columnDesc = columnDesc;
   }
-  
+
   public ExprNodeColumnDesc getColumnDesc() {
     return columnDesc;
   }
@@ -89,7 +89,7 @@ public void setComparisonExpr(ExprNodeDesc comparisonExpr) {
   public ExprNodeDesc getComparisonExpr() {
     return comparisonExpr;
   }
-  
+
   @Override
   public String toString() {
     return comparisonExpr.getExprString();

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java
Patch:
@@ -333,7 +333,7 @@ private void beginLinearSearch() throws IOException {
   /**
    * Returns true if the current comparison is in the list of stop comparisons, i.e. we've found
    * all records which won't be filtered
-   * @return
+   * @return true if the current comparison is found
    */
   public boolean foundAllTargets() {
     if (this.getIOContext().getComparison() == null ||

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/AuthorizationException.java
Patch:
@@ -30,8 +30,8 @@ public AuthorizationException() {
   /**
    * Constructs an {@link AuthorizationException} with the specified detail
    * message.
-   * 
-   * @param s
+   *
+   * @param message
    *          the detail message.
    */
   public AuthorizationException(String message) {
@@ -40,7 +40,7 @@ public AuthorizationException(String message) {
 
   /**
    * Constructs an {@link AuthorizationException} with the specified cause.
-   * 
+   *
    * @param cause
    *          the cause
    */

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
Patch:
@@ -290,7 +290,7 @@ public void setInputFormatClass(Class<? extends InputFormat> inputFormatClass) {
   }
 
   /**
-   * @param class1
+   * @param outputFormatClass
    */
   public void setOutputFormatClass(Class<? extends HiveOutputFormat> outputFormatClass) {
     this.outputFormatClass = outputFormatClass;

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
Patch:
@@ -526,8 +526,6 @@ public int getNumBuckets() {
    *
    * @param srcf
    *          Source directory
-   * @param tmpd
-   *          Temporary directory
    */
   protected void replaceFiles(Path srcf) throws HiveException {
     Path tableDest =  new Path(getDataLocation().getPath());

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/index/RewriteParseContextGenerator.java
Patch:
@@ -53,7 +53,7 @@ private RewriteParseContextGenerator(){
    * Parse the input {@link String} command and generate a ASTNode tree.
    * @param conf
    * @param command
-   * @return
+   * @return the parse context
    * @throws SemanticException
    */
   public static ParseContext generateOperatorTree(HiveConf conf,

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/lineage/LineageCtx.java
Patch:
@@ -100,7 +100,7 @@ public void putDependency(Operator<? extends Serializable> op,
      *
      * @param op The operator of the column whose dependency is being modified.
      * @param ci The column info of the associated column.
-     * @param dependency The new dependency.
+     * @param dep The new dependency.
      */
     public void mergeDependency(Operator<? extends Serializable> op,
         ColumnInfo ci, Dependency dep) {
@@ -173,7 +173,7 @@ public Index getIndex() {
    *
    * @param old_type The old dependency type.
    * @param curr_type The current operators dependency type.
-   * @return
+   * @return the dependency type
    */
   public static LineageInfo.DependencyType getNewDependencyType(
       LineageInfo.DependencyType old_type, LineageInfo.DependencyType curr_type) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/PhysicalPlanResolver.java
Patch:
@@ -28,9 +28,9 @@ public interface PhysicalPlanResolver {
 
   /**
    * All physical plan resolvers have to implement this entry method.
-   * 
+   *
    * @param pctx
-   * @return
+   * @return the physical plan
    */
   PhysicalContext resolve(PhysicalContext pctx) throws SemanticException;
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -319,7 +319,8 @@ public static String charSetString(String charSetName, String charSetString)
   }
 
   /**
-   * @param Get the name from a table node
+   * Get the name from a table node.
+   * @param tableNameNode the table node
    * @return if DB name is give, db.tab is returned. Otherwise, tab.
    */
   public static String getUnescapedName(ASTNode tableNameNode) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/HiveSemanticAnalyzerHookContext.java
Patch:
@@ -48,7 +48,7 @@ public interface HiveSemanticAnalyzerHookContext extends Configurable{
   /**
    * The following methods will only be available to hooks executing postAnalyze.  If called in a
    * preAnalyze method, they should return null.
-   * @return
+   * @return the set of read entities
    */
   public Set<ReadEntity> getInputs();
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/QBJoinTree.java
Patch:
@@ -263,7 +263,7 @@ public void addRHSSemijoinColumns(String alias, ArrayList<ASTNode> columns) {
    * Remeber the mapping of table alias to set of columns.
    *
    * @param alias
-   * @param columns
+   * @param column
    */
   public void addRHSSemijoinColumns(String alias, ASTNode column) {
     ArrayList<ASTNode> cols = rhsSemijoin.get(alias);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/DescTableDesc.java
Patch:
@@ -97,7 +97,7 @@ public boolean isFormatted() {
   }
 
   /**
-   * @param isFormatted
+   * @param isFormat
    *          the isFormat to set
    */
   public void setFormatted(boolean isFormat) {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/JoinDesc.java
Patch:
@@ -317,7 +317,7 @@ public Map<Byte, Map<Byte, String>> getSmallKeysDirMap() {
   /**
    * set the mapping from tbl to dir for small keys.
    *
-   * @param bigKeysDirMap
+   * @param smallKeysDirMap
    */
   public void setSmallKeysDirMap(Map<Byte, Map<Byte, String>> smallKeysDirMap) {
     this.smallKeysDirMap = smallKeysDirMap;
@@ -348,7 +348,7 @@ public Map<Byte, TableDesc> getSkewKeysValuesTables() {
   }
 
   /**
-   * @param skewKeysValuesTable
+   * @param skewKeysValuesTables
    *          set the table desc for storing skew keys and their corresponding
    *          value;
    */

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ShowPartitionsDesc.java
Patch:
@@ -93,8 +93,7 @@ public Map<String, String> getPartSpec() {
   }
 
   /**
-   * @param tabName
-   *          the table whose partitions have to be listed
+   * @param partSpec the partSpec to set.
    */
   public void setPartSpec(Map<String, String> partSpec) {
     this.partSpec = partSpec;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ShowTableStatusDesc.java
Patch:
@@ -57,7 +57,7 @@ public String getSchema() {
    */
   public ShowTableStatusDesc() {
   }
-  
+
   /**
    * @param pattern
    *          names of tables to show
@@ -74,7 +74,7 @@ public ShowTableStatusDesc(String resFile, String dbName, String pattern) {
    *          data base name
    * @param pattern
    *          names of tables to show
-   * @param part
+   * @param partSpec
    *          partition specification
    */
   public ShowTableStatusDesc(String resFile, String dbName, String pattern,

File: ql/src/java/org/apache/hadoop/hive/ql/ppd/ExprWalkerInfo.java
Patch:
@@ -283,7 +283,7 @@ public void addNonFinalCandidate(ExprNodeDesc expr) {
   /**
    * Returns list of non-final candidate predicate for each map.
    *
-   * @return
+   * @return list of non-final candidate predicates
    */
   public Map<String, List<ExprNodeDesc>> getNonFinalCandidates() {
     return nonFinalPreds;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFPercentile.java
Patch:
@@ -36,7 +36,8 @@
  * UDAF for calculating the percentile values.
  * There are several definitions of percentile, and we take the method recommended by
  * NIST.
- * @see http://en.wikipedia.org/wiki/Percentile#Alternative_methods
+ * @see <a href="http://en.wikipedia.org/wiki/Percentile#Alternative_methods">
+ *      Percentile references</a>
  */
 @Description(name = "percentile",
     value = "_FUNC_(expr, pc) - Returns the percentile(s) of expr at pc (range: [0,1])."

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTF.java
Patch:
@@ -36,7 +36,7 @@ public abstract class GenericUDTF {
   /**
    * Initialize this GenericUDTF. This will be called only once per instance.
    *
-   * @param args
+   * @param argOIs
    *          An array of ObjectInspectors for the arguments
    * @return A StructObjectInspector for output. The output struct represents a
    *         row of the table where the fields of the stuct are the columns. The
@@ -49,7 +49,7 @@ public abstract StructObjectInspector initialize(ObjectInspector[] argOIs)
   /**
    * Give a set of arguments for the UDTF to process.
    *
-   * @param o
+   * @param args
    *          object array of arguments
    */
   public abstract void process(Object[] args) throws HiveException;

File: serde/src/java/org/apache/hadoop/hive/serde2/columnar/LazyDecompressionCallback.java
Patch:
@@ -22,8 +22,8 @@
 
 /**
  * Used to call back lazy decompression process.
- * 
- * @see #BytesRefWritable
+ *
+ * @see org.apache.hadoop.hive.serde2.columnar.BytesRefWritable
  */
 public interface LazyDecompressionCallback {
 

File: serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java
Patch:
@@ -354,7 +354,7 @@ private void populateTimestamp() {
    * Gets seconds stored as integer at bytes[offset]
    * @param bytes
    * @param offset
-   * @return
+   * @return the number of seconds
    */
   public static int getSeconds(byte[] bytes, int offset) {
     return NO_DECIMAL_MASK & bytesToInt(bytes, offset);
@@ -445,7 +445,7 @@ private static boolean setNanosBytes(int nanos, byte[] b, int offset) {
   /**
    * Interprets a float as a unix timestamp and returns a Timestamp object
    * @param f
-   * @return
+   * @return the equivalent Timestamp object
    */
   public static Timestamp floatToTimestamp(float f) {
     return doubleToTimestamp((double) f);

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryUtils.java
Patch:
@@ -314,7 +314,7 @@ public static void writeVInt(Output byteStream, int i) {
   /**
    * Write a zero-compressed encoded long to a byte array.
    *
-   * @param byteStream
+   * @param bytes
    *          the byte array/stream
    * @param l
    *          the long

File: shims/src/common/java/org/apache/hadoop/hive/io/HiveIOExceptionHandler.java
Patch:
@@ -32,16 +32,16 @@ public interface HiveIOExceptionHandler {
 
   /**
    * process exceptions raised when creating a record reader.
-   * 
+   *
    * @param e
-   * @return
+   * @return RecordReader
    */
   public RecordReader<?, ?> handleRecordReaderCreationException(Exception e)
       throws IOException;
 
   /**
    * process exceptions thrown when calling rr's next
-   * 
+   *
    * @param e
    * @param result
    * @throws IOException

File: shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
Patch:
@@ -215,15 +215,15 @@ public void doAs(UserGroupInformation ugi, PrivilegedExceptionAction<Void> pvea)
   String getTokenStrForm(String tokenSignature) throws IOException;
 
 
+  enum JobTrackerState { INITIALIZING, RUNNING };
+
   /**
    * Convert the ClusterStatus to its Thrift equivalent: JobTrackerState.
    * See MAPREDUCE-2455 for why this is a part of the shim.
    * @param clusterStatus
    * @return the matching JobTrackerState
    * @throws Exception if no equivalent JobTrackerState exists
    */
-  enum JobTrackerState { INITIALIZING, RUNNING };
-
   public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception;
 
   public TaskAttemptContext newTaskAttemptContext(Configuration conf, final Progressable progressable);

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
Patch:
@@ -346,9 +346,9 @@ protected void pushFilters(JobConf jobConf, TableScanOperator tableScan) {
       return;
     }
 
-    // construct column name list for reference by filter push down
+    // construct column name list and types for reference by filter push down
     Utilities.setColumnNameList(jobConf, tableScan);
-
+    Utilities.setColumnTypeList(jobConf, tableScan);
     // push down filters
     ExprNodeDesc filterExpr = scanDesc.getFilterExpr();
     if (filterExpr == null) {

File: ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java
Patch:
@@ -782,6 +782,7 @@ private static ExprNodeDesc pushFilterToStorageHandler(
       (HiveStoragePredicateHandler) storageHandler;
     JobConf jobConf = new JobConf(owi.getParseContext().getConf());
     Utilities.setColumnNameList(jobConf, tableScanOp);
+    Utilities.setColumnTypeList(jobConf, tableScanOp);
     Utilities.copyTableJobPropertiesToConf(
       Utilities.getTableDesc(tbl),
       jobConf);

File: hwi/src/java/org/apache/hadoop/hive/hwi/HWIServer.java
Patch:
@@ -78,7 +78,7 @@ public void start() throws IOException {
     String hivehome = System.getenv().get("HIVE_HOME");
     File hwiWARFile = new File(hivehome, hwiWAR);
     if (!hwiWARFile.exists()) {
-      l4j.fatal("HWI WAR file not found at " + hwiWAR);
+      l4j.fatal("HWI WAR file not found at " + hwiWARFile.toString());
       System.exit(1);
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -3081,7 +3081,7 @@ private Operator genCommonGroupByPlanReduceSinkOperator(QB qb, List<String> dest
     // them
     for (String destination : dests) {
 
-      getReduceValuesForReduceSinkNoMapAgg(parseInfo, dest, reduceSinkInputRowResolver,
+      getReduceValuesForReduceSinkNoMapAgg(parseInfo, destination, reduceSinkInputRowResolver,
           reduceSinkOutputRowResolver, outputValueColumnNames, reduceValues);
 
       // Need to pass all of the columns used in the where clauses as reduce values

File: ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
Patch:
@@ -418,8 +418,9 @@ public static boolean registerJar(String newJar) {
     LogHelper console = getConsole();
     try {
       ClassLoader loader = Thread.currentThread().getContextClassLoader();
-      Thread.currentThread().setContextClassLoader(
-          Utilities.addToClassPath(loader, StringUtils.split(newJar, ",")));
+      ClassLoader newLoader = Utilities.addToClassPath(loader, StringUtils.split(newJar, ","));
+      Thread.currentThread().setContextClassLoader(newLoader);
+      SessionState.get().getConf().setClassLoader(newLoader);
       console.printInfo("Added " + newJar + " to class path");
       return true;
     } catch (Exception e) {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -1166,12 +1166,12 @@ private void analyzeAlterTablePartMergeFiles(ASTNode tablePartAST, ASTNode ast,
           bucketCols = part.getBucketCols();
           inputFormatClass = part.getInputFormatClass();
           isArchived = ArchiveUtils.isArchived(part);
-          tblPartLoc = part.getDataLocation().toString();
+          tblPartLoc = part.getPartitionPath().toString();
         }
       } else {
         inputFormatClass = tblObj.getInputFormatClass();
         bucketCols = tblObj.getBucketCols();
-        tblPartLoc = tblObj.getDataLocation().toString();
+        tblPartLoc = tblObj.getPath().toString();
       }
 
       // throw a HiveException for non-rcfile.

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
Patch:
@@ -87,7 +87,9 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
           statsWork.setAggKey(op.getConf().getStatsAggPrefix());
           Task<StatsWork> statsTask = TaskFactory.get(statsWork, parseCtx.getConf());
           currTask.addDependentTask(statsTask);
-          ctx.getRootTasks().add(currTask);
+          if (!ctx.getRootTasks().contains(currTask)) {
+            ctx.getRootTasks().add(currTask);
+          }
           currWork.setGatheringStats(true);
           // NOTE: here we should use the new partition predicate pushdown API to get a list of pruned list,
           // and pass it to setTaskPlan as the last parameter

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcFactory.java
Patch:
@@ -68,7 +68,6 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
       if (uCtx == null) {
         uCtx = new UnionParseContext(union.getConf().getNumInputs());
       }
-
       ctx.setMapOnlySubq(false);
       uCtx.setMapOnlySubq(pos, false);
       uCtx.setRootTask(pos, false);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java
Patch:
@@ -303,7 +303,7 @@ public int execute(DriverContext driverContext) {
     }
   }
 
-  private void configureDebugVariablesForChildJVM(Map<String, String> environmentVariables) {
+  static void configureDebugVariablesForChildJVM(Map<String, String> environmentVariables) {
     // this method contains various asserts to warn if environment variables are in a buggy state
     assert environmentVariables.containsKey(HADOOP_CLIENT_OPTS)
         && environmentVariables.get(HADOOP_CLIENT_OPTS) != null : HADOOP_CLIENT_OPTS
@@ -539,7 +539,7 @@ public static String isEligibleForLocalMode(HiveConf conf,
 
     return null;
   }
-  
+
   @Override
   public Operator<? extends Serializable> getReducer() {
     return getWork().getReducer();

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -239,7 +239,7 @@ public HiveConf getHiveConf() {
     }
 
     private boolean init() throws MetaException {
-      rawStoreClassName = hiveConf.get("hive.metastore.rawstore.impl");
+      rawStoreClassName = hiveConf.getVar(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL);
       checkForDefaultDb = hiveConf.getBoolean(
           "hive.metastore.checkForDefaultDb", true);
       String alterHandlerName = hiveConf.get("hive.metastore.alter.impl",

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -180,7 +180,7 @@ public void alter_table(String dbname, String tbl_name, Table new_tbl)
    */
   public void renamePartition(final String dbname, final String name, final List<String> part_vals, final Partition newPart)
       throws InvalidOperationException, MetaException, TException {
-    client.alter_partition(dbname, name, part_vals, newPart);
+    client.rename_partition(dbname, name, part_vals, newPart);
   }
 
   private void open() throws MetaException {
@@ -794,7 +794,7 @@ public List<String> listPartitionNames(String db_name, String tbl_name,
 
   public void alter_partition(String dbName, String tblName, Partition newPart)
       throws InvalidOperationException, MetaException, TException {
-    client.alter_partition(dbName, tblName, null, newPart);
+    client.alter_partition(dbName, tblName, newPart);
   }
 
   public void alterDatabase(String dbName, Database db)

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveConnection.java
Patch:
@@ -76,7 +76,7 @@ public HiveConnection(String uri, Properties info) throws SQLException {
         client = new HiveServer.HiveServerHandler();
       } catch (MetaException e) {
         throw new SQLException("Error accessing Hive metastore: "
-            + e.getMessage(), "08S01");
+            + e.getMessage(), "08S01",e);
       }
     } else {
       // parse uri

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveDataSource.java
Patch:
@@ -58,7 +58,7 @@ public Connection getConnection(String username, String password)
     try {
       return new HiveConnection("", null);
     } catch (Exception ex) {
-      throw new SQLException();
+      throw new SQLException("Error in getting HiveConnection",ex);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
Patch:
@@ -151,7 +151,7 @@ public Partition(Table tbl, Map<String, String> partSpec, Path location)
       sd.read(prot);
     } catch (TException e) {
       LOG.error("Could not create a copy of StorageDescription");
-      throw new HiveException("Could not create a copy of StorageDescription");
+      throw new HiveException("Could not create a copy of StorageDescription",e);
     }
 
     tpart.setSd(sd);

File: serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/SimpleCharStream.java
Patch:
@@ -76,7 +76,7 @@ protected void ExpandBuff(boolean wrapAround) {
         maxNextCharInd = (bufpos -= tokenBegin);
       }
     } catch (Throwable t) {
-      throw new Error(t.getMessage());
+      throw new Error("Error in ExpandBuff",t);
     }
 
     bufsize += 2048;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -2516,7 +2516,7 @@ private int describeTable(Hive db, DescTableDesc descTbl) throws HiveException {
       Path resFile = new Path(descTbl.getResFile());
       if (tbl == null) {
         FileSystem fs = resFile.getFileSystem(conf);
-        outStream = (DataOutput) fs.open(resFile);
+        outStream = fs.create(resFile);
         String errMsg = "Table " + tableName + " does not exist";
         outStream.write(errMsg.getBytes("UTF-8"));
         ((FSDataOutputStream) outStream).close();
@@ -2527,7 +2527,7 @@ private int describeTable(Hive db, DescTableDesc descTbl) throws HiveException {
         part = db.getPartition(tbl, descTbl.getPartSpec(), false);
         if (part == null) {
           FileSystem fs = resFile.getFileSystem(conf);
-          outStream = (DataOutput) fs.open(resFile);
+          outStream = fs.create(resFile);
           String errMsg = "Partition " + descTbl.getPartSpec() + " for table "
               + tableName + " does not exist";
           outStream.write(errMsg.getBytes("UTF-8"));

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyBoolean.java
Patch:
@@ -64,6 +64,7 @@ public void init(ByteArrayRef bytes, int start, int length) {
       isNull = false;
     } else {
       isNull = true;
+      logExceptionMessage(bytes, start, length, "BOOLEAN");
     }
   }
 

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyByte.java
Patch:
@@ -53,6 +53,7 @@ public void init(ByteArrayRef bytes, int start, int length) {
       isNull = false;
     } catch (NumberFormatException e) {
       isNull = true;
+      logExceptionMessage(bytes, start, length, "TINYINT");
     }
   }
 

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyInteger.java
Patch:
@@ -56,6 +56,7 @@ public void init(ByteArrayRef bytes, int start, int length) {
       isNull = false;
     } catch (NumberFormatException e) {
       isNull = true;
+      logExceptionMessage(bytes, start, length, "INT");
     }
   }
 

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyLong.java
Patch:
@@ -56,6 +56,7 @@ public void init(ByteArrayRef bytes, int start, int length) {
       isNull = false;
     } catch (NumberFormatException e) {
       isNull = true;
+      logExceptionMessage(bytes, start, length, "BIGINT");
     }
   }
 

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyShort.java
Patch:
@@ -53,6 +53,7 @@ public void init(ByteArrayRef bytes, int start, int length) {
       isNull = false;
     } catch (NumberFormatException e) {
       isNull = true;
+      logExceptionMessage(bytes, start, length, "SMALLINT");
     }
   }
 

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyTimestamp.java
Patch:
@@ -69,6 +69,7 @@ public void init(ByteArrayRef bytes, int start, int length) {
     Timestamp t;
     if (s.compareTo("NULL") == 0) {
       t = null;
+      logExceptionMessage(bytes, start, length, "TIMESTAMP");
     } else {
       t = Timestamp.valueOf(s);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
Patch:
@@ -189,6 +189,7 @@ public enum ErrorMsg {
       + "If you really want to perform the operation, set hive.mapred.mode=nonstrict"),
   PARTSPEC_DIFFER_FROM_SCHEMA("Partition columns in partition specification are not the same as "
       + "that defined in the table schema. The names and orders have to be exactly the same."),
+  PARTITION_COLUMN_NON_PRIMITIVE("Partition column must be of primitive type."),
       ;
 
   private String mesg;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/HadoopJobExecHelper.java
Patch:
@@ -735,8 +735,6 @@ private List<ClientStatsPublisher> getClientStatPublishers() {
       try {
         clientStatsPublishers.add((ClientStatsPublisher) Class.forName(
             clientStatsPublisherClass.trim(), true, JavaUtils.getClassLoader()).newInstance());
-      } catch (RuntimeException e) {
-        throw e;
       } catch (Exception e) {
         LOG.warn(e.getClass().getName() + " occured when trying to create class: "
             + clientStatsPublisherClass.trim() + " implementing ClientStatsPublisher interface");

File: serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestStandardObjectInspectors.java
Patch:
@@ -517,7 +517,7 @@ public void testStandardUnionObjectInspector() throws Throwable {
       Throwable th = null;
       try {
         ObjectInspectorUtils.compare(union, uoi1,
-            new StandardUnion((byte) 4, map.clone()), uoi2);
+            new StandardUnion((byte) 4, map.clone()), uoi2, null);
       } catch (Throwable t) {
         th = t;
       }

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
Patch:
@@ -360,6 +360,8 @@ public static void setSerdeParam(SerDeInfo sdi, Properties schema,
         org.apache.hadoop.hive.serde.Constants.MAP_TYPE_NAME, "map");
     typeToThriftTypeMap.put(
         org.apache.hadoop.hive.serde.Constants.STRING_TYPE_NAME, "string");
+    typeToThriftTypeMap.put(
+        org.apache.hadoop.hive.serde.Constants.BINARY_TYPE_NAME, "binary");
     // These 3 types are not supported yet.
     // We should define a complex type date in thrift that contains a single int
     // member, and DynamicSerDe

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -184,6 +184,7 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFStruct;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFTimestamp;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFToBinary;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUtcTimestamp;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnion;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFWhen;
@@ -374,6 +375,8 @@ public final class FunctionRegistry {
 
     registerGenericUDF(Constants.TIMESTAMP_TYPE_NAME,
         GenericUDFTimestamp.class);
+    registerGenericUDF(Constants.BINARY_TYPE_NAME,
+        GenericUDFToBinary.class);
 
     // Aggregate functions
     registerGenericUDAF("max", new GenericUDAFMax());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -42,8 +42,6 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.Warehouse;
@@ -135,6 +133,7 @@ public class DDLSemanticAnalyzer extends BaseSemanticAnalyzer {
     TokenToTypeName.put(HiveParser.TOK_FLOAT, Constants.FLOAT_TYPE_NAME);
     TokenToTypeName.put(HiveParser.TOK_DOUBLE, Constants.DOUBLE_TYPE_NAME);
     TokenToTypeName.put(HiveParser.TOK_STRING, Constants.STRING_TYPE_NAME);
+    TokenToTypeName.put(HiveParser.TOK_BINARY, Constants.BINARY_TYPE_NAME);
     TokenToTypeName.put(HiveParser.TOK_DATE, Constants.DATE_TYPE_NAME);
     TokenToTypeName.put(HiveParser.TOK_DATETIME, Constants.DATETIME_TYPE_NAME);
     TokenToTypeName.put(HiveParser.TOK_TIMESTAMP, Constants.TIMESTAMP_TYPE_NAME);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java
Patch:
@@ -116,6 +116,7 @@ public class ParseDriver {
     xlateMap.put("KW_DATETIME", "DATETIME");
     xlateMap.put("KW_TIMESTAMP", "TIMESTAMP");
     xlateMap.put("KW_STRING", "STRING");
+    xlateMap.put("KW_BINARY", "BINARY");
     xlateMap.put("KW_ARRAY", "ARRAY");
     xlateMap.put("KW_MAP", "MAP");
     xlateMap.put("KW_REDUCE", "REDUCE");

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -458,6 +458,8 @@ public static class DefaultExprProcessor implements NodeProcessor {
           Constants.DOUBLE_TYPE_NAME);
       conversionFunctionTextHashMap.put(HiveParser.TOK_STRING,
           Constants.STRING_TYPE_NAME);
+      conversionFunctionTextHashMap.put(HiveParser.TOK_BINARY,
+          Constants.BINARY_TYPE_NAME);
       conversionFunctionTextHashMap.put(HiveParser.TOK_TIMESTAMP,
           Constants.TIMESTAMP_TYPE_NAME);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTimestamp.java
Patch:
@@ -73,9 +73,9 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
   public String getDisplayString(String[] children) {
     assert (children.length == 1);
     StringBuilder sb = new StringBuilder();
-    sb.append("CAST ");
+    sb.append("CAST( ");
     sb.append(children[0]);
-    sb.append(" AS TIMESTAMP");
+    sb.append(" AS TIMESTAMP)");
     return sb.toString();
   }
 

File: serde/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/serde/Constants.java
Patch:
@@ -74,6 +74,8 @@ public class Constants {
 
   public static final String TIMESTAMP_TYPE_NAME = "timestamp";
 
+  public static final String BINARY_TYPE_NAME = "binary";
+
   public static final String LIST_TYPE_NAME = "array";
 
   public static final String MAP_TYPE_NAME = "map";
@@ -100,6 +102,7 @@ public class Constants {
     PrimitiveTypes.add("date");
     PrimitiveTypes.add("datetime");
     PrimitiveTypes.add("timestamp");
+    PrimitiveTypes.add("binary");
   }
 
   public static final Set<String> CollectionTypes = new HashSet<String>();

File: serde/src/java/org/apache/hadoop/hive/serde2/DelimitedJSONSerDe.java
Patch:
@@ -22,6 +22,7 @@
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.serde.Constants;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
@@ -53,7 +54,7 @@ public Object deserialize(Writable field) throws SerDeException {
   @Override
   protected void serializeField(ByteStream.Output out, Object obj, ObjectInspector objInspector,
       SerDeParameters serdeParams) throws SerDeException {
-    if (!objInspector.getCategory().equals(Category.PRIMITIVE)) {
+    if (!objInspector.getCategory().equals(Category.PRIMITIVE) || (objInspector.getTypeName().equalsIgnoreCase(Constants.BINARY_TYPE_NAME))) {
       try {
         serialize(out, SerDeUtils.getJSONString(obj, objInspector),
             PrimitiveObjectInspectorFactory.javaStringObjectInspector, serdeParams.getSeparators(),

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector;
+import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyByteObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDoubleObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyFloatObjectInspector;
@@ -78,6 +79,8 @@ public final class LazyFactory {
       return new LazyString((LazyStringObjectInspector) oi);
     case TIMESTAMP:
       return new LazyTimestamp((LazyTimestampObjectInspector) oi);
+    case BINARY:
+      return new LazyBinary((LazyBinaryObjectInspector) oi);
     default:
       throw new RuntimeException("Internal error: no LazyObject for " + p);
     }

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryFactory.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector;
@@ -72,6 +73,8 @@ public final class LazyBinaryFactory {
       return new LazyBinaryVoid((WritableVoidObjectInspector) oi);
     case TIMESTAMP:
       return new LazyBinaryTimestamp((WritableTimestampObjectInspector) oi);
+    case BINARY:
+      return new LazyBinaryBinary((WritableBinaryObjectInspector) oi);
     default:
       throw new RuntimeException("Internal error: no LazyBinaryObject for " + p);
     }

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/PrimitiveObjectInspector.java
Patch:
@@ -27,7 +27,7 @@ public interface PrimitiveObjectInspector extends ObjectInspector {
    * The primitive types supported by Hive.
    */
   public static enum PrimitiveCategory {
-    VOID, BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, STRING, TIMESTAMP, UNKNOWN
+    VOID, BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, STRING, TIMESTAMP, BINARY, UNKNOWN
   };
 
   /**

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java
Patch:
@@ -63,6 +63,7 @@ public static TypeInfo getPrimitiveTypeInfo(String typeName) {
   public static final TypeInfo byteTypeInfo = getPrimitiveTypeInfo(Constants.TINYINT_TYPE_NAME);
   public static final TypeInfo shortTypeInfo = getPrimitiveTypeInfo(Constants.SMALLINT_TYPE_NAME);
   public static final TypeInfo timestampTypeInfo = getPrimitiveTypeInfo(Constants.TIMESTAMP_TYPE_NAME);
+  public static final TypeInfo binaryTypeInfo = getPrimitiveTypeInfo(Constants.BINARY_TYPE_NAME);
 
   public static final TypeInfo unknownTypeInfo = getPrimitiveTypeInfo("unknown");
 

File: serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable;
 import org.apache.hadoop.hive.serde2.columnar.BytesRefWritable;
 import org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe;
+import org.apache.hadoop.hive.serde2.lazy.ByteArrayRef;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 import org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -116,7 +117,8 @@ public void testLazyBinarySerDe() throws Throwable {
             .nextInt(5) - 2, r.nextInt(5) - 2);
         List<Integer> li = randField > 8 ? null : TestBinarySortableSerDe
             .getRandIntegerArray(r);
-        MyTestClass t = new MyTestClass(b, s, n, l, f, d, st, is, li);
+        ByteArrayRef ba = TestBinarySortableSerDe.getRandBA(r, i);
+        MyTestClass t = new MyTestClass(b, s, n, l, f, d, st, is, li,ba);
         rows[i] = t;
       }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java
Patch:
@@ -95,9 +95,12 @@ private int createFunction(CreateFunctionDesc createFunctionDesc) {
             .newInstance(udfClass, null));
         return 0;
       }
+      console.printError("FAILED: Class " + createFunctionDesc.getClassName()
+          + " does not implement UDF, GenericUDF, or UDAF");
       return 1;
 
     } catch (ClassNotFoundException e) {
+      console.printError("FAILED: Class " + createFunctionDesc.getClassName() + " not found");
       LOG.info("create function: " + StringUtils.stringifyException(e));
       return 1;
     }

File: ql/src/java/org/apache/hadoop/hive/ql/index/AggregateIndexHandler.java
Patch:
@@ -81,10 +81,10 @@ private void createAggregationFunction(List<FieldSchema> indexTblCols, String pr
       String funcName = aggFuncCol[0];
       String colName = aggFuncCol[1].substring(0, aggFuncCol[1].length() - 1);
       if(colName.contains("*")){
-        colName = colName.replace("*", "ALL");
+        colName = colName.replace("*", "all");
       }
       FieldSchema aggregationFunction =
-        new FieldSchema("_" + funcName + "_Of_" + colName + "", "bigint", "");
+        new FieldSchema("_" + funcName + "_of_" + colName + "", "bigint", "");
       indexTblCols.add(aggregationFunction);
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/index/RewriteCanApplyProcFactory.java
Patch:
@@ -127,7 +127,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,
                  }else if(para.size() == 0){
                    //count(*) case
                    canApplyCtx.setCountOnAllCols(true);
-                   canApplyCtx.setAggFunction("_count_Of_ALL");
+                   canApplyCtx.setAggFunction("_count_of_all");
                  }else{
                    assert para.size()==1;
                    for(int i=0; i< para.size(); i++){
@@ -144,12 +144,12 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,
                        //if columns contained in agg func are index key columns
                        canApplyCtx.getAggFuncColList().add(
                            ((ExprNodeColumnDesc) expr).getColumn());
-                       canApplyCtx.setAggFunction("_count_Of_" +
+                       canApplyCtx.setAggFunction("_count_of_" +
                            ((ExprNodeColumnDesc) expr).getColumn() + "");
                      }else if(expr instanceof ExprNodeConstantDesc){
                        //count(1) case
                        canApplyCtx.setCountOfOne(true);
-                       canApplyCtx.setAggFunction("_count_Of_1");
+                       canApplyCtx.setAggFunction("_count_of_1");
                      }
                    }
                  }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/index/RewriteGBUsingIndex.java
Patch:
@@ -63,7 +63,7 @@
  * </code>
  *  to
  *  <code>
- *   select sum(_count_Of_key)
+ *   select sum(_count_of_key)
  *   from idx_table
  *   group by key;
  *  </code>
@@ -250,7 +250,7 @@ private boolean checkIfRewriteCanBeApplied(TableScanOperator topOp, Table baseTa
             String aggregationFunction = indexTableMap.get(index).toString();
             aggregationFunction = aggregationFunction.substring(1,
                 aggregationFunction.length() - 1);
-            canApplyCtx.setAggFunction("_count_Of_" + aggregationFunction + "");
+            canApplyCtx.setAggFunction("_count_of_" + aggregationFunction + "");
           }
         }
         break;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/index/RewriteQueryUsingAggregateIndex.java
Patch:
@@ -84,7 +84,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,
 
       //we need to set the colList, outputColumnNames, colExprMap,
       // rowSchema for only that SelectOperator which precedes the GroupByOperator
-      // count(indexed_key_column) needs to be replaced by sum(`_count_Of_indexed_key_column`)
+      // count(indexed_key_column) needs to be replaced by sum(`_count_of_indexed_key_column`)
       if (childOp instanceof GroupByOperator){
         List<ExprNodeDesc> selColList =
           operator.getConf().getColList();
@@ -97,7 +97,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,
         RowSchema selRS = operator.getSchema();
         List<ColumnInfo> selRSSignature =
           selRS.getSignature();
-        //Need to create a new type for Column[_count_Of_indexed_key_column] node
+        //Need to create a new type for Column[_count_of_indexed_key_column] node
         PrimitiveTypeInfo pti = (PrimitiveTypeInfo) TypeInfoFactory.getPrimitiveTypeInfo("bigint");
         pti.setTypeName("bigint");
         ColumnInfo newCI = new ColumnInfo(rewriteQueryCtx.getAggregateFunction(), pti, "", false);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -1571,6 +1571,7 @@ private void analyzeLockTable(ASTNode ast)
 
     LockTableDesc lockTblDesc = new LockTableDesc(tableName, mode, partSpec,
                                                   HiveConf.getVar(conf, ConfVars.HIVEQUERYID));
+    lockTblDesc.setQueryStr(this.ctx.getCmd());
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
                                               lockTblDesc), conf));
 

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -948,6 +948,7 @@ public int checkCliDriverResults(String tname) throws Exception {
         "-I", "at junit",
         "-I", "Caused by:",
         "-I", "LOCK_QUERYID:",
+        "-I", "LOCK_TIME:",
         "-I", "grantTime",
         "-I", "[.][.][.] [0-9]* more",
         "-I", "job_[0-9]*_[0-9]*",

File: ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
Patch:
@@ -479,9 +479,8 @@ public void readFields(DataInput in) throws IOException {
         valBuf.reset();
         valBuf.write(in, vaRowsLen);
         if (codec != null) {
-          if (lazyDecompress) {
-            decompressedFlag[addIndex] = false;
-          } else {
+          decompressedFlag[addIndex] = false;
+          if (!lazyDecompress) {
             lazyDecompressCallbackObjs[addIndex].decompress();
             decompressedFlag[addIndex] = true;
           }

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveConnection.java
Patch:
@@ -95,7 +95,7 @@ public HiveConnection(String uri, Properties info) throws SQLException {
       try {
         transport.open();
       } catch (TTransportException e) {
-        throw new SQLException("Could not establish connecton to "
+        throw new SQLException("Could not establish connection to "
             + uri + ": " + e.getMessage(), "08S01");
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
Patch:
@@ -309,7 +309,8 @@ private Task<?> addSinglePartition(URI fromURI, FileSystem fs, CreateTableDesc t
               Warehouse.makePartPath(addPartitionDesc.getPartSpec()));
         }
       } else {
-        tgtPath = new Path(tblDesc.getLocation());
+        tgtPath = new Path(tblDesc.getLocation(),
+            Warehouse.makePartPath(addPartitionDesc.getPartSpec()));
       }
       checkTargetLocationEmpty(fs, tgtPath);
       addPartitionDesc.setLocation(tgtPath.toString());

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
Patch:
@@ -193,7 +193,6 @@ public void alterTable(RawStore msdb, Warehouse wh, String dbname,
     } finally {
       if (!success) {
         msdb.rollbackTransaction();
-        throw new MetaException("Committing the alter table transaction was not successful.");
       }
       if (success && moveData) {
         // change the file name in hdfs
@@ -220,6 +219,9 @@ public void alterTable(RawStore msdb, Warehouse wh, String dbname,
         }
       }
     }
+    if (!success) {
+      throw new MetaException("Committing the alter table transaction was not successful.");
+    }
   }
 
   private boolean checkPartialPartKeysEqual(List<FieldSchema> oldPartKeys,

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
Patch:
@@ -151,8 +151,8 @@ public enum ErrorMsg {
       + "Please check your hive.input.format setting and make sure your Hadoop version support "
       + "CombineFileInputFormat"),
   NONEXISTPARTCOL("Non-Partition column appears in the partition specification: "),
-  UNSUPPORTED_TYPE("DATE, DATETIME, and TIMESTAMP types aren't supported yet. Please use "
-      + "STRING instead"),
+  UNSUPPORTED_TYPE("DATE and DATETIME types aren't supported yet. Please use "
+      + "TIMESTAMP instead"),
   CREATE_NON_NATIVE_AS("CREATE TABLE AS SELECT cannot be used for a non-native table"),
   LOAD_INTO_NON_NATIVE("A non-native table cannot be used as target for LOAD"),
   LOCKMGR_NOT_SPECIFIED("Lock manager not specified correctly, set hive.lock.manager"),

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -458,6 +458,8 @@ public static class DefaultExprProcessor implements NodeProcessor {
           Constants.DOUBLE_TYPE_NAME);
       conversionFunctionTextHashMap.put(HiveParser.TOK_STRING,
           Constants.STRING_TYPE_NAME);
+      conversionFunctionTextHashMap.put(HiveParser.TOK_TIMESTAMP,
+          Constants.TIMESTAMP_TYPE_NAME);
     }
 
     public static boolean isRedundantConversionFunction(ASTNode expr,

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java
Patch:
@@ -70,6 +70,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case FLOAT:
     case DOUBLE:
     case STRING:
+    case TIMESTAMP:
       return new GenericUDAFAverageEvaluator();
     case BOOLEAN:
     default:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFContextNGrams.java
Patch:
@@ -132,6 +132,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case SHORT:
     case INT:
     case LONG:
+    case TIMESTAMP:
       break;
 
     default:
@@ -151,6 +152,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
       case SHORT:
       case INT:
       case LONG:
+      case TIMESTAMP:
         break;
 
       default:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCorrelation.java
Patch:
@@ -101,13 +101,15 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case LONG:
     case FLOAT:
     case DOUBLE:
+    case TIMESTAMP:
       switch (((PrimitiveTypeInfo) parameters[1]).getPrimitiveCategory()) {
       case BYTE:
       case SHORT:
       case INT:
       case LONG:
       case FLOAT:
       case DOUBLE:
+      case TIMESTAMP:
         return new GenericUDAFCorrelationEvaluator();
       case STRING:
       case BOOLEAN:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCovariance.java
Patch:
@@ -92,13 +92,15 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case LONG:
     case FLOAT:
     case DOUBLE:
+    case TIMESTAMP:
       switch (((PrimitiveTypeInfo) parameters[1]).getPrimitiveCategory()) {
       case BYTE:
       case SHORT:
       case INT:
       case LONG:
       case FLOAT:
       case DOUBLE:
+      case TIMESTAMP:
         return new GenericUDAFCovarianceEvaluator();
       case STRING:
       case BOOLEAN:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCovarianceSample.java
Patch:
@@ -66,13 +66,15 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case LONG:
     case FLOAT:
     case DOUBLE:
+    case TIMESTAMP:
       switch (((PrimitiveTypeInfo) parameters[1]).getPrimitiveCategory()) {
       case BYTE:
       case SHORT:
       case INT:
       case LONG:
       case FLOAT:
       case DOUBLE:
+      case TIMESTAMP:
         return new GenericUDAFCovarianceSampleEvaluator();
       case STRING:
       case BOOLEAN:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFHistogramNumeric.java
Patch:
@@ -86,6 +86,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case LONG:
     case FLOAT:
     case DOUBLE:
+    case TIMESTAMP:
       break;
     case STRING:
     case BOOLEAN:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFPercentileApprox.java
Patch:
@@ -91,6 +91,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case LONG:
     case FLOAT:
     case DOUBLE:
+    case TIMESTAMP:
       break;
     default:
       throw new UDFArgumentTypeException(0,
@@ -155,6 +156,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
       case SHORT:
       case INT:
       case LONG:
+      case TIMESTAMP:
         break;
       default:
         throw new UDFArgumentTypeException(2, "Only an integer argument is accepted as "

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFStd.java
Patch:
@@ -55,6 +55,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case FLOAT:
     case DOUBLE:
     case STRING:
+    case TIMESTAMP:
       return new GenericUDAFStdEvaluator();
     case BOOLEAN:
     default:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFStdSample.java
Patch:
@@ -54,6 +54,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case FLOAT:
     case DOUBLE:
     case STRING:
+    case TIMESTAMP:
       return new GenericUDAFStdSampleEvaluator();
     case BOOLEAN:
     default:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java
Patch:
@@ -60,6 +60,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case SHORT:
     case INT:
     case LONG:
+    case TIMESTAMP:
       return new GenericUDAFSumLong();
     case FLOAT:
     case DOUBLE:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFVariance.java
Patch:
@@ -71,6 +71,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case FLOAT:
     case DOUBLE:
     case STRING:
+    case TIMESTAMP:
       return new GenericUDAFVarianceEvaluator();
     case BOOLEAN:
     default:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFVarianceSample.java
Patch:
@@ -55,6 +55,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case FLOAT:
     case DOUBLE:
     case STRING:
+    case TIMESTAMP:
       return new GenericUDAFVarianceSampleEvaluator();
     case BOOLEAN:
     default:

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFnGrams.java
Patch:
@@ -113,6 +113,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case SHORT:
     case INT:
     case LONG:
+    case TIMESTAMP:
       break;
 
     default:
@@ -130,6 +131,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
     case SHORT:
     case INT:
     case LONG:
+    case TIMESTAMP:
       break;
 
     default:
@@ -148,6 +150,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticE
       case SHORT:
       case INT:
       case LONG:
+      case TIMESTAMP:
         break;
 
       default:

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyShortObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector;
+import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyTimestampObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
@@ -75,6 +76,8 @@ public final class LazyFactory {
       return new LazyDouble((LazyDoubleObjectInspector) oi);
     case STRING:
       return new LazyString((LazyStringObjectInspector) oi);
+    case TIMESTAMP:
+      return new LazyTimestamp((LazyTimestampObjectInspector) oi);
     default:
       throw new RuntimeException("Internal error: no LazyObject for " + p);
     }

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryFactory.java
Patch:
@@ -35,6 +35,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableVoidObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 
@@ -69,6 +70,8 @@ public final class LazyBinaryFactory {
       return new LazyBinaryString((WritableStringObjectInspector) oi);
     case VOID: // for NULL
       return new LazyBinaryVoid((WritableVoidObjectInspector) oi);
+    case TIMESTAMP:
+      return new LazyBinaryTimestamp((WritableTimestampObjectInspector) oi);
     default:
       throw new RuntimeException("Internal error: no LazyBinaryObject for " + p);
     }

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/PrimitiveObjectInspector.java
Patch:
@@ -27,7 +27,7 @@ public interface PrimitiveObjectInspector extends ObjectInspector {
    * The primitive types supported by Hive.
    */
   public static enum PrimitiveCategory {
-    VOID, BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, STRING, UNKNOWN
+    VOID, BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, STRING, TIMESTAMP, UNKNOWN
   };
 
   /**
@@ -61,7 +61,7 @@ public static enum PrimitiveCategory {
   /**
    * Get a copy of the Object in the same class, so the return value can be
    * stored independently of the parameter.
-   * 
+   *
    * If the Object is a Primitive Java Object, we just return the parameter
    * since Primitive Java Object is immutable.
    */

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java
Patch:
@@ -27,7 +27,7 @@
 
 /**
  * TypeInfoFactory can be used to create the TypeInfo object for any types.
- * 
+ *
  * TypeInfo objects are all read-only so we can reuse them easily.
  * TypeInfoFactory has internal cache to make sure we don't create 2 TypeInfo
  * objects that represents the same type.
@@ -62,6 +62,7 @@ public static TypeInfo getPrimitiveTypeInfo(String typeName) {
   public static final TypeInfo doubleTypeInfo = getPrimitiveTypeInfo(Constants.DOUBLE_TYPE_NAME);
   public static final TypeInfo byteTypeInfo = getPrimitiveTypeInfo(Constants.TINYINT_TYPE_NAME);
   public static final TypeInfo shortTypeInfo = getPrimitiveTypeInfo(Constants.SMALLINT_TYPE_NAME);
+  public static final TypeInfo timestampTypeInfo = getPrimitiveTypeInfo(Constants.TIMESTAMP_TYPE_NAME);
 
   public static final TypeInfo unknownTypeInfo = getPrimitiveTypeInfo("unknown");
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java
Patch:
@@ -267,7 +267,7 @@ public int execute(DriverContext driverContext) {
       outPrinter.start();
       errPrinter.start();
 
-      int exitVal = executor.waitFor();
+      int exitVal = jobExecHelper.progressLocal(executor, getId());
 
       if (exitVal != 0) {
         LOG.error("Execution failed with exit status: " + exitVal);

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -950,6 +950,7 @@ public int checkCliDriverResults(String tname) throws Exception {
         "-I", "LOCK_QUERYID:",
         "-I", "grantTime",
         "-I", "[.][.][.] [0-9]* more",
+        "-I", "job_[0-9]*_[0-9]*",
         "-I", "USING 'java -cp",
         (new File(logDir, tname + ".out")).getPath(),
         outFileName };

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -1109,9 +1109,10 @@ public static void renameOrMoveFiles(FileSystem fs, Path src, Path dst) throws I
   /**
    * The first group will contain the task id. The second group is the optional extension. The file
    * name looks like: "0_0" or "0_0.gz". There may be a leading prefix (tmp_). Since getTaskId() can
-   * return an integer only - this should match a pure integer as well
+   * return an integer only - this should match a pure integer as well. {1,3} is used to limit
+   * matching for attempts #'s 0-999.
    */
-  private static Pattern fileNameTaskIdRegex = Pattern.compile("^.*?([0-9]+)(_[0-9])?(\\..*)?$");
+  private static Pattern fileNameTaskIdRegex = Pattern.compile("^.*?([0-9]+)(_[0-9]{1,3})?(\\..*)?$");
 
   /**
    * Get the task id from the filename. It is assumed that the filename is derived from the output

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -938,6 +938,7 @@ public int checkCliDriverResults(String tname) throws Exception {
         "-I", "CreateTime",
         "-I", "LastAccessTime",
         "-I", "Location",
+        "-I", "LOCATION '",
         "-I", "transient_lastDdlTime",
         "-I", "last_modified_",
         "-I", "java.lang.RuntimeException",

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -1363,7 +1363,7 @@ private int unarchive(Hive db, AlterTableSimpleDesc simpleDesc)
       try {
 
         // Copy the files out of the archive into the temporary directory
-        String copySource = (new Path(sourceDir, "*")).toString();
+        String copySource = sourceDir.toString();
         String copyDest = tmpDir.toString();
         List<String> args = new ArrayList<String>();
         args.add("-cp");

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -353,7 +353,7 @@ public static enum ConfVars {
 
     HIVEJOBPROGRESS("hive.task.progress", false),
 
-    HIVEINPUTFORMAT("hive.input.format", ""),
+    HIVEINPUTFORMAT("hive.input.format", "org.apache.hadoop.hive.ql.io.CombineHiveInputFormat"),
 
     HIVEENFORCEBUCKETING("hive.enforce.bucketing", false),
     HIVEENFORCESORTING("hive.enforce.sorting", false),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java
Patch:
@@ -74,7 +74,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     // Get outputFieldOIs
     columnTypeResolvers = new ReturnObjectInspectorResolver[columns];
     for (int c = 0; c < columns; c++) {
-      columnTypeResolvers[c] = new ReturnObjectInspectorResolver();
+      columnTypeResolvers[c] = new ReturnObjectInspectorResolver(true);
     }
 
     for (int p = 0; p < parents; p++) {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCoalesce.java
Patch:
@@ -42,7 +42,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen
 
     argumentOIs = arguments;
 
-    returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver();
+    returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);
     for (int i = 0; i < arguments.length; i++) {
       if (!returnOIResolver.update(arguments[i])) {
         throw new UDFArgumentTypeException(i,

File: ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java
Patch:
@@ -212,7 +212,7 @@ public int execute(DriverContext driverContext) {
           }
           HadoopJobExecHelper.runningJobKillURIs.remove(rj.getJobID());
         }
-        RCFileMergeMapper.jobClose(outputPath, noName, job, console);
+        RCFileMergeMapper.jobClose(outputPath, success, job, console);
       } catch (Exception e) {
       }
     }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -395,6 +395,8 @@ public static enum ConfVars {
         0),     // maximum # of retries to insert/select/delete the stats DB
     HIVE_STATS_RETRIES_WAIT("hive.stats.retries.wait",
         3000),  // # milliseconds to wait before the next retry
+    HIVE_STATS_COLLECT_RAWDATASIZE("hive.stats.collect.rawdatasize", true),
+    // should the raw data size be collected when analayzing tables
 
 
     // Concurrency

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -372,6 +372,8 @@ public static enum ConfVars {
     // Indexes
     HIVEOPTINDEXFILTER_COMPACT_MINSIZE("hive.optimize.index.filter.compact.minsize", (long) 5 * 1024 * 1024 * 1024), // 5G
     HIVEOPTINDEXFILTER_COMPACT_MAXSIZE("hive.optimize.index.filter.compact.maxsize", (long) -1), // infinity
+    HIVE_INDEX_COMPACT_QUERY_MAX_ENTRIES("hive.index.compact.query.max.entries", (long) 10000000), // 10M
+    HIVE_INDEX_COMPACT_QUERY_MAX_SIZE("hive.index.compact.query.max.size", (long) 10 * 1024 * 1024 * 1024), // 10G
 
     // Statistics
     HIVESTATSAUTOGATHER("hive.stats.autogather", true),

File: ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
Patch:
@@ -136,8 +136,8 @@ protected void analyzeRowFormat(AnalyzeCreateCommonVars shared, ASTNode child) t
               .getText());
           if (!lineDelim.equals("\n")
               && !lineDelim.equals("10")) {
-            throw new SemanticException(
-                ErrorMsg.LINES_TERMINATED_BY_NON_NEWLINE.getMsg());
+            throw new SemanticException(SemanticAnalyzer.generateErrorMessage(rowChild,
+                ErrorMsg.LINES_TERMINATED_BY_NON_NEWLINE.getMsg()));
           }
           break;
         default:

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverSkewJoin.java
Patch:
@@ -94,7 +94,7 @@ public List<Task<? extends Serializable>> getTasks(HiveConf conf,
         Path dirPath = new Path(path);
         FileSystem inpFs = dirPath.getFileSystem(conf);
         FileStatus[] fstatus = inpFs.listStatus(dirPath);
-        if (fstatus.length > 0) {
+        if (fstatus != null && fstatus.length > 0) {
           Task <? extends Serializable> task = entry.getValue();
           List<Task <? extends Serializable>> parentOps = task.getParentTasks();
           if(parentOps!=null){

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -213,7 +213,8 @@ public static enum ConfVars {
     METASTORE_CACHE_LEVEL2_TYPE("datanucleus.cache.level2.type", "SOFT"),
     METASTORE_IDENTIFIER_FACTORY("datanucleus.identifierFactory", "datanucleus"),
     METASTORE_PLUGIN_REGISTRY_BUNDLE_CHECK("datanucleus.plugin.pluginRegistryBundleCheck", "LOG"),
-      
+    METASTORE_BATCH_RETRIEVE_MAX("hive.metastore.batch.retrieve.max", 300),
+
 
     // Default parameters for creating tables
     NEWTABLEDEFAULTPARA("hive.table.parameters.default",""),

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HivePreparedStatement.java
Patch:
@@ -159,7 +159,6 @@ protected ResultSet executeImmediate(String sql) throws SQLException {
     }
 
     try {
-      clearWarnings();
       resultSet = null;
       client.execute(sql);
     } catch (HiveServerException e) {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -150,6 +150,7 @@ public static enum ConfVars {
     METASTOREPWD("javax.jdo.option.ConnectionPassword", ""),
     // Class name of JDO connection url hook
     METASTORECONNECTURLHOOK("hive.metastore.ds.connection.url.hook", ""),
+    METASTOREMULTITHREADED("javax.jdo.option.Multithreaded", "true"),
     // Name of the connection url in the configuration
     METASTORECONNECTURLKEY("javax.jdo.option.ConnectionURL", ""),
     // Number of attempts to retry connecting after there is a JDO datastore err

File: contrib/src/java/org/apache/hadoop/hive/contrib/udf/UDFRowSequence.java
Patch:
@@ -28,7 +28,7 @@
  */
 @Description(name = "row_sequence",
     value = "_FUNC_() - Returns a generated row sequence number starting from 1")
-@UDFType(deterministic = false)
+@UDFType(deterministic = false, stateful = true)
 public class UDFRowSequence extends UDF
 {
   private LongWritable result = new LongWritable();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
Patch:
@@ -131,6 +131,7 @@ public enum ErrorMsg {
   UDTF_LATERAL_VIEW("UDTF's cannot be in a select expression when there is a lateral view"),
   UDTF_ALIAS_MISMATCH("The number of aliases supplied in the AS clause does not match the "
       + "number of columns output by the UDTF"),
+  UDF_STATEFUL_INVALID_LOCATION("Stateful UDF's can only be invoked in the SELECT list"),
   LATERAL_VIEW_WITH_JOIN("Join with a lateral view is not supported"),
   LATERAL_VIEW_INVALID_CHILD("Lateral view AST with invalid child"),
   OUTPUT_SPECIFIED_MULTIPLE_TIMES("The same output cannot be present multiple times: "),

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFType.java
Patch:
@@ -32,4 +32,5 @@
 @Inherited
 public @interface UDFType {
   boolean deterministic() default true;
+  boolean stateful() default false;
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -962,7 +962,7 @@ public Object dispatch(Node nd, java.util.Stack<Node> stack,
 
   private boolean isPresent(String[] list, String elem) {
     for (String s : list) {
-      if (s.equals(elem)) {
+      if (s.toLowerCase().equals(elem)) {
         return true;
       }
     }
@@ -4518,7 +4518,7 @@ private Operator genJoinOperator(QB qb, QBJoinTree joinTree,
     int pos = 0;
     for (String src : joinTree.getBaseSrc()) {
       if (src != null) {
-        Operator srcOp = map.get(src);
+        Operator srcOp = map.get(src.toLowerCase());
 
         // for left-semi join, generate an additional selection & group-by
         // operator before ReduceSink

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
Patch:
@@ -143,9 +143,9 @@ public enum ErrorMsg {
       + "hive.exec.dynamic.partition=true or specify partition column values"),
   DYNAMIC_PARTITION_STRICT_MODE("Dynamic partition strict mode requires at least one "
       + "static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict"),
-  DYNAMIC_PARTITION_MERGE("Dynamic partition does not support merging mapfiles/mapredfiles yet."
-      + "Please set hive.merge.mapfiles and hive.merge.mapredfiles to false or use static "
-      +	"partitions"),
+  DYNAMIC_PARTITION_MERGE("Dynamic partition does not support merging using non-CombineHiveInputFormat."
+      + "Please check your hive.input.format setting and make sure your Hadoop version support "
+      + "CombineFileInputFormat."),
   NONEXISTPARTCOL("Non-Partition column appears in the partition specification: "),
   UNSUPPORTED_TYPE("DATE, DATETIME, and TIMESTAMP types aren't supported yet. Please use "
       + "STRING instead."),

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -362,7 +362,7 @@ public static enum ConfVars {
     HIVE_AUTHORIZATION_TABLE_USER_GRANTS("hive.security.authorization.createtable.user.grants", null),
     HIVE_AUTHORIZATION_TABLE_GROUP_GRANTS("hive.security.authorization.createtable.group.grants", null),
     HIVE_AUTHORIZATION_TABLE_ROLE_GRANTS("hive.security.authorization.createtable.role.grants", null),
-    HIVE_AUTHORIZATION_TABLE_OWNER_GRANTS("hive.security.authorization.createtable.owner.grants", "All"),
+    HIVE_AUTHORIZATION_TABLE_OWNER_GRANTS("hive.security.authorization.createtable.owner.grants", null),
     // Print column names in output
     HIVE_CLI_PRINT_HEADER("hive.cli.print.header", false),
 

File: hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java
Patch:
@@ -345,7 +345,8 @@ static IndexPredicateAnalyzer newIndexPredicateAnalyzer(
     IndexPredicateAnalyzer analyzer = new IndexPredicateAnalyzer();
 
     // for now, we only support equality comparisons
-    analyzer.addComparisonOp("=");
+    analyzer.addComparisonOp(
+      "org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual");
 
     // and only on the key column
     analyzer.clearAllowedColumnNames();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java
Patch:
@@ -398,8 +398,7 @@ public List<ASTNode> getLateralViewsForAlias(String alias) {
   }
 
   public void addLateralViewForAlias(String alias, ASTNode lateralView) {
-    String lowerAlias = alias.toLowerCase();
-    ArrayList<ASTNode> lateralViews = aliasToLateralViews.get(lowerAlias);
+    ArrayList<ASTNode> lateralViews = aliasToLateralViews.get(alias);
     if (lateralViews == null) {
       lateralViews = new ArrayList<ASTNode>();
       aliasToLateralViews.put(alias, lateralViews);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -548,6 +548,7 @@ private String processLateralView(QB qb, ASTNode lateralView)
       throw new SemanticException(ErrorMsg.LATERAL_VIEW_INVALID_CHILD
           .getMsg(lateralView));
     }
+    alias = alias.toLowerCase();
     qb.getParseInfo().addLateralViewForAlias(alias, lateralView);
     qb.addAlias(alias);
     return alias;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -1730,6 +1730,9 @@ static List<ASTNode> getGroupByForClause(QBParseInfo parseInfo, String dest) {
           : selectExprs.getChildCount());
       if (selectExprs != null) {
         for (int i = 0; i < selectExprs.getChildCount(); ++i) {
+          if (((ASTNode) selectExprs.getChild(i)).getToken().getType() == HiveParser.TOK_HINTLIST) {
+            continue;
+          }
           // table.column AS alias
           ASTNode grpbyExpr = (ASTNode) selectExprs.getChild(i).getChild(0);
           result.add(grpbyExpr);

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -897,7 +897,7 @@ public int checkCliDriverResults(String tname) throws Exception {
         "-I", "at java",
         "-I", "at junit",
         "-I", "Caused by:",
-        "-I", "QUERYID_LOCK:",
+        "-I", "LOCK_QUERYID:",
         "-I", "[.][.][.] [0-9]* more",
         (new File(logDir, tname + ".out")).getPath(),
         outFileName };

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -256,8 +256,8 @@ public static enum ConfVars {
     HIVEMAXMAPJOINSIZE("hive.mapjoin.maxsize", 100000),
     HIVEHASHTABLETHRESHOLD("hive.hashtable.initialCapacity", 100000),
     HIVEHASHTABLELOADFACTOR("hive.hashtable.loadfactor", (float) 0.75),
-    HIVEHASHTABLEMAXMEMORYUSAGE("hive.hashtable.max.memory.usage", (float) 0.90),
-    HIVEHASHTABLESCALE("hive.hashtable.scale", (long)100000),
+    HIVEHASHTABLEMAXMEMORYUSAGE("hive.mapjoin.localtask.max.memory.usage", (float) 0.90),
+    HIVEHASHTABLESCALE("hive.mapjoin.check.memory.rows", (long)100000),
 
     HIVEDEBUGLOCALTASK("hive.debug.localtask",false),
 

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -837,10 +837,10 @@ public int execute() {
           if (backupTask != null) {
             errorMessage = "FAILED: Execution Error, return code " + exitVal + " from "
                 + tsk.getClass().getName();
-            console.printInfo(errorMessage);
+            console.printError(errorMessage);
 
             errorMessage = "ATTEMPT: Execute BackupTask: " + backupTask.getClass().getName();
-            console.printInfo(errorMessage);
+            console.printError(errorMessage);
 
             // add backup task to runnable
             if (DriverContext.isLaunchable(backupTask)) {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPAnd.java
Patch:
@@ -49,7 +49,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments)
   @Override
   public Object evaluate(DeferredObject[] arguments) throws HiveException {
     boolean bool_a0 = false, bool_a1 = false;
-    BooleanWritable a0 = (BooleanWritable) arguments[0].get();
+    Object a0 = arguments[0].get();
     if (a0 != null) {
       bool_a0 = boi0.get(a0);
       if (bool_a0 == false) {
@@ -58,7 +58,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
       }
     }
 
-    BooleanWritable a1 = (BooleanWritable) arguments[1].get();
+    Object a1 = arguments[1].get();
     if (a1 != null) {
       bool_a1 = boi1.get(a1);
       if (bool_a1 == false) {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPOr.java
Patch:
@@ -50,7 +50,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments)
   @Override
   public Object evaluate(DeferredObject[] arguments) throws HiveException {
     boolean bool_a0 = false, bool_a1 = false;
-    BooleanWritable a0 = (BooleanWritable) arguments[0].get();
+    Object a0 = arguments[0].get();
     if (a0 != null) {
       bool_a0 = boi0.get(a0);
       if (bool_a0 == true) {
@@ -59,7 +59,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
       }
     }
 
-    BooleanWritable a1 = (BooleanWritable) arguments[1].get();
+    Object a1 = arguments[1].get();
     if (a1 != null) {
       bool_a1 = boi1.get(a1);
       if (bool_a1 == true) {

File: ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
Patch:
@@ -161,7 +161,8 @@ public ZooKeeperHiveLock lock(HiveLockObject key, HiveLockMode mode, boolean kee
 
         if ((childSeq >= 0) && (childSeq < seqNo)) {
           zooKeeper.delete(res, -1);
-          console.printError("conflicting lock present ");
+          console.printError("conflicting lock present for " + key.getName() +
+                             " mode " + mode);
           return null;
         }
       }

File: ql/src/java/org/apache/hadoop/hive/ql/Context.java
Patch:
@@ -143,6 +143,7 @@ private String getScratchDir(String scheme, String authority,
       if (mkdir) {
         try {
           FileSystem fs = dirPath.getFileSystem(conf);
+          dirPath = new Path(fs.makeQualified(dirPath).toString());
           if (!fs.mkdirs(dirPath))
             throw new RuntimeException("Cannot make directory: "
                                        + dirPath.toString());

File: hwi/src/test/org/apache/hadoop/hive/hwi/TestHWISessionManager.java
Patch:
@@ -121,10 +121,10 @@ public final void testHiveDriver() throws Exception {
 
     ArrayList<ArrayList<String>> searchBlockRes = searchItem.getResultBucket();
 
-    String resLine = searchBlockRes.get(0).get(2);
+    String resLine = searchBlockRes.get(0).get(0);
     assertEquals(true, resLine.contains("key"));
     assertEquals(true, resLine.contains("int"));
-    String resLine2 = searchBlockRes.get(0).get(3);
+    String resLine2 = searchBlockRes.get(0).get(1);
     assertEquals(true, resLine2.contains("value"));
     assertEquals(true, resLine2.contains("string"));
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -175,6 +175,7 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFSize;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFSplit;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFStruct;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnion;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFWhen;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode;
@@ -391,6 +392,7 @@ public final class FunctionRegistry {
     registerGenericUDF("array", GenericUDFArray.class);
     registerGenericUDF("map", GenericUDFMap.class);
     registerGenericUDF("struct", GenericUDFStruct.class);
+    registerGenericUDF("create_union", GenericUDFUnion.class);
 
     registerGenericUDF("case", GenericUDFCase.class);
     registerGenericUDF("when", GenericUDFWhen.class);

File: serde/src/gen-java/org/apache/hadoop/hive/serde/Constants.java
Patch:
@@ -74,6 +74,8 @@ public class Constants {
 
   public static final String STRUCT_TYPE_NAME = "struct";
 
+  public static final String UNION_TYPE_NAME = "uniontype";
+
   public static final String LIST_COLUMNS = "columns";
 
   public static final String LIST_COLUMN_TYPES = "columns.types";

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspector.java
Patch:
@@ -42,7 +42,7 @@ public interface ObjectInspector extends Cloneable {
    *
    */
   public static enum Category {
-    PRIMITIVE, LIST, MAP, STRUCT
+    PRIMITIVE, LIST, MAP, STRUCT, UNION
   };
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -186,7 +186,7 @@ public static void initMapJoinPlan(Operator<? extends Serializable> op,
         TableDesc tt_desc;
         Operator<? extends Serializable> rootOp;
 
-        if (mjCtx.getOldMapJoin() == null) {
+        if (mjCtx.getOldMapJoin() == null || setReducer) {
           taskTmpDir = mjCtx.getTaskTmpDir();
           tt_desc = mjCtx.getTTDesc();
           rootOp = mjCtx.getRootMapJoinOp();

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -151,6 +151,9 @@ public static enum ConfVars {
     METASTORE_INT_EXTRACTED("hive.metastore.archive.intermediate.extracted",
         "_INTERMEDIATE_EXTRACTED"),
 
+    // Default parameters for creating tables
+    NEWTABLEDEFAULTPARA("hive.table.parameters.default",""),
+
     // CLI
     CLIIGNOREERRORS("hive.cli.errors.ignore", false),
 

File: contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/base64/Base64TextOutputFormat.java
Patch:
@@ -101,7 +101,7 @@ public void close(boolean abort) throws IOException {
     }
 
     private byte[] signature;
-    private final Base64 base64 = new Base64();
+    private final Base64 base64 = Base64TextInputFormat.createBase64();
 
     @Override
     public void configure(JobConf job) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
Patch:
@@ -1389,8 +1389,7 @@ public static void copyTableJobPropertiesToConf(TableDesc tbl, JobConf job) {
     return new ContentSummary(summary[0], summary[1], summary[2]);
   }
 
-  public static boolean isEmptyPath(JobConf job, String path) throws Exception {
-    Path dirPath = new Path(path);
+  public static boolean isEmptyPath(JobConf job, Path dirPath) throws Exception {
     FileSystem inpFs = dirPath.getFileSystem(job);
 
     if (inpFs.exists(dirPath)) {

File: serde/src/java/org/apache/hadoop/hive/serde2/io/DoubleWritable.java
Patch:
@@ -74,7 +74,8 @@ public boolean equals(Object o) {
 
   @Override
   public int hashCode() {
-    return (int) Double.doubleToLongBits(value);
+    long v = Double.doubleToLongBits(value);
+    return (int) (v ^ (v >>> 32));
   }
 
   public int compareTo(Object o) {

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -53,6 +53,7 @@
 import org.apache.hadoop.hive.ql.history.HiveHistory.Keys;
 import org.apache.hadoop.hive.ql.hooks.PostExecute;
 import org.apache.hadoop.hive.ql.hooks.PreExecute;
+import org.apache.hadoop.hive.ql.io.IOPrepareCache;
 import org.apache.hadoop.hive.ql.parse.ASTNode;
 import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;
 import org.apache.hadoop.hive.ql.parse.ErrorMsg;

File: ql/src/java/org/apache/hadoop/hive/ql/DriverContext.java
Patch:
@@ -22,7 +22,6 @@
 import java.util.LinkedList;
 import java.util.Queue;
 
-import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.mapred.JobConf;
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -240,6 +240,7 @@ public static enum ConfVars {
     HIVESKEWJOINMAPJOINNUMMAPTASK("hive.skewjoin.mapjoin.map.tasks", 10000),
     HIVESKEWJOINMAPJOINMINSPLIT("hive.skewjoin.mapjoin.min.split", 33554432), //32M
     MAPREDMINSPLITSIZE("mapred.min.split.size", 1),
+    HIVEMERGEMAPONLY("hive.mergejob.maponly", true),
 
     HIVESENDHEARTBEAT("hive.heartbeat.interval", 1000),
     HIVEMAXMAPJOINSIZE("hive.mapjoin.maxsize", 100000),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
Patch:
@@ -42,7 +42,6 @@
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
@@ -287,7 +286,7 @@ public void setChildren(Configuration hconf) throws HiveException {
         MapOpCtx opCtx = initObjectInspector(conf, hconf, onefile);
         Path onepath = new Path(new Path(onefile).toUri().getPath());
         List<String> aliases = conf.getPathToAliases().get(onefile);
-        
+
         for (String onealias : aliases) {
           Operator<? extends Serializable> op = conf.getAliasToWork().get(
               onealias);
@@ -346,7 +345,7 @@ public void setChildren(Configuration hconf) throws HiveException {
                     if(partObjectInspector == null) {
                       this.rowObjectInspector = ObjectInspectorFactory.getUnionStructObjectInspector(Arrays
                           .asList(new StructObjectInspector[] {
-                              rowObjectInspector, vcStructObjectInspector }));  
+                              rowObjectInspector, vcStructObjectInspector }));
                     } else {
                       this.rowObjectInspector = ObjectInspectorFactory.getUnionStructObjectInspector(Arrays
                           .asList(new StructObjectInspector[] {

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -1210,7 +1210,7 @@ static protected void replaceFiles(Path srcf, Path destf, FileSystem fs,
       Path tmppath) throws HiveException {
     FileStatus[] srcs;
     try {
-      srcs = fs.globStatus(srcf);
+      srcs = fs.listStatus(srcf);
     } catch (IOException e) {
       throw new HiveException("addFiles: filesystem error in check phase", e);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/index/compact/CompactIndexHandler.java
Patch:
@@ -163,8 +163,6 @@ private Task<?> getIndexBuilderMapRedTask(List<FieldSchema> indexField, boolean
     }
     command.append(" GROUP BY ");
     command.append(indexCols + ", " + VirtualColumn.FILENAME.getName());
-    command.append(" SORT BY ");
-    command.append(indexCols);
 
     Driver driver = new Driver(db.getConf());
     driver.compile(command.toString());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -137,6 +137,7 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFResolver;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStdSample;
@@ -360,6 +361,7 @@ public final class FunctionRegistry {
     registerGenericUDAF("var_samp", new GenericUDAFVarianceSample());
 
     registerGenericUDAF("histogram_numeric", new GenericUDAFHistogramNumeric());
+    registerGenericUDAF("percentile_approx", new GenericUDAFPercentileApprox());
 
     registerUDAF("percentile", UDAFPercentile.class);
 

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/PrimitiveTypeInfo.java
Patch:
@@ -31,11 +31,11 @@
  * Always use the TypeInfoFactory to create new TypeInfo objects, instead of
  * directly creating an instance of this class.
  */
-public class PrimitiveTypeInfo extends TypeInfo implements Serializable {
+public final class PrimitiveTypeInfo extends TypeInfo implements Serializable {
 
   private static final long serialVersionUID = 1L;
 
-  String typeName;
+  private String typeName;
 
   /**
    * For java serialization use only.

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java
Patch:
@@ -179,7 +179,9 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,
       RowResolver inputRR = cppCtx.getOpToParseCtxMap().get(scanOp).getRR();
       for (int i = 0; i < cols.size(); i++) {
         int position = inputRR.getPosition(cols.get(i));
-        needed_columns.add(position);
+        if (position >=0) {
+          needed_columns.add(position);          
+        }
       }
       scanOp.setNeededColumnIDs(needed_columns);
       return null;

File: contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/Type.java
Patch:
@@ -25,7 +25,7 @@ public enum Type {
 
   // codes for supported types (< 50):
   BYTES(0), BYTE(1), BOOL(2), INT(3), LONG(4), FLOAT(5), DOUBLE(6), STRING(7), VECTOR(
-      8), LIST(9), MAP(10), SHORT(11),
+      8), LIST(9), MAP(10), SHORT(11), NULL(12),
 
   // application-specific codes (50-200):
   WRITABLE(50),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -316,7 +316,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {
 
       	// Create all the files - this is required because empty files need to be created for
       	// empty buckets
-      	createBucketFiles(fsp);
+      	// createBucketFiles(fsp);
       	valToPaths.put("", fsp); // special entry for non-DP case
       }
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java
Patch:
@@ -252,7 +252,7 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
 
       // Since there is no easy way of knowing whether MAPREDUCE-1597 is present in the tree or not,
       // we use a configuration variable for the same
-      if (this.mrwork != null && this.mrwork.getHadoopSupportsSplittable()) {
+      if (this.mrwork != null && !this.mrwork.getHadoopSupportsSplittable()) {
         // The following code should be removed, once
         // https://issues.apache.org/jira/browse/MAPREDUCE-1597 is fixed.
         // Hadoop does not handle non-splittable files correctly for CombineFileInputFormat,

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateAdd.java
Patch:
@@ -22,7 +22,6 @@
 import java.text.SimpleDateFormat;
 import java.util.Calendar;
 import java.util.Date;
-import java.util.TimeZone;
 
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDF;
@@ -43,7 +42,7 @@
     + "  '2009-31-07'")
 public class UDFDateAdd extends UDF {
   private final SimpleDateFormat formatter = new SimpleDateFormat("yyyy-MM-dd");
-  private final Calendar calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"));
+  private final Calendar calendar = Calendar.getInstance();
 
   private Text result = new Text();
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateSub.java
Patch:
@@ -22,7 +22,6 @@
 import java.text.SimpleDateFormat;
 import java.util.Calendar;
 import java.util.Date;
-import java.util.TimeZone;
 
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDF;
@@ -43,8 +42,7 @@
     + "  '2009-29-07'")
 public class UDFDateSub extends UDF {
   private final SimpleDateFormat formatter = new SimpleDateFormat("yyyy-MM-dd");
-  private final Calendar calendar = Calendar.getInstance(TimeZone
-      .getTimeZone("UTC"));
+  private final Calendar calendar = Calendar.getInstance();
 
   private Text result = new Text();
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
Patch:
@@ -1281,7 +1281,7 @@ private void validateSerDe(String serdeName) throws HiveException {
     try {
       Deserializer d = SerDeUtils.lookupDeserializer(serdeName);
       if (d != null) {
-        System.out.println("Found class for " + serdeName);
+        LOG.debug("Found class for " + serdeName);
       }
     } catch (SerDeException e) {
       throw new HiveException("Cannot validate serde: " + serdeName, e);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
Patch:
@@ -320,6 +320,7 @@ public void close() {
     // detecting failed executions by exceptions thrown by the operator tree
     // ideally hadoop should let us know whether map execution failed or not
     try {
+      inputFileChanged();
       mo.close(abort);
       if (fetchOperators != null) {
         MapredLocalWork localWork = mo.getConf().getMapLocalWork();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java
Patch:
@@ -56,6 +56,7 @@ public final class SemanticAnalyzerFactory {
     commandType.put(HiveParser.TOK_DROPFUNCTION, "DROPFUNCTION");
     commandType.put(HiveParser.TOK_CREATEVIEW, "CREATEVIEW");
     commandType.put(HiveParser.TOK_DROPVIEW, "DROPVIEW");
+    commandType.put(HiveParser.TOK_ALTERVIEW_PROPERTIES, "ALTERVIEW_PROPERTIES");
     commandType.put(HiveParser.TOK_QUERY, "QUERY");
   }
 
@@ -88,6 +89,7 @@ public static BaseSemanticAnalyzer get(HiveConf conf, ASTNode tree)
       case HiveParser.TOK_ALTERTABLE_PROPERTIES:
       case HiveParser.TOK_ALTERTABLE_SERIALIZER:
       case HiveParser.TOK_ALTERTABLE_SERDEPROPERTIES:
+      case HiveParser.TOK_ALTERVIEW_PROPERTIES:
       case HiveParser.TOK_SHOWTABLES:
       case HiveParser.TOK_SHOW_TABLESTATUS:
       case HiveParser.TOK_SHOWFUNCTIONS:

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -251,6 +251,7 @@ public void cleanUp() throws Exception {
       deleteDirectory(new File(warehousePath, s));
     }
     FunctionRegistry.unregisterTemporaryUDF("test_udaf");
+    FunctionRegistry.unregisterTemporaryUDF("test_error");
   }
 
   private void runLoadCmd(String loadCmd) throws Exception {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ComparisonOpMethodResolver.java
Patch:
@@ -96,7 +96,8 @@ public Method getEvalMethod(List<TypeInfo> argTypeInfos) throws AmbiguousMethodE
 
         if (match) {
           if (udfMethod != null) {
-            throw new AmbiguousMethodException(udfClass, argTypeInfos);
+            throw new AmbiguousMethodException(udfClass, argTypeInfos,
+                Arrays.asList(new Method[]{udfMethod, m}));
           } else {
             udfMethod = m;
           }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/NumericOpMethodResolver.java
Patch:
@@ -121,7 +121,8 @@ public Method getEvalMethod(List<TypeInfo> argTypeInfos) throws UDFArgumentExcep
 
         if (match) {
           if (udfMethod != null) {
-            throw new AmbiguousMethodException(udfClass, argTypeInfos);
+            throw new AmbiguousMethodException(udfClass, argTypeInfos, 
+                Arrays.asList(new Method[]{udfMethod, m}));
           } else {
             udfMethod = m;
           }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/NumericUDAFEvaluatorResolver.java
Patch:
@@ -47,7 +47,7 @@ public NumericUDAFEvaluatorResolver(Class<? extends UDAF> udafClass) {
    */
   @Override
   public Class<? extends UDAFEvaluator> getEvaluatorClass(
-      List<TypeInfo> argTypeInfos) throws AmbiguousMethodException {
+      List<TypeInfo> argTypeInfos) throws UDFArgumentException {
     // Go through the argClasses and for any string, void or date time, start
     // looking for doubles
     ArrayList<TypeInfo> args = new ArrayList<TypeInfo>();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/UDAFEvaluatorResolver.java
Patch:
@@ -41,6 +41,6 @@ public interface UDAFEvaluatorResolver {
    * Gets the evaluator class corresponding to the passed parameter list.
    */
   Class<? extends UDAFEvaluator> getEvaluatorClass(List<TypeInfo> argClasses)
-      throws AmbiguousMethodException;
+      throws UDFArgumentException;
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.ql.exec.RowSchema;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UnionOperator;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.lib.Node;
@@ -129,7 +130,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
   }
 
   private void createMergeJob(FileSinkOperator fsOp, GenMRProcContext ctx,
-      String finalName) {
+      String finalName) throws SemanticException {
     Task<? extends Serializable> currTask = ctx.getCurrTask();
     RowSchema fsRS = fsOp.getSchema();
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/OpProcFactory.java
Patch:
@@ -23,6 +23,7 @@
 
 import org.apache.hadoop.hive.ql.exec.FilterOperator;
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.lib.NodeProcessor;
 import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
@@ -97,7 +98,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
     }
 
     private void addPruningPred(Map<TableScanOperator, ExprNodeDesc> opToPPR,
-        TableScanOperator top, ExprNodeDesc new_ppr_pred) {
+        TableScanOperator top, ExprNodeDesc new_ppr_pred) throws UDFArgumentException {
       ExprNodeDesc old_ppr_pred = opToPPR.get(top);
       ExprNodeDesc ppr_pred = null;
       if (old_ppr_pred != null) {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/MapredLocalWork.java
Patch:
@@ -24,12 +24,10 @@
 import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
 import java.util.Map.Entry;
 
-import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.BucketMatcher;
+import org.apache.hadoop.hive.ql.exec.Operator;
 
 /**
  * MapredLocalWork.

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
Patch:
@@ -36,6 +36,7 @@
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
 import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
 import org.apache.hadoop.hive.serde.Constants;
 import org.apache.hadoop.hive.serde2.Deserializer;
@@ -455,7 +456,7 @@ public static ReduceSinkDesc getReduceSinkDesc(
   public static ReduceSinkDesc getReduceSinkDesc(
       ArrayList<ExprNodeDesc> keyCols, ArrayList<ExprNodeDesc> valueCols,
       List<String> outputColumnNames, boolean includeKey, int tag,
-      int numPartitionFields, int numReducers) {
+      int numPartitionFields, int numReducers) throws SemanticException {
     ArrayList<ExprNodeDesc> partitionCols = null;
 
     if (numPartitionFields >= keyCols.size()) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
Patch:
@@ -297,9 +297,9 @@ public void processOp(Object row, int tag) throws HiveException {
             new OutputStreamProcessor(scriptOutputDeserializer
             .getObjectInspector()), "OutputProcessor");
 
-        RecordReader scriptErrReader = conf.getOutRecordReaderClass()
+        RecordReader scriptErrReader = conf.getErrRecordReaderClass()
             .newInstance();
-        scriptErrReader.initialize(scriptErr, hconf, conf.getScriptOutputInfo()
+        scriptErrReader.initialize(scriptErr, hconf, conf.getScriptErrInfo()
             .getProperties());
 
         errThread = new StreamThread(scriptErrReader, new ErrorStreamProcessor(
@@ -482,7 +482,7 @@ public void close() {
 
   /**
    * The processor for stderr stream.
-   * 
+   *
    * TODO: In the future when we move to hadoop 0.18 and above, we should borrow
    * the logic from HadoopStreaming: PipeMapRed.java MRErrorThread to support
    * counters and status updates.

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
Patch:
@@ -215,7 +215,9 @@ public void testScriptOperator() throws Throwable {
       TableDesc scriptInput = PlanUtils.getDefaultTableDesc(""
           + Utilities.tabCode, "a,b");
       ScriptDesc sd = new ScriptDesc("cat", scriptOutput,
-          TextRecordWriter.class, scriptInput, TextRecordReader.class);
+                                     TextRecordWriter.class, scriptInput,
+                                     TextRecordReader.class, TextRecordReader.class,
+                                     PlanUtils.getDefaultTableDesc("" + Utilities.tabCode, "key"));
       Operator<ScriptDesc> sop = OperatorFactory.getAndMakeChild(sd, op);
 
       // Collect operator to observe the output of the script

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
Patch:
@@ -254,9 +254,6 @@ private void setUpFetchOpContext(FetchOperator fetchOp, String alias)
     String currentInputFile = HiveConf.getVar(jc, HiveConf.ConfVars.HADOOPMAPFILENAME);
     BucketMapJoinContext bucketMatcherCxt = this.localWork.getBucketMapjoinContext();
     Class<? extends BucketMatcher> bucketMatcherCls = bucketMatcherCxt.getBucketMatcherClass();
-    if(bucketMatcherCls == null) {
-      bucketMatcherCls = org.apache.hadoop.hive.ql.exec.DefaultBucketMatcher.class;
-    }
     BucketMatcher bucketMatcher = (BucketMatcher) ReflectionUtils.newInstance(bucketMatcherCls, null);
     bucketMatcher.setAliasBucketFileNameMapping(bucketMatcherCxt.getAliasBucketFileNameMapping());
     List<Path> aliasFiles = bucketMatcher.getAliasBucketFiles(currentInputFile,

File: shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
Patch:
@@ -332,4 +332,7 @@ public String[] getTaskJobIDs(TaskCompletionEvent t) {
     return ret;
   }
 
+  public void setFloatConf(Configuration conf, String varName, float val) {
+    conf.setFloat(varName, val);
+  }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ConditionalTask.java
Patch:
@@ -85,7 +85,9 @@ public int execute() {
         console.printInfo(ExecDriver.getJobEndMsg(""
             + Utilities.randGen.nextInt())
             + ", job is filtered out (removed at runtime).");
-        driverContext.incCurJobNo(1);
+        if(tsk.isMapRedTask()) {
+          driverContext.incCurJobNo(1);          
+        }
         if (tsk.getChildTasks() != null) {
           for (Task<? extends Serializable> child : tsk.getChildTasks()) {
             child.parentTasks.remove(tsk);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -1805,8 +1805,7 @@ && isRegex(unescapeIdentifier(expr.getChild(1).getText()))) {
         col_list.add(exp);
         if (!StringUtils.isEmpty(alias)
             && (out_rwsch.get(null, colAlias) != null)) {
-          throw new SemanticException(ErrorMsg.AMBIGUOUS_COLUMN.getMsg(expr
-              .getChild(1)));
+          throw new SemanticException(ErrorMsg.AMBIGUOUS_COLUMN.getMsg(colAlias));
         }
         out_rwsch.put(tabAlias, colAlias, new ColumnInfo(
             getColumnInternalName(pos), exp.getTypeInfo(), tabAlias, false));

File: contrib/src/java/org/apache/hadoop/hive/contrib/serde2/RegexSerDe.java
Patch:
@@ -211,7 +211,7 @@ public Writable serialize(Object obj, ObjectInspector objInspector)
     if (outputFormatString == null) {
       throw new SerDeException(
           "Cannot write data into table because \"output.format.string\""
-              + " is not specified in serde properties of the table.");
+          + " is not specified in serde properties of the table.");
     }
 
     // Get all the fields out.

File: hwi/src/java/org/apache/hadoop/hive/hwi/HWIAuth.java
Patch:
@@ -29,7 +29,7 @@ public void setGroups(String[] groups) {
   }
 
   /**
-   * HWIAuth is used in SortedSets(s) the compartTo method is required
+   * HWIAuth is used in SortedSets(s) the compartTo method is required.
    * 
    * @return chained call to String.compareTo based on user property
    */
@@ -45,7 +45,7 @@ public int compareTo(Object obj) {
   }
 
   /**
-   * HWIAuth is used in Map(s) the hashCode method is required
+   * HWIAuth is used in Map(s) the hashCode method is required.
    * 
    * @see java.lang.Object#hashCode()
    */
@@ -58,7 +58,7 @@ public int hashCode() {
   }
 
   /**
-   * HWIAuth is used in Map(s) the equals method is required
+   * HWIAuth is used in Map(s) the equals method is required.
    * 
    * @see java.lang.Object#equals(java.lang.Object)
    */

File: hwi/src/java/org/apache/hadoop/hive/hwi/HWIServer.java
Patch:
@@ -115,7 +115,7 @@ public static void main(String[] args) throws Exception {
   }
 
   /**
-   * Shut down the running HWI Server
+   * Shut down the running HWI Server.
    * 
    * @throws Exception
    *           Running Thread.stop() can and probably will throw this

File: hwi/src/java/org/apache/hadoop/hive/hwi/HWISessionManager.java
Patch:
@@ -17,11 +17,11 @@
  */
 package org.apache.hadoop.hive.hwi;
 
+import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Set;
 import java.util.TreeMap;
 import java.util.TreeSet;
-import java.util.ArrayList;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -200,7 +200,7 @@ public HWISessionItem findSessionItemByName(HWIAuth auth, String sessionname) {
   }
 
   /**
-   * Used to list all users that have at least one session
+   * Used to list all users that have at least one session.
    * 
    * @return keySet of items all users that have any sessions
    */
@@ -209,7 +209,7 @@ public Set<HWIAuth> findAllUsersWithSessions() {
   }
 
   /**
-   * Used to list all the sessions of a user
+   * Used to list all the sessions of a user.
    * 
    * @param auth
    *          the user being enquired about

File: ql/src/java/org/apache/hadoop/hive/ql/exec/AutoProgressor.java
Patch:
@@ -64,7 +64,7 @@ public void run() {
       if (rp != null) {
         LOG
             .info("ReporterTask calling reporter.progress() for "
-                + logClassName);
+            + logClassName);
         rp.progress();
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java
Patch:
@@ -29,14 +29,14 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 
 /**
- * Buffers rows emitted by other operators
+ * Buffers rows emitted by other operators.
  **/
 public class CollectOperator extends Operator<CollectDesc> implements
     Serializable {
 
   private static final long serialVersionUID = 1L;
-  transient protected ArrayList<Object> rowList;
-  transient protected ObjectInspector standardRowInspector;
+  protected transient ArrayList<Object> rowList;
+  protected transient ObjectInspector standardRowInspector;
   transient int maxSize;
 
   @Override

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnInfo.java
Patch:
@@ -36,7 +36,7 @@ public class ColumnInfo implements Serializable {
   private String internalName;
 
   private String alias = null; // [optional] alias of the column (external name
-                               // as seen by the users)
+  // as seen by the users)
 
   /**
    * Store the alias of the table where available.
@@ -48,7 +48,7 @@ public class ColumnInfo implements Serializable {
    */
   private boolean isPartitionCol;
 
-  transient private TypeInfo type;
+  private transient TypeInfo type;
 
   public ColumnInfo() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ComparisonOpMethodResolver.java
Patch:
@@ -56,8 +56,7 @@ public ComparisonOpMethodResolver(Class<? extends UDF> udfClass) {
    * .List)
    */
   @Override
-  public Method getEvalMethod(List<TypeInfo> argTypeInfos)
-      throws AmbiguousMethodException {
+  public Method getEvalMethod(List<TypeInfo> argTypeInfos) throws AmbiguousMethodException {
     assert (argTypeInfos.size() == 2);
 
     List<TypeInfo> pTypeInfos = null;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ConditionalTask.java
Patch:
@@ -29,9 +29,8 @@
 import org.apache.hadoop.hive.ql.plan.api.StageType;
 
 /**
- * Conditional Task implementation
- **/
-
+ * Conditional Task implementation.
+ */
 public class ConditionalTask extends Task<ConditionalWork> implements
     Serializable {
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/CopyTask.java
Patch:
@@ -30,7 +30,7 @@
 import org.apache.hadoop.util.StringUtils;
 
 /**
- * CopyTask implementation
+ * CopyTask implementation.
  **/
 public class CopyTask extends Task<CopyWork> implements Serializable {
 
@@ -70,7 +70,7 @@ public int execute() {
       for (FileStatus oneSrc : srcs) {
         LOG.debug("Copying file: " + oneSrc.getPath().toString());
         if (!FileUtil.copy(srcFs, oneSrc.getPath(), dstFs, toPath, false, // delete
-                                                                          // source
+            // source
             true, // overwrite destination
             conf)) {
           console.printError("Failed to copy: '" + oneSrc.getPath().toString()

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultUDAFEvaluatorResolver.java
Patch:
@@ -57,7 +57,8 @@ public DefaultUDAFEvaluatorResolver(Class<? extends UDAF> udafClass) {
   public Class<? extends UDAFEvaluator> getEvaluatorClass(
       List<TypeInfo> argClasses) throws AmbiguousMethodException {
 
-    ArrayList<Class<? extends UDAFEvaluator>> classList = new ArrayList<Class<? extends UDAFEvaluator>>();
+    ArrayList<Class<? extends UDAFEvaluator>> classList =
+        new ArrayList<Class<? extends UDAFEvaluator>>();
 
     // Add all the public member classes that implement an evaluator
     for (Class<?> enclClass : udafClass.getClasses()) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultUDFMethodResolver.java
Patch:
@@ -55,8 +55,7 @@ public DefaultUDFMethodResolver(Class<? extends UDF> udfClass) {
    *          evaluate function signature.
    */
   @Override
-  public Method getEvalMethod(List<TypeInfo> argClasses)
-      throws AmbiguousMethodException {
+  public Method getEvalMethod(List<TypeInfo> argClasses) throws AmbiguousMethodException {
     Method m = FunctionRegistry.getMethodInternal(udfClass, "evaluate", false,
         argClasses);
     if (m == null) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeColumnEvaluator.java
Patch:
@@ -39,8 +39,7 @@ public ExprNodeColumnEvaluator(ExprNodeColumnDesc expr) {
   }
 
   @Override
-  public ObjectInspector initialize(ObjectInspector rowInspector)
-      throws HiveException {
+  public ObjectInspector initialize(ObjectInspector rowInspector) throws HiveException {
 
     // We need to support field names like KEY.0, VALUE.1 between
     // map-reduce boundary.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeFieldEvaluator.java
Patch:
@@ -50,8 +50,7 @@ public ExprNodeFieldEvaluator(ExprNodeFieldDesc desc) {
   }
 
   @Override
-  public ObjectInspector initialize(ObjectInspector rowInspector)
-      throws HiveException {
+  public ObjectInspector initialize(ObjectInspector rowInspector) throws HiveException {
 
     leftInspector = leftEvaluator.initialize(rowInspector);
     if (desc.getIsList()) {
@@ -72,7 +71,7 @@ public ObjectInspector initialize(ObjectInspector rowInspector)
     return resultObjectInspector;
   }
 
-  List<Object> cachedList = new ArrayList<Object>();
+  private List<Object> cachedList = new ArrayList<Object>();
 
   @Override
   public Object evaluate(Object row) throws HiveException {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java
Patch:
@@ -31,7 +31,7 @@
 public class ExtractOperator extends Operator<ExtractDesc> implements
     Serializable {
   private static final long serialVersionUID = 1L;
-  transient protected ExprNodeEvaluator eval;
+  protected transient ExprNodeEvaluator eval;
 
   @Override
   protected void initializeOp(Configuration hconf) throws HiveException {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
Patch:
@@ -31,8 +31,8 @@
 public class LimitOperator extends Operator<LimitDesc> implements Serializable {
   private static final long serialVersionUID = 1L;
 
-  transient protected int limit;
-  transient protected int currCount;
+  protected transient int limit;
+  protected transient int currCount;
 
   @Override
   protected void initializeOp(Configuration hconf) throws HiveException {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -41,7 +41,7 @@
 import org.apache.hadoop.util.StringUtils;
 
 /**
- * MoveTask implementation
+ * MoveTask implementation.
  **/
 public class MoveTask extends Task<MoveWork> implements Serializable {
 
@@ -102,7 +102,7 @@ public int execute() {
           } else {
             throw new AccessControlException(
                 "Unable to delete the existing destination directory: "
-                    + targetPath);
+                + targetPath);
           }
         }
       }
@@ -113,7 +113,7 @@ public int execute() {
         String mesg = "Loading data to table "
             + tbd.getTable().getTableName()
             + ((tbd.getPartitionSpec().size() > 0) ? " partition "
-                + tbd.getPartitionSpec().toString() : "");
+            + tbd.getPartitionSpec().toString() : "");
         String mesg_detail = " from " + tbd.getSourceDir();
         console.printInfo(mesg, mesg_detail);
         Table table = db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, tbd

File: ql/src/java/org/apache/hadoop/hive/ql/exec/NumericOpMethodResolver.java
Patch:
@@ -60,8 +60,7 @@ public NumericOpMethodResolver(Class<? extends UDF> udfClass) {
    * .List)
    */
   @Override
-  public Method getEvalMethod(List<TypeInfo> argTypeInfos)
-      throws AmbiguousMethodException, UDFArgumentException {
+  public Method getEvalMethod(List<TypeInfo> argTypeInfos) throws UDFArgumentException {
     assert (argTypeInfos.size() == 2);
 
     List<TypeInfo> pTypeInfos = null;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/RowSchema.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.ArrayList;
 
 /**
- * RowSchema Implementation
+ * RowSchema Implementation.
  */
 public class RowSchema implements Serializable {
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
Patch:
@@ -29,13 +29,13 @@
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 
 /**
- * Select operator implementation
- **/
+ * Select operator implementation.
+ */
 public class SelectOperator extends Operator<SelectDesc> implements
     Serializable {
 
   private static final long serialVersionUID = 1L;
-  transient protected ExprNodeEvaluator[] eval;
+  protected transient ExprNodeEvaluator[] eval;
 
   transient Object[] output;
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TaskResult.java
Patch:
@@ -18,9 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-
 /**
- * TaskResult implementation
+ * TaskResult implementation.
  **/
 
 public class TaskResult {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TaskRunner.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.session.SessionState;
 
 /**
- * TaskRunner implementation
+ * TaskRunner implementation.
  **/
 
 public class TaskRunner extends Thread {
@@ -48,7 +48,7 @@ public void run() {
   }
 
   /**
-   * Launches a task, and sets its exit value in the result variable
+   * Launches a task, and sets its exit value in the result variable.
    */
 
   public void runSequential() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TerminalOperator.java
Patch:
@@ -21,7 +21,7 @@
 import java.io.Serializable;
 
 /**
- * Terminal Operator Base Class
+ * Terminal Operator Base Class.
  **/
 public abstract class TerminalOperator<T extends Serializable> extends
     Operator<T> implements Serializable {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/UDAF.java
Patch:
@@ -71,7 +71,7 @@ public UDAF(UDAFEvaluatorResolver rslv) {
   }
 
   /**
-   * Sets the resolver
+   * Sets the resolver.
    * 
    * @param rslv
    *          The method resolver to use for method resolution.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/UDAFEvaluator.java
Patch:
@@ -29,5 +29,5 @@ public interface UDAFEvaluator {
   /**
    * Initializer. Initializes the state for the evaluator.
    */
-  public void init();
+  void init();
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/UDF.java
Patch:
@@ -42,7 +42,7 @@ public class UDF {
   private UDFMethodResolver rslv;
 
   /**
-   * The constructor
+   * The constructor.
    */
   public UDF() {
     rslv = new DefaultUDFMethodResolver(this.getClass());
@@ -56,7 +56,7 @@ protected UDF(UDFMethodResolver rslv) {
   }
 
   /**
-   * Sets the resolver
+   * Sets the resolver.
    * 
    * @param rslv
    *          The method resolver to use for method resolution.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/UDFMethodResolver.java
Patch:
@@ -45,6 +45,5 @@ public interface UDFMethodResolver {
    *          The list of the argument types that need to matched with the
    *          evaluate function signature.
    */
-  public Method getEvalMethod(List<TypeInfo> argClasses)
-      throws AmbiguousMethodException, UDFArgumentException;
+  Method getEvalMethod(List<TypeInfo> argClasses) throws UDFArgumentException;
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/DCLLItem.java
Patch:
@@ -59,7 +59,7 @@ public void setNext(DCLLItem itm) {
   }
 
   /**
-   * Set the previous item as itm
+   * Set the previous item as itm.
    * 
    * @param itm
    *          the item to be set as previous.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java
Patch:
@@ -48,12 +48,12 @@ public class HashMapWrapper<K, V> {
   private static final int THRESHOLD = 25000;
 
   private int threshold; // threshold to put data into persistent hash table
-                         // instead
+  // instead
   private HashMap<K, MRUItem> mHash; // main memory HashMap
   private HTree pHash; // persistent HashMap
   private RecordManager recman; // record manager required by HTree
   private File tmpFile; // temp file holding the persistent data from record
-                        // manager.
+  // manager.
   private MRU<MRUItem> MRUList; // MRU cache entry
 
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MRU.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.persistence;
 
-
 /**
  * An MRU (Most Recently Used) cache implementation. This implementation
  * maintains a doubly circular linked list and it can be used with an auxiliary
@@ -79,7 +78,7 @@ public T tail() {
   }
 
   /**
-   * Insert a new item as the head
+   * Insert a new item as the head.
    * 
    * @param v
    *          the new linked list item to be added to the head.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectKey.java
Patch:
@@ -32,12 +32,12 @@
 import org.apache.hadoop.io.Writable;
 
 /**
- * Map Join Object used for both key
+ * Map Join Object used for both key.
  */
 public class MapJoinObjectKey implements Externalizable {
 
-  transient protected int metadataTag;
-  transient protected ArrayList<Object> obj;
+  protected transient int metadataTag;
+  protected transient ArrayList<Object> obj;
 
   public MapJoinObjectKey() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecute.java
Patch:
@@ -41,7 +41,7 @@ public interface PostExecute {
    * @param ugi
    *          The user group security information.
    */
-  public void run(SessionState sess, Set<ReadEntity> inputs,
+  void run(SessionState sess, Set<ReadEntity> inputs,
       Set<WriteEntity> outputs, UserGroupInformation ugi) throws Exception;
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/hooks/PreExecute.java
Patch:
@@ -41,7 +41,7 @@ public interface PreExecute {
    * @param ugi
    *          The user group security information.
    */
-  public void run(SessionState sess, Set<ReadEntity> inputs,
+  void run(SessionState sess, Set<ReadEntity> inputs,
       Set<WriteEntity> outputs, UserGroupInformation ugi) throws Exception;
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
Patch:
@@ -142,7 +142,7 @@ public void readFields(DataInput in) throws IOException {
       } catch (Exception e) {
         throw new IOException(
             "Cannot create an instance of InputSplit class = "
-                + inputSplitClassName + ":" + e.getMessage());
+            + inputSplitClassName + ":" + e.getMessage());
       }
       inputSplit.readFields(in);
       inputFormatClassName = in.readUTF();

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveOutputFormat.java
Patch:
@@ -58,7 +58,7 @@ public interface HiveOutputFormat<K extends WritableComparable, V extends Writab
    *          progress used for status report
    * @return the RecordWriter for the output file
    */
-  public RecordWriter getHiveRecordWriter(JobConf jc, Path finalOutPath,
+  RecordWriter getHiveRecordWriter(JobConf jc, Path finalOutPath,
       final Class<? extends Writable> valueClass, boolean isCompressed,
       Properties tableProperties, Progressable progress) throws IOException;
 

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveSequenceFileOutputFormat.java
Patch:
@@ -40,7 +40,7 @@ public class HiveSequenceFileOutputFormat extends SequenceFileOutputFormat
   BytesWritable EMPTY_KEY = new BytesWritable();
 
   /**
-   * create the final out file, and output an empty key as the key
+   * create the final out file, and output an empty key as the key.
    * 
    * @param jc
    *          the job configuration file

File: ql/src/java/org/apache/hadoop/hive/ql/io/InputFormatChecker.java
Patch:
@@ -31,10 +31,10 @@
 public interface InputFormatChecker {
 
   /**
-   * This method is used to validate the input files
+   * This method is used to validate the input files.
    * 
    */
-  public boolean validateInput(FileSystem fs, HiveConf conf,
+  boolean validateInput(FileSystem fs, HiveConf conf,
       ArrayList<FileStatus> files) throws IOException;
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/lib/DefaultRuleDispatcher.java
Patch:
@@ -35,7 +35,7 @@ public class DefaultRuleDispatcher implements Dispatcher {
   private final NodeProcessor defaultProc;
 
   /**
-   * constructor
+   * Constructor.
    * 
    * @param defaultProc
    *          default processor to be fired if no rule matches
@@ -52,7 +52,7 @@ public DefaultRuleDispatcher(NodeProcessor defaultProc,
   }
 
   /**
-   * dispatcher function
+   * Dispatcher function.
    * 
    * @param nd
    *          operator to process

File: ql/src/java/org/apache/hadoop/hive/ql/lib/Dispatcher.java
Patch:
@@ -24,7 +24,7 @@
 
 /**
  * Dispatcher interface for Operators Used in operator graph walking to dispatch
- * process/visitor functions for operators
+ * process/visitor functions for operators.
  */
 public interface Dispatcher {
 
@@ -41,6 +41,6 @@ public interface Dispatcher {
    * @return Object The return object from the processing call.
    * @throws SemanticException
    */
-  public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
+  Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
       throws SemanticException;
 }

File: ql/src/java/org/apache/hadoop/hive/ql/lib/GraphWalker.java
Patch:
@@ -38,7 +38,7 @@ public interface GraphWalker {
    *          the map from node to objects returned by the processors.
    * @throws SemanticException
    */
-  public void startWalking(Collection<Node> startNodes,
+  void startWalking(Collection<Node> startNodes,
       HashMap<Node, Object> nodeOutput) throws SemanticException;
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/lib/Node.java
Patch:
@@ -32,12 +32,12 @@ public interface Node {
    * 
    * @return List<? extends Node>
    */
-  public List<? extends Node> getChildren();
+  List<? extends Node> getChildren();
 
   /**
    * Gets the name of the node. This is used in the rule dispatchers.
    * 
    * @return String
    */
-  public String getName();
+  String getName();
 }

File: ql/src/java/org/apache/hadoop/hive/ql/lib/NodeProcessor.java
Patch:
@@ -28,7 +28,7 @@
 public interface NodeProcessor {
 
   /**
-   * generic process for all ops that don't have specific implementations
+   * Generic process for all ops that don't have specific implementations.
    * 
    * @param nd
    *          operator to process
@@ -39,6 +39,6 @@ public interface NodeProcessor {
    * @return Object to be returned by the process call
    * @throws SemanticException
    */
-  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
+  Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
       Object... nodeOutputs) throws SemanticException;
 }

File: ql/src/java/org/apache/hadoop/hive/ql/lib/NodeProcessorCtx.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.lib;
 
 /**
- * Operator Processor Context
+ * Operator Processor Context.
  */
 public interface NodeProcessorCtx {
 }

File: ql/src/java/org/apache/hadoop/hive/ql/lib/PreOrderWalker.java
Patch:
@@ -32,7 +32,7 @@ public class PreOrderWalker extends DefaultGraphWalker {
    */
 
   /**
-   * Constructor
+   * Constructor.
    * 
    * @param disp
    *          dispatcher to call for each op encountered
@@ -42,7 +42,7 @@ public PreOrderWalker(Dispatcher disp) {
   }
 
   /**
-   * walk the current operator and its descendants
+   * Walk the current operator and its descendants.
    * 
    * @param nd
    *          current operator in the graph

File: ql/src/java/org/apache/hadoop/hive/ql/lib/Rule.java
Patch:
@@ -24,7 +24,7 @@
 
 /**
  * Rule interface for Operators Used in operator dispatching to dispatch
- * process/visitor functions for operators
+ * process/visitor functions for operators.
  */
 public interface Rule {
 
@@ -33,10 +33,10 @@ public interface Rule {
    *         matches
    * @throws SemanticException
    */
-  public int cost(Stack<Node> stack) throws SemanticException;
+  int cost(Stack<Node> stack) throws SemanticException;
 
   /**
    * @return the name of the rule - may be useful for debugging
    */
-  public String getName();
+  String getName();
 }

File: ql/src/java/org/apache/hadoop/hive/ql/lib/RuleRegExp.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  * Rule interface for Nodes Used in Node dispatching to dispatch process/visitor
- * functions for Nodes
+ * functions for Nodes.
  */
 public class RuleRegExp implements Rule {
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveException.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.metadata;
 
 /**
- * Generic exception class for Hive
+ * Generic exception class for Hive.
  */
 
 public class HiveException extends Exception {

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java
Patch:
@@ -258,7 +258,7 @@ void checkTable(Table table, List<Partition> parts,
   }
 
   /**
-   * Find partitions on the fs that are unknown to the metastore
+   * Find partitions on the fs that are unknown to the metastore.
    * 
    * @param table
    *          Table where the partitions would be located

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/InvalidTableException.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.metadata;
 
 /**
- * Generic exception class for Hive
+ * Generic exception class for Hive.
  * 
  */
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Sample.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.metadata;
 
 /**
- * A sample defines a subset of data based on sampling on a given dimension
+ * A sample defines a subset of data based on sampling on a given dimension.
  * 
  **/
 public class Sample {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMROperator.java
Patch:
@@ -30,15 +30,15 @@
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 
 /**
- * Processor for the rule - no specific rule fired
+ * Processor for the rule - no specific rule fired.
  */
 public class GenMROperator implements NodeProcessor {
 
   public GenMROperator() {
   }
 
   /**
-   * Reduce Scan encountered
+   * Reduce Scan encountered.
    * 
    * @param nd
    *          the reduce sink operator encountered
@@ -54,7 +54,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
     GenMapRedCtx mapredCtx = mapCurrCtx.get(stack.get(stack.size() - 2));
     mapCurrCtx.put((Operator<? extends Serializable>) nd, new GenMapRedCtx(
         mapredCtx.getCurrTask(), mapredCtx.getCurrTopOp(), mapredCtx
-            .getCurrAliasId()));
+        .getCurrAliasId()));
     return null;
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java
Patch:
@@ -32,15 +32,15 @@
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 
 /**
- * Processor for the rule - reduce sink followed by reduce sink
+ * Processor for the rule - reduce sink followed by reduce sink.
  */
 public class GenMRRedSink2 implements NodeProcessor {
 
   public GenMRRedSink2() {
   }
 
   /**
-   * Reduce Scan encountered
+   * Reduce Scan encountered.
    * 
    * @param nd
    *          the reduce sink operator encountered

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
Patch:
@@ -34,14 +34,14 @@
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 
 /**
- * Processor for the rule - table scan
+ * Processor for the rule - table scan.
  */
 public class GenMRTableScan1 implements NodeProcessor {
   public GenMRTableScan1() {
   }
 
   /**
-   * Table Sink encountered
+   * Table Sink encountered.
    * 
    * @param nd
    *          the table sink operator encountered

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/JoinReorder.java
Patch:
@@ -86,7 +86,7 @@ private int getOutputSize(Operator<? extends Serializable> operator,
   }
 
   /**
-   * Find all big tables from STREAMTABLE hints
+   * Find all big tables from STREAMTABLE hints.
    * 
    * @param joinCtx
    *          The join context
@@ -106,7 +106,7 @@ private Set<String> getBigTables(ParseContext joinCtx) {
 
   /**
    * Reorder the tables in a join operator appropriately (by reordering the tags
-   * of the reduces sinks)
+   * of the reduces sinks).
    * 
    * @param joinOp
    *          The join operator to be processed

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java
Patch:
@@ -29,14 +29,14 @@
 import org.apache.hadoop.hive.ql.ppd.PredicatePushDown;
 
 /**
- * Implementation of the optimizer
+ * Implementation of the optimizer.
  */
 public class Optimizer {
   private ParseContext pctx;
   private List<Transform> transformations;
 
   /**
-   * create the list of transformations
+   * Create the list of transformations.
    * 
    * @param hiveConf
    */
@@ -61,7 +61,7 @@ public void initialize(HiveConf hiveConf) {
   }
 
   /**
-   * invoke all the transformations one-by-one, and alter the query plan
+   * Invoke all the transformations one-by-one, and alter the query plan.
    * 
    * @return ParseContext
    * @throws SemanticException

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/Transform.java
Patch:
@@ -29,12 +29,12 @@
  */
 public interface Transform {
   /**
-   * All transformation steps implement this interface
+   * All transformation steps implement this interface.
    * 
    * @param pctx
    *          input parse context
    * @return ParseContext
    * @throws SemanticException
    */
-  public ParseContext transform(ParseContext pctx) throws SemanticException;
+  ParseContext transform(ParseContext pctx) throws SemanticException;
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/PhysicalOptimizer.java
Patch:
@@ -40,7 +40,7 @@ public PhysicalOptimizer(PhysicalContext pctx, HiveConf hiveConf) {
   }
 
   /**
-   * create the list of physical plan resolvers
+   * create the list of physical plan resolvers.
    * 
    * @param hiveConf
    */
@@ -52,7 +52,7 @@ private void initialize(HiveConf hiveConf) {
   }
 
   /**
-   * invoke all the resolvers one-by-one, and alter the physical plan
+   * invoke all the resolvers one-by-one, and alter the physical plan.
    * 
    * @return PhysicalContext
    * @throws HiveException

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/PhysicalPlanResolver.java
Patch:
@@ -32,6 +32,6 @@ public interface PhysicalPlanResolver {
    * @param pctx
    * @return
    */
-  public PhysicalContext resolve(PhysicalContext pctx) throws SemanticException;
+  PhysicalContext resolve(PhysicalContext pctx) throws SemanticException;
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SkewJoinResolver.java
Patch:
@@ -56,7 +56,7 @@ public PhysicalContext resolve(PhysicalContext pctx) throws SemanticException {
   }
 
   /**
-   * Iterator a task with a rule dispatcher for its reducer operator tree,
+   * Iterator a task with a rule dispatcher for its reducer operator tree.
    */
   class SkewJoinTaskDispatcher implements Dispatcher {
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/OpWalkerCtx.java
Patch:
@@ -33,12 +33,12 @@ public class OpWalkerCtx implements NodeProcessorCtx {
 
   /**
    * Map from tablescan operator to partition pruning predicate that is
-   * initialized from the ParseContext
+   * initialized from the ParseContext.
    */
   private final HashMap<TableScanOperator, ExprNodeDesc> opToPartPruner;
 
   /**
-   * Constructor
+   * Constructor.
    */
   public OpWalkerCtx(HashMap<TableScanOperator, ExprNodeDesc> opToPartPruner) {
     this.opToPartPruner = opToPartPruner;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcessor.java
Patch:
@@ -47,7 +47,7 @@
 public class UnionProcessor implements Transform {
 
   /**
-   * empty constructor
+   * empty constructor.
    */
   public UnionProcessor() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java
Patch:
@@ -36,7 +36,7 @@ public ASTNode() {
   }
 
   /**
-   * Constructor
+   * Constructor.
    * 
    * @param t
    *          Token for the CommonTree Node

File: ql/src/java/org/apache/hadoop/hive/ql/parse/JoinCond.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.parse;
 
-
 /**
  * Join conditions Descriptor implementation.
  * 
@@ -39,7 +38,7 @@ public JoinCond(int left, int right, JoinType joinType) {
   }
 
   /**
-   * Constructor for a UNIQUEJOIN cond
+   * Constructor for a UNIQUEJOIN cond.
    * 
    * @param p
    *          true if table is preserved, false otherwise

File: ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hive.ql.plan.CreateTableDesc;
 
 /**
- * Implementation of the query block
+ * Implementation of the query block.
  * 
  **/
 
@@ -46,7 +46,8 @@ public class QB {
   private String id;
   private boolean isQuery;
   private CreateTableDesc tblDesc = null; // table descriptor of the final
-                                          // results
+
+  // results
 
   public void print(String msg) {
     LOG.info(msg + "alias=" + qbp.getAlias());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/QBMetaData.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hive.ql.metadata.Table;
 
 /**
- * Implementation of the metadata information related to a query block
+ * Implementation of the metadata information related to a query block.
  * 
  **/
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java
Patch:
@@ -30,7 +30,7 @@
 import org.apache.commons.logging.LogFactory;
 
 /**
- * Implementation of the parse information related to a query block
+ * Implementation of the parse information related to a query block.
  * 
  **/
 
@@ -63,7 +63,7 @@ public class QBParseInfo {
   private final HashMap<String, ASTNode> destToSortby;
 
   /**
-   * Maping from table/subquery aliases to all the associated lateral view nodes
+   * Maping from table/subquery aliases to all the associated lateral view nodes.
    */
   private final HashMap<String, ArrayList<ASTNode>> aliasToLateralViews;
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticException.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 
 /**
- * Exception from SemanticAnalyzer
+ * Exception from SemanticAnalyzer.
  */
 
 public class SemanticException extends HiveException {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/AddPartitionDesc.java
Patch:
@@ -118,7 +118,7 @@ public void setPartSpec(Map<String, String> partSpec) {
   public boolean getIfNotExists() {
     return this.ifNotExists;
   }
-  
+
   /**
    * @param ifNotExists 
    *          if the part should be added only if it doesn't exist

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolver.java
Patch:
@@ -26,17 +26,17 @@
 
 /**
  * Conditional task resolution interface. This is invoked at run time to get the
- * task to invoke. Developers can plug in their own resolvers
+ * task to invoke. Developers can plug in their own resolvers.
  */
 public interface ConditionalResolver {
   /**
-   * All conditional resolvers implement this interface
+   * All conditional resolvers implement this interface.
    * 
    * @param conf
    *          configuration
    * @param ctx
    *          opaque context
    * @return position of the task
    */
-  public List<Task<? extends Serializable>> getTasks(HiveConf conf, Object ctx);
+  List<Task<? extends Serializable>> getTasks(HiveConf conf, Object ctx);
 }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java
Patch:
@@ -147,7 +147,7 @@ public static ExprNodeGenericFuncDesc newInstance(GenericUDF genericUDF,
     for (int i = 0; i < childrenOIs.length; i++) {
       childrenOIs[i] = TypeInfoUtils
           .getStandardWritableObjectInspectorFromTypeInfo(children.get(i)
-              .getTypeInfo());
+          .getTypeInfo());
     }
 
     ObjectInspector oi = genericUDF.initialize(childrenOIs);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/UnionDesc.java
Patch:
@@ -28,7 +28,7 @@
 public class UnionDesc implements Serializable {
   private static final long serialVersionUID = 1L;
 
-  transient private int numInputs;
+  private transient int numInputs;
 
   @SuppressWarnings("nls")
   public UnionDesc() {

File: ql/src/java/org/apache/hadoop/hive/ql/tools/LineageInfo.java
Patch:
@@ -49,11 +49,11 @@
 public class LineageInfo implements NodeProcessor {
 
   /**
-   * Stores input tables in sql
+   * Stores input tables in sql.
    */
   TreeSet<String> inputTableList = new TreeSet<String>();
   /**
-   * Stores output tables in sql
+   * Stores output tables in sql.
    */
   TreeSet<String> OutputTableList = new TreeSet<String>();
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFBridge.java
Patch:
@@ -154,7 +154,6 @@ static class UDAFAgg implements AggregationBuffer {
 
     @Override
     public AggregationBuffer getNewAggregationBuffer() {
-      System.err.println("udafEvaluator = " + udafEvaluator);
       return new UDAFAgg((UDAFEvaluator) ReflectionUtils.newInstance(
           udafEvaluator, null));
     }

File: contrib/src/java/org/apache/hadoop/hive/contrib/udaf/example/UDAFExampleMax.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.udf;
+package org.apache.hadoop.hive.contrib.udaf.example;
 
 import org.apache.hadoop.hive.ql.exec.UDAF;
 import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;
@@ -29,8 +29,8 @@
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
 
-@Description(name = "max", value = "_FUNC_(expr) - Returns the maximum value of expr")
-public class UDAFMax extends UDAF {
+@Description(name = "example_max", value = "_FUNC_(expr) - Returns the maximum value of expr")
+public class UDAFExampleMax extends UDAF {
 
   static public class MaxShortEvaluator implements UDAFEvaluator {
     private short mMax;

File: contrib/src/java/org/apache/hadoop/hive/contrib/udaf/example/UDAFExampleMin.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.udf;
+package org.apache.hadoop.hive.contrib.udaf.example;
 
 import org.apache.hadoop.hive.ql.exec.UDAF;
 import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;
@@ -29,8 +29,8 @@
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
 
-@Description(name = "min", value = "_FUNC_(expr) - Returns the minimum value of expr")
-public class UDAFMin extends UDAF {
+@Description(name = "example_min", value = "_FUNC_(expr) - Returns the minimum value of expr")
+public class UDAFExampleMin extends UDAF {
 
   static public class MinShortEvaluator implements UDAFEvaluator {
     private short mMin;

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
Patch:
@@ -189,6 +189,9 @@ static public Deserializer getDeserializer(Configuration conf,
   static public Deserializer getDeserializer(Configuration conf,
       org.apache.hadoop.hive.metastore.api.Table table) throws MetaException {
     String lib = table.getSd().getSerdeInfo().getSerializationLib();
+    if (lib == null) {
+      return null;
+    }
     try {
       Deserializer deserializer = SerDeUtils.lookupDeserializer(lib);
       deserializer.initialize(conf, MetaStoreUtils.getSchema(table));

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -427,9 +427,10 @@ public Table getTable(final String dbName, final String tableName,
       // of type "array<string>". This happens when the table is created using
       // an
       // earlier version of Hive.
-      if (tTable.getSd().getSerdeInfo().getSerializationLib().equals(
+      if (
           org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.class
-              .getName())
+          .getName().equals(
+            tTable.getSd().getSerdeInfo().getSerializationLib())
           && tTable.getSd().getColsSize() > 0
           && tTable.getSd().getCols().get(0).getType().indexOf('<') == -1) {
         tTable.getSd().getSerdeInfo().setSerializationLib(

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -6198,7 +6198,7 @@ private void validateCreateTable(CreateTableDesc crtTblDesc)
     if ((crtTblDesc.getCols() == null) || (crtTblDesc.getCols().size() == 0)) {
       // for now make sure that serde exists
       if (StringUtils.isEmpty(crtTblDesc.getSerName())
-          || SerDeUtils.isNativeSerDe(crtTblDesc.getSerName())) {
+          || !SerDeUtils.shouldGetColsFromSerDe(crtTblDesc.getSerName())) {
         throw new SemanticException(ErrorMsg.INVALID_TBL_DDL_SERDE.getMsg());
       }
       return;

File: serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
Patch:
@@ -91,8 +91,8 @@ public static Deserializer lookupDeserializer(String name)
         .add(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
   }
 
-  public static boolean isNativeSerDe(String serde) {
-    return nativeSerDeNames.contains(serde);
+  public static boolean shouldGetColsFromSerDe(String serde) {
+    return (serde != null) && !nativeSerDeNames.contains(serde);
   }
 
   private static boolean initCoreSerDes = registerCoreSerDes();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
Patch:
@@ -118,7 +118,8 @@ public enum ErrorMsg {
       "The same output cannot be present multiple times: "), INVALID_AS(
       "AS clause has an invalid number of aliases"), VIEW_COL_MISMATCH(
       "The number of columns produced by the SELECT clause does not match the number of column names specified by CREATE VIEW"), DML_AGAINST_VIEW(
-      "A view cannot be used as target table for LOAD or INSERT");
+      "A view cannot be used as target table for LOAD or INSERT"), UNSUPPORTED_TYPE(
+      "DATE, DATETIME, and TIMESTAMP types aren't supported yet. Please use STRING instead.");
   private String mesg;
   private String SQLState;
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -169,6 +169,7 @@ public static enum ConfVars {
     HIVEMERGEMAPFILESAVGSIZE("hive.merge.smallfiles.avgsize", (long)(16*1000*1000)),
 
     HIVESENDHEARTBEAT("hive.heartbeat.interval", 1000),
+    HIVEMAXMAPJOINSIZE("hive.mapjoin.maxsize", 100000),
 
     HIVEJOBPROGRESS("hive.task.progress", false),
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/RecordReader.java
Patch:
@@ -20,18 +20,19 @@
 
 import java.io.IOException;
 import java.io.InputStream;
+import java.util.Properties;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Writable;
 
 
 public interface RecordReader {
 
-  public void initialize(InputStream in, Configuration conf) throws IOException;
+  public void initialize(InputStream in, Configuration conf, Properties tbl) throws IOException;
 
   public Writable createRow() throws IOException;
 
   public int next(Writable row) throws IOException;
-  
+
   public void close() throws IOException;
 }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TextRecordReader.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.io.IOException;
 import java.io.InputStream;
+import java.util.Properties;
 
 import org.apache.hadoop.mapred.LineRecordReader.LineReader;
 
@@ -34,7 +35,7 @@ public class TextRecordReader implements RecordReader {
   private InputStream in;
   private Text        row;
 
-  public void initialize(InputStream in, Configuration conf) throws IOException {
+  public void initialize(InputStream in, Configuration conf, Properties tbl) throws IOException {
     lineReader = new LineReader(in, conf);
     this.in = in;
   }
@@ -50,7 +51,7 @@ public int next(Writable row) throws IOException {
 
     return lineReader.readLine((Text)row);
   }
-  
+
   public void close() throws IOException {
     if (in != null)
       in.close();

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -125,6 +125,7 @@ public static enum ConfVars {
     HIVEMAPSIDEAGGREGATE("hive.map.aggr", "true"),
     HIVEGROUPBYSKEW("hive.groupby.skewindata", "false"),
     HIVEJOINEMITINTERVAL("hive.join.emit.interval", 1000),
+    HIVEJOINCACHESIZE("hive.join.cache.size", 25000),
     HIVEMAPJOINROWSIZE("hive.mapjoin.size.key", 10000),
     HIVEMAPJOINCACHEROWS("hive.mapjoin.cache.numrows", 25000),
     HIVEGROUPBYMAPINTERVAL("hive.groupby.mapaggr.checkinterval", 100000),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.joinDesc;
 import org.apache.hadoop.hive.ql.plan.api.OperatorType;
+import org.apache.hadoop.hive.ql.exec.persistence.RowContainer;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/DCLLItem.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.exec;
+package org.apache.hadoop.hive.ql.exec.persistence;
 
 /**
  *  Doubly circular linked list item.

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MRU.java
Patch:
@@ -16,17 +16,17 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.exec;
+package org.apache.hadoop.hive.ql.exec.persistence;
 
-import org.apache.hadoop.hive.ql.exec.DCLLItem;
+import org.apache.hadoop.hive.ql.exec.persistence.DCLLItem;
 
 /**
  *  An MRU (Most Recently Used) cache implementation.
  *  This implementation maintains a doubly circular linked list and it can be used
  *  with an auxiliary data structure such as a HashMap to locate the item quickly.
  */
 public class MRU<T extends DCLLItem> {
-
+  
   T head;   // head of the linked list -- MRU; tail (head.prev) will be the LRU
   
   public MRU() {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectKey.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.exec;
+package org.apache.hadoop.hive.ql.exec.persistence;
 
 import java.io.Externalizable;
 import java.io.IOException;
@@ -25,6 +25,7 @@
 import java.io.ObjectOutput;
 import java.util.ArrayList;
 
+import org.apache.hadoop.hive.ql.exec.MapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.MapJoinOperator.MapJoinObjectCtx;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestHashMapWrapper.java
Patch:
@@ -23,7 +23,7 @@
 import java.util.Random;
 
 import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.exec.HashMapWrapper;
+import org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper;
 
 public class TestHashMapWrapper extends TestCase {
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
Patch:
@@ -119,6 +119,7 @@ public enum ErrorMsg {
   UDTF_ALIAS_MISMATCH("The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF"),
   LATERAL_VIEW_WITH_JOIN("Join with a lateral view is not supported"),
   LATERAL_VIEW_INVALID_CHILD("Lateral view AST with invalid child"),
+  OUTPUT_SPECIFIED_MULTIPLE_TIMES("The same output cannot be present multiple times: "),
   INVALID_AS("AS clause has an invalid number of aliases");
   private String mesg;
   private String SQLState;

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
Patch:
@@ -329,6 +329,8 @@ public Path getBucketPath(int bucketNum) {
       for (FileStatus src: srcs) {
         LOG.info("Got file: " + src.getPath());
       }
+      if(srcs.length == 0)
+        return null;
       return srcs[bucketNum].getPath();
     }
     catch (Exception e) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -459,6 +459,8 @@ public static void setTaskPlan(String alias_id, Operator<? extends Serializable>
       }
 
       for (Path p: paths) {
+        if(p == null)
+          continue;
         String path = p.toString();
         LOG.debug("Adding " + path + " of table" + alias_id);
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
Patch:
@@ -97,6 +97,7 @@ public enum ErrorMsg {
   INVALID_MAPJOIN_HINT("neither table specified as map-table"),
   INVALID_MAPJOIN_TABLE("result of a union cannot be a map table"),
   NON_BUCKETED_TABLE("Sampling Expression Needed for Non-Bucketed Table"),
+  BUCKETED_NUMBERATOR_BIGGER_DENOMINATOR("Numberator should not be bigger than denaminator in sample clause for Table"),
   NEED_PARTITION_ERROR("need to specify partition columns because the destination table is partitioned."),
   CTAS_CTLT_COEXISTENCE("Create table command does not allow LIKE and AS-SELECT in the same command"),
   CTAS_COLLST_COEXISTENCE("Create table as select command cannot specify the list of columns for the target table."),

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java
Patch:
@@ -53,6 +53,7 @@ public void initialize(HiveConf hiveConf) {
       transformations.add(new GroupByOptimizer());
     }
 
+    transformations.add(new SamplePruner());
     transformations.add(new MapJoinProcessor());
     transformations.add(new UnionProcessor());
     transformations.add(new JoinReorder());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
Patch:
@@ -337,7 +337,7 @@ public void close(boolean abort) throws HiveException {
         }
         int exitVal = 0;
         if (scriptPid != null)
-          scriptPid.waitFor();
+          exitVal = scriptPid.waitFor();
         if (exitVal != 0) {
           LOG.error("Script failed with code " + exitVal);
           new_abort = true;

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -158,7 +158,8 @@ public static enum ConfVars {
     HIVEMERGEMAPFILES("hive.merge.mapfiles", true),
     HIVEMERGEMAPREDFILES("hive.merge.mapredfiles", false),
     HIVEMERGEMAPFILESSIZE("hive.merge.size.per.task", (long)(256*1000*1000)),
-
+    HIVEMERGEMAPFILESAVGSIZE("hive.merge.smallfiles.avgsize", (long)(16*1000*1000)),
+    
     HIVESENDHEARTBEAT("hive.heartbeat.interval", 1000),
 
     HIVEJOBPROGRESS("hive.task.progress", false),

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
Patch:
@@ -104,7 +104,8 @@ public enum ErrorMsg {
   CTAS_PARCOL_COEXISTENCE("CREATE-TABLE-AS-SELECT does not support partitioning in the target table."),
   CTAS_MULTI_LOADFILE("CREATE-TABLE-AS-SELECT results in multiple file load."),
   CTAS_EXTTBL_COEXISTENCE("CREATE-TABLE-AS-SELECT cannot create external table."),
-  TABLE_ALREADY_EXISTS("Table already exists:", "42S02");
+  TABLE_ALREADY_EXISTS("Table already exists:", "42S02"),
+  COLUMN_ALIAS_ALREADY_EXISTS("Column alias already exists:", "42S02");
   
   private String mesg;
   private String SQLState;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
Patch:
@@ -661,6 +661,8 @@ private void showJobFailDebugInfo(JobConf conf, RunningJob rj) throws IOExceptio
         String jobId = taskToJob.get(task);
         String taskUrl = jtUrl + "/taskdetails.jsp?jobid=" + jobId + "&tipid=" + task.toString();
         console.printError("Task URL: " + taskUrl +"\n");
+        // Only print out one task because that's good enough for debugging.
+        break;
       }
     }
     return;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
Patch:
@@ -198,7 +198,7 @@ public void reduce(Object key, Iterator values,
         } catch (SerDeException e) {
           throw new HiveException("Unable to deserialize reduce input value (tag=" + tag.get()
               + ") from " + 
-              Utilities.formatBinaryString(valueWritable.getBytes(), 0, valueWritable.getLength())
+              Utilities.formatBinaryString(valueWritable.get(), 0, valueWritable.getSize())
               + " with properties " + valueTableDesc[tag.get()].getProperties(),
               e);
         }

File: ql/src/java/org/apache/hadoop/hive/ql/util/jdbm/recman/PhysicalRowIdManager.java
Patch:
@@ -138,7 +138,7 @@ void delete( Location rowid )
      *  Retrieves a record.
      */
     byte[] fetch( Location rowid )
-        throws IOException 
+        throws IOException
     {
         // fetch the record header
         PageCursor curs = new PageCursor( pageman, rowid.getBlock() );
@@ -190,7 +190,7 @@ private Location alloc( int size )
     {
         Location retval = freeman.get( size );
         if ( retval == null ) {
-            retval = allocNew( 2*size, pageman.getLast( Magic.USED_PAGE ) );
+            retval = allocNew( size, pageman.getLast( Magic.USED_PAGE ) );
         }
         return retval;
     }
@@ -337,7 +337,7 @@ private void write(Location rowid, byte[] data, int start, int length )
             if ( leftToWrite < toCopy ) {
                 toCopy = leftToWrite;
             }
-            System.arraycopy( data, offsetInBuffer, block.getData(), 
+            System.arraycopy( data, offsetInBuffer, block.getData(),
                               dataOffset, toCopy );
 
             // Go to the next block

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveDatabaseMetaData.java
Patch:
@@ -22,6 +22,7 @@
 import java.sql.ResultSet;
 import java.sql.RowIdLifetime;
 import java.sql.SQLException;
+import java.sql.DatabaseMetaData;
 import java.net.URL;
 import java.util.jar.Manifest;
 import java.util.jar.Attributes;
@@ -619,8 +620,7 @@ public String getSQLKeywords() throws SQLException {
    */
 
   public int getSQLStateType() throws SQLException {
-    // TODO Auto-generated method stub
-    throw new SQLException("Method not supported");
+    return DatabaseMetaData.sqlStateSQL99;
   }
 
   /* (non-Javadoc)

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -121,6 +121,9 @@ public class FunctionRegistry {
     registerUDF("dayofmonth", UDFDayOfMonth.class, false);
     registerUDF("month", UDFMonth.class, false);
     registerUDF("year", UDFYear.class, false);
+    registerUDF("hour", UDFHour.class, false);
+    registerUDF("minute", UDFMinute.class, false);
+    registerUDF("second", UDFSecond.class, false);
     registerUDF("from_unixtime", UDFFromUnixTime.class, false);
     registerUDF("unix_timestamp", UDFUnixTimeStamp.class, false);
     registerUDF("to_date", UDFDate.class, false);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -122,7 +122,7 @@ public static enum ConfVars {
     HIVEGROUPBYSKEW("hive.groupby.skewindata", "false"),
     HIVEJOINEMITINTERVAL("hive.join.emit.interval", 1000),
     HIVEMAPJOINROWSIZE("hive.mapjoin.size.key", 10000),
-    HIVEMAPJOINCACHEROWS("hive.mapjoin.cache.numrows", 10000),
+    HIVEMAPJOINCACHEROWS("hive.mapjoin.cache.numrows", 25000),
     HIVEGROUPBYMAPINTERVAL("hive.groupby.mapaggr.checkinterval", 100000),
     HIVEMAPAGGRHASHMEMORY("hive.map.aggr.hash.percentmemory", (float)0.5),
     HIVEMAPAGGRHASHMINREDUCTION("hive.map.aggr.hash.min.reduction", (float)0.5),

File: ql/src/java/org/apache/hadoop/hive/ql/Context.java
Patch:
@@ -206,14 +206,14 @@ private void removeScratchDir() {
     if (explain) {
       try {
         if (localScratchDir != null)
-          FileSystem.getLocal(conf).delete(localScratchDir);
+          FileSystem.getLocal(conf).delete(localScratchDir, true);
       } catch (Exception e) {
         LOG.warn("Error Removing Scratch: " + StringUtils.stringifyException(e));
       }
     } else {
       for (Path p: allScratchDirs) {
         try {
-          p.getFileSystem(conf).delete(p);
+          p.getFileSystem(conf).delete(p, true);
         } catch (Exception e) {
           LOG.warn("Error Removing Scratch: " + StringUtils.stringifyException(e));
         }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinObjectKey.java
Patch:
@@ -47,7 +47,6 @@ public MapJoinObjectKey() {
 
   /**
    * @param metadataTag
-   * @param objectTypeTag
    * @param obj
    */
   public MapJoinObjectKey(int metadataTag, ArrayList<Object> obj) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinObjectValue.java
Patch:
@@ -47,7 +47,6 @@ public MapJoinObjectValue() {
 
   /**
    * @param metadataTag
-   * @param objectTypeTag
    * @param obj
    */
   public MapJoinObjectValue(int metadataTag, ArrayList<ArrayList<Object>> obj) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveFileFormatUtils.java
Patch:
@@ -41,8 +41,8 @@
 
 /**
  * An util class for various Hive file format tasks.
- * {@link #registerOutputFormatSubstitute(Class, Class) and 
- * {@link #getOutputFormatSubstitute(Class)} are added for backward 
+ * registerOutputFormatSubstitute(Class, Class) 
+ * getOutputFormatSubstitute(Class) are added for backward 
  * compatibility. They return the newly added HiveOutputFormat for the older 
  * ones.
  * 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java
Patch:
@@ -143,7 +143,7 @@ public GenMRMapJoinCtx() {
     /**
      * @param taskTmpDir
      * @param tt_desc
-     * @param childSelect
+     * @param rootMapJoinOp
      * @param oldMapJoin
      */
     public GenMRMapJoinCtx(String taskTmpDir, tableDesc tt_desc, 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
Patch:
@@ -493,7 +493,7 @@ public static class MapJoinWalkerCtx implements NodeProcessorCtx {
     MapJoinOperator       currMapJoinOp;
 
     /**
-     * @param listMapJoins
+     * @param listMapJoinsNoRed
      */
     public MapJoinWalkerCtx(List<MapJoinOperator> listMapJoinsNoRed) {
       this.listMapJoinsNoRed = listMapJoinsNoRed;
@@ -509,7 +509,7 @@ public List<MapJoinOperator> getListMapJoinsNoRed() {
     }
 
     /**
-     * @param listMapJoins the listMapJoins to set
+     * @param listMapJoinsNoRed the listMapJoins to set
      */
     public void setListMapJoins(List<MapJoinOperator> listMapJoinsNoRed) {
       this.listMapJoinsNoRed = listMapJoinsNoRed;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
Patch:
@@ -98,7 +98,7 @@ public ParseContext transform(ParseContext pctx) throws SemanticException {
    * @param prunerExpr  the pruner expression for the alias
    * @param conf   for checking whether "strict" mode is on.
    * @param alias  for generating error message only.
-   * @return
+   * @return the partition list for the table that satisfies the partition pruner condition.
    * @throws HiveException
    */
   public static PrunedPartitionList prune(Table tab, exprNodeDesc prunerExpr,

File: ql/src/java/org/apache/hadoop/hive/ql/parse/QBJoinTree.java
Patch:
@@ -153,7 +153,7 @@ public boolean isMapSideJoin() {
   }
 
   /**
-   * @param mapSidejoin the mapSidejoin to set
+   * @param mapSideJoin the mapSidejoin to set
    */
   public void setMapSideJoin(boolean mapSideJoin) {
     this.mapSideJoin = mapSideJoin;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolver.java
Patch:
@@ -27,7 +27,8 @@
 public interface ConditionalResolver {
 	/**
 	 * All conditional resolvers implement this interface
-	 * @param pctx opaque context
+	 * @param conf configuration
+	 * @param ctx  opaque context
 	 * @return position of the task
 	 */
 	public int getTaskId(HiveConf conf, Object ctx);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/DDLWork.java
Patch:
@@ -135,7 +135,7 @@ public createTableLikeDesc getCreateTblLikeDesc() {
   }
 
   /**
-   * @param createTblDesc the createTblDesc to set
+   * @param createTblLikeDesc the createTblDesc to set
    */
   public void setCreateTblLikeDesc(createTableLikeDesc createTblLikeDesc) {
     this.createTblLikeDesc = createTblLikeDesc;
@@ -210,7 +210,7 @@ public void setShowFuncsDesc(showFunctionsDesc showFuncsDesc) {
   }
   
   /**
-   * @param showFuncsDesc the showFuncsDesc to set
+   * @param descFuncDesc the showFuncsDesc to set
    */
   public void setDescFuncDesc(descFunctionDesc descFuncDesc) {
     this.descFunctionDesc = descFuncDesc;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java
Patch:
@@ -135,7 +135,7 @@ public mapredLocalWork getMapLocalWork() {
   }
 
   /**
-   * @param mapredLocalWork the mapredLocalWork to set
+   * @param mapLocalWork the mapredLocalWork to set
    */
   public void setMapLocalWork(final mapredLocalWork mapLocalWork) {
     this.mapLocalWork = mapLocalWork;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPosMod.java
Patch:
@@ -32,8 +32,8 @@
 /**
  * class for computing positive modulo.
  * Used for positive_mod command in Cli
- * @ See {@link org.apache.hadoop.hive.ql.udf.UDFOPMod}
- * @ See {@link org.apache.hadoop.hive.ql.exec.FunctionRegistry}
+ * See {org.apache.hadoop.hive.ql.udf.UDFOPMod}
+ * See {org.apache.hadoop.hive.ql.exec.FunctionRegistry}
  */
 @description(
     name = "pmod",

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEvaluator.java
Patch:
@@ -117,7 +117,6 @@ public void aggregate(AggregationBuffer agg, Object[] parameters) throws HiveExc
   /**
    * This function will be called by GroupByOperator when it sees a new input row.
    * @param agg  The object to store the aggregation result.  
-   * @param parameterOIs  The row, can be inspected by the OIs passed in init().
    */
   public Object evaluate(AggregationBuffer agg) throws HiveException {
     if (mode == Mode.PARTIAL1 || mode == Mode.PARTIAL2) {

File: serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/objectinspector/LazyBinaryListObjectInspector.java
Patch:
@@ -25,7 +25,6 @@
 
 /**
  * ObjectInspector for LazyBinaryList
- * @see LazyBinaryList
  */
 public class LazyBinaryListObjectInspector extends StandardListObjectInspector {
 

File: service/src/java/org/apache/hadoop/hive/service/HiveServer.java
Patch:
@@ -98,7 +98,7 @@ public HiveServerHandler() throws MetaException {
     /**
      * Executes a query.
      *
-     * @param query HiveQL query to execute
+     * @param cmd HiveQL query to execute
      */
     public void execute(String cmd) throws HiveServerException, TException {
       HiveServerHandler.LOG.info("Running the query: " + cmd);

File: shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
Patch:
@@ -45,7 +45,7 @@ public interface HadoopShims {
   /**
    * Calls fs.deleteOnExit(path) if such a function exists.
    *
-   * @returns true if the call was successful
+   * @return true if the call was successful
    */
   public boolean fileSystemDeleteOnExit(FileSystem fs, Path path) throws IOException;
 

File: shims/src/0.20/java/org/apache/hadoop/hive/shims/Jetty20Shims.java
Patch:
@@ -34,6 +34,8 @@ public Server startServer(String listen, int port) throws IOException {
   private static class Server extends org.mortbay.jetty.Server implements JettyShims.Server {
     public void addWar(String war, String contextPath) {
       WebAppContext wac = new WebAppContext();
+      wac.setContextPath(contextPath);
+      wac.setWar(war);
       RequestLogHandler rlh = new RequestLogHandler();
       rlh.setHandler(wac);
       this.addHandler(rlh);
@@ -48,4 +50,4 @@ public void setupListenerHostPort(String listen, int port)
       this.addConnector(connector);
     }
   }
-}
\ No newline at end of file
+}

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
Patch:
@@ -191,7 +191,7 @@ public void testScriptOperator() throws Throwable {
       // scriptOperator to echo the output of the select
       tableDesc scriptOutput = PlanUtils.getDefaultTableDesc("" + Utilities.tabCode, "a,b");
       tableDesc scriptInput  = PlanUtils.getDefaultTableDesc("" + Utilities.tabCode, "a,b");
-      scriptDesc sd = new scriptDesc("cat", scriptOutput, scriptInput);
+      scriptDesc sd = new scriptDesc("cat", scriptOutput, scriptInput, TextRecordReader.class);
       Operator<scriptDesc> sop = OperatorFactory.getAndMakeChild(sd, op);
 
       // Collect operator to observe the output of the script

File: serde/src/gen-java/org/apache/hadoop/hive/serde/Constants.java
Patch:
@@ -30,6 +30,8 @@ public class Constants {
 
   public static final String SERIALIZATION_SORT_ORDER = "serialization.sort.order";
 
+  public static final String SERIALIZATION_USE_JSON_OBJECTS = "serialization.use.json.object";
+
   public static final String FIELD_DELIM = "field.delim";
 
   public static final String COLLECTION_DELIM = "colelction.delim";

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -571,7 +571,7 @@ public static Method getMethodInternal(ArrayList<Method> mlist, boolean exact,
           conversionCost += cost;
         }
       }
-      LOG.info("Method " + (match ? "did": "didn't") + " match: passed = " + argumentsPassed
+      LOG.debug("Method " + (match ? "did": "didn't") + " match: passed = " + argumentsPassed
           + " accepted = " + argumentsAccepted + " method = " + m);
       if (match) {
         // Always choose the function with least implicit conversions.

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -43,6 +43,7 @@
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.exec.Utilities.StreamPrinter;
@@ -246,6 +247,7 @@ public void cleanUp() throws Exception {
     for(String s: new String [] {"dest4.out", "union.out"}) {
       deleteDirectory(new File(warehousePath, s));
     }
+    FunctionRegistry.unregisterTemporaryUDF("test_udaf");
   }
 
   private void runLoadCmd(String loadCmd) throws Exception {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
Patch:
@@ -351,7 +351,7 @@ public void updateCounters(TaskHandle t) throws IOException {
     ExecDriverTaskHandle th = (ExecDriverTaskHandle)t;
     RunningJob rj = th.getRunningJob();
     this.mapProgress = Math.round(rj.mapProgress() * 100);
-    this.reduceProgress = Math.round(rj.mapProgress() * 100);
+    this.reduceProgress = Math.round(rj.reduceProgress() * 100);
     taskCounters.put("CNTR_NAME_" + getId() + "_MAP_PROGRESS", Long.valueOf(this.mapProgress));
     taskCounters.put("CNTR_NAME_" + getId() + "_REDUCE_PROGRESS", Long.valueOf(this.reduceProgress));
     Counters ctrs = th.getCounters();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -135,6 +135,7 @@ public class FunctionRegistry {
     registerUDF("*", UDFOPMultiply.class, true);
     registerUDF("/", UDFOPDivide.class, true);
     registerUDF("%", UDFOPMod.class, true);
+    registerUDF("div", UDFOPLongDivide.class, true);
 
     registerUDF("&", UDFOPBitAnd.class, true);
     registerUDF("|", UDFOPBitOr.class, true);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -2730,7 +2730,7 @@ Operator genConversionSelectOperator(String dest, QB qb,
     StructObjectInspector oi = null;
     try {
       Deserializer deserializer = table_desc.getDeserializerClass().newInstance();
-      deserializer.initialize(null, table_desc.getProperties());
+      deserializer.initialize(conf, table_desc.getProperties());
       oi = (StructObjectInspector) deserializer.getObjectInspector();
     } catch (Exception e) {
       throw new SemanticException(e);

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -63,6 +63,8 @@ public static enum ConfVars {
     SCRIPTERRORLIMIT("hive.exec.script.maxerrsize", 100000),
     COMPRESSRESULT("hive.exec.compress.output", false),
     COMPRESSINTERMEDIATE("hive.exec.compress.intermediate", false),
+    COMPRESSINTERMEDIATECODEC("hive.intermediate.compression.codec", ""),
+    COMPRESSINTERMEDIATETYPE("hive.intermediate.compression.type", ""),
     BYTESPERREDUCER("hive.exec.reducers.bytes.per.reducer", (long)(1000*1000*1000)),
     MAXREDUCERS("hive.exec.reducers.max", 999),
     PREEXECHOOKS("hive.exec.pre.hooks", ""),

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveResultSet.java
Patch:
@@ -58,7 +58,7 @@ public HiveResultSet(HiveInterface client) {
    */
   public void initDynamicSerde() {
     try {
-      Schema fullSchema = client.getSchema();
+      Schema fullSchema = client.getThriftSchema();
       List<FieldSchema> schema = fullSchema.getFieldSchemas();
       columnNames = new ArrayList<String>();
       columnTypes = new ArrayList<String>();

File: serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableSerDe.java
Patch:
@@ -151,8 +151,8 @@ public void testBinarySortableSerDe() throws Throwable {
         t.myShort = randField > 1 ? null : Short.valueOf((short)r.nextInt());
         t.myInt = randField > 2 ? null : Integer.valueOf(r.nextInt());
         t.myLong = randField > 3 ? null : Long.valueOf(r.nextLong());
-        t.myFloat = randField > 4 ? null : Float.valueOf(r.nextFloat());
-        t.myDouble = randField > 5 ? null : Double.valueOf(r.nextDouble());
+        t.myFloat = randField > 4 ? null : Float.valueOf(r.nextFloat() * 10 - 5);
+        t.myDouble = randField > 5 ? null : Double.valueOf(r.nextDouble() * 10 - 5);
         t.myString = randField > 6 ? null : getRandString(r);
         t.myStruct = randField > 7 ? null : new MyTestInnerStruct(r.nextInt(5)-2, r.nextInt(5)-2);
         t.myList = randField > 8 ? null: getRandIntegerArray(r);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java
Patch:
@@ -47,6 +47,7 @@ public static BaseSemanticAnalyzer get(HiveConf conf, ASTNode tree) throws Seman
       case HiveParser.TOK_SHOWPARTITIONS:
         return new DDLSemanticAnalyzer(conf);
       case HiveParser.TOK_CREATEFUNCTION: 
+      case HiveParser.TOK_DROPFUNCTION:
         return new FunctionSemanticAnalyzer(conf);
       default: return new SemanticAnalyzer(conf);
       }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -112,6 +112,7 @@ public static enum ConfVars {
     // for hive script operator
     HIVETABLENAME("hive.table.name", ""),
     HIVEPARTITIONNAME("hive.partition.name", ""),
+    HIVESCRIPTAUTOPROGRESS("hive.script.auto.progress", false),
     HIVEMAPREDMODE("hive.mapred.mode", "nonstrict"),
     HIVEALIAS("hive.alias", ""),
     HIVEMAPSIDEAGGREGATE("hive.map.aggr", "true"),

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -4362,7 +4362,7 @@ public static ArrayList<exprNodeDesc> convertParameters(Method m, List<exprNodeD
         // get the conversion method
         ArrayList<exprNodeDesc> conversionArg = new ArrayList<exprNodeDesc>(1);
         conversionArg.add(descPassed);
-        newParameters.add(new exprNodeFuncDesc(typeInfoAccepted, c, conv, conversionArg));
+        newParameters.add(new exprNodeFuncDesc(to.getTypeName(), typeInfoAccepted, c, conv, conversionArg));
       }
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -464,6 +464,7 @@ public static exprNodeDesc getFuncExprNodeDesc(String udfName, List<exprNodeDesc
       }
       
       exprNodeFuncDesc desc = new exprNodeFuncDesc(
+        udfName,
         resultTypeInfo,
         FunctionRegistry.getUDFClass(udfName),
         udfMethod, ch);

File: ql/src/java/org/apache/hadoop/hive/ql/ppd/OpProcFactory.java
Patch:
@@ -120,6 +120,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
           children.add(condn);
           children.add((exprNodeDesc) preds.get(i));
           condn = new exprNodeFuncDesc(
+              "AND",
               TypeInfoFactory.booleanTypeInfo,
               FunctionRegistry.getUDFClass("AND"),
               FunctionRegistry.getUDFMethod("AND",

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestExecDriver.java
Patch:
@@ -157,6 +157,7 @@ private filterDesc getTestFilterDesc(String column) {
     ArrayList<exprNodeDesc> children1 = new ArrayList<exprNodeDesc>();
     children1.add(new exprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, column));
     exprNodeDesc lhs = new exprNodeFuncDesc(
+        Constants.DOUBLE_TYPE_NAME,
         TypeInfoFactory.doubleTypeInfo,
         FunctionRegistry.getUDFClass(Constants.DOUBLE_TYPE_NAME),
         FunctionRegistry.getUDFMethod(Constants.DOUBLE_TYPE_NAME, TypeInfoFactory.stringTypeInfo),
@@ -165,6 +166,7 @@ private filterDesc getTestFilterDesc(String column) {
     ArrayList<exprNodeDesc> children2 = new ArrayList<exprNodeDesc>();
     children2.add(new exprNodeConstantDesc(TypeInfoFactory.longTypeInfo, Long.valueOf(100)));
     exprNodeDesc rhs = new exprNodeFuncDesc(
+        Constants.DOUBLE_TYPE_NAME,
         TypeInfoFactory.doubleTypeInfo,
         FunctionRegistry.getUDFClass(Constants.DOUBLE_TYPE_NAME),
         FunctionRegistry.getUDFMethod(Constants.DOUBLE_TYPE_NAME, TypeInfoFactory.longTypeInfo),
@@ -175,6 +177,7 @@ private filterDesc getTestFilterDesc(String column) {
     children3.add(rhs);
     
     exprNodeDesc desc = new exprNodeFuncDesc(
+        "<",
         TypeInfoFactory.booleanTypeInfo,
         FunctionRegistry.getUDFClass("<"),
         FunctionRegistry.getUDFMethod("<", TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.doubleTypeInfo),

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -384,7 +384,7 @@ public int execute() {
     return (0);
   }
 
-  public boolean getResults(Vector<String> res) {
+  public boolean getResults(Vector<String> res) throws IOException {
     if (plan != null && plan.getPlan().getFetchTask() != null) {
       BaseSemanticAnalyzer sem = plan.getPlan();
       if (!sem.getFetchTaskInit()) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java
Patch:
@@ -245,7 +245,7 @@ private RecordReader<WritableComparable, Writable> getRecordReader()
    * @param ctx
    *          fetch context
    **/
-  public InspectableObject getNextRow() {
+  public InspectableObject getNextRow() throws IOException {
     try {
       if (currRecReader == null) {
         currRecReader = getRecordReader();
@@ -272,7 +272,7 @@ public InspectableObject getNextRow() {
           return getNextRow();
       }
     } catch (Exception e) {
-      return null;
+      throw new IOException(e);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java
Patch:
@@ -80,7 +80,7 @@ public void initialize (HiveConf conf) {
   public abstract int execute();
   
   // dummy method - FetchTask overwrites this
-  public boolean fetch(Vector<String> res) { 
+  public boolean fetch(Vector<String> res) throws IOException { 
     assert false;
   	return false;
   }

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -264,7 +264,8 @@ public void cleanUp() throws Exception {
     for(String s: new String [] {"src", "src1", "src_json", "src_thrift", "src_sequencefile", 
                                  "srcpart", "srcbucket", "dest1", "dest2", 
                                  "dest3", "dest4", "dest4_sequencefile",
-                                 "dest_j1", "dest_j2", "dest_g1", "dest_g2"}) {
+                                 "dest_j1", "dest_j2", "dest_g1", "dest_g2",
+                                 "fetchtask_ioexception"}) {
       db.dropTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, s);
     }
     for(String s: new String [] {"dest4.out", "union.out"}) {

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -146,7 +146,7 @@ public static enum ConfVars {
 
     HIVEMERGEMAPFILES("hive.merge.mapfiles", true),
     HIVEMERGEMAPREDFILES("hive.merge.mapredfiles", false),
-    HIVEMERGEMAPFILESSIZE("hive.merge.size.per.task", (long)(1000*1000*1000)),
+    HIVEMERGEMAPFILESSIZE("hive.merge.size.per.task", (long)(256*1000*1000)),
       
     HIVESENDHEARTBEAT("hive.heartbeat.interval", 1000),
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
Patch:
@@ -147,9 +147,11 @@ private void createMergeJob(FileSinkOperator fsOp, GenMRProcContext ctx, String
         new extractDesc(new exprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, Utilities.ReduceField.VALUE.toString())),
         new RowSchema(out_rwsch.getColumnInfos()));
     
+    tableDesc ts = (tableDesc)fsConf.getTableInfo().clone();
+    fsConf.getTableInfo().getProperties().remove(org.apache.hadoop.hive.metastore.api.Constants.META_TABLE_PARTITION_COLUMNS);
     FileSinkOperator newOutput = 
       (FileSinkOperator)OperatorFactory.getAndMakeChild(
-         new fileSinkDesc(finalName, fsConf.getTableInfo(), 
+         new fileSinkDesc(finalName, ts, 
                           parseCtx.getConf().getBoolVar(HiveConf.ConfVars.COMPRESSINTERMEDIATE)),
          fsRS, extract);
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/AmbiguousMethodException.java
Patch:
@@ -20,13 +20,15 @@
 
 import java.util.List;
 
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 
 /**
  * Exception thrown by the UDF and UDAF method resolvers in case a unique method is not found.
  *
  */
-public class AmbiguousMethodException extends Exception {
+public class AmbiguousMethodException extends SemanticException {
 
   /**
    * 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
Patch:
@@ -339,8 +339,6 @@ public int execute() {
     job.setMapperClass(ExecMapper.class);
 
     job.setMapOutputKeyClass(HiveKey.class);
-    // LazySimpleSerDe writes to Text
-    // Revert to DynamicSerDe: job.setMapOutputValueClass(BytesWritable.class); 
     job.setMapOutputValueClass(Text.class);
 
     job.setNumReduceTasks(work.getNumReduceTasks().intValue());

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java
Patch:
@@ -57,7 +57,7 @@ public int execute() {
                                               (Class<? extends GenericUDF>) udfClass);
           return 0;
         } else if(UDAF.class.isAssignableFrom(udfClass)) {
-          FunctionRegistry.registerUDAF(createFunctionDesc.getFunctionName(), 
+          FunctionRegistry.registerUDAF(createFunctionDesc.getFunctionName(),
                                         (Class<? extends UDAF>) udfClass);
           return 0;
         } 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hadoop.hive.ql.plan.reduceSinkDesc;
 import org.apache.hadoop.hive.ql.plan.tableDesc;
 import org.apache.hadoop.hive.serde2.SerDeException;
+import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.Serializer;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -204,8 +205,9 @@ public void process(Object row, ObjectInspector rowInspector, int tag) throws Hi
     }
     
     try {
-      if (out != null)
+      if (out != null) {
         out.collect(keyWritable, value);
+      }
     } catch (IOException e) {
       throw new HiveException (e);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/UDFArgumentException.java
Patch:
@@ -18,12 +18,12 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
-import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
 
 /**                                                                                     
  * exception class, thrown when udf argument have something wrong.
  */
-public class UDFArgumentException extends HiveException {
+public class UDFArgumentException extends SemanticException {
 
   public UDFArgumentException() {
     super();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java
Patch:
@@ -404,7 +404,7 @@ private static void pruneReduceSinkOperator(boolean[] retainFlags,
     reduce.getSchema().setSignature(sig);
     reduceConf.setOutputValueColumnNames(newOutputColNames);
     reduceConf.setValueCols(newValueEval);
-    tableDesc newValueTable = PlanUtils.getLazySimpleSerDeTableDesc(PlanUtils.getFieldSchemasFromColumnList(
+    tableDesc newValueTable = PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(
         reduceConf.getValueCols(), newOutputColNames, 0, ""));
     reduceConf.setValueSerializeInfo(newValueTable);
   }
@@ -508,7 +508,7 @@ private static void pruneJoinOperator(NodeProcessorCtx ctx,
         }
 
         tableDesc valueTableDesc = PlanUtils
-            .getLazySimpleSerDeTableDesc(PlanUtils
+            .getMapJoinValueTableDesc(PlanUtils
                 .getFieldSchemasFromColumnList(valueCols, "mapjoinvalue"));
 
         valueTableDescs.add(valueTableDesc);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java
Patch:
@@ -113,8 +113,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx, Ob
       uPlan = (mapredWork)uTask.getWork();
     }
 
-    tableDesc tt_desc = 
-      PlanUtils.getBinaryTableDesc(PlanUtils.getFieldSchemasFromRowSchema(parent.getSchema(), "temporarycol")); 
+    tableDesc tt_desc = PlanUtils.getIntermediateFileTableDesc(
+          PlanUtils.getFieldSchemasFromRowSchema(parent.getSchema(), "temporarycol")); 
     
     // generate the temporary file
     Context baseCtx = parseCtx.getContext();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -615,7 +615,7 @@ public static void splitTasks(Operator<? extends Serializable> op,
     
     Operator<? extends Serializable> parent = op.getParentOperators().get(posn);
     tableDesc tt_desc = 
-      PlanUtils.getBinaryTableDesc(PlanUtils.getFieldSchemasFromRowSchema(parent.getSchema(), "temporarycol")); 
+      PlanUtils.getIntermediateFileTableDesc(PlanUtils.getFieldSchemasFromRowSchema(parent.getSchema(), "temporarycol")); 
     
     // Create a file sink operator for this file name
     Operator<? extends Serializable> fs_op =

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java
Patch:
@@ -197,7 +197,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
       Task<? extends Serializable> mjTask = TaskFactory.get(mjPlan, parseCtx.getConf());
       
       tableDesc tt_desc = 
-        PlanUtils.getLazySimpleSerDeTableDesc(PlanUtils.sortFieldSchemas(PlanUtils.getFieldSchemasFromRowSchema(mapJoin.getSchema(), "temporarycol"))); 
+        PlanUtils.getIntermediateFileTableDesc(PlanUtils.sortFieldSchemas(
+            PlanUtils.getFieldSchemasFromRowSchema(mapJoin.getSchema(), "temporarycol"))); 
       
       // generate the temporary file
       Context baseCtx = parseCtx.getContext();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
Patch:
@@ -225,7 +225,7 @@ private MapJoinOperator convertMapJoin(ParseContext pctx, JoinOperator op, QBJoi
     }
     
     tableDesc keyTableDesc = 
-      PlanUtils.getLazySimpleSerDeTableDesc(PlanUtils.getFieldSchemasFromColumnList(keyCols, "mapjoinkey"));
+      PlanUtils.getMapJoinKeyTableDesc(PlanUtils.getFieldSchemasFromColumnList(keyCols, "mapjoinkey"));
 
     List<tableDesc> valueTableDescs = new ArrayList<tableDesc>();
     
@@ -237,7 +237,7 @@ private MapJoinOperator convertMapJoin(ParseContext pctx, JoinOperator op, QBJoi
       }
               
       tableDesc valueTableDesc = 
-        PlanUtils.getLazySimpleSerDeTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, "mapjoinvalue"));
+        PlanUtils.getMapJoinValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, "mapjoinvalue"));
     
       valueTableDescs.add(valueTableDesc);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticException.java
Patch:
@@ -18,11 +18,13 @@
 
 package org.apache.hadoop.hive.ql.parse;
 
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+
 /**
  * Exception from SemanticAnalyzer
  */
 
-public class SemanticException extends Exception {
+public class SemanticException extends HiveException {
 
     private static final long serialVersionUID = 1L;
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeFuncDesc.java
Patch:
@@ -94,7 +94,7 @@ public String toString() {
   @explain(displayName="expr")
   @Override
   public String getExprString() {
-    FunctionInfo fI = FunctionRegistry.getInfo(UDFClass);
+    FunctionInfo fI = FunctionRegistry.getUDFInfo(UDFClass);
     StringBuilder sb = new StringBuilder();
     
     if (fI.getOpType() == FunctionInfo.OperatorType.PREFIX ||

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeGenericFuncDesc.java
Patch:
@@ -127,7 +127,7 @@ public static exprNodeGenericFuncDesc newInstance(Class<? extends GenericUDF> ge
       List<exprNodeDesc> children) throws UDFArgumentException {
     ObjectInspector[] childrenOIs = new ObjectInspector[children.size()];
     for(int i=0; i<childrenOIs.length; i++) {
-      childrenOIs[i] = TypeInfoUtils.getStandardObjectInspectorFromTypeInfo(
+      childrenOIs[i] = TypeInfoUtils.getStandardWritableObjectInspectorFromTypeInfo(
           children.get(i).getTypeInfo());
     }
     GenericUDF genericUDF = (GenericUDF) ReflectionUtils.newInstance(genericUDFClass, null);

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestExpressionEvaluator.java
Patch:
@@ -85,7 +85,7 @@ public TestExpressionEvaluator() {
       
       r = new InspectableObject();
       r.o = data;
-      r.oi = TypeInfoUtils.getStandardObjectInspectorFromTypeInfo(dataType);
+      r.oi = TypeInfoUtils.getStandardWritableObjectInspectorFromTypeInfo(dataType);
     } catch (Throwable e) {
       e.printStackTrace();
       throw new RuntimeException (e);

File: serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java
Patch:
@@ -124,7 +124,7 @@ public void initialize(Configuration conf, Properties tbl)
     // Create row related objects
     rowTypeInfo = TypeInfoFactory.getStructTypeInfo(columnNames, columnTypes);
     rowObjectInspector = (StructObjectInspector)TypeInfoUtils
-        .getStandardObjectInspectorFromTypeInfo(rowTypeInfo);
+        .getStandardWritableObjectInspectorFromTypeInfo(rowTypeInfo);
     row = new ArrayList<Object>(columnNames.size());
     for (int i=0; i<columnNames.size(); i++) {
       row.add(null);

File: serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/LazySimpleStructObjectInspector.java
Patch:
@@ -39,7 +39,7 @@
  * Always use the ObjectInspectorFactory to create new ObjectInspector objects, instead
  * of directly creating an instance of this class.
  */
-public class LazySimpleStructObjectInspector implements StructObjectInspector {
+public class LazySimpleStructObjectInspector extends StructObjectInspector {
 
   public static final Log LOG = LogFactory.getLog(LazySimpleStructObjectInspector.class.getName());
   

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ColumnarStructObjectInspector.java
Patch:
@@ -36,7 +36,7 @@
  * Always use the ObjectInspectorFactory to create new ObjectInspector objects,
  * instead of directly creating an instance of this class.
  */
-class ColumnarStructObjectInspector implements StructObjectInspector {
+class ColumnarStructObjectInspector extends StructObjectInspector {
 
   public static final Log LOG = LogFactory
       .getLog(ColumnarStructObjectInspector.class.getName());

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ReflectionStructObjectInspector.java
Patch:
@@ -32,7 +32,7 @@
  * of directly creating an instance of this class. 
  *
  */
-public class ReflectionStructObjectInspector implements StructObjectInspector {
+public class ReflectionStructObjectInspector extends StructObjectInspector {
 
   public static class MyField implements StructField {
     protected Field field;

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardStructObjectInspector.java
Patch:
@@ -35,7 +35,7 @@
  * Always use the ObjectInspectorFactory to create new ObjectInspector objects, instead
  * of directly creating an instance of this class. 
  */
-public class StandardStructObjectInspector implements StructObjectInspector {
+public class StandardStructObjectInspector extends StructObjectInspector {
 
   public static final Log LOG = LogFactory.getLog(StandardStructObjectInspector.class.getName());
   

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/UnionStructObjectInspector.java
Patch:
@@ -31,7 +31,7 @@
  * Always use the ObjectInspectorFactory to create new ObjectInspector objects, instead
  * of directly creating an instance of this class. 
  */
-public class UnionStructObjectInspector implements StructObjectInspector {
+public class UnionStructObjectInspector extends StructObjectInspector {
 
   public static class MyField implements StructField {
     public int structID;

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hive.serde2.lazy.LazyInteger;
 import org.apache.hadoop.hive.serde2.lazy.LazyLong;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
 import org.apache.hadoop.io.BooleanWritable;
 import org.apache.hadoop.io.FloatWritable;
@@ -478,6 +479,7 @@ public static int getInt(Object o, PrimitiveObjectInspector oi) throws NumberFor
     }
     return result;
   }
+
   
   /**
    * Get the long value out of a primitive object. 
@@ -601,4 +603,5 @@ public static float getFloat(Object o, PrimitiveObjectInspector oi) throws Numbe
     return (float)getDouble(o, oi);
   }
 
+
 }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -150,7 +150,8 @@ public static enum ConfVars {
     HIVESENDHEARTBEAT("hive.heartbeat.interval", 1000),
 
     // Optimizer
-    HIVEOPTPPD("hive.optimize.ppd", false); // predicate pushdown
+    HIVEOPTCP("hive.optimize.cp", true), // column pruner
+    HIVEOPTPPD("hive.optimize.ppd", true); // predicate pushdown
     
     
     public final String varname;

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java
Patch:
@@ -540,8 +540,8 @@ private static void pruneJoinOperator(NodeProcessorCtx ctx,
       }
     }
 
-    for (int i = 0; i < childColLists.size(); i++) {
-      String internalName = childColLists.get(i);
+    for (int i = 0; i < outputCols.size(); i++) {
+      String internalName = outputCols.get(i);
       String[] nm = joinRR.reverseLookup(internalName);
       ColumnInfo col = joinRR.get(nm[0], nm[1]);
       newJoinRR.put(nm[0], nm[1], col);

File: ql/src/java/org/apache/hadoop/hive/ql/Context.java
Patch:
@@ -61,6 +61,9 @@ public class Context {
   protected int pathid = 10000;
   protected boolean explain = false;
 
+  public Context() {  
+  }
+  
   public Context(HiveConf conf) {
     this.conf = conf;
     Path tmpPath = new Path(conf.getVar(HiveConf.ConfVars.SCRATCHDIR));

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java
Patch:
@@ -29,6 +29,9 @@
  */
 public class ASTNode extends CommonTree implements Node {
 
+  public ASTNode() {  
+  }
+  
   /**
    * Constructor
    * @param t Token for the CommonTree Node

File: ql/src/java/org/apache/hadoop/hive/ql/parse/OpParseContext.java
Patch:
@@ -28,6 +28,9 @@
 public class OpParseContext {
   private RowResolver rr;  // row resolver for the operator
 
+  public OpParseContext() {  
+  }
+  
   /**
    * @param rr row resolver
    */

File: ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java
Patch:
@@ -76,6 +76,9 @@ public class PartitionPruner {
   // a map-reduce job
   private boolean onlyContainsPartCols;
 
+  public PartitionPruner() {  
+  }
+  
   /** Creates a new instance of PartitionPruner */
   public PartitionPruner(String tableAlias, QBMetaData metaData) {
     this.tableAlias = tableAlias;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/QB.java
Patch:
@@ -57,6 +57,9 @@ public void print(String msg) {
     }
   }
 
+  public QB() {  
+  }
+  
   public QB(String outer_id, String alias, boolean isSubQ) {
     aliasToTabs = new HashMap<String, String>();
     aliasToSubq = new HashMap<String, QBExpr>();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -137,7 +137,7 @@ public class SemanticAnalyzer extends BaseSemanticAnalyzer {
   private HashMap<String, SamplePruner> aliasToSamplePruner;
   private HashMap<String, Operator<? extends Serializable>> topOps;
   private HashMap<String, Operator<? extends Serializable>> topSelOps;
-  private HashMap<Operator<? extends Serializable>, OpParseContext> opParseCtx;
+  private LinkedHashMap<Operator<? extends Serializable>, OpParseContext> opParseCtx;
   private List<loadTableDesc> loadTableWork;
   private List<loadFileDesc> loadFileWork;
   private Map<JoinOperator, QBJoinTree> joinContext;
@@ -171,7 +171,7 @@ public SemanticAnalyzer(HiveConf conf) throws SemanticException {
     this.topSelOps = new HashMap<String, Operator<? extends Serializable>>();
     this.loadTableWork = new ArrayList<loadTableDesc>();
     this.loadFileWork = new ArrayList<loadFileDesc>();
-    opParseCtx = new HashMap<Operator<? extends Serializable>, OpParseContext>();
+    opParseCtx = new LinkedHashMap<Operator<? extends Serializable>, OpParseContext>();
     joinContext = new HashMap<JoinOperator, QBJoinTree>();
     this.destTableId = 1;
     this.uCtx = null;
@@ -3758,7 +3758,7 @@ private void genMapRedTasks(QB qb) throws SemanticException {
         conf, new HashMap<Operator<? extends Serializable>, Task<? extends Serializable>>(),
         new ArrayList<Operator<? extends Serializable>>(),
         getParseContext(), mvTask, this.rootTasks,
-        new HashMap<Operator<? extends Serializable>, GenMapRedCtx>(),
+        new LinkedHashMap<Operator<? extends Serializable>, GenMapRedCtx>(),
         inputs, outputs);
 
     // create a walker which walks the tree in a DFS manner while maintaining the operator stack. 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java
Patch:
@@ -41,6 +41,9 @@ public static class ConditionalResolverMergeFilesCtx {
     List<Task<? extends Serializable>> listTasks;
     private String dir;
 
+    public ConditionalResolverMergeFilesCtx() {  
+    }
+    
     /**
      * @param dir
      */

File: serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java
Patch:
@@ -136,7 +136,6 @@ public void initialize(Configuration conf, Properties tbl)
     for (int i=0; i<columnSortOrderIsDesc.length; i++) {
       columnSortOrderIsDesc[i] = (columnSortOrder != null && columnSortOrder.charAt(i) == '-');
     }
-    System.out.println("Column sort order = " + columnSortOrder);
   }
   
   @Override

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/ByteObjectInspector.java
Patch:
@@ -29,4 +29,5 @@ public interface ByteObjectInspector extends PrimitiveObjectInspector {
    * Get the byte data.
    */
   byte get(Object o);
+  
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
Patch:
@@ -197,7 +197,7 @@ private MapJoinOperator convertMapJoin(ParseContext pctx, JoinOperator op, QBJoi
       for (int i=1; i < newParentOps.size(); i++) {
         TypeInfo a = commonType;
         TypeInfo b = keyExprMap.get(new Byte((byte)i)).get(k).getTypeInfo(); 
-        commonType = FunctionRegistry.getCommonClass(a, b);
+        commonType = FunctionRegistry.getCommonClassForComparison(a, b);
         if (commonType == null) {
           throw new SemanticException("Cannot do equality join on different types: " + a.getTypeName() + " and " + b.getTypeName());
         }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -2810,7 +2810,7 @@ private void genJoinOperatorTypeCheck(Operator left, Operator[] right) throws Se
       for(int i=1; i<right.length; i++) {
         TypeInfo a = commonType;
         TypeInfo b = keys.get(i).get(k).getTypeInfo(); 
-        commonType = FunctionRegistry.getCommonClass(a, b);
+        commonType = FunctionRegistry.getCommonClassForComparison(a, b);
         if (commonType == null) {
           throw new SemanticException("Cannot do equality join on different types: " + a.getTypeName() + " and " + b.getTypeName());
         }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIf.java
Patch:
@@ -41,7 +41,7 @@ public class GenericUDFIf extends GenericUDF {
   public ObjectInspector initialize(ObjectInspector[] arguments)
       throws UDFArgumentException {
     this.argumentOIs = arguments;
-    returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver();
+    returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);
 
     if (arguments.length != 3) {
       throw new UDFArgumentLengthException(

File: metastore/src/gen-javabean/org/apache/hadoop/hive/metastore/api/Partition.java
Patch:
@@ -454,7 +454,7 @@ public String toString() {
 sb.append(",lastAccessTime:");
 sb.append(this.lastAccessTime);
 sb.append(",sd:");
-sb.append(this.sd.toString());
+sb.append(this.sd);
 sb.append(",parameters:");
 sb.append(this.parameters);
 sb.append(")");

File: metastore/src/gen-javabean/org/apache/hadoop/hive/metastore/api/StorageDescriptor.java
Patch:
@@ -653,7 +653,7 @@ public String toString() {
 sb.append(",numBuckets:");
 sb.append(this.numBuckets);
 sb.append(",serdeInfo:");
-sb.append(this.serdeInfo.toString());
+sb.append(this.serdeInfo);
 sb.append(",bucketCols:");
 sb.append(this.bucketCols);
 sb.append(",sortCols:");

File: metastore/src/gen-javabean/org/apache/hadoop/hive/metastore/api/Table.java
Patch:
@@ -541,7 +541,7 @@ public String toString() {
 sb.append(",retention:");
 sb.append(this.retention);
 sb.append(",sd:");
-sb.append(this.sd.toString());
+sb.append(this.sd);
 sb.append(",partitionKeys:");
 sb.append(this.partitionKeys);
 sb.append(",parameters:");

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
Patch:
@@ -82,6 +82,7 @@ public void initialize (HiveConf conf) {
        mSerdeProp.put(Constants.SERIALIZATION_FORMAT, "" + Utilities.tabCode);
        mSerdeProp.put(Constants.SERIALIZATION_NULL_FORMAT, ((fetchWork)work).getSerializationNullFormat());
        mSerde.initialize(job, mSerdeProp);
+       mSerde.setUseJSONSerialize(true);
        
        ftOp = new FetchOperator(work, job);
     } catch (Exception e) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -39,8 +39,6 @@
 import org.apache.hadoop.hive.serde2.Serializer;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 
-import com.sun.corba.se.pept.encoding.InputObject;
-
 /**
  * File Sink operator implementation
  **/

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
Patch:
@@ -161,7 +161,7 @@ private void createMergeJob(FileSinkOperator fsOp, GenMRProcContext ctx, String
     cplan.getPathToPartitionInfo().put(fsConf.getDirName(), new partitionDesc(fsConf.getTableInfo(), null));
     cplan.setNumReduceTasks(-1);
     
-    moveWork dummyMv = new moveWork(null, new loadFileDesc(fsOp.getConf().getDirName(), finalName, true, null), false);
+    moveWork dummyMv = new moveWork(null, new loadFileDesc(fsOp.getConf().getDirName(), finalName, true, null, null), false);
     Task<? extends Serializable> dummyMergeTask = TaskFactory.get(dummyMv, ctx.getConf());
     List<Serializable> listWorks = new ArrayList<Serializable>();
     listWorks.add(dummyMv);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.util.*;
 import java.io.*;
+import java.net.URLClassLoader;
 
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
@@ -153,7 +154,7 @@ private MapOpCtx initObjectInspector(Configuration hconf, String onefile) throws
       if ((className == "") || (className == null)) {
         throw new HiveException("SerDe class or the SerDe class name is not set for table: " + td.getProperties().getProperty("name"));
       }
-      sdclass = MapOperator.class.getClassLoader().loadClass(className);
+      sdclass = hconf.getClassByName(className);
     }
     
     deserializer = (Deserializer) sdclass.newInstance();

File: ql/src/java/org/apache/hadoop/hive/ql/io/RCFile.java
Patch:
@@ -981,8 +981,8 @@ private void init() throws IOException {
         throw new VersionMismatchException(VERSION[3], version);
 
       try {
-        Class<?> keyCls = Class.forName(Text.readString(in));
-        Class<?> valCls = Class.forName(Text.readString(in));
+        Class<?> keyCls = conf.getClassByName(Text.readString(in));
+        Class<?> valCls = conf.getClassByName(Text.readString(in));
         if (!keyCls.equals(KeyBuffer.class)
             || !valCls.equals(ValueBuffer.class))
           throw new IOException(file + " not a RCFile");

File: ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
Patch:
@@ -314,7 +314,9 @@ public static String validateFile(Set<String> curFiles, String newFile) {
   public static boolean registerJar(String newJar) {
     LogHelper console = getConsole();
     try {
-      Utilities.addToClassPath(StringUtils.split(newJar, ","));
+      ClassLoader loader = Thread.currentThread().getContextClassLoader();
+      Thread.currentThread().setContextClassLoader(
+          Utilities.addToClassPath(loader, StringUtils.split(newJar, ",")));
       console.printInfo("Added " + newJar + " to class path");
       return true;
     } catch (Exception e) {

File: serde/src/java/org/apache/hadoop/hive/serde2/ThriftDeserializer.java
Patch:
@@ -39,7 +39,7 @@ public void initialize(Configuration job, Properties tbl) throws SerDeException
       // per Table basis
 
       String className = tbl.getProperty(org.apache.hadoop.hive.serde.Constants.SERIALIZATION_CLASS);
-      Class<?> recordClass = Class.forName(className);
+      Class<?> recordClass = job.getClassByName(className);
 
       String protoName = tbl.getProperty(org.apache.hadoop.hive.serde.Constants.SERIALIZATION_FORMAT);
       if (protoName == null) {

File: metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -305,9 +305,9 @@ private MDatabase getMDatabase(String name) throws NoSuchObjectException {
       if(!commited) {
         rollbackTransaction();
       }
-      if(db == null) {
-        throw new NoSuchObjectException("There is no database named " + name);
-      }
+    }
+    if(db == null) {
+      throw new NoSuchObjectException("There is no database named " + name);
     }
     return db;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.io.File;
 import java.io.IOException;
+import java.lang.Exception;
 import java.io.Serializable;
 import java.util.HashMap;
 import java.util.Map;
@@ -44,6 +45,7 @@
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.hive.ql.util.jdbm.htree.HTree;
+import org.apache.hadoop.hive.ql.util.jdbm.helper.FastIterator;
 import org.apache.hadoop.hive.ql.util.jdbm.RecordManager;
 import org.apache.hadoop.hive.ql.util.jdbm.RecordManagerFactory;
 import org.apache.hadoop.hive.ql.util.jdbm.RecordManagerOptions;
@@ -244,6 +246,7 @@ public void process(Object row, ObjectInspector rowInspector, int tag) throws Hi
         // This may potentially increase the size of the hashmap on the mapper
         if (res.size() > mapJoinRowsKey) {
           LOG.warn("Number of values for a given key " + keyObj + " are " + res.size());
+          LOG.warn("used memory " + Runtime.getRuntime().totalMemory());
         }
         
         hashTable.put(keyObj, valueObj);

File: ql/src/java/org/apache/hadoop/hive/ql/util/jdbm/recman/PhysicalRowIdManager.java
Patch:
@@ -190,7 +190,7 @@ private Location alloc( int size )
     {
         Location retval = freeman.get( size );
         if ( retval == null ) {
-            retval = allocNew( size, pageman.getLast( Magic.USED_PAGE ) );
+            retval = allocNew( 2*size, pageman.getLast( Magic.USED_PAGE ) );
         }
         return retval;
     }

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -379,6 +379,7 @@ public String getAuxJars() {
    */
   public void setAuxJars(String auxJars) {
     this.auxJars = auxJars;
+    setVar(this, ConfVars.HIVEAUXJARS, auxJars);
   }
   
   /**

File: hwi/src/java/org/apache/hadoop/hive/hwi/HWISessionItem.java
Patch:
@@ -9,7 +9,7 @@
 
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.cli.OptionsProcessor;
-import org.apache.hadoop.hive.cli.SetProcessor;
+import org.apache.hadoop.hive.ql.processors.SetProcessor;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.exec.ExecDriver;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Order;
@@ -296,7 +297,7 @@ private void validateCreateTable(createTableDesc crtTblDesc) throws SemanticExce
     }
     
     try {
-      Class<?> origin = Class.forName(crtTblDesc.getOutputFormat());
+      Class<?> origin = Class.forName(crtTblDesc.getOutputFormat(), true, JavaUtils.getClassLoader());
       Class<? extends HiveOutputFormat> replaced = HiveFileFormatUtils.getOutputFormatSubstitute(origin);
       if(replaced == null)
         throw new SemanticException(ErrorMsg.INVALID_OUTPUT_FORMAT_TYPE.getMsg());

File: ql/src/java/org/apache/hadoop/hive/ql/processors/CommandProcessor.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql;
+package org.apache.hadoop.hive.ql.processors;
 
 public interface CommandProcessor {
   public int run(String command);

File: ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java
Patch:
@@ -16,12 +16,11 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.cli;
+package org.apache.hadoop.hive.ql.processors;
 
 import java.util.*;
 
 import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.CommandProcessor;
 
 public class SetProcessor implements CommandProcessor {
 

File: serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.util.*;
 
+import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.serde.Constants;
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
@@ -55,7 +56,7 @@ public static Deserializer lookupDeserializer(String name) throws SerDeException
         c = serdes.get(name);
     } else {
       try {
-        c = Class.forName(name);
+        c = Class.forName(name, true, JavaUtils.getClassLoader());
       } catch(ClassNotFoundException e) {
         throw new SerDeException("SerDe " + name + " does not exist");
       }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
Patch:
@@ -53,7 +53,7 @@ public void process(Object row, ObjectInspector rowInspector, int tag) throws Hi
     else
       setDone(true);
   }
-
+  
   public String getName() {
     return "LIM";
   }

File: serde/src/test/org/apache/hadoop/hive/serde2/thrift_test/CreateSequenceFile.java
Patch:
@@ -106,7 +106,7 @@ public static void main(String[] args) throws Exception {
       ArrayList<String> slist = new ArrayList<String>();
       slist.add("" + i*10); slist.add("" + i*100); slist.add("" + i*1000);
       ArrayList<IntString> islist = new ArrayList<IntString>();
-      islist.add(new IntString(i*i, ""+ i*i*i));
+      islist.add(new IntString(i*i, ""+ i*i*i, i));
       HashMap<String,String> hash = new HashMap<String,String>();
       hash.put("key_" + i, "value_" + i);
       

File: serde/src/java/org/apache/hadoop/hive/serde2/thrift/TCTLSeparatedProtocol.java
Patch:
@@ -307,8 +307,6 @@ public TCTLSeparatedProtocol(TTransport trans, String primarySeparator, String s
     this.innerTransport = trans;
     this.bufferSize = bufferSize;
     this.nullString  = "\\N";
-
-    internalInitialize();
   }
 
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
Patch:
@@ -629,7 +629,6 @@ public boolean hasReduce() {
   }
 
   private void addInputPaths(JobConf job, mapredWork work, String hiveScratchDir) throws Exception {
-    FileSystem inpFs = FileSystem.get(job);
     int numEmptyPaths = 0;
     
     // If the query references non-existent partitions
@@ -672,6 +671,7 @@ private void addInputPaths(JobConf job, mapredWork work, String hiveScratchDir)
         
         // If the input file does not exist, replace it by a empty file
         Path dirPath = new Path(onefile);
+        FileSystem inpFs = dirPath.getFileSystem(job);
         boolean emptyInput = true;
         
         if (inpFs.exists(dirPath)) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java
Patch:
@@ -47,8 +47,8 @@ public class ExplainTask extends Task<explainWork> implements Serializable {
   public int execute() {
     
     try {
-    	OutputStream outS = FileSystem.get(conf).create(work.getResFile());
-    	PrintStream out = new PrintStream(outS);
+      OutputStream outS = work.getResFile().getFileSystem(conf).create(work.getResFile());
+      PrintStream out = new PrintStream(outS);
     	
       // Print out the parse AST
       outputAST(work.getAstStringTree(), out, 0);

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveIgnoreKeyTextOutputFormat.java
Patch:
@@ -76,8 +76,9 @@ public RecordWriter getHiveRecordWriter(JobConf jc, Path outPath,
     }
 
     final int finalRowSeparator = rowSeparator;
+    FileSystem fs = outPath.getFileSystem(jc);
     final OutputStream outStream = Utilities.createCompressedStream(jc,
-        FileSystem.get(jc).create(outPath), isCompressed);
+        fs.create(outPath), isCompressed);
     return new RecordWriter() {
       public void write(Writable r) throws IOException {
         if (r instanceof Text) {

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveSequenceFileOutputFormat.java
Patch:
@@ -62,8 +62,9 @@ public RecordWriter getHiveRecordWriter(JobConf jc, Path finalOutPath,
       Class<? extends Writable> valueClass, boolean isCompressed,
       Properties tableProperties, Progressable progress) throws IOException {
 
+    FileSystem fs = finalOutPath.getFileSystem(jc);
     final SequenceFile.Writer outStream = Utilities.createSequenceWriter(jc,
-        FileSystem.get(jc), finalOutPath, BytesWritable.class, valueClass,
+        fs, finalOutPath, BytesWritable.class, valueClass,
         isCompressed);
 
     return new RecordWriter() {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java
Patch:
@@ -34,7 +34,7 @@ public FunctionSemanticAnalyzer(HiveConf conf) throws SemanticException {
     super(conf);
   }
   
-  public void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException {
+  public void analyzeInternal(ASTNode ast) throws SemanticException {
     String functionName = ast.getChild(0).getText();
     String className = unescapeSQLString(ast.getChild(1).getText());
     createFunctionDesc desc = new createFunctionDesc(functionName, className);

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -745,13 +745,12 @@ public List<Task<? extends Serializable>> analyzeAST(ASTNode ast) throws Excepti
 
     // Do semantic analysis and plan generation
     Context ctx = new Context(conf);
-    ctx.makeScratchDir();
     while((ast.getToken() == null) && (ast.getChildCount() > 0)) {
       ast = (ASTNode)ast.getChild(0);
     }
     
     sem.analyze(ast, ctx);
-    ctx.removeScratchDir();
+    ctx.clear();
     return sem.getRootTasks();
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPruner.java
Patch:
@@ -108,6 +108,7 @@ private Operator genSelectPlan(Operator input, List<String> colNames)
     RowResolver inputRR  = pGraphContext.getOpParseCtx().get(input).getRR();
     RowResolver outputRR = new RowResolver();
     ArrayList<exprNodeDesc> col_list = new ArrayList<exprNodeDesc>();
+    Map<String, exprNodeDesc> colExprMap = new HashMap<String, exprNodeDesc>();
     
     // Iterate over the selects
     for (int pos = 0; pos < colNames.size(); pos++) {
@@ -117,11 +118,13 @@ private Operator genSelectPlan(Operator input, List<String> colNames)
       outputRR.put(colName[0], colName[1], 
                    new ColumnInfo((Integer.valueOf(pos)).toString(), in.getType()));
       col_list.add(new exprNodeColumnDesc(in.getType(), internalName));
+      colExprMap.put(Integer.toString(pos), col_list.get(pos));
     }
 
     Operator output = putOpInsertMap(OperatorFactory.getAndMakeChild(
       new selectDesc(col_list), new RowSchema(outputRR.getColumnInfos()), input), outputRR);
 
+    output.setColumnExprMap(colExprMap);
     return output;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
Patch:
@@ -482,7 +482,9 @@ public ReporterTask(Reporter rp) {
     
     @Override
     public void run() {
-      rp.progress();
+      if (rp != null) {
+        rp.progress();
+      }
     }
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java
Patch:
@@ -34,7 +34,7 @@ public Text evaluate(Text s) {
     if (s == null) {
       return null;
     }
-    t.set(s.toString().toUpperCase());
+    t.set(s.toString().toLowerCase());
     return t;
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -377,7 +377,7 @@ public static boolean isRedundantConversionFunction(ASTNode expr, boolean isFunc
       // not a conversion function 
       if (funcText == null) return false;
       // return true when the child type and the conversion target type is the same
-      return ((PrimitiveTypeInfo)children.get(0).getTypeInfo()).getPrimitiveWritableClass().getName().equals(funcText);
+      return ((PrimitiveTypeInfo)children.get(0).getTypeInfo()).getTypeName().equalsIgnoreCase(funcText);
     }
     
     public static String getFunctionText(ASTNode expr, boolean isFunction) {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExp.java
Patch:
@@ -38,7 +38,7 @@ public BooleanWritable evaluate(Text s, Text regex) {
     if (s == null || regex == null) {
       return null;
     }
-    if (!regex.equals(lastRegex)) {
+    if (!regex.equals(lastRegex) || p == null) {
       lastRegex.set(regex);
       p = Pattern.compile(regex.toString());
     }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpExtract.java
Patch:
@@ -43,7 +43,7 @@ public String evaluate(String s, String regex, Integer extractIndex) {
     if (s == null || regex == null) {
       return null;
     }
-    if (!regex.equals(lastRegex)) {
+    if (!regex.equals(lastRegex) || p == null) {
       lastRegex = regex;
       p = Pattern.compile(regex);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpReplace.java
Patch:
@@ -30,7 +30,7 @@ public class UDFRegExpReplace extends UDF {
   private Pattern p = null;
   
   private Text lastReplacement = new Text();
-  private String replacementString = null; 
+  private String replacementString = ""; 
 
   Text result = new Text();
   public UDFRegExpReplace() {
@@ -41,7 +41,7 @@ public Text evaluate(Text s, Text regex, Text replacement) {
       return null;
     }
     // If the regex is changed, make sure we compile the regex again.
-    if (!regex.equals(lastRegex)) {
+    if (!regex.equals(lastRegex) || p == null) {
       lastRegex.set(regex);
       p = Pattern.compile(regex.toString());
     }

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -243,14 +243,14 @@ public void createSources() throws Exception {
         part_spec.put("ds", ds);
         part_spec.put("hr", hr);
         // System.out.println("Loading partition with spec: " + part_spec);
-        db.createPartition(srcpart, part_spec);
+        //db.createPartition(srcpart, part_spec);
         fpath = new Path(testFiles, "kv1.txt");
         newfpath = new Path(tmppath, "kv1.txt");
         fs.copyFromLocalFile(false, true, fpath, newfpath);
         fpath = newfpath;
         //db.loadPartition(fpath, srcpart.getName(), part_spec, true);
         runLoadCmd("LOAD DATA INPATH '" +  newfpath.toString() +
-                   "' INTO TABLE srcpart PARTITION (ds='" + ds + "',hr='" + hr +"')");
+                   "' OVERWRITE INTO TABLE srcpart PARTITION (ds='" + ds + "',hr='" + hr +"')");
       }
     }
     ArrayList<String> bucketCols = new ArrayList<String>();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/CollectOperator.java
Patch:
@@ -39,8 +39,8 @@ public class CollectOperator extends Operator <collectDesc> implements Serializa
   transient protected ObjectInspector standardRowInspector;
   transient int maxSize;
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     rowList = new ArrayList<Object> ();
     maxSize = conf.getBufferSize().intValue();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecMapper.java
Patch:
@@ -54,7 +54,7 @@ public void map(Object key, Object value,
       try {
         oc = output;
         mo.setOutputCollector(oc);
-        mo.initialize(jc, reporter);
+        mo.initialize(jc, reporter, null);
         rp = reporter;
       } catch (HiveException e) {
         abort = true;
@@ -81,7 +81,7 @@ public void close() {
     if(oc == null) {
       try {
         l4j.trace("Close called no row");
-        mo.initialize(jc, null);
+        mo.initialize(jc, null, null);
         rp = null;
       } catch (HiveException e) {
         abort = true;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecReducer.java
Patch:
@@ -116,7 +116,7 @@ public void reduce(Object key, Iterator values,
       try {
         oc = output;
         reducer.setOutputCollector(oc);
-        reducer.initialize(jc, reporter);
+        reducer.initialize(jc, reporter, rowObjectInspector);
         rp = reporter;
       } catch (HiveException e) {
         abort = true;
@@ -196,7 +196,7 @@ public void close() {
     if(oc == null) {
       try {
         l4j.trace("Close called no row");
-        reducer.initialize(jc, null);
+        reducer.initialize(jc, null, rowObjectInspector);
         rp = null;
       } catch (HiveException e) {
         abort = true;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java
Patch:
@@ -35,8 +35,8 @@ public class ExtractOperator extends Operator<extractDesc> implements Serializab
   private static final long serialVersionUID = 1L;
   transient protected ExprNodeEvaluator eval;
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     eval = ExprNodeEvaluatorFactory.get(conf.getCol());
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
Patch:
@@ -47,8 +47,8 @@ public FilterOperator () {
     passed_count = new LongWritable();
   }
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     try {
       this.conditionEvaluator = ExprNodeEvaluatorFactory.get(conf.getPredicate());
       statsMap.put(Counter.FILTERED, filtered_count);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ForwardOperator.java
Patch:
@@ -32,8 +32,8 @@
  **/
 public class ForwardOperator extends  Operator<forwardDesc>  implements Serializable {
   private static final long serialVersionUID = 1L;
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     // nothing to do really ..
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
Patch:
@@ -131,8 +131,8 @@ List<Field> getFields() {
   transient int           numEntriesVarSize;
   transient int           numEntriesHashTable;
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     totalMemory = Runtime.getRuntime().totalMemory();
     numRowsInput = 0;
     numRowsHashTbl = 0;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/LimitOperator.java
Patch:
@@ -39,8 +39,8 @@ public class LimitOperator extends Operator<limitDesc> implements Serializable {
   transient protected int limit;
   transient protected int currCount;
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     limit = conf.getLimit();
     currCount = 0;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
Patch:
@@ -64,8 +64,8 @@ public static enum Counter {DESERIALIZE_ERRORS}
   transient private List<ObjectInspector> partObjectInspectors;
   
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     Path fpath = new Path((new Path (HiveConf.getVar(hconf, HiveConf.ConfVars.HADOOPMAPFILENAME))).toUri().getPath());
     ArrayList<Operator<? extends Serializable>> todo = new ArrayList<Operator<? extends Serializable>> ();
     statsMap.put(Counter.DESERIALIZE_ERRORS, deserialize_error_count);
@@ -171,7 +171,7 @@ public void initialize(Configuration hconf, Reporter reporter) throws HiveExcept
     this.setOutputCollector(out);
 
     for(Operator op: todo) {
-      op.initialize(hconf, reporter);
+      op.initialize(hconf, reporter, inputObjInspector);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
Patch:
@@ -240,7 +240,7 @@ public Map<Enum<?>, Long> getStats() {
     return(ret);
   }
 
-  public void initialize (Configuration hconf, Reporter reporter) throws HiveException {
+  public void initialize (Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
     if (state == state.INIT) {
       LOG.info("Already Initialized");
       return;
@@ -254,7 +254,7 @@ public void initialize (Configuration hconf, Reporter reporter) throws HiveExcep
     }
     LOG.info("Initializing children:");
     for(Operator<? extends Serializable> op: childOperators) {
-      op.initialize(hconf, reporter);
+      op.initialize(hconf, reporter, inputObjInspector);
     }    
 
     state = State.INIT;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
Patch:
@@ -69,8 +69,8 @@ public class ReduceSinkOperator extends TerminalOperator <reduceSinkDesc> implem
   transient int tag;
   transient byte[] tagByte = new byte[1];
   
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     try {
       keyEval = new ExprNodeEvaluator[conf.getKeyCols().size()];
       int i=0;
@@ -182,7 +182,7 @@ public void process(Object row, ObjectInspector rowInspector, int tag) throws Hi
         for(ExprNodeEvaluator e: partitionEval) {
           Object o = e.evaluate(row);
           keyHashCode = keyHashCode * 31 
-            + (o == null ? 0 : o.hashCode());
+            + (o == null ? 0 : Utilities.hashCode(o, keyObjectInspector));
         }
       }
       keyWritable.setHashCode(keyHashCode);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
Patch:
@@ -162,8 +162,8 @@ public File getAbsolutePath(String filename)
     }
   }
 
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     statsMap.put(Counter.DESERIALIZE_ERRORS, deserialize_error_count);
     statsMap.put(Counter.SERIALIZE_ERRORS, serialize_error_count);
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
Patch:
@@ -43,8 +43,8 @@ public class SelectOperator extends Operator <selectDesc> implements Serializabl
   
   boolean firstRow;
   
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
 
     ArrayList<exprNodeDesc> colList = conf.getColList();
     eval = new ExprNodeEvaluator[colList.size()];

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
Patch:
@@ -33,8 +33,8 @@
  **/
 public class TableScanOperator extends Operator<tableScanDesc> implements Serializable {
   private static final long serialVersionUID = 1L;
-  public void initialize(Configuration hconf, Reporter reporter) throws HiveException {
-    super.initialize(hconf, reporter);
+  public void initialize(Configuration hconf, Reporter reporter, ObjectInspector[] inputObjInspector) throws HiveException {
+    super.initialize(hconf, reporter, inputObjInspector);
     // nothing to do really ..
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
Patch:
@@ -77,6 +77,7 @@ public enum ErrorMsg {
   UNION_NOTIN_SUBQ("Top level Union is not supported currently; use a subquery for the union"),
   INVALID_INPUT_FORMAT_TYPE("Input Format must implement InputFormat"),
   INVALID_OUTPUT_FORMAT_TYPE("Output Format must implement HiveOutputFormat, otherwise it should be either IgnoreKeyTextOutputFormat or SequenceFileOutputFormat"),
+  NO_VALID_PARTN("The query does not reference any valid partition. To run this query, set hive.mapred.mode=nonstrict"),
   NON_BUCKETED_TABLE("Sampling Expression Needed for Non-Bucketed Table");
 
   private String mesg;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java
Patch:
@@ -87,15 +87,14 @@ public static <T extends Serializable> Task<T> get(Class<T> workClass, HiveConf
     
     if(workClass == mapredWork.class) {
 
-      String viachild = conf.getVar(HiveConf.ConfVars.SUBMITVIACHILD);
+      boolean viachild = conf.getBoolVar(HiveConf.ConfVars.SUBMITVIACHILD);
       
       try {
 
         // in local mode - or if otherwise so configured - always submit
         // jobs via separate jvm
         Task<T> ret = null;
-        if(conf.getVar(HiveConf.ConfVars.HADOOPJT).equals("local") ||
-           viachild.equals("true")) {
+        if(conf.getVar(HiveConf.ConfVars.HADOOPJT).equals("local") || viachild) {
           ret = (Task<T>)MapRedTask.class.newInstance();
         } else {
           ret = (Task<T>)ExecDriver.class.newInstance();

File: ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
Patch:
@@ -191,8 +191,10 @@ public void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException {
       // that the hive warehouse is also located in the local file system - but that's just a test case.
       URI copyURI;
       try {
+        // extract out the path name only from the scratchdir configuration
+        String scratchPath = (new Path(conf.getVar(HiveConf.ConfVars.SCRATCHDIR))).toUri().getPath();
         copyURI = new URI(toURI.getScheme(), toURI.getAuthority(),
-                          conf.getVar(HiveConf.ConfVars.SCRATCHDIR) + "/" + Utilities.randGen.nextInt(),
+                          scratchPath + "/" + Utilities.randGen.nextInt(),
                           null, null);                          
       } catch (URISyntaxException e) {
         // Has to use full name to make sure it does not conflict with org.apache.commons.lang.StringUtils

File: ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java
Patch:
@@ -39,8 +39,7 @@ public class TestHiveMetaStoreChecker extends TestCase {
   protected void setUp() throws Exception {
     super.setUp();
     hive = Hive.get();
-    fs = FileSystem.getLocal(hive.getConf());
-    checker = new HiveMetaStoreChecker(hive, fs);
+    checker = new HiveMetaStoreChecker(hive);
 
     partCols = new ArrayList<FieldSchema>();
     partCols.add(new FieldSchema(partDateName, Constants.STRING_TYPE_NAME, 
@@ -115,6 +114,7 @@ public void testTableCheck() throws HiveException, MetaException,
     assertTrue(result.getPartitionsNotInMs().isEmpty());
 
     // remove the table folder
+    fs = table.getPath().getFileSystem(hive.getConf());
     fs.delete(table.getPath(), true);
 
     // now this shouldn't find the path on the fs
@@ -185,6 +185,7 @@ public void testPartitionsCheck() throws HiveException, MetaException,
     assertEquals(2, partitions.size());
     Partition partToRemove = partitions.get(0);
     Path partToRemovePath = new Path(partToRemove.getDataLocation().toString());
+    fs = partToRemovePath.getFileSystem(hive.getConf());
     fs.delete(partToRemovePath, true);
 
     result = new CheckResult();    

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -124,11 +124,11 @@ public int execute() {
         }
 
         if(tbd.getPartitionSpec().size() == 0) {
-          db.loadTable(new Path(tbd.getSourceDir()), tbd.getTable().getTableName(), tbd.getReplace());
+          db.loadTable(new Path(tbd.getSourceDir()), tbd.getTable().getTableName(), tbd.getReplace(), new Path(tbd.getTmpDir()));
         } else {
           LOG.info("Partition is: " + tbd.getPartitionSpec().toString());
           db.loadPartition(new Path(tbd.getSourceDir()), tbd.getTable().getTableName(),
-              tbd.getPartitionSpec(), tbd.getReplace());
+              tbd.getPartitionSpec(), tbd.getReplace(), new Path(tbd.getTmpDir()));
         }
       }
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java
Patch:
@@ -303,13 +303,14 @@ public LinkedHashMap<String, String> getSpec() {
    * Replaces files in the partition with new data set specified by srcf. Works by moving files
    *
    * @param srcf Files to be moved. Leaf Directories or Globbed File Paths
+   * @param tmpd Temporary directory
    */
   @SuppressWarnings("nls")
-  protected void replaceFiles(Path srcf) throws HiveException {
+  protected void replaceFiles(Path srcf, Path tmpd) throws HiveException {
     FileSystem fs;
     try {
       fs = FileSystem.get(table.getDataLocation(), Hive.get().getConf());
-      Hive.get().replaceFiles(srcf, partPath, fs);
+      Hive.get().replaceFiles(srcf, partPath, fs, tmpd);
     } catch (IOException e) {
       throw new HiveException("addFiles: filesystem error in check phase", e);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
Patch:
@@ -445,12 +445,13 @@ public int getNumBuckets() {
   /**
    * Replaces files in the partition with new data set specified by srcf. Works by moving files
    * @param srcf Files to be replaced. Leaf directories or globbed file paths
+   * @param tmpd Temporary directory
    */
-  protected void replaceFiles(Path srcf) throws HiveException {
+  protected void replaceFiles(Path srcf, Path tmpd) throws HiveException {
     FileSystem fs;
     try {
       fs = FileSystem.get(getDataLocation(), Hive.get().getConf());
-      Hive.get().replaceFiles(srcf, new Path(getDataLocation().getPath()), fs);
+      Hive.get().replaceFiles(srcf, new Path(getDataLocation().getPath()), fs, tmpd);
     } catch (IOException e) {
       throw new HiveException("addFiles: filesystem error in check phase", e);
     }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
Patch:
@@ -208,7 +208,7 @@ public void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException {
     List<loadTableDesc> loadTableWork =  new ArrayList<loadTableDesc>();
     List<loadFileDesc> loadFileWork = new ArrayList<loadFileDesc>();
 
-    loadTableWork.add(new loadTableDesc(fromURI.toString(), Utilities.getTableDesc(ts.tableHandle),
+    loadTableWork.add(new loadTableDesc(fromURI.toString(), getTmpFileName(), Utilities.getTableDesc(ts.tableHandle),
                                         (ts.partSpec != null) ? ts.partSpec : new HashMap<String, String> (),
                                         isOverWrite));
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -2206,7 +2206,7 @@ private Operator genFileSinkPlan(String dest, QB qb,
 
         dest_path = dest_tab.getPath().toString();
         // Create the work for moving the table
-        this.loadTableWork.add(new loadTableDesc(queryTmpdir,
+        this.loadTableWork.add(new loadTableDesc(queryTmpdir, getTmpFileName(),
                                             table_desc,
                                             new HashMap<String, String>()));
         break;
@@ -2221,7 +2221,7 @@ private Operator genFileSinkPlan(String dest, QB qb,
         currentTableId = this.destTableId;
         this.destTableId ++;
         
-        this.loadTableWork.add(new loadTableDesc(queryTmpdir, table_desc, dest_part.getSpec()));
+        this.loadTableWork.add(new loadTableDesc(queryTmpdir, getTmpFileName(), table_desc, dest_part.getSpec()));
         break;
       }
     case QBMetaData.DEST_LOCAL_FILE:

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestExecDriver.java
Patch:
@@ -105,7 +105,7 @@ public class TestExecDriver extends TestCase {
       for(String src: srctables) {
         db.dropTable(src, true, true);
         db.createTable(src, cols, null, TextInputFormat.class, IgnoreKeyTextOutputFormat.class);
-        db.loadTable(hadoopDataFile[i], src, false);
+        db.loadTable(hadoopDataFile[i], src, false, null);
         i++;
       }
 

File: ql/src/test/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
Patch:
@@ -96,7 +96,7 @@ protected void setUp(){
         db.dropTable(src, true, true);
         db.createTable(src, cols, null, TextInputFormat.class,
             IgnoreKeyTextOutputFormat.class);
-        db.loadTable(hadoopDataFile[i], src, false);
+        db.loadTable(hadoopDataFile[i], src, false, null);
         i++;
       }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -324,7 +324,7 @@ public static mapredWork getMapRedWork() {
     mapredWork work = new mapredWork();
     work.setPathToAliases(new LinkedHashMap<String, ArrayList<String>>());
     work.setPathToPartitionInfo(new LinkedHashMap<String, partitionDesc>());
-    work.setAliasToWork(new HashMap<String, Operator<? extends Serializable>>());
+    work.setAliasToWork(new LinkedHashMap<String, Operator<? extends Serializable>>());
     work.setTagToValueDesc(new ArrayList<tableDesc>());
     work.setReducer(null);
     return work;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
Patch:
@@ -49,7 +49,7 @@ public static mapredWork getMapRedWork() {
     return new mapredWork("", 
                           new LinkedHashMap<String, ArrayList<String>> (),
                           new LinkedHashMap<String, partitionDesc> (),
-                          new HashMap<String, Operator<? extends Serializable>> (),
+                          new LinkedHashMap<String, Operator<? extends Serializable>> (),
                           new tableDesc(),
                           new ArrayList<tableDesc> (),
                           null,
@@ -251,4 +251,4 @@ public static reduceSinkDesc getReduceSinkDesc(ArrayList<exprNodeDesc> keyCols,
   
   
 }
-  
\ No newline at end of file
+  

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
Patch:
@@ -262,7 +262,7 @@ public void testMapOperator() throws Throwable {
       cdop1.setConf(cd);
       CollectOperator cdop2 = (CollectOperator) OperatorFactory.get(collectDesc.class);
       cdop2.setConf(cd);
-      HashMap<String,Operator<? extends Serializable>> aliasToWork = new HashMap<String,Operator<? extends Serializable>> ();
+      LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork = new LinkedHashMap<String,Operator<? extends Serializable>> ();
       aliasToWork.put("a", cdop1);
       aliasToWork.put("b", cdop2);
 

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestPlan.java
Patch:
@@ -61,7 +61,7 @@ public void testPlan() throws Exception {
       LinkedHashMap<String, partitionDesc> pt = new LinkedHashMap<String, partitionDesc> ();
       pt.put("/tmp/testfolder", partDesc);
 
-      HashMap<String, Operator<? extends Serializable>> ao = new HashMap<String, Operator<? extends Serializable>> ();
+      LinkedHashMap<String, Operator<? extends Serializable>> ao = new LinkedHashMap<String, Operator<? extends Serializable>> ();
       ao.put("a", op);
 
       mapredWork mrwork = new mapredWork();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
Patch:
@@ -68,14 +68,14 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx, Ob
       if (mapTask == null) {
         assert (!seenOps.contains(currTopOp));
         seenOps.add(currTopOp);
-        GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, (mapredWork) currTask.getWork(), false, ctx);
+        GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, null, (mapredWork) currTask.getWork(), false, ctx);
         opTaskMap.put(null, currTask);
         rootTasks.add(currTask);
       }
       else {
         if (!seenOps.contains(currTopOp)) {
           seenOps.add(currTopOp);
-          GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, (mapredWork) mapTask.getWork(), false, ctx);
+          GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, null, (mapredWork) mapTask.getWork(), false, ctx);
         }
         if (ret)
           currTask.removeDependentTask(mvTask);

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java
Patch:
@@ -144,7 +144,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx, Ob
 
     // If it is map-only task, add the files to be processed
     if (uPrsCtx.getMapOnlySubq(pos) && uPrsCtx.getRootTask(pos))
-      GenMapRedUtils.setTaskPlan(ctx.getCurrAliasId(), ctx.getCurrTopOp(), (mapredWork) currTask.getWork(), false, ctx);
+      GenMapRedUtils.setTaskPlan(ctx.getCurrAliasId(), ctx.getCurrTopOp(), union, (mapredWork) currTask.getWork(), false, ctx);
 
     ctx.setCurrTask(uTask);
     ctx.setCurrAliasId(null);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
Patch:
@@ -213,9 +213,9 @@ public void analyzeInternal(ASTNode ast, Context ctx) throws SemanticException {
                                         isOverWrite));
 
     if(rTask != null) {
-      rTask.addDependentTask(TaskFactory.get(new moveWork(loadTableWork, loadFileWork), this.conf));
+      rTask.addDependentTask(TaskFactory.get(new moveWork(loadTableWork, loadFileWork, true), this.conf));
     } else {
-      rTask = TaskFactory.get(new moveWork(loadTableWork, loadFileWork), this.conf);
+      rTask = TaskFactory.get(new moveWork(loadTableWork, loadFileWork, true), this.conf);
     }
 
     rootTasks.add(rTask);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -3437,7 +3437,7 @@ private void genMapRedTasks(QB qb) throws SemanticException {
     else {
       // First we generate the move work as this needs to be made dependent on all
       // the tasks that have a file sink operation
-      mv = new moveWork(loadTableWork, loadFileWork);
+      mv = new moveWork(loadTableWork, loadFileWork, false);
       mvTask = TaskFactory.get(mv, this.conf);
     }
 

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -123,6 +123,9 @@ public static enum ConfVars {
     HIVEHWILISTENPORT("hive.hwi.listen.port","9999"),
     HIVEHWIWARFILE("hive.hwi.war.file",System.getenv("HIVE_HOME")+"/lib/hive_hwi.war"),
 
+    // mapper/reducer memory in local mode
+    HIVEHADOOPMAXMEM("hive.mapred.local.mem", 0),
+
     // Optimizer
     HIVEOPTPPD("hive.optimize.ppd", false); // predicate pushdown
     

File: jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveResultSet.java
Patch:
@@ -981,7 +981,7 @@ public void moveToInsertRow() throws SQLException {
    */
 
   public boolean next() throws SQLException {
-    String row_str;
+    String row_str = "";
     try {
       row_str = (String)client.fetchOne();
       if (!row_str.equals("")) {
@@ -992,7 +992,7 @@ public boolean next() throws SQLException {
       throw new SQLException("Error retrieving next row");
     }
     // NOTE: fetchOne dosn't throw new SQLException("Method not supported").
-    return row_str != "";
+    return !row_str.equals("");
   }
 
   /* (non-Javadoc)

File: ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorFactory.java
Patch:
@@ -50,6 +50,7 @@ public opTuple(Class<T> descClass, Class<? extends Operator<T>> opClass) {
     opvec.add(new opTuple<joinDesc> (joinDesc.class, JoinOperator.class));
     opvec.add(new opTuple<limitDesc> (limitDesc.class, LimitOperator.class));
     opvec.add(new opTuple<tableScanDesc> (tableScanDesc.class, TableScanOperator.class));
+    opvec.add(new opTuple<unionDesc> (unionDesc.class, UnionOperator.class));
   }
               
 

File: ql/src/java/org/apache/hadoop/hive/ql/lib/DefaultRuleDispatcher.java
Patch:
@@ -77,7 +77,7 @@ public Object dispatch(Node nd, Stack<Node> ndStack, Object... nodeOutputs)
     // Do nothing in case proc is null
     if (proc != null) {
       // Call the process function
-      return proc.process(nd, procCtx, nodeOutputs);
+      return proc.process(nd, ndStack, procCtx, nodeOutputs);
     }
     else
       return null;

File: ql/src/java/org/apache/hadoop/hive/ql/lib/NodeProcessor.java
Patch:
@@ -18,6 +18,8 @@
 package org.apache.hadoop.hive.ql.lib;
 
 
+import java.util.Stack;
+
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 
 /**
@@ -34,6 +36,6 @@ public interface NodeProcessor {
    * @return Object to be returned by the process call
    * @throws SemanticException
    */
-  public Object process(Node nd, NodeProcessorCtx procCtx, Object... nodeOutputs) 
+  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) 
     throws SemanticException;
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
Patch:
@@ -20,10 +20,10 @@
 
 import java.util.List;
 import java.util.HashMap;
+import java.util.Stack;
 import java.io.Serializable;
 
 import org.apache.hadoop.hive.ql.exec.Operator;
-import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.plan.mapredWork;
 import org.apache.hadoop.hive.ql.lib.Node;
@@ -44,8 +44,7 @@ public GenMRFileSink1() {
    * @param nd the file sink operator encountered
    * @param opProcCtx context
    */
-  public Object process(Node nd, NodeProcessorCtx opProcCtx, Object... nodeOutputs) throws SemanticException {
-    FileSinkOperator op = (FileSinkOperator)nd;
+  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx, Object... nodeOutputs) throws SemanticException {
     GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
     boolean ret = false;
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMROperator.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.io.Serializable;
 import java.util.Map;
+import java.util.Stack;
 
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
@@ -41,7 +42,7 @@ public GenMROperator() {
    * @param nd the reduce sink operator encountered
    * @param procCtx context
    */
-  public Object process(Node nd, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {
+  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {
     GenMRProcContext ctx = (GenMRProcContext)procCtx;
 
     Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx = ctx.getMapCurrCtx();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.util.Map;
 import java.util.HashMap;
+import java.util.Stack;
 import java.io.Serializable;
 
 import org.apache.hadoop.hive.ql.exec.Operator;
@@ -45,7 +46,7 @@ public GenMRRedSink1() {
    * @param nd the reduce sink operator encountered
    * @param opProcCtx context
    */
-  public Object process(Node nd, NodeProcessorCtx opProcCtx, Object... nodeOutputs) throws SemanticException {
+  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx, Object... nodeOutputs) throws SemanticException {
     ReduceSinkOperator op = (ReduceSinkOperator)nd;
     GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.io.Serializable;
 import java.util.Map;
+import java.util.Stack;
 
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
@@ -43,7 +44,7 @@ public GenMRRedSink2() {
    * @param nd the reduce sink operator encountered
    * @param opProcCtx context
    */
-  public Object process(Node nd, NodeProcessorCtx opProcCtx, Object... nodeOutputs) throws SemanticException {
+  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx, Object... nodeOutputs) throws SemanticException {
     ReduceSinkOperator op = (ReduceSinkOperator)nd;
     GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.io.Serializable;
 import java.util.Map;
+import java.util.Stack;
 
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
@@ -44,7 +45,7 @@ public GenMRTableScan1() {
    * @param nd the table sink operator encountered
    * @param opProcCtx context
    */
-  public Object process(Node nd, NodeProcessorCtx opProcCtx, Object... nodeOutputs) throws SemanticException {
+  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx, Object... nodeOutputs) throws SemanticException {
     TableScanOperator op = (TableScanOperator)nd;
     GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
     ParseContext parseCtx = ctx.getParseCtx();

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
Patch:
@@ -233,7 +233,7 @@ public static void setTaskPlan(String alias_id, Operator<? extends Serializable>
    * @param plan     current plan
    * @param topOp    current top operator in the path
    */
-  private static void setKeyAndValueDesc(mapredWork plan, Operator<? extends Serializable> topOp) {
+  public static void setKeyAndValueDesc(mapredWork plan, Operator<? extends Serializable> topOp) {
     if (topOp instanceof ReduceSinkOperator) {
       ReduceSinkOperator rs = (ReduceSinkOperator)topOp;
       plan.setKeyDesc(rs.getConf().getKeySerializeInfo());

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java
Patch:
@@ -23,6 +23,7 @@
 
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcessor;
 
 /**
  * Implementation of the optimizer
@@ -43,6 +44,7 @@ public Optimizer() {
 	public void initialize() {
 		transformations = new ArrayList<Transform>();
 		transformations.add(new ColumnPruner());
+		transformations.add(new UnionProcessor());
 	}
 	
 	/**

File: ql/src/java/org/apache/hadoop/hive/ql/parse/PrintOpTreeProcessor.java
Patch:
@@ -21,6 +21,8 @@
 import java.io.PrintStream;
 import java.io.Serializable;
 import java.util.HashMap;
+import java.util.Stack;
+
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.lib.NodeProcessor;
@@ -70,7 +72,7 @@ private String getChildren(Operator<? extends Serializable> op) {
     return ret.toString();
   }
   
-  public Object process(Node nd, NodeProcessorCtx ctx, Object... nodeOutputs) throws SemanticException {
+  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object... nodeOutputs) throws SemanticException {
     Operator<? extends Serializable> op = (Operator<? extends Serializable>)nd;
     if (opMap.get(op) == null) {
       opMap.put(op, curNum++);

File: ql/src/java/org/apache/hadoop/hive/ql/tools/LineageInfo.java
Patch:
@@ -23,6 +23,7 @@
 import java.util.ArrayList;
 import java.util.LinkedHashMap;
 import java.util.Map;
+import java.util.Stack;
 import java.util.TreeSet;
 
 import org.apache.hadoop.hive.ql.lib.DefaultGraphWalker;
@@ -76,7 +77,7 @@ public TreeSet<String> getOutputTableList() {
   /**
    * Implements the process method for the NodeProcessor interface.
    */
-  public Object process(Node nd, NodeProcessorCtx procCtx, Object... nodeOutputs)
+  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs)
   throws SemanticException {
     ASTNode pt = (ASTNode)nd;
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnInfo.java
Patch:
@@ -21,8 +21,8 @@
 import java.lang.Class;
 import java.io.*;
 
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * Implementation for ColumnInfo which contains the internal name for the 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
Patch:
@@ -337,7 +337,9 @@ public int execute() {
     job.setMapperClass(ExecMapper.class);
 
     job.setMapOutputKeyClass(HiveKey.class);
-    job.setMapOutputValueClass(BytesWritable.class);
+    // LazySimpleSerDe writes to Text
+    // Revert to DynamicSerDe: job.setMapOutputValueClass(BytesWritable.class); 
+    job.setMapOutputValueClass(Text.class);
 
     job.setNumReduceTasks(work.getNumReduceTasks().intValue());
     job.setReducerClass(ExecReducer.class);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
Patch:
@@ -37,8 +37,8 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.parse.OpParseContext;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.ql.typeinfo.PrimitiveTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.commons.logging.Log;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/InputSignature.java
Patch:
@@ -23,8 +23,8 @@
 import java.lang.Object;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 /**
  * The input signature of a function or operator. The signature basically consists

File: ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java
Patch:
@@ -30,11 +30,11 @@
 import org.apache.hadoop.hive.ql.plan.exprNodeFuncDesc;
 import org.apache.hadoop.hive.ql.plan.exprNodeIndexDesc;
 import org.apache.hadoop.hive.ql.plan.exprNodeNullDesc;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfoUtils;
 import org.apache.hadoop.hive.ql.udf.UDFOPAnd;
 import org.apache.hadoop.hive.ql.udf.UDFOPNot;
 import org.apache.hadoop.hive.ql.udf.UDFOPOr;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 import org.apache.hadoop.hive.ql.udf.UDFType;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
Patch:
@@ -38,8 +38,8 @@
 import org.apache.hadoop.hive.ql.plan.exprNodeFuncDesc;
 import org.apache.hadoop.hive.ql.plan.exprNodeIndexDesc;
 import org.apache.hadoop.hive.ql.plan.exprNodeNullDesc;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.hive.ql.udf.UDFOPPositive;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeColumnDesc.java
Patch:
@@ -22,8 +22,8 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 public class exprNodeColumnDesc extends exprNodeDesc implements Serializable {
   private static final long serialVersionUID = 1L;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeConstantDesc.java
Patch:
@@ -20,8 +20,8 @@
 
 import java.io.Serializable;
 
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 
 public class exprNodeConstantDesc extends exprNodeDesc implements Serializable {

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeDesc.java
Patch:
@@ -21,7 +21,7 @@
 import java.io.Serializable;
 import java.util.List;
 
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 
 public class exprNodeDesc implements Serializable {  
   private static final long serialVersionUID = 1L;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeFieldDesc.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.parse.RowResolver;
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeFuncDesc.java
Patch:
@@ -23,7 +23,7 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.ql.exec.FunctionInfo;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.UDF;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeIndexDesc.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.parse.RowResolver;
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeNullDesc.java
Patch:
@@ -20,7 +20,7 @@
 
 import java.io.Serializable;
 
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 public class exprNodeNullDesc extends exprNodeDesc implements Serializable {
   

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestExecDriver.java
Patch:
@@ -38,8 +38,8 @@
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.plan.PlanUtils.ExpressionTypes;
 import org.apache.hadoop.hive.ql.plan.*;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 
 import org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestExpressionEvaluator.java
Patch:
@@ -31,9 +31,9 @@
 import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.exprNodeIndexDesc;
 import org.apache.hadoop.hive.ql.plan.PlanUtils.ExpressionTypes;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfoUtils;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 
 public class TestExpressionEvaluator extends TestCase {

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
 import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
 import org.apache.hadoop.hive.ql.plan.*;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestPlan.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
 import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
 import org.apache.hadoop.hive.ql.plan.*;
-import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;
 import org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe;

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardMapObjectInspector.java
Patch:
@@ -59,7 +59,8 @@ public Object getMapValueElement(Object data, Object key) {
     Map<?,?> map = (Map<?,?>)data;
     return map.get(key);
   }
-  int getMapSize(Object data) {
+  
+  public int getMapSize(Object data) {
     if (data == null) return -1;
     Map<?,?> map = (Map<?,?>)data;
     return map.size();

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/ListTypeInfo.java
Patch:
@@ -16,10 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.typeinfo;
+package org.apache.hadoop.hive.serde2.typeinfo;
 
 import java.io.Serializable;
-import java.util.List;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 
 /** A List Type has homogeneous elements.  All elements of the List has

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/MapTypeInfo.java
Patch:
@@ -16,10 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.typeinfo;
+package org.apache.hadoop.hive.serde2.typeinfo;
 
 import java.io.Serializable;
-import java.util.List;
 
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 

File: serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfo.java
Patch:
@@ -16,9 +16,10 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hive.ql.typeinfo;
+package org.apache.hadoop.hive.serde2.typeinfo;
 
-import java.io.Serializable;import java.util.List;
+import java.io.Serializable;
+import java.util.List;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 
 /**

File: serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazySimpleSerDe.java
Patch:
@@ -183,5 +183,4 @@ public void testLazySimpleSerDeMissingColumns() throws Throwable {
     }
   }
   
-  
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -170,11 +170,11 @@ public void doPhase1QBExpr(ASTNode ast, QBExpr qbexpr, String id,
     }
   }
 
-  private HashMap<String, ASTNode> doPhase1GetAggregationsFromSelect(
+  private LinkedHashMap<String, ASTNode> doPhase1GetAggregationsFromSelect(
       ASTNode selExpr) {
     // Iterate over the selects search for aggregation Trees.
     // Use String as keys to eliminate duplicate trees.
-    HashMap<String, ASTNode> aggregationTrees = new HashMap<String, ASTNode>();
+    LinkedHashMap<String, ASTNode> aggregationTrees = new LinkedHashMap<String, ASTNode>();
     for (int i = 0; i < selExpr.getChildCount(); ++i) {
       ASTNode sel = (ASTNode) selExpr.getChild(i).getChild(0);
       doPhase1GetAllAggregations(sel, aggregationTrees);
@@ -348,7 +348,7 @@ public void doPhase1(ASTNode ast, QB qb, Phase1Ctx ctx_1)
       case HiveParser.TOK_SELECT:
         qb.countSel();
         qbp.setSelExprForClause(ctx_1.dest, ast);
-        HashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast);
+        LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast);
         qbp.setAggregationExprsForClause(ctx_1.dest, aggregations);
         qbp.setDistinctFuncExprForClause(ctx_1.dest,
             doPhase1GetDistinctFuncExpr(aggregations));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/UDF.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
+import org.apache.hadoop.hive.ql.udf.UDFType;
+
 /**
  * A User-defined function (UDF) for the use with Hive.
  * 
@@ -33,6 +35,7 @@
  * 
  *    "evaluate" should never be a void method.  However it can return "null" if needed.
  */
+@UDFType(deterministic=true)
 public class UDF {
   
   /**

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRand.java
Patch:
@@ -22,7 +22,9 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.ql.exec.UDF;
 import java.util.Random;
+import org.apache.hadoop.hive.ql.udf.UDFType;
 
+@UDFType(deterministic=false)
 public class UDFRand extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFRand.class.getName());

File: service/src/java/org/apache/hadoop/hive/service/HiveServer.java
Patch:
@@ -200,8 +200,8 @@ public String getVersion() {
   public static void main(String[] args) {
     try {
       int port = 10000;
-      if (args.length > 1) {
-        port = Integer.getInteger(args[0]);
+      if (args.length >= 1) {
+        port = Integer.parseInt(args[0]);
       }
       TServerTransport serverTransport = new TServerSocket(port);
       Iface handler = new HiveServerHandler();

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -75,7 +75,8 @@ public void close(boolean abort) throws HiveException {
           outWriter.close(abort);
           commit();
         } catch (IOException e) {
-          throw new HiveException("Error in committing output in file: "+ outPath.toString());
+          // Don't throw an exception, just ignore and return
+          return;
         }
       }
     } else {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
Patch:
@@ -191,7 +191,8 @@ public void close(boolean abort) throws IOException {
   Writable recordValue; 
   public void process(Object row, ObjectInspector rowInspector) throws HiveException {
     try {
-      reporter.progress();
+      if (reporter != null)
+        reporter.progress();
       // user SerDe to serialize r, and write it out
       recordValue = serializer.serialize(row, rowInspector);
       if (row_count != null){

File: ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
Patch:
@@ -407,10 +407,10 @@ public void process(Object row, ObjectInspector rowInspector) throws HiveExcepti
     // Total number of input rows is needed for hash aggregation only
     if (hashAggr) {
       numRowsInput++;
-      // if hash aggregation is not behvaing properly, disable it
+      // if hash aggregation is not behaving properly, disable it
       if (numRowsInput == numRowsCompareHashAggr) {
         numRowsCompareHashAggr += groupbyMapAggrInterval;
-        // map-side aggregation should reduce the entries by atleast half
+        // map-side aggregation should reduce the entries by at-least half
         if ((numRowsHashTbl * 2) > numRowsInput) {
           LOG.warn("Disable Hash Aggr: #hash table = " + numRowsHashTbl + " #total = " + numRowsInput);
           flush(true);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
Patch:
@@ -99,7 +99,7 @@ public List<Operator<? extends Serializable>> getParentOperators() {
     return parentOperators;
   }
 
-  protected String id;
+  transient protected String id;
   protected T conf;
   protected boolean done;
 

File: ql/src/java/org/apache/hadoop/hive/ql/plan/groupByDesc.java
Patch:
@@ -28,7 +28,7 @@ public class groupByDesc implements java.io.Serializable {
    *  HASH: the same as PARTIAL1 but use hash-table-based aggregation  
    */
   private static final long serialVersionUID = 1L;
-  public static enum Mode { COMPLETE, PARTIAL1, PARTIAL2, FINAL, HASH };
+  public static enum Mode { COMPLETE, PARTIAL1, PARTIAL2, FINAL, HASH, MERGEPARTIAL };
   private Mode mode;
   private java.util.ArrayList<exprNodeDesc> keys;
   private java.util.ArrayList<org.apache.hadoop.hive.ql.plan.aggregationDesc> aggregators;
@@ -63,6 +63,8 @@ public String getModeString() {
       return "hash";
     case FINAL:
       return "final";
+    case MERGEPARTIAL:
+      return "mergepartial";
     }
   
     return "unknown";

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -267,7 +267,7 @@ public int run(String command) {
       if (SessionState.get() != null){
         SessionState.get().getHiveHistory().setQueryProperty(queryId,
             Keys.QUERY_RET_CODE, String.valueOf(0));
-        SessionState.get().getHiveHistory().printRowCount();
+        SessionState.get().getHiveHistory().printRowCount(queryId);
       }
     } catch (SemanticException e) {
       if (SessionState.get() != null)

File: ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
Patch:
@@ -516,7 +516,9 @@ private boolean shouldBeFlushed(ArrayList<Object> newKeys) {
     if ((numEntriesHashTable == 0) || ((numEntries % NUMROWSESTIMATESIZE) == 0)) {
       for (Integer pos : keyPositionsSize) {
         Object key = newKeys.get(pos.intValue());
-        totalVariableSize += ((String)key).length();
+        // Ignore nulls
+        if (key != null)
+          totalVariableSize += ((String)key).length();
       }
 
       UDAFEvaluator[] aggs = null;

File: metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreClient.java
Patch:
@@ -255,7 +255,6 @@ public Properties getSchema(String tableName) throws MetaException, TException,
           String val = (String)e.getValue();
           p.setProperty(key,val);
         }
-        p = MetaStoreUtils.hive1Tohive3ClassNames(p);
         this.close();
         return p;
       } catch(TException e) {

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java
Patch:
@@ -38,6 +38,7 @@
 import org.apache.hadoop.hive.ql.plan.tableDesc;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe;
+import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
@@ -73,7 +74,7 @@ public void initialize (HiveConf conf) {
        fs = FileSystem.get(conf);   
        job = new JobConf(conf, ExecDriver.class);
        
-	 	   mSerde = new MetadataTypedColumnsetSerDe();
+	 	   mSerde = new LazySimpleSerDe();
        Properties mSerdeProp = new Properties();
        mSerdeProp.put(Constants.SERIALIZATION_FORMAT, "" + Utilities.tabCode);
        mSerdeProp.put(Constants.SERIALIZATION_NULL_FORMAT, "NULL");
@@ -135,7 +136,7 @@ static InputFormat<WritableComparable, Writable> getInputFormatFromCache(Class i
 	private WritableComparable key; 
 	private Writable value;
 	private Deserializer  serde;
-	private MetadataTypedColumnsetSerDe mSerde;
+	private LazySimpleSerDe mSerde;
 	private int totalRows;
   private Iterator<Path> iterPath;
   private Iterator<partitionDesc> iterPartDesc; 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
Patch:
@@ -45,6 +45,7 @@
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
+import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.io.Writable;
@@ -107,6 +108,8 @@ public Table(String name) {
     initEmpty();
     getTTable().setTableName(name);
     getTTable().setDbName(MetaStoreUtils.DEFAULT_DATABASE_NAME);
+    // We have to use MetadataTypedColumnsetSerDe because LazySimpleSerDe does not 
+    // support a table with no columns.
     getSerdeInfo().setSerializationLib(MetadataTypedColumnsetSerDe.class.getName());
     getSerdeInfo().getParameters().put(Constants.SERIALIZATION_FORMAT, "1");
   }

File: ql/src/test/org/apache/hadoop/hive/ql/plan/TestAddPartition.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.serde.Constants;
 import org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe;
+import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 
 public class TestAddPartition extends TestCase {
 
@@ -66,7 +67,7 @@ public void testAddPartition() throws Exception {
       StorageDescriptor sd = new StorageDescriptor();
       sd.setSerdeInfo(new SerDeInfo());
       sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setSerializationLib(MetadataTypedColumnsetSerDe.class.getName());
+      sd.getSerdeInfo().setSerializationLib(LazySimpleSerDe.class.getName());
 
       List<FieldSchema> fss = new ArrayList<FieldSchema>();
       fss.add(new FieldSchema("name", Constants.STRING_TYPE_NAME, ""));

File: serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
Patch:
@@ -84,6 +84,7 @@ protected static boolean registerCoreSerDes() {
     try {
       // loading these classes will automatically register the short names
       Class.forName(org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.class.getName());
+      Class.forName(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
       Class.forName(org.apache.hadoop.hive.serde2.ThriftDeserializer.class.getName());
     } catch (ClassNotFoundException e) {
       throw new RuntimeException("IMPOSSIBLE Exception: Unable to initialize core serdes", e);

File: metastore/src/gen-javabean/org/apache/hadoop/hive/metastore/api/Constants.java
Patch:
@@ -17,6 +17,8 @@ public class Constants {
 
   public static final String META_TABLE_COLUMNS = "columns";
 
+  public static final String META_TABLE_COLUMN_TYPES = "columns.types";
+
   public static final String BUCKET_FIELD_NAME = "bucket_field_name";
 
   public static final String BUCKET_COUNT = "bucket_count";

File: metastore/src/gen-javabean/org/apache/hadoop/hive/metastore/api/StorageDescriptor.java
Patch:
@@ -43,8 +43,6 @@ public static final class Isset implements java.io.Serializable {
 }
 
 public StorageDescriptor() {
-this.numBuckets = 32;
-
 }
 
 public StorageDescriptor(

File: serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
Patch:
@@ -70,12 +70,12 @@ public static Deserializer lookupDeserializer(String name) throws SerDeException
     nativeSerDeNames.add(org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.class.getName());
     // For backward compatibility
     nativeSerDeNames.add("org.apache.hadoop.hive.serde.thrift.columnsetSerDe");
+    nativeSerDeNames.add(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());
   }
 
   public static boolean isNativeSerDe(String serde) {
     return nativeSerDeNames.contains(serde);
   }
-  
 
   private static boolean initCoreSerDes = registerCoreSerDes();
   

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -416,7 +416,8 @@ public static Object invoke(Method m, Object thisObject, Object[] arguments) thr
         }
         argumentString.append("} of size " + arguments.length);
       }
-      
+     
+      e.printStackTrace();
       throw new HiveException("Unable to execute method " + m + " " 
           + " on object " + thisObjectString
           + " with arguments " + argumentString.toString() 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFMin.java
Patch:
@@ -240,7 +240,7 @@ public boolean iterate(String o) {
         if (mEmpty) {
           mMin = o;
           mEmpty = false;
-        } else if (mMin.compareTo(o) < 0) {
+        } else if (mMin.compareTo(o) > 0) {
           mMin = o;
         }
       }
@@ -279,7 +279,7 @@ public boolean iterate(Date o) {
         if (mEmpty) {
           mMin = o;
           mEmpty = false;
-        } else if (mMin.compareTo(o) < 0){
+        } else if (mMin.compareTo(o) > 0){
           mMin = o;
         }
       }

File: ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
Patch:
@@ -650,7 +650,7 @@ public int checkCliDriverResults(String tname) throws Exception {
     cmdArray[4] = (new File(logDir, tname + ".out")).getPath();
     cmdArray[5] = (new File(outDir, tname + ".out")).getPath();
     System.out.println(cmdArray[0] + " " + cmdArray[1] + " " + cmdArray[2] + " " +
-                       cmdArray[3] + " " + cmdArray[4]);
+                       cmdArray[3] + " " + cmdArray[4] + " " + cmdArray[5]);
 
     Process executor = Runtime.getRuntime().exec(cmdArray);
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -2618,8 +2618,8 @@ private void mergeJoins(QB qb, QBJoinTree parent, QBJoinTree node,
       if (nodeCondn.getLeft() == 0)
         nodeCondn.setLeft(pos);
       else
-        nodeCondn.setLeft(nodeCondn.getLeft() + targetCondnsSize - 1);
-      nodeCondn.setRight(nodeCondn.getRight() + targetCondnsSize - 1);
+        nodeCondn.setLeft(nodeCondn.getLeft() + targetCondnsSize);
+      nodeCondn.setRight(nodeCondn.getRight() + targetCondnsSize);
       newCondns[targetCondnsSize + i] = nodeCondn;
     }
 

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java
Patch:
@@ -110,7 +110,7 @@ void findUnknownTables(String dbName, List<String> tables,
       Table table = hive.getTable(dbName, tableName);
       // hack, instead figure out a way to get the db paths
       String isExternal = table.getParameters().get("EXTERNAL");
-      if (isExternal == null || "TRUE".equalsIgnoreCase(isExternal)) {
+      if (isExternal == null || !"TRUE".equalsIgnoreCase(isExternal)) {
         dbPaths.add(table.getPath().getParent());
       }
     }

File: ql/src/java/org/apache/hadoop/hive/ql/lib/NodeProcessor.java
Patch:
@@ -30,8 +30,10 @@ public interface NodeProcessor {
    * generic process for all ops that don't have specific implementations
    * @param nd operator to process
    * @param procCtx operator processor context
+   * @param nodeOutputs A variable argument list of outputs from other nodes in the walk
+   * @return Object to be returned by the process call
    * @throws SemanticException
    */
-  public void process(Node nd, NodeProcessorCtx procCtx) 
+  public Object process(Node nd, NodeProcessorCtx procCtx, Object... nodeOutputs) 
     throws SemanticException;
 }

File: ql/src/java/org/apache/hadoop/hive/ql/lib/NodeProcessorCtx.java
Patch:
@@ -21,5 +21,5 @@
 /**
  * Operator Processor Context
  */
-public abstract class NodeProcessorCtx {
+public interface NodeProcessorCtx {
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPruner.java
Patch:
@@ -171,7 +171,7 @@ public ParseContext transform(ParseContext pactx) throws SemanticException {
     // Create a list of topop nodes
     ArrayList<Node> topNodes = new ArrayList<Node>();
     topNodes.addAll(pGraphContext.getTopOps().values());
-    ogw.startWalking(topNodes);
+    ogw.startWalking(topNodes, null);
 
     // create a new select operator if any of input tables' columns can be pruned
     for (String alias_id : pGraphContext.getTopOps().keySet()) {

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcCtx.java
Patch:
@@ -36,7 +36,7 @@
 /**
  * This class implements the processor context for Column Pruner.
  */
-public class ColumnPrunerProcCtx extends NodeProcessorCtx {
+public class ColumnPrunerProcCtx implements NodeProcessorCtx {
   
   private  Map<Operator<? extends Serializable>,List<String>> prunedColLists;
   

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
Patch:
@@ -44,7 +44,7 @@ public GenMRFileSink1() {
    * @param nd the file sink operator encountered
    * @param opProcCtx context
    */
-  public void process(Node nd, NodeProcessorCtx opProcCtx) throws SemanticException {
+  public Object process(Node nd, NodeProcessorCtx opProcCtx, Object... nodeOutputs) throws SemanticException {
     FileSinkOperator op = (FileSinkOperator)nd;
     GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
     boolean ret = false;
@@ -82,5 +82,6 @@ public void process(Node nd, NodeProcessorCtx opProcCtx) throws SemanticExceptio
           currTask.removeDependentTask(mvTask);
       }
     }
+    return null;
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMROperator.java
Patch:
@@ -41,10 +41,11 @@ public GenMROperator() {
    * @param nd the reduce sink operator encountered
    * @param procCtx context
    */
-  public void process(Node nd, NodeProcessorCtx procCtx) throws SemanticException {
+  public Object process(Node nd, NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {
     GenMRProcContext ctx = (GenMRProcContext)procCtx;
 
     Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx = ctx.getMapCurrCtx();
     mapCurrCtx.put((Operator<? extends Serializable>)nd, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(), ctx.getCurrAliasId()));
+    return null;
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java
Patch:
@@ -55,7 +55,7 @@
  * Processor Context for creating map reduce task. Walk the tree in a DFS manner and process the nodes. Some state is 
  * maintained about the current nodes visited so far.
  */
-public class GenMRProcContext extends NodeProcessorCtx {
+public class GenMRProcContext implements NodeProcessorCtx {
 
   /** 
    * GenMapRedCtx is used to keep track of the current state. 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java
Patch:
@@ -45,7 +45,7 @@ public GenMRRedSink1() {
    * @param nd the reduce sink operator encountered
    * @param opProcCtx context
    */
-  public void process(Node nd, NodeProcessorCtx opProcCtx) throws SemanticException {
+  public Object process(Node nd, NodeProcessorCtx opProcCtx, Object... nodeOutputs) throws SemanticException {
     ReduceSinkOperator op = (ReduceSinkOperator)nd;
     GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
 
@@ -79,6 +79,7 @@ public void process(Node nd, NodeProcessorCtx opProcCtx) throws SemanticExceptio
     }
 
     mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(), ctx.getCurrAliasId()));
+    return null;
   }
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java
Patch:
@@ -43,7 +43,7 @@ public GenMRRedSink2() {
    * @param nd the reduce sink operator encountered
    * @param opProcCtx context
    */
-  public void process(Node nd, NodeProcessorCtx opProcCtx) throws SemanticException {
+  public Object process(Node nd, NodeProcessorCtx opProcCtx, Object... nodeOutputs) throws SemanticException {
     ReduceSinkOperator op = (ReduceSinkOperator)nd;
     GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
 
@@ -69,6 +69,7 @@ public void process(Node nd, NodeProcessorCtx opProcCtx) throws SemanticExceptio
     }
 
     mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(), ctx.getCurrAliasId()));
+    return null;
   }
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
Patch:
@@ -44,7 +44,7 @@ public GenMRTableScan1() {
    * @param nd the table sink operator encountered
    * @param opProcCtx context
    */
-  public void process(Node nd, NodeProcessorCtx opProcCtx) throws SemanticException {
+  public Object process(Node nd, NodeProcessorCtx opProcCtx, Object... nodeOutputs) throws SemanticException {
     TableScanOperator op = (TableScanOperator)nd;
     GenMRProcContext ctx = (GenMRProcContext)opProcCtx;
     ParseContext parseCtx = ctx.getParseCtx();
@@ -62,10 +62,11 @@ public void process(Node nd, NodeProcessorCtx opProcCtx) throws SemanticExceptio
         String currAliasId = alias;
         ctx.setCurrAliasId(currAliasId);
         mapCurrCtx.put(op, new GenMapRedCtx(currTask, currTopOp, currAliasId));
-        return;
+        return null;
       }
     }
     assert false;
+    return null;
   }
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java
Patch:
@@ -154,7 +154,7 @@ private exprNodeDesc genExprNodeDesc(ASTNode expr)
         }
 
         // Create function desc
-        desc = SemanticAnalyzer.getXpathOrFuncExprNodeDesc(expr, isFunction, children);
+        desc = TypeCheckProcFactory.DefaultExprProcessor.getXpathOrFuncExprNodeDesc(expr, isFunction, children);
         
         if (desc instanceof exprNodeFuncDesc && (
             ((exprNodeFuncDesc)desc).getUDFMethod().getDeclaringClass().equals(UDFOPAnd.class) 
@@ -245,7 +245,7 @@ public void addExpression(ASTNode expr) throws SemanticException {
       if (this.prunerExpr == null)
         this.prunerExpr = desc;
       else
-        this.prunerExpr = SemanticAnalyzer.getFuncExprNodeDesc("OR", this.prunerExpr, desc);
+        this.prunerExpr = TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc("OR", this.prunerExpr, desc);
     }
   }
 
@@ -264,7 +264,7 @@ public void addJoinOnExpression(ASTNode expr) throws SemanticException {
       if (this.prunerExpr == null)
         this.prunerExpr = desc;
       else
-        this.prunerExpr = SemanticAnalyzer.getFuncExprNodeDesc("AND", this.prunerExpr, desc);
+        this.prunerExpr = TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc("AND", this.prunerExpr, desc);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/parse/PrintOpTreeProcessor.java
Patch:
@@ -70,14 +70,15 @@ private String getChildren(Operator<? extends Serializable> op) {
     return ret.toString();
   }
   
-  public void process(Node nd, NodeProcessorCtx ctx) throws SemanticException {
+  public Object process(Node nd, NodeProcessorCtx ctx, Object... nodeOutputs) throws SemanticException {
     Operator<? extends Serializable> op = (Operator<? extends Serializable>)nd;
     if (opMap.get(op) == null) {
       opMap.put(op, curNum++);
     }
     out.println("[" + opMap.get(op) + "] " + op.getClass().getName() + " =p=> " + getParents(op) + " =c=> " + getChildren(op));
     if(op.getConf() == null) {
-      return;
+      return null;
     }
+    return null;
   }
 }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;
 import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
+import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
 import org.apache.hadoop.hive.ql.typeinfo.TypeInfoUtils;
 import org.apache.hadoop.hive.serde.Constants;
 import org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe;
@@ -222,7 +223,7 @@ public static reduceSinkDesc getReduceSinkDesc(ArrayList<exprNodeDesc> keyCols,
     } else {
       // numPartitionFields = -1 means random partitioning
       partitionCols = new ArrayList<exprNodeDesc>(1);
-      partitionCols.add(SemanticAnalyzer.getFuncExprNodeDesc("rand"));
+      partitionCols.add(TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc("rand"));
     }
     
     StringBuilder order = new StringBuilder();

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeColumnDesc.java
Patch:
@@ -24,8 +24,6 @@
 
 import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.hive.ql.parse.RowResolver;
-import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 
 public class exprNodeColumnDesc extends exprNodeDesc implements Serializable {
   private static final long serialVersionUID = 1L;

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeDesc.java
Patch:
@@ -22,8 +22,6 @@
 import java.util.List;
 
 import org.apache.hadoop.hive.ql.typeinfo.TypeInfo;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
-import org.apache.hadoop.hive.ql.parse.RowResolver;
 
 public class exprNodeDesc implements Serializable {  
   private static final long serialVersionUID = 1L;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFCeil.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.ql.exec.UDF;
 
-public class UDFCeil implements UDF {
+public class UDFCeil extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFCeil.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFConcat.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFConcat implements UDF {
+public class UDFConcat extends UDF {
 
   public UDFConcat() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDate.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFDate implements UDF {
+public class UDFDate extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFDate.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDayOfMonth.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFDayOfMonth implements UDF {
+public class UDFDayOfMonth extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFDayOfMonth.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDefaultSampleHashFn.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 
-public class UDFDefaultSampleHashFn implements UDF {
+public class UDFDefaultSampleHashFn extends UDF {
   protected final Log LOG;
 
   public UDFDefaultSampleHashFn() {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFFloor.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFFloor implements UDF {
+public class UDFFloor extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFFloor.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFFromUnixTime.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFFromUnixTime implements UDF {
+public class UDFFromUnixTime extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFFromUnixTime.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFJson.java
Patch:
@@ -33,7 +33,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFJson implements UDF {
+public class UDFJson extends UDF {
   private static Log LOG = LogFactory.getLog(UDFJson.class.getName());
   private Pattern pattern_key = Pattern.compile("^([a-zA-Z0-9_\\-]+).*");
   private Pattern pattern_index = Pattern.compile("\\[([0-9]+|\\*)\\]");

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLTrim.java
Patch:
@@ -24,7 +24,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFLTrim implements UDF {
+public class UDFLTrim extends UDF {
 
   public UDFLTrim() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLike.java
Patch:
@@ -24,7 +24,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFLike implements UDF {
+public class UDFLike extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFLike.class.getName());
   private String lastLikePattern = null;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFLower implements UDF {
+public class UDFLower extends UDF {
 
   public UDFLower() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFMonth.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFMonth implements UDF {
+public class UDFMonth extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFMonth.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPAnd.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPAnd implements UDF {
+public class UDFOPAnd extends UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPAnd");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPBitAnd.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPBitAnd implements UDF {
+public class UDFOPBitAnd extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFOPBitAnd.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPBitNot.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPBitNot implements UDF {
+public class UDFOPBitNot extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFOPBitNot.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPBitOr.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPBitOr implements UDF {
+public class UDFOPBitOr extends UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPBitOr");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPBitXor.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPBitXor implements UDF {
+public class UDFOPBitXor extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFOPBitXor.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPDivide.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPDivide implements UDF {
+public class UDFOPDivide extends UDFBaseNumericOp {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPDivide");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMinus.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPMinus implements UDF {
+public class UDFOPMinus extends UDFBaseNumericOp {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPMinus");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPMod implements UDF {
+public class UDFOPMod extends UDFBaseNumericOp {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPMod");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMultiply.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPMultiply implements UDF {
+public class UDFOPMultiply extends UDFBaseNumericOp {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPMultiply");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPNegative.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPNegative implements UDF {
+public class UDFOPNegative extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFOPNegative.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPNot.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPNot implements UDF {
+public class UDFOPNot extends UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPNot");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPNotNull.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPNotNull implements UDF {
+public class UDFOPNotNull extends UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPNotNull");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPNull.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPNull implements UDF {
+public class UDFOPNull extends UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPNull");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPOr.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPOr implements UDF {
+public class UDFOPOr extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFOPOr.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPlus.java
Patch:
@@ -32,7 +32,7 @@
  * The case of int + double will be handled by implicit type casting using 
  * UDFRegistry.implicitConvertable method. 
  */
-public class UDFOPPlus implements UDF {
+public class UDFOPPlus extends UDFBaseNumericOp {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPPlus");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPositive.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPPositive implements UDF {
+public class UDFOPPositive extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFOPPositive.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRTrim.java
Patch:
@@ -24,7 +24,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFRTrim implements UDF {
+public class UDFRTrim extends UDF {
 
   public UDFRTrim() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRand.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 import java.util.Random;
 
-public class UDFRand implements UDF {
+public class UDFRand extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFRand.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExp.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFRegExp implements UDF {
+public class UDFRegExp extends UDF {
 
   private String lastRegex = null;
   private Pattern p = null;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpReplace.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFRegExpReplace implements UDF {
+public class UDFRegExpReplace extends UDF {
 
   private String lastRegex = null;
   private Pattern p = null;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRound.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFRound implements UDF {
+public class UDFRound extends UDF {
 
   public UDFRound() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFSize.java
Patch:
@@ -23,7 +23,7 @@
 import java.util.Map;
 import java.util.List;
 
-public class UDFSize implements UDF {
+public class UDFSize extends UDF {
 
   public UDFSize() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFTrim.java
Patch:
@@ -24,7 +24,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFTrim implements UDF {
+public class UDFTrim extends UDF {
 
   public UDFTrim() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFUpper.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFUpper implements UDF {
+public class UDFUpper extends UDF {
 
   public UDFUpper() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFYear.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFYear implements UDF {
+public class UDFYear extends UDF {
 
   private static Log LOG = LogFactory.getLog(UDFYear.class.getName());
 

File: ql/src/test/org/apache/hadoop/hive/ql/exec/TestPlan.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hadoop.mapred.TextInputFormat;
 
 import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
+import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;
 import org.apache.hadoop.hive.ql.plan.*;
 import org.apache.hadoop.hive.ql.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.hive.ql.exec.Utilities;
@@ -44,7 +45,7 @@ public void testPlan() throws Exception {
       // initialize a complete map reduce configuration
       exprNodeDesc expr1 = new exprNodeColumnDesc(TypeInfoFactory.getPrimitiveTypeInfo(String.class), F1);
       exprNodeDesc expr2 = new exprNodeColumnDesc(TypeInfoFactory.getPrimitiveTypeInfo(String.class), F2);
-      exprNodeDesc filterExpr = SemanticAnalyzer.getFuncExprNodeDesc("==", expr1, expr2);
+      exprNodeDesc filterExpr = TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc("==", expr1, expr2);
 
       filterDesc filterCtx = new filterDesc(filterExpr);
       Operator<filterDesc> op = OperatorFactory.get(filterDesc.class);

File: ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestLength.java
Patch:
@@ -23,7 +23,7 @@
 /**
  * A UDF for testing, which evaluates the length of a string.
  */
-public class UDFTestLength implements UDF {
+public class UDFTestLength extends UDF {
   public Integer evaluate(String s) {
     return s == null ? null : s.length();
   }

File: cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
Patch:
@@ -301,12 +301,12 @@ public static void main(String[] args) throws IOException {
     String curPrompt = prompt;
     while ((line = reader.readLine(curPrompt+"> ")) != null) {
       if(line.trim().endsWith(";")) {
-        line = prefix + " " + line;
+        line = prefix + "\n" + line;
         ret = cli.processLine(line);
         prefix = "";
         curPrompt = prompt;
       } else {
-        prefix = prefix + line;
+        prefix = prefix + "\n" + line;
         curPrompt = prompt2;
         continue;
       }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
Patch:
@@ -124,7 +124,7 @@ public int execute() {
               Hive.get().getConf());
           dirs = fs.globStatus(new Path(tbd.getSourceDir()));
           files = new ArrayList<FileStatus>();
-          for (int i=0; i<dirs.length; i++) {
+          for (int i=0; (dirs != null && i<dirs.length); i++) {
             files.addAll(Arrays.asList(fs.listStatus(dirs[i].getPath())));
             // We only check one file, so exit the loop when we have at least one.
             if (files.size()>0) break;

File: ql/src/test/org/apache/hadoop/hive/ql/TestMTQueries.java
Patch:
@@ -35,7 +35,7 @@ public class TestMTQueries extends TestCase {
 
   private String inpDir = System.getProperty("ql.test.query.clientpositive.dir");
   private String resDir = System.getProperty("ql.test.results.clientpositive.dir");
-  private String logDir = System.getProperty("test.log.dir"+"/clientpositive");
+  private String logDir = System.getProperty("test.log.dir") + "/clientpositive";
 
   public void testMTQueries1()  throws Exception {
     String[] testNames = new String [] {"join1.q", "join2.q", "groupby1.q", "groupby2.q", "join3.q", "input1.q", "input19.q"};

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java
Patch:
@@ -581,7 +581,6 @@ public void alter_table(String dbname, String name, Table newTable) throws Inval
         }
       }
 
-      @Override
       public List<String> get_tables(String dbname, String pattern) throws MetaException {
         this.incrementCounter("get_tables");
         logStartFunction("get_tables: db=" + dbname + " pat=" + pattern);

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -472,7 +472,6 @@ public Table getTable(String tableName) throws MetaException, TException, NoSuch
     return getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, tableName);
   }
 
-  @Override
   public List<String> listPartitionNames(String dbName, String tblName, short max)
       throws MetaException, TException {
     // TODO Auto-generated method stub

File: metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
Patch:
@@ -90,12 +90,10 @@ private static enum TXN_STATUS {
   
   public ObjectStore() {}
 
-  @Override
   public Configuration getConf() {
     return hiveConf;
   }
 
-  @Override
   @SuppressWarnings("nls")
   public void setConf(Configuration conf) {
     this.hiveConf = conf;

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java
Patch:
@@ -40,7 +40,6 @@ public ASTNode(Token t) {
   /* (non-Javadoc)
    * @see org.apache.hadoop.hive.ql.lib.Node#getChildren()
    */
-  @Override
   public Vector<Node> getChildren() {
     if (super.getChildCount() == 0) {
       return null;
@@ -57,7 +56,6 @@ public Vector<Node> getChildren() {
   /* (non-Javadoc)
    * @see org.apache.hadoop.hive.ql.lib.Node#getName()
    */
-  @Override
   public String getName() {
     return (new Integer(super.getToken().getType())).toString();
   }

File: ql/src/java/org/apache/hadoop/hive/ql/tools/LineageInfo.java
Patch:
@@ -76,7 +76,6 @@ public TreeSet<String> getOutputTableList() {
   /**
    * Implements the process method for the NodeProcessor interface.
    */
-  @Override
   public void process(Node nd, NodeProcessorCtx procCtx)
   throws SemanticException {
     ASTNode pt = (ASTNode)nd;

File: serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDe.java
Patch:
@@ -158,18 +158,15 @@ public static ObjectInspector dynamicSerDeStructBaseToObjectInspector(DynamicSer
     }
   }
 
-  @Override
   public ObjectInspector getObjectInspector() throws SerDeException {
     return dynamicSerDeStructBaseToObjectInspector(this.bt);
   }
 
-  @Override
   public Class<? extends Writable> getSerializedClass() {
     return BytesWritable.class;
   }
 
   BytesWritable ret = new BytesWritable();
-  @Override
   public Writable serialize(Object obj, ObjectInspector objInspector)
   throws SerDeException {
     try {

File: serde/src/java/org/apache/hadoop/hive/serde2/thrift/TBinarySortableProtocol.java
Patch:
@@ -104,7 +104,6 @@ public TBinarySortableProtocol(TTransport trans) {
    */
   boolean ascending; 
   
-  @Override
   public void initialize(Configuration conf, Properties tbl) throws TException {
     sortOrder = tbl.getProperty(Constants.SERIALIZATION_SORT_ORDER);
     if (sortOrder == null) {
@@ -534,12 +533,10 @@ public byte[] readBinary() throws TException {
   }
 
   boolean lastPrimitiveWasNull;
-  @Override
   public boolean lastPrimitiveWasNull() throws TException {
     return lastPrimitiveWasNull;
   }
 
-  @Override
   public void writeNull() throws TException {
     writeRawBytes(nullByte, 0, 1);
   }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java
Patch:
@@ -213,7 +213,7 @@ public boolean hasPartitionPredicate(ASTNode expr) {
         assert(expr.getChildCount() == 2);
         String tabAlias = BaseSemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getText());
         String colName = BaseSemanticAnalyzer.unescapeIdentifier(expr.getChild(1).getText());
-        if (tabAlias.equals(tableAlias) && tab.isPartitionKey(colName)) {
+        if (tabAlias.equalsIgnoreCase(tableAlias) && tab.isPartitionKey(colName)) {
           hasPPred = true;
         }
         break;

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java
Patch:
@@ -331,9 +331,10 @@ public void outputStagePlans(PrintStream out,
     throws Exception {
     out.print(indentString(indent));
     out.println("STAGE PLANS:");
+    HashSet<Task<? extends Serializable>> displayedSet = new HashSet<Task<? extends Serializable>>();
     for(Task<? extends Serializable> rootTask: rootTasks) {
       outputPlan(rootTask, out, work.getExtended(),
-                 new HashSet<Task<? extends Serializable>>(), indent+2);
+                 displayedSet, indent+2);
     }
   }
 

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java
Patch:
@@ -92,8 +92,9 @@ public void process(ReduceSinkOperator op, OperatorProcessorContext opProcCtx) t
     // This will happen in case of joins. The current plan can be thrown away after being merged with the
     // original plan
     else {
-      GenMapRedUtils.joinPlan(opMapTask, ctx);
+      GenMapRedUtils.joinPlan(op, null, opMapTask, ctx);
       currTask = opMapTask;
+      ctx.setCurrTask(currTask);
     }
 
     mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(), ctx.getCurrAliasId()));

File: ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java
Patch:
@@ -67,8 +67,9 @@ public void process(ReduceSinkOperator op, OperatorProcessorContext opProcCtx) t
     if (opMapTask == null)
       GenMapRedUtils.splitPlan(op, ctx);
     else {
-      GenMapRedUtils.joinPlan(opMapTask, ctx);
+      GenMapRedUtils.joinPlan(op, currTask, opMapTask, ctx);
       currTask = opMapTask;
+      ctx.setCurrTask(currTask);
     }
 
     mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(), ctx.getCurrAliasId()));

File: ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java
Patch:
@@ -175,7 +175,7 @@ public void initialize(Configuration hconf) throws HiveException {
 
     iterators = new Stack<Iterator<ArrayList<Object>>>();
     
-    joinEmitInterval = HiveConf.getIntVar(hconf, HiveConf.ConfVars.HIVEPARTITIONNAME);
+    joinEmitInterval = HiveConf.getIntVar(hconf, HiveConf.ConfVars.HIVEJOINEMITINTERVAL);
   }
 
   public void startGroup() throws HiveException {

File: ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
Patch:
@@ -65,6 +65,7 @@ public enum ErrorMsg {
   TABLE_ALIAS_NOT_ALLOWED("Table Alias not Allowed in Sampling Clause"),
   CLUSTERBY_DISTRIBUTEBY_CONFLICT("Cannot have both Cluster By and Distribute By Clauses"),
   CLUSTERBY_SORTBY_CONFLICT("Cannot have both Cluster By and Sort By Clauses"),
+  UNION_NOTIN_SUBQ("Top level Union is not supported currently; use a subquery for the union"),
   NON_BUCKETED_TABLE("Sampling Expression Needed for Non-Bucketed Table");
 
   private String mesg;

File: serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/thrift_grammarTreeConstants.java
Patch:
@@ -1,5 +1,4 @@
-/* Generated By:JJTree: Do not edit this line. /home/pwyckoff/projects/hadoop/trunk/VENDOR/hadoop-0.17/src/contrib/hive/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/thrift_grammarTreeConstants.java */
-
+/* Generated By:JavaCC: Do not edit this line. thrift_grammarTreeConstants.java Version 4.1 */
 package org.apache.hadoop.hive.serde2.dynamic_type;
 
 public interface thrift_grammarTreeConstants
@@ -103,3 +102,4 @@ public interface thrift_grammarTreeConstants
     "TypeList",
   };
 }
+/* JavaCC - OriginalChecksum=7edd3e61472739e9fede55c18a336638 (do not edit this line) */

File: serde/src/test/org/apache/hadoop/hive/serde2/dynamic_type/TestDynamicSerDe.java
Patch:
@@ -107,7 +107,7 @@ public void testDynamicSerDe() throws Throwable {
         schema.setProperty(Constants.SERIALIZATION_FORMAT, protocol);
         schema.setProperty(org.apache.hadoop.hive.metastore.api.Constants.META_TABLE_NAME, "test");
         schema.setProperty(Constants.SERIALIZATION_DDL,
-        "struct test { i32 hello, list<string> bye, map<string,i32> another, i32 nhello, double d, double nd}");
+        "struct test { i32 _hello, list<string> 2bye, map<string,i32> another, i32 nhello, double d, double nd}");
         schema.setProperty(Constants.SERIALIZATION_LIB, new DynamicSerDe().getClass().toString());
         HashMap<String, String> p = additionalParams.get(pp);
         if (p != null) {

File: metastore/src/test/org/apache/hadoop/hive/metastore/TestGetDBs.java
Patch:
@@ -55,9 +55,9 @@ public void testGetDBs() throws Exception {
       DB.createDB("foo2", conf_);
       List<String> dbs = MetaStore.getDbs(conf_);
       assertTrue(dbs.size() == 3);
-      assertTrue(dbs.get(0).equals("foo1"));
-      assertTrue(dbs.get(1).equals("foo2"));
-      assertTrue(dbs.get(2).equals("default"));
+      assertTrue(dbs.contains("foo1"));
+      assertTrue(dbs.contains("foo2"));
+      assertTrue(dbs.contains("default"));
       cleanup();
     } catch(MetaException e) {
       e.printStackTrace();

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -47,6 +47,7 @@ public static enum ConfVars {
     PLAN("hive.exec.plan", null),
     SCRATCHDIR("hive.exec.scratchdir", "/tmp/"+System.getProperty("user.name")+"/hive"),
     SUBMITVIACHILD("hive.exec.submitviachild", "false"),
+    SCRIPTERRORLIMIT("hive.exec.script.maxerrsize", 100000),
 
     // hadoop stuff
     HADOOPBIN("hadoop.bin.path", System.getProperty("user.dir") + "/../../../bin/hadoop"),

File: ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
Patch:
@@ -115,6 +115,8 @@ public class FunctionRegistry {
                 UDFToBoolean.class.getSimpleName());
     registerUDF(Byte.class.getName(), UDFToByte.class, OperatorType.PREFIX, false,
                 UDFToByte.class.getSimpleName());
+    registerUDF(Short.class.getName(), UDFToShort.class, OperatorType.PREFIX, false,
+                UDFToShort.class.getSimpleName());
     registerUDF(Integer.class.getName(), UDFToInteger.class, OperatorType.PREFIX, false,
                 UDFToInteger.class.getSimpleName());
     registerUDF(Long.class.getName(), UDFToLong.class, OperatorType.PREFIX, false,

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
Patch:
@@ -3401,6 +3401,7 @@ else if (tabAlias != null && !input.hasTableAlias(tabAlias)) {
     conversionFunctionTextHashMap = new HashMap<Integer, String>();
     conversionFunctionTextHashMap.put(HiveParser.TOK_BOOLEAN, Boolean.class.getName());
     conversionFunctionTextHashMap.put(HiveParser.TOK_TINYINT, Byte.class.getName());
+    conversionFunctionTextHashMap.put(HiveParser.TOK_SMALLINT, Short.class.getName());
     conversionFunctionTextHashMap.put(HiveParser.TOK_INT, Integer.class.getName());
     conversionFunctionTextHashMap.put(HiveParser.TOK_BIGINT, Long.class.getName());
     conversionFunctionTextHashMap.put(HiveParser.TOK_FLOAT, Float.class.getName());

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeFuncDesc.java
Patch:
@@ -48,6 +48,7 @@ public exprNodeFuncDesc(TypeInfo typeInfo, Class UDFClass, Method UDFMethod, Arr
     super(typeInfo);
     assert(UDFClass != null);
     this.UDFClass = UDFClass;
+    assert(UDFMethod != null);
     this.UDFMethod = UDFMethod;
     this.children = children;
   }

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
Patch:
@@ -662,9 +662,9 @@ public static List<FieldSchema> getFieldsFromDeserializer(String name, Deseriali
     try {
       return MetaStoreUtils.getFieldsFromDeserializer(name, serde);
     } catch (SerDeException e) {
-      throw new HiveException("Error in getting fields from serde.", e);
+      throw new HiveException("Error in getting fields from serde. " + e.getMessage(), e);
     } catch (MetaException e) {
-      throw new HiveException("Error in getting fields from serde.", e);
+      throw new HiveException("Error in getting fields from serde." + e.getMessage(), e);
     }
   }
 

File: serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
Patch:
@@ -261,7 +261,4 @@ public static StructField getStandardStructFieldRef(String fieldName, List<? ext
     throw new RuntimeException("cannot find field " + fieldName + " from " + fields); 
     // return null;
   }
-
-  
-  
 }

File: cli/src/java/org/apache/hadoop/hive/cli/SetProcessor.java
Patch:
@@ -81,8 +81,8 @@ public int run(String command) {
       part[0] = nwcmd.substring(0, nwcmd.length()-1);
       part[1] = "";
     } else {
-      part[0] = nwcmd.substring(0, eqIndex);
-      part[1] = nwcmd.substring(eqIndex+1);
+      part[0] = nwcmd.substring(0, eqIndex).trim();
+      part[1] = nwcmd.substring(eqIndex+1).trim();
     }
 
     try {

File: metastore/src/test/org/apache/hadoop/hive/metastore/TestPartitions.java
Patch:
@@ -60,8 +60,8 @@ public void testPartitions() throws Exception {
       fileSys_.mkdirs(part2);
       List<String> partitions = bar1.getPartitions();
       assertTrue(partitions.size() == 2);
-      assertTrue(partitions.get(0).equals("ds=2008-01-01"));
-      assertTrue(partitions.get(1).equals("ds=2008-01-02"));
+      assertTrue(partitions.contains("ds=2008-01-01"));
+      assertTrue(partitions.contains("ds=2008-01-02"));
       cleanup();
     } catch(MetaException e) {
       e.printStackTrace();

File: ql/src/java/org/apache/hadoop/hive/ql/Driver.java
Patch:
@@ -218,7 +218,7 @@ public int run(String command) {
   
   public boolean getResults(Vector<String> res) 
   {
-  	if (sem.getFetchTask() != null) {
+  	if (sem != null && sem.getFetchTask() != null) {
       if (!sem.getFetchTaskInit()) {
         sem.setFetchTaskInit(true);
         sem.getFetchTask().initialize(conf);

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ExtractOperator.java
Patch:
@@ -44,4 +44,6 @@ public void process(Object row, ObjectInspector rowInspector) throws HiveExcepti
     eval.evaluate(row, rowInspector, result);
     forward(result.o, result.oi);
   }
+
+  
 }

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
Patch:
@@ -219,7 +219,6 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
     return result.toArray(new HiveInputSplit[result.size()]);
   }
 
-
   private tableDesc getTableDescFromPath(Path dir) throws IOException {
 
     partitionDesc partDesc = pathToPartitionInfo.get(dir.toString());

File: ql/src/java/org/apache/hadoop/hive/ql/parse/QBMetaData.java
Patch:
@@ -115,5 +115,4 @@ public String getDestFileForAlias(String alias) {
   public Table getSrcForAlias(String alias) {
     return this.aliasToTable.get(alias.toLowerCase());
   }
-  
 }

File: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java
Patch:
@@ -39,6 +39,9 @@ public static BaseSemanticAnalyzer get(HiveConf conf, CommonTree tree) throws Se
       case HiveParser.TOK_ALTERTABLE_REPLACECOLS:
       case HiveParser.TOK_ALTERTABLE_RENAME:
       case HiveParser.TOK_ALTERTABLE_DROPPARTS:
+      case HiveParser.TOK_ALTERTABLE_PROPERTIES:
+      case HiveParser.TOK_ALTERTABLE_SERIALIZER:
+      case HiveParser.TOK_ALTERTABLE_SERDEPROPERTIES:
       case HiveParser.TOK_SHOWTABLES:
       case HiveParser.TOK_SHOWPARTITIONS:
         return new DDLSemanticAnalyzer(conf);

File: ql/src/java/org/apache/hadoop/hive/ql/plan/groupByDesc.java
Patch:
@@ -23,11 +23,12 @@ public class groupByDesc implements java.io.Serializable {
   /** Group-by Mode:
    *  COMPLETE: complete 1-phase aggregation: aggregate, evaluate
    *  PARTIAL1: partial aggregation - first phase:  aggregate, evaluatePartial
-   *  PARTIAL2: partial aggregation - second phase: aggregatePartial, evaluate
+   *  PARTIAL2: partial aggregation - second phase: aggregatePartial, evaluatePartial
+   *  FINAL: partial aggregation - final phase: aggregatePartial, evaluate
    *  HASH: the same as PARTIAL1 but use hash-table-based aggregation  
    */
   private static final long serialVersionUID = 1L;
-  public static enum Mode { COMPLETE, PARTIAL1, PARTIAL2, HASH };
+  public static enum Mode { COMPLETE, PARTIAL1, PARTIAL2, FINAL, HASH };
   private Mode mode;
   private java.util.ArrayList<exprNodeDesc> keys;
   private java.util.ArrayList<org.apache.hadoop.hive.ql.plan.aggregationDesc> aggregators;

File: ql/src/java/org/apache/hadoop/hive/ql/typeinfo/PrimitiveTypeInfo.java
Patch:
@@ -43,7 +43,7 @@ public class PrimitiveTypeInfo extends TypeInfo implements Serializable {
   public PrimitiveTypeInfo() {}
 
   public String getTypeName() {
-    return ObjectInspectorUtils.getClassShortName(primitiveClass.getName());
+    return ObjectInspectorUtils.getClassShortName(primitiveClass);
   }
   
   

File: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
Patch:
@@ -80,6 +80,7 @@ public static enum ConfVars {
     // for hive script operator
     HIVETABLENAME("hive.table.name", ""),
     HIVEPARTITIONNAME("hive.partition.name", ""),
+    HIVEPARTITIONPRUNER("hive.partition.pruning", "nonstrict"),
     HIVEALIAS("hive.alias", "");
     
     public final String varname;
@@ -170,13 +171,13 @@ private void initialize(Class cls) {
     // let's add the hive configuration 
     URL hconfurl = getClassLoader().getResource("hive-default.xml");
     if(hconfurl == null) {
-      l4j.warn("Unable to locate default hive configuration");
+      l4j.debug("hive-default.xml not found.");
     } else {
       addResource(hconfurl);
     }
     URL hsiteurl = getClassLoader().getResource("hive-site.xml");
     if(hsiteurl == null) {
-      l4j.warn("Unable to locate hive site configuration");
+      l4j.debug("hive-site.xml not found.");
     } else {
       addResource(hsiteurl);
     }

File: metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
Patch:
@@ -142,12 +142,13 @@ private void open() throws MetaException {
     if(!open) {
       throw new MetaException("Could not connect to meta store using any of the URIs provided");
     }
+    LOG.info("Connected to metastore.");
   }
  
   private void openStore(URI store) throws MetaException {
     open = false;
     transport = new TSocket(store.getHost(), store.getPort());
-    ((TSocket)transport).setTimeout(2000);
+    ((TSocket)transport).setTimeout(20000);
     TProtocol protocol = new TBinaryProtocol(transport);
     client = new ThriftHiveMetastore.Client(protocol);
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorFactory.java
Patch:
@@ -48,6 +48,7 @@ public opTuple(Class<T> descClass, Class<? extends Operator<T>> opClass) {
     opvec.add(new opTuple<extractDesc> (extractDesc.class, ExtractOperator.class));
     opvec.add(new opTuple<groupByDesc> (groupByDesc.class, GroupByOperator.class));
     opvec.add(new opTuple<joinDesc> (joinDesc.class, JoinOperator.class));
+    opvec.add(new opTuple<limitDesc> (limitDesc.class, LimitOperator.class));
   }
               
 

File: ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
Patch:
@@ -134,10 +134,10 @@ public void process(Object row, ObjectInspector rowInspector) throws HiveExcepti
       }
       keyWritable.setHashCode(keyHashCode);
       
-      ArrayList<String> values = new ArrayList<String>(valueEval.length);
+      ArrayList<Object> values = new ArrayList<Object>(valueEval.length);
       for(ExprNodeEvaluator e: valueEval) {
         e.evaluate(row, rowInspector, tempInspectableObject);
-        values.add(tempInspectableObject.o == null ? null : tempInspectableObject.o.toString());
+        values.add(tempInspectableObject.o);
         if (valueObjectInspector == null) {
           valueFieldsObjectInspectors.add(tempInspectableObject.oi);
         }

File: ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java
Patch:
@@ -44,6 +44,7 @@ public taskTuple(Class<T> workClass, Class<? extends Task<T>> taskClass) {
     id = 0;
     taskvec = new ArrayList<taskTuple<? extends Serializable>>();
     taskvec.add(new taskTuple<moveWork>(moveWork.class, MoveTask.class));
+    taskvec.add(new taskTuple<fetchWork>(fetchWork.class, FetchTask.class));
     taskvec.add(new taskTuple<copyWork>(copyWork.class, CopyTask.class));
     taskvec.add(new taskTuple<DDLWork>(DDLWork.class, DDLTask.class));
     taskvec.add(new taskTuple<FunctionWork>(FunctionWork.class, FunctionTask.class));

File: ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
Patch:
@@ -179,8 +179,8 @@ public RecordReader getRecordReader(InputSplit split, JobConf job,
     }
 
     InputFormat inputFormat = getInputFormatFromCache(inputFormatClass);
-
-    return inputFormat.getRecordReader(inputSplit, job, reporter);
+    
+    return new HiveRecordReader(inputFormat.getRecordReader(inputSplit, job, reporter));
   }
 
 
@@ -219,6 +219,7 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
     return result.toArray(new HiveInputSplit[result.size()]);
   }
 
+
   private tableDesc getTableDescFromPath(Path dir) throws IOException {
 
     partitionDesc partDesc = pathToPartitionInfo.get(dir.toString());

File: ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java
Patch:
@@ -95,7 +95,7 @@ public Table(String name, Properties schema, Deserializer deserializer,
     initEmpty();
     this.schema = schema;
     this.deserializer = deserializer; //TODO: convert to SerDeInfo format
-    this.getTTable().getSd().getSerdeInfo().setSerializationLib(deserializer.getShortName());
+    this.getTTable().getSd().getSerdeInfo().setSerializationLib(deserializer.getClass().getName());
     getTTable().setTableName(name);
     getSerdeInfo().setSerializationLib(deserializer.getClass().getName());
     setInputFormatClass(inputFormatClass);
@@ -108,7 +108,7 @@ public Table(String name) {
     initEmpty();
     getTTable().setTableName(name);
     getTTable().setDbName(MetaStoreUtils.DEFAULT_DATABASE_NAME);
-    getSerdeInfo().setSerializationLib(MetadataTypedColumnsetSerDe.shortName());
+    getSerdeInfo().setSerializationLib(MetadataTypedColumnsetSerDe.class.getName());
     getSerdeInfo().getParameters().put(Constants.SERIALIZATION_FORMAT, "1");
   }
   

File: ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java
Patch:
@@ -36,7 +36,7 @@ public FunctionSemanticAnalyzer(HiveConf conf) throws SemanticException {
     super(conf);
   }
   
-  public void analyze(CommonTree ast, Context ctx) throws SemanticException {
+  public void analyzeInternal(CommonTree ast, Context ctx) throws SemanticException {
     String functionName = ast.getChild(0).getText();
     String className = unescapeSQLString(ast.getChild(1).getText());
     createFunctionDesc desc = new createFunctionDesc(functionName, className);

File: ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
Patch:
@@ -156,7 +156,7 @@ private void applyConstraints(URI fromURI, URI toURI, Tree ast, boolean isLocal)
   }
 
   @Override
-  public void analyze(CommonTree ast, Context ctx) throws SemanticException {
+  public void analyzeInternal(CommonTree ast, Context ctx) throws SemanticException {
     isLocal = isOverWrite = false;
     Tree from_t = ast.getChild(0);
     Tree table_t = ast.getChild(1);
@@ -185,7 +185,7 @@ public void analyze(CommonTree ast, Context ctx) throws SemanticException {
     }
 
     // initialize destination table/partition
-    tableSpec ts = new tableSpec(db, (CommonTree) table_t);
+    tableSpec ts = new tableSpec(db, (CommonTree) table_t, true);
     URI toURI = (ts.partHandle != null) ? ts.partHandle.getDataLocation() : ts.tableHandle.getDataLocation();
 
     // make sure the arguments make sense

File: ql/src/java/org/apache/hadoop/hive/ql/plan/exprNodeColumnDesc.java
Patch:
@@ -26,11 +26,13 @@
 public class exprNodeColumnDesc extends exprNodeDesc implements Serializable {
   private static final long serialVersionUID = 1L;
   private String column;
+  private boolean isVirtual;
   
   public exprNodeColumnDesc() {}
   public exprNodeColumnDesc(TypeInfo typeInfo, String column) {
     super(typeInfo);
     this.column = column;
+    this.isVirtual = isVirtual;
   }
   public exprNodeColumnDesc(Class<?> c, String column) {
     super(TypeInfoFactory.getPrimitiveTypeInfo(c));
@@ -42,6 +44,7 @@ public String getColumn() {
   public void setColumn(String column) {
     this.column = column;
   }
+
   public String toString() {
     return "Column[" + column + "]";
   }

File: ql/src/java/org/apache/hadoop/hive/ql/plan/tableDesc.java
Patch:
@@ -47,7 +47,7 @@ public tableDesc(
   public Class<? extends Deserializer> getDeserializerClass() {
     return this.deserializerClass;
   }
-  public void setDeserializerClass(final Class<? extends Deserializer> serdeClass) {
+  public void setDeserializerClass(final Class<? extends SerDe> serdeClass) {
     this.deserializerClass = serdeClass;
   }
   public Class<? extends InputFormat> getInputFileFormatClass() {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFConcat.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFConcat extends UDF {
+public class UDFConcat implements UDF {
 
   public UDFConcat() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDefaultSampleHashFn.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 
-public class UDFDefaultSampleHashFn extends UDF {
+public class UDFDefaultSampleHashFn implements UDF {
   protected final Log LOG;
 
   public UDFDefaultSampleHashFn() {

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLTrim.java
Patch:
@@ -24,7 +24,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFLTrim extends UDF {
+public class UDFLTrim implements UDF {
 
   public UDFLTrim() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLike.java
Patch:
@@ -24,7 +24,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFLike extends UDF {
+public class UDFLike implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFLike.class.getName());
   private String lastLikePattern = null;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFLower extends UDF {
+public class UDFLower implements UDF {
 
   public UDFLower() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPAnd.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPAnd extends UDF {
+public class UDFOPAnd implements UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPAnd");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPBitAnd.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPBitAnd extends UDF {
+public class UDFOPBitAnd implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFOPBitAnd.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPBitNot.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPBitNot extends UDF {
+public class UDFOPBitNot implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFOPBitNot.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPBitOr.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPBitOr extends UDF {
+public class UDFOPBitOr implements UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPBitOr");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPBitXor.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPBitXor extends UDF {
+public class UDFOPBitXor implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFOPBitXor.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPDivide.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPDivide extends UDF {
+public class UDFOPDivide implements UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPDivide");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMinus.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPMinus extends UDF {
+public class UDFOPMinus implements UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPMinus");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPMod extends UDF {
+public class UDFOPMod implements UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPMod");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMultiply.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPMultiply extends UDF {
+public class UDFOPMultiply implements UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPMultiply");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPNot.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPNot extends UDF {
+public class UDFOPNot implements UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPNot");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPOr.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFOPOr extends UDF {
+public class UDFOPOr implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFOPOr.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPlus.java
Patch:
@@ -32,7 +32,7 @@
  * The case of int + double will be handled by implicit type casting using 
  * UDFRegistry.implicitConvertable method. 
  */
-public class UDFOPPlus extends UDF {
+public class UDFOPPlus implements UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFOPPlus");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRTrim.java
Patch:
@@ -24,7 +24,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFRTrim extends UDF {
+public class UDFRTrim implements UDF {
 
   public UDFRTrim() {
   }
@@ -33,7 +33,7 @@ public String evaluate(String s) {
     if (s == null) {
       return null;
     }
-    return StringUtils.strip(s, " ");
+    return StringUtils.stripEnd(s, " ");
   }
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExp.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFRegExp extends UDF {
+public class UDFRegExp implements UDF {
 
   private String lastRegex = null;
   private Pattern p = null;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpReplace.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFRegExpReplace extends UDF {
+public class UDFRegExpReplace implements UDF {
 
   private String lastRegex = null;
   private Pattern p = null;

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFStrEq.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFStrEq extends UDF {
+public class UDFStrEq implements UDF {
 
   private static Log LOG = LogFactory.getLog("org.apache.hadoop.hive.ql.udf.UDFStrEq");
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFStrGe.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFStrGe extends UDF {
+public class UDFStrGe implements UDF {
 
   public UDFStrGe() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFStrGt.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFStrGt extends UDF {
+public class UDFStrGt implements UDF {
 
   public UDFStrGt() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFStrLe.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFStrLe extends UDF {
+public class UDFStrLe implements UDF {
 
   public UDFStrLe() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFStrLt.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFStrLt extends UDF {
+public class UDFStrLt implements UDF {
 
   public UDFStrLt() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFStrNe.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFStrNe extends UDF {
+public class UDFStrNe implements UDF {
 
   public UDFStrNe() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFSubstr.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFSubstr extends UDF {
+public class UDFSubstr implements UDF {
 
   public UDFSubstr() {
   }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToBoolean.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFToBoolean extends UDF {
+public class UDFToBoolean implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFToBoolean.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToByte.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFToByte extends UDF {
+public class UDFToByte implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFToByte.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToDate.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFToDate extends UDF {
+public class UDFToDate implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFToDate.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToDouble.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFToDouble extends UDF {
+public class UDFToDouble implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFToDouble.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToFloat.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFToFloat extends UDF {
+public class UDFToFloat implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFToFloat.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToInteger.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFToInteger extends UDF {
+public class UDFToInteger implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFToInteger.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToLong.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFToLong extends UDF {
+public class UDFToLong implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFToLong.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToString.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hadoop.hive.ql.exec.UDF;
 
 
-public class UDFToString extends UDF {
+public class UDFToString implements UDF {
 
   private static Log LOG = LogFactory.getLog(UDFToString.class.getName());
 

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFTrim.java
Patch:
@@ -24,7 +24,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFTrim extends UDF {
+public class UDFTrim implements UDF {
 
   public UDFTrim() {
   }
@@ -33,7 +33,7 @@ public String evaluate(String s) {
     if (s == null) {
       return null;
     }
-    return StringUtils.stripEnd(s, " ");
+    return StringUtils.strip(s, " ");
   }
 
 }

File: ql/src/java/org/apache/hadoop/hive/ql/udf/UDFUpper.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 
-public class UDFUpper extends UDF {
+public class UDFUpper implements UDF {
 
   public UDFUpper() {
   }

File: ql/src/test/org/apache/hadoop/hive/ql/udf/UDFTestLength.java
Patch:
@@ -23,7 +23,7 @@
 /**
  * A UDF for testing, which evaluates the length of a string.
  */
-public class UDFTestLength extends UDF {
+public class UDFTestLength implements UDF {
   public Integer evaluate(String s) {
     return s == null ? null : s.length();
   }

