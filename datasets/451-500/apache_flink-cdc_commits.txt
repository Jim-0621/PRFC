File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/main/java/org/apache/flink/cdc/connectors/mysql/factory/MySqlDataSourceFactory.java
Patch:
@@ -335,6 +335,7 @@ public Set<ConfigOption<?>> optionalOptions() {
         options.add(METADATA_LIST);
         options.add(INCLUDE_COMMENTS_ENABLED);
         options.add(USE_LEGACY_JSON_FORMAT);
+        options.add(TREAT_TINYINT1_AS_BOOLEAN_ENABLED);
         return options;
     }
 

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-doris/src/main/java/org/apache/flink/cdc/connectors/doris/sink/DorisRowConverter.java
Patch:
@@ -34,6 +34,7 @@
 import java.io.IOException;
 import java.io.Serializable;
 import java.time.LocalDate;
+import java.time.LocalTime;
 import java.time.ZoneId;
 import java.time.ZonedDateTime;
 import java.util.Arrays;
@@ -116,6 +117,8 @@ static SerializationConverter createExternalConverter(DataType type, ZoneId pipe
             case TIMESTAMP_WITH_TIME_ZONE:
                 final int zonedP = ((ZonedTimestampType) type).getPrecision();
                 return (index, val) -> val.getTimestamp(index, zonedP).toTimestamp();
+            case TIME_WITHOUT_TIME_ZONE:
+                return (index, val) -> LocalTime.ofNanoOfDay(val.getLong(index) * 1_000_000L);
             case ARRAY:
                 return (index, val) -> convertArrayData(val.getArray(index), type);
             case MAP:

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/main/java/org/apache/flink/cdc/connectors/mysql/source/MySqlDataSource.java
Patch:
@@ -71,7 +71,8 @@ public EventSourceProvider getEventSourceProvider() {
                         DebeziumChangelogMode.ALL,
                         sourceConfig.isIncludeSchemaChanges(),
                         readableMetadataList,
-                        includeComments);
+                        includeComments,
+                        sourceConfig.isTreatTinyInt1AsBoolean());
 
         MySqlSource<Event> source =
                 new MySqlSource<>(

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/main/java/org/apache/flink/cdc/connectors/mysql/source/reader/MySqlPipelineRecordEmitter.java
Patch:
@@ -202,7 +202,8 @@ private Schema parseDDL(String ddlStatement, TableId tableId) {
             Column column = columns.get(i);
 
             String colName = column.name();
-            DataType dataType = MySqlTypeUtils.fromDbzColumn(column);
+            DataType dataType =
+                    MySqlTypeUtils.fromDbzColumn(column, sourceConfig.isTreatTinyInt1AsBoolean());
             if (!column.isOptional()) {
                 dataType = dataType.notNull();
             }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/main/java/org/apache/flink/cdc/connectors/mysql/debezium/reader/BinlogSplitReader.java
Patch:
@@ -252,7 +252,8 @@ private boolean shouldEmit(SourceRecord sourceRecord) {
                 RowType splitKeyType =
                         ChunkUtils.getChunkKeyColumnType(
                                 statefulTaskContext.getDatabaseSchema().tableFor(tableId),
-                                statefulTaskContext.getSourceConfig().getChunkKeyColumns());
+                                statefulTaskContext.getSourceConfig().getChunkKeyColumns(),
+                                statefulTaskContext.getSourceConfig().isTreatTinyInt1AsBoolean());
 
                 Struct target = RecordUtils.getStructContainsChunkKey(sourceRecord);
                 Object[] chunkKey =

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/test/java/org/apache/flink/cdc/connectors/mysql/source/assigners/MySqlSnapshotSplitAssignerTest.java
Patch:
@@ -585,7 +585,7 @@ private List<String> getTestAssignSnapshotSplitsFromCheckpoint(AssignerStatus as
 
         RowType splitKeyType =
                 ChunkUtils.getChunkKeyColumnType(
-                        Column.editor().name("id").type("INT").jdbcType(4).create());
+                        Column.editor().name("id").type("INT").jdbcType(4).create(), true);
         List<MySqlSchemalessSnapshotSplit> remainingSplits =
                 Arrays.asList(
                         new MySqlSchemalessSnapshotSplit(

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-maxcompute/src/test/java/org/apache/flink/cdc/connectors/maxcompute/EmulatorTestBase.java
Patch:
@@ -41,7 +41,7 @@ public class EmulatorTestBase {
     private static final Logger LOG = LoggerFactory.getLogger(EmulatorTestBase.class);
 
     public static final DockerImageName MAXCOMPUTE_IMAGE =
-            DockerImageName.parse("maxcompute/maxcompute-emulator:v0.0.4");
+            DockerImageName.parse("maxcompute/maxcompute-emulator:v0.0.7");
 
     @ClassRule
     public static GenericContainer<?> maxcompute =

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-maxcompute/src/test/java/org/apache/flink/cdc/connectors/maxcompute/utils/TypeConvertUtilsTest.java
Patch:
@@ -167,7 +167,7 @@ public void testRecordConvert() {
                             123456.789d,
                             12345,
                             12345,
-                            TimestampData.fromTimestamp(Timestamp.from(Instant.ofEpochSecond(0))),
+                            TimestampData.fromTimestamp(Timestamp.valueOf("1970-01-01 00:00:00")),
                             LocalZonedTimestampData.fromInstant(Instant.ofEpochSecond(0)),
                             ZonedTimestampData.fromZonedDateTime(
                                     ZonedDateTime.ofInstant(
@@ -179,7 +179,7 @@ public void testRecordConvert() {
         TypeConvertUtils.toMaxComputeRecord(schemaWithoutComplexType, record1, arrayRecord);
 
         String expect =
-                "char,varchar,string,false,=01=02=03=04=05,=01=02=03=04=05=06=07=08=09=0A,0.00,1,2,12345,12345,123.456,123456.789,00:00:00.012345,2003-10-20,1970-01-01T08:00,1970-01-01T00:00:00Z,1970-01-01T00:00:00Z";
+                "char,varchar,string,false,=01=02=03=04=05,=01=02=03=04=05=06=07=08=09=0A,0.00,1,2,12345,12345,123.456,123456.789,00:00:00.012345,2003-10-20,1970-01-01T00:00,1970-01-01T00:00:00Z,1970-01-01T00:00:00Z";
         Assert.assertEquals(expect, arrayRecord.toString());
     }
 }

File: flink-cdc-composer/src/test/java/org/apache/flink/cdc/composer/flink/FlinkPipelineTransformITCase.java
Patch:
@@ -1979,7 +1979,7 @@ void testTransformWithLargeLiterals() throws Exception {
                                                 + "2147483648 AS greater_than_int_max, "
                                                 + "-2147483648 AS int_min, "
                                                 + "-2147483649 AS less_than_int_min, "
-                                                + "CAST(1234567890123456789 AS DECIMAL(20, 0)) AS really_big_decimal",
+                                                + "CAST(1234567890123456789 AS DECIMAL(19, 0)) AS really_big_decimal",
                                         "CAST(id AS BIGINT) + 2147483648 > 2147483649", // equivalent to id > 1
                                         null,
                                         null,

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/parser/JaninoCompiler.java
Patch:
@@ -470,7 +470,7 @@ private static Java.Rvalue generateTypeConvertMethod(
                 return new Java.MethodInvocation(
                         Location.NOWHERE,
                         null,
-                        "castToBigDecimal",
+                        "castToDecimalData",
                         newAtoms.toArray(new Java.Rvalue[0]));
             case "CHAR":
             case "VARCHAR":
@@ -496,7 +496,7 @@ private static String generateInvokeExpression(UserDefinedFunctionDescriptor udf
             return String.format(
                     "(%s) __instanceOf%s.eval",
                     DataTypeConverter.convertOriginalClass(udfFunction.getReturnTypeHint())
-                            .getName(),
+                            .getCanonicalName(),
                     udfFunction.getClassName());
         } else {
             return String.format("__instanceOf%s.eval", udfFunction.getClassName());

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/parser/TransformParser.java
Patch:
@@ -176,8 +176,8 @@ private static RelNode sqlToRel(
                                 new HepPlanner(new HepProgramBuilder().build()),
                                 new RexBuilder(factory)),
                         StandardConvertletTable.INSTANCE,
-                        SqlToRelConverter.config().withTrimUnusedFields(false));
-        RelRoot relRoot = sqlToRelConverter.convertQuery(validateSqlNode, false, true);
+                        SqlToRelConverter.config().withTrimUnusedFields(true));
+        RelRoot relRoot = sqlToRelConverter.convertQuery(validateSqlNode, false, false);
         return relRoot.rel;
     }
 

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-paimon/src/main/java/org/apache/flink/cdc/connectors/paimon/sink/v2/bucket/BucketWrapperEventSerializer.java
Patch:
@@ -87,6 +87,7 @@ public void serialize(Event event, DataOutputView dataOutputView) throws IOExcep
             BucketWrapperFlushEvent bucketWrapperFlushEvent = (BucketWrapperFlushEvent) event;
             dataOutputView.writeInt(bucketWrapperFlushEvent.getBucket());
             dataOutputView.writeInt(bucketWrapperFlushEvent.getSourceSubTaskId());
+            dataOutputView.writeInt(bucketWrapperFlushEvent.getBucketAssignTaskId());
             tableIdListSerializer.serialize(bucketWrapperFlushEvent.getTableIds(), dataOutputView);
             schemaChangeEventTypeEnumSerializer.serialize(
                     bucketWrapperFlushEvent.getSchemaChangeEventType(), dataOutputView);
@@ -98,6 +99,7 @@ public Event deserialize(DataInputView source) throws IOException {
         EventClass eventClass = enumSerializer.deserialize(source);
         if (eventClass.equals(EventClass.BUCKET_WRAPPER_FLUSH_EVENT)) {
             return new BucketWrapperFlushEvent(
+                    source.readInt(),
                     source.readInt(),
                     source.readInt(),
                     tableIdListSerializer.deserialize(source),

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/operators/transform/ProjectionColumnProcessor.java
Patch:
@@ -186,9 +186,7 @@ private TransformExpressionKey generateTransformExpressionKey() {
                     break;
                 }
             }
-        }
 
-        for (String originalColumnName : originalColumnNames) {
             METADATA_COLUMNS.stream()
                     .filter(col -> col.f0.equals(originalColumnName))
                     .findFirst()

File: flink-cdc-composer/src/test/java/org/apache/flink/cdc/composer/flink/FlinkPipelineUdfITCase.java
Patch:
@@ -40,6 +40,7 @@
 
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.extension.RegisterExtension;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.Arguments;
@@ -839,6 +840,7 @@ void testComplicatedFlinkUdf(ValuesDataSink.SinkApi sinkApi, String language) th
 
     @ParameterizedTest
     @MethodSource("testParams")
+    @Disabled("For manual test as there is a limit for quota.")
     void testTransformWithModel(ValuesDataSink.SinkApi sinkApi) throws Exception {
         FlinkPipelineComposer composer = FlinkPipelineComposer.ofMiniCluster();
 

File: flink-cdc-pipeline-model/src/test/java/org/apache/flink/cdc/runtime/model/TestOpenAIChatModel.java
Patch:
@@ -21,11 +21,13 @@
 import org.apache.flink.cdc.common.udf.UserDefinedFunctionContext;
 
 import org.junit.jupiter.api.Assertions;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 
 /** A test for {@link OpenAIChatModel}. */
 public class TestOpenAIChatModel {
     @Test
+    @Disabled("For manual test as there is a limit for quota.")
     public void testEval() {
         OpenAIChatModel openAIChatModel = new OpenAIChatModel();
         Configuration configuration = new Configuration();

File: flink-cdc-pipeline-model/src/test/java/org/apache/flink/cdc/runtime/model/TestOpenAIEmbeddingModel.java
Patch:
@@ -22,12 +22,14 @@
 import org.apache.flink.cdc.common.udf.UserDefinedFunctionContext;
 
 import org.junit.jupiter.api.Assertions;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 
 /** A test for {@link OpenAIEmbeddingModel}. */
 public class TestOpenAIEmbeddingModel {
 
     @Test
+    @Disabled("For manual test as there is a limit for quota.")
     public void testEval() {
         OpenAIEmbeddingModel openAIEmbeddingModel = new OpenAIEmbeddingModel();
         Configuration configuration = new Configuration();

File: flink-cdc-composer/src/test/java/org/apache/flink/cdc/composer/flink/FlinkPipelineComposerITCase.java
Patch:
@@ -1241,7 +1241,7 @@ void testTransformMergingWithRouteChangeOrder(ValuesDataSink.SinkApi sinkApi) th
         assertThat(mergedTableSchema)
                 .isEqualTo(
                         Schema.newBuilder()
-                                .physicalColumn("id", DataTypes.BIGINT())
+                                .physicalColumn("id", DataTypes.BIGINT().notNull())
                                 .physicalColumn("name", DataTypes.STRING())
                                 .physicalColumn("age", DataTypes.INT())
                                 .physicalColumn("last_name", DataTypes.STRING())
@@ -1252,9 +1252,9 @@ void testTransformMergingWithRouteChangeOrder(ValuesDataSink.SinkApi sinkApi) th
         String[] outputEvents = outCaptor.toString().trim().split("\n");
         assertThat(outputEvents)
                 .containsExactly(
-                        "CreateTableEvent{tableId=default_namespace.default_schema.merged, schema=columns={`id` INT,`name` STRING,`age` INT,`last_name` STRING}, primaryKeys=id, options=()}",
+                        "CreateTableEvent{tableId=default_namespace.default_schema.merged, schema=columns={`id` INT NOT NULL,`name` STRING,`age` INT,`last_name` STRING}, primaryKeys=id, options=()}",
                         "AddColumnEvent{tableId=default_namespace.default_schema.merged, addedColumns=[ColumnWithPosition{column=`description` STRING, position=AFTER, existedColumnName=last_name}]}",
-                        "AlterColumnTypeEvent{tableId=default_namespace.default_schema.merged, typeMapping={id=BIGINT}, oldTypeMapping={id=INT}}",
+                        "AlterColumnTypeEvent{tableId=default_namespace.default_schema.merged, typeMapping={id=BIGINT NOT NULL}, oldTypeMapping={id=INT NOT NULL}}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[], after=[1, Alice, 18, last_name, null], op=INSERT, meta=()}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[], after=[2, Bob, 20, last_name, null], op=INSERT, meta=()}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[2, Bob, 20, last_name, null], after=[2, Bob, 30, last_name, null], op=UPDATE, meta=()}",

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-paimon/src/test/java/org/apache/flink/cdc/connectors/paimon/sink/v2/PaimonSinkITCase.java
Patch:
@@ -276,7 +276,8 @@ public void testSinkWithDataChange(String metastore, boolean enableDeleteVector)
         // Each commit will generate one sequence number(equal to checkpointId).
         List<Row> expected =
                 enableDeleteVector
-                        ? Collections.singletonList(Row.ofKind(RowKind.INSERT, 3L))
+                        ? Arrays.asList(
+                                Row.ofKind(RowKind.INSERT, 1L), Row.ofKind(RowKind.INSERT, 3L))
                         : Arrays.asList(
                                 Row.ofKind(RowKind.INSERT, 1L),
                                 Row.ofKind(RowKind.INSERT, 2L),

File: flink-cdc-common/src/main/java/org/apache/flink/cdc/common/utils/SchemaMergingUtils.java
Patch:
@@ -810,7 +810,9 @@ private static Map<Class<? extends DataType>, List<DataType>> getTypeMergingTree
         mergingTree.put(DoubleType.class, ImmutableList.of(doubleType, stringType));
         mergingTree.put(FloatType.class, ImmutableList.of(floatType, doubleType, stringType));
         mergingTree.put(DecimalType.class, ImmutableList.of(stringType));
-        mergingTree.put(BigIntType.class, ImmutableList.of(bigIntType, decimalType, stringType));
+        mergingTree.put(
+                BigIntType.class,
+                ImmutableList.of(bigIntType, decimalType, doubleType, stringType));
         mergingTree.put(
                 IntType.class,
                 ImmutableList.of(intType, bigIntType, decimalType, doubleType, stringType));

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/operators/schema/common/SchemaDerivator.java
Patch:
@@ -311,7 +311,6 @@ public Optional<DataChangeEvent> coerceDataRecord(
 
         List<RecordData.FieldGetter> upstreamSchemaReader =
                 upstreamRecordGetterCache.getUnchecked(upstreamSchema);
-        SchemaUtils.createFieldGetters(upstreamSchema);
         BinaryRecordDataGenerator evolvedSchemaWriter =
                 evolvedRecordWriterCache.getUnchecked(evolvedSchema);
 

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/main/java/org/apache/flink/cdc/connectors/mysql/source/MySqlDataSourceOptions.java
Patch:
@@ -126,8 +126,8 @@ public class MySqlDataSourceOptions {
                     .defaultValue("initial")
                     .withDescription(
                             "Optional startup mode for MySQL CDC consumer, valid enumerations are "
-                                    + "\"initial\", \"earliest-offset\", \"latest-offset\", \"timestamp\"\n"
-                                    + "or \"specific-offset\"");
+                                    + "\"initial\", \"earliest-offset\", \"latest-offset\", \"timestamp\", "
+                                    + "\"specific-offset\" or \"snapshot\".");
 
     public static final ConfigOption<String> SCAN_STARTUP_SPECIFIC_OFFSET_FILE =
             ConfigOptions.key("scan.startup.specific-offset.file")

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/typeutils/BinaryRecordDataGenerator.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.flink.cdc.common.types.DataType;
 import org.apache.flink.cdc.common.types.RowType;
 import org.apache.flink.cdc.runtime.serializer.InternalSerializers;
-import org.apache.flink.cdc.runtime.serializer.NullableSerializerWrapper;
 import org.apache.flink.cdc.runtime.serializer.data.writer.BinaryRecordDataWriter;
 import org.apache.flink.cdc.runtime.serializer.data.writer.BinaryWriter;
 
@@ -51,7 +50,6 @@ public BinaryRecordDataGenerator(DataType[] dataTypes) {
                 dataTypes,
                 Arrays.stream(dataTypes)
                         .map(InternalSerializers::create)
-                        .map(NullableSerializerWrapper::new)
                         .toArray(TypeSerializer[]::new));
     }
 

File: flink-cdc-runtime/src/test/java/org/apache/flink/cdc/runtime/operators/transform/PostTransformOperatorTest.java
Patch:
@@ -36,6 +36,7 @@
 import org.testcontainers.shaded.com.google.common.collect.ImmutableMap;
 
 import java.math.BigDecimal;
+import java.time.format.DateTimeParseException;
 
 /** Unit tests for the {@link PostTransformOperator}. */
 public class PostTransformOperatorTest {
@@ -1479,8 +1480,8 @@ void testCastErrorTransform() throws Exception {
                             transform.processElement(new StreamRecord<>(insertEvent1));
                         })
                 .isExactlyInstanceOf(RuntimeException.class)
-                .hasRootCauseInstanceOf(NumberFormatException.class)
-                .hasRootCauseMessage("For input string: \"1.0\"");
+                .hasRootCauseInstanceOf(DateTimeParseException.class)
+                .hasRootCauseMessage("Text '1.0' could not be parsed at index 0");
     }
 
     @Test

File: flink-cdc-composer/src/test/java/org/apache/flink/cdc/composer/flink/FlinkPipelineComposerITCase.java
Patch:
@@ -411,7 +411,7 @@ void testOpTypeMetadataColumn(ValuesDataSink.SinkApi sinkApi) throws Exception {
         String[] outputEvents = outCaptor.toString().trim().split("\n");
         assertThat(outputEvents)
                 .containsExactly(
-                        "CreateTableEvent{tableId=default_namespace.default_schema.table1, schema=columns={`col1` STRING,`col2` STRING,`col12` STRING,`rk` STRING}, primaryKeys=col1, partitionKeys=col12, options=({key1=value1})}",
+                        "CreateTableEvent{tableId=default_namespace.default_schema.table1, schema=columns={`col1` STRING,`col2` STRING,`col12` STRING,`rk` STRING NOT NULL}, primaryKeys=col1, partitionKeys=col12, options=({key1=value1})}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.table1, before=[], after=[1, 1, 10, +I], op=INSERT, meta=()}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.table1, before=[], after=[2, 2, 20, +I], op=INSERT, meta=()}",
                         "AddColumnEvent{tableId=default_namespace.default_schema.table1, addedColumns=[ColumnWithPosition{column=`col3` STRING, position=AFTER, existedColumnName=col2}]}",

File: flink-cdc-composer/src/test/java/org/apache/flink/cdc/composer/flink/FlinkPipelineComposerLenientITCase.java
Patch:
@@ -453,7 +453,7 @@ void testOpTypeMetadataColumn(ValuesDataSink.SinkApi sinkApi) throws Exception {
         String[] outputEvents = outCaptor.toString().trim().split(LINE_SEPARATOR);
         assertThat(outputEvents)
                 .containsExactly(
-                        "CreateTableEvent{tableId=default_namespace.default_schema.table1, schema=columns={`col1` STRING,`col2` STRING,`col12` STRING,`rk` STRING}, primaryKeys=col1, partitionKeys=col12, options=({key1=value1})}",
+                        "CreateTableEvent{tableId=default_namespace.default_schema.table1, schema=columns={`col1` STRING,`col2` STRING,`col12` STRING,`rk` STRING NOT NULL}, primaryKeys=col1, partitionKeys=col12, options=({key1=value1})}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.table1, before=[], after=[1, 1, 10, +I], op=INSERT, meta=()}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.table1, before=[], after=[2, 2, 20, +I], op=INSERT, meta=()}",
                         "AddColumnEvent{tableId=default_namespace.default_schema.table1, addedColumns=[ColumnWithPosition{column=`col3` STRING, position=LAST, existedColumnName=null}]}",

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/operators/transform/PostTransformOperator.java
Patch:
@@ -329,7 +329,7 @@ private PostTransformChangeInfo getPostTransformChangeInfo(TableId tableId) {
         return tableInfo;
     }
 
-    private Schema transformSchema(TableId tableId, Schema schema) throws Exception {
+    private Schema transformSchema(TableId tableId, Schema schema) {
         List<Schema> newSchemas = new ArrayList<>();
         for (PostTransformer transform : transforms) {
             Selectors selectors = transform.getSelectors();

File: flink-cdc-runtime/src/test/java/org/apache/flink/cdc/runtime/operators/transform/PostTransformOperatorTest.java
Patch:
@@ -92,7 +92,7 @@ public class PostTransformOperatorTest {
                     .physicalColumn("sid", DataTypes.INT())
                     .physicalColumn("name", DataTypes.STRING())
                     .physicalColumn("name_upper", DataTypes.STRING())
-                    .physicalColumn("tbname", DataTypes.STRING())
+                    .physicalColumn("tbname", DataTypes.STRING().notNull())
                     .primaryKey("sid")
                     .build();
 

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/test/java/org/apache/flink/cdc/connectors/mysql/testutils/MySqlVersion.java
Patch:
@@ -22,6 +22,9 @@ public enum MySqlVersion {
     V5_5("5.5"),
     V5_6("5.6"),
     V5_7("5.7"),
+    V8_0_17("8.0.17"),
+    V8_0_18("8.0.18"),
+    V8_0_19("8.0.19"),
     V8_0("8.0");
 
     private String version;

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/main/java/org/apache/flink/cdc/connectors/mysql/source/enumerator/MySqlSourceEnumerator.java
Patch:
@@ -277,8 +277,8 @@ private void syncWithReaders(int[] subtaskIds, Throwable t) {
     private void requestBinlogSplitUpdateIfNeed() {
         if (!isBinlogSplitUpdateRequestAlreadySent
                 && isNewlyAddedAssigningSnapshotFinished(splitAssigner.getAssignerStatus())) {
-            isBinlogSplitUpdateRequestAlreadySent = true;
             for (int subtaskId : getRegisteredReader()) {
+                isBinlogSplitUpdateRequestAlreadySent = true;
                 LOG.info(
                         "The enumerator requests subtask {} to update the binlog split after newly added table.",
                         subtaskId);

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-oracle-cdc/src/test/java/org/apache/flink/cdc/connectors/oracle/source/NewlyAddedTableITCase.java
Patch:
@@ -495,6 +495,8 @@ private void testRemoveTablesOneByOne(
             waitForSinkSize("sink", fetchedDataList.size());
             assertEqualsInAnyOrder(
                     fetchedDataList, TestValuesTableFactory.getRawResultsAsStrings("sink"));
+            // wait task to stream phase
+            sleepMs(10000);
             finishedSavePointPath = triggerSavepointWithRetry(jobClient, savepointDirectory);
             jobClient.cancel().get();
         }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-cdc-base/src/test/java/org/apache/flink/cdc/connectors/base/mocked/MockedPooledDataSourceFactory.java
Patch:
@@ -15,15 +15,15 @@
  * limitations under the License.
  */
 
-package org.apache.flink.cdc.connectors.base.experimental;
+package org.apache.flink.cdc.connectors.base.mocked;
 
 import org.apache.flink.cdc.common.annotation.Experimental;
 import org.apache.flink.cdc.connectors.base.config.JdbcSourceConfig;
 import org.apache.flink.cdc.connectors.base.relational.connection.JdbcConnectionPoolFactory;
 
 /** A MySQL datasource factory. */
 @Experimental
-public class MysqlPooledDataSourceFactory extends JdbcConnectionPoolFactory {
+public class MockedPooledDataSourceFactory extends JdbcConnectionPoolFactory {
 
     public static final String JDBC_URL_PATTERN =
             "jdbc:mysql://%s:%s/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL";

File: flink-cdc-connect/flink-cdc-source-connectors/flink-cdc-base/src/test/java/org/apache/flink/cdc/connectors/base/mocked/MockedSourceConfig.java
Patch:
@@ -15,7 +15,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.cdc.connectors.base.experimental.config;
+package org.apache.flink.cdc.connectors.base.mocked;
 
 import org.apache.flink.cdc.connectors.base.config.JdbcSourceConfig;
 import org.apache.flink.cdc.connectors.base.options.StartupOptions;
@@ -31,13 +31,13 @@
  * Describes the connection information of the Mysql database and the configuration information for
  * performing snapshotting and streaming reading, such as splitSize.
  */
-public class MySqlSourceConfig extends JdbcSourceConfig {
+public class MockedSourceConfig extends JdbcSourceConfig {
 
     private static final long serialVersionUID = 1L;
 
     private transient MySqlConnectorConfig dbzMySqlConfig;
 
-    public MySqlSourceConfig(
+    public MockedSourceConfig(
             StartupOptions startupOptions,
             List<String> databaseList,
             List<String> tableList,

File: flink-cdc-connect/flink-cdc-source-connectors/flink-cdc-base/src/main/java/org/apache/flink/cdc/connectors/base/source/assigner/SnapshotSplitAssigner.java
Patch:
@@ -428,15 +428,15 @@ public AssignerStatus getAssignerStatus() {
     @Override
     public void startAssignNewlyAddedTables() {
         Preconditions.checkState(
-                isAssigningFinished(assignerStatus), "Invalid assigner status {}", assignerStatus);
+                isAssigningFinished(assignerStatus), "Invalid assigner status %s", assignerStatus);
         assignerStatus = assignerStatus.startAssignNewlyTables();
     }
 
     @Override
     public void onStreamSplitUpdated() {
         Preconditions.checkState(
                 isNewlyAddedAssigningSnapshotFinished(assignerStatus),
-                "Invalid assigner status {}",
+                "Invalid assigner status %s",
                 assignerStatus);
         assignerStatus = assignerStatus.onStreamSplitUpdated();
     }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/main/java/org/apache/flink/cdc/connectors/mysql/source/assigners/MySqlSnapshotSplitAssigner.java
Patch:
@@ -471,7 +471,7 @@ public AssignerStatus getAssignerStatus() {
     public void startAssignNewlyAddedTables() {
         Preconditions.checkState(
                 AssignerStatus.isAssigningFinished(assignerStatus),
-                "Invalid assigner status {}",
+                "Invalid assigner status %s",
                 assignerStatus);
         assignerStatus = assignerStatus.startAssignNewlyTables();
     }
@@ -480,7 +480,7 @@ public void startAssignNewlyAddedTables() {
     public void onBinlogSplitUpdated() {
         Preconditions.checkState(
                 AssignerStatus.isNewlyAddedAssigningSnapshotFinished(assignerStatus),
-                "Invalid assigner status {}",
+                "Invalid assigner status %s",
                 assignerStatus);
         assignerStatus = assignerStatus.onBinlogSplitUpdated();
     }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/test/java/org/apache/flink/cdc/connectors/mysql/table/MySqlConnectorShardingTableITCase.java
Patch:
@@ -196,7 +196,7 @@ public void testShardingTablesWithTinyInt1() throws Exception {
                 new String[] {
                     "+I[1, 1]", "+I[2, 0]", "+I[3, 1]", "+I[4, 0]", "+I[5, 1]", "+I[6, 0]",
                 };
-        List<String> actual = TestValuesTableFactory.getResults("sink");
+        List<String> actual = TestValuesTableFactory.getResultsAsStrings("sink");
         assertEqualsInAnyOrder(Arrays.asList(expected), actual);
         result.getJobClient().get().cancel().get();
     }

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/parser/TransformParser.java
Patch:
@@ -588,7 +588,8 @@ public static String normalizeFilter(String projection, String filter) {
 
     public static boolean hasAsterisk(@Nullable String projection) {
         if (isNullOrWhitespaceOnly(projection)) {
-            return false;
+            // Providing an empty projection expression is equivalent to writing `*` explicitly.
+            return true;
         }
         return parseProjectionExpression(projection).getOperandList().stream()
                 .anyMatch(TransformParser::hasAsterisk);

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-values/src/main/java/org/apache/flink/cdc/connectors/values/source/ValuesDataSourceOptions.java
Patch:
@@ -40,6 +40,8 @@ public class ValuesDataSourceOptions {
                                             ListElement.list(
                                                     text(
                                                             "SINGLE_SPLIT_SINGLE_TABLE: Default and predetermined case. Creating schema changes of single table and put them into one split."),
+                                                    text(
+                                                            "SINGLE_SPLIT_SINGLE_TABLE_WITH_DEFAULT_VALUE: A predetermined case. Creating schema changes of single table (some columns have default value) and put them into one split."),
                                                     text(
                                                             "SINGLE_SPLIT_MULTI_TABLES: A predetermined case. Creating schema changes of multiple tables and put them into one split."),
                                                     text(

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-kafka/src/test/java/org/apache/flink/cdc/connectors/kafka/json/TableSchemaInfoTest.java
Patch:
@@ -156,11 +156,10 @@ public void testGetRowDataFromRecordData() {
                                 Timestamp.valueOf("2023-01-01 00:00:00.000")),
                         org.apache.flink.table.data.TimestampData.fromTimestamp(
                                 Timestamp.valueOf("2023-01-01 00:00:00")),
-                        // plus 8 hours.
                         org.apache.flink.table.data.TimestampData.fromInstant(
-                                Instant.parse("2023-01-01T08:00:00.000Z")),
+                                Instant.parse("2023-01-01T00:00:00.000Z")),
                         org.apache.flink.table.data.TimestampData.fromInstant(
-                                Instant.parse("2023-01-01T08:00:00.000Z")),
+                                Instant.parse("2023-01-01T00:00:00.000Z")),
                         null),
                 tableSchemaInfo.getRowDataFromRecordData(recordData, false));
     }

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-paimon/src/test/java/org/apache/flink/cdc/connectors/paimon/sink/v2/PaimonWriterHelperTest.java
Patch:
@@ -130,9 +130,8 @@ public void testConvertEventToGenericRowOfAllDataTypes() {
                                 java.sql.Timestamp.valueOf("2023-01-01 00:00:00.000")),
                         Timestamp.fromSQLTimestamp(
                                 java.sql.Timestamp.valueOf("2023-01-01 00:00:00")),
-                        // plus 8 hours.
-                        Timestamp.fromInstant(Instant.parse("2023-01-01T08:00:00.000Z")),
-                        Timestamp.fromInstant(Instant.parse("2023-01-01T08:00:00.000Z")),
+                        Timestamp.fromInstant(Instant.parse("2023-01-01T00:00:00.000Z")),
+                        Timestamp.fromInstant(Instant.parse("2023-01-01T00:00:00.000Z")),
                         null),
                 genericRow);
     }

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-starrocks/src/main/java/org/apache/flink/cdc/connectors/starrocks/sink/StarRocksDataSinkFactory.java
Patch:
@@ -51,7 +51,7 @@ public DataSink createDataSink(Context context) {
                 TableCreateConfig.from(context.getFactoryConfiguration());
         SchemaChangeConfig schemaChangeConfig =
                 SchemaChangeConfig.from(context.getFactoryConfiguration());
-        String zoneStr = context.getFactoryConfiguration().get(PIPELINE_LOCAL_TIME_ZONE);
+        String zoneStr = context.getPipelineConfiguration().get(PIPELINE_LOCAL_TIME_ZONE);
         ZoneId zoneId =
                 PIPELINE_LOCAL_TIME_ZONE.defaultValue().equals(zoneStr)
                         ? ZoneId.systemDefault()

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-oracle-cdc/src/main/java/io/debezium/connector/oracle/logminer/LogMinerStreamingChangeEventSource.java
Patch:
@@ -247,10 +247,10 @@ public void execute(
                                 retryAttempts++;
                             } else {
                                 retryAttempts = 1;
-                                startScn = processor.process(partition, startScn, endScn);
                                 streamingMetrics.setCurrentBatchProcessingTime(
                                         Duration.between(start, Instant.now()));
                                 captureSessionMemoryStatistics(jdbcConnection);
+                                startScn = processor.process(partition, startScn, endScn);
                             }
                             pauseBetweenMiningSessions();
                         }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/main/java/org/apache/flink/cdc/connectors/mysql/source/utils/StatementUtils.java
Patch:
@@ -235,7 +235,6 @@ public static String quote(TableId tableId) {
     private static PreparedStatement initStatement(JdbcConnection jdbc, String sql, int fetchSize)
             throws SQLException {
         final Connection connection = jdbc.connection();
-        connection.setAutoCommit(false);
         final PreparedStatement statement = connection.prepareStatement(sql);
         statement.setFetchSize(fetchSize);
         return statement;

File: flink-cdc-cli/src/main/java/org/apache/flink/cdc/cli/parser/YamlPipelineDefinitionParser.java
Patch:
@@ -179,6 +179,7 @@ private SourceDef toSourceDef(JsonNode sourceNode) {
     private SinkDef toSinkDef(JsonNode sinkNode, SchemaChangeBehavior schemaChangeBehavior) {
         List<String> includedSETypes = new ArrayList<>();
         List<String> excludedSETypes = new ArrayList<>();
+        boolean excludedFieldNotPresent = sinkNode.get(EXCLUDE_SCHEMA_EVOLUTION_TYPES) == null;
 
         Optional.ofNullable(sinkNode.get(INCLUDE_SCHEMA_EVOLUTION_TYPES))
                 .ifPresent(e -> e.forEach(tag -> includedSETypes.add(tag.asText())));
@@ -194,8 +195,7 @@ private SinkDef toSinkDef(JsonNode sinkNode, SchemaChangeBehavior schemaChangeBe
                     .forEach(includedSETypes::add);
         }
 
-        if (excludedSETypes.isEmpty()
-                && SchemaChangeBehavior.LENIENT.equals(schemaChangeBehavior)) {
+        if (excludedFieldNotPresent && SchemaChangeBehavior.LENIENT.equals(schemaChangeBehavior)) {
             // In lenient mode, we exclude DROP_TABLE and TRUNCATE_TABLE by default. This could be
             // overridden by manually specifying excluded types.
             Stream.of(SchemaChangeEventType.DROP_TABLE, SchemaChangeEventType.TRUNCATE_TABLE)

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mongodb-cdc/src/test/java/org/apache/flink/cdc/connectors/mongodb/source/MongoDBFullChangelogITCase.java
Patch:
@@ -90,7 +90,7 @@ public MongoDBFullChangelogITCase(String mongoVersion, boolean parallelismSnapsh
     @Parameterized.Parameters(name = "mongoVersion: {0} parallelismSnapshot: {1}")
     public static Object[] parameters() {
         List<Object[]> parameterTuples = new ArrayList<>();
-        for (String mongoVersion : MONGO_VERSIONS) {
+        for (String mongoVersion : getMongoVersions()) {
             parameterTuples.add(new Object[] {mongoVersion, true});
             parameterTuples.add(new Object[] {mongoVersion, false});
         }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mongodb-cdc/src/test/java/org/apache/flink/cdc/connectors/mongodb/source/MongoDBParallelSourceExampleTest.java
Patch:
@@ -41,7 +41,7 @@ public class MongoDBParallelSourceExampleTest extends MongoDBSourceTestBase {
     @Parameterized.Parameters(name = "mongoVersion: {0} parallelismSnapshot: {1}")
     public static Object[] parameters() {
         List<Object[]> parameterTuples = new ArrayList<>();
-        for (String mongoVersion : MONGO_VERSIONS) {
+        for (String mongoVersion : getMongoVersions()) {
             parameterTuples.add(new Object[] {mongoVersion, true});
             parameterTuples.add(new Object[] {mongoVersion, false});
         }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mongodb-cdc/src/test/java/org/apache/flink/cdc/connectors/mongodb/source/MongoDBParallelSourceITCase.java
Patch:
@@ -80,7 +80,7 @@ public MongoDBParallelSourceITCase(String mongoVersion) {
 
     @Parameterized.Parameters(name = "mongoVersion: {0}")
     public static Object[] parameters() {
-        return Stream.of(MONGO_VERSIONS).map(e -> new Object[] {e}).toArray();
+        return Stream.of(getMongoVersions()).map(e -> new Object[] {e}).toArray();
     }
 
     @Rule public final Timeout timeoutPerTest = Timeout.seconds(300);

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mongodb-cdc/src/test/java/org/apache/flink/cdc/connectors/mongodb/source/NewlyAddedTableITCase.java
Patch:
@@ -79,7 +79,7 @@ public NewlyAddedTableITCase(String mongoVersion) {
 
     @Parameterized.Parameters(name = "mongoVersion: {0}")
     public static Object[] parameters() {
-        return Stream.of(MONGO_VERSIONS).map(e -> new Object[] {e}).toArray();
+        return Stream.of(getMongoVersions()).map(e -> new Object[] {e}).toArray();
     }
 
     private final ScheduledExecutorService mockChangelogExecutor =

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mongodb-cdc/src/test/java/org/apache/flink/cdc/connectors/mongodb/source/reader/MongoDBSnapshotSplitReaderTest.java
Patch:
@@ -81,7 +81,7 @@ public MongoDBSnapshotSplitReaderTest(String mongoVersion) {
 
     @Parameterized.Parameters(name = "mongoVersion: {0}")
     public static Object[] parameters() {
-        return Stream.of(MONGO_VERSIONS).map(e -> new Object[] {e}).toArray();
+        return Stream.of(getMongoVersions()).map(e -> new Object[] {e}).toArray();
     }
 
     @Before

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mongodb-cdc/src/test/java/org/apache/flink/cdc/connectors/mongodb/source/reader/MongoDBStreamSplitReaderTest.java
Patch:
@@ -95,7 +95,7 @@ public MongoDBStreamSplitReaderTest(String mongoVersion) {
 
     @Parameterized.Parameters(name = "mongoVersion: {0}")
     public static Object[] parameters() {
-        return Stream.of(MONGO_VERSIONS).map(e -> new Object[] {e}).toArray();
+        return Stream.of(getMongoVersions()).map(e -> new Object[] {e}).toArray();
     }
 
     @Before

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mongodb-cdc/src/test/java/org/apache/flink/cdc/connectors/mongodb/table/MongoDBTimeZoneITCase.java
Patch:
@@ -66,7 +66,7 @@ public MongoDBTimeZoneITCase(
             name = "mongoVersion: {0}, localTimeZone: {1}, parallelismSnapshot: {2}")
     public static Object[] parameters() {
         List<Object[]> parameterTuples = new ArrayList<>();
-        for (String mongoVersion : MONGO_VERSIONS) {
+        for (String mongoVersion : getMongoVersions()) {
             for (String timezone : new String[] {"Asia/Shanghai", "Europe/Berlin", "UTC"}) {
                 for (boolean parallelismSnapshot : new boolean[] {true, false}) {
                     parameterTuples.add(new Object[] {mongoVersion, timezone, parallelismSnapshot});

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/operators/schema/coordinator/SchemaRegistryRequestHandler.java
Patch:
@@ -352,8 +352,8 @@ public void close() throws IOException {
 
     private List<SchemaChangeEvent> calculateDerivedSchemaChangeEvents(SchemaChangeEvent event) {
         if (SchemaChangeBehavior.LENIENT.equals(schemaChangeBehavior)) {
-            return lenientizeSchemaChangeEvent(event).stream()
-                    .flatMap(evt -> schemaDerivation.applySchemaChange(evt).stream())
+            return schemaDerivation.applySchemaChange(event).stream()
+                    .flatMap(evt -> lenientizeSchemaChangeEvent(evt).stream())
                     .collect(Collectors.toList());
         } else {
             return schemaDerivation.applySchemaChange(event);

File: flink-cdc-runtime/src/test/java/org/apache/flink/cdc/runtime/testutils/operators/EventOperatorTestHarness.java
Patch:
@@ -164,7 +164,7 @@ public OP getOperator() {
 
     public void registerTableSchema(TableId tableId, Schema schema) {
         schemaRegistry.handleCoordinationRequest(
-                new SchemaChangeRequest(tableId, new CreateTableEvent(tableId, schema)));
+                new SchemaChangeRequest(tableId, new CreateTableEvent(tableId, schema), 0));
         schemaRegistry.handleApplyEvolvedSchemaChangeRequest(new CreateTableEvent(tableId, schema));
     }
 

File: flink-cdc-common/src/main/java/org/apache/flink/cdc/common/pipeline/PipelineOptions.java
Patch:
@@ -48,7 +48,7 @@ public class PipelineOptions {
     public static final ConfigOption<SchemaChangeBehavior> PIPELINE_SCHEMA_CHANGE_BEHAVIOR =
             ConfigOptions.key("schema.change.behavior")
                     .enumType(SchemaChangeBehavior.class)
-                    .defaultValue(SchemaChangeBehavior.EVOLVE)
+                    .defaultValue(SchemaChangeBehavior.LENIENT)
                     .withDescription(
                             Description.builder()
                                     .text("Behavior for handling schema change events. ")

File: flink-cdc-e2e-tests/flink-cdc-pipeline-e2e-tests/src/test/java/org/apache/flink/cdc/pipeline/tests/MysqlE2eITCase.java
Patch:
@@ -210,7 +210,8 @@ public void testSchemaChangeEvents() throws Exception {
                                 + "  type: values\n"
                                 + "\n"
                                 + "pipeline:\n"
-                                + "  parallelism: %d",
+                                + "  parallelism: %d\n"
+                                + "  schema.change.behavior: evolve",
                         INTER_CONTAINER_MYSQL_ALIAS,
                         MYSQL_TEST_USER,
                         MYSQL_TEST_PASSWORD,

File: flink-cdc-e2e-tests/flink-cdc-pipeline-e2e-tests/src/test/java/org/apache/flink/cdc/pipeline/tests/SchemaEvolveE2eITCase.java
Patch:
@@ -190,7 +190,6 @@ public void testLenientSchemaEvolution() throws Exception {
                         "AddColumnEvent{tableId=%s.members, addedColumns=[ColumnWithPosition{column=`precise_age` DOUBLE, position=LAST, existedColumnName=null}]}",
                         "AddColumnEvent{tableId=%s.members, addedColumns=[ColumnWithPosition{column=`biological_sex` TINYINT, position=LAST, existedColumnName=null}]}",
                         "DataChangeEvent{tableId=%s.members, before=[], after=[1013, Fiona, null, null, 16.0, null], op=INSERT, meta=()}",
-                        "TruncateTableEvent{tableId=%s.members}",
                         "DataChangeEvent{tableId=%s.members, before=[], after=[1014, Gem, null, null, 17.0, null], op=INSERT, meta=()}"));
 
         assertNotExists(

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mongodb-cdc/src/test/java/org/apache/flink/cdc/connectors/mongodb/LegacyMongoDBSourceExampleTest.java
Patch:
@@ -26,19 +26,18 @@
 
 import static org.apache.flink.cdc.connectors.mongodb.LegacyMongoDBContainer.FLINK_USER;
 import static org.apache.flink.cdc.connectors.mongodb.LegacyMongoDBContainer.FLINK_USER_PASSWORD;
-import static org.apache.flink.cdc.connectors.mongodb.LegacyMongoDBTestBase.MONGODB_CONTAINER;
 
 /** Example Tests for {@link MongoDBSource}. */
 public class LegacyMongoDBSourceExampleTest extends LegacyMongoDBSourceTestBase {
 
     @Test
     @Ignore("Test ignored because it won't stop and is used for manual test")
     public void testConsumingAllEvents() throws Exception {
-        String inventory = SHARD.executeCommandFileInSeparateDatabase("inventory");
+        String inventory = ROUTER.executeCommandFileInSeparateDatabase("inventory");
 
         SourceFunction<String> sourceFunction =
                 MongoDBSource.<String>builder()
-                        .hosts(MONGODB_CONTAINER.getHostAndPort())
+                        .hosts(ROUTER.getHostAndPort())
                         .username(FLINK_USER)
                         .password(FLINK_USER_PASSWORD)
                         .databaseList(inventory)

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-sqlserver-cdc/src/main/java/org/apache/flink/cdc/connectors/sqlserver/source/utils/SqlServerTypeUtils.java
Patch:
@@ -71,8 +71,8 @@ private static DataType convertFromColumn(Column column) {
                 return DataTypes.DATE();
             case Types.TIMESTAMP:
             case Types.TIMESTAMP_WITH_TIMEZONE:
-                return column.length() >= 0
-                        ? DataTypes.TIMESTAMP(column.length())
+                return column.scale().isPresent()
+                        ? DataTypes.TIMESTAMP(column.scale().get())
                         : DataTypes.TIMESTAMP();
             case Types.BOOLEAN:
                 return DataTypes.BOOLEAN();

File: flink-cdc-e2e-tests/flink-cdc-pipeline-e2e-tests/src/test/java/org/apache/flink/cdc/pipeline/tests/RouteE2eITCase.java
Patch:
@@ -709,7 +709,7 @@ public void testMergeTableRouteWithTransform() throws Exception {
 
         validateResult(
                 "AddColumnEvent{tableId=%s.ALL, addedColumns=[ColumnWithPosition{column=`NAME` VARCHAR(17), position=LAST, existedColumnName=null}]}",
-                "DataChangeEvent{tableId=%s.ALL, before=[], after=[10001, 12, Derrida, extras], op=INSERT, meta=()}",
+                "DataChangeEvent{tableId=%s.ALL, before=[], after=[10001, 12, extras, Derrida], op=INSERT, meta=()}",
                 "AddColumnEvent{tableId=%s.ALL, addedColumns=[ColumnWithPosition{column=`VERSION_EX` VARCHAR(17), position=LAST, existedColumnName=null}]}",
                 "DataChangeEvent{tableId=%s.ALL, before=[], after=[10002, null, extras, null, 15], op=INSERT, meta=()}",
                 "AlterColumnTypeEvent{tableId=%s.ALL, typeMapping={VERSION=STRING}, oldTypeMapping={VERSION=VARCHAR(17)}}",

File: flink-cdc-common/src/main/java/org/apache/flink/cdc/common/event/RenameColumnEvent.java
Patch:
@@ -17,13 +17,16 @@
 
 package org.apache.flink.cdc.common.event;
 
+import org.apache.flink.cdc.common.annotation.PublicEvolving;
+
 import java.util.Map;
 import java.util.Objects;
 
 /**
  * A {@link SchemaChangeEvent} that represents an {@code RENAME COLUMN} DDL, which may contain the
  * lenient column type changes.
  */
+@PublicEvolving
 public class RenameColumnEvent implements SchemaChangeEvent {
 
     private static final long serialVersionUID = 1L;

File: flink-cdc-common/src/test/java/org/apache/flink/cdc/common/utils/SchemaUtilsTest.java
Patch:
@@ -40,7 +40,7 @@
 public class SchemaUtilsTest {
 
     @Test
-    public void testApplySchemaChangeEvent() {
+    public void testApplyColumnSchemaChangeEvent() {
         TableId tableId = TableId.parse("default.default.table1");
         Schema schema =
                 Schema.newBuilder()

File: flink-cdc-composer/src/test/java/org/apache/flink/cdc/composer/flink/FlinkPipelineComposerITCase.java
Patch:
@@ -798,7 +798,7 @@ void testMergingWithRoute() throws Exception {
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[], after=[2, Bob, 20], op=INSERT, meta=()}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[2, Bob, 20], after=[2, Bob, 30], op=UPDATE, meta=()}",
                         "AddColumnEvent{tableId=default_namespace.default_schema.merged, addedColumns=[ColumnWithPosition{column=`description` STRING, position=LAST, existedColumnName=null}]}",
-                        "AlterColumnTypeEvent{tableId=default_namespace.default_schema.merged, nameMapping={age=BIGINT, id=BIGINT}}",
+                        "AlterColumnTypeEvent{tableId=default_namespace.default_schema.merged, typeMapping={age=BIGINT, id=BIGINT}, oldTypeMapping={age=INT, id=INT}}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[], after=[3, Charlie, 15, student], op=INSERT, meta=()}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[], after=[4, Donald, 25, student], op=INSERT, meta=()}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[4, Donald, 25, student], after=[], op=DELETE, meta=()}",
@@ -1004,7 +1004,7 @@ void testTransformMergingWithRoute() throws Exception {
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[], after=[2, Bob, 20, last_name], op=INSERT, meta=()}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[2, Bob, 20, last_name], after=[2, Bob, 30, last_name], op=UPDATE, meta=()}",
                         "AddColumnEvent{tableId=default_namespace.default_schema.merged, addedColumns=[ColumnWithPosition{column=`description` STRING, position=LAST, existedColumnName=null}]}",
-                        "AlterColumnTypeEvent{tableId=default_namespace.default_schema.merged, nameMapping={age=BIGINT, id=BIGINT}}",
+                        "AlterColumnTypeEvent{tableId=default_namespace.default_schema.merged, typeMapping={age=BIGINT, id=BIGINT}, oldTypeMapping={age=INT, id=INT}}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[], after=[3, Charlie, 15, last_name, student], op=INSERT, meta=()}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[], after=[4, Donald, 25, last_name, student], op=INSERT, meta=()}",
                         "DataChangeEvent{tableId=default_namespace.default_schema.merged, before=[4, Donald, 25, last_name, student], after=[], op=DELETE, meta=()}",

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-paimon/src/main/java/org/apache/flink/cdc/connectors/paimon/sink/v2/bucket/BucketAssignOperator.java
Patch:
@@ -130,7 +130,7 @@ public void processElement(StreamRecord<Event> streamRecord) throws Exception {
 
         if (event instanceof DataChangeEvent) {
             DataChangeEvent dataChangeEvent = (DataChangeEvent) event;
-            if (schemaMaps.containsKey(dataChangeEvent.tableId())) {
+            if (!schemaMaps.containsKey(dataChangeEvent.tableId())) {
                 Optional<Schema> schema =
                         schemaEvolutionClient.getLatestEvolvedSchema(dataChangeEvent.tableId());
                 if (schema.isPresent()) {

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/operators/transform/TransformProjectionProcessor.java
Patch:
@@ -113,7 +113,7 @@ public Schema processSchemaChangeEvent(Schema schema) {
                         .collect(Collectors.toList()));
     }
 
-    public BinaryRecordData processData(BinaryRecordData payload, long epochTime) {
+    public BinaryRecordData processData(BinaryRecordData payload, long epochTime, String opType) {
         List<Object> valueList = new ArrayList<>();
         List<Column> columns = postTransformChangeInfo.getPostTransformedSchema().getColumns();
 
@@ -124,7 +124,7 @@ public BinaryRecordData processData(BinaryRecordData payload, long epochTime) {
                 ProjectionColumn projectionColumn = projectionColumnProcessor.getProjectionColumn();
                 valueList.add(
                         DataTypeConverter.convert(
-                                projectionColumnProcessor.evaluate(payload, epochTime),
+                                projectionColumnProcessor.evaluate(payload, epochTime, opType),
                                 projectionColumn.getDataType()));
             } else {
                 Column column = columns.get(i);

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/test/java/org/apache/flink/cdc/connectors/mysql/source/MySqlMetadataAccessorITCase.java
Patch:
@@ -210,7 +210,7 @@ public void testMysql8AccessTimeTypesSchema() {
     private void testAccessDatabaseAndTable(UniqueDatabase database) {
         database.createAndInitialize();
 
-        String[] tables = new String[] {"common_types", "time_types"};
+        String[] tables = new String[] {"common_types", "time_types", "precision_types"};
         MySqlMetadataAccessor metadataAccessor = getMetadataAccessor(tables, database);
 
         assertThatThrownBy(metadataAccessor::listNamespaces)

File: flink-cdc-e2e-tests/flink-cdc-pipeline-e2e-tests/src/test/java/org/apache/flink/cdc/pipeline/tests/MysqlE2eITCase.java
Patch:
@@ -127,7 +127,7 @@ public void testSyncWholeDatabase() throws Exception {
         List<String> expectedEvents =
                 Arrays.asList(
                         String.format(
-                                "CreateTableEvent{tableId=%s.customers, schema=columns={`id` INT NOT NULL,`name` VARCHAR(255) NOT NULL,`address` VARCHAR(1024),`phone_number` VARCHAR(512)}, primaryKeys=id, options=()}",
+                                "CreateTableEvent{tableId=%s.customers, schema=columns={`id` INT NOT NULL,`name` VARCHAR(255) NOT NULL 'flink',`address` VARCHAR(1024),`phone_number` VARCHAR(512)}, primaryKeys=id, options=()}",
                                 mysqlInventoryDatabase.getDatabaseName()),
                         String.format(
                                 "DataChangeEvent{tableId=%s.customers, before=[], after=[104, user_4, Shanghai, 123567891234], op=INSERT, meta=()}",
@@ -142,7 +142,7 @@ public void testSyncWholeDatabase() throws Exception {
                                 "DataChangeEvent{tableId=%s.customers, before=[], after=[101, user_1, Shanghai, 123567891234], op=INSERT, meta=()}",
                                 mysqlInventoryDatabase.getDatabaseName()),
                         String.format(
-                                "CreateTableEvent{tableId=%s.products, schema=columns={`id` INT NOT NULL,`name` VARCHAR(255) NOT NULL,`description` VARCHAR(512),`weight` FLOAT,`enum_c` STRING,`json_c` STRING,`point_c` STRING}, primaryKeys=id, options=()}",
+                                "CreateTableEvent{tableId=%s.products, schema=columns={`id` INT NOT NULL,`name` VARCHAR(255) NOT NULL 'flink',`description` VARCHAR(512),`weight` FLOAT,`enum_c` STRING 'red',`json_c` STRING,`point_c` STRING}, primaryKeys=id, options=()}",
                                 mysqlInventoryDatabase.getDatabaseName()),
                         String.format(
                                 "DataChangeEvent{tableId=%s.products, before=[], after=[109, spare tire, 24 inch spare tire, 22.2, null, null, null], op=INSERT, meta=()}",

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/test/java/org/apache/flink/cdc/connectors/mysql/source/MySqlPipelineITCase.java
Patch:
@@ -355,7 +355,7 @@ private CreateTableEvent getProductsCreateTableEvent(TableId tableId) {
                 tableId,
                 Schema.newBuilder()
                         .physicalColumn("id", DataTypes.INT().notNull())
-                        .physicalColumn("name", DataTypes.VARCHAR(255).notNull())
+                        .physicalColumn("name", DataTypes.VARCHAR(255).notNull(), null, "flink")
                         .physicalColumn("description", DataTypes.VARCHAR(512))
                         .physicalColumn("weight", DataTypes.FLOAT())
                         .primaryKey(Collections.singletonList("id"))

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-starrocks/src/main/java/org/apache/flink/cdc/connectors/starrocks/sink/StarRocksUtils.java
Patch:
@@ -84,7 +84,8 @@ public static StarRocksTable toStarRocksTable(
                     new StarRocksColumn.Builder()
                             .setColumnName(column.getName())
                             .setOrdinalPosition(i)
-                            .setColumnComment(column.getComment());
+                            .setColumnComment(column.getComment())
+                            .setDefaultValue(column.getDefaultValueExpression());
             toStarRocksDataType(column, i < primaryKeyCount, builder);
             starRocksColumns.add(builder.build());
         }

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/operators/schema/coordinator/SchemaRegistry.java
Patch:
@@ -214,6 +214,7 @@ public void resetToCheckpoint(long checkpointId, @Nullable byte[] checkpointData
                         break;
                     }
                 case 1:
+                case 2:
                     {
                         int length = in.readInt();
                         byte[] serializedSchemaManager = new byte[length];

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/operators/transform/TableChangeInfo.java
Patch:
@@ -109,7 +109,7 @@ public static TableChangeInfo of(
     /** Serializer for {@link TableChangeInfo}. */
     public static class Serializer implements SimpleVersionedSerializer<TableChangeInfo> {
 
-        public static final int CURRENT_VERSION = 1;
+        public static final int CURRENT_VERSION = 2;
 
         @Override
         public int getVersion() {

File: flink-cdc-runtime/src/test/java/org/apache/flink/cdc/runtime/serializer/schema/PhysicalColumnSerializerTest.java
Patch:
@@ -44,7 +44,8 @@ protected Class<PhysicalColumn> getTypeClass() {
     protected PhysicalColumn[] getTestData() {
         return new PhysicalColumn[] {
             Column.physicalColumn("col1", DataTypes.BIGINT()),
-            Column.physicalColumn("col1", DataTypes.BIGINT(), "comment")
+            Column.physicalColumn("col1", DataTypes.BIGINT(), "comment"),
+            Column.physicalColumn("col1", DataTypes.BIGINT(), "comment", "default value")
         };
     }
 }

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-starrocks/src/test/java/org/apache/flink/cdc/connectors/starrocks/sink/StarRocksMetadataApplierITCase.java
Patch:
@@ -357,9 +357,6 @@ private void runJobWithEvents(List<Event> events) throws Exception {
                         .set(JDBC_URL, STARROCKS_CONTAINER.getJdbcUrl())
                         .set(USERNAME, StarRocksContainer.STARROCKS_USERNAME)
                         .set(PASSWORD, StarRocksContainer.STARROCKS_PASSWORD);
-        config.addAll(
-                Configuration.fromMap(
-                        Collections.singletonMap("table.create.properties.replication_num", "1")));
 
         DataSink starRocksSink = createStarRocksDataSink(config);
 

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-starrocks/src/test/java/org/apache/flink/cdc/connectors/starrocks/sink/StarRocksMetadataApplierITCase.java
Patch:
@@ -357,6 +357,9 @@ private void runJobWithEvents(List<Event> events) throws Exception {
                         .set(JDBC_URL, STARROCKS_CONTAINER.getJdbcUrl())
                         .set(USERNAME, StarRocksContainer.STARROCKS_USERNAME)
                         .set(PASSWORD, StarRocksContainer.STARROCKS_PASSWORD);
+        config.addAll(
+                Configuration.fromMap(
+                        Collections.singletonMap("table.create.properties.replication_num", "1")));
 
         DataSink starRocksSink = createStarRocksDataSink(config);
 

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-oracle-cdc/src/main/java/org/apache/flink/cdc/connectors/oracle/source/assigner/splitter/OracleChunkSplitter.java
Patch:
@@ -102,7 +102,7 @@ protected boolean isEvenlySplitColumn(Column splitColumn) {
 
     /** ChunkEnd less than or equal to max. */
     @Override
-    protected boolean isChunkEndLeMax(Object chunkEnd, Object max) {
+    protected boolean isChunkEndLeMax(Object chunkEnd, Object max, Column splitColumn) {
         boolean chunkEndMaxCompare;
         if (chunkEnd instanceof ROWID && max instanceof ROWID) {
             chunkEndMaxCompare =
@@ -116,7 +116,7 @@ protected boolean isChunkEndLeMax(Object chunkEnd, Object max) {
 
     /** ChunkEnd greater than or equal to max. */
     @Override
-    protected boolean isChunkEndGeMax(Object chunkEnd, Object max) {
+    protected boolean isChunkEndGeMax(Object chunkEnd, Object max, Column splitColumn) {
         boolean chunkEndMaxCompare;
         if (chunkEnd instanceof ROWID && max instanceof ROWID) {
             chunkEndMaxCompare =

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-sqlserver-cdc/src/main/java/org/apache/flink/cdc/connectors/sqlserver/source/utils/SqlServerTypeUtils.java
Patch:
@@ -27,6 +27,9 @@
 /** Utilities for converting from SqlServer types to Flink types. */
 public class SqlServerTypeUtils {
 
+    /** Microsoft SQL type GUID's type name. */
+    static final String UNIQUEIDENTIFIRER = "uniqueidentifier";
+
     /** Returns a corresponding Flink data type from a debezium {@link Column}. */
     public static DataType fromDbzColumn(Column column) {
         DataType dataType = convertFromColumn(column);

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/main/java/org/apache/flink/cdc/connectors/mysql/debezium/task/context/StatefulTaskContext.java
Patch:
@@ -193,7 +193,9 @@ protected MySqlOffsetContext loadStartingOffsetState(
                 mySqlSplit.isSnapshotSplit()
                         ? BinlogOffset.ofEarliest()
                         : initializeEffectiveOffset(
-                                mySqlSplit.asBinlogSplit().getStartingOffset(), connection);
+                                mySqlSplit.asBinlogSplit().getStartingOffset(),
+                                connection,
+                                sourceConfig);
 
         LOG.info("Starting offset is initialized to {}", offset);
 

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/test/java/org/apache/flink/cdc/connectors/mysql/debezium/reader/BinlogSplitReaderTest.java
Patch:
@@ -1262,7 +1262,8 @@ protected MySqlOffsetContext loadStartingOffsetState(
                             ? BinlogOffset.ofEarliest()
                             : initializeEffectiveOffset(
                                     mySqlSplit.asBinlogSplit().getStartingOffset(),
-                                    getConnection());
+                                    getConnection(),
+                                    getSourceConfig());
 
             LOG.info("Starting offset is initialized to {}", offset);
 

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/serializer/data/writer/AbstractBinaryWriter.java
Patch:
@@ -138,7 +138,7 @@ public void writeBinary(int pos, byte[] bytes) {
 
     @Override
     public void writeDecimal(int pos, DecimalData value, int precision) {
-        assert value == null || (value.precision() == precision);
+        assert value == null || (value.precision() <= precision);
 
         if (DecimalData.isCompact(precision)) {
             assert value != null;

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-doris/src/test/java/org/apache/flink/cdc/connectors/doris/sink/DorisMetadataApplierITCase.java
Patch:
@@ -45,7 +45,6 @@
 import org.junit.After;
 import org.junit.Before;
 import org.junit.BeforeClass;
-import org.junit.Ignore;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
@@ -367,7 +366,6 @@ public void testDorisRenameColumn() throws Exception {
     }
 
     @Test
-    @Ignore("AlterColumnType is yet to be supported until we close FLINK-35072.")
     public void testDorisAlterColumnType() throws Exception {
         TableId tableId =
                 TableId.tableId(

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-sqlserver-cdc/src/main/java/org/apache/flink/cdc/connectors/sqlserver/source/reader/fetch/SqlServerStreamFetchTask.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.flink.cdc.connectors.base.source.meta.wartermark.WatermarkKind;
 import org.apache.flink.cdc.connectors.base.source.reader.external.FetchTask;
 import org.apache.flink.cdc.connectors.sqlserver.source.offset.LsnOffset;
+import org.apache.flink.cdc.connectors.sqlserver.source.reader.fetch.SqlServerScanFetchTask.SqlServerSnapshotSplitChangeEventSourceContext;
 
 import io.debezium.DebeziumException;
 import io.debezium.connector.sqlserver.Lsn;
@@ -142,9 +143,7 @@ public void afterHandleLsn(SqlServerPartition partition, Lsn toLsn) {
                                 new DebeziumException("Error processing binlog signal event", e));
                     }
                     // tell fetcher the streaming task finished
-                    ((SqlServerScanFetchTask.SqlserverSnapshotSplitChangeEventSourceContext)
-                                    context)
-                            .finished();
+                    ((SqlServerSnapshotSplitChangeEventSourceContext) context).finished();
                 }
             }
         }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-sqlserver-cdc/src/test/java/org/apache/flink/cdc/connectors/sqlserver/table/SqlServerConnectorITCase.java
Patch:
@@ -57,7 +57,7 @@ public class SqlServerConnectorITCase extends SqlServerTestBase {
 
     @ClassRule public static LegacyRowResource usesLegacyRows = LegacyRowResource.INSTANCE;
 
-    // enable the parallelismSnapshot (i.e: The new source OracleParallelSource)
+    // enable the parallelismSnapshot (i.e: The new source JdbcIncrementalSource)
     private final boolean parallelismSnapshot;
 
     public SqlServerConnectorITCase(boolean parallelismSnapshot) {

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-postgres-cdc/src/main/java/org/apache/flink/cdc/connectors/postgres/source/utils/PostgresTypeUtils.java
Patch:
@@ -60,6 +60,7 @@ public class PostgresTypeUtils {
     private static final String PG_CHARACTER_ARRAY = "_character";
     private static final String PG_CHARACTER_VARYING = "varchar";
     private static final String PG_CHARACTER_VARYING_ARRAY = "_varchar";
+    private static final String PG_UUID = "uuid";
 
     /** Returns a corresponding Flink data type from a debezium {@link Column}. */
     public static DataType fromDbzColumn(Column column) {
@@ -136,6 +137,7 @@ private static DataType convertFromColumn(Column column) {
             case PG_CHARACTER_VARYING_ARRAY:
                 return DataTypes.ARRAY(DataTypes.VARCHAR(precision));
             case PG_TEXT:
+            case PG_UUID:
                 return DataTypes.STRING();
             case PG_TEXT_ARRAY:
                 return DataTypes.ARRAY(DataTypes.STRING());

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-postgres-cdc/src/test/java/org/apache/flink/cdc/connectors/postgres/source/PostgresSourceExampleTest.java
Patch:
@@ -108,7 +108,6 @@ public void testConsumingScanEvents() throws Exception {
                         .slotName(SLOT_NAME)
                         .decodingPluginName(PLUGIN_NAME)
                         .deserializer(deserializer)
-                        .includeSchemaChanges(true) // output the schema changes as well
                         .splitSize(2)
                         .build();
 
@@ -153,7 +152,6 @@ public void testConsumingAllEvents() throws Exception {
                         .slotName(SLOT_NAME)
                         .decodingPluginName(PLUGIN_NAME)
                         .deserializer(buildRowDataDebeziumDeserializeSchema(dataType))
-                        .includeSchemaChanges(true) // output the schema changes as well
                         .splitSize(2)
                         .debeziumProperties(debeziumProps)
                         .build();

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-oracle-cdc/src/main/java/org/apache/flink/cdc/connectors/oracle/source/config/OracleSourceConfig.java
Patch:
@@ -63,7 +63,8 @@ public OracleSourceConfig(
             int connectMaxRetries,
             int connectionPoolSize,
             String chunkKeyColumn,
-            boolean skipSnapshotBackfill) {
+            boolean skipSnapshotBackfill,
+            boolean scanNewlyAddedTableEnabled) {
         super(
                 startupOptions,
                 databaseList,
@@ -89,7 +90,7 @@ public OracleSourceConfig(
                 connectionPoolSize,
                 chunkKeyColumn,
                 skipSnapshotBackfill,
-                false);
+                scanNewlyAddedTableEnabled);
         this.url = url;
     }
 

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-oracle-cdc/src/main/java/org/apache/flink/cdc/connectors/oracle/source/config/OracleSourceConfigFactory.java
Patch:
@@ -133,6 +133,7 @@ public OracleSourceConfig create(int subtaskId) {
                 connectMaxRetries,
                 connectionPoolSize,
                 chunkKeyColumn,
-                skipSnapshotBackfill);
+                skipSnapshotBackfill,
+                scanNewlyAddedTableEnabled);
     }
 }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-oracle-cdc/src/test/java/org/apache/flink/cdc/connectors/oracle/source/OracleSourceTestBase.java
Patch:
@@ -174,7 +174,7 @@ public static void createAndInitialize(String sqlFile) throws Exception {
     }
 
     // ------------------ utils -----------------------
-    private static List<TableId> listTables(Connection connection) {
+    protected static List<TableId> listTables(Connection connection) {
 
         Set<TableId> tableIdSet = new HashSet<>();
         String queryTablesSql =

File: flink-cdc-common/src/main/java/org/apache/flink/cdc/common/pipeline/PipelineOptions.java
Patch:
@@ -42,7 +42,7 @@ public class PipelineOptions {
     public static final ConfigOption<Integer> PIPELINE_PARALLELISM =
             ConfigOptions.key("parallelism")
                     .intType()
-                    .noDefaultValue()
+                    .defaultValue(1)
                     .withDescription("Parallelism of the pipeline");
 
     public static final ConfigOption<SchemaChangeBehavior> PIPELINE_SCHEMA_CHANGE_BEHAVIOR =

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-doris/src/main/java/org/apache/flink/cdc/connectors/doris/sink/DorisEventSerializer.java
Patch:
@@ -56,7 +56,7 @@ public class DorisEventSerializer implements DorisRecordSerializer<Event> {
 
     /** Format timestamp-related type data. */
     public static final DateTimeFormatter DATE_TIME_FORMATTER =
-            DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");
+            DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSSSSS");
 
     /** ZoneId from pipeline config to support timestamp with local time zone. */
     public final ZoneId pipelineZoneId;

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/operators/transform/TableChangeInfo.java
Patch:
@@ -109,7 +109,7 @@ public static TableChangeInfo of(
     /** Serializer for {@link TableChangeInfo}. */
     public static class Serializer implements SimpleVersionedSerializer<TableChangeInfo> {
 
-        public static final int CURRENT_VERSION = 0;
+        public static final int CURRENT_VERSION = 1;
 
         @Override
         public int getVersion() {
@@ -140,9 +140,9 @@ public TableChangeInfo deserialize(int version, byte[] serialized) throws IOExce
                     DataInputStream in = new DataInputStream(bais)) {
                 TableId tableId = tableIdSerializer.deserialize(new DataInputViewStreamWrapper(in));
                 Schema originalSchema =
-                        schemaSerializer.deserialize(new DataInputViewStreamWrapper(in));
+                        schemaSerializer.deserialize(version, new DataInputViewStreamWrapper(in));
                 Schema transformedSchema =
-                        schemaSerializer.deserialize(new DataInputViewStreamWrapper(in));
+                        schemaSerializer.deserialize(version, new DataInputViewStreamWrapper(in));
                 return TableChangeInfo.of(tableId, originalSchema, transformedSchema);
             }
         }

File: flink-cdc-runtime/src/test/java/org/apache/flink/cdc/runtime/operators/schema/coordinator/SchemaManagerTest.java
Patch:
@@ -195,7 +195,9 @@ void testSerde() throws Exception {
         schemaManager.applySchemaChange(new CreateTableEvent(CUSTOMERS, CUSTOMERS_SCHEMA));
         schemaManager.applySchemaChange(new CreateTableEvent(PRODUCTS, PRODUCTS_SCHEMA));
         byte[] serialized = SchemaManager.SERIALIZER.serialize(schemaManager);
-        SchemaManager deserialized = SchemaManager.SERIALIZER.deserialize(0, serialized);
+        SchemaManager deserialized =
+                SchemaManager.SERIALIZER.deserialize(
+                        SchemaManager.Serializer.CURRENT_VERSION, serialized);
         assertThat(deserialized).isEqualTo(schemaManager);
     }
 }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-db2-cdc/src/main/java/io/debezium/connector/db2/Db2Connection.java
Patch:
@@ -323,7 +323,8 @@ public Set<Db2ChangeTable> listOfChangeTables() throws SQLException {
                          */
                         changeTables.add(
                                 new Db2ChangeTable(
-                                        new TableId("", rs.getString(1), rs.getString(2)),
+                                        new TableId(
+                                                realDatabaseName, rs.getString(1), rs.getString(2)),
                                         rs.getString(4),
                                         rs.getInt(9),
                                         Lsn.valueOf(rs.getBytes(5)),

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/operators/transform/TransformSchemaOperator.java
Patch:
@@ -96,6 +96,7 @@ private TransformSchemaOperator(
     @Override
     public void open() throws Exception {
         super.open();
+        transforms = new ArrayList<>();
         for (Tuple5<String, String, String, String, String> transformRule : transformRules) {
             String tableInclusions = transformRule.f0;
             String projection = transformRule.f1;
@@ -104,7 +105,6 @@ public void open() throws Exception {
             String tableOptions = transformRule.f4;
             Selectors selectors =
                     new Selectors.SelectorsBuilder().includeTables(tableInclusions).build();
-            transforms = new ArrayList<>();
             transforms.add(new Tuple2<>(selectors, TransformProjection.of(projection)));
             schemaMetadataTransformers.add(
                     new Tuple2<>(

File: flink-cdc-runtime/src/test/java/org/apache/flink/cdc/runtime/operators/transform/TransformDataOperatorTest.java
Patch:
@@ -465,7 +465,7 @@ void testTimestampTransform() throws Exception {
                         .addTransform(
                                 TIMESTAMP_TABLEID.identifier(),
                                 "col1, IF(LOCALTIME = CURRENT_TIME, 1, 0) as time_equal,"
-                                        + " IF(LOCALTIMESTAMP = CURRENT_TIMESTAMP and NOW() = LOCALTIMESTAMP, 1, 0) as timestamp_equal,"
+                                        + " IF(LOCALTIMESTAMP = CURRENT_TIMESTAMP, 1, 0) as timestamp_equal,"
                                         + " IF(TO_DATE(DATE_FORMAT(LOCALTIMESTAMP, 'yyyy-MM-dd')) = CURRENT_DATE, 1, 0) as date_equal",
                                 "LOCALTIMESTAMP = CURRENT_TIMESTAMP")
                         .addTimezone("GMT")

File: flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/operators/schema/coordinator/SchemaDerivation.java
Patch:
@@ -123,7 +123,7 @@ public static void serializeDerivationMapping(
         TableIdSerializer tableIdSerializer = TableIdSerializer.INSTANCE;
         // Serialize derivation mapping in SchemaDerivation
         Map<TableId, Set<TableId>> derivationMapping = schemaDerivation.getDerivationMapping();
-        out.write(derivationMapping.size());
+        out.writeInt(derivationMapping.size());
         for (Map.Entry<TableId, Set<TableId>> entry : derivationMapping.entrySet()) {
             // Routed table ID
             TableId routedTableId = entry.getKey();

File: flink-cdc-connect/flink-cdc-source-connectors/flink-cdc-base/src/main/java/org/apache/flink/cdc/connectors/base/relational/handler/SchemaChangeEventHandler.java
Patch:
@@ -26,5 +26,6 @@
 /** This handler helps to parse the source struct in SchemaChangeEvent and generate source info. */
 @Experimental
 public interface SchemaChangeEventHandler {
+
     Map<String, Object> parseSource(SchemaChangeEvent event);
 }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-db2-cdc/src/main/java/org/apache/flink/cdc/connectors/db2/source/Db2SourceBuilder.java
Patch:
@@ -17,6 +17,7 @@
 
 package org.apache.flink.cdc.connectors.db2.source;
 
+import org.apache.flink.cdc.common.annotation.PublicEvolving;
 import org.apache.flink.cdc.connectors.base.options.StartupOptions;
 import org.apache.flink.cdc.connectors.base.source.jdbc.JdbcIncrementalSource;
 import org.apache.flink.cdc.connectors.db2.source.config.Db2SourceConfigFactory;
@@ -36,6 +37,7 @@
  * <p>Check the Java docs of each individual method to learn more about the settings to build a
  * {@link Db2IncrementalSource}.
  */
+@PublicEvolving
 public class Db2SourceBuilder<T> {
 
     private final Db2SourceConfigFactory configFactory = new Db2SourceConfigFactory();

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-db2-cdc/src/main/java/org/apache/flink/cdc/connectors/db2/source/handler/Db2SchemaChangeEventHandler.java
Patch:
@@ -29,7 +29,7 @@
 import static io.debezium.connector.db2.SourceInfo.COMMIT_LSN_KEY;
 
 /**
- * This SqlServerSchemaChangeEventHandler helps to parse the source struct in SchemaChangeEvent and
+ * This Db2SchemaChangeEventHandler helps to parse the source struct in SchemaChangeEvent and
  * generate source info.
  */
 public class Db2SchemaChangeEventHandler implements SchemaChangeEventHandler {

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-db2-cdc/src/main/java/org/apache/flink/cdc/connectors/db2/source/utils/Db2ConnectionUtils.java
Patch:
@@ -35,6 +35,7 @@
 
 /** Utils for Db2 connection. */
 public class Db2ConnectionUtils {
+
     private static final Logger LOG = LoggerFactory.getLogger(Db2ConnectionUtils.class);
 
     public static Db2Connection createDb2Connection(

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-db2-cdc/src/main/java/org/apache/flink/cdc/connectors/db2/source/utils/Db2TypeUtils.java
Patch:
@@ -24,7 +24,7 @@
 
 import java.sql.Types;
 
-/** Utilities for converting from Db2 types to Flink types. */
+/** Utilities for converting from Db2 types to Flink SQL types. */
 public class Db2TypeUtils {
 
     /** Returns a corresponding Flink data type from a debezium {@link Column}. */

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-db2-cdc/src/main/java/org/apache/flink/cdc/connectors/db2/table/StartupOptions.java
Patch:
@@ -23,6 +23,7 @@
 
 /** Debezium startup options. */
 public final class StartupOptions {
+
     public final StartupMode startupMode;
 
     /**

File: flink-cdc-e2e-tests/flink-cdc-source-e2e-tests/src/test/java/org/apache/flink/cdc/connectors/tests/Db2E2eITCase.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.flink.cdc.connectors.db2.Db2TestBase;
 import org.apache.flink.cdc.connectors.tests.utils.FlinkContainerTestEnvironment;
 
+import org.awaitility.Awaitility;
 import org.junit.After;
 import org.junit.Before;
 import org.junit.BeforeClass;
@@ -33,6 +34,7 @@
 import org.testcontainers.containers.output.Slf4jLogConsumer;
 import org.testcontainers.images.builder.ImageFromDockerfile;
 import org.testcontainers.lifecycle.Startables;
+import org.testcontainers.shaded.org.awaitility.core.ConditionTimeoutException;
 import org.testcontainers.utility.DockerImageName;
 
 import java.net.URISyntaxException;

File: flink-cdc-cli/src/main/java/org/apache/flink/cdc/cli/utils/FlinkEnvironmentUtils.java
Patch:
@@ -46,7 +46,7 @@ public static Configuration loadFlinkConfiguration(Path flinkHome) throws Except
                     FLINK_CONF_FILENAME,
                     LEGACY_FLINK_CONF_FILENAME);
             return ConfigurationUtils.loadConfigFile(
-                    flinkHome.resolve(FLINK_CONF_DIR).resolve(LEGACY_FLINK_CONF_FILENAME));
+                    flinkHome.resolve(FLINK_CONF_DIR).resolve(LEGACY_FLINK_CONF_FILENAME), true);
         }
     }
 

File: flink-cdc-cli/src/test/java/org/apache/flink/cdc/cli/utils/ConfigurationUtilsTest.java
Patch:
@@ -75,7 +75,8 @@ class ConfigurationUtilsTest {
     void loadConfigFile(String resourcePath) throws Exception {
         URL resource = Resources.getResource(resourcePath);
         Path path = Paths.get(resource.toURI());
-        Configuration configuration = ConfigurationUtils.loadConfigFile(path);
+        Configuration configuration =
+                ConfigurationUtils.loadConfigFile(path, resourcePath.endsWith("flink-conf.yaml"));
         Map<String, String> configMap = configuration.toMap();
         for (Map.Entry<ConfigOption<?>, Object> entry : CONFIG_OPTIONS.entrySet()) {
             String key = entry.getKey().key();

File: flink-cdc-e2e-tests/flink-cdc-pipeline-e2e-tests/src/test/java/org/apache/flink/cdc/pipeline/tests/MysqlE2eITCase.java
Patch:
@@ -110,7 +110,8 @@ public void testSyncWholeDatabase() throws Exception {
                         mysqlInventoryDatabase.getDatabaseName());
         Path mysqlCdcJar = TestUtils.getResource("mysql-cdc-pipeline-connector.jar");
         Path valuesCdcJar = TestUtils.getResource("values-cdc-pipeline-connector.jar");
-        submitPipelineJob(pipelineJob, mysqlCdcJar, valuesCdcJar);
+        Path mysqlDriverJar = TestUtils.getResource("mysql-driver.jar");
+        submitPipelineJob(pipelineJob, mysqlCdcJar, valuesCdcJar, mysqlDriverJar);
         waitUntilJobRunning(Duration.ofSeconds(30));
         LOG.info("Pipeline job is running");
         waitUtilSpecificEvent(

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/main/java/org/apache/flink/cdc/connectors/mysql/source/assigners/MySqlSnapshotSplitAssigner.java
Patch:
@@ -253,6 +253,7 @@ private void captureNewlyAddedTables() {
                             .entrySet()
                             .removeIf(schema -> tablesToRemove.contains(schema.getKey()));
                     remainingSplits.removeIf(split -> tablesToRemove.contains(split.getTableId()));
+                    LOG.info("Enumerator remove tables after restart: {}", tablesToRemove);
                     remainingTables.removeAll(tablesToRemove);
                     alreadyProcessedTables.removeIf(tableId -> tablesToRemove.contains(tableId));
                 }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/main/java/io/debezium/connector/mysql/MySqlStreamingChangeEventSource.java
Patch:
@@ -268,7 +268,8 @@ public Event nextEvent(ByteArrayInputStream inputStream) throws IOException {
 
                             // DBZ-5126 Clean cache on rotate event to prevent it from growing
                             // indefinitely.
-                            if (event.getHeader().getEventType() == EventType.ROTATE) {
+                            if (event.getHeader().getEventType() == EventType.ROTATE
+                                    && event.getHeader().getTimestamp() != 0) {
                                 tableMapEventByTableId.clear();
                             }
                             return event;

File: flink-cdc-e2e-tests/flink-cdc-source-e2e-tests/src/test/java/org/apache/flink/cdc/connectors/tests/MySqlE2eITCase.java
Patch:
@@ -119,6 +119,8 @@ public void testMySqlCDC() throws Exception {
                     "UPDATE products_source SET description='new water resistent white wind breaker', weight='0.5' WHERE id=110;");
             stat.execute("UPDATE products_source SET weight='5.17' WHERE id=111;");
             stat.execute("DELETE FROM products_source WHERE id=111;");
+            // add schema change event in the last.
+            stat.execute("CREATE TABLE new_table (id int, age int);");
         } catch (SQLException e) {
             LOG.error("Update table for CDC failed.", e);
             throw e;

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-oracle-cdc/src/test/java/com/ververica/cdc/connectors/oracle/table/OracleTableSourceFactoryTest.java
Patch:
@@ -83,7 +83,7 @@ public class OracleTableSourceFactoryTest {
     private static final String MY_LOCALHOST = "localhost";
     private static final String MY_USERNAME = "flinkuser";
     private static final String MY_PASSWORD = "flinkpw";
-    private static final String MY_DATABASE = "myDB";
+    private static final String MY_DATABASE = "MYDB";
     private static final String MY_TABLE = "myTable";
     private static final String MY_SCHEMA = "mySchema";
     private static final Properties PROPERTIES = new Properties();

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/source/reader/fetch/MongoDBStreamFetchTask.java
Patch:
@@ -225,7 +225,9 @@ public StreamSplit getSplit() {
     }
 
     @Override
-    public void close() {}
+    public void close() {
+        taskRunning = false;
+    }
 
     private MongoChangeStreamCursor<BsonDocument> openChangeStreamCursor(
             ChangeStreamDescriptor changeStreamDescriptor) {

File: flink-cdc-connect/flink-cdc-source-connectors/flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/IncrementalSource.java
Patch:
@@ -78,7 +78,7 @@ public class IncrementalSource<T, C extends SourceConfig>
     // This field is introduced for testing purpose, for example testing if changes made in the
     // snapshot phase are correctly backfilled into the snapshot by registering a pre high watermark
     // hook for generating changes.
-    private SnapshotPhaseHooks snapshotHooks = SnapshotPhaseHooks.empty();
+    protected SnapshotPhaseHooks snapshotHooks = SnapshotPhaseHooks.empty();
 
     public IncrementalSource(
             SourceConfig.Factory<C> configFactory,

File: flink-cdc-connect/flink-cdc-source-connectors/flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/reader/IncrementalSourceReader.java
Patch:
@@ -77,7 +77,7 @@ public class IncrementalSourceReader<T, C extends SourceConfig>
     private final int subtaskId;
     private final SourceSplitSerializer sourceSplitSerializer;
     private final C sourceConfig;
-    private final DataSourceDialect<C> dialect;
+    protected final DataSourceDialect<C> dialect;
 
     public IncrementalSourceReader(
             FutureCompletingBlockingQueue<RecordsWithSplitIds<SourceRecords>> elementQueue,

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-postgres-cdc/src/main/java/com/ververica/cdc/connectors/postgres/source/PostgresDialect.java
Patch:
@@ -214,9 +214,9 @@ public JdbcSourceFetchTaskContext createFetchTaskContext(
     }
 
     @Override
-    public void notifyCheckpointComplete(long checkpointId) throws Exception {
+    public void notifyCheckpointComplete(long checkpointId, Offset offset) throws Exception {
         if (streamFetchTask != null) {
-            streamFetchTask.commitCurrentOffset();
+            streamFetchTask.commitCurrentOffset(offset);
         }
     }
 

File: flink-cdc-connect/flink-cdc-source-connectors/flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/meta/split/SourceSplitSerializer.java
Patch:
@@ -159,7 +159,7 @@ public SourceSplitBase deserializeSplit(int version, byte[] serialized) throws I
                     readFinishedSplitsInfo(version, in);
             Map<TableId, TableChange> tableChangeMap = readTableSchemas(version, in);
             int totalFinishedSplitSize = finishedSplitsInfo.size();
-            if (version == 3) {
+            if (version >= 3) {
                 totalFinishedSplitSize = in.readInt();
             }
             in.releaseArrays();

File: flink-cdc-common/src/main/java/com/ververica/cdc/common/utils/InstantiationUtil.java
Patch:
@@ -19,7 +19,6 @@
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.core.memory.DataInputViewStreamWrapper;
 import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
-import org.apache.flink.util.CollectionUtil;
 
 import java.io.ByteArrayInputStream;
 import java.io.ByteArrayOutputStream;
@@ -112,8 +111,7 @@ protected Class<?> resolveProxyClass(String[] interfaces)
 
         // ------------------------------------------------
 
-        private static final HashMap<String, Class<?>> primitiveClasses =
-                CollectionUtil.newHashMapWithExpectedSize(9);
+        private static final HashMap<String, Class<?>> primitiveClasses = new HashMap<>(9);
 
         static {
             primitiveClasses.put("boolean", boolean.class);

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-debezium/src/main/java/com/ververica/cdc/debezium/event/DebeziumSchemaDataTypeInference.java
Patch:
@@ -142,7 +142,7 @@ protected DataType inferString(Object value, Schema schema) {
         if (ZonedTimestamp.SCHEMA_NAME.equals(schema.name())) {
             int nano =
                     Optional.ofNullable((String) value)
-                            .map(Instant::parse)
+                            .map(s -> ZonedTimestamp.FORMATTER.parse(s, Instant::from))
                             .map(Instant::getNano)
                             .orElse(0);
 

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-starrocks/src/main/java/com/ververica/cdc/connectors/starrocks/sink/StarRocksMetadataApplier.java
Patch:
@@ -119,7 +119,7 @@ private void applyAddColumn(AddColumnEvent addColumnEvent) {
                             .setColumnName(column.getName())
                             .setOrdinalPosition(-1)
                             .setColumnComment(column.getComment());
-            toStarRocksDataType(column, builder);
+            toStarRocksDataType(column, false, builder);
             addColumns.add(builder.build());
         }
 

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/offset/BinlogOffsetUtils.java
Patch:
@@ -47,8 +47,9 @@ public static BinlogOffset initializeEffectiveOffset(
         BinlogOffsetKind offsetKind = offset.getOffsetKind();
         switch (offsetKind) {
             case EARLIEST:
-            case TIMESTAMP:
                 return BinlogOffset.ofBinlogFilePosition("", 0);
+            case TIMESTAMP:
+                return DebeziumUtils.findBinlogOffset(offset.getTimestampSec() * 1000, connection);
             case LATEST:
                 return DebeziumUtils.currentBinlogOffset(connection);
             default:

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-doris/src/main/java/com/ververica/cdc/connectors/doris/sink/DorisMetadataApplier.java
Patch:
@@ -79,7 +79,7 @@ public void applySchemaChange(SchemaChangeEvent event) {
             } else if (event instanceof RenameColumnEvent) {
                 applyRenameColumnEvent((RenameColumnEvent) event);
             } else if (event instanceof AlterColumnTypeEvent) {
-                throw new RuntimeException("Unsupport schema change event, " + event);
+                throw new RuntimeException("Unsupported schema change event, " + event);
             }
         } catch (Exception ex) {
             throw new RuntimeException(

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-debezium/src/main/java/com/ververica/cdc/debezium/table/MetadataConverter.java
Patch:
@@ -25,5 +25,6 @@
 @FunctionalInterface
 @Internal
 public interface MetadataConverter extends Serializable {
+
     Object read(SourceRecord record);
 }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/table/MySqlConnectorITCase.java
Patch:
@@ -933,7 +933,7 @@ public void testMetadataColumns() throws Exception {
                         "CREATE TABLE mysql_users ("
                                 + " db_name STRING METADATA FROM 'database_name' VIRTUAL,"
                                 + " table_name STRING METADATA VIRTUAL,"
-                                + " op STRING METADATA FROM 'op' VIRTUAL,"
+                                + " row_kind STRING METADATA FROM 'row_kind' VIRTUAL,"
                                 + " `id` DECIMAL(20, 0) NOT NULL,"
                                 + " name STRING,"
                                 + " address STRING,"
@@ -968,7 +968,7 @@ public void testMetadataColumns() throws Exception {
                 "CREATE TABLE sink ("
                         + " database_name STRING,"
                         + " table_name STRING,"
-                        + " op STRING,"
+                        + " row_kind STRING,"
                         + " `id` DECIMAL(20, 0) NOT NULL,"
                         + " name STRING,"
                         + " address STRING,"

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mongodb-cdc/src/test/java/com/ververica/cdc/connectors/mongodb/source/MongoDBFullChangelogITCase.java
Patch:
@@ -398,7 +398,7 @@ private List<String> testBackfillWhenWritingEvents(
         env.enableCheckpointing(1000);
         env.setParallelism(1);
 
-        ResolvedSchema customersSchame =
+        ResolvedSchema customersSchema =
                 new ResolvedSchema(
                         Arrays.asList(
                                 physical("cid", BIGINT().notNull()),
@@ -407,7 +407,7 @@ private List<String> testBackfillWhenWritingEvents(
                                 physical("phone_number", STRING())),
                         new ArrayList<>(),
                         UniqueConstraint.primaryKey("pk", Collections.singletonList("cid")));
-        TestTable customerTable = new TestTable(customerDatabase, "customers", customersSchame);
+        TestTable customerTable = new TestTable(customerDatabase, "customers", customersSchema);
         MongoDBSource source =
                 new MongoDBSourceBuilder()
                         .hosts(CONTAINER.getHostAndPort())

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/data/binary/BinaryRecordDataSerializer.java
Patch:
@@ -96,7 +96,6 @@ public BinaryRecordData deserialize(BinaryRecordData reuse, DataInputView source
                 segments == null || (segments.length == 1 && reuse.getOffset() == 0),
                 "Reuse BinaryRecordData should have no segments or only one segment and offset start at 0.");
 
-        int arity = source.readInt();
         int length = source.readInt();
         if (segments == null || segments[0].size() < length) {
             segments = new MemorySegment[] {MemorySegmentFactory.wrap(new byte[length])};

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-sqlserver-cdc/src/main/java/com/ververica/cdc/connectors/sqlserver/table/SqlServerTableFactory.java
Patch:
@@ -168,8 +168,8 @@ public DynamicTableSource createDynamicTableSource(Context context) {
                 splitMetaGroupSize,
                 fetchSize,
                 connectTimeout,
-                connectionPoolSize,
                 connectMaxRetries,
+                connectionPoolSize,
                 distributionFactorUpper,
                 distributionFactorLower,
                 chunkKeyColumn,

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-sqlserver-cdc/src/main/java/com/ververica/cdc/connectors/sqlserver/table/SqlServerTableSource.java
Patch:
@@ -130,8 +130,8 @@ public SqlServerTableSource(
         this.splitMetaGroupSize = splitMetaGroupSize;
         this.fetchSize = fetchSize;
         this.connectTimeout = connectTimeout;
-        this.connectionPoolSize = connectionPoolSize;
         this.connectMaxRetries = connectMaxRetries;
+        this.connectionPoolSize = connectionPoolSize;
         this.distributionFactorUpper = distributionFactorUpper;
         this.distributionFactorLower = distributionFactorLower;
         this.chunkKeyColumn = chunkKeyColumn;
@@ -178,8 +178,8 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {
                             .splitMetaGroupSize(splitMetaGroupSize)
                             .fetchSize(fetchSize)
                             .connectTimeout(connectTimeout)
-                            .connectionPoolSize(connectionPoolSize)
                             .connectMaxRetries(connectMaxRetries)
+                            .connectionPoolSize(connectionPoolSize)
                             .distributionFactorUpper(distributionFactorUpper)
                             .distributionFactorLower(distributionFactorLower)
                             .chunkKeyColumn(chunkKeyColumn)
@@ -239,8 +239,8 @@ public DynamicTableSource copy() {
                         splitMetaGroupSize,
                         fetchSize,
                         connectTimeout,
-                        connectionPoolSize,
                         connectMaxRetries,
+                        connectionPoolSize,
                         distributionFactorUpper,
                         distributionFactorLower,
                         chunkKeyColumn,

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-sqlserver-cdc/src/test/java/com/ververica/cdc/connectors/sqlserver/table/SqlServerTableFactoryTest.java
Patch:
@@ -103,8 +103,8 @@ public void testCommonProperties() {
                         SourceOptions.CHUNK_META_GROUP_SIZE.defaultValue(),
                         SourceOptions.SCAN_SNAPSHOT_FETCH_SIZE.defaultValue(),
                         JdbcSourceOptions.CONNECT_TIMEOUT.defaultValue(),
-                        JdbcSourceOptions.CONNECTION_POOL_SIZE.defaultValue(),
                         JdbcSourceOptions.CONNECT_MAX_RETRIES.defaultValue(),
+                        JdbcSourceOptions.CONNECTION_POOL_SIZE.defaultValue(),
                         JdbcSourceOptions.SPLIT_KEY_EVEN_DISTRIBUTION_FACTOR_UPPER_BOUND
                                 .defaultValue(),
                         JdbcSourceOptions.SPLIT_KEY_EVEN_DISTRIBUTION_FACTOR_LOWER_BOUND
@@ -148,8 +148,8 @@ public void testEnableParallelReadSource() {
                         3000,
                         100,
                         Duration.ofSeconds(45),
-                        JdbcSourceOptions.CONNECTION_POOL_SIZE.defaultValue(),
                         JdbcSourceOptions.CONNECT_MAX_RETRIES.defaultValue(),
+                        JdbcSourceOptions.CONNECTION_POOL_SIZE.defaultValue(),
                         40.5d,
                         0.01d,
                         "testCol",
@@ -187,8 +187,8 @@ public void testOptionalProperties() {
                         SourceOptions.CHUNK_META_GROUP_SIZE.defaultValue(),
                         SourceOptions.SCAN_SNAPSHOT_FETCH_SIZE.defaultValue(),
                         JdbcSourceOptions.CONNECT_TIMEOUT.defaultValue(),
-                        JdbcSourceOptions.CONNECTION_POOL_SIZE.defaultValue(),
                         JdbcSourceOptions.CONNECT_MAX_RETRIES.defaultValue(),
+                        JdbcSourceOptions.CONNECTION_POOL_SIZE.defaultValue(),
                         JdbcSourceOptions.SPLIT_KEY_EVEN_DISTRIBUTION_FACTOR_UPPER_BOUND
                                 .defaultValue(),
                         JdbcSourceOptions.SPLIT_KEY_EVEN_DISTRIBUTION_FACTOR_LOWER_BOUND

File: flink-cdc-composer/src/main/java/com/ververica/cdc/composer/flink/translator/DataSourceTranslator.java
Patch:
@@ -62,7 +62,7 @@ public DataStreamSource<Event> translate(
                 .ifPresent(jar -> FlinkEnvironmentUtils.addJar(env, jar));
 
         // Get source provider
-        final int sourceParallelism = pipelineConfig.get(PipelineOptions.GLOBAL_PARALLELISM);
+        final int sourceParallelism = pipelineConfig.get(PipelineOptions.PIPELINE_PARALLELISM);
         EventSourceProvider eventSourceProvider = dataSource.getEventSourceProvider();
         if (eventSourceProvider instanceof FlinkSourceProvider) {
             // Source

File: flink-cdc-composer/src/test/java/com/ververica/cdc/composer/flink/FlinkPipelineComposerITCase.java
Patch:
@@ -109,7 +109,7 @@ void testSingleSplitSingleTable() throws Exception {
 
         // Setup pipeline
         Configuration pipelineConfig = new Configuration();
-        pipelineConfig.set(PipelineOptions.GLOBAL_PARALLELISM, 1);
+        pipelineConfig.set(PipelineOptions.PIPELINE_PARALLELISM, 1);
         PipelineDef pipelineDef =
                 new PipelineDef(
                         sourceDef,
@@ -163,7 +163,7 @@ void testSingleSplitMultipleTables() throws Exception {
 
         // Setup pipeline
         Configuration pipelineConfig = new Configuration();
-        pipelineConfig.set(PipelineOptions.GLOBAL_PARALLELISM, 1);
+        pipelineConfig.set(PipelineOptions.PIPELINE_PARALLELISM, 1);
         PipelineDef pipelineDef =
                 new PipelineDef(
                         sourceDef,
@@ -227,7 +227,7 @@ void testMultiSplitsSingleTable() throws Exception {
 
         // Setup pipeline
         Configuration pipelineConfig = new Configuration();
-        pipelineConfig.set(PipelineOptions.GLOBAL_PARALLELISM, MAX_PARALLELISM);
+        pipelineConfig.set(PipelineOptions.PIPELINE_PARALLELISM, MAX_PARALLELISM);
         PipelineDef pipelineDef =
                 new PipelineDef(
                         sourceDef,

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/main/java/com/ververica/cdc/connectors/mysql/utils/MySqlTypeUtils.java
Patch:
@@ -124,7 +124,7 @@ private static DataType convertFromColumn(Column column) {
             case BIT:
                 return column.length() == 1
                         ? DataTypes.BOOLEAN()
-                        : DataTypes.BINARY(column.length() / 8);
+                        : DataTypes.BINARY((column.length() + 7) / 8);
             case BOOL:
             case BOOLEAN:
                 return DataTypes.BOOLEAN();

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/test/java/com/ververica/cdc/connectors/mysql/source/MySqlFullTypesITCase.java
Patch:
@@ -274,6 +274,7 @@ private void testCommonDataTypes(UniqueDatabase database) throws Exception {
                     // Decimal precision larger than 38 will be treated as string.
                     BinaryStringData.fromString("34567892.1"),
                     false,
+                    new byte[] {3},
                     true,
                     true,
                     parseHexBinary("651aed08-390f-4893-b2f1-36923e7b7400".replace("-", "")),
@@ -308,7 +309,7 @@ private void testCommonDataTypes(UniqueDatabase database) throws Exception {
 
         expectedSnapshot[30] = null;
         // The json string from binlog will remove useless space
-        expectedSnapshot[43] = BinaryStringData.fromString("{\"key1\":\"value1\"}");
+        expectedSnapshot[44] = BinaryStringData.fromString("{\"key1\":\"value1\"}");
         Object[] expectedStreamRecord = expectedSnapshot;
 
         List<Event> streamResults = fetchResults(iterator, 1);
@@ -406,6 +407,7 @@ private FlinkSourceProvider getFlinkSourceProvider(
                     // Decimal precision larger than 38 will be treated as string.
                     DataTypes.STRING(),
                     DataTypes.BOOLEAN(),
+                    DataTypes.BINARY(1),
                     DataTypes.BOOLEAN(),
                     DataTypes.BOOLEAN(),
                     DataTypes.BINARY(16),

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/test/java/com/ververica/cdc/connectors/mysql/source/MySqlMetadataAccessorITCase.java
Patch:
@@ -265,6 +265,7 @@ private void testAccessCommonTypesSchema(UniqueDatabase database) {
                                             // string.
                                             DataTypes.STRING(),
                                             DataTypes.BOOLEAN(),
+                                            DataTypes.BINARY(1),
                                             DataTypes.BOOLEAN(),
                                             DataTypes.BOOLEAN(),
                                             DataTypes.BINARY(16),
@@ -319,6 +320,7 @@ private void testAccessCommonTypesSchema(UniqueDatabase database) {
                                             "numeric_c",
                                             "big_decimal_c",
                                             "bit1_c",
+                                            "bit3_c",
                                             "tiny1_c",
                                             "boolean_c",
                                             "file_uuid",

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/schema/MySqlTypeUtils.java
Patch:
@@ -125,7 +125,7 @@ private static DataType convertFromColumn(Column column) {
             case BIT:
                 return column.length() == 1
                         ? DataTypes.BOOLEAN()
-                        : DataTypes.BINARY(column.length() / 8);
+                        : DataTypes.BINARY((column.length() + 7) / 8);
             case BOOL:
             case BOOLEAN:
                 return DataTypes.BOOLEAN();

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/main/java/com/github/shyiko/mysql/binlog/event/deserialization/TableMapEventDataDeserializer.java
Patch:
@@ -77,6 +77,7 @@ private int numericColumnCount(byte[] types) {
                 case NEWDECIMAL:
                 case FLOAT:
                 case DOUBLE:
+                case YEAR:
                     count++;
                     break;
                 default:

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/main/java/com/ververica/cdc/connectors/mysql/utils/MySqlTypeUtils.java
Patch:
@@ -143,14 +143,14 @@ private static DataType convertFromColumn(Column column) {
             case INT:
             case INTEGER:
             case MEDIUMINT:
+            case MEDIUMINT_UNSIGNED:
+            case MEDIUMINT_UNSIGNED_ZEROFILL:
             case YEAR:
                 return DataTypes.INT();
             case INT_UNSIGNED:
             case INT_UNSIGNED_ZEROFILL:
             case INTEGER_UNSIGNED:
             case INTEGER_UNSIGNED_ZEROFILL:
-            case MEDIUMINT_UNSIGNED:
-            case MEDIUMINT_UNSIGNED_ZEROFILL:
             case BIGINT:
                 return DataTypes.BIGINT();
             case BIGINT_UNSIGNED:

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/main/java/com/ververica/cdc/connectors/mysql/source/MySqlDataSourceOptions.java
Patch:
@@ -227,7 +227,7 @@ public class MySqlDataSourceOptions {
     public static final ConfigOption<Boolean> SCHEMA_CHANGE_ENABLED =
             ConfigOptions.key("schema-change.enabled")
                     .booleanType()
-                    .defaultValue(false)
+                    .defaultValue(true)
                     .withDescription(
-                            "Whether send schema change events, by default is false. If set to false, the schema changes will not be sent.");
+                            "Whether send schema change events, by default is true. If set to false, the schema changes will not be sent.");
 }

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/test/java/com/ververica/cdc/connectors/mysql/source/MySqlPipelineITCase.java
Patch:
@@ -60,6 +60,7 @@
 import java.util.List;
 import java.util.stream.Stream;
 
+import static com.ververica.cdc.connectors.mysql.source.MySqlDataSourceOptions.SCHEMA_CHANGE_ENABLED;
 import static com.ververica.cdc.connectors.mysql.testutils.MySqSourceTestUtils.TEST_PASSWORD;
 import static com.ververica.cdc.connectors.mysql.testutils.MySqSourceTestUtils.TEST_USER;
 import static com.ververica.cdc.connectors.mysql.testutils.MySqSourceTestUtils.fetchResults;
@@ -114,7 +115,7 @@ public void testInitialStartupMode() throws Exception {
                         .startupOptions(StartupOptions.initial())
                         .serverId(getServerId(env.getParallelism()))
                         .serverTimeZone("UTC")
-                        .includeSchemaChanges(true);
+                        .includeSchemaChanges(SCHEMA_CHANGE_ENABLED.defaultValue());
 
         FlinkSourceProvider sourceProvider =
                 (FlinkSourceProvider) new MySqlDataSource(configFactory).getEventSourceProvider();
@@ -242,7 +243,7 @@ public void testParseAlterStatement() throws Exception {
                         .startupOptions(StartupOptions.latest())
                         .serverId(getServerId(env.getParallelism()))
                         .serverTimeZone("UTC")
-                        .includeSchemaChanges(true);
+                        .includeSchemaChanges(SCHEMA_CHANGE_ENABLED.defaultValue());
 
         FlinkSourceProvider sourceProvider =
                 (FlinkSourceProvider) new MySqlDataSource(configFactory).getEventSourceProvider();

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-debezium/src/main/java/com/ververica/cdc/debezium/event/DebeziumEventDeserializationSchema.java
Patch:
@@ -21,6 +21,7 @@
 
 import com.ververica.cdc.common.annotation.Internal;
 import com.ververica.cdc.common.data.DecimalData;
+import com.ververica.cdc.common.data.LocalZonedTimestampData;
 import com.ververica.cdc.common.data.RecordData;
 import com.ververica.cdc.common.data.TimestampData;
 import com.ververica.cdc.common.data.binary.BinaryStringData;
@@ -330,7 +331,7 @@ protected Object convertToLocalTimeZoneTimestamp(Object dbzObj, Schema schema) {
             String str = (String) dbzObj;
             // TIMESTAMP_LTZ type is encoded in string type
             Instant instant = Instant.parse(str);
-            return TimestampData.fromMillis(instant.toEpochMilli(), instant.getNano());
+            return LocalZonedTimestampData.fromInstant(instant);
         }
         throw new IllegalArgumentException(
                 "Unable to convert to TimestampData from unexpected value '"

File: flink-cdc-common/src/main/java/com/ververica/cdc/common/event/EventDeserializer.java
Patch:
@@ -16,13 +16,13 @@
 
 package com.ververica.cdc.common.event;
 
-import com.ververica.cdc.common.annotation.Public;
+import com.ververica.cdc.common.annotation.PublicEvolving;
 
 import java.io.Serializable;
 import java.util.List;
 
 /** Deserializer to deserialize given record to {@link Event}. */
-@Public
+@PublicEvolving
 public interface EventDeserializer<T> extends Serializable {
 
     /** Deserialize given record to {@link Event}s. */

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/main/java/com/ververica/cdc/connectors/mysql/source/MySqlDataSourceOptions.java
Patch:
@@ -228,5 +228,6 @@ public class MySqlDataSourceOptions {
             ConfigOptions.key("schema-change.enabled")
                     .booleanType()
                     .defaultValue(false)
-                    .withDescription("Whether , by default is false.");
+                    .withDescription(
+                            "Whether send schema change events, by default is false. If set to false, the schema changes will not be sent.");
 }

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/main/java/com/ververica/cdc/connectors/mysql/source/reader/MySqlPipelineRecordEmitter.java
Patch:
@@ -23,13 +23,13 @@
 import com.ververica.cdc.common.event.Event;
 import com.ververica.cdc.common.schema.Schema;
 import com.ververica.cdc.common.types.DataType;
-import com.ververica.cdc.connectors.mysql.schema.MySqlCdcCommonTypeUtils;
 import com.ververica.cdc.connectors.mysql.schema.MySqlFieldDefinition;
 import com.ververica.cdc.connectors.mysql.schema.MySqlTableDefinition;
 import com.ververica.cdc.connectors.mysql.source.config.MySqlSourceConfig;
 import com.ververica.cdc.connectors.mysql.source.metrics.MySqlSourceReaderMetrics;
 import com.ververica.cdc.connectors.mysql.source.split.MySqlSplitState;
 import com.ververica.cdc.connectors.mysql.table.StartupMode;
+import com.ververica.cdc.connectors.mysql.utils.MySqlTypeUtils;
 import com.ververica.cdc.debezium.DebeziumDeserializationSchema;
 import io.debezium.connector.mysql.antlr.MySqlAntlrDdlParser;
 import io.debezium.jdbc.JdbcConnection;
@@ -200,7 +200,7 @@ private Schema parseDDL(String ddlStatement, TableId tableId) {
             Column column = columns.get(i);
 
             String colName = column.name();
-            DataType dataType = MySqlCdcCommonTypeUtils.fromDbzColumn(column);
+            DataType dataType = MySqlTypeUtils.fromDbzColumn(column);
             if (!column.isOptional()) {
                 dataType = dataType.notNull();
             }

File: flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-mysql/src/main/java/com/ververica/cdc/connectors/mysql/utils/MySqlSchemaUtils.java
Patch:
@@ -19,7 +19,6 @@
 import com.ververica.cdc.common.event.TableId;
 import com.ververica.cdc.common.schema.Column;
 import com.ververica.cdc.common.schema.Schema;
-import com.ververica.cdc.connectors.mysql.schema.MySqlCdcCommonTypeUtils;
 import com.ververica.cdc.connectors.mysql.schema.MySqlSchema;
 import com.ververica.cdc.connectors.mysql.source.config.MySqlSourceConfig;
 import io.debezium.connector.mysql.MySqlConnection;
@@ -147,7 +146,7 @@ public static Schema toSchema(Table table) {
 
     public static Column toColumn(io.debezium.relational.Column column) {
         return Column.physicalColumn(
-                column.name(), MySqlCdcCommonTypeUtils.fromDbzColumn(column), column.comment());
+                column.name(), MySqlTypeUtils.fromDbzColumn(column), column.comment());
     }
 
     public static io.debezium.relational.TableId toDbzTableId(TableId tableId) {

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-debezium/src/main/java/com/ververica/cdc/debezium/event/SourceRecordEventDeserializer.java
Patch:
@@ -68,10 +68,10 @@ protected abstract List<DataChangeEvent> deserializeDataChangeRecord(SourceRecor
     protected abstract List<SchemaChangeEvent> deserializeSchemaChangeRecord(SourceRecord record)
             throws Exception;
 
-    /** Get {@link TableId} from given record. */
+    /** Get {@link TableId} from data change record. */
     protected abstract TableId getTableId(SourceRecord record);
 
-    /** Get metadata from given record. */
+    /** Get metadata from data change record. */
     protected abstract Map<String, String> getMetadata(SourceRecord record);
 
     public static Schema fieldSchema(Schema schema, String fieldName) {

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/MySqlSource.java
Patch:
@@ -16,7 +16,6 @@
 
 package com.ververica.cdc.connectors.mysql.source;
 
-import com.ververica.cdc.connectors.mysql.source.split.MySqlSplitState;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.connector.source.Boundedness;
 import org.apache.flink.api.connector.source.Source;
@@ -48,11 +47,13 @@
 import com.ververica.cdc.connectors.mysql.source.config.MySqlSourceConfigFactory;
 import com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator;
 import com.ververica.cdc.connectors.mysql.source.metrics.MySqlSourceReaderMetrics;
+import com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter;
 import com.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReader;
 import com.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReaderContext;
 import com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader;
 import com.ververica.cdc.connectors.mysql.source.split.MySqlSplit;
 import com.ververica.cdc.connectors.mysql.source.split.MySqlSplitSerializer;
+import com.ververica.cdc.connectors.mysql.source.split.MySqlSplitState;
 import com.ververica.cdc.connectors.mysql.source.split.SourceRecords;
 import com.ververica.cdc.connectors.mysql.source.utils.hooks.SnapshotPhaseHooks;
 import com.ververica.cdc.connectors.mysql.table.StartupMode;

File: flink-cdc-common/src/main/java/com/ververica/cdc/common/data/binary/BinaryRecordData.java
Patch:
@@ -37,7 +37,7 @@
 import java.nio.ByteOrder;
 
 import static com.ververica.cdc.common.types.DataTypeRoot.DECIMAL;
-import static org.apache.flink.util.Preconditions.checkArgument;
+import static com.ververica.cdc.common.utils.Preconditions.checkArgument;
 
 /**
  * An implementation of {@link RecordData} which is backed by {@link MemorySegment} instead of

File: flink-cdc-common/src/main/java/com/ververica/cdc/common/factories/FactoryHelper.java
Patch:
@@ -16,8 +16,7 @@
 
 package com.ververica.cdc.common.factories;
 
-import org.apache.flink.annotation.PublicEvolving;
-
+import com.ververica.cdc.common.annotation.PublicEvolving;
 import com.ververica.cdc.common.configuration.Configuration;
 
 import java.util.Map;

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/LinkedListSerializer.java
Patch:
@@ -16,18 +16,19 @@
 
 package com.ververica.cdc.runtime.serializer;
 
-import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
 import org.apache.flink.api.java.typeutils.runtime.MaskUtils;
 import org.apache.flink.core.memory.DataInputView;
 import org.apache.flink.core.memory.DataOutputView;
 
+import com.ververica.cdc.common.annotation.Internal;
+
 import java.io.IOException;
 import java.util.LinkedList;
 
-import static org.apache.flink.util.Preconditions.checkNotNull;
+import static com.ververica.cdc.common.utils.Preconditions.checkNotNull;
 
 /**
  * A serializer for {@link LinkedList}. The serializer relies on an element serializer for the

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/ListSerializer.java
Patch:
@@ -26,7 +26,7 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import static org.apache.flink.util.Preconditions.checkNotNull;
+import static com.ververica.cdc.common.utils.Preconditions.checkNotNull;
 
 /**
  * A serializer for {@link List Lists}. The serializer relies on an element serializer for the

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/NestedSerializersSnapshotDelegate.java
Patch:
@@ -27,8 +27,8 @@
 import java.io.IOException;
 import java.util.Arrays;
 
-import static org.apache.flink.util.Preconditions.checkArgument;
-import static org.apache.flink.util.Preconditions.checkNotNull;
+import static com.ververica.cdc.common.utils.Preconditions.checkArgument;
+import static com.ververica.cdc.common.utils.Preconditions.checkNotNull;
 
 /**
  * A NestedSerializersSnapshotDelegate represents the snapshots of multiple serializers that are

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/data/ArrayDataSerializer.java
Patch:
@@ -16,7 +16,6 @@
 
 package com.ververica.cdc.runtime.serializer.data;
 
-import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
@@ -25,6 +24,7 @@
 import org.apache.flink.core.memory.DataInputView;
 import org.apache.flink.core.memory.DataOutputView;
 
+import com.ververica.cdc.common.annotation.VisibleForTesting;
 import com.ververica.cdc.common.data.ArrayData;
 import com.ververica.cdc.common.data.GenericArrayData;
 import com.ververica.cdc.common.types.DataType;

File: flink-cdc-composer/src/main/java/com/ververica/cdc/composer/flink/FlinkPipelineComposer.java
Patch:
@@ -43,6 +43,7 @@
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.Paths;
+import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Optional;
@@ -77,6 +78,7 @@ public PipelineExecution compose(PipelineDef pipelineDef) {
         int parallelism = pipelineDef.getConfig().get(PipelineOptions.GLOBAL_PARALLELISM);
         env.getConfig().setParallelism(parallelism);
 
+
         // Source
         DataSourceTranslator sourceTranslator = new DataSourceTranslator();
         DataStream<Event> stream =

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/operators/sink/DataSinkWriterOperator.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.flink.streaming.api.graph.StreamConfig;
 import org.apache.flink.streaming.api.operators.AbstractStreamOperator;
 import org.apache.flink.streaming.api.operators.BoundedOneInput;
+import org.apache.flink.streaming.api.operators.ChainingStrategy;
 import org.apache.flink.streaming.api.operators.OneInputStreamOperator;
 import org.apache.flink.streaming.api.operators.Output;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
@@ -91,6 +92,7 @@ public DataSinkWriterOperator(
         this.mailboxExecutor = mailboxExecutor;
         this.schemaOperatorID = schemaOperatorID;
         this.processedTableIds = new HashSet<>();
+        this.chainingStrategy = ChainingStrategy.ALWAYS;
     }
 
     @Override

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/partitioning/PrePartitionOperator.java
Patch:
@@ -19,6 +19,7 @@
 import org.apache.flink.runtime.jobgraph.OperatorID;
 import org.apache.flink.runtime.jobgraph.tasks.TaskOperatorEventGateway;
 import org.apache.flink.streaming.api.operators.AbstractStreamOperator;
+import org.apache.flink.streaming.api.operators.ChainingStrategy;
 import org.apache.flink.streaming.api.operators.OneInputStreamOperator;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 
@@ -62,6 +63,7 @@ public class PrePartitionOperator extends AbstractStreamOperator<PartitioningEve
     private transient LoadingCache<TableId, HashFunction> cachedHashFunctions;
 
     public PrePartitionOperator(OperatorID schemaOperatorId, int downstreamParallelism) {
+        this.chainingStrategy = ChainingStrategy.ALWAYS;
         this.schemaOperatorId = schemaOperatorId;
         this.downstreamParallelism = downstreamParallelism;
     }

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/debezium/reader/SnapshotSplitReader.java
Patch:
@@ -124,6 +124,7 @@ public void submitSplit(MySqlSplit mySqlSplit) {
         this.reachEnd.set(false);
         this.splitSnapshotReadTask =
                 new MySqlSnapshotSplitReadTask(
+                        statefulTaskContext.getSourceConfig(),
                         statefulTaskContext.getConnectorConfig(),
                         statefulTaskContext.getSnapshotChangeEventSourceMetrics(),
                         statefulTaskContext.getDatabaseSchema(),

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/source/SpecificStartingOffsetITCase.java
Patch:
@@ -163,7 +163,6 @@ void testStartingFromEarliestOffset() throws Exception {
                                 savepointDir.toAbsolutePath().toString(),
                                 SavepointFormatType.DEFAULT)
                         .get();
-        jobClient.cancel().get();
 
         // Make some changes after the savepoint
         executeStatements(
@@ -239,7 +238,6 @@ void testStartingFromSpecificOffset() throws Exception {
                                 savepointDir.toAbsolutePath().toString(),
                                 SavepointFormatType.DEFAULT)
                         .get();
-        jobClient.cancel().get();
 
         // Make some changes after the savepoint
         executeStatements(
@@ -325,7 +323,6 @@ void testStartingFromTimestampOffset() throws Exception {
                                 savepointDir.toAbsolutePath().toString(),
                                 SavepointFormatType.DEFAULT)
                         .get();
-        jobClient.cancel().get();
 
         // Make some changes after the savepoint
         executeStatements(

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/source/reader/fetch/MongoDBFetchTaskContext.java
Patch:
@@ -166,6 +166,8 @@ public void rewriteOutputBuffer(
                 case INSERT:
                 case UPDATE:
                 case REPLACE:
+                    value.put(
+                            MongoDBEnvelope.OPERATION_TYPE_FIELD, OperationType.INSERT.getValue());
                     outputBuffer.put(key, changeRecord);
                     break;
                 case DELETE:

File: flink-cdc-composer/src/main/java/com/ververica/cdc/composer/flink/FlinkPipelineComposer.java
Patch:
@@ -22,7 +22,6 @@
 import com.ververica.cdc.common.annotation.Internal;
 import com.ververica.cdc.common.event.Event;
 import com.ververica.cdc.common.factories.DataSinkFactory;
-import com.ververica.cdc.common.factories.DataSourceFactory;
 import com.ververica.cdc.common.factories.FactoryHelper;
 import com.ververica.cdc.common.pipeline.PipelineOptions;
 import com.ververica.cdc.common.sink.DataSink;
@@ -107,7 +106,7 @@ private DataSink createDataSink(SinkDef sinkDef) {
                         sinkDef.getType(), DataSinkFactory.class);
 
         // Include sink connector JAR
-        FactoryDiscoveryUtils.getJarPathByIdentifier(sinkDef.getType(), DataSourceFactory.class)
+        FactoryDiscoveryUtils.getJarPathByIdentifier(sinkDef.getType(), DataSinkFactory.class)
                 .ifPresent(jar -> FlinkEnvironmentUtils.addJar(env, jar));
 
         // Create data sink

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/operators/schema/coordinator/SchemaRegistry.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.flink.runtime.operators.coordination.OperatorEvent;
 import org.apache.flink.util.FlinkException;
 
-import com.ververica.cdc.common.annotation.Internal;
 import com.ververica.cdc.common.event.TableId;
 import com.ververica.cdc.common.sink.MetadataApplier;
 import com.ververica.cdc.runtime.operators.schema.SchemaOperator;
@@ -46,6 +45,8 @@
 import java.util.Map;
 import java.util.concurrent.CompletableFuture;
 
+import static com.ververica.cdc.runtime.operators.schema.event.CoordinationResponseUtils.wrap;
+
 /**
  * The implementation of the {@link OperatorCoordinator} for the {@link SchemaOperator}.
  *
@@ -62,7 +63,6 @@
  *       FlushSuccessEvent} from its registered sink writer
  * </ul>
  */
-@Internal
 public class SchemaRegistry implements OperatorCoordinator, CoordinationRequestHandler {
 
     private static final Logger LOG = LoggerFactory.getLogger(SchemaRegistry.class);
@@ -158,7 +158,7 @@ public CompletableFuture<CoordinationResponse> handleCoordinationRequest(
             return requestHandler.handleReleaseUpstreamRequest();
         } else if (request instanceof GetSchemaRequest) {
             return CompletableFuture.completedFuture(
-                    handleGetSchemaRequest(((GetSchemaRequest) request)));
+                    wrap(handleGetSchemaRequest(((GetSchemaRequest) request))));
         } else {
             throw new IllegalArgumentException("Unrecognized CoordinationRequest type: " + request);
         }

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/operators/sink/DataSinkWriterOperatorFactory.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.flink.streaming.api.operators.StreamOperator;
 import org.apache.flink.streaming.api.operators.StreamOperatorParameters;
 import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;
-import org.apache.flink.streaming.runtime.operators.sink.DataSinkWriterOperator;
 
 import com.ververica.cdc.common.annotation.Internal;
 import com.ververica.cdc.common.event.Event;

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-vitess-cdc/src/test/java/com/ververica/cdc/connectors/vitess/VitessSourceTest.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.vervetica.cdc.connectors.vitess;
+package com.ververica.cdc.connectors.vitess;
 
 import org.apache.flink.api.common.state.BroadcastState;
 import org.apache.flink.api.common.state.KeyedStateStore;
@@ -31,7 +31,6 @@
 import org.apache.flink.util.Collector;
 
 import com.ververica.cdc.connectors.utils.TestSourceContext;
-import com.ververica.cdc.connectors.vitess.VitessSource;
 import com.ververica.cdc.connectors.vitess.config.TabletType;
 import com.ververica.cdc.debezium.DebeziumDeserializationSchema;
 import com.ververica.cdc.debezium.DebeziumSourceFunction;

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-vitess-cdc/src/test/java/com/ververica/cdc/connectors/vitess/container/VitessContainer.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.vervetica.cdc.connectors.vitess.container;
+package com.ververica.cdc.connectors.vitess.container;
 
 import org.testcontainers.containers.JdbcDatabaseContainer;
 

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-vitess-cdc/src/test/java/com/ververica/cdc/connectors/vitess/table/VitessConnectorITCase.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.vervetica.cdc.connectors.vitess.table;
+package com.ververica.cdc.connectors.vitess.table;
 
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.api.EnvironmentSettings;
@@ -24,7 +24,7 @@
 import org.apache.flink.types.Row;
 import org.apache.flink.util.CloseableIterator;
 
-import com.vervetica.cdc.connectors.vitess.VitessTestBase;
+import com.ververica.cdc.connectors.vitess.VitessTestBase;
 import org.junit.Before;
 import org.junit.Test;
 

File: flink-cdc-connect/flink-cdc-source-connectors/flink-connector-vitess-cdc/src/test/java/com/ververica/cdc/connectors/vitess/table/VitessTableFactoryTest.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.vervetica.cdc.connectors.vitess.table;
+package com.ververica.cdc.connectors.vitess.table;
 
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.Configuration;
@@ -32,8 +32,6 @@
 
 import com.ververica.cdc.connectors.vitess.config.SchemaAdjustmentMode;
 import com.ververica.cdc.connectors.vitess.config.TabletType;
-import com.ververica.cdc.connectors.vitess.table.VitessTableFactory;
-import com.ververica.cdc.connectors.vitess.table.VitessTableSource;
 import org.junit.Test;
 
 import java.util.ArrayList;

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/BooleanSerializer.java
Patch:
@@ -14,11 +14,10 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
-import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;
 import org.apache.flink.core.memory.DataInputView;
 import org.apache.flink.core.memory.DataOutputView;
 

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/BytePrimitiveArraySerializer.java
Patch:
@@ -14,11 +14,10 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
-import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;
 import org.apache.flink.core.memory.DataInputView;
 import org.apache.flink.core.memory.DataOutputView;
 

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/ByteSerializer.java
Patch:
@@ -14,11 +14,10 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
-import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;
 import org.apache.flink.core.memory.DataInputView;
 import org.apache.flink.core.memory.DataOutputView;
 

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/DoubleSerializer.java
Patch:
@@ -14,11 +14,10 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
-import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;
 import org.apache.flink.core.memory.DataInputView;
 import org.apache.flink.core.memory.DataOutputView;
 

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/EnumSerializer.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.typeutils.TypeSerializer;

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/FloatSerializer.java
Patch:
@@ -14,11 +14,10 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
-import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;
 import org.apache.flink.core.memory.DataInputView;
 import org.apache.flink.core.memory.DataOutputView;
 

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/IntSerializer.java
Patch:
@@ -14,11 +14,10 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
-import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;
 import org.apache.flink.core.memory.DataInputView;
 import org.apache.flink.core.memory.DataOutputView;
 

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/LinkedListSerializer.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot;

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/ListSerializer.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializer;

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/LongSerializer.java
Patch:
@@ -14,11 +14,10 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
-import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;
 import org.apache.flink.core.memory.DataInputView;
 import org.apache.flink.core.memory.DataOutputView;
 

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/MapSerializer.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializer;

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/NestedSerializersSnapshotDelegate.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializer;

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/ShortSerializer.java
Patch:
@@ -14,11 +14,10 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
-import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;
 import org.apache.flink.core.memory.DataInputView;
 import org.apache.flink.core.memory.DataOutputView;
 

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/StringSerializer.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/TypeSerializerSingleton.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer;
+package com.ververica.cdc.runtime.serializer;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/data/ArrayDataSerializer.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer.data;
+package com.ververica.cdc.runtime.serializer.data;
 
 import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
@@ -27,11 +27,11 @@
 
 import com.ververica.cdc.common.data.ArrayData;
 import com.ververica.cdc.common.data.GenericArrayData;
-import com.ververica.cdc.common.serializer.InternalSerializers;
-import com.ververica.cdc.common.serializer.NullableSerializerWrapper;
 import com.ververica.cdc.common.types.DataType;
 import com.ververica.cdc.common.types.utils.DataTypeUtils;
 import com.ververica.cdc.common.utils.InstantiationUtil;
+import com.ververica.cdc.runtime.serializer.InternalSerializers;
+import com.ververica.cdc.runtime.serializer.NullableSerializerWrapper;
 
 import java.io.IOException;
 import java.lang.reflect.Array;

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/data/DecimalDataSerializer.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer.data;
+package com.ververica.cdc.runtime.serializer.data;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility;

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/data/LocalZonedTimestampDataSerializer.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer.data;
+package com.ververica.cdc.runtime.serializer.data;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility;
@@ -61,7 +61,8 @@ public LocalZonedTimestampData createInstance() {
 
     @Override
     public LocalZonedTimestampData copy(LocalZonedTimestampData from) {
-        return from;
+        return LocalZonedTimestampData.fromEpochMillis(
+                from.getEpochMillisecond(), from.getEpochNanoOfMillisecond());
     }
 
     @Override

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/data/StringDataSerializer.java
Patch:
@@ -14,17 +14,17 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer.data;
+package com.ververica.cdc.runtime.serializer.data;
 
 import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
-import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;
 import org.apache.flink.core.memory.DataInputView;
 import org.apache.flink.core.memory.DataOutputView;
 
 import com.ververica.cdc.common.data.GenericStringData;
 import com.ververica.cdc.common.data.StringData;
 import com.ververica.cdc.common.utils.StringUtf8Utils;
+import com.ververica.cdc.runtime.serializer.TypeSerializerSingleton;
 
 import java.io.IOException;
 

File: flink-cdc-runtime/src/main/java/com/ververica/cdc/runtime/serializer/data/TimestampDataSerializer.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.common.serializer.data;
+package com.ververica.cdc.runtime.serializer.data;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility;
@@ -59,7 +59,7 @@ public TimestampData createInstance() {
 
     @Override
     public TimestampData copy(TimestampData from) {
-        return from;
+        return TimestampData.fromEpochMillis(from.getMillisecond(), from.getNanoOfMillisecond());
     }
 
     @Override

File: flink-cdc-runtime/src/main/java/org/apache/flink/streaming/runtime/operators/sink/DataSinkWriterOperator.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.flink.streaming.runtime.tasks.StreamTask;
 
 import com.ververica.cdc.common.event.Event;
-import com.ververica.cdc.runtime.operators.schema.event.FlushEvent;
+import com.ververica.cdc.common.event.FlushEvent;
 import com.ververica.cdc.runtime.operators.sink.SchemaEvolutionClient;
 
 import java.lang.reflect.Field;

File: flink-cdc-common/src/main/java/com/ververica/cdc/common/types/DataField.java
Patch:
@@ -23,7 +23,6 @@
 
 import java.io.Serializable;
 import java.util.Objects;
-import java.util.Optional;
 
 import static com.ververica.cdc.common.utils.EncodingUtils.escapeIdentifier;
 import static com.ververica.cdc.common.utils.EncodingUtils.escapeSingleQuotes;
@@ -66,8 +65,9 @@ public DataType getType() {
         return type;
     }
 
-    public Optional<String> getDescription() {
-        return Optional.ofNullable(description);
+    @Nullable
+    public String getDescription() {
+        return description;
     }
 
     public DataField copy() {

File: flink-connector-debezium/src/main/java/com/ververica/cdc/debezium/internal/FlinkOffsetBackingStore.java
Patch:
@@ -191,9 +191,7 @@ public Future<Void> set(
             final Map<ByteBuffer, ByteBuffer> values, final Callback<Void> callback) {
         return executor.submit(
                 () -> {
-                    for (Map.Entry<ByteBuffer, ByteBuffer> entry : values.entrySet()) {
-                        data.put(entry.getKey(), entry.getValue());
-                    }
+                    data.putAll(values);
                     if (callback != null) {
                         callback.onCompletion(null, null);
                     }

File: flink-connector-vitess-cdc/src/test/java/com/vervetica/cdc/connectors/vitess/container/VitessContainer.java
Patch:
@@ -22,7 +22,7 @@
 public class VitessContainer extends JdbcDatabaseContainer {
 
     public static final String IMAGE = "vitess/vttestserver";
-    public static final String DEFAULT_TAG = "mysql80";
+    public static final String DEFAULT_TAG = "v17.0.2-mysql80";
     private static final Integer VITESS_PORT = 15991;
     public static final Integer GRPC_PORT = VITESS_PORT + 1;
     public static final Integer MYSQL_PORT = VITESS_PORT + 3;

File: flink-connector-sqlserver-cdc/src/main/java/com/ververica/cdc/connectors/sqlserver/source/utils/SqlServerUtils.java
Patch:
@@ -200,7 +200,7 @@ public static LsnOffset getLsnPosition(Map<String, ?> offset) {
     /** Fetch current largest log sequence number (LSN) of the database. */
     public static LsnOffset currentLsn(SqlServerConnection connection) {
         try {
-            Lsn maxLsn = connection.getMaxLsn(connection.database());
+            Lsn maxLsn = connection.getMaxTransactionLsn(connection.database());
             return new LsnOffset(maxLsn, maxLsn, null);
         } catch (SQLException e) {
             throw new FlinkRuntimeException(e.getMessage(), e);

File: flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/table/MongoDBTableSource.java
Patch:
@@ -173,8 +173,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {
                 checkDatabaseNameValidity(database);
                 checkCollectionNameValidity(collection);
                 databaseList = database;
-                // match dot explicitly since it will be used for regex match later
-                collectionList = database + "[.]" + collection;
+                collectionList = database + "." + collection;
             } else {
                 databaseList = database;
                 collectionList = collection;

File: flink-cdc-e2e-tests/src/test/java/com/ververica/cdc/connectors/tests/MongoE2eITCase.java
Patch:
@@ -89,7 +89,7 @@ public void before() {
         super.before();
 
         container =
-                new MongoDBContainer("mongo:6.0.6")
+                new MongoDBContainer("mongo:6.0.9")
                         .withSharding()
                         .withNetwork(NETWORK)
                         .withNetworkAliases(INTER_CONTAINER_MONGO_ALIAS)

File: flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/table/MongoDBTableSource.java
Patch:
@@ -173,7 +173,8 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {
                 checkDatabaseNameValidity(database);
                 checkCollectionNameValidity(collection);
                 databaseList = database;
-                collectionList = database + "." + collection;
+                // match dot explicitly since it will be used for regex match later
+                collectionList = database + "[.]" + collection;
             } else {
                 databaseList = database;
                 collectionList = collection;

File: flink-connector-mongodb-cdc/src/test/java/com/ververica/cdc/connectors/mongodb/source/MongoDBFullChangelogITCase.java
Patch:
@@ -79,7 +79,7 @@ public void testGetMongoDBVersion() {
                         .pollAwaitTimeMillis(500)
                         .create(0);
 
-        assertEquals(MongoUtils.getMongoVersion(config), "6.0.6");
+        assertEquals(MongoUtils.getMongoVersion(config), "6.0.9");
     }
 
     @Test

File: flink-connector-mongodb-cdc/src/test/java/com/ververica/cdc/connectors/mongodb/source/MongoDBSourceTestBase.java
Patch:
@@ -71,7 +71,7 @@ public static void startContainers() {
 
     @ClassRule
     public static final MongoDBContainer CONTAINER =
-            new MongoDBContainer("mongo:6.0.6")
+            new MongoDBContainer("mongo:6.0.9")
                     .withSharding()
                     .withLogConsumer(new Slf4jLogConsumer(LOG));
 }

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/meta/split/SourceSplitSerializer.java
Patch:
@@ -70,7 +70,7 @@ public byte[] serialize(SourceSplitBase split) throws IOException {
             boolean useCatalogBeforeSchema =
                     SerializerUtils.shouldUseCatalogBeforeSchema(snapshotSplit.getTableId());
             out.writeBoolean(useCatalogBeforeSchema);
-            out.writeUTF(snapshotSplit.getTableId().toString());
+            out.writeUTF(snapshotSplit.getTableId().toDoubleQuotedString());
             out.writeUTF(snapshotSplit.splitId());
             out.writeUTF(snapshotSplit.getSplitKeyType().asSerializableString());
 

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/table/OracleTableSource.java
Patch:
@@ -51,7 +51,7 @@
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
 /**
- * A {@link DynamicTableSource} that describes how to create a Oracle binlog from a logical
+ * A {@link DynamicTableSource} that describes how to create a Oracle redo log from a logical
  * description.
  */
 public class OracleTableSource implements ScanTableSource, SupportsReadingMetadata {

File: flink-connector-oracle-cdc/src/test/java/com/ververica/cdc/connectors/oracle/OracleSourceTest.java
Patch:
@@ -271,7 +271,7 @@ public void go() throws Exception {
                 String state = new String(offsetState.list.get(0), StandardCharsets.UTF_8);
                 assertEquals("oracle_logminer", JsonPath.read(state, "$.sourcePartition.server"));
 
-                // execute 2 more DMLs to have more binlog
+                // execute 2 more DMLs to have more redo log
                 statement.execute(
                         "INSERT INTO debezium.products VALUES (1001,'roy','old robot',1234.56)"); // 1001
                 statement.execute("UPDATE debezium.products SET weight=1345.67 WHERE id=1001");
@@ -300,7 +300,7 @@ public void go() throws Exception {
                     };
             runThread3.start();
 
-            // consume the unconsumed binlog
+            // consume the unconsumed redo log
             List<SourceRecord> records = drain(sourceContext3, 2);
             assertInsert(records.get(0), "ID", 1001);
             assertUpdate(records.get(1), "ID", 1001);

File: flink-connector-oracle-cdc/src/test/java/com/ververica/cdc/connectors/oracle/table/OracleConnectorITCase.java
Patch:
@@ -65,7 +65,7 @@
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertThat;
 
-/** Integration tests for Oracle binlog SQL source. */
+/** Integration tests for Oracle redo log SQL source. */
 @RunWith(Parameterized.class)
 public class OracleConnectorITCase extends AbstractTestBase {
     private static final int RECORDS_COUNT = 10_000;

File: flink-connector-sqlserver-cdc/src/main/java/com/ververica/cdc/connectors/sqlserver/source/utils/SqlServerTypeUtils.java
Patch:
@@ -55,6 +55,8 @@ private static DataType convertFromColumn(Column column) {
             case Types.SMALLINT:
             case Types.TINYINT:
                 return DataTypes.INT();
+            case Types.BIGINT:
+                return DataTypes.BIGINT();
             case Types.FLOAT:
             case Types.REAL:
             case Types.DOUBLE:

File: flink-connector-sqlserver-cdc/src/main/java/com/ververica/cdc/connectors/sqlserver/source/dialect/SqlServerChunkSplitter.java
Patch:
@@ -274,7 +274,7 @@ private List<ChunkRange> splitUnevenlySizedChunks(
         while (chunkEnd != null && ObjectUtils.compare(chunkEnd, max) <= 0) {
             // we start from [null, min + chunk_size) and avoid [null, min)
             splits.add(ChunkRange.of(chunkStart, chunkEnd));
-            // may sleep a while to avoid DDOS on SqlServer server
+            // may sleep awhile to avoid DDOS on SqlServer server
             maySleep(count++, tableId);
             chunkStart = chunkEnd;
             chunkEnd = nextChunkEnd(jdbc, chunkEnd, tableId, splitColumnName, max, chunkSize);

File: flink-connector-sqlserver-cdc/src/main/java/com/ververica/cdc/connectors/sqlserver/source/reader/fetch/SqlServerSourceFetchTaskContext.java
Patch:
@@ -77,8 +77,8 @@ public class SqlServerSourceFetchTaskContext extends JdbcSourceFetchTaskContext
      * A separate connection for retrieving details of the schema changes; without it, adaptive
      * buffering will not work.
      *
-     * @link
-     *     https://docs.microsoft.com/en-us/sql/connect/jdbc/using-adaptive-buffering?view=sql-server-2017#guidelines-for-using-adaptive-buffering
+     * <p>For more details, please refer to <a
+     * href="https://docs.microsoft.com/en-us/sql/connect/jdbc/using-adaptive-buffering?view=sql-server-2017#guidelines-for-using-adaptive-buffering">guidelines-for-using-adaptive-buffering</a>
      */
     private final SqlServerConnection metaDataConnection;
 

File: flink-cdc-e2e-tests/src/test/java/com/ververica/cdc/connectors/tests/TiDBE2eITCase.java
Patch:
@@ -71,7 +71,7 @@ public class TiDBE2eITCase extends FlinkContainerTestEnvironment {
 
     @ClassRule
     public static final GenericContainer<?> PD =
-            new GenericContainer<>("pingcap/pd:v6.0.0")
+            new GenericContainer<>("pingcap/pd:v6.1.0")
                     .withExposedPorts(PD_PORT)
                     .withFileSystemBind("src/test/resources/docker/tidb/pd.toml", "/pd.toml")
                     .withCommand(
@@ -92,7 +92,7 @@ public class TiDBE2eITCase extends FlinkContainerTestEnvironment {
 
     @ClassRule
     public static final GenericContainer<?> TIKV =
-            new GenericContainer<>("pingcap/tikv:v6.0.0")
+            new GenericContainer<>("pingcap/tikv:v6.1.0")
                     .withExposedPorts(TIKV_PORT)
                     .withFileSystemBind("src/test/resources/docker/tidb/tikv.toml", "/tikv.toml")
                     .withCommand(
@@ -110,7 +110,7 @@ public class TiDBE2eITCase extends FlinkContainerTestEnvironment {
 
     @ClassRule
     public static final GenericContainer<?> TIDB =
-            new GenericContainer<>("pingcap/tidb:v6.0.0")
+            new GenericContainer<>("pingcap/tidb:v6.1.0")
                     .withExposedPorts(TIDB_PORT)
                     .withFileSystemBind("src/test/resources/docker/tidb/tidb.toml", "/tidb.toml")
                     .withCommand(

File: flink-connector-tidb-cdc/src/test/java/com/ververica/cdc/connectors/tidb/TiDBTestBase.java
Patch:
@@ -72,7 +72,7 @@ public class TiDBTestBase extends AbstractTestBase {
 
     @ClassRule
     public static final GenericContainer<?> PD =
-            new FixedHostPortGenericContainer<>("pingcap/pd:v6.0.0")
+            new FixedHostPortGenericContainer<>("pingcap/pd:v6.1.0")
                     .withFileSystemBind("src/test/resources/config/pd.toml", "/pd.toml")
                     .withFixedExposedPort(pdPort, PD_PORT_ORIGIN)
                     .withCommand(
@@ -92,7 +92,7 @@ public class TiDBTestBase extends AbstractTestBase {
 
     @ClassRule
     public static final GenericContainer<?> TIKV =
-            new FixedHostPortGenericContainer<>("pingcap/tikv:v6.0.0")
+            new FixedHostPortGenericContainer<>("pingcap/tikv:v6.1.0")
                     .withFixedExposedPort(TIKV_PORT_ORIGIN, TIKV_PORT_ORIGIN)
                     .withFileSystemBind("src/test/resources/config/tikv.toml", "/tikv.toml")
                     .withCommand(
@@ -110,7 +110,7 @@ public class TiDBTestBase extends AbstractTestBase {
 
     @ClassRule
     public static final GenericContainer<?> TIDB =
-            new GenericContainer<>("pingcap/tidb:v6.0.0")
+            new GenericContainer<>("pingcap/tidb:v6.1.0")
                     .withExposedPorts(TIDB_PORT)
                     .withFileSystemBind("src/test/resources/config/tidb.toml", "/tidb.toml")
                     .withCommand(

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/debezium/reader/SnapshotSplitReader.java
Patch:
@@ -174,7 +174,7 @@ public void submitSplit(MySqlSplit mySqlSplit) {
                                                     "Read snapshot for mysql split %s fail",
                                                     currentSnapshotSplit)));
                         }
-                    } catch (Exception e) {
+                    } catch (Throwable e) {
                         setReadException(e);
                     }
                 });

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/metrics/SourceReaderMetricConstants.java
Patch:
@@ -22,4 +22,6 @@ public class SourceReaderMetricConstants {
     public static final String SOURCE_IDLE_TIME = "sourceIdleTime";
     public static final String CURRENT_FETCH_EVENT_TIME_LAG = "currentFetchEventTimeLag";
     public static final String CURRENT_EMIT_EVENT_TIME_LAG = "currentEmitEventTimeLag";
+    public static final String PENDING_RECORDS = "pendingRecords";
+    public static final String NUM_RECORDS_IN_ERRORS = "numRecordsInErrors";
 }

File: flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/source/reader/fetch/MongoDBFetchTaskContext.java
Patch:
@@ -132,7 +132,7 @@ public boolean isRecordBetween(SourceRecord record, Object[] splitStart, Object[
         BsonDocument splitKeys = (BsonDocument) splitStart[0];
         String firstKey = splitKeys.getFirstKey();
         BsonValue keyValue = documentKey.get(firstKey);
-        BsonValue lowerBound = ((BsonDocument) splitEnd[1]).get(firstKey);
+        BsonValue lowerBound = ((BsonDocument) splitStart[1]).get(firstKey);
         BsonValue upperBound = ((BsonDocument) splitEnd[1]).get(firstKey);
 
         // for all range

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/split/MySqlRecords.java
Patch:
@@ -36,7 +36,7 @@ public final class MySqlRecords implements RecordsWithSplitIds<SourceRecords> {
 
     public MySqlRecords(
             @Nullable String splitId,
-            @Nullable Iterator recordsForSplit,
+            @Nullable Iterator<SourceRecords> recordsForSplit,
             Set<String> finishedSnapshotSplits) {
         this.splitId = splitId;
         this.recordsForSplit = recordsForSplit;

File: flink-connector-vitess-cdc/src/test/java/com/vervetica/cdc/connectors/vitess/VitessTestBase.java
Patch:
@@ -41,7 +41,6 @@
 
 import static com.vervetica.cdc.connectors.vitess.container.VitessContainer.GRPC_PORT;
 import static com.vervetica.cdc.connectors.vitess.container.VitessContainer.MYSQL_PORT;
-import static com.vervetica.cdc.connectors.vitess.container.VitessContainer.VTCTLD_GRPC_PORT;
 import static org.junit.Assert.assertNotNull;
 
 /** Basic class for testing Vitess source, this contains a Vitess container. */
@@ -56,7 +55,7 @@ public abstract class VitessTestBase extends AbstractTestBase {
                             .withKeyspace("test")
                             .withUsername("flinkuser")
                             .withPassword("flinkpwd")
-                            .withExposedPorts(MYSQL_PORT, GRPC_PORT, VTCTLD_GRPC_PORT)
+                            .withExposedPorts(MYSQL_PORT, GRPC_PORT)
                             .withLogConsumer(new Slf4jLogConsumer(LOG));
 
     @BeforeClass

File: flink-connector-sqlserver-cdc/src/main/java/com/ververica/cdc/connectors/sqlserver/source/dialect/SqlServerDialect.java
Patch:
@@ -99,7 +99,9 @@ public List<TableId> discoverDataCollections(JdbcSourceConfig sourceConfig) {
         SqlServerSourceConfig sqlserverSourceConfig = (SqlServerSourceConfig) sourceConfig;
         try (JdbcConnection jdbcConnection = openJdbcConnection(sourceConfig)) {
             return SqlServerConnectionUtils.listTables(
-                    jdbcConnection, sqlserverSourceConfig.getTableFilters());
+                    jdbcConnection,
+                    sqlserverSourceConfig.getTableFilters(),
+                    sqlserverSourceConfig.getDatabaseList());
         } catch (SQLException e) {
             throw new FlinkRuntimeException("Error to discover tables: " + e.getMessage(), e);
         }

File: flink-connector-postgres-cdc/src/main/java/com/ververica/cdc/connectors/postgres/source/fetch/PostgresScanFetchTask.java
Patch:
@@ -109,7 +109,7 @@ public void execute(Context context) throws Exception {
                         ctx.getDispatcher(),
                         ctx.getSnapshotChangeEventSourceMetrics(),
                         split,
-                        ctx.getSlotName(),
+                        ((PostgresSourceConfig) ctx.getSourceConfig()).getSlotNameForBackfillTask(),
                         ctx.getPluginName());
 
         SnapshotSplitChangeEventSourceContext changeEventSourceContext =

File: flink-connector-postgres-cdc/src/test/java/com/ververica/cdc/connectors/postgres/PostgreSQLSourceTest.java
Patch:
@@ -77,8 +77,8 @@
 import static org.testcontainers.containers.PostgreSQLContainer.POSTGRESQL_PORT;
 
 /** Tests for {@link PostgreSQLSource} which also heavily tests {@link DebeziumSourceFunction}. */
-public class PostgresSQLSourceTest extends PostgresTestBase {
-    private static final Logger LOG = LoggerFactory.getLogger(PostgresSQLSourceTest.class);
+public class PostgreSQLSourceTest extends PostgresTestBase {
+    private static final Logger LOG = LoggerFactory.getLogger(PostgreSQLSourceTest.class);
     private static final String SLOT_NAME = "flink";
     // These tests only passes at the docker postgres:9.6
     private static final PostgreSQLContainer<?> POSTGRES_CONTAINER_OLD =

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/table/MySqlTableSourceFactoryTest.java
Patch:
@@ -707,7 +707,7 @@ public void testValidation() {
         } catch (Throwable t) {
             String msg =
                     "Invalid value for option 'scan.startup.mode'. Supported values are "
-                            + "[initial, latest-offset], "
+                            + "[initial, latest-offset, earliest-offset, specific-offset, timestamp], "
                             + "but was: abc";
             assertTrue(ExceptionUtils.findThrowableWithMessage(t, msg).isPresent());
         }

File: flink-connector-db2-cdc/src/main/java/com/ververica/cdc/connectors/db2/Db2Source.java
Patch:
@@ -63,7 +63,7 @@ public DebeziumSourceFunction<T> build() {
             props.setProperty("database.history.skip.unparseable.ddl", String.valueOf(true));
 
             if (tableList != null) {
-                props.setProperty("table.whitelist", String.join(",", tableList));
+                props.setProperty("table.include.list", String.join(",", tableList));
             }
             if (dbzProperties != null) {
                 props.putAll(dbzProperties);

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/MySqlSource.java
Patch:
@@ -180,10 +180,10 @@ public DebeziumSourceFunction<T> build() {
                 props.setProperty("database.server.id", String.valueOf(serverId));
             }
             if (databaseList != null) {
-                props.setProperty("database.whitelist", String.join(",", databaseList));
+                props.setProperty("database.include.list", String.join(",", databaseList));
             }
             if (tableList != null) {
-                props.setProperty("table.whitelist", String.join(",", tableList));
+                props.setProperty("table.include.list", String.join(",", tableList));
             }
             if (serverTimeZone != null) {
                 props.setProperty("database.serverTimezone", serverTimeZone);

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/OracleSource.java
Patch:
@@ -160,7 +160,7 @@ public DebeziumSourceFunction<T> build() {
             props.setProperty("database.history.skip.unparseable.ddl", String.valueOf(true));
             props.setProperty("database.dbname", checkNotNull(database));
             if (schemaList != null) {
-                props.setProperty("schema.whitelist", String.join(",", schemaList));
+                props.setProperty("schema.include.list", String.join(",", schemaList));
             }
             if (tableList != null) {
                 props.setProperty("table.include.list", String.join(",", tableList));

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/source/config/OracleSourceConfigFactory.java
Patch:
@@ -94,7 +94,7 @@ public OracleSourceConfig create(int subtaskId) {
         }
 
         if (schemaList != null) {
-            props.setProperty("schema.whitelist", String.join(",", schemaList));
+            props.setProperty("schema.include.list", String.join(",", schemaList));
         }
 
         if (tableList != null) {

File: flink-connector-postgres-cdc/src/main/java/com/ververica/cdc/connectors/postgres/PostgreSQLSource.java
Patch:
@@ -166,10 +166,10 @@ public DebeziumSourceFunction<T> build() {
             props.setProperty("heartbeat.interval.ms", String.valueOf(DEFAULT_HEARTBEAT_MS));
 
             if (schemaList != null) {
-                props.setProperty("schema.whitelist", String.join(",", schemaList));
+                props.setProperty("schema.include.list", String.join(",", schemaList));
             }
             if (tableList != null) {
-                props.setProperty("table.whitelist", String.join(",", tableList));
+                props.setProperty("table.include.list", String.join(",", tableList));
             }
 
             if (dbzProperties != null) {

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/config/JdbcSourceConfigFactory.java
Patch:
@@ -105,7 +105,7 @@ public JdbcSourceConfigFactory password(String password) {
     /**
      * The session time zone in database server, e.g. "America/Los_Angeles". It controls how the
      * TIMESTAMP type converted to STRING. See more
-     * https://debezium.io/documentation/reference/1.5/connectors/mysql.html#mysql-temporal-types
+     * https://debezium.io/documentation/reference/1.9/connectors/mysql.html#mysql-temporal-types
      */
     public JdbcSourceConfigFactory serverTimeZone(String timeZone) {
         this.serverTimeZone = timeZone;

File: flink-cdc-base/src/test/java/com/ververica/cdc/connectors/base/experimental/MySqlSourceBuilder.java
Patch:
@@ -117,7 +117,7 @@ public MySqlSourceBuilder<T> serverId(String serverId) {
     /**
      * The session time zone in database server, e.g. "America/Los_Angeles". It controls how the
      * TIMESTAMP type in MYSQL converted to STRING. See more
-     * https://debezium.io/documentation/reference/1.5/connectors/mysql.html#mysql-temporal-types
+     * https://debezium.io/documentation/reference/1.9/connectors/mysql.html#mysql-temporal-types
      */
     public MySqlSourceBuilder<T> serverTimeZone(String timeZone) {
         this.configFactory.serverTimeZone(timeZone);

File: flink-connector-debezium/src/main/java/com/ververica/cdc/debezium/DebeziumSourceFunction.java
Patch:
@@ -100,7 +100,7 @@
  * <p>Note: currently, the source function can't run in multiple parallel instances.
  *
  * <p>Please refer to Debezium's documentation for the available configuration properties:
- * https://debezium.io/documentation/reference/1.5/development/engine.html#engine-properties
+ * https://debezium.io/documentation/reference/1.9/development/engine.html#engine-properties
  */
 @PublicEvolving
 public class DebeziumSourceFunction<T> extends RichSourceFunction<T>

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/MySqlSource.java
Patch:
@@ -116,7 +116,7 @@ public Builder<T> password(String password) {
         /**
          * The session time zone in database server, e.g. "America/Los_Angeles". It controls how the
          * TIMESTAMP type in MYSQL converted to STRING. See more
-         * https://debezium.io/documentation/reference/1.5/connectors/mysql.html#mysql-temporal-types
+         * https://debezium.io/documentation/reference/1.9/connectors/mysql.html#mysql-temporal-types
          */
         public Builder<T> serverTimeZone(String timeZone) {
             this.serverTimeZone = timeZone;

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/MySqlSourceBuilder.java
Patch:
@@ -113,7 +113,7 @@ public MySqlSourceBuilder<T> serverId(String serverId) {
     /**
      * The session time zone in database server, e.g. "America/Los_Angeles". It controls how the
      * TIMESTAMP type in MYSQL converted to STRING. See more
-     * https://debezium.io/documentation/reference/1.5/connectors/mysql.html#mysql-temporal-types
+     * https://debezium.io/documentation/reference/1.9/connectors/mysql.html#mysql-temporal-types
      */
     public MySqlSourceBuilder<T> serverTimeZone(String timeZone) {
         this.configFactory.serverTimeZone(timeZone);

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/config/MySqlSourceConfigFactory.java
Patch:
@@ -139,7 +139,7 @@ public MySqlSourceConfigFactory serverId(String serverId) {
     /**
      * The session time zone in database server, e.g. "America/Los_Angeles". It controls how the
      * TIMESTAMP type in MYSQL converted to STRING. See more
-     * https://debezium.io/documentation/reference/1.5/connectors/mysql.html#mysql-temporal-types
+     * https://debezium.io/documentation/reference/1.9/connectors/mysql.html#mysql-temporal-types
      */
     public MySqlSourceConfigFactory serverTimeZone(String timeZone) {
         this.serverTimeZone = timeZone;

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/source/OracleSourceBuilder.java
Patch:
@@ -106,7 +106,7 @@ public OracleSourceBuilder<T> password(String password) {
     /**
      * The session time zone in database server, e.g. "America/Los_Angeles". It controls how the
      * TIMESTAMP type in Oracle converted to STRING. See more
-     * https://debezium.io/documentation/reference/1.5/connectors/Oracle.html#Oracle-temporal-types
+     * https://debezium.io/documentation/reference/1.9/connectors/oracle.html#oracle-temporal-types
      */
     public OracleSourceBuilder<T> serverTimeZone(String timeZone) {
         this.configFactory.serverTimeZone(timeZone);

File: flink-connector-postgres-cdc/src/main/java/com/ververica/cdc/connectors/postgres/table/PostgresValueValidator.java
Patch:
@@ -30,7 +30,7 @@ public final class PostgresValueValidator
             "The \"before\" field of UPDATE/DELETE message is null, "
                     + "please check the Postgres table has been set REPLICA IDENTITY to FULL level. "
                     + "You can update the setting by running the command in Postgres 'ALTER TABLE %s REPLICA IDENTITY FULL'. "
-                    + "Please see more in Debezium documentation: https://debezium.io/documentation/reference/1.5/connectors/postgresql.html#postgresql-replica-identity";
+                    + "Please see more in Debezium documentation: https://debezium.io/documentation/reference/1.9/connectors/postgresql.html#postgresql-replica-identity";
 
     private final String schemaTable;
 

File: flink-cdc-e2e-tests/src/test/java/com/ververica/cdc/connectors/tests/OracleE2eITCase.java
Patch:
@@ -168,11 +168,12 @@ public void testOracleCDC() throws Exception {
                         "108,jacket,water resistent black wind breaker,0.1",
                         "109,spare tire,24 inch spare tire,22.2",
                         "111,jacket,new water resistent white wind breaker,0.5");
+        // Oracle cdc's backfill task will cost much time, increase the timeout here
         proxy.checkResultWithTimeout(
                 expectResult,
                 "products_sink",
                 new String[] {"id", "name", "description", "weight"},
-                150000L);
+                300000L);
     }
 
     private Connection getOracleJdbcConnection() throws SQLException {

File: flink-connector-sqlserver-cdc/src/main/java/com/ververica/cdc/connectors/sqlserver/source/offset/LsnOffset.java
Patch:
@@ -52,7 +52,7 @@ public LsnOffset(Lsn lsn) {
     }
 
     public Lsn getLcn() {
-        return Lsn.valueOf(offset.get(SourceInfo.CHANGE_LSN_KEY));
+        return Lsn.valueOf(offset.get(SourceInfo.COMMIT_LSN_KEY));
     }
 
     @Override

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/reader/external/IncrementalSourceStreamFetcher.java
Patch:
@@ -110,6 +110,8 @@ public Iterator<SourceRecords> pollSplitRecords() throws InterruptedException {
             for (DataChangeEvent event : batch) {
                 if (shouldEmit(event.getRecord())) {
                     sourceRecords.add(event.getRecord());
+                } else {
+                    LOG.debug("{} data change event should not emit", event);
                 }
             }
         }

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/reader/external/JdbcSourceFetchTaskContext.java
Patch:
@@ -157,7 +157,7 @@ public CommonConnectorConfig getDbzConnectorConfig() {
     }
 
     public SchemaNameAdjuster getSchemaNameAdjuster() {
-        return null;
+        return schemaNameAdjuster;
     }
 
     public abstract RelationalDatabaseSchema getDatabaseSchema();

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/OracleSource.java
Patch:
@@ -19,7 +19,6 @@
 import com.ververica.cdc.connectors.base.options.StartupOptions;
 import com.ververica.cdc.debezium.DebeziumDeserializationSchema;
 import com.ververica.cdc.debezium.DebeziumSourceFunction;
-import com.ververica.cdc.debezium.internal.DebeziumOffset;
 import io.debezium.connector.oracle.OracleConnector;
 
 import javax.annotation.Nullable;
@@ -167,7 +166,6 @@ public DebeziumSourceFunction<T> build() {
                 props.setProperty("table.include.list", String.join(",", tableList));
             }
 
-            DebeziumOffset specificOffset = null;
             switch (startupOptions.startupMode) {
                 case INITIAL:
                     props.setProperty("snapshot.mode", "initial");
@@ -193,7 +191,7 @@ public DebeziumSourceFunction<T> build() {
             }
 
             return new DebeziumSourceFunction<>(
-                    deserializer, props, specificOffset, new OracleValidator(props));
+                    deserializer, props, null, new OracleValidator(props));
         }
     }
 }

File: flink-connector-oracle-cdc/src/main/java/io/debezium/connector/oracle/logminer/TransactionalBuffer.java
Patch:
@@ -410,7 +410,8 @@ boolean commit(
                                 event.getEntry().getOldValues(),
                                 event.getEntry().getNewValues(),
                                 schema.tableFor(event.getTableId()),
-                                clock));
+                                clock,
+                                event.rowId));
             }
 
             lastCommittedScn = Scn.valueOf(scn.longValue());

File: flink-connector-oracle-cdc/src/test/java/com/ververica/cdc/connectors/oracle/OracleChangeEventSourceExampleTest.java
Patch:
@@ -116,7 +116,6 @@ public void testConsumingAllEvents() throws Exception {
                 .setParallelism(DEFAULT_PARALLELISM)
                 .print()
                 .setParallelism(1);
-
         env.execute("Print Oracle Snapshot + RedoLog");
     }
 }

File: flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/table/MongoDBTableSource.java
Patch:
@@ -50,7 +50,7 @@
 
 import static com.mongodb.MongoNamespace.checkCollectionNameValidity;
 import static com.mongodb.MongoNamespace.checkDatabaseNameValidity;
-import static com.ververica.cdc.connectors.mongodb.source.utils.CollectionDiscoveryUtils.containsRegexMetaCharacters;
+import static com.ververica.cdc.connectors.mongodb.source.utils.CollectionDiscoveryUtils.inferIsRegularExpression;
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
 /**
@@ -158,8 +158,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {
         String collectionList = null;
         if (StringUtils.isNotEmpty(database) && StringUtils.isNotEmpty(collection)) {
             // explicitly specified database and collection.
-            if (!containsRegexMetaCharacters(database)
-                    && !containsRegexMetaCharacters(collection)) {
+            if (!inferIsRegularExpression(database) && !inferIsRegularExpression(collection)) {
                 checkDatabaseNameValidity(database);
                 checkCollectionNameValidity(collection);
                 databaseList = database;

File: flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/table/MongoDBConnectorDeserializationSchema.java
Patch:
@@ -124,10 +124,9 @@ public void deserialize(SourceRecord record, Collector<RowData> out) throws Exce
         Schema valueSchema = record.valueSchema();
 
         OperationType op = operationTypeFor(record);
+
         BsonDocument documentKey =
-                checkNotNull(
-                        extractBsonDocument(
-                                value, valueSchema, MongoDBEnvelope.DOCUMENT_KEY_FIELD));
+                extractBsonDocument(value, valueSchema, MongoDBEnvelope.DOCUMENT_KEY_FIELD);
         BsonDocument fullDocument =
                 extractBsonDocument(value, valueSchema, MongoDBEnvelope.FULL_DOCUMENT_FIELD);
 

File: flink-connector-mysql-cdc/src/main/java/io/debezium/connector/mysql/util/ErrorMessageUtils.java
Patch:
@@ -27,7 +27,8 @@ public class ErrorMessageUtils {
             Pattern.compile(
                     ".*The connector is trying to read binlog.*but this is no longer available on the server.*");
     private static final Pattern MISSING_TRANSACTION_WHEN_BINLOG_EXPIRE =
-            Pattern.compile(".*Cannot replicate because the master purged required binary logs.*");
+            Pattern.compile(
+                    ".*Cannot replicate because the (master|source) purged required binary logs.*");
 
     /** Add more error details for some exceptions. */
     public static String optimizeErrorMessage(String msg) {

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/debezium/task/context/StatefulTaskContext.java
Patch:
@@ -176,7 +176,7 @@ private void validateAndLoadDatabaseHistory(
     }
 
     /** Loads the connector's persistent offset (if present) via the given loader. */
-    private MySqlOffsetContext loadStartingOffsetState(
+    protected MySqlOffsetContext loadStartingOffsetState(
             OffsetContext.Loader<MySqlOffsetContext> loader, MySqlSplit mySqlSplit) {
         BinlogOffset offset =
                 mySqlSplit.isSnapshotSplit()

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/split/MySqlSplitSerializer.java
Patch:
@@ -41,6 +41,7 @@
 import static com.ververica.cdc.connectors.mysql.source.utils.SerializerUtils.rowToSerializedString;
 import static com.ververica.cdc.connectors.mysql.source.utils.SerializerUtils.serializedStringToRow;
 import static com.ververica.cdc.connectors.mysql.source.utils.SerializerUtils.writeBinlogPosition;
+import static com.ververica.cdc.connectors.mysql.source.utils.StatementUtils.quote;
 
 /** A serializer for the {@link MySqlSplit}. */
 public final class MySqlSplitSerializer implements SimpleVersionedSerializer<MySqlSplit> {
@@ -70,7 +71,7 @@ public byte[] serialize(MySqlSplit split) throws IOException {
 
             final DataOutputSerializer out = SERIALIZER_CACHE.get();
             out.writeInt(SNAPSHOT_SPLIT_FLAG);
-            out.writeUTF(snapshotSplit.getTableId().toString());
+            out.writeUTF(quote(snapshotSplit.getTableId()));
             out.writeUTF(snapshotSplit.splitId());
             out.writeUTF(snapshotSplit.getSplitKeyType().asSerializableString());
 

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/testutils/UniqueDatabase.java
Patch:
@@ -45,8 +45,8 @@
 public class UniqueDatabase {
 
     private static final String[] CREATE_DATABASE_DDL =
-            new String[] {"CREATE DATABASE $DBNAME$;", "USE $DBNAME$;"};
-    private static final String DROP_DATABASE_DDL = "DROP DATABASE IF EXISTS $DBNAME$;";
+            new String[] {"CREATE DATABASE `$DBNAME$`;", "USE `$DBNAME$`;"};
+    private static final String DROP_DATABASE_DDL = "DROP DATABASE IF EXISTS `$DBNAME$`;";
     private static final Pattern COMMENT_PATTERN = Pattern.compile("^(.*)--.*$");
 
     private final MySqlContainer container;

File: flink-connector-sqlserver-cdc/src/main/java/com/ververica/cdc/connectors/sqlserver/source/offset/LsnFactory.java
Patch:
@@ -27,7 +27,9 @@
 public class LsnFactory extends OffsetFactory {
     @Override
     public Offset newOffset(Map<String, String> offset) {
-        return new LsnOffset(Lsn.valueOf(offset.get(SourceInfo.CHANGE_LSN_KEY)));
+        Lsn changeLsn = Lsn.valueOf(offset.get(SourceInfo.CHANGE_LSN_KEY));
+        Lsn commitLsn = Lsn.valueOf(offset.get(SourceInfo.COMMIT_LSN_KEY));
+        return new LsnOffset(changeLsn, commitLsn, null);
     }
 
     @Override

File: flink-connector-sqlserver-cdc/src/main/java/com/ververica/cdc/connectors/sqlserver/source/reader/fetch/SqlServerStreamFetchTask.java
Patch:
@@ -52,6 +52,7 @@ public SqlServerStreamFetchTask(StreamSplit split) {
     public void execute(Context context) throws Exception {
         SqlServerSourceFetchTaskContext sourceFetchContext =
                 (SqlServerSourceFetchTaskContext) context;
+        sourceFetchContext.getOffsetContext().preSnapshotCompletion();
         taskRunning = true;
         redoLogSplitReadTask =
                 new LsnSplitReadTask(

File: flink-connector-oracle-cdc/src/test/java/com/ververica/cdc/connectors/oracle/table/OracleConnectorITCase.java
Patch:
@@ -26,6 +26,7 @@
 import com.ververica.cdc.connectors.oracle.utils.OracleTestUtils;
 import org.junit.After;
 import org.junit.Before;
+import org.junit.Ignore;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
@@ -102,6 +103,7 @@ public void teardown() {
     }
 
     @Test
+    @Ignore("It can be open until issue 1875 fix")
     public void testConsumingAllEvents()
             throws SQLException, ExecutionException, InterruptedException {
         String sourceDDL =

File: flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/source/reader/fetch/MongoDBStreamFetchTask.java
Patch:
@@ -130,6 +130,8 @@ public void execute(Context context) throws Exception {
                                         .map(this::normalizeHeartbeatRecord)
                                         .orElse(null);
                     }
+                    // update nextUpdateTime
+                    nextUpdate = time.milliseconds() + sourceConfig.getPollAwaitTimeMillis();
                 } else {
                     BsonDocument changeStreamDocument = next.get();
                     MongoNamespace namespace = getMongoNamespace(changeStreamDocument);

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/assigners/MySqlChunkSplitter.java
Patch:
@@ -426,7 +426,7 @@ private static String splitId(TableId tableId, int chunkId) {
     }
 
     private static void maySleep(int count, TableId tableId) {
-        // every 100 queries to sleep 1s
+        // every 10 queries to sleep 0.1s
         if (count % 10 == 0) {
             try {
                 Thread.sleep(100);

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/source/assigner/splitter/OracleChunkSplitter.java
Patch:
@@ -367,7 +367,7 @@ private static String splitId(TableId tableId, int chunkId) {
     }
 
     private static void maySleep(int count, TableId tableId) {
-        // every 100 queries to sleep 1s
+        // every 10 queries to sleep 0.1s
         if (count % 10 == 0) {
             try {
                 Thread.sleep(100);

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/assigner/SplitAssigner.java
Patch:
@@ -56,7 +56,7 @@ public interface SplitAssigner {
     boolean waitingForFinishedSplits();
 
     /**
-     * Gets the finished splits information. This is useful meta data to generate a stream split
+     * Gets the finished splits' information. This is useful metadata to generate a stream split
      * that considering finished snapshot splits.
      */
     List<FinishedSnapshotSplitInfo> getFinishedSplitInfos();

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/reader/IncrementalSourceReader.java
Patch:
@@ -243,7 +243,7 @@ private void fillMetaDataForStreamSplit(StreamSplitMetaEvent metadataEvent) {
                         streamSplit.splitId(),
                         StreamSplit.appendFinishedSplitInfos(streamSplit, metaDataGroup));
 
-                LOG.info("Fill meta data of group {} to stream split", metaDataGroup.size());
+                LOG.info("Fill metadata of group {} to stream split", metaDataGroup.size());
             } else {
                 LOG.warn(
                         "Received out of oder metadata event for split {}, the received meta group id is {}, but expected is {}, ignore it",

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/utils/ChunkUtils.java
Patch:
@@ -81,8 +81,6 @@ public static Column getChunkKeyColumn(Table table, @Nullable String chunkKeyCol
     /** Returns next meta group id according to received meta number and meta group size. */
     public static int getNextMetaGroupId(int receivedMetaNum, int metaGroupSize) {
         Preconditions.checkState(metaGroupSize > 0);
-        return receivedMetaNum % metaGroupSize == 0
-                ? (receivedMetaNum / metaGroupSize)
-                : (receivedMetaNum / metaGroupSize) + 1;
+        return receivedMetaNum / metaGroupSize;
     }
 }

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/reader/external/IncrementalSourceScanFetcher.java
Patch:
@@ -190,7 +190,7 @@ public void close() {
         try {
             if (executorService != null) {
                 executorService.shutdown();
-                if (executorService.awaitTermination(
+                if (!executorService.awaitTermination(
                         READER_CLOSE_TIMEOUT_SECONDS, TimeUnit.SECONDS)) {
                     LOG.warn(
                             "Failed to close the scan fetcher in {} seconds.",

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/reader/external/IncrementalSourceStreamFetcher.java
Patch:
@@ -133,7 +133,7 @@ public void close() {
         try {
             if (executorService != null) {
                 executorService.shutdown();
-                if (executorService.awaitTermination(
+                if (!executorService.awaitTermination(
                         READER_CLOSE_TIMEOUT_SECONDS, TimeUnit.SECONDS)) {
                     LOG.warn(
                             "Failed to close the stream fetcher in {} seconds.",

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/debezium/reader/BinlogSplitReader.java
Patch:
@@ -193,7 +193,7 @@ public void close() {
             currentTaskRunning = false;
             if (executorService != null) {
                 executorService.shutdown();
-                if (executorService.awaitTermination(READER_CLOSE_TIMEOUT, TimeUnit.SECONDS)) {
+                if (!executorService.awaitTermination(READER_CLOSE_TIMEOUT, TimeUnit.SECONDS)) {
                     LOG.warn(
                             "Failed to close the binlog split reader in {} seconds.",
                             READER_CLOSE_TIMEOUT);

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/debezium/reader/SnapshotSplitReader.java
Patch:
@@ -339,7 +339,7 @@ public void close() {
             }
             if (executorService != null) {
                 executorService.shutdown();
-                if (executorService.awaitTermination(READER_CLOSE_TIMEOUT, TimeUnit.SECONDS)) {
+                if (!executorService.awaitTermination(READER_CLOSE_TIMEOUT, TimeUnit.SECONDS)) {
                     LOG.warn(
                             "Failed to close the snapshot split reader in {} seconds.",
                             READER_CLOSE_TIMEOUT);

File: flink-connector-tidb-cdc/src/main/java/com/ververica/cdc/connectors/tidb/TiKVRichParallelSourceFunction.java
Patch:
@@ -272,7 +272,7 @@ public void cancel() {
             }
             if (executorService != null) {
                 executorService.shutdown();
-                if (executorService.awaitTermination(CLOSE_TIMEOUT, TimeUnit.SECONDS)) {
+                if (!executorService.awaitTermination(CLOSE_TIMEOUT, TimeUnit.SECONDS)) {
                     LOG.warn(
                             "Failed to close the tidb source function in {} seconds.",
                             CLOSE_TIMEOUT);

File: flink-connector-debezium/src/main/java/com/ververica/cdc/debezium/DebeziumSourceFunction.java
Patch:
@@ -305,7 +305,8 @@ public void snapshotState(FunctionSnapshotContext functionSnapshotContext) throw
         if (handover.hasError()) {
             LOG.debug("snapshotState() called on closed source");
             throw new FlinkRuntimeException(
-                    "Call snapshotState() on closed source, checkpoint failed.");
+                    "Call snapshotState() on closed source, checkpoint failed.",
+                    handover.getError());
         } else {
             snapshotOffsetState(functionSnapshotContext.getCheckpointId());
             snapshotHistoryRecordsState();

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/debezium/task/context/StatefulTaskContext.java
Patch:
@@ -165,7 +165,8 @@ public void configure(MySqlSplit mySqlSplit) {
                 changeEventSourceMetricsFactory.getStreamingMetrics(
                         taskContext, queue, metadataProvider);
         this.errorHandler =
-                new MySqlErrorHandler(connectorConfig.getLogicalName(), queue, taskContext);
+                new MySqlErrorHandler(
+                        connectorConfig.getLogicalName(), queue, taskContext, sourceConfig);
     }
 
     private void validateAndLoadDatabaseHistory(

File: flink-connector-db2-cdc/src/test/java/com/ververica/cdc/connectors/db2/Db2SourceTest.java
Patch:
@@ -294,7 +294,7 @@ public void go() throws Exception {
             assertDelete(records.get(0), "ID", 1001);
 
             // ---------------------------------------------------------------------------
-            // Step-6: trigger checkpoint-2 to make sure we can continue to to further checkpoints
+            // Step-6: trigger checkpoint-2 to make sure we can continue to further checkpoints
             // ---------------------------------------------------------------------------
             synchronized (sourceContext3.getCheckpointLock()) {
                 // checkpoint 3

File: flink-connector-tidb-cdc/src/main/java/com/ververica/cdc/connectors/tidb/TiKVChangeEventDeserializationSchema.java
Patch:
@@ -20,7 +20,6 @@
 import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
 import org.apache.flink.util.Collector;
 
-import org.apache.kafka.connect.source.SourceRecord;
 import org.tikv.kvproto.Cdcpb.Event.Row;
 
 import java.io.Serializable;
@@ -35,6 +34,6 @@
 public interface TiKVChangeEventDeserializationSchema<T>
         extends Serializable, ResultTypeQueryable<T> {
 
-    /** Deserialize the Debezium record, it is represented in Kafka {@link SourceRecord}. */
+    /** Deserialize the TiDB record. */
     void deserialize(Row record, Collector<T> out) throws Exception;
 }

File: flink-connector-tidb-cdc/src/main/java/com/ververica/cdc/connectors/tidb/TiKVSnapshotEventDeserializationSchema.java
Patch:
@@ -20,7 +20,6 @@
 import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
 import org.apache.flink.util.Collector;
 
-import org.apache.kafka.connect.source.SourceRecord;
 import org.tikv.kvproto.Kvrpcpb.KvPair;
 
 import java.io.Serializable;
@@ -35,6 +34,6 @@
 public interface TiKVSnapshotEventDeserializationSchema<T>
         extends Serializable, ResultTypeQueryable<T> {
 
-    /** Deserialize the Debezium record, it is represented in Kafka {@link SourceRecord}. */
+    /** Deserialize the TiDB record. */
     void deserialize(KvPair record, Collector<T> out) throws Exception;
 }

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/debezium/reader/BinlogSplitReaderTest.java
Patch:
@@ -671,7 +671,6 @@ private List<String> readBinlogSplits(
                     formatResult(
                             pollRecordsFromReader(reader, RecordUtils::isDataChangeRecord),
                             dataType);
-            results.forEach(System.out::println);
             actual.addAll(results);
         }
         return actual;

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/source/assigner/splitter/OracleChunkSplitter.java
Patch:
@@ -51,7 +51,9 @@
 import static com.ververica.cdc.connectors.base.utils.ObjectUtils.doubleCompare;
 import static java.math.BigDecimal.ROUND_CEILING;
 
-/** The {@code ChunkSplitter} used to split table into a set of chunks for JDBC data source. */
+/**
+ * The {@code ChunkSplitter} used to split Oracle table into a set of chunks for JDBC data source.
+ */
 public class OracleChunkSplitter implements JdbcSourceChunkSplitter {
 
     private static final Logger LOG = LoggerFactory.getLogger(OracleChunkSplitter.class);

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/source/config/OracleSourceConfigFactory.java
Patch:
@@ -33,6 +33,7 @@
 /** A factory to initialize {@link OracleSourceConfig}. */
 public class OracleSourceConfigFactory extends JdbcSourceConfigFactory {
 
+    private static final long serialVersionUID = 1L;
     private static final String DATABASE_SERVER_NAME = "oracle_logminer";
     private static final String DRIVER_ClASS_NAME = "oracle.jdbc.OracleDriver";
 

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/table/OracleTableSource.java
Patch:
@@ -40,7 +40,6 @@
 
 import javax.annotation.Nullable;
 
-import java.time.ZoneId;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;

File: flink-connector-oracle-cdc/src/test/java/com/ververica/cdc/connectors/oracle/source/OracleSourceITCase.java
Patch:
@@ -261,7 +261,6 @@ private static List<String> fetchRows(Iterator<Row> iter, int size) {
         while (size > 0 && iter.hasNext()) {
             Row row = iter.next();
             rows.add(row.toString());
-            LOG.info("fetch row:{}", row);
             size--;
         }
         return rows;
@@ -289,7 +288,7 @@ private void createAndInitialize(String sqlFile) throws Exception {
                 statement.execute("DROP TABLE DEBEZIUM.CUSTOMERS");
                 statement.execute("DROP TABLE DEBEZIUM.CUSTOMERS_1");
             } catch (Exception e) {
-                LOG.info("DEBEZIUM.CUSTOMERS DEBEZIUM.CUSTOMERS_1 NOT EXITS");
+                LOG.error("DEBEZIUM.CUSTOMERS DEBEZIUM.CUSTOMERS_1 NOT EXITS", e);
             }
 
             final List<String> statements =

File: flink-connector-oracle-cdc/src/test/java/com/ververica/cdc/connectors/oracle/table/OracleConnectorITCase.java
Patch:
@@ -39,6 +39,7 @@
 import java.sql.DriverManager;
 import java.sql.SQLException;
 import java.sql.Statement;
+import java.time.ZoneId;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
@@ -646,6 +647,8 @@ public void testXmlType() throws Exception {
     public void testAllDataTypes() throws Throwable {
         OracleTestUtils.createAndInitialize(
                 OracleTestUtils.ORACLE_CONTAINER, "column_type_test.sql");
+
+        tEnv.getConfig().setLocalTimeZone(ZoneId.of("Asia/Shanghai"));
         String sourceDDL =
                 String.format(
                         "CREATE TABLE full_types ("

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/table/OracleTableSource.java
Patch:
@@ -131,7 +131,6 @@ public ChangelogMode getChangelogMode() {
 
     @Override
     public ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {
-
         RowType physicalDataType =
                 (RowType) physicalSchema.toPhysicalRowDataType().getLogicalType();
         MetadataConverter[] metadataConverters = getMetadataConverters();

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/source/meta/offset/RedoLogOffsetFactory.java
Patch:
@@ -37,7 +37,7 @@ public Offset newOffset(Map<String, String> offset) {
 
     @Override
     public Offset newOffset(String filename, Long position) {
-        return new RedoLogOffset(filename, position);
+        throw new FlinkRuntimeException("not supported create new Offset by filename and position.");
     }
 
     @Override

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/source/utils/OracleConnectionUtils.java
Patch:
@@ -25,6 +25,7 @@
 import io.debezium.connector.oracle.OracleConnection;
 import io.debezium.connector.oracle.OracleConnectorConfig;
 import io.debezium.connector.oracle.OracleDatabaseSchema;
+import io.debezium.connector.oracle.Scn;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.Column;
 import io.debezium.relational.RelationalTableFilters;
@@ -69,7 +70,7 @@ public static RedoLogOffset currentRedoLogOffset(JdbcConnection jdbc) {
                     rs -> {
                         if (rs.next()) {
                             final String scn = rs.getString(1);
-                            return new RedoLogOffset(scn);
+                            return new RedoLogOffset(Scn.valueOf(scn).longValue());
                         } else {
                             throw new FlinkRuntimeException(
                                     "Cannot read the scn via '"

File: flink-connector-oracle-cdc/src/test/java/com/ververica/cdc/connectors/oracle/OracleSourceTest.java
Patch:
@@ -167,6 +167,7 @@ public void go() throws Exception {
     }
 
     @Test
+    @Ignore("It can be open until DBZ-5245 and DBZ-4936 fix")
     public void testCheckpointAndRestore() throws Exception {
         final TestingListState<byte[]> offsetState = new TestingListState<>();
         final TestingListState<String> historyState = new TestingListState<>();

File: flink-connector-tidb-cdc/src/test/java/com/ververica/cdc/connectors/tidb/table/TiDBConnectorITCase.java
Patch:
@@ -27,6 +27,8 @@
 import org.junit.Before;
 import org.junit.ClassRule;
 import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import java.sql.Connection;
 import java.sql.Statement;
@@ -42,6 +44,7 @@
 /** Integration tests for TiDB change stream event SQL source. */
 public class TiDBConnectorITCase extends TiDBTestBase {
 
+    private static final Logger LOG = LoggerFactory.getLogger(TiDBConnectorITCase.class);
     private final StreamExecutionEnvironment env =
             StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(1);
     private final StreamTableEnvironment tEnv =

File: flink-connector-tidb-cdc/src/main/java/com/ververica/cdc/connectors/tidb/TiKVRichParallelSourceFunction.java
Patch:
@@ -132,10 +132,10 @@ public void run(final SourceContext<T> ctx) throws Exception {
             }
         } else {
             LOG.info("Skip snapshot read");
+            resolvedTs = session.getTimestamp().getVersion();
         }
 
         LOG.info("start read change events");
-        resolvedTs = session.getTimestamp().getVersion();
         cdcClient.start(resolvedTs);
         readChangeEvents();
     }

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/assigner/StreamSplitAssigner.java
Patch:
@@ -127,7 +127,7 @@ public StreamSplit createStreamSplit() {
         return new StreamSplit(
                 BINLOG_SPLIT_ID,
                 dialect.displayCurrentOffset(sourceConfig),
-                offsetFactory.createInitialOffset(),
+                offsetFactory.createNoStoppingOffset(),
                 new ArrayList<>(),
                 new HashMap<>(),
                 0);

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/dialect/DataSourceDialect.java
Patch:
@@ -18,7 +18,6 @@
 
 import org.apache.flink.annotation.Experimental;
 
-import com.ververica.cdc.connectors.base.config.JdbcSourceConfig;
 import com.ververica.cdc.connectors.base.config.SourceConfig;
 import com.ververica.cdc.connectors.base.source.assigner.splitter.ChunkSplitter;
 import com.ververica.cdc.connectors.base.source.meta.offset.Offset;
@@ -70,6 +69,5 @@ public interface DataSourceDialect<ID extends DataCollectionId, S, C extends Sou
     FetchTask<SourceSplitBase> createFetchTask(SourceSplitBase sourceSplitBase);
 
     /** The task context used for fetch task to fetch data from external systems. */
-    FetchTask.Context createFetchTaskContext(
-            SourceSplitBase sourceSplitBase, JdbcSourceConfig taskSourceConfig);
+    FetchTask.Context createFetchTaskContext(SourceSplitBase sourceSplitBase, C sourceConfig);
 }

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/assigner/HybridSplitAssigner.java
Patch:
@@ -23,7 +23,7 @@
 import com.ververica.cdc.connectors.base.source.meta.offset.Offset;
 import com.ververica.cdc.connectors.base.source.meta.offset.OffsetFactory;
 import com.ververica.cdc.connectors.base.source.meta.split.FinishedSnapshotSplitInfo;
-import com.ververica.cdc.connectors.base.source.meta.split.SchemaLessSnapshotSplit;
+import com.ververica.cdc.connectors.base.source.meta.split.SchemalessSnapshotSplit;
 import com.ververica.cdc.connectors.base.source.meta.split.SourceSplitBase;
 import com.ververica.cdc.connectors.base.source.meta.split.StreamSplit;
 import io.debezium.relational.TableId;
@@ -175,7 +175,7 @@ public void close() {
     // --------------------------------------------------------------------------------------------
 
     public StreamSplit createStreamSplit() {
-        final List<SchemaLessSnapshotSplit> assignedSnapshotSplit =
+        final List<SchemalessSnapshotSplit> assignedSnapshotSplit =
                 snapshotSplitAssigner.getAssignedSplits().values().stream()
                         .sorted(Comparator.comparing(SourceSplitBase::splitId))
                         .collect(Collectors.toList());
@@ -184,7 +184,7 @@ public StreamSplit createStreamSplit() {
         final List<FinishedSnapshotSplitInfo> finishedSnapshotSplitInfos = new ArrayList<>();
 
         Offset minBinlogOffset = null;
-        for (SchemaLessSnapshotSplit split : assignedSnapshotSplit) {
+        for (SchemalessSnapshotSplit split : assignedSnapshotSplit) {
             // find the min binlog offset
             Offset binlogOffset = splitFinishedOffsets.get(split.splitId());
             if (minBinlogOffset == null || binlogOffset.isBefore(minBinlogOffset)) {

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/meta/split/SchemalessSnapshotSplit.java
Patch:
@@ -29,9 +29,9 @@
  * A kind of {@link SnapshotSplit} without table schema information, it is useful to reduce memory
  * usage in JobManager.
  */
-public class SchemaLessSnapshotSplit extends SnapshotSplit {
+public class SchemalessSnapshotSplit extends SnapshotSplit {
 
-    public SchemaLessSnapshotSplit(
+    public SchemalessSnapshotSplit(
             TableId tableId,
             String splitId,
             RowType splitKeyType,
@@ -49,7 +49,7 @@ public SchemaLessSnapshotSplit(
     }
 
     /**
-     * Converts current {@link SchemaLessSnapshotSplit} to {@link SnapshotSplit} with given table
+     * Converts current {@link SchemalessSnapshotSplit} to {@link SnapshotSplit} with given table
      * schema information.
      */
     public final SnapshotSplit toSnapshotSplit(TableChange tableSchema) {

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/meta/split/SnapshotSplit.java
Patch:
@@ -88,9 +88,9 @@ public Map<TableId, TableChange> getTableSchemas() {
         return tableSchemas;
     }
 
-    /** Casts this split into a {@link SchemaLessSnapshotSplit}. */
-    public final SchemaLessSnapshotSplit toSchemaLessSnapshotSplit() {
-        return new SchemaLessSnapshotSplit(
+    /** Casts this split into a {@link SchemalessSnapshotSplit}. */
+    public final SchemalessSnapshotSplit toSchemalessSnapshotSplit() {
+        return new SchemalessSnapshotSplit(
                 tableId, splitId, splitKeyType, splitStart, splitEnd, highWatermark);
     }
 

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/meta/split/SourceSplitBase.java
Patch:
@@ -37,7 +37,7 @@ public SourceSplitBase(String splitId) {
 
     /** Checks whether this split is a snapshot split. */
     public final boolean isSnapshotSplit() {
-        return getClass() == SnapshotSplit.class || getClass() == SchemaLessSnapshotSplit.class;
+        return getClass() == SnapshotSplit.class || getClass() == SchemalessSnapshotSplit.class;
     }
 
     /** Checks whether this split is a stream split. */

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/assigners/MySqlHybridSplitAssigner.java
Patch:
@@ -22,7 +22,7 @@
 import com.ververica.cdc.connectors.mysql.source.offset.BinlogOffset;
 import com.ververica.cdc.connectors.mysql.source.split.FinishedSnapshotSplitInfo;
 import com.ververica.cdc.connectors.mysql.source.split.MySqlBinlogSplit;
-import com.ververica.cdc.connectors.mysql.source.split.MySqlSchemaLessSnapshotSplit;
+import com.ververica.cdc.connectors.mysql.source.split.MySqlSchemalessSnapshotSplit;
 import com.ververica.cdc.connectors.mysql.source.split.MySqlSplit;
 import io.debezium.relational.TableId;
 import org.slf4j.Logger;
@@ -187,7 +187,7 @@ public void close() {
     // --------------------------------------------------------------------------------------------
 
     private MySqlBinlogSplit createBinlogSplit() {
-        final List<MySqlSchemaLessSnapshotSplit> assignedSnapshotSplit =
+        final List<MySqlSchemalessSnapshotSplit> assignedSnapshotSplit =
                 snapshotSplitAssigner.getAssignedSplits().values().stream()
                         .sorted(Comparator.comparing(MySqlSplit::splitId))
                         .collect(Collectors.toList());
@@ -197,7 +197,7 @@ private MySqlBinlogSplit createBinlogSplit() {
         final List<FinishedSnapshotSplitInfo> finishedSnapshotSplitInfos = new ArrayList<>();
 
         BinlogOffset minBinlogOffset = null;
-        for (MySqlSchemaLessSnapshotSplit split : assignedSnapshotSplit) {
+        for (MySqlSchemalessSnapshotSplit split : assignedSnapshotSplit) {
             // find the min binlog offset
             BinlogOffset binlogOffset = splitFinishedOffsets.get(split.splitId());
             if (minBinlogOffset == null || binlogOffset.isBefore(minBinlogOffset)) {

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/split/MySqlSchemalessSnapshotSplit.java
Patch:
@@ -29,9 +29,9 @@
  * A kind of {@link MySqlSnapshotSplit} without table schema information, it is useful to reduce
  * memory usage in JobManager.
  */
-public class MySqlSchemaLessSnapshotSplit extends MySqlSnapshotSplit {
+public class MySqlSchemalessSnapshotSplit extends MySqlSnapshotSplit {
 
-    public MySqlSchemaLessSnapshotSplit(
+    public MySqlSchemalessSnapshotSplit(
             TableId tableId,
             String splitId,
             RowType splitKeyType,
@@ -49,7 +49,7 @@ public MySqlSchemaLessSnapshotSplit(
     }
 
     /**
-     * Converts current {@link MySqlSchemaLessSnapshotSplit} to {@link MySqlSnapshotSplit} with
+     * Converts current {@link MySqlSchemalessSnapshotSplit} to {@link MySqlSnapshotSplit} with
      * given table schema information.
      */
     public final MySqlSnapshotSplit toMySqlSnapshotSplit(TableChange tableSchema) {

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/split/MySqlSnapshotSplit.java
Patch:
@@ -88,9 +88,9 @@ public Map<TableId, TableChange> getTableSchemas() {
         return tableSchemas;
     }
 
-    /** Casts this split into a {@link MySqlSchemaLessSnapshotSplit}. */
-    public final MySqlSchemaLessSnapshotSplit toSchemaLessSnapshotSplit() {
-        return new MySqlSchemaLessSnapshotSplit(
+    /** Casts this split into a {@link MySqlSchemalessSnapshotSplit}. */
+    public final MySqlSchemalessSnapshotSplit toSchemalessSnapshotSplit() {
+        return new MySqlSchemalessSnapshotSplit(
                 tableId, splitId, splitKeyType, splitStart, splitEnd, highWatermark);
     }
 

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/split/MySqlSplit.java
Patch:
@@ -36,7 +36,7 @@ public MySqlSplit(String splitId) {
     /** Checks whether this split is a snapshot split. */
     public final boolean isSnapshotSplit() {
         return getClass() == MySqlSnapshotSplit.class
-                || getClass() == MySqlSchemaLessSnapshotSplit.class;
+                || getClass() == MySqlSchemalessSnapshotSplit.class;
     }
 
     /** Checks whether this split is a binlog split. */

File: flink-connector-mysql-cdc/src/main/java/io/debezium/connector/mysql/MySqlDefaultValueConverter.java
Patch:
@@ -30,9 +30,10 @@
 
 /**
  * Copied from Debezium project(v1.6.4.Final) to fix error when parsing the string default value for
- * numeric types.
+ * numeric types. This class should be deleted after https://issues.redhat.com/browse/DBZ-4150
+ * included.
  *
- * <p>Line 81, Line 108~124: trim the default string value when the type is a numeric type.
+ * <p>Line 87, Line 114~129: trim the default string value when the type is a numeric type.
  */
 @Immutable
 public class MySqlDefaultValueConverter {

File: flink-connector-postgres-cdc/src/main/java/com/ververica/cdc/connectors/postgres/table/PostgreSQLTableSource.java
Patch:
@@ -121,6 +121,8 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {
                         .setPhysicalRowType(physicalDataType)
                         .setMetadataConverters(metadataConverters)
                         .setResultTypeInfo(typeInfo)
+                        .setUserDefinedConverterFactory(
+                                PostgreSQLDeserializationConverterFactory.instance())
                         .setValueValidator(new PostgresValueValidator(schemaName, tableName))
                         .build();
         DebeziumSourceFunction<RowData> sourceFunction =

File: flink-connector-postgres-cdc/src/test/java/com/ververica/cdc/connectors/postgres/PostgresTestBase.java
Patch:
@@ -43,7 +43,7 @@
 import static org.junit.Assert.assertNotNull;
 
 /**
- * Basic class for testing PostgresSQL source, this contains a PostgreSQL container which enables
+ * Basic class for testing PostgreSQL source, this contains a PostgreSQL container which enables
  * binlog.
  */
 public abstract class PostgresTestBase extends AbstractTestBase {
@@ -96,7 +96,7 @@ protected void initializePostgresTable(String sqlFile) {
                                                         return m.matches() ? m.group(1) : x;
                                                     })
                                             .collect(Collectors.joining("\n"))
-                                            .split(";"))
+                                            .split(";\n"))
                             .collect(Collectors.toList());
             for (String stmt : statements) {
                 statement.execute(stmt);

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/split/MySqlSnapshotSplit.java
Patch:
@@ -38,7 +38,7 @@ public class MySqlSnapshotSplit extends MySqlSplit {
 
     @Nullable private final Object[] splitStart;
     @Nullable private final Object[] splitEnd;
-    /** The high watermark is not bull when the split read finished. */
+    /** The high watermark is not null when the split read finished. */
     @Nullable private final BinlogOffset highWatermark;
 
     @Nullable transient byte[] serializedFormCache;

File: flink-cdc-e2e-tests/src/test/java/com/ververica/cdc/connectors/tests/MySqlE2eITCase.java
Patch:
@@ -60,6 +60,7 @@ public void testMySqlCDC() throws Exception {
                         " 'password' = '" + MYSQL_TEST_PASSWORD + "',",
                         " 'database-name' = '" + mysqlInventoryDatabase.getDatabaseName() + "',",
                         " 'table-name' = 'products_source',",
+                        " 'server-time-zone' = 'UTC',",
                         " 'server-id' = '5800-5900',",
                         " 'scan.incremental.snapshot.chunk.size' = '4'",
                         ");",

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/config/MySqlSourceConfigFactory.java
Patch:
@@ -24,6 +24,7 @@
 
 import java.io.Serializable;
 import java.time.Duration;
+import java.time.ZoneId;
 import java.util.Arrays;
 import java.util.List;
 import java.util.Properties;
@@ -38,7 +39,6 @@
 import static com.ververica.cdc.connectors.mysql.source.config.MySqlSourceOptions.HEARTBEAT_INTERVAL;
 import static com.ververica.cdc.connectors.mysql.source.config.MySqlSourceOptions.SCAN_INCREMENTAL_SNAPSHOT_CHUNK_SIZE;
 import static com.ververica.cdc.connectors.mysql.source.config.MySqlSourceOptions.SCAN_SNAPSHOT_FETCH_SIZE;
-import static com.ververica.cdc.connectors.mysql.source.config.MySqlSourceOptions.SERVER_TIME_ZONE;
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
 /** A factory to construct {@link MySqlSourceConfig}. */
@@ -54,7 +54,7 @@ public class MySqlSourceConfigFactory implements Serializable {
     private ServerIdRange serverIdRange;
     private List<String> databaseList;
     private List<String> tableList;
-    private String serverTimeZone = SERVER_TIME_ZONE.defaultValue();
+    private String serverTimeZone = ZoneId.systemDefault().getId();
     private StartupOptions startupOptions = StartupOptions.initial();
     private int splitSize = SCAN_INCREMENTAL_SNAPSHOT_CHUNK_SIZE.defaultValue();
     private int splitMetaGroupSize = CHUNK_META_GROUP_SIZE.defaultValue();

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/LegacyMySqlSourceTest.java
Patch:
@@ -682,7 +682,7 @@ public void testConsumingEmptyTable() throws Exception {
             // Step-1: start the source from empty state
             // ---------------------------------------------------------------------------
             DebeziumSourceFunction<SourceRecord> source =
-                    basicSourceBuilder(database, useLegacyImplementation)
+                    basicSourceBuilder(database, "UTC", useLegacyImplementation)
                             .tableList(database.getDatabaseName() + "." + "category")
                             .build();
             // we use blocking context to block the source to emit before last snapshot record
@@ -1038,13 +1038,13 @@ private void assertHistoryState(TestingListState<String> historyState) {
 
     private DebeziumSourceFunction<SourceRecord> createMySqlBinlogSource(
             String offsetFile, int offsetPos) {
-        return basicSourceBuilder(database, useLegacyImplementation)
+        return basicSourceBuilder(database, "UTC", useLegacyImplementation)
                 .startupOptions(StartupOptions.specificOffset(offsetFile, offsetPos))
                 .build();
     }
 
     private DebeziumSourceFunction<SourceRecord> createMySqlBinlogSource() {
-        return basicSourceBuilder(database, useLegacyImplementation).build();
+        return basicSourceBuilder(database, "UTC", useLegacyImplementation).build();
     }
 
     private boolean waitForCheckpointLock(Object checkpointLock, Duration timeout)

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/MySqlTestUtils.java
Patch:
@@ -48,7 +48,7 @@
 public class MySqlTestUtils {
 
     public static MySqlSource.Builder<SourceRecord> basicSourceBuilder(
-            UniqueDatabase database, boolean useLegacyImplementation) {
+            UniqueDatabase database, String serverTimezone, boolean useLegacyImplementation) {
         Properties debeziumProps = createDebeziumProperties(useLegacyImplementation);
         return MySqlSource.<SourceRecord>builder()
                 .hostname(database.getHost())
@@ -59,6 +59,7 @@ public static MySqlSource.Builder<SourceRecord> basicSourceBuilder(
                 .username(database.getUsername())
                 .password(database.getPassword())
                 .deserializer(new ForwardDeserializeSchema())
+                .serverTimeZone(serverTimezone)
                 .debeziumProperties(debeziumProps);
     }
 

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/source/MySqlSourceITCase.java
Patch:
@@ -302,6 +302,7 @@ private void testMySqlParallelSource(
                                 + " 'database-name' = '%s',"
                                 + " 'table-name' = '%s',"
                                 + " 'scan.incremental.snapshot.chunk.size' = '100',"
+                                + " 'server-time-zone' = 'UTC',"
                                 + " 'server-id' = '%s'"
                                 + ")",
                         MYSQL_CONTAINER.getHost(),
@@ -535,6 +536,7 @@ private String getCreateTableStatement(String... captureTableNames) {
                         + " 'database-name' = '%s',"
                         + " 'table-name' = '%s',"
                         + " 'scan.incremental.snapshot.chunk.size' = '2',"
+                        + " 'server-time-zone' = 'UTC',"
                         + " 'server-id' = '%s',"
                         + " 'scan.newly-added-table.enabled' = 'true'"
                         + ")",

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/table/MySqlCompatibilityITCase.java
Patch:
@@ -154,6 +154,7 @@ private void testDifferentMySqlVersion(MySqlVersion version, boolean enableGtid)
                                 + " 'password' = '%s',"
                                 + " 'database-name' = '%s',"
                                 + " 'table-name' = '%s',"
+                                + " 'server-time-zone' = 'UTC',"
                                 + " 'server-id' = '%s'"
                                 + ")",
                         mySqlContainer.getHost(),

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/table/MysqlConnectorCharsetITCase.java
Patch:
@@ -357,6 +357,7 @@ public void testCharset() throws Exception {
                                 + " 'table-name' = '%s',"
                                 + " 'scan.incremental.snapshot.enabled' = '%s',"
                                 + " 'server-id' = '%s',"
+                                + " 'server-time-zone' = 'UTC',"
                                 + " 'scan.incremental.snapshot.chunk.size' = '%s'"
                                 + ")",
                         testName,

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/polardbx/PolardbxCharsetITCase.java
Patch:
@@ -163,6 +163,7 @@ public void testCharset() throws Exception {
                                 + " 'table-name' = '%s',"
                                 + " 'scan.incremental.snapshot.enabled' = '%s',"
                                 + " 'server-id' = '%s',"
+                                + " 'server-time-zone' = 'UTC',"
                                 + " 'scan.incremental.snapshot.chunk.size' = '%s'"
                                 + ")",
                         testName,

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/polardbx/PolardbxSourceITCase.java
Patch:
@@ -79,6 +79,7 @@ public void testSingleKey() throws Exception {
                                 + " 'database-name' = '%s',"
                                 + " 'table-name' = '%s',"
                                 + " 'scan.incremental.snapshot.chunk.size' = '100',"
+                                + " 'server-time-zone' = 'UTC',"
                                 + " 'server-id' = '%s'"
                                 + ")",
                         HOST_NAME,
@@ -227,6 +228,7 @@ public void testFullTypesDdl() {
                                 + " 'database-name' = '%s',"
                                 + " 'table-name' = '%s',"
                                 + " 'scan.incremental.snapshot.chunk.size' = '100',"
+                                + " 'server-time-zone' = 'UTC',"
                                 + " 'server-id' = '%s'"
                                 + ")",
                         HOST_NAME,
@@ -292,6 +294,7 @@ public void testMultiKeys() throws Exception {
                                 + " 'database-name' = '%s',"
                                 + " 'table-name' = '%s',"
                                 + " 'scan.incremental.snapshot.chunk.size' = '100',"
+                                + " 'server-time-zone' = 'UTC',"
                                 + " 'server-id' = '%s'"
                                 + ")",
                         HOST_NAME,

File: flink-connector-oceanbase-cdc/src/main/java/com/ververica/cdc/connectors/oceanbase/OceanBaseSource.java
Patch:
@@ -200,7 +200,7 @@ public SourceFunction<T> build() {
             }
 
             if (serverTimeZone == null) {
-                serverTimeZone = "UTC";
+                serverTimeZone = "+00:00";
             }
             ZoneOffset zoneOffset = ZoneId.of(serverTimeZone).getRules().getOffset(Instant.now());
 
@@ -235,7 +235,7 @@ public SourceFunction<T> build() {
             obReaderConfig.setUsername(username);
             obReaderConfig.setPassword(password);
             obReaderConfig.setStartTimestamp(startupTimestamp);
-            obReaderConfig.setTimezone(zoneOffset.getId());
+            obReaderConfig.setTimezone(serverTimeZone);
 
             return new OceanBaseRichSourceFunction<T>(
                     StartupMode.INITIAL.equals(startupMode),

File: flink-connector-oceanbase-cdc/src/main/java/com/ververica/cdc/connectors/oceanbase/table/OceanBaseTableSourceFactory.java
Patch:
@@ -84,7 +84,7 @@ public class OceanBaseTableSourceFactory implements DynamicTableSourceFactory {
     public static final ConfigOption<String> SERVER_TIME_ZONE =
             ConfigOptions.key("server-time-zone")
                     .stringType()
-                    .defaultValue("UTC")
+                    .defaultValue("+00:00")
                     .withDescription("The session time zone in database server.");
 
     public static final ConfigOption<Duration> CONNECT_TIMEOUT =

File: flink-connector-oceanbase-cdc/src/test/java/com/ververica/cdc/connectors/oceanbase/table/OceanBaseTableFactoryTest.java
Patch:
@@ -78,7 +78,7 @@ public class OceanBaseTableFactoryTest {
     private static final String DATABASE_NAME = "db[0-9]";
     private static final String TABLE_NAME = "table[0-9]";
     private static final String TABLE_LIST = "db.table";
-    private static final String SERVER_TIME_ZONE = "UTC";
+    private static final String SERVER_TIME_ZONE = "+00:00";
     private static final String CONNECT_TIMEOUT = "30s";
     private static final String HOSTNAME = "127.0.0.1";
     private static final Integer PORT = 2881;

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/table/MySqlConnectorITCase.java
Patch:
@@ -854,7 +854,9 @@ public void testMetadataColumns() throws Exception {
                                 "+I[%s, user_table_1_1, 300, user_300, Hangzhou, 123567891234, user_300@foo.com, null]",
                                 "+U[%s, user_table_1_1, 300, user_300, Beijing, 123567891234, user_300@foo.com, null]",
                                 "+U[%s, user_table_1_2, 121, user_121, Shanghai, 88888888, null, null]",
-                                "-D[%s, user_table_1_1, 111, user_111, Shanghai, 123567891234, user_111@foo.com, null]")
+                                "-D[%s, user_table_1_1, 111, user_111, Shanghai, 123567891234, user_111@foo.com, null]",
+                                "-U[%s, user_table_1_1, 300, user_300, Hangzhou, 123567891234, user_300@foo.com, null]",
+                                "-U[%s, user_table_1_2, 121, user_121, Shanghai, 123567891234, null, null]")
                         .map(s -> String.format(s, userDatabase1.getDatabaseName()))
                         .sorted()
                         .collect(Collectors.toList());

File: flink-connector-oracle-cdc/src/test/java/com/ververica/cdc/connectors/oracle/table/OracleConnectorITCase.java
Patch:
@@ -170,7 +170,8 @@ public void testMetadataColumns() throws Throwable {
                                 + " ID INT NOT NULL,"
                                 + " NAME STRING,"
                                 + " DESCRIPTION STRING,"
-                                + " WEIGHT DECIMAL(10,3)"
+                                + " WEIGHT DECIMAL(10,3),"
+                                + " PRIMARY KEY (ID) NOT ENFORCED"
                                 + ") WITH ("
                                 + " 'connector' = 'oracle-cdc',"
                                 + " 'hostname' = '%s',"
@@ -249,6 +250,7 @@ public void testMetadataColumns() throws Throwable {
                         "-D[XE, DEBEZIUM, PRODUCTS, 112, scooter, Big 2-wheel scooter , 5.170]");
 
         List<String> actual = TestValuesTableFactory.getRawResults("sink");
+        Collections.sort(expected);
         Collections.sort(actual);
         assertEquals(expected, actual);
         result.getJobClient().get().cancel().get();

File: flink-connector-postgres-cdc/src/test/java/com/ververica/cdc/connectors/postgres/table/PostgreSQLConnectorITCase.java
Patch:
@@ -348,7 +348,8 @@ public void testMetadataColumns() throws Throwable {
                                 + " id INT NOT NULL,"
                                 + " name STRING,"
                                 + " description STRING,"
-                                + " weight DECIMAL(10,3)"
+                                + " weight DECIMAL(10,3),"
+                                + " PRIMARY KEY (id) NOT ENFORCED"
                                 + ") WITH ("
                                 + " 'connector' = 'postgres-cdc',"
                                 + " 'hostname' = '%s',"
@@ -431,6 +432,7 @@ public void testMetadataColumns() throws Throwable {
                         "-D(postgres,inventory,products,111,scooter,Big 2-wheel scooter ,5.170)");
         List<String> actual = TestValuesTableFactory.getRawResults("sink");
         Collections.sort(actual);
+        Collections.sort(expected);
         assertEquals(expected, actual);
         result.getJobClient().get().cancel().get();
     }

File: flink-connector-sqlserver-cdc/src/test/java/com/ververica/cdc/connectors/sqlserver/table/SqlServerConnectorITCase.java
Patch:
@@ -285,7 +285,8 @@ public void testMetadataColumns() throws Throwable {
                                 + " id INT NOT NULL,"
                                 + " name STRING,"
                                 + " description STRING,"
-                                + " weight DECIMAL(10,3)"
+                                + " weight DECIMAL(10,3),"
+                                + " PRIMARY KEY (id) NOT ENFORCED"
                                 + ") WITH ("
                                 + " 'connector' = 'sqlserver-cdc',"
                                 + " 'hostname' = '%s',"
@@ -366,6 +367,7 @@ public void testMetadataColumns() throws Throwable {
                         "-D(inventory,dbo,products,111,scooter,Big 2-wheel scooter ,5.170)");
         List<String> actual = TestValuesTableFactory.getRawResults("sink");
         Collections.sort(actual);
+        Collections.sort(expected);
         assertEquals(expected, actual);
         result.getJobClient().get().cancel().get();
     }

File: flink-connector-tidb-cdc/src/test/java/com/ververica/cdc/connectors/tidb/table/TiDBConnectorITCase.java
Patch:
@@ -392,7 +392,8 @@ public void testMetadataColumns() throws Exception {
                         "+I(inventory,products,107,rocks,box of assorted rocks,5.3000000000)",
                         "+I(inventory,products,108,jacket,water resistent black wind breaker,0.1000000000)",
                         "+I(inventory,products,109,spare tire,24 inch spare tire,22.2000000000)",
-                        "+U(inventory,products,106,hammer,18oz carpenter hammer,1.0000000000)");
+                        "+U(inventory,products,106,hammer,18oz carpenter hammer,1.0000000000)",
+                        "-U(inventory,products,106,hammer,16oz carpenter's hammer,1.0000000000)");
         List<String> actual = TestValuesTableFactory.getRawResults("sink");
         assertEqualsInAnyOrder(expected, actual);
         result.getJobClient().get().cancel().get();

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/source/assigners/MySqlSnapshotSplitAssignerTest.java
Patch:
@@ -266,7 +266,7 @@ public void testAssignTableWithSparseDistributionSplitKey() {
                 getTestAssignSnapshotSplits(
                         8,
                         10d,
-                        SPLIT_KEY_EVEN_DISTRIBUTION_FACTOR_LOWER_BOUND.defaultValue(),
+                        CHUNK_KEY_EVEN_DISTRIBUTION_FACTOR_LOWER_BOUND.defaultValue(),
                         new String[] {
                             customerDatabase.getDatabaseName() + ".customers_sparse_dist"
                         });

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/reader/external/JdbcSourceScanFetcher.java
Patch:
@@ -111,6 +111,7 @@ public Iterator<SourceRecord> pollSplitRecords() throws InterruptedException {
             boolean reachBinlogEnd = false;
             final List<SourceRecord> sourceRecords = new ArrayList<>();
             while (!reachBinlogEnd) {
+                checkReadException();
                 List<DataChangeEvent> batch = queue.poll();
                 for (DataChangeEvent event : batch) {
                     sourceRecords.add(event.getRecord());

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/reader/external/JdbcSourceStreamFetcher.java
Patch:
@@ -77,6 +77,7 @@ public JdbcSourceStreamFetcher(JdbcSourceFetchTaskContext taskContext, int subTa
     public void submitTask(FetchTask<SourceSplitBase> fetchTask) {
         this.streamFetchTask = fetchTask;
         this.currentStreamSplit = fetchTask.getSplit().asStreamSplit();
+        configureFilter();
         taskContext.configure(currentStreamSplit);
         this.queue = taskContext.getQueue();
         executor.submit(

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/metrics/SourceReaderMetricConstants.java
Patch:
@@ -17,7 +17,7 @@
 package com.ververica.cdc.connectors.base.source.metrics;
 
 /** A collection of Source Reader metrics related constant strings. */
-public class SoureReaderMetricConstants {
+public class SourceReaderMetricConstants {
 
     public static final String SOURCE_IDLE_TIME = "sourceIdleTime";
     public static final String CURRENT_FETCH_EVENT_TIME_LAG = "currentFetchEventTimeLag";

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/metrics/SourceReaderMetrics.java
Patch:
@@ -51,13 +51,13 @@ public SourceReaderMetrics(MetricGroup metricGroup) {
 
     public void registerMetrics() {
         metricGroup.gauge(
-                SoureReaderMetricConstants.CURRENT_FETCH_EVENT_TIME_LAG,
+                SourceReaderMetricConstants.CURRENT_FETCH_EVENT_TIME_LAG,
                 (Gauge<Long>) this::getFetchDelay);
         metricGroup.gauge(
-                SoureReaderMetricConstants.CURRENT_EMIT_EVENT_TIME_LAG,
+                SourceReaderMetricConstants.CURRENT_EMIT_EVENT_TIME_LAG,
                 (Gauge<Long>) this::getEmitDelay);
         metricGroup.gauge(
-                SoureReaderMetricConstants.SOURCE_IDLE_TIME, (Gauge<Long>) this::getIdleTime);
+                SourceReaderMetricConstants.SOURCE_IDLE_TIME, (Gauge<Long>) this::getIdleTime);
     }
 
     public long getFetchDelay() {

File: flink-connector-oceanbase-cdc/src/main/java/com/ververica/cdc/connectors/oceanbase/source/OceanBaseTableSchema.java
Patch:
@@ -75,7 +75,6 @@ public static TableSchema getTableSchema(
         for (int i = 0; i < columnNames.length; i++) {
             tableEditor.addColumn(getColumn(columnNames[i], jdbcTypes[i]));
         }
-        // TODO add column filter and mapper
         return tableSchemaBuilder(zoneOffset)
                 .create(
                         null,

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/config/JdbcSourceConfigFactory.java
Patch:
@@ -122,8 +122,8 @@ public JdbcSourceConfigFactory splitSize(int splitSize) {
     }
 
     /**
-     * The group size of split meta, if the meta size exceeds the group size, the meta will be will
-     * be divided into multiple groups.
+     * The group size of split meta, if the meta size exceeds the group size, the meta will be
+     * divided into multiple groups.
      */
     public JdbcSourceConfigFactory splitMetaGroupSize(int splitMetaGroupSize) {
         this.splitMetaGroupSize = splitMetaGroupSize;

File: flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/options/SourceOptions.java
Patch:
@@ -88,7 +88,7 @@ public class SourceOptions {
                     .intType()
                     .defaultValue(1000)
                     .withDescription(
-                            "The group size of chunk meta, if the meta size exceeds the group size, the meta will be will be divided into multiple groups.");
+                            "The group size of chunk meta, if the meta size exceeds the group size, the meta will be divided into multiple groups.");
 
     public static final ConfigOption<Double> SPLIT_KEY_EVEN_DISTRIBUTION_FACTOR_UPPER_BOUND =
             ConfigOptions.key("split-key.even-distribution.factor.upper-bound")

File: flink-cdc-base/src/test/java/com/ververica/cdc/connectors/base/experimental/MySqlSourceBuilder.java
Patch:
@@ -135,8 +135,8 @@ public MySqlSourceBuilder<T> splitSize(int splitSize) {
     }
 
     /**
-     * The group size of split meta, if the meta size exceeds the group size, the meta will be will
-     * be divided into multiple groups.
+     * The group size of split meta, if the meta size exceeds the group size, the meta will be
+     * divided into multiple groups.
      */
     public MySqlSourceBuilder<T> splitMetaGroupSize(int splitMetaGroupSize) {
         this.configFactory.splitMetaGroupSize(splitMetaGroupSize);

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/MySqlSourceBuilder.java
Patch:
@@ -131,8 +131,8 @@ public MySqlSourceBuilder<T> splitSize(int splitSize) {
     }
 
     /**
-     * The group size of split meta, if the meta size exceeds the group size, the meta will be will
-     * be divided into multiple groups.
+     * The group size of split meta, if the meta size exceeds the group size, the meta will be
+     * divided into multiple groups.
      */
     public MySqlSourceBuilder<T> splitMetaGroupSize(int splitMetaGroupSize) {
         this.configFactory.splitMetaGroupSize(splitMetaGroupSize);

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/config/MySqlSourceConfigFactory.java
Patch:
@@ -152,8 +152,8 @@ public MySqlSourceConfigFactory splitSize(int splitSize) {
     }
 
     /**
-     * The group size of split meta, if the meta size exceeds the group size, the meta will be will
-     * be divided into multiple groups.
+     * The group size of split meta, if the meta size exceeds the group size, the meta will be
+     * divided into multiple groups.
      */
     public MySqlSourceConfigFactory splitMetaGroupSize(int splitMetaGroupSize) {
         this.splitMetaGroupSize = splitMetaGroupSize;

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/config/MySqlSourceOptions.java
Patch:
@@ -180,7 +180,7 @@ public class MySqlSourceOptions {
                     .intType()
                     .defaultValue(1000)
                     .withDescription(
-                            "The group size of chunk meta, if the meta size exceeds the group size, the meta will be will be divided into multiple groups.");
+                            "The group size of chunk meta, if the meta size exceeds the group size, the meta will be divided into multiple groups.");
 
     @Experimental
     public static final ConfigOption<Double> SPLIT_KEY_EVEN_DISTRIBUTION_FACTOR_UPPER_BOUND =

File: flink-connector-oceanbase-cdc/src/main/java/com/ververica/cdc/connectors/oceanbase/source/OceanBaseJdbcConverter.java
Patch:
@@ -134,7 +134,7 @@ public static Object getField(
             case Types.BINARY:
                 return ByteBuffer.wrap(value.toString().getBytes(StandardCharsets.UTF_8));
             default:
-                return value.toString();
+                return value.toString(StandardCharsets.UTF_8.toString());
         }
     }
 

File: flink-connector-tidb-cdc/src/main/java/com/ververica/cdc/connectors/tidb/table/RowDataTiKVChangeEventDeserializationSchema.java
Patch:
@@ -81,7 +81,7 @@ public void deserialize(Row row, Collector<RowData> out) throws Exception {
                                     row.getValue().toByteArray(),
                                     RowKey.decode(row.getKey().toByteArray()).getHandle(),
                                     tableInfo);
-                    if (row.getOldValue() == null) {
+                    if (row.getOldValue() == null || row.getOldValue().isEmpty()) {
                         RowData rowDataUpdateBefore =
                                 (RowData) physicalConverter.convert(tikvValues, tableInfo, null);
                         rowDataUpdateBefore.setRowKind(RowKind.INSERT);

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/debezium/task/context/StatefulTaskContext.java
Patch:
@@ -31,7 +31,6 @@
 import io.debezium.connector.mysql.MySqlConnection;
 import io.debezium.connector.mysql.MySqlConnectorConfig;
 import io.debezium.connector.mysql.MySqlDatabaseSchema;
-import io.debezium.connector.mysql.MySqlErrorHandler;
 import io.debezium.connector.mysql.MySqlOffsetContext;
 import io.debezium.connector.mysql.MySqlStreamingChangeEventSourceMetrics;
 import io.debezium.connector.mysql.MySqlTopicSelector;
@@ -153,7 +152,8 @@ public void configure(MySqlSplit mySqlSplit) {
         this.streamingChangeEventSourceMetrics =
                 changeEventSourceMetricsFactory.getStreamingMetrics(
                         taskContext, queue, metadataProvider);
-        this.errorHandler = new MySqlErrorHandler(connectorConfig.getLogicalName(), queue);
+        this.errorHandler =
+                new MySqlErrorHandler(connectorConfig.getLogicalName(), queue, taskContext);
     }
 
     private void validateAndLoadDatabaseHistory(

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/polardbx/PolardbxSourceTCase.java
Patch:
@@ -356,7 +356,6 @@ public void testFullTypesDdl() {
         TableResult tableResult = tEnv.executeSql("select * from polardbx_full_types");
         CloseableIterator<Row> iterator = tableResult.collect();
         List<String> realSnapshotData = fetchRows(iterator, 1);
-        realSnapshotData.forEach(System.out::println);
         String[] expectedSnapshotData =
                 new String[] {
                     "+I[100001, 127, 255, 32767, 65535, 8388607, 16777215, 2147483647, 4294967295, 2147483647, "

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/reader/MySqlSourceReader.java
Patch:
@@ -141,7 +141,7 @@ public List<MySqlSplit> snapshotState(long checkpointId) {
         if (suspendedBinlogSplit != null) {
             unfinishedSplits.add(suspendedBinlogSplit);
         }
-        return stateSplits;
+        return unfinishedSplits;
     }
 
     @Override

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/reader/MySqlRecordEmitter.java
Patch:
@@ -89,6 +89,8 @@ public void emitRecord(SourceRecord element, SourceOutput<T> output, MySqlSplitS
                 splitState.asBinlogSplitState().recordSchema(tableChange.getId(), tableChange);
             }
             if (includeSchemaChanges) {
+                BinlogOffset position = getBinlogPosition(element);
+                splitState.asBinlogSplitState().setStartingOffset(position);
                 emitElement(element, output);
             }
         } else if (isDataChangeRecord(element)) {

File: flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/table/MongoDBTableSource.java
Patch:
@@ -135,8 +135,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {
         RowType physicalDataType =
                 (RowType) physicalSchema.toPhysicalRowDataType().getLogicalType();
         MetadataConverter[] metadataConverters = getMetadataConverters();
-        TypeInformation<RowData> typeInfo =
-                scanContext.createTypeInformation(physicalSchema.toPhysicalRowDataType());
+        TypeInformation<RowData> typeInfo = scanContext.createTypeInformation(producedDataType);
 
         DebeziumDeserializationSchema<RowData> deserializer =
                 new MongoDBConnectorDeserializationSchema(

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/table/MySqlCompatibilityITCase.java
Patch:
@@ -125,7 +125,7 @@ private void testDifferentMySqlVersion(MySqlVersion version, boolean enableGtid)
                 (MySqlContainer)
                         new MySqlContainer(version)
                                 .withConfigurationOverride(
-                                        buildMySqlConfigWithTimezone(version, enableGtid))
+                                        buildCustomMySqlConfig(version, enableGtid))
                                 .withSetupSQL("docker/setup.sql")
                                 .withDatabaseName("flink-test")
                                 .withUsername("flinkuser")
@@ -239,7 +239,7 @@ private static List<String> fetchRows(Iterator<Row> iter, int size) {
         return rows;
     }
 
-    private String buildMySqlConfigWithTimezone(MySqlVersion version, boolean enableGtid) {
+    private String buildCustomMySqlConfig(MySqlVersion version, boolean enableGtid) {
         try {
             File folder = tempFolder.newFolder(String.valueOf(UUID.randomUUID()));
             Path cnf = Files.createFile(Paths.get(folder.getPath(), "my.cnf"));

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/debezium/reader/SnapshotSplitReader.java
Patch:
@@ -124,7 +124,7 @@ public void submitSplit(MySqlSplit mySqlSplit) {
                                         .getEndingOffset()
                                         .isAfter(backfillBinlogSplit.getStartingOffset());
                         if (!binlogBackfillRequired) {
-                            dispatchHighWatermark(backfillBinlogSplit);
+                            dispatchBinlogEndEvent(backfillBinlogSplit);
                             currentTaskRunning = false;
                             return;
                         }
@@ -196,7 +196,7 @@ private MySqlBinlogSplitReadTask createBackfillBinlogReadTask(
                 backfillBinlogSplit);
     }
 
-    private void dispatchHighWatermark(MySqlBinlogSplit backFillBinlogSplit)
+    private void dispatchBinlogEndEvent(MySqlBinlogSplit backFillBinlogSplit)
             throws InterruptedException {
         final SignalEventDispatcher signalEventDispatcher =
                 new SignalEventDispatcher(

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/MySqlSource.java
Patch:
@@ -70,7 +70,7 @@
  * <pre>
  *     1. The source supports parallel capturing table change.
  *     2. The source supports checkpoint in split level when read snapshot data.
- *     3. The source does need apply any lock of MySQL.
+ *     3. The source doesn't need apply any lock of MySQL.
  * </pre>
  *
  * <pre>{@code

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/table/OracleTableSource.java
Patch:
@@ -121,6 +121,8 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {
                         .setPhysicalRowType(physicalDataType)
                         .setMetadataConverters(metadataConverters)
                         .setResultTypeInfo(typeInfo)
+                        .setUserDefinedConverterFactory(
+                                OracleDeserializationConverterFactory.instance())
                         .build();
         OracleSource.Builder<RowData> builder =
                 OracleSource.<RowData>builder()

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/reader/MySqlRecordEmitter.java
Patch:
@@ -118,7 +118,7 @@ private void reportMetrics(SourceRecord element) {
         if (messageTimestamp != null && messageTimestamp > 0L) {
             // report fetch delay
             Long fetchTimestamp = getFetchTimestamp(element);
-            if (fetchTimestamp != null) {
+            if (fetchTimestamp != null && fetchTimestamp >= messageTimestamp) {
                 sourceReaderMetrics.recordFetchDelay(fetchTimestamp - messageTimestamp);
             }
             // report emit delay

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/connection/PooledDataSourceFactory.java
Patch:
@@ -30,6 +30,7 @@ public class PooledDataSourceFactory {
             "jdbc:mysql://%s:%s/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL";
     public static final String CONNECTION_POOL_PREFIX = "connection-pool-";
     public static final String SERVER_TIMEZONE_KEY = "serverTimezone";
+    public static final int MINIMUM_POOL_SIZE = 1;
 
     private PooledDataSourceFactory() {}
 
@@ -43,6 +44,7 @@ public static HikariDataSource createPooledDataSource(MySqlSourceConfig sourceCo
         config.setJdbcUrl(String.format(JDBC_URL_PATTERN, hostName, port));
         config.setUsername(sourceConfig.getUsername());
         config.setPassword(sourceConfig.getPassword());
+        config.setMinimumIdle(MINIMUM_POOL_SIZE);
         config.setMaximumPoolSize(sourceConfig.getConnectionPoolSize());
         config.setConnectionTimeout(sourceConfig.getConnectTimeout().toMillis());
         config.addDataSourceProperty(SERVER_TIMEZONE_KEY, sourceConfig.getServerTimeZone());

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/MySqlSourceBuilder.java
Patch:
@@ -35,7 +35,7 @@
  *
  * <pre>{@code
  * MySqlSource
- *     .<RowData>builder()
+ *     .<String>builder()
  *     .hostname("localhost")
  *     .port(3306)
  *     .databaseList("mydb")

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/assigners/ChunkSplitter.java
Patch:
@@ -39,7 +39,6 @@
 
 import java.math.BigDecimal;
 import java.sql.SQLException;
-import java.time.Duration;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
@@ -109,7 +108,7 @@ public Collection<MySqlSnapshotSplit> generateSplits(TableId tableId) {
                     "Split table {} into {} chunks, time cost: {}ms.",
                     tableId,
                     splits.size(),
-                    Duration.ofMillis(end - start));
+                    end - start);
             return splits;
         } catch (Exception e) {
             throw new FlinkRuntimeException(

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/connection/PooledDataSourceFactory.java
Patch:
@@ -21,6 +21,7 @@
 import com.ververica.cdc.connectors.mysql.source.config.MySqlSourceConfig;
 import com.zaxxer.hikari.HikariConfig;
 import com.zaxxer.hikari.HikariDataSource;
+import io.debezium.connector.mysql.MySqlConnectorConfig;
 
 /** A connection pool factory to create pooled DataSource {@link HikariDataSource}. */
 public class PooledDataSourceFactory {
@@ -45,6 +46,8 @@ public static HikariDataSource createPooledDataSource(MySqlSourceConfig sourceCo
         config.setMaximumPoolSize(sourceConfig.getConnectionPoolSize());
         config.setConnectionTimeout(sourceConfig.getConnectTimeout().toMillis());
         config.addDataSourceProperty(SERVER_TIMEZONE_KEY, sourceConfig.getServerTimeZone());
+        config.setDriverClassName(
+                sourceConfig.getDbzConfiguration().getString(MySqlConnectorConfig.JDBC_DRIVER));
 
         // optional optimization configurations for pooled DataSource
         config.addDataSourceProperty("cachePrepStmts", "true");

File: flink-connector-debezium/src/main/java/com/ververica/cdc/debezium/table/RowDataDebeziumDeserializeSchema.java
Patch:
@@ -605,7 +605,7 @@ public Object convert(Object dbzObj, Schema schema) throws Exception {
                     if (field == null) {
                         row.setField(i, null);
                     } else {
-                        Object fieldValue = struct.get(field);
+                        Object fieldValue = struct.getWithoutDefault(fieldName);
                         Schema fieldSchema = schema.field(fieldName).schema();
                         Object convertedField =
                                 convertField(fieldConverters[i], fieldValue, fieldSchema);

File: flink-connector-mongodb-cdc/src/test/java/com/ververica/cdc/connectors/mongodb/table/MongoDBConnectorITCase.java
Patch:
@@ -102,8 +102,7 @@ public void testConsumingAllEvents() throws ExecutionException, InterruptedExcep
                         + " PRIMARY KEY (name) NOT ENFORCED"
                         + ") WITH ("
                         + " 'connector' = 'values',"
-                        + " 'sink-insert-only' = 'false',"
-                        + " 'sink-expected-messages-num' = '20'"
+                        + " 'sink-insert-only' = 'false'"
                         + ")";
         tEnv.executeSql(sourceDDL);
         tEnv.executeSql(sinkDDL);

File: flink-connector-mongodb-cdc/src/test/java/com/ververica/cdc/connectors/mongodb/table/MongoDBConnectorITCase.java
Patch:
@@ -86,7 +86,8 @@ public void testConsumingAllEvents() throws ExecutionException, InterruptedExcep
                                 + " 'username' = '%s',"
                                 + " 'password' = '%s',"
                                 + " 'database' = '%s',"
-                                + " 'collection' = '%s'"
+                                + " 'collection' = '%s',"
+                                + " 'heartbeat.interval.ms' = '1000'"
                                 + ")",
                         MONGODB_CONTAINER.getHostAndPort(),
                         FLINK_USER,

File: flink-connector-mongodb-cdc/src/test/java/com/ververica/cdc/connectors/mongodb/MongoDBTestBase.java
Patch:
@@ -52,7 +52,7 @@ public class MongoDBTestBase extends AbstractTestBase {
     private static final Pattern COMMENT_PATTERN = Pattern.compile("^(.*)//.*$");
 
     protected static final String FLINK_USER = "flinkuser";
-    protected static final String FLINK_USER_PASSWORD = "flinkpw";
+    protected static final String FLINK_USER_PASSWORD = "a1?~!@#$%^&*(){}[]<>.,+_-=/|:;";
 
     protected static final String MONGO_SUPER_USER = "superuser";
     protected static final String MONGO_SUPER_PASSWORD = "superpw";

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/reader/MySqlRecordEmitter.java
Patch:
@@ -26,9 +26,9 @@
 import com.ververica.cdc.connectors.mysql.source.offset.BinlogOffset;
 import com.ververica.cdc.connectors.mysql.source.split.MySqlSplitState;
 import com.ververica.cdc.debezium.DebeziumDeserializationSchema;
+import com.ververica.cdc.debezium.history.FlinkJsonTableChangeSerializer;
 import io.debezium.document.Array;
 import io.debezium.relational.history.HistoryRecord;
-import io.debezium.relational.history.JsonTableChangeSerializer;
 import io.debezium.relational.history.TableChanges;
 import org.apache.kafka.connect.source.SourceRecord;
 import org.slf4j.Logger;
@@ -54,8 +54,8 @@ public final class MySqlRecordEmitter<T>
         implements RecordEmitter<SourceRecord, T, MySqlSplitState> {
 
     private static final Logger LOG = LoggerFactory.getLogger(MySqlRecordEmitter.class);
-    private static final JsonTableChangeSerializer TABLE_CHANGE_SERIALIZER =
-            new JsonTableChangeSerializer();
+    private static final FlinkJsonTableChangeSerializer TABLE_CHANGE_SERIALIZER =
+            new FlinkJsonTableChangeSerializer();
 
     private final DebeziumDeserializationSchema<T> debeziumDeserializationSchema;
     private final MySqlSourceReaderMetrics sourceReaderMetrics;

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/split/MySqlSplitSerializer.java
Patch:
@@ -25,11 +25,11 @@
 import org.apache.flink.table.types.logical.utils.LogicalTypeParser;
 
 import com.ververica.cdc.connectors.mysql.source.offset.BinlogOffset;
+import com.ververica.cdc.debezium.history.FlinkJsonTableChangeSerializer;
 import io.debezium.document.Document;
 import io.debezium.document.DocumentReader;
 import io.debezium.document.DocumentWriter;
 import io.debezium.relational.TableId;
-import io.debezium.relational.history.JsonTableChangeSerializer;
 import io.debezium.relational.history.TableChanges.TableChange;
 
 import java.io.IOException;
@@ -174,7 +174,7 @@ public MySqlSplit deserializeSplit(int version, byte[] serialized) throws IOExce
 
     private static void writeTableSchemas(
             Map<TableId, TableChange> tableSchemas, DataOutputSerializer out) throws IOException {
-        JsonTableChangeSerializer jsonSerializer = new JsonTableChangeSerializer();
+        FlinkJsonTableChangeSerializer jsonSerializer = new FlinkJsonTableChangeSerializer();
         DocumentWriter documentWriter = DocumentWriter.defaultWriter();
         final int size = tableSchemas.size();
         out.writeInt(size);
@@ -211,7 +211,7 @@ private static Map<TableId, TableChange> readTableSchemas(int version, DataInput
                     throw new IOException("Unknown version: " + version);
             }
             Document document = documentReader.read(tableChangeStr);
-            TableChange tableChange = JsonTableChangeSerializer.fromDocument(document, true);
+            TableChange tableChange = FlinkJsonTableChangeSerializer.fromDocument(document, true);
             tableSchemas.put(tableId, tableChange);
         }
         return tableSchemas;

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/table/MySqlTableSource.java
Patch:
@@ -147,6 +147,8 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {
                         .setMetadataConverters(metadataConverters)
                         .setResultTypeInfo(typeInfo)
                         .setServerTimeZone(serverTimeZone)
+                        .setUserDefinedConverterFactory(
+                                MySqlDeserializationConverterFactory.instance())
                         .build();
         if (enableParallelRead) {
             MySqlSource<RowData> parallelSource =

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/LegacyMySqlSourceTest.java
Patch:
@@ -29,14 +29,14 @@
 import com.ververica.cdc.connectors.mysql.testutils.UniqueDatabase;
 import com.ververica.cdc.connectors.utils.TestSourceContext;
 import com.ververica.cdc.debezium.DebeziumSourceFunction;
+import com.ververica.cdc.debezium.history.FlinkJsonTableChangeSerializer;
 import io.debezium.document.Document;
 import io.debezium.document.DocumentWriter;
 import io.debezium.relational.Column;
 import io.debezium.relational.Table;
 import io.debezium.relational.TableEditor;
 import io.debezium.relational.TableId;
 import io.debezium.relational.history.HistoryRecord;
-import io.debezium.relational.history.JsonTableChangeSerializer;
 import io.debezium.relational.history.TableChanges;
 import org.apache.kafka.connect.source.SourceRecord;
 import org.junit.Before;
@@ -767,7 +767,8 @@ public void testChooseDatabase() throws Exception {
         DocumentWriter writer = DocumentWriter.defaultWriter();
         if (useLegacyImplementation) {
             // build a non-legacy state
-            JsonTableChangeSerializer tableChangesSerializer = new JsonTableChangeSerializer();
+            FlinkJsonTableChangeSerializer tableChangesSerializer =
+                    new FlinkJsonTableChangeSerializer();
             historyState.add(
                     writer.write(
                             tableChangesSerializer.toDocument(

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/source/split/MySqlSplitSerializerTest.java
Patch:
@@ -22,10 +22,10 @@
 import org.apache.flink.table.types.logical.RowType;
 
 import com.ververica.cdc.connectors.mysql.source.offset.BinlogOffset;
+import com.ververica.cdc.debezium.history.FlinkJsonTableChangeSerializer;
 import io.debezium.document.Document;
 import io.debezium.document.DocumentReader;
 import io.debezium.relational.TableId;
-import io.debezium.relational.history.JsonTableChangeSerializer;
 import io.debezium.relational.history.TableChanges.TableChange;
 import org.junit.Test;
 
@@ -155,6 +155,6 @@ public static TableChange getTestTableSchema() throws Exception {
                         + "\"typeExpression\":\"VARCHAR\",\"charsetName\":\"latin1\",\"length\":1024,"
                         + "\"position\":4,\"optional\":true,\"autoIncremented\":false,\"generated\":false}]}}";
         final Document doc = DocumentReader.defaultReader().read(tableChangeJsonStr);
-        return JsonTableChangeSerializer.fromDocument(doc, true);
+        return FlinkJsonTableChangeSerializer.fromDocument(doc, true);
     }
 }

File: flink-connector-oracle-cdc/src/main/java/com/ververica/cdc/connectors/oracle/OracleSource.java
Patch:
@@ -42,7 +42,7 @@ public static <T> Builder<T> builder() {
     /** Builder class of {@link OracleSource}. */
     public static class Builder<T> {
 
-        private int port = 1521; // default 3306 port
+        private int port = 1521; // default 1521 port
         private String hostname;
         private String database;
         private String username;
@@ -77,8 +77,8 @@ public Builder<T> database(String database) {
         /**
          * An optional list of regular expressions that match fully-qualified table identifiers for
          * tables to be monitored; any table not included in the list will be excluded from
-         * monitoring. Each identifier is of the form schemaName.tableName. By default the
-         * connector will monitor every non-system table in each monitored database.
+         * monitoring. Each identifier is of the form schemaName.tableName. By default the connector
+         * will monitor every non-system table in each monitored database.
          */
         public Builder<T> tableList(String... tableList) {
             this.tableList = tableList;

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/MySqlSourceITCase.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.flink.util.CloseableIterator;
 
 import com.alibaba.fastjson.JSONObject;
-import com.ververica.cdc.connectors.mysql.source.utils.UniqueDatabase;
+import com.ververica.cdc.connectors.mysql.testutils.UniqueDatabase;
 import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema;
 import com.ververica.cdc.debezium.StringDebeziumDeserializationSchema;
 import org.junit.Ignore;

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/MySqlSourceTest.java
Patch:
@@ -25,8 +25,8 @@
 import com.fasterxml.jackson.core.JsonParseException;
 import com.jayway.jsonpath.JsonPath;
 import com.ververica.cdc.connectors.mysql.MySqlTestUtils.TestingListState;
-import com.ververica.cdc.connectors.mysql.source.utils.UniqueDatabase;
 import com.ververica.cdc.connectors.mysql.table.StartupOptions;
+import com.ververica.cdc.connectors.mysql.testutils.UniqueDatabase;
 import com.ververica.cdc.connectors.utils.TestSourceContext;
 import com.ververica.cdc.debezium.DebeziumSourceFunction;
 import io.debezium.document.Document;

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/MySqlTestUtils.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.flink.util.Collector;
 import org.apache.flink.util.Preconditions;
 
-import com.ververica.cdc.connectors.mysql.source.utils.UniqueDatabase;
+import com.ververica.cdc.connectors.mysql.testutils.UniqueDatabase;
 import com.ververica.cdc.connectors.utils.TestSourceContext;
 import com.ververica.cdc.debezium.DebeziumDeserializationSchema;
 import com.ververica.cdc.debezium.DebeziumSourceFunction;

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/MySqlValidatorTest.java
Patch:
@@ -23,8 +23,8 @@
 import org.apache.flink.table.api.ValidationException;
 
 import com.ververica.cdc.connectors.mysql.source.MySqlParallelSource;
-import com.ververica.cdc.connectors.mysql.source.utils.MySqlContainer;
-import com.ververica.cdc.connectors.mysql.source.utils.UniqueDatabase;
+import com.ververica.cdc.connectors.mysql.testutils.MySqlContainer;
+import com.ververica.cdc.connectors.mysql.testutils.UniqueDatabase;
 import com.ververica.cdc.debezium.DebeziumSourceFunction;
 import org.apache.kafka.connect.source.SourceRecord;
 import org.junit.AfterClass;
@@ -98,7 +98,7 @@ public void testValidateVersion() {
                 String.format(
                         "Currently Flink MySql CDC connector only supports MySql whose version is larger or equal to 5.7, but actual is %s.",
                         version);
-        doValidate(version, "docker/my.cnf", message);
+        doValidate(version, "docker/server/my.cnf", message);
     }
 
     @Test

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/source/split/MySqlSplitSerializerTest.java
Patch:
@@ -127,7 +127,7 @@ public void testRepeatedSerializationCache() throws Exception {
     private MySqlSplit serializeAndDeserializeSplit(MySqlSplit split) throws Exception {
         final MySqlSplitSerializer sqlSplitSerializer = new MySqlSplitSerializer();
         byte[] serialized = sqlSplitSerializer.serialize(split);
-        return sqlSplitSerializer.deserializeV1(serialized);
+        return sqlSplitSerializer.deserialize(sqlSplitSerializer.getVersion(), serialized);
     }
 
     public static TableChange getTestTableSchema() throws Exception {

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/table/MysqlTimezoneITCase.java
Patch:
@@ -26,8 +26,8 @@
 import org.apache.flink.util.CloseableIterator;
 
 import com.ververica.cdc.connectors.mysql.MySqlValidatorTest;
-import com.ververica.cdc.connectors.mysql.source.utils.MySqlContainer;
-import com.ververica.cdc.connectors.mysql.source.utils.UniqueDatabase;
+import com.ververica.cdc.connectors.mysql.testutils.MySqlContainer;
+import com.ververica.cdc.connectors.mysql.testutils.UniqueDatabase;
 import org.junit.Before;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/testutils/MySqlContainer.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.connectors.mysql.source.utils;
+package com.ververica.cdc.connectors.mysql.testutils;
 
 import org.testcontainers.containers.ContainerLaunchException;
 import org.testcontainers.containers.JdbcDatabaseContainer;

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/testutils/UniqueDatabase.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.ververica.cdc.connectors.mysql.source.utils;
+package com.ververica.cdc.connectors.mysql.testutils;
 
 import java.net.URL;
 import java.nio.file.Files;

File: flink-connector-mongodb-cdc/src/main/java/com/ververica/cdc/connectors/mongodb/table/MongoDBTableSourceFactory.java
Patch:
@@ -29,8 +29,6 @@
 import org.apache.flink.table.factories.FactoryUtil;
 import org.apache.flink.table.utils.TableSchemaUtils;
 
-import com.ververica.cdc.debezium.table.DebeziumOptions;
-
 import java.time.ZoneId;
 import java.util.HashSet;
 import java.util.Set;
@@ -177,7 +175,7 @@ public class MongoDBTableSourceFactory implements DynamicTableSourceFactory {
     public DynamicTableSource createDynamicTableSource(Context context) {
         final FactoryUtil.TableFactoryHelper helper =
                 FactoryUtil.createTableFactoryHelper(this, context);
-        helper.validateExcept(DebeziumOptions.DEBEZIUM_OPTIONS_PREFIX);
+        helper.validate();
 
         final ReadableConfig config = helper.getOptions();
 

File: flink-connector-mysql-cdc/src/test/java/com/ververica/cdc/connectors/mysql/source/MySqlParallelSourceTestBase.java
Patch:
@@ -130,7 +130,7 @@ protected void testMySqlParallelSource(
                                 + " 'password' = '%s',"
                                 + " 'database-name' = '%s',"
                                 + " 'table-name' = '%s',"
-                                + " 'scan.incremental.snapshot.chunk.size' = '1024',"
+                                + " 'scan.incremental.snapshot.chunk.size' = '100',"
                                 + " 'server-id' = '%s'"
                                 + ")",
                         MYSQL_CONTAINER.getHost(),

File: flink-connector-postgres-cdc/src/main/java/com/ververica/cdc/connectors/postgres/table/PostgreSQLTableFactory.java
Patch:
@@ -156,6 +156,7 @@ public Set<ConfigOption<?>> optionalOptions() {
         Set<ConfigOption<?>> options = new HashSet<>();
         options.add(PORT);
         options.add(DECODING_PLUGIN_NAME);
+        options.add(SLOT_NAME);
         return options;
     }
 }

File: flink-connector-postgres-cdc/src/test/java/com/ververica/cdc/connectors/postgres/table/PostgreSQLTableFactoryTest.java
Patch:
@@ -96,6 +96,7 @@ public void testOptionalProperties() {
         options.put("port", "5444");
         options.put("decoding.plugin.name", "wal2json");
         options.put("debezium.snapshot.mode", "never");
+        options.put("slot.name", "flink");
 
         DynamicTableSource actualSource = createTableSource(options);
         Properties dbzProperties = new Properties();

File: flink-connector-debezium/src/main/java/com/ververica/cdc/debezium/Validator.java
Patch:
@@ -18,8 +18,10 @@
 
 package com.ververica.cdc.debezium;
 
+import java.io.Serializable;
+
 /** Validator to validate the connected database satisfies the cdc connector's requirements. */
-public interface Validator {
+public interface Validator extends Serializable {
 
     void validate();
 

File: flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/MySqlValidator.java
Patch:
@@ -28,7 +28,6 @@
 import com.ververica.cdc.debezium.Validator;
 import io.debezium.connector.mysql.MySqlConnection;
 
-import java.io.Serializable;
 import java.sql.SQLException;
 import java.util.Arrays;
 import java.util.Properties;
@@ -37,7 +36,7 @@
  * The validator for MySql: it only cares about the version of the database is larger than or equal
  * to 5.7. It also requires the binlog format in the database is ROW and row image is FULL.
  */
-public class MySqlValidator implements Validator, Serializable {
+public class MySqlValidator implements Validator {
 
     private static final long serialVersionUID = 1L;
 

File: flink-format-changelog-json/src/main/java/com/ververica/cdc/formats/json/ChangelogJsonDeserializationSchema.java
Patch:
@@ -90,7 +90,7 @@ public void deserialize(byte[] bytes, Collector<RowData> out) throws IOException
             // a big try catch to protect the processing.
             if (!ignoreParseErrors) {
                 throw new IOException(
-                        format("Corrupt Debezium JSON message '%s'.", new String(bytes)), t);
+                        format("Corrupt Changelog JSON message '%s'.", new String(bytes)), t);
             }
         }
     }

File: flink-connector-mysql-cdc/src/main/java/com/alibaba/ververica/cdc/connectors/mysql/MySqlSource.java
Patch:
@@ -28,19 +28,19 @@
 import java.util.Map;
 import java.util.Properties;
 
-import static com.alibaba.ververica.cdc.connectors.mysql.source.MySQLSourceOptions.DATABASE_SERVER_NAME;
+import static com.alibaba.ververica.cdc.connectors.mysql.source.MySqlSourceOptions.DATABASE_SERVER_NAME;
 import static com.alibaba.ververica.cdc.debezium.DebeziumSourceFunction.LEGACY_IMPLEMENTATION_KEY;
 import static com.alibaba.ververica.cdc.debezium.DebeziumSourceFunction.LEGACY_IMPLEMENTATION_VALUE;
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
 /** A builder to build a SourceFunction which can read snapshot and continue to consume binlog. */
-public class MySQLSource {
+public class MySqlSource {
 
     public static <T> Builder<T> builder() {
         return new Builder<>();
     }
 
-    /** Builder class of {@link MySQLSource}. */
+    /** Builder class of {@link MySqlSource}. */
     public static class Builder<T> {
 
         private int port = 3306; // default 3306 port

File: flink-connector-mysql-cdc/src/main/java/com/alibaba/ververica/cdc/connectors/mysql/debezium/EmbeddedFlinkDatabaseHistory.java
Patch:
@@ -19,7 +19,7 @@
 package com.alibaba.ververica.cdc.connectors.mysql.debezium;
 
 import com.alibaba.ververica.cdc.connectors.mysql.debezium.task.context.StatefulTaskContext;
-import com.alibaba.ververica.cdc.connectors.mysql.source.split.MySQLSplitState;
+import com.alibaba.ververica.cdc.connectors.mysql.source.split.MySqlSplitState;
 import com.alibaba.ververica.cdc.debezium.internal.SchemaRecord;
 import io.debezium.config.Configuration;
 import io.debezium.relational.TableId;
@@ -43,7 +43,7 @@
 /**
  * A {@link DatabaseHistory} implementation which store the latest table schema in Flink state.
  *
- * <p>It stores/recovers history using data offered by {@link MySQLSplitState}.
+ * <p>It stores/recovers history using data offered by {@link MySqlSplitState}.
  */
 public class EmbeddedFlinkDatabaseHistory implements DatabaseHistory {
 

File: flink-connector-mysql-cdc/src/main/java/com/alibaba/ververica/cdc/connectors/mysql/debezium/dispatcher/SignalEventDispatcher.java
Patch:
@@ -19,7 +19,7 @@
 package com.alibaba.ververica.cdc.connectors.mysql.debezium.dispatcher;
 
 import com.alibaba.ververica.cdc.connectors.mysql.source.offset.BinlogOffset;
-import com.alibaba.ververica.cdc.connectors.mysql.source.split.MySQLSplit;
+import com.alibaba.ververica.cdc.connectors.mysql.source.split.MySqlSplit;
 import io.debezium.connector.base.ChangeEventQueue;
 import io.debezium.connector.mysql.MySqlOffsetContext;
 import io.debezium.pipeline.DataChangeEvent;
@@ -83,7 +83,7 @@ public SignalEventDispatcher(
     }
 
     public void dispatchWatermarkEvent(
-            MySQLSplit mySQLSplit, BinlogOffset watermark, WatermarkKind watermarkKind)
+            MySqlSplit mySQLSplit, BinlogOffset watermark, WatermarkKind watermarkKind)
             throws InterruptedException {
         SourceRecord sourceRecord =
                 new SourceRecord(

File: flink-connector-mysql-cdc/src/main/java/com/alibaba/ververica/cdc/connectors/mysql/source/events/EnumeratorAckEvent.java
Patch:
@@ -20,13 +20,13 @@
 
 import org.apache.flink.api.connector.source.SourceEvent;
 
-import com.alibaba.ververica.cdc.connectors.mysql.source.enumerator.MySQLSourceEnumerator;
-import com.alibaba.ververica.cdc.connectors.mysql.source.reader.MySQLSourceReader;
+import com.alibaba.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator;
+import com.alibaba.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReader;
 
 import java.util.List;
 
 /**
- * The {@link SourceEvent} that {@link MySQLSourceEnumerator} sends to {@link MySQLSourceReader} to
+ * The {@link SourceEvent} that {@link MySqlSourceEnumerator} sends to {@link MySqlSourceReader} to
  * notify the split has received.
  */
 public class EnumeratorAckEvent implements SourceEvent {

File: flink-connector-mysql-cdc/src/main/java/com/alibaba/ververica/cdc/connectors/mysql/source/events/EnumeratorRequestReportEvent.java
Patch:
@@ -20,11 +20,11 @@
 
 import org.apache.flink.api.connector.source.SourceEvent;
 
-import com.alibaba.ververica.cdc.connectors.mysql.source.enumerator.MySQLSourceEnumerator;
-import com.alibaba.ververica.cdc.connectors.mysql.source.reader.MySQLSourceReader;
+import com.alibaba.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator;
+import com.alibaba.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReader;
 
 /**
- * The {@link SourceEvent} that {@link MySQLSourceEnumerator} sends to {@link MySQLSourceReader} to
+ * The {@link SourceEvent} that {@link MySqlSourceEnumerator} sends to {@link MySqlSourceReader} to
  * notify reader should report its finished snapshot splits.
  */
 public class EnumeratorRequestReportEvent implements SourceEvent {

File: flink-connector-mysql-cdc/src/main/java/com/alibaba/ververica/cdc/connectors/mysql/source/events/SourceReaderReportEvent.java
Patch:
@@ -21,14 +21,14 @@
 import org.apache.flink.api.connector.source.SourceEvent;
 import org.apache.flink.api.java.tuple.Tuple2;
 
-import com.alibaba.ververica.cdc.connectors.mysql.source.enumerator.MySQLSourceEnumerator;
+import com.alibaba.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator;
 import com.alibaba.ververica.cdc.connectors.mysql.source.offset.BinlogOffset;
-import com.alibaba.ververica.cdc.connectors.mysql.source.reader.MySQLSourceReader;
+import com.alibaba.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReader;
 
 import java.util.ArrayList;
 
 /**
- * The {@link SourceEvent} that {@link MySQLSourceReader} sends to {@link MySQLSourceEnumerator} to
+ * The {@link SourceEvent} that {@link MySqlSourceReader} sends to {@link MySqlSourceEnumerator} to
  * notify the snapshot split has read finished.
  */
 public class SourceReaderReportEvent implements SourceEvent {

File: flink-connector-mysql-cdc/src/main/java/com/alibaba/ververica/cdc/connectors/mysql/source/split/MySqlSplitKind.java
Patch:
@@ -19,15 +19,15 @@
 package com.alibaba.ververica.cdc.connectors.mysql.source.split;
 
 /** The kinds of MySQL table split. */
-public enum MySQLSplitKind {
+public enum MySqlSplitKind {
 
     /** The split that reads snapshot records of MySQL table. */
     SNAPSHOT,
 
     /** The split that reads binlog records of MySQL table. */
     BINLOG;
 
-    public static MySQLSplitKind fromString(String kind) {
+    public static MySqlSplitKind fromString(String kind) {
         if (SNAPSHOT.toString().equalsIgnoreCase(kind)) {
             return SNAPSHOT;
         }

File: flink-connector-mysql-cdc/src/main/java/com/alibaba/ververica/cdc/connectors/mysql/source/utils/StatementUtils.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.Preconditions;
 
-import com.alibaba.ververica.cdc.connectors.mysql.source.MySQLSourceOptions;
+import com.alibaba.ververica.cdc.connectors.mysql.source.MySqlSourceOptions;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.TableId;
 
@@ -53,7 +53,7 @@ public static RowType getSplitKey(Configuration configuration, RowType pkType) {
         if (pkType.getFieldCount() == 1) {
             return pkType;
         } else {
-            String splitColumnName = configuration.getString(MySQLSourceOptions.SCAN_SPLIT_COLUMN);
+            String splitColumnName = configuration.getString(MySqlSourceOptions.SCAN_SPLIT_COLUMN);
             return new RowType(
                     pkType.getFields().stream()
                             .filter(r -> splitColumnName.equalsIgnoreCase(r.getName()))

File: flink-connector-mysql-cdc/src/test/java/com/alibaba/ververica/cdc/connectors/mysql/MySqlSourceITCase.java
Patch:
@@ -26,9 +26,9 @@
 import org.junit.Ignore;
 import org.junit.Test;
 
-/** Integration tests for {@link MySQLSource}. */
+/** Integration tests for {@link MySqlSource}. */
 @Ignore
-public class MySQLSourceITCase extends MySQLTestBase {
+public class MySqlSourceITCase extends MySqlTestBase {
 
     private final UniqueDatabase inventoryDatabase =
             new UniqueDatabase(MYSQL_CONTAINER, "inventory", "mysqluser", "mysqlpw");
@@ -37,7 +37,7 @@ public class MySQLSourceITCase extends MySQLTestBase {
     public void testConsumingAllEvents() throws Exception {
         inventoryDatabase.createAndInitialize();
         SourceFunction<String> sourceFunction =
-                MySQLSource.<String>builder()
+                MySqlSource.<String>builder()
                         .hostname(MYSQL_CONTAINER.getHost())
                         .port(MYSQL_CONTAINER.getDatabasePort())
                         .databaseList(

File: flink-connector-mysql-cdc/src/test/java/com/alibaba/ververica/cdc/connectors/mysql/source/utils/UniqueDatabase.java
Patch:
@@ -50,14 +50,14 @@ public class UniqueDatabase {
             new String[] {"CREATE DATABASE $DBNAME$;", "USE $DBNAME$;"};
     private static final Pattern COMMENT_PATTERN = Pattern.compile("^(.*)--.*$");
 
-    private final MySQLContainer container;
+    private final MySqlContainer container;
     private final String databaseName;
     private final String templateName;
     private final String username;
     private final String password;
 
     public UniqueDatabase(
-            MySQLContainer container, String databaseName, String username, String password) {
+            MySqlContainer container, String databaseName, String username, String password) {
         this(
                 container,
                 databaseName,
@@ -67,7 +67,7 @@ public UniqueDatabase(
     }
 
     private UniqueDatabase(
-            MySQLContainer container,
+            MySqlContainer container,
             String databaseName,
             final String identifier,
             String username,

File: flink-connector-mysql-cdc/src/test/java/com/alibaba/ververica/cdc/connectors/mysql/source/MySQLParallelSourceITCase.java
Patch:
@@ -288,9 +288,9 @@ private Configuration getConfig() {
         properties.put("database.user", customDatabase.getUsername());
         properties.put("database.password", customDatabase.getPassword());
         properties.put("database.history.skip.unparseable.ddl", "true");
-        properties.put("server-id.range", "1001,1004");
-        properties.put("scan.split.size", "4");
-        properties.put("scan.fetch.size", "2");
+        properties.put("server-id", "1001,1004");
+        properties.put("scan.split.size", "1024");
+        properties.put("scan.fetch.size", "512");
         properties.put("database.serverTimezone", ZoneId.of("UTC").toString());
         properties.put("snapshot.mode", "initial");
         properties.put("database.history", EmbeddedFlinkDatabaseHistory.class.getCanonicalName());

File: flink-connector-debezium/src/main/java/com/alibaba/ververica/cdc/debezium/table/RowDataDebeziumDeserializeSchema.java
Patch:
@@ -228,7 +228,7 @@ private int convertToInt(Object dbzObj, Schema schema) {
 
     private long convertToLong(Object dbzObj, Schema schema) {
         if (dbzObj instanceof Integer) {
-            return (long) dbzObj;
+            return ((Integer) dbzObj).longValue();
         } else if (dbzObj instanceof Long) {
             return (long) dbzObj;
         } else {
@@ -238,7 +238,7 @@ private long convertToLong(Object dbzObj, Schema schema) {
 
     private double convertToDouble(Object dbzObj, Schema schema) {
         if (dbzObj instanceof Float) {
-            return (double) dbzObj;
+            return ((Float) dbzObj).doubleValue();
         } else if (dbzObj instanceof Double) {
             return (double) dbzObj;
         } else {

File: flink-connector-debezium/src/main/java/com/alibaba/ververica/cdc/debezium/DebeziumSourceFunction.java
Patch:
@@ -265,7 +265,7 @@ private void snapshotOffsetState(long checkpointId) throws Exception {
             }
         } else {
             byte[] currentState = consumer.snapshotCurrentState();
-            if (currentState == null) {
+            if (currentState == null && restoredOffsetState != null) {
                 // the consumer has been initialized, but has not yet received any data,
                 // which means we need to return the originally restored offsets
                 serializedOffset = restoredOffsetState.getBytes(StandardCharsets.UTF_8);

File: flink-connector-mysql-cdc/src/test/java/com/alibaba/ververica/cdc/connectors/mysql/utils/MySQLContainer.java
Patch:
@@ -18,7 +18,6 @@
 
 package com.alibaba.ververica.cdc.connectors.mysql.utils;
 
-import org.jetbrains.annotations.NotNull;
 import org.testcontainers.containers.ContainerLaunchException;
 import org.testcontainers.containers.JdbcDatabaseContainer;
 
@@ -50,7 +49,6 @@ public MySQLContainer() {
 		addExposedPort(MYSQL_PORT);
 	}
 
-	@NotNull
 	@Override
 	protected Set<Integer> getLivenessCheckPorts() {
 		return new HashSet<>(getMappedPort(MYSQL_PORT));

File: flink-connector-debezium/src/main/java/com/alibaba/ververica/cdc/debezium/DebeziumSourceFunction.java
Patch:
@@ -266,8 +266,6 @@ public void run(SourceContext<T> sourceContext) throws Exception {
 		// see https://stackoverflow.com/questions/57147584/debezium-error-schema-isnt-know-to-this-connector
 		// and https://debezium.io/blog/2018/03/16/note-on-database-history-topic-configuration/
 		properties.setProperty("database.history", FlinkDatabaseHistory.class.getCanonicalName());
-		// reduce the history records to store
-		properties.setProperty("database.history.store.only.monitored.tables.ddl", "true");
 		if (engineInstanceName == null) {
 			// not restore from recovery
 			engineInstanceName = UUID.randomUUID().toString();

File: flink-connector-postgres-cdc/src/main/java/com/alibaba/ververica/cdc/connectors/postgres/table/PostgreSQLTableSource.java
Patch:
@@ -33,6 +33,7 @@
 import com.alibaba.ververica.cdc.debezium.DebeziumSourceFunction;
 import com.alibaba.ververica.cdc.debezium.table.RowDataDebeziumDeserializeSchema;
 
+import java.time.ZoneId;
 import java.util.Objects;
 import java.util.Properties;
 
@@ -96,7 +97,8 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {
 		DebeziumDeserializationSchema<RowData> deserializer = new RowDataDebeziumDeserializeSchema(
 			rowType,
 			typeInfo,
-			new PostgresValueValidator(schemaName, tableName));
+			new PostgresValueValidator(schemaName, tableName),
+			ZoneId.of("UTC"));
 		DebeziumSourceFunction<RowData> sourceFunction = PostgreSQLSource.<RowData>builder()
 			.hostname(hostname)
 			.port(port)

File: flink-debezium-common/src/main/java/com/alibaba/ververica/cdc/debezium/DebeziumSourceFunction.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;
 import org.apache.flink.streaming.api.functions.source.RichSourceFunction;
 import org.apache.flink.util.ExceptionUtils;
+
 import org.apache.flink.shaded.guava18.com.google.common.util.concurrent.ThreadFactoryBuilder;
 
 import com.alibaba.ververica.cdc.debezium.internal.DebeziumChangeConsumer;
@@ -320,7 +321,6 @@ public void run(SourceContext<T> sourceContext) throws Exception {
 		}
 	}
 
-
 	@Override
 	public void cancel() {
 		// flag the main thread to exit. A thread interrupt will come anyways.

File: flink-debezium-common/src/main/java/com/alibaba/ververica/cdc/debezium/internal/DebeziumChangeConsumer.java
Patch:
@@ -131,7 +131,6 @@ private boolean isSnapshotRecord(SourceRecord record) {
 		return false;
 	}
 
-
 	private void emitRecordsUnderCheckpointLock(
 			Queue<T> records,
 			Map<String, ?> sourcePartition,
@@ -192,4 +191,4 @@ public void close() {
 
 		}
 	}
-}
\ No newline at end of file
+}

File: flink-debezium-common/src/main/java/com/alibaba/ververica/cdc/debezium/internal/DebeziumState.java
Patch:
@@ -33,8 +33,8 @@
  * name, or topic-partition). The sourceOffset represents a position in that sourcePartition which can be used
  * to resume consumption of data.
  * </p>
- * <p>
- * These values can have arbitrary structure and should be represented using
+ *
+ * <p>These values can have arbitrary structure and should be represented using
  * org.apache.kafka.connect.data objects (or primitive values). For example, a database connector
  * might specify the sourcePartition as a record containing { "db": "database_name", "table":
  * "table_name"} and the sourceOffset as a Long containing the timestamp of the row.

File: flink-debezium-common/src/main/java/com/alibaba/ververica/cdc/debezium/internal/DebeziumStateSerializer.java
Patch:
@@ -19,6 +19,7 @@
 package com.alibaba.ververica.cdc.debezium.internal;
 
 import org.apache.flink.annotation.Internal;
+
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper;
 
 import java.io.IOException;

File: flink-debezium-common/src/main/java/com/alibaba/ververica/cdc/debezium/internal/FlinkDatabaseHistory.java
Patch:
@@ -106,7 +106,6 @@ public void configure(Configuration config, HistoryRecordComparator comparator,
 		}
 	}
 
-
 	@Override
 	public void stop() {
 		super.stop();

File: flink-debezium-common/src/main/java/com/alibaba/ververica/cdc/debezium/internal/FlinkOffsetBackingStore.java
Patch:
@@ -61,7 +61,7 @@ public class FlinkOffsetBackingStore implements OffsetBackingStore {
 
 	public static final String OFFSET_STATE_VALUE = "offset.storage.flink.state.value";
 	public static final int FLUSH_TIMEOUT_SECONDS = 10;
-	
+
 	protected Map<ByteBuffer, ByteBuffer> data = new HashMap<>();
 	protected ExecutorService executor;
 
@@ -181,8 +181,9 @@ public Future<Void> set(final Map<ByteBuffer, ByteBuffer> values,
 			for (Map.Entry<ByteBuffer, ByteBuffer> entry : values.entrySet()) {
 				data.put(entry.getKey(), entry.getValue());
 			}
-			if (callback != null)
+			if (callback != null) {
 				callback.onCompletion(null, null);
+			}
 			return null;
 		});
 	}

File: flink-debezium-common/src/main/java/com/alibaba/ververica/cdc/debezium/table/RowDataDebeziumDeserializeSchema.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.RowKind;
 import org.apache.flink.util.Collector;
+
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;
 
 import com.alibaba.ververica.cdc.debezium.DebeziumDeserializationSchema;
@@ -254,10 +255,10 @@ private TimestampData convertToTimestamp(Object dbzObj, Schema schema) {
 					return TimestampData.fromEpochMillis((Long) dbzObj);
 				case MicroTimestamp.SCHEMA_NAME:
 					long micro = (long) dbzObj;
-					return TimestampData.fromEpochMillis(micro/1000, (int) (micro % 1000 * 1000));
+					return TimestampData.fromEpochMillis(micro / 1000, (int) (micro % 1000 * 1000));
 				case NanoTimestamp.SCHEMA_NAME:
 					long nano = (long) dbzObj;
-					return TimestampData.fromEpochMillis(nano/1000_000, (int) (nano % 1000_000));
+					return TimestampData.fromEpochMillis(nano / 1000_000, (int) (nano % 1000_000));
 			}
 		}
 		LocalDateTime localDateTime = TemporalConversions.toLocalDateTime(dbzObj);

File: flink-debezium-common/src/main/java/com/alibaba/ververica/cdc/debezium/utils/TemporalConversions.java
Patch:
@@ -18,7 +18,6 @@
 
 package com.alibaba.ververica.cdc.debezium.utils;
 
-
 import java.time.Duration;
 import java.time.Instant;
 import java.time.LocalDate;

File: flink-mysql-cdc-connector/src/main/java/com/alibaba/ververica/cdc/connectors/mysql/MySqlBinlogSource.java
Patch:
@@ -35,6 +35,9 @@ public static <T> Builder<T> builder() {
 		return new Builder<>();
 	}
 
+	/**
+	 * Builder class of {@link MySqlBinlogSource}.
+	 */
 	public static class Builder<T> {
 
 		private int port = 3306; // default 3306 port

File: flink-mysql-cdc-connector/src/test/java/com/alibaba/ververica/cdc/connectors/mysql/utils/AssertUtils.java
Patch:
@@ -26,6 +26,9 @@
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertNull;
 
+/**
+ * Utilities for asserting {@link SourceRecord}.
+ */
 public class AssertUtils {
 	/**
 	 * Verify that the given {@link SourceRecord} is a {@link Envelope.Operation#CREATE INSERT/CREATE} record.

File: flink-mysql-cdc-connector/src/test/java/com/alibaba/ververica/cdc/connectors/mysql/utils/MySqlBinlogContainer.java
Patch:
@@ -112,12 +112,12 @@ public int getDatabasePort() {
 	protected String constructUrlForConnection(String queryString) {
 		String url = super.constructUrlForConnection(queryString);
 
-		if (! url.contains("useSSL=")) {
+		if (!url.contains("useSSL=")) {
 			String separator = url.contains("?") ? "&" : "?";
 			url = url + separator + "useSSL=false";
 		}
 
-		if (! url.contains("allowPublicKeyRetrieval=")) {
+		if (!url.contains("allowPublicKeyRetrieval=")) {
 			url = url + "&allowPublicKeyRetrieval=true";
 		}
 

File: flink-mysql-cdc-connector/src/test/java/com/alibaba/ververica/cdc/connectors/mysql/utils/UniqueDatabase.java
Patch:
@@ -99,7 +99,7 @@ public void createAndInitialize() {
 		assertNotNull("Cannot locate " + ddlFile, ddlTestFile);
 		try {
 			try (Connection connection = DriverManager.getConnection(container.getJdbcUrl(), username, password);
-				 Statement statement = connection.createStatement()) {
+					Statement statement = connection.createStatement()) {
 				final List<String> statements = Arrays.stream(
 					Stream.concat(
 						Arrays.stream(CREATE_DATABASE_DDL),

