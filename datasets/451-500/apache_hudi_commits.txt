File: hudi-spark-datasource/hudi-spark3.5.x/src/test/java/org/apache/hudi/internal/HoodieBulkInsertInternalWriterTestBase.java
Patch:
@@ -121,8 +121,8 @@ protected void assertWriteStatuses(List<WriteStatus> writeStatuses, int batches,
         assertEquals(writeStatus.getTotalRecords(), sizeMap.get(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[counter % 3]));
       }
       assertNull(writeStatus.getGlobalError());
-      assertEquals(writeStatus.getTotalErrorRecords(), 0);
-      assertEquals(writeStatus.getTotalErrorRecords(), 0);
+      assertEquals(0, writeStatus.getTotalErrorRecords());
+      assertEquals(0, writeStatus.getTotalErrorRecords());
       assertFalse(writeStatus.hasErrors());
       assertNotNull(writeStatus.getFileId());
       String fileId = writeStatus.getFileId();

File: hudi-spark-datasource/hudi-spark3.5.x/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java
Patch:
@@ -57,7 +57,7 @@
 /**
  * Unit tests {@link HoodieDataSourceInternalBatchWrite}.
  */
-public class TestHoodieDataSourceInternalBatchWrite extends
+class TestHoodieDataSourceInternalBatchWrite extends
     HoodieBulkInsertInternalWriterTestBase {
 
   private static Stream<Arguments> bulkInsertTypeParams() {
@@ -129,7 +129,7 @@ private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map
   }
 
   @Test
-  public void testDataSourceWriterExtraCommitMetadata() throws Exception {
+  void testDataSourceWriterExtraCommitMetadata() throws Exception {
     String commitExtraMetaPrefix = "commit_extra_meta_";
     Map<String, String> extraMeta = new HashMap<>();
     extraMeta.put(DataSourceWriteOptions.COMMIT_METADATA_KEYPREFIX().key(), commitExtraMetaPrefix);
@@ -146,7 +146,7 @@ public void testDataSourceWriterExtraCommitMetadata() throws Exception {
   }
 
   @Test
-  public void testDataSourceWriterEmptyExtraCommitMetadata() throws Exception {
+  void testDataSourceWriterEmptyExtraCommitMetadata() throws Exception {
     String commitExtraMetaPrefix = "commit_extra_meta_";
     Map<String, String> extraMeta = new HashMap<>();
     extraMeta.put(DataSourceWriteOptions.COMMIT_METADATA_KEYPREFIX().key(), commitExtraMetaPrefix);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandleWithChangeLog.java
Patch:
@@ -62,7 +62,7 @@ public HoodieMergeHandleWithChangeLog(HoodieWriteConfig config, String instantTi
         partitionPath,
         storage,
         getWriterSchema(),
-        createLogWriter(instantTime, HoodieCDCUtils.CDC_LOGFILE_SUFFIX),
+        createLogWriter(instantTime, HoodieCDCUtils.CDC_LOGFILE_SUFFIX, Option.empty()),
         IOUtils.getMaxMemoryPerPartitionMerge(taskContextSupplier, config));
   }
 
@@ -80,7 +80,7 @@ public HoodieMergeHandleWithChangeLog(HoodieWriteConfig config, String instantTi
         partitionPath,
         storage,
         getWriterSchema(),
-        createLogWriter(instantTime, HoodieCDCUtils.CDC_LOGFILE_SUFFIX),
+        createLogWriter(instantTime, HoodieCDCUtils.CDC_LOGFILE_SUFFIX, Option.empty()),
         IOUtils.getMaxMemoryPerPartitionMerge(taskContextSupplier, config));
   }
 

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/testutils/providers/HoodieMetaClientProvider.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.hudi.testutils.providers;
 
+import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.view.HoodieTableFileSystemView;
@@ -36,6 +37,8 @@ public interface HoodieMetaClientProvider {
 
   HoodieTableMetaClient getHoodieMetaClient(StorageConfiguration<?> storageConf, String basePath, Properties props) throws IOException;
 
+  HoodieTableMetaClient getHoodieMetaClient(StorageConfiguration<?> storageConf, String basePath, Properties props, HoodieTableType tableType) throws IOException;
+
   default HoodieTableFileSystemView getHoodieTableFileSystemView(
       HoodieTableMetaClient metaClient, HoodieTimeline visibleActiveTimeline,
       List<StoragePathInfo> pathInfoList) {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkMergeAndReplaceHandleWithChangeLog.java
Patch:
@@ -64,7 +64,7 @@ public FlinkMergeAndReplaceHandleWithChangeLog(HoodieWriteConfig config, String
         partitionPath,
         getStorage(),
         getWriterSchema(),
-        createLogWriter(instantTime, HoodieCDCUtils.CDC_LOGFILE_SUFFIX),
+        createLogWriter(instantTime, HoodieCDCUtils.CDC_LOGFILE_SUFFIX, Option.empty()),
         IOUtils.getMaxMemoryPerPartitionMerge(taskContextSupplier, config));
   }
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkMergeHandleWithChangeLog.java
Patch:
@@ -62,7 +62,7 @@ public FlinkMergeHandleWithChangeLog(HoodieWriteConfig config, String instantTim
         partitionPath,
         getStorage(),
         getWriterSchema(),
-        createLogWriter(instantTime, HoodieCDCUtils.CDC_LOGFILE_SUFFIX),
+        createLogWriter(instantTime, HoodieCDCUtils.CDC_LOGFILE_SUFFIX, Option.empty()),
         IOUtils.getMaxMemoryPerPartitionMerge(taskContextSupplier, config));
   }
 

File: hudi-hadoop-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java
Patch:
@@ -102,6 +102,9 @@ private FSDataOutputStream getOutputStream() throws IOException {
       boolean created = false;
       while (!created) {
         try {
+          if (storage.exists(logFile.getPath())) {
+            rollOver();
+          }
           // Block size does not matter as we will always manually auto-flush
           createNewFile();
           LOG.info("Created a new log file: {}", logFile);

File: hudi-common/src/main/java/org/apache/hudi/client/transaction/lock/NoopLockProvider.java
Patch:
@@ -31,9 +31,9 @@
 
 /**
  * NoopLockProvider as the name suggests, is a no op lock provider. Any caller asking for a lock will be able to get hold of the lock.
- * This is not meant to be used a producation grade lock providers. This is meant to be used for Hudi's internal operations.
- * For eg: During upgrade, we have nested lock situations and we leverage this {@code NoopLockProvider} for any operations we
- * might want to do within the upgradeHandler blocks to avoid re-entrant situations. Not all lock providers might support re-entrancy and during upgrade,
+ * This is not meant to be used a production grade lock providers. This is meant to be used for Hudi's internal operations.
+ * For eg: During upgrade, we have nested lock situations, and we leverage this {@code NoopLockProvider} for any operations we
+ * might want to do within the upgradeHandler blocks to avoid re-entrant situations. Not all lock providers support re-entrance and during upgrade,
  * it is expected to have a single writer to the Hudi table of interest.
  */
 public class NoopLockProvider implements LockProvider<ReentrantReadWriteLock>, Serializable {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/bloom/TestBucketizedBloomCheckPartitioner.java
Patch:
@@ -29,6 +29,8 @@
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
+import scala.Tuple2;
+
 import static org.junit.jupiter.api.Assertions.assertArrayEquals;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertTrue;
@@ -96,7 +98,7 @@ public void testGetPartitions() {
     BucketizedBloomCheckPartitioner p = new BucketizedBloomCheckPartitioner(1000, comparisons1, 10);
 
     IntStream.range(0, 100000).forEach(f -> {
-      int partition = p.getPartition(Pair.of(new HoodieFileGroupId("p1", "f" + f), "value"));
+      int partition = p.getPartition(Tuple2.apply(new HoodieFileGroupId("p1", "f" + f), "value"));
       assertTrue(0 <= partition && partition <= 1000, "partition is out of range: " + partition);
     });
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -3496,7 +3496,7 @@ private boolean isLockRequiredForSingleWriter() {
 
     private void autoAdjustConfigsForConcurrencyMode(boolean isLockProviderPropertySet) {
       // for a single writer scenario, with all table services inline, lets set InProcessLockProvider
-      if (writeConfig.getWriteConcurrencyMode() == WriteConcurrencyMode.SINGLE_WRITER && !writeConfig.areAnyTableServicesAsync()) {
+      if (writeConfig.isAutoAdjustLockConfigs() && writeConfig.getWriteConcurrencyMode() == WriteConcurrencyMode.SINGLE_WRITER && !writeConfig.areAnyTableServicesAsync()) {
         if (writeConfig.getLockProviderClass() != null && !writeConfig.getLockProviderClass().equals(InProcessLockProvider.class.getCanonicalName())) {
           // add logs only when explicitly overridden by the user.
           LOG.warn(String.format("For a single writer mode, overriding lock provider class (%s) to %s. So, user configured lock provider %s may not take effect",

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -2785,14 +2785,14 @@ public boolean areReleaseResourceEnabled() {
    * Returns whether the explicit guard of lock is required.
    */
   public boolean isLockRequired() {
-    return !isDefaultLockProvider() || getWriteConcurrencyMode().supportsMultiWriter();
+    return isLockProviderSet() || getWriteConcurrencyMode().supportsMultiWriter();
   }
 
   /**
    * Returns whether the lock provider is default.
    */
-  private boolean isDefaultLockProvider() {
-    return HoodieLockConfig.LOCK_PROVIDER_CLASS_NAME.defaultValue().equals(getLockProviderClass());
+  private boolean isLockProviderSet() {
+    return getLockProviderClass() != null;
   }
 
   /**

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/HoodieStreamerUtils.java
Patch:
@@ -79,6 +79,7 @@ public static Option<JavaRDD<HoodieRecord>> createHoodieRecords(HoodieStreamer.C
                                                                   SchemaProvider schemaProvider, HoodieRecord.HoodieRecordType recordType, boolean autoGenerateRecordKeys,
                                                                   String instantTime, Option<BaseErrorTableWriter> errorTableWriter) {
     boolean shouldCombine = cfg.filterDupes || cfg.operation.equals(WriteOperationType.UPSERT);
+    boolean shouldUseOrderingField = shouldCombine && !StringUtils.isNullOrEmpty(cfg.sourceOrderingField);
     boolean shouldErrorTable = errorTableWriter.isPresent() && props.getBoolean(ERROR_ENABLE_VALIDATE_RECORD_CREATION.key(), ERROR_ENABLE_VALIDATE_RECORD_CREATION.defaultValue());
     boolean useConsistentLogicalTimestamp = ConfigUtils.getBooleanWithAltKeys(
         props, KeyGeneratorOptions.KEYGENERATOR_CONSISTENT_LOGICAL_TIMESTAMP_ENABLED);
@@ -102,7 +103,7 @@ public static Option<JavaRDD<HoodieRecord>> createHoodieRecords(HoodieStreamer.C
                 try {
                   HoodieKey hoodieKey = new HoodieKey(builtinKeyGenerator.getRecordKey(genRec), builtinKeyGenerator.getPartitionPath(genRec));
                   GenericRecord gr = isDropPartitionColumns(props) ? HoodieAvroUtils.removeFields(genRec, partitionColumns) : genRec;
-                  HoodieRecordPayload payload = shouldCombine ? DataSourceUtils.createPayload(payloadClassName, gr,
+                  HoodieRecordPayload payload = shouldUseOrderingField ? DataSourceUtils.createPayload(payloadClassName, gr,
                       (Comparable) HoodieAvroUtils.getNestedFieldVal(gr, cfg.sourceOrderingField, false, useConsistentLogicalTimestamp))
                       : DataSourceUtils.createPayload(payloadClassName, gr);
                   return Either.left(new HoodieAvroRecord<>(hoodieKey, payload));

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java
Patch:
@@ -325,7 +325,7 @@ public static HoodieTableMetaClient createMetaClient(SparkSession spark, String
     return HoodieTestUtils.createMetaClient(new HadoopStorageConfiguration(spark.sessionState().newHadoopConf()), basePath);
   }
 
-  private static Option<HoodieCommitMetadata> getCommitMetadataForInstant(HoodieTableMetaClient metaClient, HoodieInstant instant) {
+  public static Option<HoodieCommitMetadata> getCommitMetadataForInstant(HoodieTableMetaClient metaClient, HoodieInstant instant) {
     try {
       HoodieTimeline timeline = metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();
       byte[] data = timeline.getInstantDetails(instant).get();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java
Patch:
@@ -190,7 +190,7 @@ protected Option<Checkpoint> translateCheckpoint(Option<Checkpoint> lastCheckpoi
 
   @Override
   public Pair<Option<Dataset<Row>>, Checkpoint> fetchNextBatch(Option<Checkpoint> lastCheckpoint, long sourceLimit) {
-    if (CheckpointUtils.targetCheckpointV2(writeTableVersion)) {
+    if (CheckpointUtils.shouldTargetCheckpointV2(writeTableVersion, getClass().getName())) {
       return fetchNextBatchBasedOnCompletionTime(lastCheckpoint, sourceLimit);
     } else {
       return fetchNextBatchBasedOnRequestedTime(lastCheckpoint, sourceLimit);
@@ -215,7 +215,7 @@ private Pair<Option<Dataset<Row>>, Checkpoint> fetchNextBatchBasedOnCompletionTi
     IncrementalQueryAnalyzer analyzer = IncrSourceHelper.getIncrementalQueryAnalyzer(
         sparkContext, srcPath, lastCheckpoint, missingCheckpointStrategy,
         getIntWithAltKeys(props, HoodieIncrSourceConfig.NUM_INSTANTS_PER_FETCH),
-        getLatestSourceProfile());
+        getLatestSourceProfile(), getHollowCommitHandleMode(props));
     QueryContext queryContext = analyzer.analyze();
     Option<InstantRange> instantRange = queryContext.getInstantRange();
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/Source.java
Patch:
@@ -104,7 +104,7 @@ protected Option<Checkpoint> translateCheckpoint(Option<Checkpoint> lastCheckpoi
     if (lastCheckpoint.isEmpty()) {
       return Option.empty();
     }
-    if (CheckpointUtils.targetCheckpointV2(writeTableVersion)) {
+    if (CheckpointUtils.shouldTargetCheckpointV2(writeTableVersion, getClass().getName())) {
       // V2 -> V2
       if (lastCheckpoint.get() instanceof StreamerCheckpointV2) {
         return lastCheckpoint;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hudi.common.table.read.IncrementalQueryAnalyzer;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.TimelineUtils;
 import org.apache.hudi.common.table.timeline.TimelineUtils.HollowCommitHandling;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ValidationUtils;
@@ -184,7 +185,7 @@ public static IncrementalQueryAnalyzer getIncrementalQueryAnalyzer(
       Option<Checkpoint> lastCheckpoint,
       MissingCheckpointStrategy missingCheckpointStrategy,
       int numInstantsFromConfig,
-      Option<SourceProfile<Integer>> latestSourceProfile) {
+      Option<SourceProfile<Integer>> latestSourceProfile, TimelineUtils.HollowCommitHandling handlingMode) {
     HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder()
         .setConf(HadoopFSUtils.getStorageConfWithCopy(jssc.hadoopConfiguration()))
         .setBasePath(srcPath)
@@ -197,7 +198,7 @@ public static IncrementalQueryAnalyzer getIncrementalQueryAnalyzer(
     if (lastCheckpoint.isPresent() && !lastCheckpoint.get().getCheckpointKey().isEmpty()) {
       // Translate checkpoint
       StreamerCheckpointV2 lastStreamerCheckpointV2 = CheckpointUtils.convertToCheckpointV2ForCommitTime(
-          lastCheckpoint.get(), metaClient);
+          lastCheckpoint.get(), metaClient, handlingMode);
       startCompletionTime = lastStreamerCheckpointV2.getCheckpointKey();
       rangeType = RangeType.OPEN_CLOSED;
     } else if (missingCheckpointStrategy != null) {

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerTestBase.java
Patch:
@@ -563,7 +563,7 @@ void assertNoPartitionMatch(String basePath, SQLContext sqlContext, String parti
         .count());
   }
 
-  static class TestHelpers {
+  public static class TestHelpers {
 
     static HoodieDeltaStreamer.Config makeDropAllConfig(String basePath, WriteOperationType op) {
       return makeConfig(basePath, op, Collections.singletonList(TestHoodieDeltaStreamer.DropAllTransformer.class.getName()));
@@ -590,7 +590,7 @@ static HoodieDeltaStreamer.Config makeConfig(String basePath, WriteOperationType
           useSchemaProviderClass, 1000, updatePayloadClass, payloadClassName, tableType, "timestamp", null);
     }
 
-    static HoodieDeltaStreamer.Config makeConfig(String basePath, WriteOperationType op, String sourceClassName,
+    public static HoodieDeltaStreamer.Config makeConfig(String basePath, WriteOperationType op, String sourceClassName,
                                                  List<String> transformerClassNames, String propsFilename, boolean enableHiveSync, boolean useSchemaProviderClass,
                                                  int sourceLimit, boolean updatePayloadClass, String payloadClassName, String tableType, String sourceOrderingField,
                                                  String checkpoint) {

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestGcsEventsHoodieIncrSource.java
Patch:
@@ -44,12 +44,12 @@
 import org.apache.hudi.utilities.ingestion.HoodieIngestionMetrics;
 import org.apache.hudi.utilities.schema.FilebasedSchemaProvider;
 import org.apache.hudi.utilities.schema.SchemaProvider;
-import org.apache.hudi.utilities.sources.TestS3EventsHoodieIncrSource.TestSourceProfile;
 import org.apache.hudi.utilities.sources.helpers.CloudDataFetcher;
 import org.apache.hudi.utilities.sources.helpers.CloudObjectsSelectorCommon;
 import org.apache.hudi.utilities.sources.helpers.IncrSourceHelper;
 import org.apache.hudi.utilities.sources.helpers.QueryInfo;
 import org.apache.hudi.utilities.sources.helpers.QueryRunner;
+import org.apache.hudi.utilities.sources.S3EventsHoodieIncrSourceHarness.TestSourceProfile;
 import org.apache.hudi.utilities.streamer.DefaultStreamContext;
 import org.apache.hudi.utilities.streamer.SourceProfileSupplier;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -166,7 +166,7 @@ public class HoodieWriteConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> PRECOMBINE_FIELD_NAME = ConfigProperty
       .key("hoodie.datasource.write.precombine.field")
-      .defaultValue("ts")
+      .noDefaultValue()
       .withDocumentation("Field used in preCombining before actual write. When two records have the same key value, "
           + "we will pick the one with the largest value for the precombine field, determined by Object.compareTo(..)");
 
@@ -181,7 +181,7 @@ public class HoodieWriteConfig extends HoodieConfig {
   // This ConfigProperty is also used in SQL options which expect String type
   public static final ConfigProperty<String> RECORD_MERGE_MODE = ConfigProperty
       .key("hoodie.write.record.merge.mode")
-      .defaultValue(RecordMergeMode.EVENT_TIME_ORDERING.name())
+      .noDefaultValue("COMMIT_TIME_ORDERING if ordering field is not set; EVENT_TIME_ORDERING if ordering field is set")
       .sinceVersion("1.0.0")
       .withDocumentation(RecordMergeMode.class);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/EightToSevenDowngradeHandler.java
Patch:
@@ -166,7 +166,9 @@ static void unsetInitialVersion(HoodieTableConfig tableConfig, Map<ConfigPropert
 
   static void unsetRecordMergeMode(HoodieTableConfig tableConfig, Map<ConfigProperty, String> tablePropsToAdd) {
     Triple<RecordMergeMode, String, String> mergingConfigs =
-        HoodieTableConfig.inferCorrectMergingBehavior(tableConfig.getRecordMergeMode(), tableConfig.getPayloadClass(), tableConfig.getRecordMergeStrategyId());
+        HoodieTableConfig.inferCorrectMergingBehavior(
+            tableConfig.getRecordMergeMode(), tableConfig.getPayloadClass(),
+            tableConfig.getRecordMergeStrategyId(), tableConfig.getPreCombineField());
     if (StringUtils.nonEmpty(mergingConfigs.getMiddle())) {
       tablePropsToAdd.put(HoodieTableConfig.PAYLOAD_CLASS_NAME, mergingConfigs.getMiddle());
     }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/DefaultSparkRecordMerger.java
Patch:
@@ -40,7 +40,7 @@ public class DefaultSparkRecordMerger extends HoodieSparkRecordMerger {
 
   @Override
   public String getMergingStrategy() {
-    return HoodieRecordMerger.DEFAULT_MERGE_STRATEGY_UUID;
+    return HoodieRecordMerger.EVENT_TIME_BASED_MERGE_STRATEGY_UUID;
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/common/config/ConfigProperty.java
Patch:
@@ -254,7 +254,6 @@ public <T> ConfigProperty<T> defaultValue(T value) {
     }
 
     public <T> ConfigProperty<T> defaultValue(T value, String docOnDefaultValue) {
-      Objects.requireNonNull(value);
       Objects.requireNonNull(docOnDefaultValue);
       ConfigProperty<T> configProperty = new ConfigProperty<>(key, value, docOnDefaultValue, "", Option.empty(), Option.empty(), Option.empty(), Collections.emptySet(), false);
       return configProperty;

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -1393,7 +1393,8 @@ public Properties build() {
       }
 
       Triple<RecordMergeMode, String, String> mergeConfigs =
-          HoodieTableConfig.inferCorrectMergingBehavior(recordMergeMode, payloadClassName, recordMergerStrategyId);
+          HoodieTableConfig.inferCorrectMergingBehavior(
+              recordMergeMode, payloadClassName, recordMergerStrategyId, preCombineField);
       tableConfig.setValue(RECORD_MERGE_MODE, mergeConfigs.getLeft().name());
       tableConfig.setValue(PAYLOAD_CLASS_NAME.key(), mergeConfigs.getMiddle());
       tableConfig.setValue(RECORD_MERGE_STRATEGY_ID, mergeConfigs.getRight());

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -142,7 +142,7 @@ private FlinkOptions() {
   public static final ConfigOption<String> RECORD_MERGER_STRATEGY_ID = ConfigOptions
       .key("record.merger.strategy")
       .stringType()
-      .defaultValue(HoodieRecordMerger.DEFAULT_MERGE_STRATEGY_UUID)
+      .defaultValue(HoodieRecordMerger.EVENT_TIME_BASED_MERGE_STRATEGY_UUID)
       .withFallbackKeys(HoodieWriteConfig.RECORD_MERGE_STRATEGY_ID.key())
       .withDescription("Id of merger strategy. Hudi will pick HoodieRecordMerger implementations in record.merger.impls which has the same merger strategy id");
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/streamer/FlinkStreamerConfig.java
Patch:
@@ -132,7 +132,7 @@ public class FlinkStreamerConfig extends Configuration {
 
   @Parameter(names = {"--record-merger-strategy"}, description = "Id of record merger strategy. Hudi will pick HoodieRecordMerger implementations in record-merger-impls "
       + "which has the same record merger strategy id")
-  public String recordMergerStrategy = HoodieRecordMerger.DEFAULT_MERGE_STRATEGY_UUID;
+  public String recordMergerStrategy = HoodieRecordMerger.EVENT_TIME_BASED_MERGE_STRATEGY_UUID;
 
   @Parameter(names = {"--op"}, description = "Takes one of these values : UPSERT (default), INSERT (use when input "
       + "is purely new data/inserts to gain speed).", converter = OperationConverter.class)

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/table/read/TestCustomMerger.java
Patch:
@@ -21,7 +21,6 @@
 
 import org.apache.hudi.common.config.RecordMergeMode;
 import org.apache.hudi.common.config.TypedProperties;
-import org.apache.hudi.common.model.DefaultHoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieAvroIndexedRecord;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordMerger;
@@ -64,10 +63,9 @@ public class TestCustomMerger extends HoodieFileGroupReaderTestHarness {
 
   @Override
   protected Properties getMetaProps() {
-    Properties metaProps =  super.getMetaProps();
+    Properties metaProps = super.getMetaProps();
     metaProps.setProperty(HoodieTableConfig.RECORD_MERGE_MODE.key(), RecordMergeMode.CUSTOM.name());
     metaProps.setProperty(HoodieTableConfig.RECORD_MERGE_STRATEGY_ID.key(), CustomAvroMerger.KEEP_CERTAIN_TIMESTAMP_VALUE_ONLY);
-    metaProps.setProperty(HoodieTableConfig.PAYLOAD_CLASS_NAME.key(), DefaultHoodieRecordPayload.class.getName());
     return metaProps;
   }
 

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/table/read/TestOverwriteWithLatestMerger.java
Patch:
@@ -60,8 +60,6 @@ public class TestOverwriteWithLatestMerger extends HoodieFileGroupReaderTestHarn
   protected Properties getMetaProps() {
     Properties metaProps =  super.getMetaProps();
     metaProps.setProperty(HoodieTableConfig.RECORD_MERGE_MODE.key(), RecordMergeMode.COMMIT_TIME_ORDERING.name());
-    metaProps.setProperty(HoodieTableConfig.RECORD_MERGE_STRATEGY_ID.key(), HoodieRecordMerger.COMMIT_TIME_BASED_MERGE_STRATEGY_UUID);
-    metaProps.setProperty(HoodieTableConfig.PAYLOAD_CLASS_NAME.key(), OverwriteWithLatestAvroPayload.class.getName());
     return metaProps;
   }
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/DefaultHiveRecordMerger.java
Patch:
@@ -64,6 +64,6 @@ public Option<Pair<HoodieRecord, Schema>> merge(HoodieRecord older, Schema oldSc
 
   @Override
   public String getMergingStrategy() {
-    return HoodieRecordMerger.DEFAULT_MERGE_STRATEGY_UUID;
+    return HoodieRecordMerger.EVENT_TIME_BASED_MERGE_STRATEGY_UUID;
   }
 }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/processor/maxwell/MaxwellJsonKafkaSourcePostProcessor.java
Patch:
@@ -124,8 +124,7 @@ private String processDelete(JsonNode inputJson, ObjectNode result) {
 
     // we can update the `update_time`(delete time) only when it is in timestamp format.
     if (!preCombineFieldType.equals(NON_TIMESTAMP)) {
-      String preCombineField = this.props.getString(HoodieWriteConfig.PRECOMBINE_FIELD_NAME.key(),
-          HoodieWriteConfig.PRECOMBINE_FIELD_NAME.defaultValue());
+      String preCombineField = this.props.getString(HoodieWriteConfig.PRECOMBINE_FIELD_NAME.key(), null);
 
       // ts from maxwell
       long ts = inputJson.get(TS).longValue();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/BootstrapExecutor.java
Patch:
@@ -207,8 +207,7 @@ private void initializeTable() throws IOException {
         .setTableType(cfg.tableType)
         .setTableName(cfg.targetTableName)
         .setRecordKeyFields(props.getString(RECORDKEY_FIELD_NAME.key()))
-        .setPreCombineField(props.getString(
-            PRECOMBINE_FIELD_NAME.key(), PRECOMBINE_FIELD_NAME.defaultValue()))
+        .setPreCombineField(props.getString(PRECOMBINE_FIELD_NAME.key(), null))
         .setTableVersion(ConfigUtils.getIntWithAltKeys(props, WRITE_TABLE_VERSION))
         .setPopulateMetaFields(props.getBoolean(
             POPULATE_META_FIELDS.key(), POPULATE_META_FIELDS.defaultValue()))

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/HoodieStreamerUtils.java
Patch:
@@ -83,7 +83,9 @@ public static Option<JavaRDD<HoodieRecord>> createHoodieRecords(HoodieStreamer.C
     boolean useConsistentLogicalTimestamp = ConfigUtils.getBooleanWithAltKeys(
         props, KeyGeneratorOptions.KEYGENERATOR_CONSISTENT_LOGICAL_TIMESTAMP_ENABLED);
     Set<String> partitionColumns = getPartitionColumns(props);
-    String payloadClassName = StringUtils.isNullOrEmpty(cfg.payloadClassName) ? HoodieRecordPayload.getAvroPayloadForMergeMode(cfg.recordMergeMode) : cfg.payloadClassName;
+    String payloadClassName = StringUtils.isNullOrEmpty(cfg.payloadClassName)
+        ? HoodieRecordPayload.getAvroPayloadForMergeMode(cfg.recordMergeMode, cfg.payloadClassName)
+        : cfg.payloadClassName;
     return avroRDDOptional.map(avroRDD -> {
       SerializableSchema avroSchema = new SerializableSchema(schemaProvider.getTargetSchema());
       SerializableSchema processedAvroSchema = new SerializableSchema(isDropPartitionColumns(props) ? HoodieAvroUtils.removeMetadataFields(avroSchema.get()) : avroSchema.get());

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerTestBase.java
Patch:
@@ -622,7 +622,8 @@ static HoodieDeltaStreamer.Config makeConfig(String basePath, WriteOperationType
       }
       cfg.allowCommitOnNoCheckpointChange = allowCommitOnNoCheckpointChange;
       Triple<RecordMergeMode, String, String> mergeCfgs =
-          HoodieTableConfig.inferCorrectMergingBehavior(cfg.recordMergeMode, cfg.payloadClassName, cfg.recordMergeStrategyId);
+          HoodieTableConfig.inferCorrectMergingBehavior(
+              cfg.recordMergeMode, cfg.payloadClassName, cfg.recordMergeStrategyId, cfg.sourceOrderingField);
       cfg.recordMergeMode = mergeCfgs.getLeft();
       cfg.payloadClassName = mergeCfgs.getMiddle();
       cfg.recordMergeStrategyId = mergeCfgs.getRight();

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -192,7 +192,7 @@ private void addRecordMerger(HoodieRecordType type, List<String> hoodieConfig) {
       opts.put(HoodieWriteConfig.RECORD_MERGE_IMPL_CLASSES.key(), DefaultSparkRecordMerger.class.getName());
       opts.put(HoodieStorageConfig.LOGFILE_DATA_BLOCK_FORMAT.key(), "parquet");
       opts.put(HoodieWriteConfig.RECORD_MERGE_MODE.key(), RecordMergeMode.CUSTOM.name());
-      opts.put(HoodieWriteConfig.RECORD_MERGE_STRATEGY_ID.key(), HoodieRecordMerger.DEFAULT_MERGE_STRATEGY_UUID);
+      opts.put(HoodieWriteConfig.RECORD_MERGE_STRATEGY_ID.key(), HoodieRecordMerger.EVENT_TIME_BASED_MERGE_STRATEGY_UUID);
       for (Map.Entry<String, String> entry : opts.entrySet()) {
         hoodieConfig.add(String.format("%s=%s", entry.getKey(), entry.getValue()));
       }
@@ -561,7 +561,7 @@ public void testSchemaEvolution(String tableType, boolean useUserProvidedSchema,
         PROPS_FILENAME_TEST_SOURCE, false, true, false, null, tableType);
     addRecordMerger(recordType, cfg.configs);
     cfg.payloadClassName = DefaultHoodieRecordPayload.class.getName();
-    cfg.recordMergeStrategyId = HoodieRecordMerger.DEFAULT_MERGE_STRATEGY_UUID;
+    cfg.recordMergeStrategyId = HoodieRecordMerger.EVENT_TIME_BASED_MERGE_STRATEGY_UUID;
     cfg.recordMergeMode = RecordMergeMode.EVENT_TIME_ORDERING;
     cfg.configs.add("hoodie.streamer.schemaprovider.source.schema.file=" + basePath + "/source.avsc");
     cfg.configs.add("hoodie.streamer.schemaprovider.target.schema.file=" + basePath + "/source.avsc");

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -24,11 +24,11 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieAvroRecord;
+import org.apache.hudi.common.model.HoodieEmptyRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.common.model.HoodieSparkRecord;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.util.Option;
@@ -242,7 +242,7 @@ public static HoodieWriteResult doDeleteOperation(SparkRDDWriteClient client, Ja
       JavaRDD<HoodieRecord> records = hoodieKeysAndLocations.map(tuple -> {
         HoodieRecord record = recordType == HoodieRecord.HoodieRecordType.AVRO
             ? new HoodieAvroRecord(tuple._1, new EmptyHoodieRecordPayload())
-            : new HoodieSparkRecord(tuple._1, null, false);
+            : new HoodieEmptyRecord(tuple._1, HoodieRecord.HoodieRecordType.SPARK);
         record.setCurrentLocation(tuple._2.get());
         return record;
       });

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestCleanPlanExecutor.java
Patch:
@@ -342,7 +342,8 @@ public void testKeepLatestFileVersions() throws Exception {
   public void testKeepLatestFileVersionsWithBootstrapFileClean() throws Exception {
     HoodieWriteConfig config =
         HoodieWriteConfig.newBuilder().withPath(basePath)
-            .withMetadataConfig(HoodieMetadataConfig.newBuilder().withMetadataIndexColumnStats(false).build())
+            .withMetadataConfig(HoodieMetadataConfig.newBuilder().withMetadataIndexColumnStats(false)
+                .withMetadataIndexPartitionStats(false).build())
             .withCleanConfig(HoodieCleanConfig.newBuilder()
                 .withCleanBootstrapBaseFileEnabled(true)
                 .withCleanerParallelism(1)
@@ -377,6 +378,7 @@ public void testKeepLatestFileVersionsWithBootstrapFileClean() throws Exception
       Map<String, List<Pair<String, Integer>>> c2PartitionToFilesNameLengthMap = new HashMap<>();
       c2PartitionToFilesNameLengthMap.put(p0, Arrays.asList(Pair.of(file1P0C0, 101), Pair.of(file2P0C1, 100)));
       c2PartitionToFilesNameLengthMap.put(p1, Arrays.asList(Pair.of(file1P1C0, 201), Pair.of(file2P1C1, 200)));
+      testTable = HoodieMetadataTestTable.of(metaClient, getMetadataWriter(config), Option.of(context));
       testTable.doWriteOperation("00000000000003", WriteOperationType.UPSERT, Collections.emptyList(),
           c2PartitionToFilesNameLengthMap, false, false);
 

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -890,6 +890,7 @@ public Builder withDropMetadataIndex(String indexName) {
     public HoodieMetadataConfig build() {
       metadataConfig.setDefaultValue(ENABLE, getDefaultMetadataEnable(engineType));
       metadataConfig.setDefaultValue(ENABLE_METADATA_INDEX_COLUMN_STATS, getDefaultColStatsEnable(engineType));
+      metadataConfig.setDefaultValue(ENABLE_METADATA_INDEX_PARTITION_STATS, metadataConfig.isColumnStatsIndexEnabled());
       // fix me: disable when schema on read is enabled.
       metadataConfig.setDefaults(HoodieMetadataConfig.class.getName());
       return metadataConfig;

File: hudi-common/src/main/java/org/apache/hudi/metadata/MetadataPartitionType.java
Patch:
@@ -27,7 +27,6 @@
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.HoodieIndexDefinition;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.index.expression.HoodieExpressionIndex;
@@ -309,8 +308,8 @@ private static void constructColumnStatsMetadataPayload(HoodieMetadataPayload pa
           // AVRO-2377 1.9.2 Modified the type of org.apache.avro.Schema#FIELD_RESERVED to Collections.unmodifiableSet.
           // This causes Kryo to fail when deserializing a GenericRecord, See HUDI-5484.
           // We should avoid using GenericRecord and convert GenericRecord into a serializable type.
-          .setMinValue(wrapValueIntoAvro(unwrapAvroValueWrapper(columnStatsRecord.get(COLUMN_STATS_FIELD_MIN_VALUE), true, Option.of(COLUMN_STATS_FIELD_MIN_VALUE), Option.of(record))))
-          .setMaxValue(wrapValueIntoAvro(unwrapAvroValueWrapper(columnStatsRecord.get(COLUMN_STATS_FIELD_MAX_VALUE), true, Option.of(COLUMN_STATS_FIELD_MAX_VALUE), Option.of(record))))
+          .setMinValue(wrapValueIntoAvro(unwrapAvroValueWrapper(columnStatsRecord.get(COLUMN_STATS_FIELD_MIN_VALUE))))
+          .setMaxValue(wrapValueIntoAvro(unwrapAvroValueWrapper(columnStatsRecord.get(COLUMN_STATS_FIELD_MAX_VALUE))))
           .setValueCount((Long) columnStatsRecord.get(COLUMN_STATS_FIELD_VALUE_COUNT))
           .setNullCount((Long) columnStatsRecord.get(COLUMN_STATS_FIELD_NULL_COUNT))
           .setTotalSize((Long) columnStatsRecord.get(COLUMN_STATS_FIELD_TOTAL_SIZE))

File: hudi-common/src/test/java/org/apache/hudi/avro/TestHoodieAvroUtils.java
Patch:
@@ -66,7 +66,6 @@
 import java.nio.ByteBuffer;
 import java.sql.Date;
 import java.sql.Timestamp;
-import java.time.Instant;
 import java.time.LocalDate;
 import java.time.temporal.ChronoUnit;
 import java.util.ArrayDeque;
@@ -715,7 +714,7 @@ public void testWrapAndUnwrapJavaValues(Comparable value, Class expectedWrapper)
       assertEquals(((Timestamp) value).getTime() * 1000L,
           ((GenericRecord) wrapperValue).get(0));
       assertEquals(((Timestamp) value).getTime(),
-          ((Instant) unwrapAvroValueWrapper(wrapperValue)).toEpochMilli());
+          ((Timestamp) unwrapAvroValueWrapper(wrapperValue)).getTime());
     } else if (value instanceof Date) {
       assertEquals((int) ChronoUnit.DAYS.between(
               LocalDate.ofEpochDay(0), ((Date) value).toLocalDate()),

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/table/log/block/TestHoodieDeleteBlock.java
Patch:
@@ -32,7 +32,6 @@
 import java.io.IOException;
 import java.math.BigDecimal;
 import java.sql.Timestamp;
-import java.time.Instant;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Comparator;
@@ -133,7 +132,7 @@ public void testDeleteBlockWithValidation(DeleteRecord[] deleteRecords) throws I
       if (deleteRecords[i].getOrderingValue() != null) {
         if (deleteRecords[i].getOrderingValue() instanceof Timestamp) {
           assertEquals(((Timestamp) deleteRecords[i].getOrderingValue()).getTime(),
-              ((Instant) deserializedDeleteRecords[i].getOrderingValue()).toEpochMilli());
+              ((Timestamp) deserializedDeleteRecords[i].getOrderingValue()).getTime());
         } else if (deleteRecords[i].getOrderingValue() instanceof BigDecimal) {
           assertEquals("0.000000000000000",
               ((BigDecimal) deleteRecords[i].getOrderingValue())

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/ColumnStatsIndexHelper.java
Patch:
@@ -241,7 +241,7 @@ public static Dataset<Row> buildColumnStatsTableFor(
 
     StructType indexSchema = ColumnStatsIndexSupport$.MODULE$.composeIndexSchema(
         JavaScalaConverters.<String>convertJavaListToScalaSeq(columnNames),
-        JavaScalaConverters.convertJavaListToScalaList(columnNames).toSet(),
+        JavaScalaConverters.convertJavaListToScalaList(columnNames),
           StructType$.MODULE$.apply(orderedColumnSchemas)
     )._1;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -1096,9 +1096,9 @@ engineContext, dataWriteConfig, commitMetadata, instantTime, dataMetaClient, get
 
       // Updates for record index are created by parsing the WriteStatus which is a hudi-client object. Hence, we cannot yet move this code
       // to the HoodieTableMetadataUtil class in hudi-common.
-      if (dataWriteConfig.isRecordIndexEnabled()) {
-        HoodieData<HoodieRecord> additionalUpdates = getRecordIndexAdditionalUpserts(partitionToRecordMap.get(MetadataPartitionType.RECORD_INDEX.getPartitionPath()), commitMetadata);
-        partitionToRecordMap.put(RECORD_INDEX.getPartitionPath(), partitionToRecordMap.get(MetadataPartitionType.RECORD_INDEX.getPartitionPath()).union(additionalUpdates));
+      if (getMetadataPartitionsToUpdate().contains(RECORD_INDEX.getPartitionPath())) {
+        HoodieData<HoodieRecord> additionalUpdates = getRecordIndexAdditionalUpserts(partitionToRecordMap.get(RECORD_INDEX.getPartitionPath()), commitMetadata);
+        partitionToRecordMap.put(RECORD_INDEX.getPartitionPath(), partitionToRecordMap.get(RECORD_INDEX.getPartitionPath()).union(additionalUpdates));
       }
       updateExpressionIndexIfPresent(commitMetadata, instantTime, partitionToRecordMap);
       updateSecondaryIndexIfPresent(commitMetadata, partitionToRecordMap, instantTime);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/strategy/TestHoodieCompactionStrategy.java
Patch:
@@ -95,7 +95,7 @@ public void testBoundedIOSimple(boolean enableIncrTableService) {
     List<HoodieCompactionOperation> returned = resPair.getLeft();
     List<String> missingPartitions = resPair.getRight();
     if (enableIncrTableService) {
-      assertEquals(1, missingPartitions.stream().distinct().count());
+      assertTrue(missingPartitions.stream().distinct().count() > 0);
     }
     assertTrue(returned.size() < operations.size(), "BoundedIOCompaction should have resulted in fewer compactions");
     assertEquals(2, returned.size(), "BoundedIOCompaction should have resulted in 2 compactions being chosen");

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/BaseConsistentHashingBucketClusteringPlanStrategy.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.util.ValidationUtils;
+import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.common.util.collection.Triple;
 import org.apache.hudi.config.HoodieClusteringConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -95,7 +96,7 @@ public boolean checkPrecondition() {
    * Generate cluster group based on split, merge and sort rules
    */
   @Override
-  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {
+  protected Pair<Stream<HoodieClusteringGroup>, Boolean> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {
     Option<HoodieConsistentHashingMetadata> metadata = ConsistentBucketIndexUtils.loadMetadata(getHoodieTable(), partitionPath);
     ValidationUtils.checkArgument(metadata.isPresent(), "Metadata is empty for partition: " + partitionPath);
     ConsistentBucketIdentifier identifier = new ConsistentBucketIdentifier(metadata.get());
@@ -128,7 +129,7 @@ protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String
             .build();
       }).collect(Collectors.toList()));
     }
-    return ret.stream();
+    return Pair.of(ret.stream(), true);
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java
Patch:
@@ -32,7 +32,9 @@
 import org.apache.hudi.config.HoodieClusteringConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
+import org.apache.hudi.table.action.cluster.ClusteringPlanActionExecutor;
 import org.apache.hudi.table.action.cluster.ClusteringPlanPartitionFilterMode;
+import org.apache.hudi.util.Lazy;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -101,7 +103,7 @@ public ClusteringPlanStrategy(HoodieTable table, HoodieEngineContext engineConte
    *
    * If there is no data available to cluster, return None.
    */
-  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();
+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan(ClusteringPlanActionExecutor executor, Lazy<List<String>> partitions);
 
   /**
    * Check if the clustering can proceed. If not (i.e., return false), the PlanStrategy will generate an empty plan to stop the scheduling.

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/FlinkSizeBasedClusteringPlanStrategy.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.StringUtils;
+import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.cluster.strategy.PartitionAwareClusteringPlanStrategy;
@@ -56,7 +57,7 @@ public FlinkSizeBasedClusteringPlanStrategy(HoodieTable table,
   }
 
   @Override
-  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {
+  protected Pair<Stream<HoodieClusteringGroup>, Boolean> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {
     return super.buildClusteringGroupsForPartition(partitionPath, fileSlices);
   }
 

File: hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/table/action/cluster/strategy/TestFlinkSizeBasedClusteringPlanStrategy.java
Patch:
@@ -74,7 +74,7 @@ public void testBuildClusteringGroupsForPartitionOnlyOneFile() {
           .build())
         .build();
     PartitionAwareClusteringPlanStrategy strategyWithSortEnabled = new FlinkSizeBasedClusteringPlanStrategy(table, context, configWithSortEnabled);
-    Stream<HoodieClusteringGroup> groupStreamSort = strategyWithSortEnabled.buildClusteringGroupsForPartition(partition,fileSliceGroups);
+    Stream<HoodieClusteringGroup> groupStreamSort = (Stream<HoodieClusteringGroup>) strategyWithSortEnabled.buildClusteringGroupsForPartition(partition,fileSliceGroups).getLeft();
     assertEquals(1, groupStreamSort.count());
 
     // test buildClusteringGroupsForPartition without ClusteringSortColumns config
@@ -86,7 +86,7 @@ public void testBuildClusteringGroupsForPartitionOnlyOneFile() {
           .build())
         .build();
     PartitionAwareClusteringPlanStrategy strategyWithSortDisabled = new FlinkSizeBasedClusteringPlanStrategy(table, context, configWithSortDisabled);
-    Stream<HoodieClusteringGroup> groupStreamWithOutSort = strategyWithSortDisabled.buildClusteringGroupsForPartition(partition,fileSliceGroups);
+    Stream<HoodieClusteringGroup> groupStreamWithOutSort = (Stream<HoodieClusteringGroup>) strategyWithSortDisabled.buildClusteringGroupsForPartition(partition,fileSliceGroups).getLeft();
     assertEquals(0, groupStreamWithOutSort.count());
   }
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/JavaSizeBasedClusteringPlanStrategy.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.StringUtils;
+import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.cluster.strategy.PartitionAwareClusteringPlanStrategy;
@@ -57,7 +58,7 @@ public JavaSizeBasedClusteringPlanStrategy(HoodieTable table,
   }
 
   @Override
-  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {
+  protected Pair<Stream<HoodieClusteringGroup>, Boolean> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {
     return super.buildClusteringGroupsForPartition(partitionPath, fileSlices);
   }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkSingleFileSortPlanStrategy.java
Patch:
@@ -43,13 +43,13 @@ public SparkSingleFileSortPlanStrategy(HoodieTable table, HoodieEngineContext en
   }
 
   @Override
-  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {
+  protected Pair<Stream<HoodieClusteringGroup>, Boolean> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {
     List<Pair<List<FileSlice>, Integer>> fileSliceGroups = fileSlices.stream()
         .map(fileSlice -> Pair.of(Collections.singletonList(fileSlice), 1)).collect(Collectors.toList());
-    return fileSliceGroups.stream().map(fileSliceGroup -> HoodieClusteringGroup.newBuilder()
+    return Pair.of(fileSliceGroups.stream().map(fileSliceGroup -> HoodieClusteringGroup.newBuilder()
         .setSlices(getFileSliceInfo(fileSliceGroup.getLeft()))
         .setNumOutputFileGroups(fileSliceGroup.getRight())
         .setMetrics(buildMetrics(fileSliceGroup.getLeft()))
-        .build());
+        .build()), true);
   }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkSizeBasedClusteringPlanStrategy.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.StringUtils;
+import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.cluster.strategy.PartitionAwareClusteringPlanStrategy;
@@ -57,7 +58,7 @@ public SparkSizeBasedClusteringPlanStrategy(HoodieTable table,
   }
 
   @Override
-  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {
+  protected Pair<Stream<HoodieClusteringGroup>, Boolean> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {
     return super.buildClusteringGroupsForPartition(partitionPath, fileSlices);
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/clustering/plan/strategy/TestSparkSizeBasedClusteringPlanStrategy.java
Patch:
@@ -67,7 +67,7 @@ public void testBuildClusteringGroup() {
     fileSlices.add(createFileSlice(400));
     fileSlices.add(createFileSlice(400));
 
-    Stream<HoodieClusteringGroup> clusteringGroupStream = planStrategy.buildClusteringGroupsForPartition("p0", fileSlices);
+    Stream<HoodieClusteringGroup> clusteringGroupStream = (Stream<HoodieClusteringGroup>) planStrategy.buildClusteringGroupsForPartition("p0", fileSlices).getLeft();
     List<HoodieClusteringGroup> clusteringGroups = clusteringGroupStream.collect(Collectors.toList());
 
     // FileSlices will be divided into two clusteringGroups

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/cluster/strategy/TestSparkBuildClusteringGroupsForPartition.java
Patch:
@@ -73,7 +73,7 @@ public void testBuildClusteringGroupsForPartitionOnlyOneFile() {
           .build())
         .build();
     PartitionAwareClusteringPlanStrategy strategyWithSortEnabled = new SparkSizeBasedClusteringPlanStrategy(table, context, configWithSortEnabled);
-    Stream<HoodieClusteringGroup> groupStreamSort = strategyWithSortEnabled.buildClusteringGroupsForPartition(partition,fileSliceGroups);
+    Stream<HoodieClusteringGroup> groupStreamSort = (Stream<HoodieClusteringGroup>) strategyWithSortEnabled.buildClusteringGroupsForPartition(partition,fileSliceGroups).getLeft();
     assertEquals(1, groupStreamSort.count());
 
     // test buildClusteringGroupsForPartition without ClusteringSortColumns config
@@ -85,7 +85,7 @@ public void testBuildClusteringGroupsForPartitionOnlyOneFile() {
           .build())
         .build();
     PartitionAwareClusteringPlanStrategy strategyWithSortDisabled = new SparkSizeBasedClusteringPlanStrategy(table, context, configWithSortDisabled);
-    Stream<HoodieClusteringGroup> groupStreamWithOutSort = strategyWithSortDisabled.buildClusteringGroupsForPartition(partition,fileSliceGroups);
+    Stream<HoodieClusteringGroup> groupStreamWithOutSort = (Stream<HoodieClusteringGroup>) strategyWithSortDisabled.buildClusteringGroupsForPartition(partition,fileSliceGroups).getLeft();
     assertEquals(0, groupStreamWithOutSort.count());
   }
 
@@ -107,7 +107,7 @@ public void testBuildClusteringGroupsWithLimitScan() {
                 .build())
         .build();
     PartitionAwareClusteringPlanStrategy clusteringPlanStrategy = new SparkSizeBasedClusteringPlanStrategy(table, context, writeConfig);
-    Stream<HoodieClusteringGroup> groups = clusteringPlanStrategy.buildClusteringGroupsForPartition(partition,fileSliceGroups);
+    Stream<HoodieClusteringGroup> groups = (Stream<HoodieClusteringGroup>) clusteringPlanStrategy.buildClusteringGroupsForPartition(partition,fileSliceGroups).getLeft();
     assertEquals(2, groups.count());
   }
 

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/CompactionTestUtils.java
Patch:
@@ -192,7 +192,7 @@ public static HoodieCompactionPlan createCompactionPlan(HoodieTableMetaClient me
       }
     }).collect(Collectors.toList());
     return new HoodieCompactionPlan(ops.isEmpty() ? null : ops, new HashMap<>(),
-        CompactionUtils.LATEST_COMPACTION_METADATA_VERSION, null, null);
+        CompactionUtils.LATEST_COMPACTION_METADATA_VERSION, null, null, null);
   }
 
   /**

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/utils/TestClusteringUtil.java
Patch:
@@ -131,7 +131,7 @@ void validateClusteringScheduling() throws Exception {
   private String generateClusteringPlan() {
     HoodieClusteringGroup group = new HoodieClusteringGroup();
     HoodieClusteringPlan plan = new HoodieClusteringPlan(Collections.singletonList(group),
-        HoodieClusteringStrategy.newBuilder().build(), Collections.emptyMap(), 1, false);
+        HoodieClusteringStrategy.newBuilder().build(), Collections.emptyMap(), 1, false, null);
     HoodieRequestedReplaceMetadata metadata = new HoodieRequestedReplaceMetadata(WriteOperationType.CLUSTER.name(),
         plan, Collections.emptyMap(), 1);
     String instantTime = table.getMetaClient().createNewInstantTime();

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/utils/TestCompactionUtil.java
Patch:
@@ -174,7 +174,7 @@ void testInferMetadataConf(boolean metadataEnabled) throws Exception {
    */
   private String generateCompactionPlan() {
     HoodieCompactionOperation operation = new HoodieCompactionOperation();
-    HoodieCompactionPlan plan = new HoodieCompactionPlan(Collections.singletonList(operation), Collections.emptyMap(), 1, null, null);
+    HoodieCompactionPlan plan = new HoodieCompactionPlan(Collections.singletonList(operation), Collections.emptyMap(), 1, null, null, null);
     String instantTime = table.getMetaClient().createNewInstantTime();
     HoodieInstant compactionInstant =
         INSTANT_GENERATOR.createNewInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, instantTime);

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestBootstrapReadBase.java
Patch:
@@ -286,6 +286,6 @@ protected static Dataset<Row> applyPartition(Dataset<Row> df, Integer n) {
     return df.withColumn("partpath" + n,
         functions.md5(functions.concat_ws("," + n + ",",
             df.col("partition_path"),
-            functions.hash(df.col("_row_key")).mod(n))));
+            functions.hash(df.col("partition_path")).mod(n))));
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/ttl/strategy/PartitionTTLStrategy.java
Patch:
@@ -61,11 +61,11 @@ public PartitionTTLStrategy(HoodieTable hoodieTable, String instantTime) {
    * @return all partitions paths for the dataset.
    */
   protected List<String> getPartitionPathsForTTL() {
-    String partitionSelected = writeConfig.getClusteringPartitionSelected();
+    String partitionSelected = writeConfig.getPartitionTTLPartitionSelected();
     HoodieTimer timer = HoodieTimer.start();
     List<String> partitionsForTTL;
     if (StringUtils.isNullOrEmpty(partitionSelected)) {
-      // Return All partition paths
+      // Return all partition paths.
       partitionsForTTL = FSUtils.getAllPartitionPaths(
           hoodieTable.getContext(), hoodieTable.getStorage(), writeConfig.getMetadataConfig(), writeConfig.getBasePath());
     } else {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteOperatorCoordinator.java
Patch:
@@ -265,10 +265,10 @@ public void notifyCheckpointComplete(long checkpointId) {
           scheduleTableServices(committed);
 
           if (committed) {
-            // start new instant.
-            startInstant();
             // sync Hive if is enabled
             syncHiveAsync();
+            // start new instant.
+            startInstant();
           }
         }, "commits the instant %s", this.instant
     );

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestHoodieIncrSource.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
-import org.apache.hudi.common.config.HoodieReaderConfig;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.HoodieAvroPayload;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -608,8 +607,6 @@ private void readAndAssert(IncrSourceHelper.MissingCheckpointStrategy missingChe
     }
     properties.setProperty("hoodie.streamer.source.hoodieincr.path", basePath());
     properties.setProperty("hoodie.streamer.source.hoodieincr.missing.checkpoint.strategy", missingCheckpointStrategy.name());
-    // TODO: [HUDI-7081] get rid of this
-    properties.setProperty(HoodieReaderConfig.FILE_GROUP_READER_ENABLED.key(), "false");
     properties.putAll(extraProps);
     snapshotCheckPointImplClassOpt.map(className ->
         properties.setProperty(SnapshotLoadQuerySplitter.Config.SNAPSHOT_LOAD_QUERY_SPLITTER_CLASS_NAME, className));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataWriteUtils.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.marker.MarkerType;
 import org.apache.hudi.common.util.ValidationUtils;
+import org.apache.hudi.common.util.VisibleForTesting;
 import org.apache.hudi.config.HoodieArchivalConfig;
 import org.apache.hudi.config.HoodieCleanConfig;
 import org.apache.hudi.config.HoodieCompactionConfig;
@@ -80,6 +81,7 @@ public class HoodieMetadataWriteUtils {
    * @param writeConfig                {@code HoodieWriteConfig} of the main dataset writer
    * @param failedWritesCleaningPolicy Cleaning policy on failed writes
    */
+  @VisibleForTesting
   public static HoodieWriteConfig createMetadataWriteConfig(
       HoodieWriteConfig writeConfig, HoodieFailedWritesCleaningPolicy failedWritesCleaningPolicy) {
     String tableName = writeConfig.getTableName() + METADATA_TABLE_NAME_SUFFIX;

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/testutils/TestHoodieMetadataBase.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.client.timeline.versioning.v2.TimelineArchiverV2;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.config.HoodieStorageConfig;
+import org.apache.hudi.common.engine.EngineType;
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.WriteOperationType;
@@ -105,6 +106,7 @@ public void init(HoodieTableType tableType, Option<HoodieWriteConfig> writeConfi
         ? writeConfig.get() : getWriteConfigBuilder(HoodieFailedWritesCleaningPolicy.EAGER, true,
         enableMetadataTable, enableMetrics, true,
         validateMetadataPayloadStateConsistency)
+        .withEngineType(EngineType.JAVA)
         .build();
     initWriteConfigAndMetatableWriter(this.writeConfig, enableMetadataTable);
   }
@@ -307,6 +309,7 @@ protected HoodieWriteConfig.Builder getWriteConfigBuilder(HoodieFailedWritesClea
             .withMetadataIndexColumnStats(false)
             .enableMetrics(enableMetrics)
             .ignoreSpuriousDeletes(validateMetadataPayloadConsistency)
+            .withMetadataIndexColumnStats(false) // HUDI-8774
             .build())
         .withMetricsConfig(HoodieMetricsConfig.newBuilder().on(enableMetrics)
             .withExecutorMetrics(enableMetrics).withReporterType(MetricsReporterType.INMEMORY.name()).build())

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java
Patch:
@@ -616,6 +616,7 @@ public void testCleanEmptyInstants() throws Exception {
   public void testCleanWithReplaceCommits() throws Exception {
     HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)
         .withMetadataConfig(HoodieMetadataConfig.newBuilder()
+            .withMetadataIndexColumnStats(false)
             .withMaxNumDeltaCommitsBeforeCompaction(1).build())
         .withCleanConfig(HoodieCleanConfig.newBuilder()
             .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS)
@@ -1042,6 +1043,7 @@ public void testCleanPreviousCorruptedCleanFiles() throws IOException {
   public void testRerunFailedClean(boolean simulateMetadataFailure) throws Exception {
     HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)
         .withMetadataConfig(HoodieMetadataConfig.newBuilder()
+            .withMetadataIndexColumnStats(false)
             .withMaxNumDeltaCommitsBeforeCompaction(1).build())
         .withCleanConfig(HoodieCleanConfig.newBuilder()
             .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(2).build())

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestCleanPlanExecutor.java
Patch:
@@ -342,7 +342,7 @@ public void testKeepLatestFileVersions() throws Exception {
   public void testKeepLatestFileVersionsWithBootstrapFileClean() throws Exception {
     HoodieWriteConfig config =
         HoodieWriteConfig.newBuilder().withPath(basePath)
-            .withMetadataConfig(HoodieMetadataConfig.newBuilder().build())
+            .withMetadataConfig(HoodieMetadataConfig.newBuilder().withMetadataIndexColumnStats(false).build())
             .withCleanConfig(HoodieCleanConfig.newBuilder()
                 .withCleanBootstrapBaseFileEnabled(true)
                 .withCleanerParallelism(1)

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieColumnRangeMetadata.java
Patch:
@@ -156,8 +156,8 @@ public static HoodieColumnRangeMetadata<Comparable> fromColumnStats(HoodieMetada
     return HoodieColumnRangeMetadata.<Comparable>create(
         columnStats.getFileName(),
         columnStats.getColumnName(),
-        unwrapAvroValueWrapper(columnStats.getMinValue()),
-        unwrapAvroValueWrapper(columnStats.getMaxValue()),
+        unwrapAvroValueWrapper(columnStats.getMinValue()), // misses for special handling.
+        unwrapAvroValueWrapper(columnStats.getMaxValue()), // misses for special handling.
         columnStats.getNullCount(),
         columnStats.getValueCount(),
         columnStats.getTotalSize(),

File: hudi-common/src/main/java/org/apache/hudi/metadata/MetadataPartitionType.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.HoodieIndexDefinition;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
+import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ValidationUtils;
 
 import org.apache.avro.generic.GenericRecord;
@@ -301,8 +302,8 @@ private static void constructColumnStatsMetadataPayload(HoodieMetadataPayload pa
           // AVRO-2377 1.9.2 Modified the type of org.apache.avro.Schema#FIELD_RESERVED to Collections.unmodifiableSet.
           // This causes Kryo to fail when deserializing a GenericRecord, See HUDI-5484.
           // We should avoid using GenericRecord and convert GenericRecord into a serializable type.
-          .setMinValue(wrapValueIntoAvro(unwrapAvroValueWrapper(columnStatsRecord.get(COLUMN_STATS_FIELD_MIN_VALUE))))
-          .setMaxValue(wrapValueIntoAvro(unwrapAvroValueWrapper(columnStatsRecord.get(COLUMN_STATS_FIELD_MAX_VALUE))))
+          .setMinValue(wrapValueIntoAvro(unwrapAvroValueWrapper(columnStatsRecord.get(COLUMN_STATS_FIELD_MIN_VALUE), true, Option.of(COLUMN_STATS_FIELD_MIN_VALUE), Option.of(record))))
+          .setMaxValue(wrapValueIntoAvro(unwrapAvroValueWrapper(columnStatsRecord.get(COLUMN_STATS_FIELD_MAX_VALUE), true, Option.of(COLUMN_STATS_FIELD_MAX_VALUE), Option.of(record))))
           .setValueCount((Long) columnStatsRecord.get(COLUMN_STATS_FIELD_VALUE_COUNT))
           .setNullCount((Long) columnStatsRecord.get(COLUMN_STATS_FIELD_NULL_COUNT))
           .setTotalSize((Long) columnStatsRecord.get(COLUMN_STATS_FIELD_TOTAL_SIZE))

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/TestWriteCopyOnWrite.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.sink;
 
 import org.apache.hudi.client.HoodieFlinkWriteClient;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.WriteConcurrencyMode;
@@ -614,6 +615,8 @@ public void testWriteMultiWriterPartialOverlapping(WriteConcurrencyMode writeCon
   public void testReuseEmbeddedServer() throws IOException {
     conf.setInteger("hoodie.filesystem.view.remote.timeout.secs", 500);
     conf.setString("hoodie.metadata.enable","true");
+    conf.setString(HoodieMetadataConfig.ENABLE_METADATA_INDEX_PARTITION_STATS.key(), "false"); // HUDI-8814
+
     HoodieFlinkWriteClient writeClient = null;
     HoodieFlinkWriteClient writeClient2 = null;
 

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/TestWriteMergeOnReadWithCompact.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.sink;
 
 import org.apache.hudi.client.HoodieFlinkWriteClient;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.PartialUpdateAvroPayload;
 import org.apache.hudi.common.model.WriteConcurrencyMode;
@@ -96,6 +97,7 @@ public void testNonBlockingConcurrencyControlWithPartialUpdatePayload() throws E
     // disable schedule compaction in writers
     conf.setBoolean(FlinkOptions.COMPACTION_SCHEDULE_ENABLED, false);
     conf.setBoolean(FlinkOptions.PRE_COMBINE, true);
+    conf.setString(HoodieMetadataConfig.ENABLE_METADATA_INDEX_PARTITION_STATS.key(), "false"); // HUDI-8814
 
     // start pipeline1 and insert record: [id1,Danny,null,1,par1], suspend the tx commit
     List<RowData> dataset1 = Collections.singletonList(
@@ -290,6 +292,7 @@ public void testBulkInsertInSequenceWithNonBlockingConcurrencyControl() throws E
     // disable schedule compaction in writers
     conf.setBoolean(FlinkOptions.COMPACTION_SCHEDULE_ENABLED, false);
     conf.setBoolean(FlinkOptions.PRE_COMBINE, true);
+    conf.setString(HoodieMetadataConfig.ENABLE_METADATA_INDEX_PARTITION_STATS.key(), "false");
 
     Configuration conf1 = conf.clone();
     conf1.setString(FlinkOptions.OPERATION, "BULK_INSERT");

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -897,6 +897,7 @@ public void testMetadataTableCompactionWithPendingInstants() throws Exception {
             .enable(true)
             .enableMetrics(false)
             .withMaxNumDeltaCommitsBeforeCompaction(4)
+            .withMetadataIndexColumnStats(false)
             .build()).build();
     initWriteConfigAndMetatableWriter(writeConfig, true);
     doWriteOperation(testTable, metaClient.createNewInstantTime(), INSERT);
@@ -2224,7 +2225,7 @@ public void testMetadataMultiWriter() throws Exception {
 
     // Ensure all commits were synced to the Metadata Table
     HoodieTableMetaClient metadataMetaClient = createMetaClient(metadataTableBasePath);
-    assertEquals(metadataMetaClient.getActiveTimeline().getDeltaCommitTimeline().filterCompletedInstants().countInstants(), 5);
+    assertEquals(metadataMetaClient.getActiveTimeline().getDeltaCommitTimeline().filterCompletedInstants().countInstants(), 6);
     assertTrue(metadataMetaClient.getActiveTimeline().containsInstant(INSTANT_GENERATOR.createNewInstant(HoodieInstant.State.COMPLETED, HoodieTimeline.DELTA_COMMIT_ACTION, "0000002")));
     assertTrue(metadataMetaClient.getActiveTimeline().containsInstant(INSTANT_GENERATOR.createNewInstant(HoodieInstant.State.COMPLETED, HoodieTimeline.DELTA_COMMIT_ACTION, "0000003")));
     assertTrue(metadataMetaClient.getActiveTimeline().containsInstant(INSTANT_GENERATOR.createNewInstant(HoodieInstant.State.COMPLETED, HoodieTimeline.DELTA_COMMIT_ACTION, "0000004")));
@@ -2277,7 +2278,7 @@ public void testMultiWriterForDoubleLocking() throws Exception {
       LOG.warn("total commits in metadata table " + metadataMetaClient.getActiveTimeline().getCommitsTimeline().countInstants());
 
       // 6 commits and 2 cleaner commits.
-      assertEquals(metadataMetaClient.getActiveTimeline().getDeltaCommitTimeline().filterCompletedInstants().countInstants(), 8);
+      assertEquals(metadataMetaClient.getActiveTimeline().getDeltaCommitTimeline().filterCompletedInstants().countInstants(), 9);
       assertTrue(metadataMetaClient.getActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants().countInstants() <= 1);
       // Validation
       validateMetadata(writeClient);

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestBootstrap.java
Patch:
@@ -267,7 +267,8 @@ private void testBootstrapCommon(boolean partitioned, boolean deltaCommit, Effec
             .withBootstrapParallelism(3)
             .withBootstrapModeSelector(bootstrapModeSelectorClass)
             .withBootstrapModeForRegexMatch(modeForRegexMatch).build())
-        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true).withMaxNumDeltaCommitsBeforeCompaction(3).build())
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true).withMaxNumDeltaCommitsBeforeCompaction(3)
+            .withMetadataIndexColumnStats(false).build()) // HUDI-8774
         .build();
 
     SparkRDDWriteClient client = new SparkRDDWriteClient(context, config);
@@ -415,7 +416,7 @@ private void checkBootstrapResults(int totalRecords, Schema schema, String insta
     reloadInputFormats();
     List<GenericRecord> records = HoodieMergeOnReadTestUtils.getRecordsUsingInputFormat(
         HadoopFSUtils.getStorageConf(jsc.hadoopConfiguration()),
-        FSUtils.getAllPartitionPaths(context, storage, basePath, HoodieMetadataConfig.DEFAULT_METADATA_ENABLE_FOR_READERS).stream()
+        FSUtils.getAllPartitionPaths(context, storage, basePath, false).stream()
             .map(f -> basePath + "/" + f).collect(Collectors.toList()),
         basePath, roJobConf, false, schema, TRIP_HIVE_COLUMN_TYPES, false, new ArrayList<>());
     assertEquals(totalRecords, records.size());

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestBootstrapReadBase.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.client.bootstrap.selector.FullRecordBootstrapModeSelector;
 import org.apache.hudi.client.bootstrap.selector.MetadataOnlyBootstrapModeSelector;
 import org.apache.hudi.client.bootstrap.translator.DecodedBootstrapPartitionPathTranslator;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.config.HoodieBootstrapConfig;
@@ -94,6 +95,7 @@ protected Map<String, String> basicOptions() {
     options.put(DataSourceWriteOptions.TABLE_TYPE().key(), tableType.name());
     options.put(DataSourceWriteOptions.HIVE_STYLE_PARTITIONING().key(), "true");
     options.put(DataSourceWriteOptions.RECORDKEY_FIELD().key(), "_row_key");
+    options.put(HoodieMetadataConfig.ENABLE_METADATA_INDEX_COLUMN_STATS.key(), "false");
     if (nPartitions == 0) {
       options.put(HoodieWriteConfig.KEYGENERATOR_CLASS_NAME.key(), NonpartitionedKeyGenerator.class.getName());
     } else {
@@ -116,6 +118,7 @@ protected Map<String, String> setBootstrapOptions() {
     Map<String, String> options = basicOptions();
     options.put(DataSourceWriteOptions.OPERATION().key(), DataSourceWriteOptions.BOOTSTRAP_OPERATION_OPT_VAL());
     options.put(HoodieBootstrapConfig.BASE_PATH.key(), bootstrapBasePath);
+    options.put(HoodieMetadataConfig.ENABLE_METADATA_INDEX_COLUMN_STATS.key(), "false");
     if (!dashPartitions) {
       options.put(HoodieBootstrapConfig.PARTITION_PATH_TRANSLATOR_CLASS_NAME.key(), DecodedBootstrapPartitionPathTranslator.class.getName());
     }

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestOrcBootstrap.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hudi.client.bootstrap.selector.MetadataOnlyBootstrapModeSelector;
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.bootstrap.index.BootstrapIndex;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieFileFormat;
@@ -246,6 +247,8 @@ private void testBootstrapCommon(boolean partitioned, boolean deltaCommit, Effec
             .withBootstrapParallelism(3)
             .withBootstrapModeSelector(bootstrapModeSelectorClass)
             .withBootstrapModeForRegexMatch(modeForRegexMatch).build())
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true).withMaxNumDeltaCommitsBeforeCompaction(3)
+            .withMetadataIndexColumnStats(false).build()) // HUDI-8774
         .build();
 
     SparkRDDWriteClient client = new SparkRDDWriteClient(context, config);

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/table/TestHoodieMergeOnReadTable.java
Patch:
@@ -426,6 +426,8 @@ public void testMetadataStatsOnCommit(Boolean rollbackUsingMarkers) throws Excep
         .withAvroSchemaValidate(false)
         .withAllowAutoEvolutionColumnDrop(true)
         .withAutoCommit(false)
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true).withMetadataIndexColumnStats(false).build())
+        // in this test we mock few entries in timeline. hence col stats initialization does not work.
         .build();
 
     setUp(cfg.getProps());

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieMetadataTableValidator.java
Patch:
@@ -286,9 +286,9 @@ public void testSecondaryIndexValidation() throws IOException {
             + "location '" + basePath + "'");
 
     Dataset<Row> rows = getRowDataset(1, "row1", "abc", "p1");
-    rows.write().format("hudi").mode(SaveMode.Append).save(basePath);
+    rows.write().mode(SaveMode.Append).save(basePath);
     rows = getRowDataset(2, "row2", "ghi", "p2");
-    rows.write().format("hudi").mode(SaveMode.Append).save(basePath);
+    rows.write().mode(SaveMode.Append).save(basePath);
     rows = getRowDataset(3, "row3", "def", "p2");
     rows.write().format("hudi").mode(SaveMode.Append).save(basePath);
 
@@ -301,6 +301,7 @@ public void testSecondaryIndexValidation() throws IOException {
     rows.write().format("hudi")
         .option("hoodie.metadata.enable", "true")
         .option("hoodie.metadata.record.index.enable", "true")
+        .option("hoodie.metadata.index.column.stats.enable", "false")
         .mode(SaveMode.Append)
         .save(basePath);
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -490,6 +490,7 @@ public void testBulkInsertsAndUpsertsWithBootstrap(HoodieRecordType recordType)
     cfg.configs.add(String.format("hoodie.datasource.write.keygenerator.class=%s", SimpleKeyGenerator.class.getName()));
     cfg.configs.add("hoodie.datasource.write.hive_style_partitioning=true");
     cfg.configs.add("hoodie.bootstrap.parallelism=5");
+    cfg.configs.add(String.format("%s=false", HoodieMetadataConfig.ENABLE_METADATA_INDEX_COLUMN_STATS.key()));
     cfg.targetBasePath = newDatasetBasePath;
     new HoodieDeltaStreamer(cfg, jsc).sync();
     Dataset<Row> res = sqlContext.read().format("org.apache.hudi").load(newDatasetBasePath);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamerSchemaEvolutionBase.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.TestHoodieSparkUtils;
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.HoodieAvroRecord;
@@ -155,6 +156,8 @@ protected HoodieDeltaStreamer.Config getDeltaStreamerConfig(String[] transformer
 
   protected HoodieDeltaStreamer.Config getDeltaStreamerConfig(String[] transformerClasses, boolean nullForDeletedCols,
                                                               TypedProperties extraProps) throws IOException {
+    extraProps.setProperty(HoodieMetadataConfig.ENABLE_METADATA_INDEX_PARTITION_STATS.key(),"false"); // HUDI-8587
+
     extraProps.setProperty("hoodie.datasource.write.table.type", tableType);
     extraProps.setProperty("hoodie.datasource.write.row.writer.enable", rowWriterEnable.toString());
     extraProps.setProperty(DataSourceWriteOptions.SET_NULL_FOR_MISSING_COLUMNS().key(), Boolean.toString(nullForDeletedCols));

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestHoodieIncrSource.java
Patch:
@@ -506,7 +506,8 @@ public void testPartitionPruningInHoodieIncrSource()
                 .withScheduleInlineCompaction(true)
                 .withMaxNumDeltaCommitsBeforeCompaction(1)
                 .build())
-        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true).build())
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true).withMetadataIndexColumnStats(false).build())
+        // if col stats is enabled, col stats based pruning kicks in and changes expected value in this test.
         .build();
     List<WriteResult> inserts = new ArrayList<>();
     try (SparkRDDWriteClient writeClient = getHoodieWriteClient(writeConfig)) {

File: hudi-common/src/main/java/org/apache/hudi/index/expression/HoodieExpressionIndex.java
Patch:
@@ -35,6 +35,8 @@ public interface HoodieExpressionIndex<S, T> extends Serializable {
   String HOODIE_EXPRESSION_INDEX_PARTITION = "_hoodie_expression_index_partition";
   String HOODIE_EXPRESSION_INDEX_FILE_SIZE = "_hoodie_expression_index_file_size";
 
+  String HOODIE_EXPRESSION_INDEX_PARTITION_STAT_PREFIX = "__partition_stat__";
+
   String EXPRESSION_OPTION = "expr";
   String TRIM_STRING_OPTION = "trimString";
   String REGEX_GROUP_INDEX_OPTION = "idx";

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java
Patch:
@@ -1028,7 +1028,7 @@ private HoodieData<HoodieMetadataColumnStats> getPartitionStatsUsingColStats(Hoo
 
       TreeSet<HoodieColumnRangeMetadata<Comparable>> aggregatedColumnStats = aggregateColumnStats(partitionPath, colStats);
       // TODO: fix `isTightBound` flag when stats based on log files are available
-      List<HoodieRecord> partitionStatRecords = HoodieMetadataPayload.createPartitionStatsRecords(partitionPath, new ArrayList<>(aggregatedColumnStats), false, false)
+      List<HoodieRecord> partitionStatRecords = HoodieMetadataPayload.createPartitionStatsRecords(partitionPath, new ArrayList<>(aggregatedColumnStats), false, false, Option.empty())
           .collect(Collectors.toList());
       return partitionStatRecords.stream()
           .map(record -> {

File: hudi-common/src/main/java/org/apache/hudi/common/engine/HoodieEngineContext.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.common.data.HoodieAccumulator;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.data.HoodieData.HoodieDataCacheKey;
+import org.apache.hudi.common.data.HoodiePairData;
 import org.apache.hudi.common.function.SerializableBiFunction;
 import org.apache.hudi.common.function.SerializableConsumer;
 import org.apache.hudi.common.function.SerializableFunction;
@@ -67,6 +68,8 @@ public TaskContextSupplier getTaskContextSupplier() {
 
   public abstract <T> HoodieData<T> emptyHoodieData();
 
+  public abstract <K, V> HoodiePairData<K, V> emptyHoodiePairData();
+
   public boolean supportsFileGroupReader() {
     return false;
   }

File: hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java
Patch:
@@ -173,7 +173,7 @@ public static boolean isPreppedWriteOperation(WriteOperationType operationType)
     return operationType == BULK_INSERT_PREPPED || operationType == INSERT_PREPPED | operationType == UPSERT_PREPPED || operationType == DELETE_PREPPED;
   }
 
-  public static boolean isPartitionStatsTightBoundRequired(WriteOperationType operationType) {
+  public static boolean isCompactionOrClustering(WriteOperationType operationType) {
     return operationType == COMPACT || operationType == CLUSTER;
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/timeline/versioning/v1/TimelineArchiverV1.java
Patch:
@@ -143,7 +143,7 @@ public int archiveIfRequired(HoodieEngineContext context, boolean acquireLock) t
       List<HoodieInstant> instantsToArchive = getInstantsToArchive();
       if (!instantsToArchive.isEmpty()) {
         this.writer = openWriter(archiveFilePath.getParent());
-        LOG.info(String.format("Archiving instants {} for table {}", instantsToArchive, config.getBasePath()));
+        LOG.info("Archiving instants {} for table {}", instantsToArchive, config.getBasePath());
         archive(context, instantsToArchive);
         LOG.info("Deleting archived instants {} for table {}", instantsToArchive, config.getBasePath());
         deleteArchivedInstants(instantsToArchive, context);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/common/HoodieSparkEngineContext.java
Patch:
@@ -197,7 +197,7 @@ public Option<String> getProperty(EngineProperty key) {
 
   @Override
   public void setJobStatus(String activeModule, String activityDescription) {
-    javaSparkContext.setJobGroup(activeModule, activityDescription);
+    javaSparkContext.setJobDescription(String.format("%s:%s", activeModule, activityDescription));
   }
 
   @Override

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -568,6 +568,7 @@ protected void postCommit(HoodieTable table, HoodieCommitMetadata metadata, Stri
       // Delete the marker directory for the instant.
       WriteMarkersFactory.get(config.getMarkersType(), table, instantTime)
           .quietDeleteMarkerDir(context, config.getMarkersDeleteParallelism());
+      metrics.updateClusteringTimeLineInstantMetrics(table.getActiveTimeline());
     } finally {
       this.heartbeatClient.stop(instantTime);
     }

File: hudi-common/src/main/java/org/apache/hudi/metadata/SecondaryIndexKeyUtils.java
Patch:
@@ -39,7 +39,7 @@ public static String getSecondaryKeyFromSecondaryIndexKey(String key) {
     return unescapeSpecialChars(key.substring(0, delimiterIndex));
   }
 
-  static String constructSecondaryIndexKey(String secondaryKey, String recordKey) {
+  public static String constructSecondaryIndexKey(String secondaryKey, String recordKey) {
     return escapeSpecialChars(secondaryKey) + SECONDARY_INDEX_RECORD_KEY_SEPARATOR + escapeSpecialChars(recordKey);
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/embedded/EmbeddedTimelineServerHelper.java
Patch:
@@ -54,7 +54,7 @@ public static void updateWriteConfigWithTimelineServer(EmbeddedTimelineService t
                                                          HoodieWriteConfig config) {
     // Allow executor to find this newly instantiated timeline service
     if (config.isEmbeddedTimelineServerEnabled()) {
-      config.setViewStorageConfig(timelineServer.getRemoteFileSystemViewConfig());
+      config.setViewStorageConfig(timelineServer.getRemoteFileSystemViewConfig(config));
     }
   }
 }
\ No newline at end of file

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/client/TestHoodieJavaWriteClientInsert.java
Patch:
@@ -118,7 +118,7 @@ public void testWriteClientAndTableServiceClientWithTimelineServer(
     HoodieJavaWriteClient writeClient;
     if (passInTimelineServer) {
       EmbeddedTimelineService timelineService = EmbeddedTimelineService.getOrStartEmbeddedTimelineService(context, null, writeConfig);
-      writeConfig.setViewStorageConfig(timelineService.getRemoteFileSystemViewConfig());
+      writeConfig.setViewStorageConfig(timelineService.getRemoteFileSystemViewConfig(writeConfig));
       writeClient = new HoodieJavaWriteClient(context, writeConfig, true, Option.of(timelineService));
       // Both the write client and the table service client should use the same passed-in
       // timeline server instance.

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestSparkRDDWriteClient.java
Patch:
@@ -85,7 +85,7 @@ public void testWriteClientAndTableServiceClientWithTimelineServer(
     SparkRDDWriteClient writeClient;
     if (passInTimelineServer) {
       EmbeddedTimelineService timelineService = EmbeddedTimelineService.getOrStartEmbeddedTimelineService(context(), null, writeConfig);
-      writeConfig.setViewStorageConfig(timelineService.getRemoteFileSystemViewConfig());
+      writeConfig.setViewStorageConfig(timelineService.getRemoteFileSystemViewConfig(writeConfig));
       writeClient = new SparkRDDWriteClient(context(), writeConfig, Option.of(timelineService));
       // Both the write client and the table service client should use the same passed-in
       // timeline server instance.

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/client/functional/TestRemoteFileSystemViewWithMetadataTable.java
Patch:
@@ -203,7 +203,7 @@ private void runAssertionsForBasePath(boolean useExistingTimelineServer, String
 
     int timelineServerPort = useExistingTimelineServer
         ? timelineService.getServerPort()
-        : writeClient.getTimelineServer().get().getRemoteFileSystemViewConfig().getRemoteViewServerPort();
+        : writeClient.getTimelineServer().get().getRemoteFileSystemViewConfig(writeClient.getConfig()).getRemoteViewServerPort();
 
     LOG.info("Connecting to Timeline Server: " + timelineServerPort);
     RemoteHoodieTableFileSystemView view =

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieCopyOnWriteTableInputFormat.java
Patch:
@@ -270,7 +270,7 @@ private List<FileStatus> listStatusForSnapshotMode(JobConf job,
                 .collect(Collectors.toList())
         );
       } else {
-        // If hoodie.metadata.enabled is set to false and the table doesn't have the metadata,
+        // If hoodie.metadata.enable is set to false and the table doesn't have the metadata,
         // read the table using fs view cache instead of file index.
         // This is because there's no file index in non-metadata table.
         String basePath = tableMetaClient.getBasePath().toString();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDBucketIndexPartitioner.java
Patch:
@@ -62,9 +62,9 @@ public RDDBucketIndexPartitioner(HoodieTable table, String sortString, boolean p
    */
 
   public JavaRDD<HoodieRecord<T>> doPartition(JavaRDD<HoodieRecord<T>> records, Partitioner partitioner) {
-    if (sortColumnNames != null && sortColumnNames.length > 0) {
+    if (isCustomSorted()) {
       return doPartitionAndCustomColumnSort(records, partitioner);
-    } else if (table.requireSortedRecords() || table.getConfig().getBulkInsertSortMode() != BulkInsertSortMode.NONE) {
+    } else if (isRecordKeySorted()) {
       return doPartitionAndSortByRecordKey(records, partitioner);
     } else {
       // By default, do partition only

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/InputPathHandler.java
Patch:
@@ -94,8 +94,10 @@ private void parseInputPaths(Path[] inputPaths, List<String> incrementalTables)
       throws IOException {
     for (Path inputPath : inputPaths) {
       boolean basePathKnown = false;
+      String inputPathStr = inputPath.toString();
       for (HoodieTableMetaClient metaClient : tableMetaClientMap.values()) {
-        if (inputPath.toString().contains(metaClient.getBasePath().toString())) {
+        String basePathStr = metaClient.getBasePath().toString();
+        if (inputPathStr.equals(basePathStr) || inputPathStr.startsWith(basePathStr + "/")) {
           // We already know the base path for this inputPath.
           basePathKnown = true;
           // Check if this is for a snapshot query

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimelineUtils.java
Patch:
@@ -126,7 +126,7 @@ public static List<String> getDroppedPartitions(HoodieTableMetaClient metaClient
           try {
             HoodieCleanMetadata cleanMetadata = TimelineMetadataUtils.deserializeHoodieCleanMetadata(cleanerTimeline.getInstantDetails(instant).get());
             cleanMetadata.getPartitionMetadata().forEach((partition, partitionMetadata) -> {
-              if (partitionMetadata.getIsPartitionDeleted()) {
+              if (Boolean.TRUE.equals(partitionMetadata.getIsPartitionDeleted())) {
                 partitionToLatestDeleteTimestamp.put(partition, instant.requestedTime());
               }
             });

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/config/S3EventsHoodieIncrSourceConfig.java
Patch:
@@ -89,6 +89,6 @@ public class S3EventsHoodieIncrSourceConfig extends HoodieConfig {
       .noDefaultValue()
       .withAlternatives(DELTA_STREAMER_CONFIG_PREFIX + "source.s3incr.spark.datasource.options")
       .markAdvanced()
-      .withDocumentation("Json string, passed to the reader while loading dataset. Example Hudi Streamer conf \n"
-          + " --hoodie-conf hoodie.streamer.source.s3incr.spark.datasource.options={\"header\":\"true\",\"encoding\":\"UTF-8\"}");
+      .withDocumentation("Json string, passed to the reader while loading dataset. Example Hudi Streamer conf "
+          + "`--hoodie-conf hoodie.streamer.source.s3incr.spark.datasource.options={\"header\":\"true\",\"encoding\":\"UTF-8\"}`");
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/RunCompactionActionExecutor.java
Patch:
@@ -106,7 +106,7 @@ public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
       }
 
       HoodieData<WriteStatus> statuses = compactor.compact(
-          context, compactionPlan, table, configCopy, instantTime, compactionHandler);
+          context, operationType, compactionPlan, table, configCopy, instantTime, compactionHandler);
 
       compactor.maybePersist(statuses, context, config, instantTime);
       context.setJobStatus(this.getClass().getSimpleName(), "Preparing compaction metadata: " + config.getTableName());

File: hudi-common/src/main/java/org/apache/hudi/common/table/read/HoodieKeyBasedFileGroupRecordBuffer.java
Patch:
@@ -53,8 +53,9 @@ public HoodieKeyBasedFileGroupRecordBuffer(HoodieReaderContext<T> readerContext,
                                              RecordMergeMode recordMergeMode,
                                              Option<String> partitionNameOverrideOpt,
                                              Option<String[]> partitionPathFieldOpt,
-                                             TypedProperties props) {
-    super(readerContext, hoodieTableMetaClient, recordMergeMode, partitionNameOverrideOpt, partitionPathFieldOpt, props);
+                                             TypedProperties props,
+                                             HoodieReadStats readStats) {
+    super(readerContext, hoodieTableMetaClient, recordMergeMode, partitionNameOverrideOpt, partitionPathFieldOpt, props, readStats);
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/common/table/read/HoodiePositionBasedFileGroupRecordBuffer.java
Patch:
@@ -71,8 +71,9 @@ public HoodiePositionBasedFileGroupRecordBuffer(HoodieReaderContext<T> readerCon
                                                   RecordMergeMode recordMergeMode,
                                                   Option<String> partitionNameOverrideOpt,
                                                   Option<String[]> partitionPathFieldOpt,
-                                                  TypedProperties props) {
-    super(readerContext, hoodieTableMetaClient, recordMergeMode, partitionNameOverrideOpt, partitionPathFieldOpt, props);
+                                                  TypedProperties props,
+                                                  HoodieReadStats readStats) {
+    super(readerContext, hoodieTableMetaClient, recordMergeMode, partitionNameOverrideOpt, partitionPathFieldOpt, props, readStats);
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/common/table/read/HoodieUnmergedFileGroupRecordBuffer.java
Patch:
@@ -51,8 +51,9 @@ public HoodieUnmergedFileGroupRecordBuffer(
       RecordMergeMode recordMergeMode,
       Option<String> partitionNameOverrideOpt,
       Option<String[]> partitionPathFieldOpt,
-      TypedProperties props) {
-    super(readerContext, hoodieTableMetaClient, recordMergeMode, partitionNameOverrideOpt, partitionPathFieldOpt, props);
+      TypedProperties props,
+      HoodieReadStats readStats) {
+    super(readerContext, hoodieTableMetaClient, recordMergeMode, partitionNameOverrideOpt, partitionPathFieldOpt, props, readStats);
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/scala/org/apache/hudi/BaseSparkInternalRowReaderContext.java
Patch:
@@ -96,7 +96,7 @@ public HoodieRecord<InternalRow> constructHoodieRecord(Option<InternalRow> rowOp
           HoodieRecord.HoodieRecordType.SPARK);
     }
 
-    Schema schema = (Schema) metadataMap.get(INTERNAL_META_SCHEMA);
+    Schema schema = getSchemaHandler().decodeAvroSchema(metadataMap.get(INTERNAL_META_SCHEMA_ID));
     InternalRow row = rowOption.get();
     return new HoodieSparkRecord(row, HoodieInternalRowUtils.getCachedSchema(schema));
   }

File: hudi-common/src/main/java/org/apache/hudi/common/engine/HoodieReaderContext.java
Patch:
@@ -142,7 +142,7 @@ public void setShouldMergeUseRecordPosition(boolean shouldMergeUseRecordPosition
   public static final String INTERNAL_META_ORDERING_FIELD = "_2";
   public static final String INTERNAL_META_OPERATION = "_3";
   public static final String INTERNAL_META_INSTANT_TIME = "_4";
-  public static final String INTERNAL_META_SCHEMA = "_5";
+  public static final String INTERNAL_META_SCHEMA_ID = "_5";
 
   /**
    * Gets the record iterator based on the type of engine-specific record representation from the
@@ -295,7 +295,7 @@ public Map<String, Object> generateMetadataForRecord(
   public Map<String, Object> generateMetadataForRecord(T record, Schema schema) {
     Map<String, Object> meta = new HashMap<>();
     meta.put(INTERNAL_META_RECORD_KEY, getRecordKey(record, schema));
-    meta.put(INTERNAL_META_SCHEMA, schema);
+    meta.put(INTERNAL_META_SCHEMA_ID, this.schemaHandler.encodeAvroSchema(schema));
     return meta;
   }
 
@@ -309,7 +309,7 @@ public Map<String, Object> generateMetadataForRecord(T record, Schema schema) {
   public Map<String, Object> updateSchemaAndResetOrderingValInMetadata(Map<String, Object> meta,
                                                                        Schema schema) {
     meta.remove(INTERNAL_META_ORDERING_FIELD);
-    meta.put(INTERNAL_META_SCHEMA, schema);
+    meta.put(INTERNAL_META_SCHEMA_ID, this.schemaHandler.encodeAvroSchema(schema));
     return meta;
   }
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HiveHoodieReaderContext.java
Patch:
@@ -207,7 +207,7 @@ public HoodieRecord<ArrayWritable> constructHoodieRecord(Option<ArrayWritable> r
     if (!recordOption.isPresent()) {
       return new HoodieEmptyRecord<>(new HoodieKey((String) metadataMap.get(INTERNAL_META_RECORD_KEY), (String) metadataMap.get(INTERNAL_META_PARTITION_PATH)), HoodieRecord.HoodieRecordType.HIVE);
     }
-    Schema schema = (Schema) metadataMap.get(INTERNAL_META_SCHEMA);
+    Schema schema = getSchemaHandler().decodeAvroSchema(metadataMap.get(INTERNAL_META_SCHEMA_ID));
     ArrayWritable writable = recordOption.get();
     return new HoodieHiveRecord(new HoodieKey((String) metadataMap.get(INTERNAL_META_RECORD_KEY), (String) metadataMap.get(INTERNAL_META_PARTITION_PATH)), writable, schema, objectInspectorCache);
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java
Patch:
@@ -196,7 +196,7 @@ public static HoodieTableFileSystemView createInMemoryFileSystemViewWithTimeline
     }
     if (metaClient.getMetaserverConfig().isMetaserverEnabled()) {
       return (HoodieTableFileSystemView) ReflectionUtils.loadClass(HOODIE_METASERVER_FILE_SYSTEM_VIEW_CLASS,
-          new Class<?>[]{HoodieTableMetaClient.class, HoodieTimeline.class, HoodieMetadataConfig.class},
+          new Class<?>[]{HoodieTableMetaClient.class, HoodieTimeline.class, HoodieMetaserverConfig.class},
           metaClient, timeline, metaClient.getMetaserverConfig());
     }
     return new HoodieTableFileSystemView(metaClient, timeline);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -443,7 +443,9 @@ private void initializeFromFilesystem(String initializationTime, List<MetadataPa
           case SECONDARY_INDEX:
             Set<String> secondaryIndexPartitionsToInit = getIndexPartitionsToInit(partitionType);
             if (secondaryIndexPartitionsToInit.size() != 1) {
-              LOG.warn("Skipping secondary index initialization as only one secondary index bootstrap at a time is supported for now. Provided: {}", secondaryIndexPartitionsToInit);
+              if (secondaryIndexPartitionsToInit.size() > 1) {
+                LOG.warn("Skipping secondary index initialization as only one secondary index bootstrap at a time is supported for now. Provided: {}", secondaryIndexPartitionsToInit);
+              }
               continue;
             }
             partitionName = secondaryIndexPartitionsToInit.iterator().next();

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimelineMetadataUtils.java
Patch:
@@ -150,9 +150,6 @@ public static Option<byte[]> serializeIndexCommitMetadata(HoodieIndexCommitMetad
 
   public static Option<byte[]> serializeCommitMetadata(CommitMetadataSerDe commitMetadataSerDe,
                                                        org.apache.hudi.common.model.HoodieCommitMetadata commitMetadata) throws IOException {
-    if (commitMetadata instanceof org.apache.hudi.common.model.HoodieReplaceCommitMetadata) {
-      return serializeAvroMetadata(MetadataConversionUtils.convertCommitMetadata(commitMetadata), HoodieReplaceCommitMetadata.class);
-    }
     return commitMetadataSerDe.serialize(commitMetadata);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java
Patch:
@@ -39,6 +39,7 @@
 import org.apache.hudi.common.table.timeline.InstantGenerator;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
 import org.apache.hudi.common.table.timeline.TimelineUtils;
+import org.apache.hudi.common.table.timeline.versioning.v2.InstantGeneratorV2;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
@@ -180,7 +181,8 @@ private static Option<HoodieRequestedReplaceMetadata> getRequestedReplaceMetadat
     } else if (pendingReplaceOrClusterInstant.isRequested()) {
       requestedInstant = pendingReplaceOrClusterInstant;
     } else {
-      requestedInstant = factory.createNewInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.CLUSTERING_ACTION, pendingReplaceOrClusterInstant.requestedTime());
+      String action = factory instanceof InstantGeneratorV2 ? HoodieTimeline.CLUSTERING_ACTION : HoodieTimeline.REPLACE_COMMIT_ACTION;
+      requestedInstant = factory.createNewInstant(HoodieInstant.State.REQUESTED, action, pendingReplaceOrClusterInstant.requestedTime());
     }
     Option<byte[]> content = Option.empty();
     try {

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -62,7 +62,7 @@ public final class HoodieMetadataConfig extends HoodieConfig {
       .sinceVersion("0.7.0")
       .withDocumentation("Enable the internal metadata table which serves table metadata like level file listings");
 
-  public static final boolean DEFAULT_METADATA_ENABLE_FOR_READERS = false;
+  public static final boolean DEFAULT_METADATA_ENABLE_FOR_READERS = true;
 
   // Enable metrics for internal Metadata Table
   public static final ConfigProperty<Boolean> METRICS_ENABLE = ConfigProperty

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/source/TestFileIndex.java
Patch:
@@ -171,6 +171,7 @@ void testFileListingWithPartitionStatsPruning(HoodieTableType tableType) throws
     conf.set(METADATA_ENABLED, true);
     conf.set(TABLE_TYPE, tableType.name());
     conf.setBoolean(HoodieMetadataConfig.ENABLE_METADATA_INDEX_PARTITION_STATS.key(), true);
+    conf.setBoolean(HoodieMetadataConfig.ENABLE_METADATA_INDEX_COLUMN_STATS.key(), true);
     if (tableType == HoodieTableType.MERGE_ON_READ) {
       // enable CSI for MOR table to collect col stats for delta write stats,
       // which will be used to construct partition stats then.

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/source/TestIncrementalInputSplits.java
Patch:
@@ -361,6 +361,7 @@ void testInputSplitsWithPartitionStatsPruner(HoodieTableType tableType) throws E
     conf.set(FlinkOptions.READ_DATA_SKIPPING_ENABLED, true);
     conf.set(FlinkOptions.TABLE_TYPE, tableType.name());
     conf.setBoolean(HoodieMetadataConfig.ENABLE_METADATA_INDEX_PARTITION_STATS.key(), true);
+    conf.setBoolean(HoodieMetadataConfig.ENABLE_METADATA_INDEX_COLUMN_STATS.key(), true);
     if (tableType == HoodieTableType.MERGE_ON_READ) {
       // enable CSI for MOR table to collect col stats for delta write stats,
       // which will be used to construct partition stats then.

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/source/stats/TestColumnStatsIndex.java
Patch:
@@ -55,6 +55,7 @@ void testReadPartitionStatsIndex() throws Exception {
     Configuration conf = TestConfigurations.getDefaultConf(path);
     conf.set(FlinkOptions.METADATA_ENABLED, true);
     conf.setString("hoodie.metadata.index.partition.stats.enable", "true");
+    conf.setString("hoodie.metadata.index.column.stats.enable", "true");
     HoodieMetadataConfig metadataConfig = HoodieMetadataConfig.newBuilder()
         .enable(true)
         .withMetadataIndexColumnStats(true)

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/ITTestHoodieDataSource.java
Patch:
@@ -516,6 +516,7 @@ void testReadWithPartitionStatsPruning(HoodieTableType tableType, boolean hiveSt
         .option(FlinkOptions.METADATA_ENABLED, true)
         .option(FlinkOptions.READ_AS_STREAMING, true)
         .option(HoodieMetadataConfig.ENABLE_METADATA_INDEX_PARTITION_STATS.key(), true)
+        .option(HoodieMetadataConfig.ENABLE_METADATA_INDEX_COLUMN_STATS.key(), false)
         .option(FlinkOptions.READ_DATA_SKIPPING_ENABLED, true)
         .option(FlinkOptions.TABLE_TYPE, tableType)
         .option(FlinkOptions.HIVE_STYLE_PARTITIONING, hiveStylePartitioning)

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/TestHoodieTableSource.java
Patch:
@@ -177,6 +177,7 @@ void testDataSkippingWithPartitionStatsPruning(List<ResolvedExpression> filters,
     final String path = tempFile.getAbsolutePath();
     conf = TestConfigurations.getDefaultConf(path);
     conf.setBoolean(HoodieMetadataConfig.ENABLE_METADATA_INDEX_PARTITION_STATS.key(), true);
+    conf.setBoolean(HoodieMetadataConfig.ENABLE_METADATA_INDEX_COLUMN_STATS.key(), true);
     conf.set(FlinkOptions.READ_DATA_SKIPPING_ENABLED, true);
     TestData.writeData(TestData.DATA_SET_INSERT, conf);
     HoodieTableSource hoodieTableSource = createHoodieTableSource(conf);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -1025,10 +1025,10 @@ public void dropIndex(List<String> metadataPartitions) {
     try {
       context.setJobStatus(this.getClass().getSimpleName(), "Dropping partitions from metadata table: " + config.getTableName());
       HoodieTableMetaClient metaClient = table.getMetaClient();
-      // For secondary index and functional index with wrong parameters, index definition for the MDT partition is
+      // For secondary index and expression index with wrong parameters, index definition for the MDT partition is
       // removed so that such indices are not recreated while initializing the writer.
       metadataPartitions.forEach(partition -> {
-        if (MetadataPartitionType.isGenericIndex(partition)) {
+        if (MetadataPartitionType.isExpressionOrSecondaryIndex(partition)) {
           metaClient.deleteIndexDefinition(partition);
         }
       });

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/functional/BaseHoodieIndexClient.java
Patch:
@@ -38,7 +38,7 @@ public BaseHoodieIndexClient() {
   }
 
   /**
-   * Register a functional index.
+   * Register a expression index.
    * Index definitions are stored in user-specified path or, by default, in .hoodie/.index_defs/index.json.
    * For the first time, the index definition file will be created if not exists.
    * For the second time, the index definition file will be updated if exists.
@@ -57,7 +57,7 @@ public void register(HoodieTableMetaClient metaClient, HoodieIndexDefinition ind
   }
 
   /**
-   * Create a functional index.
+   * Create a expression index.
    */
   public abstract void create(HoodieTableMetaClient metaClient, String indexName, String indexType, Map<String, Map<String, String>> columns, Map<String, String> options) throws Exception;
 

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/table/upgrade/TestEightToSevenDowngradeHandler.java
Patch:
@@ -79,7 +79,7 @@ class TestEightToSevenDowngradeHandler {
   private File baseDir;
 
   private static final List<String> SAMPLE_METADATA_PATHS = Arrays.asList(
-      "func_index_random",
+      "expr_index_random",
       "secondary_index_random",
       "partition_stats",
       FILES.getPartitionPath(),
@@ -108,7 +108,7 @@ void testDeleteMetadataPartition() {
 
       mockedMetadataUtils.verify(
           () -> HoodieTableMetadataUtil.deleteMetadataTablePartition(
-              metaClient, context, "func_index_random", true),
+              metaClient, context, "expr_index_random", true),
           times(1));
       mockedMetadataUtils.verify(
           () -> HoodieTableMetadataUtil.deleteMetadataTablePartition(

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -188,10 +188,10 @@ protected void preWrite(String instantTime) {
   }
 
   @Override
-  protected HoodieData<HoodieRecord> getFunctionalIndexRecords(List<Pair<String, Pair<String, Long>>> partitionFilePathAndSizeTriplet, HoodieIndexDefinition indexDefinition,
+  protected HoodieData<HoodieRecord> getExpressionIndexRecords(List<Pair<String, Pair<String, Long>>> partitionFilePathAndSizeTriplet, HoodieIndexDefinition indexDefinition,
                                                                HoodieTableMetaClient metaClient, int parallelism, Schema readerSchema, StorageConfiguration<?> storageConf,
                                                                String instantTime) {
-    throw new HoodieNotSupportedException("Flink metadata table does not support functional index yet.");
+    throw new HoodieNotSupportedException("Flink metadata table does not support expression index yet.");
   }
 
   @Override

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/metadata/JavaHoodieBackedTableMetadataWriter.java
Patch:
@@ -123,10 +123,10 @@ protected void preWrite(String instantTime) {
   }
 
   @Override
-  protected HoodieData<HoodieRecord> getFunctionalIndexRecords(List<Pair<String, Pair<String, Long>>> partitionFilePathAndSizeTriplet, HoodieIndexDefinition indexDefinition,
+  protected HoodieData<HoodieRecord> getExpressionIndexRecords(List<Pair<String, Pair<String, Long>>> partitionFilePathAndSizeTriplet, HoodieIndexDefinition indexDefinition,
                                                                HoodieTableMetaClient metaClient, int parallelism, Schema readerSchema, StorageConfiguration<?> storageConf,
                                                                String instantTime) {
-    throw new HoodieNotSupportedException("Functional index not supported for Java metadata table writer yet.");
+    throw new HoodieNotSupportedException("Expression index not supported for Java metadata table writer yet.");
   }
 
   @Override

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/table/TestHoodieTableMetaClient.java
Patch:
@@ -45,7 +45,7 @@
 
 import static org.apache.hudi.common.testutils.HoodieTestUtils.INSTANT_GENERATOR;
 import static org.apache.hudi.common.util.StringUtils.getUTF8Bytes;
-import static org.apache.hudi.index.functional.HoodieFunctionalIndex.IDENTITY_FUNCTION;
+import static org.apache.hudi.index.functional.HoodieExpressionIndex.IDENTITY_FUNCTION;
 import static org.junit.jupiter.api.Assertions.assertArrayEquals;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;
@@ -262,7 +262,7 @@ public void testDeleteDefinition() throws IOException {
         .initTable(this.metaClient.getStorageConf(), basePath);
     Map<String, Map<String, String>> columnsMap = new HashMap<>();
     columnsMap.put("c1", Collections.emptyMap());
-    String indexName = MetadataPartitionType.FUNCTIONAL_INDEX.getPartitionPath() + "idx";
+    String indexName = MetadataPartitionType.EXPRESSION_INDEX.getPartitionPath() + "idx";
     HoodieIndexDefinition indexDefinition = new HoodieIndexDefinition(indexName, "column_stats", IDENTITY_FUNCTION,
         new ArrayList<>(columnsMap.keySet()), Collections.emptyMap());
     metaClient.buildIndexDefinition(indexDefinition);

File: hudi-hadoop-common/src/test/java/org/apache/hudi/metadata/TestHoodieTableMetadataUtil.java
Patch:
@@ -329,8 +329,8 @@ public void testGetFileGroupIndexFromFileId() {
     result = getFileIDForFileGroup(MetadataPartitionType.SECONDARY_INDEX, 6, "secondary_index_idx_ts");
     assertEquals("secondary-index-idx-ts-0006-0", result);
 
-    result = getFileIDForFileGroup(MetadataPartitionType.FUNCTIONAL_INDEX, 5, "func_index_ts");
-    assertEquals("func-index-ts-0005-0", result);
+    result = getFileIDForFileGroup(MetadataPartitionType.EXPRESSION_INDEX, 5, "expr_index_ts");
+    assertEquals("expr-index-ts-0005-0", result);
   }
 
   @Test

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieMetadataTableValidator.java
Patch:
@@ -293,7 +293,7 @@ public void testSecondaryIndexValidation() throws IOException {
     rows.write().format("hudi").mode(SaveMode.Append).save(basePath);
 
     // create secondary index
-    sparkSession.sql("create index idx_not_record_key_col on tbl using secondary_index(not_record_key_col)");
+    sparkSession.sql("create index idx_not_record_key_col on tbl (not_record_key_col)");
     validateSecondaryIndex();
 
     // updating record `not_record_key_col` column from `abc` to `cde`

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/timeline/ActiveActionWithDetails.java
Patch:
@@ -98,17 +98,14 @@ public Option<byte[]> getInflightCommitMetadata(HoodieTableMetaClient metaClient
   }
 
   public Option<byte[]> getCleanPlan(HoodieTableMetaClient metaClient) {
-    ValidationUtils.checkState(this.requestedDetails.isPresent(), "clean plan does not exist");
     return this.requestedDetails;
   }
 
   public Option<byte[]> getCompactionPlan(HoodieTableMetaClient metaClient) {
-    ValidationUtils.checkState(this.requestedDetails.isPresent(), "compaction plan does not exist");
     return this.requestedDetails;
   }
 
   public Option<byte[]> getLogCompactionPlan(HoodieTableMetaClient metaClient) {
-    ValidationUtils.checkState(this.requestedDetails.isPresent(), "log compaction plan does not exist");
     return this.requestedDetails;
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/timeline/HoodieTimelineArchiver.java
Patch:
@@ -36,5 +36,5 @@ default int archiveIfRequired(HoodieEngineContext context) throws IOException {
   /**
    * Check if commits need to be archived. If yes, archive commits.
    */
-  public int archiveIfRequired(HoodieEngineContext context, boolean acquireLock) throws IOException;
+  int archiveIfRequired(HoodieEngineContext context, boolean acquireLock) throws IOException;
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/timeline/versioning/v2/TimelineArchiverV2.java
Patch:
@@ -114,7 +114,7 @@ public int archiveIfRequired(HoodieEngineContext context, boolean acquireLock) t
         };
         this.timelineWriter.write(instantsToArchive, Option.of(action -> deleteAnyLeftOverMarkers(context, action)), Option.of(exceptionHandler));
         LOG.debug("Deleting archived instants");
-        deleteArchivedInstants(instantsToArchive, context);
+        deleteArchivedActions(instantsToArchive, context);
         // triggers compaction and cleaning only after archiving action
         this.timelineWriter.compactAndClean(context);
       } else {
@@ -320,7 +320,7 @@ private Stream<ActiveAction> getInstantsToArchive() throws IOException {
     });
   }
 
-  private boolean deleteArchivedInstants(List<ActiveAction> activeActions, HoodieEngineContext context) {
+  private boolean deleteArchivedActions(List<ActiveAction> activeActions, HoodieEngineContext context) {
     List<HoodieInstant> pendingInstants = new ArrayList<>();
     List<HoodieInstant> completedInstants = new ArrayList<>();
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/v1/ArchivedTimelineV1.java
Patch:
@@ -175,7 +175,7 @@ private List<HoodieInstant> loadInstants(HoodieArchivedTimeline.TimeRangeFilter
   /**
    * Callback to read instant details.
    */
-  private class InstantsLoader implements BiConsumer<String, GenericRecord> {
+  public class InstantsLoader implements BiConsumer<String, GenericRecord> {
     private final Map<String, HoodieInstant> instantsInRange = new ConcurrentHashMap<>();
     private final boolean loadInstantDetails;
 
@@ -215,7 +215,7 @@ private HoodieInstant readCommit(String instantTime, GenericRecord record, boole
   }
 
   @Nonnull
-  private Option<String> getMetadataKey(String action) {
+  private static Option<String> getMetadataKey(String action) {
     switch (action) {
       case HoodieTimeline.CLEAN_ACTION:
         return Option.of("hoodieCleanMetadata");

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/v2/ArchivedTimelineLoaderV2.java
Patch:
@@ -54,7 +54,7 @@ public void loadInstants(HoodieTableMetaClient metaClient,
                            BiConsumer<String, GenericRecord> recordConsumer) {
     try {
       // List all files
-      List<String> fileNames = LSMTimeline.latestSnapshotManifest(metaClient).getFileNames();
+      List<String> fileNames = LSMTimeline.latestSnapshotManifest(metaClient, metaClient.getArchivePath()).getFileNames();
 
       Schema readSchema = LSMTimeline.getReadSchema(loadMode);
       fileNames.stream()

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java
Patch:
@@ -211,7 +211,7 @@ protected void autoCommit(HoodieWriteMetadata<O> result) {
 
   protected abstract void commit(HoodieWriteMetadata<O> result);
 
-  protected void commit(HoodieData<WriteStatus> writeStatuses, HoodieWriteMetadata<O> result, List<HoodieWriteStat> writeStats) {
+  protected void commit(HoodieWriteMetadata<O> result, List<HoodieWriteStat> writeStats) {
     String actionType = getCommitActionType();
     LOG.info("Committing " + instantTime + ", action Type " + actionType + ", operation Type " + operationType);
     result.setCommitted(true);
@@ -222,7 +222,7 @@ protected void commit(HoodieData<WriteStatus> writeStatuses, HoodieWriteMetadata
       HoodieActiveTimeline activeTimeline = table.getActiveTimeline();
       HoodieCommitMetadata metadata = result.getCommitMetadata().get();
 
-      writeTableMetadata(metadata, writeStatuses, actionType);
+      writeTableMetadata(metadata, actionType);
       // cannot serialize maps with null values
       metadata.getExtraMetadata().entrySet().removeIf(entry -> entry.getValue() == null);
       activeTimeline.saveAsComplete(false,

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/WriteStatBasedIndexingCatchupTask.java
Patch:
@@ -50,6 +50,6 @@ public WriteStatBasedIndexingCatchupTask(HoodieTableMetadataWriter metadataWrite
   public void updateIndexForWriteAction(HoodieInstant instant) throws IOException {
     HoodieCommitMetadata commitMetadata = metaClient.getCommitMetadataSerDe().deserialize(instant,
         metaClient.getActiveTimeline().getInstantDetails(instant).get(), HoodieCommitMetadata.class);
-    metadataWriter.updateFromWriteStatuses(commitMetadata, engineContext.emptyHoodieData(), instant.requestedTime());
+    metadataWriter.update(commitMetadata, instant.requestedTime());
   }
 }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.table.action.commit;
 
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -123,8 +122,7 @@ protected String getCommitActionType() {
 
   @Override
   protected void commit(HoodieWriteMetadata<List<WriteStatus>> result) {
-    commit(HoodieListData.eager(result.getWriteStatuses()), result,
-        result.getWriteStatuses().stream().map(WriteStatus::getStat).collect(Collectors.toList()));
+    commit(result, result.getWriteStatuses().stream().map(WriteStatus::getStat).collect(Collectors.toList()));
   }
 
   protected void setCommitMetadata(HoodieWriteMetadata<List<WriteStatus>> result) {

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java
Patch:
@@ -88,7 +88,7 @@ public boolean commit(String instantTime,
                         Map<String, List<String>> partitionToReplacedFileIds,
                         Option<BiConsumer<HoodieTableMetaClient, HoodieCommitMetadata>> extraPreCommitFunc) {
     List<HoodieWriteStat> writeStats = writeStatuses.stream().map(WriteStatus::getStat).collect(Collectors.toList());
-    return commitStats(instantTime, HoodieListData.eager(writeStatuses), writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds,
+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds,
         extraPreCommitFunc);
   }
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java
Patch:
@@ -189,8 +189,7 @@ protected Pair<HashMap<String, WorkloadStat>, WorkloadStat> buildProfile(List<Ho
 
   @Override
   protected void commit(HoodieWriteMetadata<List<WriteStatus>> result) {
-    commit(HoodieListData.eager(result.getWriteStatuses()), result,
-        result.getWriteStatuses().stream().map(WriteStatus::getStat).collect(Collectors.toList()));
+    commit(result, result.getWriteStatuses().stream().map(WriteStatus::getStat).collect(Collectors.toList()));
   }
 
   protected void setCommitMetadata(HoodieWriteMetadata<List<WriteStatus>> result) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java
Patch:
@@ -90,7 +90,7 @@ public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses, Op
                         Option<BiConsumer<HoodieTableMetaClient, HoodieCommitMetadata>> extraPreCommitFunc) {
     context.setJobStatus(this.getClass().getSimpleName(), "Committing stats: " + config.getTableName());
     List<HoodieWriteStat> writeStats = writeStatuses.map(WriteStatus::getStat).collect();
-    return commitStats(instantTime, HoodieJavaRDD.of(writeStatuses), writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds, extraPreCommitFunc);
+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds, extraPreCommitFunc);
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java
Patch:
@@ -214,7 +214,7 @@ protected void commit(HoodieWriteMetadata<HoodieData<WriteStatus>> result) {
       LOG.info("Finished writing bootstrap index for source " + config.getBootstrapSourceBasePath() + " in table "
           + config.getBasePath());
     }
-    commit(result.getWriteStatuses(), result, bootstrapSourceAndStats.values().stream()
+    commit(result, bootstrapSourceAndStats.values().stream()
         .flatMap(f -> f.stream().map(Pair::getValue)).collect(Collectors.toList()));
     LOG.info("Committing metadata bootstrap !!");
   }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java
Patch:
@@ -289,7 +289,7 @@ protected void setCommitMetadata(HoodieWriteMetadata<HoodieData<WriteStatus>> re
   @Override
   protected void commit(HoodieWriteMetadata<HoodieData<WriteStatus>> result) {
     context.setJobStatus(this.getClass().getSimpleName(), "Commit write status collect: " + config.getTableName());
-    commit(result.getWriteStatuses(), result, result.getWriteStats().isPresent()
+    commit(result, result.getWriteStats().isPresent()
         ? result.getWriteStats().get() : result.getWriteStatuses().map(WriteStatus::getStat).collectAsList());
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestSparkRDDWriteClient.java
Patch:
@@ -140,7 +140,7 @@ void testWriteClientReleaseResourcesShouldOnlyUnpersistRelevantRdds(
     String metadataTableBasePath = HoodieTableMetadata.getMetadataTableBasePath(writeConfig.getBasePath());
     List<Integer> metadataTableCacheIds0 = context().getCachedDataIds(HoodieDataCacheKey.of(metadataTableBasePath, instant0));
     List<Integer> metadataTableCacheIds1 = context().getCachedDataIds(HoodieDataCacheKey.of(metadataTableBasePath, instant1));
-    writeClient.commitStats(instant1, context().parallelize(writeStatuses, 1), writeStatuses.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
+    writeClient.commitStats(instant1, writeStatuses.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
         Option.empty(), metaClient.getCommitActionType());
     writeClient.close();
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestConsistentBucketIndex.java
Patch:
@@ -275,7 +275,7 @@ private List<WriteStatus> writeData(JavaRDD<HoodieRecord> records, String commit
     }
     org.apache.hudi.testutils.Assertions.assertNoWriteErrors(writeStatues);
     if (doCommit) {
-      boolean success = writeClient.commitStats(commitTime, context.parallelize(writeStatues, 1), writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
+      boolean success = writeClient.commitStats(commitTime, writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
           Option.empty(), metaClient.getCommitActionType());
       Assertions.assertTrue(success);
     }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/TestHoodieTimelineArchiver.java
Patch:
@@ -431,7 +431,7 @@ private HoodieInstant commitWithMdt(String instantTime, Map<String, List<String>
       });
       commitMeta = generateCommitMetadata(instantTime, partToFileIds);
       metadataWriter.performTableServices(Option.of(instantTime));
-      metadataWriter.updateFromWriteStatuses(commitMeta, context.emptyHoodieData(), instantTime);
+      metadataWriter.update(commitMeta, instantTime);
       metaClient.getActiveTimeline().saveAsComplete(
           INSTANT_GENERATOR.createNewInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, instantTime),
           serializeCommitMetadata(metaClient.getCommitMetadataSerDe(), commitMeta));

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableCompaction.java
Patch:
@@ -135,7 +135,7 @@ public void testWriteDuringCompaction(String payloadClass) throws IOException {
     List<WriteStatus> writeStatuses = writeData(insertTime, 100, false);
     Assertions.assertEquals(200, readTableTotalRecordsNum());
     // commit the write. The records should be visible now even though the compaction does not complete.
-    client.commitStats(insertTime, context().parallelize(writeStatuses, 1), writeStatuses.stream().map(WriteStatus::getStat)
+    client.commitStats(insertTime, writeStatuses.stream().map(WriteStatus::getStat)
         .collect(Collectors.toList()), Option.empty(), metaClient.getCommitActionType());
     Assertions.assertEquals(300, readTableTotalRecordsNum());
     // after the compaction, total records should remain the same
@@ -207,7 +207,7 @@ private List<WriteStatus> writeData(String instant, int numRecords, boolean doCo
     org.apache.hudi.testutils.Assertions.assertNoWriteErrors(writeStatuses);
     if (doCommit) {
       List<HoodieWriteStat> writeStats = writeStatuses.stream().map(WriteStatus::getStat).collect(Collectors.toList());
-      boolean committed = client.commitStats(instant, context().parallelize(writeStatuses, 1), writeStats, Option.empty(), metaClient.getCommitActionType());
+      boolean committed = client.commitStats(instant, writeStats, Option.empty(), metaClient.getCommitActionType());
       Assertions.assertTrue(committed);
     }
     metaClient = HoodieTableMetaClient.reload(metaClient);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieCleanerTestBase.java
Patch:
@@ -211,7 +211,7 @@ public void commitWithMdt(String instantTime, Map<String, List<String>> partToFi
     HoodieCommitMetadata commitMeta = generateCommitMetadata(instantTime, partToFileIds);
     try (HoodieTableMetadataWriter metadataWriter = getMetadataWriter(config)) {
       metadataWriter.performTableServices(Option.of(instantTime));
-      metadataWriter.updateFromWriteStatuses(commitMeta, context.emptyHoodieData(), instantTime);
+      metadataWriter.update(commitMeta, instantTime);
       metaClient.getActiveTimeline().saveAsComplete(
           INSTANT_GENERATOR.createNewInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, instantTime),
           serializeCommitMetadata(metaClient.getCommitMetadataSerDe(), commitMeta));

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/ClusteringCommitSink.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.avro.model.HoodieClusteringGroup;
 import org.apache.hudi.avro.model.HoodieClusteringPlan;
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.TableServiceType;
@@ -218,8 +217,7 @@ private void doCommit(String instant, HoodieClusteringPlan clusteringPlan, Colle
     }
     // commit the clustering
     this.table.getMetaClient().reloadActiveTimeline();
-    this.writeClient.completeTableService(
-        TableServiceType.CLUSTER, writeMetadata.getCommitMetadata().get(), table, instant, Option.of(HoodieListData.lazy(writeMetadata.getWriteStatuses())));
+    this.writeClient.completeTableService(TableServiceType.CLUSTER, writeMetadata.getCommitMetadata().get(), table, instant);
 
     clusteringMetrics.updateCommitMetrics(instant, writeMetadata.getCommitMetadata().get());
     // whether to clean up the input base parquet files used for clustering

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/DataSourceInternalWriterHelper.java
Patch:
@@ -84,7 +84,7 @@ public void onDataWriterCommit(String message) {
   public void commit(List<WriteStatus> writeStatuses) {
     try {
       List<HoodieWriteStat> writeStatList = writeStatuses.stream().map(WriteStatus::getStat).collect(Collectors.toList());
-      writeClient.commitStats(instantTime, writeClient.getEngineContext().parallelize(writeStatuses), writeStatList, Option.of(extraMetadata),
+      writeClient.commitStats(instantTime, writeStatList, Option.of(extraMetadata),
           CommitUtils.getCommitActionType(operationType, metaClient.getTableType()));
     } catch (Exception ioe) {
       throw new HoodieException(ioe.getMessage(), ioe);

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestSparkConsistentBucketClustering.java
Patch:
@@ -305,7 +305,7 @@ public void testConcurrentWrite(boolean rowWriterEnable) throws IOException {
     List<WriteStatus> writeStatues = writeData(writeTime, 2000, false);
     // Cannot schedule clustering if there is in-flight writer
     Assertions.assertFalse(writeClient.scheduleClustering(Option.empty()).isPresent());
-    Assertions.assertTrue(writeClient.commitStats(writeTime, context.parallelize(writeStatues, 1), writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
+    Assertions.assertTrue(writeClient.commitStats(writeTime, writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
         Option.empty(), metaClient.getCommitActionType()));
     metaClient = HoodieTableMetaClient.reload(metaClient);
 
@@ -341,7 +341,7 @@ private List<WriteStatus> writeData(String commitTime, int totalRecords, boolean
     List<WriteStatus> writeStatues = writeClient.upsert(writeRecords, commitTime).collect();
     org.apache.hudi.testutils.Assertions.assertNoWriteErrors(writeStatues);
     if (doCommit) {
-      Assertions.assertTrue(writeClient.commitStats(commitTime, context.parallelize(writeStatues, 1), writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
+      Assertions.assertTrue(writeClient.commitStats(commitTime, writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
           Option.empty(), metaClient.getCommitActionType()));
     }
     metaClient = HoodieTableMetaClient.reload(metaClient);

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestSparkSortAndSizeClustering.java
Patch:
@@ -157,7 +157,7 @@ private List<WriteStatus> writeData(String commitTime, int totalRecords, boolean
     org.apache.hudi.testutils.Assertions.assertNoWriteErrors(writeStatues);
 
     if (doCommit) {
-      Assertions.assertTrue(writeClient.commitStats(commitTime, context.parallelize(writeStatues, 1), writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
+      Assertions.assertTrue(writeClient.commitStats(commitTime, writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
           Option.empty(), metaClient.getCommitActionType()));
     }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/offlinejob/HoodieOfflineJobTestBase.java
Patch:
@@ -96,7 +96,7 @@ protected List<WriteStatus> writeData(boolean isUpsert, String instant, int numR
     org.apache.hudi.testutils.Assertions.assertNoWriteErrors(writeStatuses);
     if (doCommit) {
       List<HoodieWriteStat> writeStats = writeStatuses.stream().map(WriteStatus::getStat).collect(Collectors.toList());
-      boolean committed = client.commitStats(instant, context.parallelize(writeStatuses, 1), writeStats, Option.empty(), metaClient.getCommitActionType());
+      boolean committed = client.commitStats(instant, writeStats, Option.empty(), metaClient.getCommitActionType());
       Assertions.assertTrue(committed);
     }
     metaClient = HoodieTableMetaClient.reload(metaClient);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java
Patch:
@@ -20,7 +20,7 @@
 import org.apache.hudi.client.transaction.BucketIndexConcurrentFileWritesConflictResolutionStrategy;
 import org.apache.hudi.client.transaction.ConflictResolutionStrategy;
 import org.apache.hudi.client.transaction.SimpleConcurrentFileWritesConflictResolutionStrategy;
-import org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider;
+import org.apache.hudi.client.transaction.lock.InProcessLockProvider;
 import org.apache.hudi.common.config.ConfigClassProperty;
 import org.apache.hudi.common.config.ConfigGroups;
 import org.apache.hudi.common.config.ConfigProperty;
@@ -204,7 +204,7 @@ public class HoodieLockConfig extends HoodieConfig {
   // Pluggable type of lock provider
   public static final ConfigProperty<String> LOCK_PROVIDER_CLASS_NAME = ConfigProperty
       .key(LOCK_PREFIX + "provider")
-      .defaultValue(ZookeeperBasedLockProvider.class.getName())
+      .defaultValue(InProcessLockProvider.class.getName())
       .markAdvanced()
       .sinceVersion("0.8.0")
       .withDocumentation("Lock provider class name, user can provide their own implementation of LockProvider "

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -480,8 +480,7 @@ public synchronized HoodieActiveTimeline reloadActiveTimeline() {
    * Reload the table config properties.
    */
   public synchronized void reloadTableConfig() {
-    this.tableConfig = new HoodieTableConfig(this.storage, metaPath,
-        this.tableConfig.getRecordMergeMode(), this.tableConfig.getPayloadClass(), this.tableConfig.getRecordMergeStrategyId());
+    this.tableConfig = HoodieTableConfig.loadFromHoodieProps(this.storage, metaPath);
     reloadTimelineLayoutAndPath();
   }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/HoodieSparkFunctionalIndex.java
Patch:
@@ -181,7 +181,7 @@ interface SparkFunction extends Serializable {
         }
         return functions.split(columns.get(0), options.get("pattern"));
       }),
-      Pair.of(SPARK_IDENTITY, (columns, options) -> {
+      Pair.of(IDENTITY_FUNCTION, (columns, options) -> {
         if (columns.size() != 1) {
           throw new IllegalArgumentException("IDENTITY requires 1 column");
         }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -2830,7 +2830,7 @@ public void testRollbackPendingCommitWithRecordIndex(boolean performUpsert) thro
 
     // delete the metadata table partitions to check, whether rollback of pending commit succeeds and
     // metadata table partitions are rebootstrapped.
-    metadataWriter.dropMetadataPartitions(Arrays.asList(RECORD_INDEX.getPartitionPath(), FILES.getPartitionPath()));
+    client.dropIndex(Arrays.asList(RECORD_INDEX.getPartitionPath(), FILES.getPartitionPath()));
     assertFalse(storage.exists(new StoragePath(
         getMetadataTableBasePath(basePath) + StoragePath.SEPARATOR + FILES.getPartitionPath())));
     assertFalse(storage.exists(new StoragePath(getMetadataTableBasePath(basePath)

File: hudi-common/src/main/java/org/apache/hudi/index/functional/HoodieFunctionalIndex.java
Patch:
@@ -56,7 +56,7 @@ public interface HoodieFunctionalIndex<S, T> extends Serializable {
   String SPARK_REGEXP_REPLACE = "regexp_replace";
   String SPARK_REGEXP_EXTRACT = "regexp_extract";
   String SPARK_SPLIT = "split";
-  String SPARK_IDENTITY = "identity";
+  String IDENTITY_FUNCTION = "identity";
 
   /**
    * Get the name of the index.

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/functional/BaseHoodieIndexClient.java
Patch:
@@ -58,7 +58,7 @@ public void register(HoodieTableMetaClient metaClient, String indexName, String
   /**
    * Create a functional index.
    */
-  public abstract void create(HoodieTableMetaClient metaClient, String indexName, String indexType, Map<String, Map<String, String>> columns, Map<String, String> options);
+  public abstract void create(HoodieTableMetaClient metaClient, String indexName, String indexType, Map<String, Map<String, String>> columns, Map<String, String> options) throws Exception;
 
   /**
    * Drop an index. By default, ignore drop if index does not exist.

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableInsertUpdateDelete.java
Patch:
@@ -68,7 +68,6 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.spark.api.java.JavaRDD;
-import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Tag;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.params.ParameterizedTest;
@@ -264,11 +263,11 @@ public void testRepeatedRollbackOfCompaction() throws Exception {
     }
   }
 
-  @Disabled("HUDI-8203")
   @ParameterizedTest
   @ValueSource(booleans = {true, false})
   public void testSimpleInsertUpdateAndDelete(boolean populateMetaFields) throws Exception {
     Properties properties = populateMetaFields ? new Properties() : getPropertiesForKeyGen();
+    properties.setProperty(HoodieTableConfig.PRECOMBINE_FIELD.key(), "timestamp");
     properties.setProperty(HoodieTableConfig.BASE_FILE_FORMAT.key(), HoodieTableConfig.BASE_FILE_FORMAT.defaultValue().toString());
     HoodieTableMetaClient metaClient = getHoodieMetaClient(HoodieTableType.MERGE_ON_READ, properties);
 

File: hudi-common/src/main/java/org/apache/hudi/index/functional/HoodieFunctionalIndex.java
Patch:
@@ -31,7 +31,7 @@
  */
 public interface HoodieFunctionalIndex<S, T> extends Serializable {
 
-  String HOODIE_FUNCTIONAL_INDEX_FILE_PATH = "_hoodie_functional_index_file_path";
+  String HOODIE_FUNCTIONAL_INDEX_RELATIVE_FILE_PATH = "_hoodie_functional_index_relative_file_path";
   String HOODIE_FUNCTIONAL_INDEX_PARTITION = "_hoodie_functional_index_partition";
   String HOODIE_FUNCTIONAL_INDEX_FILE_SIZE = "_hoodie_functional_index_file_size";
   String SPARK_DATE_FORMAT = "date_format";

File: hudi-common/src/main/java/org/apache/hudi/metadata/FileSystemBackedTableMetadata.java
Patch:
@@ -285,13 +285,13 @@ public void reset() {
     // no-op
   }
 
-  public Option<BloomFilter> getBloomFilter(final String partitionName, final String fileName)
+  public Option<BloomFilter> getBloomFilter(final String partitionName, final String fileName, final String metadataPartitionName)
       throws HoodieMetadataException {
     throw new HoodieMetadataException("Unsupported operation: getBloomFilter for " + fileName);
   }
 
   @Override
-  public Map<Pair<String, String>, BloomFilter> getBloomFilters(final List<Pair<String, String>> partitionNameFileNameList)
+  public Map<Pair<String, String>, BloomFilter> getBloomFilters(final List<Pair<String, String>> partitionNameFileNameList, final String metadataPartitionName)
       throws HoodieMetadataException {
     throw new HoodieMetadataException("Unsupported operation: getBloomFilters!");
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/plan/generators/BaseHoodieCompactionPlanGenerator.java
Patch:
@@ -153,9 +153,9 @@ public HoodieCompactionPlan generateCompactionPlan(String compactionInstant) thr
         }), partitionPaths.size()).stream()
         .map(CompactionUtils::buildHoodieCompactionOperation).collect(toList());
 
-    LOG.info("Total of " + operations.size() + " compaction operations are retrieved");
-    LOG.info("Total number of log files " + totalLogFiles.value());
-    LOG.info("Total number of file slices " + totalFileSlices.value());
+    LOG.info("Total of {} compaction operations are retrieved", operations.size());
+    LOG.info("Total number of log files {}", totalLogFiles.value());
+    LOG.info("Total number of file slices {}", totalFileSlices.value());
 
     if (operations.isEmpty()) {
       LOG.warn("No operations are retrieved for {}", metaClient.getBasePath());

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -2952,11 +2952,11 @@ public void testDeletePartitions() throws Exception {
       validateMetadata(client);
 
       // delete partitions
-      newCommitTime = client.createNewInstantTime(5000);
+      newCommitTime = client.createNewInstantTime();
       client.deletePartitions(singletonList(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH), newCommitTime);
 
       // add 1 more commit
-      newCommitTime = client.createNewInstantTime(5000);
+      newCommitTime = client.createNewInstantTime();
       client.startCommitWithTime(newCommitTime);
       records = dataGen.generateInserts(newCommitTime, 10);
       upsertRecords = new ArrayList<>();

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieTimeGeneratorConfig.java
Patch:
@@ -53,7 +53,8 @@ public class HoodieTimeGeneratorConfig extends HoodieConfig {
       })
       .sinceVersion("1.0.0")
       .markAdvanced()
-      .withDocumentation("The max expected clock skew time for WaitBasedTimeGenerator in ms");
+      .withDocumentation("The max expected clock skew time in ms between two processes generating time. Used by "
+          + TimeGeneratorType.WAIT_TO_ADJUST_SKEW.name() + " time generator to implement TrueTime semantics.");
 
   private HoodieTimeGeneratorConfig() {
     super();

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstantTimeGenerator.java
Patch:
@@ -75,7 +75,7 @@ public static String createNewInstantTime(boolean shouldLock, TimeGenerator time
     return lastInstantTime.updateAndGet((oldVal) -> {
       String newCommitTime;
       do {
-        Date d = new Date(timeGenerator.currentTimeMillis(!shouldLock) + milliseconds);
+        Date d = new Date(timeGenerator.generateTime(!shouldLock) + milliseconds);
 
         if (commitTimeZone.equals(HoodieTimelineTimeZone.UTC)) {
           newCommitTime = d.toInstant().atZone(HoodieTimelineTimeZone.UTC.getZoneId())

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimeGeneratorBase.java
Patch:
@@ -89,7 +89,7 @@ protected LockProvider<?> getLockProvider() {
       synchronized (this) {
         if (lockProvider == null) {
           String lockProviderClass = lockConfiguration.getConfig().getString("hoodie.write.lock.provider");
-          LOG.info("LockProvider for TimeGenerator: " + lockProviderClass);
+          LOG.info("LockProvider for TimeGenerator: {}", lockProviderClass);
           lockProvider = (LockProvider<?>) ReflectionUtils.loadClass(lockProviderClass,
               new Class<?>[] {LockConfiguration.class, StorageConfiguration.class},
               lockConfiguration, storageConf);

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/v2/ActiveTimelineV2.java
Patch:
@@ -732,7 +732,7 @@ protected void createFileInMetaPath(String filename, Option<byte[]> content, boo
   protected void createCompleteFileInMetaPath(boolean shouldLock, HoodieInstant instant, Option<byte[]> content) {
     TimeGenerator timeGenerator = TimeGenerators
         .getTimeGenerator(metaClient.getTimeGeneratorConfig(), metaClient.getStorageConf());
-    timeGenerator.consumeTimestamp(!shouldLock, currentTimeMillis -> {
+    timeGenerator.consumeTime(!shouldLock, currentTimeMillis -> {
       String completionTime = HoodieInstantTimeGenerator.formatDate(new Date(currentTimeMillis));
       String fileName = instantFileNameGenerator.getFileName(completionTime, instant);
       StoragePath fullPath = getInstantFileNamePath(fileName);

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -185,9 +185,6 @@ private HoodieTableMetadataUtil() {
       DoubleWrapper.class, FloatWrapper.class, LongWrapper.class,
       StringWrapper.class, TimeMicrosWrapper.class, TimestampMicrosWrapper.class));
 
-  // we have max of 4 partitions (FILES, COL_STATS, BLOOM, RLI)
-  private static final List<String> VALID_PARTITION_INITIALIZATION_TIME_SUFFIXES = Arrays.asList("010", "011", "012", "013");
-
   /**
    * Returns whether the files partition of metadata table is ready for read.
    *

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/table/timeline/TestWaitBasedTimeGenerator.java
Patch:
@@ -113,7 +113,7 @@ public void testSlowerThreadLaterAcquiredLock(boolean slowerThreadAcquiredLockLa
       try {
         MockInProcessLockProvider.needToLockLater(!slowerThreadAcquiredLockLater);
         TimeGenerator timeGenerator = TimeGenerators.getTimeGenerator(timeGeneratorConfig, storageConf);
-        t1Timestamp.set(timeGenerator.currentTimeMillis(false));
+        t1Timestamp.set(timeGenerator.generateTime(false));
       } catch (Exception e) {
         throw new RuntimeException(e);
       }
@@ -125,7 +125,7 @@ public void testSlowerThreadLaterAcquiredLock(boolean slowerThreadAcquiredLockLa
         MockInProcessLockProvider.needToLockLater(slowerThreadAcquiredLockLater);
         TimeGenerator timeGenerator = TimeGenerators.getTimeGenerator(timeGeneratorConfig, storageConf);
         // Pretend t2 is slower 20ms than t1
-        t2Timestamp.set(timeGenerator.currentTimeMillis(false) - clockSkewTime);
+        t2Timestamp.set(timeGenerator.generateTime(false) - clockSkewTime);
       } catch (Exception e) {
         throw new RuntimeException(e);
       }

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/table/view/TestHoodieTableFileSystemView.java
Patch:
@@ -1015,7 +1015,6 @@ protected void testViewForFileSlicesWithAsyncCompaction(boolean skipCreatingData
     assertEquals(!skipCreatingDataFile ? instantTime1 : deltaInstantTime1, fileSlice.getBaseInstantTime(),
         "Base Instant of penultimate file-slice must be base instant");
     List<HoodieLogFile> logFiles = fileSlice.getLogFiles().collect(Collectors.toList());
-    //FIXME-vc: real issue here.. the files written after pending compaction are getting filtered out.
     assertEquals(4, logFiles.size(), "Log files must include those after compaction request");
     assertEquals(fileName4, logFiles.get(0).getFileName(), "Log File Order check");
     assertEquals(fileName3, logFiles.get(1).getFileName(), "Log File Order check");

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/utils/HoodieWriterClientTestHarness.java
Patch:
@@ -43,6 +43,7 @@
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.IOType;
 import org.apache.hudi.common.model.WriteConcurrencyMode;
+import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.HoodieTableVersion;
@@ -742,6 +743,7 @@ protected void testConsistencyCheckDuringFinalize(HoodieEngineContext context, b
     HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder()
             .withEnableOptimisticConsistencyGuard(enableOptimisticConsistencyGuard).build()).build();
     BaseHoodieWriteClient client = getHoodieWriteClient(cfg);
+    client.setOperationType(WriteOperationType.UPSERT);
     Pair<StoragePath, List<WriteStatus>> result = testConsistencyCheck(context, metaClient, instantTime,
               enableOptimisticConsistencyGuard, getHoodieTableFn, transformInputFn, transformOutputFn);
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestExternalPathHandling.java
Patch:
@@ -101,6 +101,7 @@ public void testFlow(FileIdAndNameGenerator fileIdAndNameGenerator, List<String>
         .build();
 
     writeClient = getHoodieWriteClient(writeConfig);
+    writeClient.setOperationType(WriteOperationType.INSERT_OVERWRITE);
     String instantTime1 = writeClient.startCommit(HoodieTimeline.REPLACE_COMMIT_ACTION, metaClient);
     String partitionPath1 = partitions.get(0);
     Pair<String, String> fileIdAndName1 = fileIdAndNameGenerator.generate(1, instantTime1);

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -360,7 +360,7 @@ public final class HoodieMetadataConfig extends HoodieConfig {
 
   public static final ConfigProperty<Boolean> SECONDARY_INDEX_ENABLE_PROP = ConfigProperty
       .key(METADATA_PREFIX + ".index.secondary.enable")
-      .defaultValue(false)
+      .defaultValue(true)
       .sinceVersion("1.0.0")
       .withDocumentation("Enable secondary index within the Metadata Table.");
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -172,6 +172,7 @@ public class HoodieTableMetadataUtil {
   public static final String PARTITION_NAME_COLUMN_STATS = "column_stats";
   public static final String PARTITION_NAME_BLOOM_FILTERS = "bloom_filters";
   public static final String PARTITION_NAME_RECORD_INDEX = "record_index";
+  public static final String PARTITION_NAME_FUNCTIONAL_INDEX = "func_index";
   public static final String PARTITION_NAME_FUNCTIONAL_INDEX_PREFIX = "func_index_";
   public static final String PARTITION_NAME_SECONDARY_INDEX = "secondary_index";
   public static final String PARTITION_NAME_SECONDARY_INDEX_PREFIX = "secondary_index_";

File: hudi-common/src/main/java/org/apache/hudi/metadata/MetadataPartitionType.java
Patch:
@@ -47,6 +47,7 @@
 import static org.apache.hudi.common.config.HoodieMetadataConfig.ENABLE_METADATA_INDEX_PARTITION_STATS;
 import static org.apache.hudi.common.config.HoodieMetadataConfig.FUNCTIONAL_INDEX_ENABLE_PROP;
 import static org.apache.hudi.common.config.HoodieMetadataConfig.RECORD_INDEX_ENABLE_PROP;
+import static org.apache.hudi.common.config.HoodieMetadataConfig.SECONDARY_INDEX_ENABLE_PROP;
 import static org.apache.hudi.common.util.ConfigUtils.getBooleanWithAltKeys;
 import static org.apache.hudi.common.util.TypeUtils.unsafeCast;
 import static org.apache.hudi.common.util.ValidationUtils.checkArgument;
@@ -200,9 +201,7 @@ public String getPartitionPath(HoodieTableMetaClient metaClient, String indexNam
   SECONDARY_INDEX(HoodieTableMetadataUtil.PARTITION_NAME_SECONDARY_INDEX_PREFIX, "secondary-index-", 7) {
     @Override
     public boolean isMetadataPartitionEnabled(TypedProperties writeConfig) {
-      // Secondary index is created via sql and not via write path.
-      // HUDI-7662 tracks adding a separate config to enable/disable secondary index.
-      return false;
+      return getBooleanWithAltKeys(writeConfig, SECONDARY_INDEX_ENABLE_PROP);
     }
 
     @Override

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestHoodieLogFileCommand.java
Patch:
@@ -112,7 +112,7 @@ public void init() throws IOException, InterruptedException, URISyntaxException
     try (HoodieLogFormat.Writer writer = HoodieLogFormat.newWriterBuilder()
         .onParentPath(new StoragePath(partitionPath))
         .withFileExtension(HoodieLogFile.DELTA_EXTENSION)
-        .withFileId("test-log-fileid1").withDeltaCommit("100").withStorage(storage)
+        .withFileId("test-log-fileid1").withInstantTime("100").withStorage(storage)
         .withSizeThreshold(1).build()) {
 
       // write data to file
@@ -209,7 +209,7 @@ public void testShowLogFileRecordsWithMerge() throws IOException, InterruptedExc
       writer =
           HoodieLogFormat.newWriterBuilder().onParentPath(new StoragePath(partitionPath))
               .withFileExtension(HoodieLogFile.DELTA_EXTENSION)
-              .withFileId("test-log-fileid1").withDeltaCommit(INSTANT_TIME).withStorage(
+              .withFileId("test-log-fileid1").withInstantTime(INSTANT_TIME).withStorage(
                   storage)
               .withSizeThreshold(500).build();
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/timeline/versioning/v1/TimelineArchiverV1.java
Patch:
@@ -110,7 +110,7 @@ public TimelineArchiverV1(HoodieWriteConfig config, HoodieTable<T, I, K, O> tabl
   private Writer openWriter() {
     try {
       if (this.writer == null) {
-        return HoodieLogFormat.newWriterBuilder().onParentPath(archiveFilePath.getParent()).withDeltaCommit("")
+        return HoodieLogFormat.newWriterBuilder().onParentPath(archiveFilePath.getParent()).withInstantTime("")
             .withFileId(archiveFilePath.getName()).withFileExtension(HoodieArchivedLogFile.ARCHIVE_EXTENSION)
             .withStorage(metaClient.getStorage()).build();
       } else {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -921,13 +921,13 @@ private void initializeFileGroups(HoodieTableMetaClient dataMetaClient, Metadata
         try (HoodieLogFormat.Writer writer = HoodieLogFormat.newWriterBuilder()
             .onParentPath(FSUtils.constructAbsolutePath(metadataWriteConfig.getBasePath(), partitionName))
             .withFileId(fileGroupFileId)
-            .withDeltaCommit(instantTime)
+            .withInstantTime(instantTime)
             .withLogVersion(HoodieLogFile.LOGFILE_BASE_VERSION)
             .withFileSize(0L)
             .withSizeThreshold(metadataWriteConfig.getLogFileMaxSize())
             .withStorage(dataMetaClient.getStorage())
-            .withRolloverLogWriteToken(HoodieLogFormat.DEFAULT_WRITE_TOKEN)
             .withLogWriteToken(HoodieLogFormat.DEFAULT_WRITE_TOKEN)
+            .withTableVersion(metadataWriteConfig.getWriteVersion())
             .withFileExtension(HoodieLogFile.DELTA_EXTENSION).build()) {
           writer.appendBlock(block);
         }

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/avro/TestHoodieAvroParquetWriter.java
Patch:
@@ -18,10 +18,10 @@
 
 package org.apache.hudi.avro;
 
-import org.apache.hudi.DummyTaskContextSupplier;
 import org.apache.hudi.common.bloom.BloomFilter;
 import org.apache.hudi.common.bloom.BloomFilterFactory;
 import org.apache.hudi.common.bloom.BloomFilterTypeCode;
+import org.apache.hudi.common.engine.LocalTaskContextSupplier;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.testutils.HoodieTestUtils;
 import org.apache.hudi.common.util.Option;
@@ -75,7 +75,7 @@ public void testProperWriting() throws IOException {
     StoragePath filePath = new StoragePath(tmpDir.resolve("test.parquet").toAbsolutePath().toString());
 
     try (HoodieAvroParquetWriter writer =
-             new HoodieAvroParquetWriter(filePath, parquetConfig, "001", new DummyTaskContextSupplier(), true)) {
+             new HoodieAvroParquetWriter(filePath, parquetConfig, "001", new LocalTaskContextSupplier(), true)) {
       for (GenericRecord record : records) {
         writer.writeAvro((String) record.get("_row_key"), record);
       }

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/client/utils/TestLegacyArchivedMetaEntryReader.java
Patch:
@@ -105,7 +105,7 @@ private HoodieLogFormat.Writer openWriter(HoodieTableMetaClient metaClient) {
       return HoodieLogFormat.newWriterBuilder()
           .onParentPath(new StoragePath(metaClient.getArchivePath()))
           .withFileId("commits").withFileExtension(HoodieArchivedLogFile.ARCHIVE_EXTENSION)
-          .withStorage(metaClient.getStorage()).withDeltaCommit("").build();
+          .withStorage(metaClient.getStorage()).withInstantTime("").build();
     } catch (IOException e) {
       throw new HoodieException("Unable to initialize HoodieLogFormat writer", e);
     }

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java
Patch:
@@ -176,7 +176,7 @@ private Pair<String, HoodieLogFile> appendRecordsToLogFile(String partitionPath,
     try (HoodieLogFormat.Writer logWriter = HoodieLogFormat.newWriterBuilder()
         .onParentPath(new StoragePath(basePath, partitionPath))
         .withFileExtension(HoodieLogFile.DELTA_EXTENSION).withFileId(fileId)
-        .withDeltaCommit(currentInstantTime).withStorage(storage).build()) {
+        .withInstantTime(currentInstantTime).withStorage(storage).build()) {
       Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
       header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, currentInstantTime);
       header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());

File: hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkWriteableTestTable.java
Patch:
@@ -139,7 +139,7 @@ private Pair<String, HoodieLogFile> appendRecordsToLogFile(List<HoodieRecord> gr
     try (HoodieLogFormat.Writer logWriter = HoodieLogFormat.newWriterBuilder()
         .onParentPath(new StoragePath(basePath, partitionPath))
         .withFileExtension(HoodieLogFile.DELTA_EXTENSION).withFileId(location.getFileId())
-        .withDeltaCommit(location.getInstantTime()).withStorage(storage).build()) {
+        .withInstantTime(location.getInstantTime()).withStorage(storage).build()) {
       Map<HeaderMetadataType, String> header = new java.util.HashMap<>();
       header.put(HeaderMetadataType.INSTANT_TIME, location.getInstantTime());
       header.put(HeaderMetadataType.SCHEMA, schema.toString());

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/client/common/TestHoodieJavaEngineContext.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.client.common;
 
-import org.apache.hudi.DummyTaskContextSupplier;
+import org.apache.hudi.common.engine.LocalTaskContextSupplier;
 import org.apache.hudi.common.util.collection.ImmutablePair;
 
 import org.junit.jupiter.api.Assertions;
@@ -34,7 +34,7 @@
 
 public class TestHoodieJavaEngineContext {
   private HoodieJavaEngineContext context =
-      new HoodieJavaEngineContext(getDefaultStorageConf(), new DummyTaskContextSupplier());
+      new HoodieJavaEngineContext(getDefaultStorageConf(), new LocalTaskContextSupplier());
 
   @Test
   public void testMap() {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableInsertUpdateDelete.java
Patch:
@@ -394,11 +394,10 @@ public void testSimpleInsertsGeneratedIntoLogFiles() throws Exception {
               FSUtils.constructAbsolutePath(config.getBasePath(),
                   correctWriteStat.getPartitionPath()))
           .withFileId(correctWriteStat.getFileId())
-          .withDeltaCommit(newCommitTime)
+          .withInstantTime(newCommitTime)
           .withLogVersion(correctLogFile.getLogVersion())
           .withFileSize(0L)
           .withSizeThreshold(config.getLogFileMaxSize()).withStorage(hoodieStorage())
-          .withRolloverLogWriteToken(fakeToken)
           .withLogWriteToken(fakeToken)
           .withFileExtension(HoodieLogFile.DELTA_EXTENSION)
           .withFileCreationCallback(new LogFileCreationCallback() {

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieLogFile.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.table.cdc.HoodieCDCUtils;
 import org.apache.hudi.exception.InvalidHoodiePathException;
-import org.apache.hudi.storage.HoodieStorage;
 import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.storage.StoragePathInfo;
 
@@ -182,7 +181,7 @@ public void setPathInfo(StoragePathInfo pathInfo) {
     this.pathInfo = pathInfo;
   }
 
-  public HoodieLogFile rollOver(HoodieStorage storage, String logWriteToken) {
+  public HoodieLogFile rollOver(String logWriteToken) {
     String fileId = getFileId();
     String deltaCommitTime = getDeltaCommitTime();
     StoragePath path = getPath();

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieMergedLogRecordReader.java
Patch:
@@ -277,7 +277,7 @@ public Builder<T> withOptimizedLogBlocksScan(boolean enableOptimizedLogBlocksSca
       return this;
     }
 
-    public Builder<T> withKeyFiledOverride(String keyFieldOverride) {
+    public Builder<T> withKeyFieldOverride(String keyFieldOverride) {
       this.keyFieldOverride = Objects.requireNonNull(keyFieldOverride);
       return this;
     }

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieUnMergedLogRecordScanner.java
Patch:
@@ -39,7 +39,7 @@
 /**
  * A scanner used to scan hoodie unmerged log records.
  */
-public class HoodieUnMergedLogRecordScanner extends AbstractHoodieLogRecordReader {
+public class HoodieUnMergedLogRecordScanner extends AbstractHoodieLogRecordScanner {
 
   private final LogRecordScannerCallback callback;
 
@@ -98,7 +98,7 @@ public interface LogRecordScannerCallback {
   /**
    * Builder used to build {@code HoodieUnMergedLogRecordScanner}.
    */
-  public static class Builder extends AbstractHoodieLogRecordReader.Builder {
+  public static class Builder extends AbstractHoodieLogRecordScanner.Builder {
     private HoodieStorage storage;
     private String basePath;
     private List<String> logFilePaths;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlockVersion.java
Patch:
@@ -20,7 +20,7 @@
 
 /**
  * A set of feature flags associated with a data log block format. Versions are changed when the log block format
- * changes. TODO(na) - Implement policies around major/minor versions
+ * changes.
  */
 final class HoodieAvroDataBlockVersion extends HoodieLogBlockVersion {
 

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/reader/HoodieFileSliceTestUtils.java
Patch:
@@ -302,7 +302,7 @@ public static HoodieLogFile createLogFile(
                  .onParentPath(new StoragePath(logFilePath).getParent())
                  .withFileExtension(HoodieLogFile.DELTA_EXTENSION)
                  .withFileId(fileId)
-                 .withDeltaCommit(logInstantTime)
+                 .withInstantTime(logInstantTime)
                  .withLogVersion(version)
                  .withStorage(storage).build()) {
       Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();

File: hudi-hadoop-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java
Patch:
@@ -217,7 +217,7 @@ private void rolloverIfNeeded() throws IOException {
 
   private void rollOver() throws IOException {
     closeStream();
-    this.logFile = logFile.rollOver(storage, rolloverLogWriteToken);
+    this.logFile = logFile.rollOver(rolloverLogWriteToken);
     this.closed = false;
   }
 

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormatAppendFailure.java
Patch:
@@ -118,7 +118,7 @@ public void testFailedToGetAppendStreamFromHDFSNameNode()
 
     Writer writer = HoodieLogFormat.newWriterBuilder().onParentPath(testPath)
         .withFileExtension(HoodieArchivedLogFile.ARCHIVE_EXTENSION).withFileId("commits")
-        .withDeltaCommit("").withStorage(storage).build();
+        .withInstantTime("").withStorage(storage).build();
 
     writer.appendBlock(dataBlock);
     // get the current log file version to compare later
@@ -150,7 +150,7 @@ public void testFailedToGetAppendStreamFromHDFSNameNode()
     // return a new writer with a bumped up logVersion
     writer = HoodieLogFormat.newWriterBuilder().onParentPath(testPath)
         .withFileExtension(HoodieArchivedLogFile.ARCHIVE_EXTENSION).withFileId("commits")
-        .withDeltaCommit("").withStorage(storage).build();
+        .withInstantTime("").withStorage(storage).build();
     header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,
         String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_BLOCK.ordinal()));

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/table/TestTableSchemaResolver.java
Patch:
@@ -109,7 +109,7 @@ private StoragePath writeLogFile(StoragePath partitionPath, Schema schema) throw
     HoodieStorage storage = HoodieTestUtils.getStorage(partitionPath);
     HoodieLogFormat.Writer writer =
         HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)
-            .withFileId("test-fileid1").withDeltaCommit("100").withStorage(storage).build();
+            .withFileId("test-fileid1").withInstantTime("100").withStorage(storage).build();
     List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);
     Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java
Patch:
@@ -709,7 +709,7 @@ private static HoodieLogFile generateLogData(StoragePath parquetFilePath,
     // Write a log file for this parquet file
     Writer logWriter = HoodieLogFormat.newWriterBuilder().onParentPath(parquetFilePath.getParent())
         .withFileExtension(HoodieLogFile.DELTA_EXTENSION).withFileId(dataFile.getFileId())
-        .withDeltaCommit(dataFile.getCommitTime()).withStorage(storage).build();
+        .withInstantTime(dataFile.getCommitTime()).withStorage(storage).build();
     List<HoodieRecord> records = (isLogSchemaSimple ? SchemaTestUtil.generateTestRecords(0, 100)
         : SchemaTestUtil.generateEvolvedTestRecords(100, 100)).stream()
         .map(HoodieAvroIndexedRecord::new).collect(Collectors.toList());
@@ -729,7 +729,7 @@ private static HoodieLogFile generateLogData(StoragePath parquetFilePath, String
     // Write a log file for this parquet file
     Writer logWriter = HoodieLogFormat.newWriterBuilder().onParentPath(parquetFilePath.getParent())
         .withFileExtension(HoodieLogFile.DELTA_EXTENSION).withFileId(dataFile.getFileId())
-        .withDeltaCommit(dataFile.getCommitTime()).withStorage(storage).build();
+        .withInstantTime(dataFile.getCommitTime()).withStorage(storage).build();
     List<HoodieRecord> records = SchemaTestUtil.generateTestRecords(logSchemaPath, dataPath).stream().map(HoodieAvroIndexedRecord::new).collect(Collectors.toList());
     Map<HeaderMetadataType, String> header = new HashMap<>(2);
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, dataFile.getCommitTime());

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordGlobalLocation.java
Patch:
@@ -67,8 +67,7 @@ public boolean equals(Object o) {
     HoodieRecordGlobalLocation otherLoc = (HoodieRecordGlobalLocation) o;
     return Objects.equals(partitionPath, otherLoc.partitionPath)
         && Objects.equals(instantTime, otherLoc.instantTime)
-        && Objects.equals(fileId, otherLoc.fileId)
-        && Objects.equals(position, otherLoc.position);
+        && Objects.equals(fileId, otherLoc.fileId);
   }
 
   @Override

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/client/TestJavaHoodieBackedMetadata.java
Patch:
@@ -2414,7 +2414,7 @@ public void testMetadataMetrics() throws Exception {
   @Test
   public void testGetFileGroupIndexFromFileId() {
     int index = new Random().nextInt(10000);
-    String fileId = HoodieTableMetadataUtil.getFileIDForFileGroup(FILES, index);
+    String fileId = HoodieTableMetadataUtil.getFileIDForFileGroup(FILES, index, FILES.getPartitionPath());
     assertEquals(fileId.substring(0, fileId.length() - 2), HoodieTableMetadataUtil.getFileGroupPrefix(fileId));
     assertEquals(index, HoodieTableMetadataUtil.getFileGroupIndexFromFileId(fileId));
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -3149,7 +3149,7 @@ public void testMetadataMetrics() throws Exception {
   @Test
   public void testGetFileGroupIndexFromFileId() {
     int index = new Random().nextInt(10000);
-    String fileId = HoodieTableMetadataUtil.getFileIDForFileGroup(FILES, index);
+    String fileId = HoodieTableMetadataUtil.getFileIDForFileGroup(FILES, index, FILES.getPartitionPath());
     assertEquals(fileId.substring(0, fileId.length() - 2), HoodieTableMetadataUtil.getFileGroupPrefix(fileId));
     assertEquals(index, HoodieTableMetadataUtil.getFileGroupIndexFromFileId(fileId));
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -1437,7 +1437,6 @@ protected void tryUpgrade(HoodieTableMetaClient metaClient, Option<String> insta
         new UpgradeDowngrade(metaClient, config, context, upgradeDowngradeHelper);
 
     if (upgradeDowngrade.needsUpgradeOrDowngrade(HoodieTableVersion.current())) {
-      metaClient = HoodieTableMetaClient.reload(metaClient);
       // Ensure no inflight commits by setting EAGER policy and explicitly cleaning all failed commits
       List<String> instantsToRollback = tableServiceClient.getInstantsToRollback(metaClient, HoodieFailedWritesCleaningPolicy.EAGER, instantTime);
 
@@ -1450,6 +1449,7 @@ protected void tryUpgrade(HoodieTableMetaClient metaClient, Option<String> insta
       new UpgradeDowngrade(metaClient, config, context, upgradeDowngradeHelper)
           .run(HoodieTableVersion.current(), instantTime.orElse(null));
 
+      metaClient.reloadTableConfig();
       metaClient.reloadActiveTimeline();
     }
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/util/CommonClientUtils.java
Patch:
@@ -28,8 +28,8 @@ public class CommonClientUtils {
   public static void validateTableVersion(HoodieTableConfig tableConfig, HoodieWriteConfig writeConfig) {
     // mismatch of table versions.
     if (!tableConfig.getTableVersion().equals(writeConfig.getWriteVersion())) {
-      throw new HoodieNotSupportedException(String.format("Table version (%s) and Writer version (%s) do not match.",
-          tableConfig.getTableVersion(), writeConfig.getWriteVersion()));
+      throw new HoodieNotSupportedException(String.format("Table version (%s) and Writer version (%s) do not match for table at: %s.",
+          tableConfig.getTableVersion(), writeConfig.getWriteVersion(), writeConfig.getBasePath()));
     }
   }
 }

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.hudi.common.util.ReflectionUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieIOException;
+import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 import org.apache.hudi.metadata.HoodieTableMetadata;
 import org.apache.hudi.storage.HoodieStorage;
 import org.apache.hudi.storage.HoodieStorageUtils;
@@ -137,6 +138,7 @@ public static HoodieTableMetaClient init(String basePath, HoodieFileFormat baseF
   public static HoodieTableMetaClient init(String basePath, HoodieTableType tableType, HoodieTableVersion version) throws IOException {
     Properties properties = new Properties();
     properties.setProperty(HoodieTableConfig.VERSION.key(), String.valueOf(version.versionCode()));
+    properties.setProperty(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME.key(), "partition");
     return init(getDefaultStorageConf(), basePath, tableType, properties);
   }
 

File: hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/HoodieFlinkQuickstart.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.examples.quickstart.utils.QuickstartConfigurations;
 
 import org.apache.flink.configuration.Configuration;
-import org.apache.flink.configuration.JobManagerOptions;
 import org.apache.flink.core.execution.JobClient;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.api.EnvironmentSettings;
@@ -42,7 +41,6 @@
 import org.apache.flink.types.Row;
 import org.jetbrains.annotations.NotNull;
 
-import java.time.Duration;
 import java.util.Collection;
 import java.util.List;
 import java.util.concurrent.ExecutionException;

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/ITTestHoodieDataSource.java
Patch:
@@ -700,7 +700,7 @@ void testBatchModeUpsertWithoutPartition(HoodieTableType tableType) {
     TableEnvironment tableEnv = batchTableEnv;
     String hoodieTableDDL = sql("t1")
         .option(FlinkOptions.PATH, tempFile.getAbsolutePath())
-        .option(FlinkOptions.TABLE_NAME, tableType.name())
+        .option(FlinkOptions.TABLE_TYPE, tableType)
         .option("hoodie.parquet.small.file.limit", "0") // invalidate the small file strategy
         .option("hoodie.parquet.max.file.size", "0")
         .noPartition()
@@ -726,7 +726,7 @@ void testBatchModeUpsert(HoodieTableType tableType, boolean hiveStylePartitionin
     TableEnvironment tableEnv = batchTableEnv;
     String hoodieTableDDL = sql("t1")
         .option(FlinkOptions.PATH, tempFile.getAbsolutePath())
-        .option(FlinkOptions.TABLE_NAME, tableType)
+        .option(FlinkOptions.TABLE_TYPE, tableType)
         .option(FlinkOptions.HIVE_STYLE_PARTITIONING, hiveStylePartitioning)
         .end();
     tableEnv.executeSql(hoodieTableDDL);

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/client/TestJavaHoodieBackedMetadata.java
Patch:
@@ -946,7 +946,7 @@ private void verifyMetadataMergedRecords(HoodieTableMetaClient metadataMetaClien
     if (enableMetaFields) {
       schema = HoodieAvroUtils.addMetadataFields(schema);
     }
-    HoodieMetadataLogRecordReader logRecordReader = HoodieMetadataLogRecordReader.newBuilder(FILES.getPartitionPath())
+    HoodieMetadataLogRecordReader logRecordReader = HoodieMetadataLogRecordReader.newBuilder()
         .withStorage(metadataMetaClient.getStorage())
         .withBasePath(metadataMetaClient.getBasePath())
         .withLogFilePaths(logFilePaths)

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -245,7 +245,7 @@ public HoodieData<HoodieRecord> getDeletedSecondaryRecordMapping(HoodieEngineCon
 
     List<HoodieRecord> deletedRecords = new ArrayList<>();
     recordKeySecondaryKeyMap.forEach((key, value) -> {
-      HoodieRecord<HoodieMetadataPayload> siRecord = HoodieMetadataPayload.createSecondaryIndex(key, value, indexDefinition.getIndexName(), true);
+      HoodieRecord<HoodieMetadataPayload> siRecord = HoodieMetadataPayload.createSecondaryIndexRecord(key, value, indexDefinition.getIndexName(), true);
       deletedRecords.add(siRecord);
     });
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -1424,7 +1424,7 @@ private void verifyMetadataMergedRecords(HoodieTableMetaClient metadataMetaClien
     if (enableMetaFields) {
       schema = HoodieAvroUtils.addMetadataFields(schema);
     }
-    HoodieMetadataLogRecordReader logRecordReader = HoodieMetadataLogRecordReader.newBuilder(FILES.getPartitionPath())
+    HoodieMetadataLogRecordReader logRecordReader = HoodieMetadataLogRecordReader.newBuilder()
         .withStorage(metadataMetaClient.getStorage())
         .withBasePath(metadataMetaClient.getBasePath())
         .withLogFilePaths(logFilePaths)

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedTableMetadata.java
Patch:
@@ -492,7 +492,7 @@ private void verifyMetadataRawRecords(HoodieTable table, List<HoodieLogFile> log
    */
   private void verifyMetadataMergedRecords(HoodieTableMetaClient metadataMetaClient, List<String> logFilePaths, String latestCommitTimestamp) {
     Schema schema = HoodieAvroUtils.addMetadataFields(HoodieMetadataRecord.getClassSchema());
-    HoodieMetadataLogRecordReader logRecordReader = HoodieMetadataLogRecordReader.newBuilder(FILES.getPartitionPath())
+    HoodieMetadataLogRecordReader logRecordReader = HoodieMetadataLogRecordReader.newBuilder()
         .withStorage(metadataMetaClient.getStorage())
         .withBasePath(metadataMetaClient.getBasePath())
         .withLogFilePaths(logFilePaths)

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/HoodieStreamer.java
Patch:
@@ -271,8 +271,9 @@ public static class Config implements Serializable {
         + " to break ties between records with same key in input data. Default: 'ts' holding unix timestamp of record")
     public String sourceOrderingField = "ts";
 
-    @Parameter(names = {"--payload-class"}, description = "Deprecated. Use --merge-mode for overwite or event time merging."
-        + " Subclass of HoodieRecordPayload, that works off a GenericRecord. Implement your own, if you want to do something "
+    @Parameter(names = {"--payload-class"}, description = "Deprecated. "
+        + "Use --merge-mode for overwrite with latest or event time merging. "
+        + "Subclass of HoodieRecordPayload, that works off a GenericRecord. Implement your own, if you want to do something "
         + "other than overwriting existing value")
     public String payloadClassName = null;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java
Patch:
@@ -82,7 +82,7 @@ public HoodieAvroDataBlock(Supplier<SeekableDataInputStream> inputStreamSupplier
                              HoodieLogBlockContentLocation logBlockContentLocation,
                              Option<Schema> readerSchema,
                              Map<HeaderMetadataType, String> header,
-                             Map<HeaderMetadataType, String> footer,
+                             Map<FooterMetadataType, String> footer,
                              String keyField) {
     super(content, inputStreamSupplier, readBlockLazily, Option.of(logBlockContentLocation), readerSchema, header, footer, keyField, false);
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieCommandBlock.java
Patch:
@@ -46,7 +46,7 @@ public HoodieCommandBlock(Map<HeaderMetadataType, String> header) {
 
   public HoodieCommandBlock(Option<byte[]> content, Supplier<SeekableDataInputStream> inputStreamSupplier, boolean readBlockLazily,
                             Option<HoodieLogBlockContentLocation> blockContentLocation, Map<HeaderMetadataType, String> header,
-                            Map<HeaderMetadataType, String> footer) {
+                            Map<FooterMetadataType, String> footer) {
     super(header, footer, blockContentLocation, content, inputStreamSupplier, readBlockLazily);
     this.type =
         HoodieCommandBlockTypeEnum.values()[Integer.parseInt(header.get(HeaderMetadataType.COMMAND_BLOCK_TYPE))];

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieCorruptBlock.java
Patch:
@@ -34,7 +34,7 @@ public class HoodieCorruptBlock extends HoodieLogBlock {
 
   public HoodieCorruptBlock(Option<byte[]> corruptedBytes, Supplier<SeekableDataInputStream> inputStreamSupplier, boolean readBlockLazily,
                             Option<HoodieLogBlockContentLocation> blockContentLocation, Map<HeaderMetadataType, String> header,
-                            Map<HeaderMetadataType, String> footer) {
+                            Map<FooterMetadataType, String> footer) {
     super(header, footer, blockContentLocation, corruptedBytes, inputStreamSupplier, readBlockLazily);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDataBlock.java
Patch:
@@ -81,7 +81,7 @@ public abstract class HoodieDataBlock extends HoodieLogBlock {
   public HoodieDataBlock(List<HoodieRecord> records,
                          boolean shouldWriteRecordPositions,
                          Map<HeaderMetadataType, String> header,
-                         Map<HeaderMetadataType, String> footer,
+                         Map<FooterMetadataType, String> footer,
                          String keyFieldName) {
     super(header, footer, Option.empty(), Option.empty(), null, false);
     if (shouldWriteRecordPositions) {
@@ -116,7 +116,7 @@ protected HoodieDataBlock(Option<byte[]> content,
                             Option<HoodieLogBlockContentLocation> blockContentLocation,
                             Option<Schema> readerSchema,
                             Map<HeaderMetadataType, String> headers,
-                            Map<HeaderMetadataType, String> footer,
+                            Map<FooterMetadataType, String> footer,
                             String keyFieldName,
                             boolean enablePointLookups) {
     super(headers, footer, blockContentLocation, content, inputStreamSupplier, readBlockLazily);

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDeleteBlock.java
Patch:
@@ -100,14 +100,14 @@ public HoodieDeleteBlock(List<Pair<DeleteRecord, Long>> recordsToDelete,
 
   public HoodieDeleteBlock(Option<byte[]> content, Supplier<SeekableDataInputStream> inputStreamSupplier, boolean readBlockLazily,
                            Option<HoodieLogBlockContentLocation> blockContentLocation, Map<HeaderMetadataType, String> header,
-                           Map<HeaderMetadataType, String> footer) {
+                           Map<FooterMetadataType, String> footer) {
     // Setting `shouldWriteRecordPositions` to false as this constructor is only used by the reader
     this(content, inputStreamSupplier, readBlockLazily, blockContentLocation, header, footer, false);
   }
 
   HoodieDeleteBlock(Option<byte[]> content, Supplier<SeekableDataInputStream> inputStreamSupplier, boolean readBlockLazily,
                     Option<HoodieLogBlockContentLocation> blockContentLocation, Map<HeaderMetadataType, String> header,
-                    Map<HeaderMetadataType, String> footer, boolean shouldWriteRecordPositions) {
+                    Map<FooterMetadataType, String> footer, boolean shouldWriteRecordPositions) {
     super(header, footer, blockContentLocation, content, inputStreamSupplier, readBlockLazily);
     this.shouldWriteRecordPositions = shouldWriteRecordPositions;
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java
Patch:
@@ -73,7 +73,7 @@ public HoodieHFileDataBlock(Supplier<SeekableDataInputStream> inputStreamSupplie
                               HoodieLogBlockContentLocation logBlockContentLocation,
                               Option<Schema> readerSchema,
                               Map<HeaderMetadataType, String> header,
-                              Map<HeaderMetadataType, String> footer,
+                              Map<FooterMetadataType, String> footer,
                               boolean enablePointLookups,
                               StoragePath pathForReader,
                               boolean useNativeHFileReader) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieParquetDataBlock.java
Patch:
@@ -59,7 +59,7 @@ public HoodieParquetDataBlock(Supplier<SeekableDataInputStream> inputStreamSuppl
                                 HoodieLogBlockContentLocation logBlockContentLocation,
                                 Option<Schema> readerSchema,
                                 Map<HeaderMetadataType, String> header,
-                                Map<HeaderMetadataType, String> footer,
+                                Map<FooterMetadataType, String> footer,
                                 String keyField) {
     super(content, inputStreamSupplier, readBlockLazily, Option.of(logBlockContentLocation), readerSchema, header, footer, keyField, false);
 

File: hudi-hadoop-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java
Patch:
@@ -146,11 +146,11 @@ public AppendResult appendBlocks(List<HoodieLogBlock> blocks) throws IOException
       outputStream.write(HoodieLogFormat.MAGIC);
 
       // bytes for header
-      byte[] headerBytes = HoodieLogBlock.getLogMetadataBytes(block.getLogBlockHeader());
+      byte[] headerBytes = HoodieLogBlock.getHeaderMetadataBytes(block.getLogBlockHeader());
       // content bytes
       byte[] content = block.getContentBytes(storage);
       // bytes for footer
-      byte[] footerBytes = HoodieLogBlock.getLogMetadataBytes(block.getLogBlockFooter());
+      byte[] footerBytes = HoodieLogBlock.getFooterMetadataBytes(block.getLogBlockFooter());
 
       // 2. Write the total size of the block (excluding Magic)
       outputStream.writeLong(getLogBlockLength(content.length, headerBytes.length, footerBytes.length));

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -1246,7 +1246,7 @@ public void testAvroLogRecordReaderWithFailedPartialBlock(ExternalSpillableMap.D
     outputStream.writeInt(HoodieLogBlockType.AVRO_DATA_BLOCK.ordinal());
 
     // Write out some header
-    outputStream.write(HoodieLogBlock.getLogMetadataBytes(header));
+    outputStream.write(HoodieLogBlock.getHeaderMetadataBytes(header));
     outputStream.writeLong(getUTF8Bytes("something-random").length);
     outputStream.write(getUTF8Bytes("something-random"));
     outputStream.flush();
@@ -2569,7 +2569,7 @@ public void testAppendAndReadOnCorruptedLogInReverse()
     outputStream.writeInt(1);
     // Write out some metadata
     // TODO : test for failure to write metadata - NA ?
-    outputStream.write(HoodieLogBlock.getLogMetadataBytes(header));
+    outputStream.write(HoodieLogBlock.getHeaderMetadataBytes(header));
     outputStream.write(getUTF8Bytes("something-random"));
     outputStream.flush();
     outputStream.close();

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieUnMergedLogRecordScanner.java
Patch:
@@ -83,7 +83,7 @@ protected <T> void processNextRecord(HoodieRecord<T> hoodieRecord) throws Except
 
   @Override
   protected void processNextDeletedRecord(DeleteRecord deleteRecord) {
-    throw new IllegalStateException("Not expected to see delete records in this log-scan mode. Check Job Config");
+    // no - op
   }
 
   /**

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieMetadataTableValidator.java
Patch:
@@ -217,7 +217,7 @@ public void testSecondaryIndexValidation() throws IOException {
 
     Dataset<Row> rows = getRowDataset(1, "row1", "abc", "p1");
     rows.write().format("hudi").mode(SaveMode.Append).save(basePath);
-    rows = getRowDataset(2, "row2", "abc", "p2");
+    rows = getRowDataset(2, "row2", "ghi", "p2");
     rows.write().format("hudi").mode(SaveMode.Append).save(basePath);
     rows = getRowDataset(3, "row3", "def", "p2");
     rows.write().format("hudi").mode(SaveMode.Append).save(basePath);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -499,6 +499,7 @@ protected void doWrite(HoodieRecord record, Schema schema, TypedProperties props
       // for a single record
       writeStatus.markFailure(record, t, recordMetadata);
       LOG.error("Error writing record " + record, t);
+      ignoreWriteFailed(t);
     }
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -170,6 +170,7 @@ protected void doWrite(HoodieRecord record, Schema schema, TypedProperties props
       // for a single record
       writeStatus.markFailure(record, t, recordMetadata);
       LOG.error("Error writing record " + record, t);
+      ignoreWriteFailed(t);
     }
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -304,6 +304,7 @@ private boolean writeRecord(HoodieRecord<T> newRecord, Option<HoodieRecord> comb
       HoodieUpsertException failureEx = new HoodieUpsertException("mismatched partition path, record partition: "
           + newRecord.getPartitionPath() + " but trying to insert into partition: " + partitionPath);
       writeStatus.markFailure(newRecord, failureEx, recordMetadata);
+      ignoreWriteFailed(failureEx);
       return false;
     }
     try {
@@ -333,6 +334,7 @@ private boolean writeRecord(HoodieRecord<T> newRecord, Option<HoodieRecord> comb
     } catch (Exception e) {
       LOG.error("Error writing record  " + newRecord, e);
       writeStatus.markFailure(newRecord, e, recordMetadata);
+      ignoreWriteFailed(e);
     }
     return false;
   }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -439,6 +439,7 @@ private FlinkOptions() {
       .key("write.ignore.failed")
       .booleanType()
       .defaultValue(false)
+      .withFallbackKeys("hoodie.write.ignore.failed")
       .withDescription("Flag to indicate whether to ignore any non exception error (e.g. writestatus error). within a checkpoint batch. \n"
           + "By default false. Turning this on, could hide the write status errors while the flink checkpoint moves ahead. \n"
           + "So, would recommend users to use this with caution.");

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/FlinkWriteClients.java
Patch:
@@ -225,7 +225,8 @@ public static HoodieWriteConfig getHoodieClientConfig(
             .withAutoCommit(false)
             .withAllowOperationMetadataField(conf.getBoolean(FlinkOptions.CHANGELOG_ENABLED))
             .withProps(flinkConf2TypedProperties(conf))
-            .withSchema(getSourceSchema(conf).toString());
+            .withSchema(getSourceSchema(conf).toString())
+            .withWriteIgnoreFailed(conf.get(FlinkOptions.IGNORE_FAILED));
 
     Option<HoodieLockConfig> lockConfig = getLockConfig(conf);
     if (lockConfig.isPresent()) {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.EngineType;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
 import org.apache.hudi.common.model.HoodieIndexDefinition;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -189,8 +188,9 @@ protected void preWrite(String instantTime) {
   }
 
   @Override
-  protected HoodieData<HoodieRecord> getFunctionalIndexRecords(List<Pair<String, FileSlice>> partitionFileSlicePairs, HoodieIndexDefinition indexDefinition, HoodieTableMetaClient metaClient,
-                                                               int parallelism, Schema readerSchema, StorageConfiguration<?> storageConf) {
+  protected HoodieData<HoodieRecord> getFunctionalIndexRecords(List<Pair<String, Pair<String, Long>>> partitionFilePathAndSizeTriplet, HoodieIndexDefinition indexDefinition,
+                                                               HoodieTableMetaClient metaClient, int parallelism, Schema readerSchema, StorageConfiguration<?> storageConf,
+                                                               String instantTime) {
     throw new HoodieNotSupportedException("Flink metadata table does not support functional index yet.");
   }
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/metadata/JavaHoodieBackedTableMetadataWriter.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.EngineType;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
 import org.apache.hudi.common.model.HoodieIndexDefinition;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -124,8 +123,9 @@ protected void preWrite(String instantTime) {
   }
 
   @Override
-  protected HoodieData<HoodieRecord> getFunctionalIndexRecords(List<Pair<String, FileSlice>> partitionFileSlicePairs, HoodieIndexDefinition indexDefinition, HoodieTableMetaClient metaClient,
-                                                               int parallelism, Schema readerSchema, StorageConfiguration<?> storageConf) {
+  protected HoodieData<HoodieRecord> getFunctionalIndexRecords(List<Pair<String, Pair<String, Long>>> partitionFilePathAndSizeTriplet, HoodieIndexDefinition indexDefinition,
+                                                               HoodieTableMetaClient metaClient, int parallelism, Schema readerSchema, StorageConfiguration<?> storageConf,
+                                                               String instantTime) {
     throw new HoodieNotSupportedException("Functional index not supported for Java metadata table writer yet.");
   }
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -896,7 +896,7 @@ public static String getPartitionIdentifierForFilesPartition(String relativePart
   /**
    * Returns partition name for the given path.
    */
-  private static String getPartitionIdentifier(@Nonnull String relativePartitionPath) {
+  public static String getPartitionIdentifier(@Nonnull String relativePartitionPath) {
     return EMPTY_PARTITION_NAME.equals(relativePartitionPath) ? NON_PARTITIONED_NAME : relativePartitionPath;
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java
Patch:
@@ -980,7 +980,7 @@ private void validatePartitionStats(HoodieMetadataValidationContext metadataTabl
         AvroConversionUtils.convertAvroSchemaToStructType(metadataTableBasedContext.getSchema()), metadataTableBasedContext.getMetadataConfig(),
         metaClient, false);
     HoodieData<HoodieMetadataColumnStats> partitionStats =
-        partitionStatsIndexSupport.loadColumnStatsIndexRecords(JavaConverters.asScalaBufferConverter(metadataTableBasedContext.allColumnNameList).asScala().toSeq(), false);
+        partitionStatsIndexSupport.loadColumnStatsIndexRecords(JavaConverters.asScalaBufferConverter(metadataTableBasedContext.allColumnNameList).asScala().toSeq(), scala.Option.empty(), false);
     JavaRDD<HoodieMetadataColumnStats> diffRDD = HoodieJavaRDD.getJavaRDD(partitionStats).subtract(HoodieJavaRDD.getJavaRDD(partitionStatsUsingColStats));
     if (!diffRDD.isEmpty()) {
       List<HoodieMetadataColumnStats> diff = diffRDD.collect();

File: hudi-common/src/main/java/org/apache/hudi/metadata/MetadataPartitionType.java
Patch:
@@ -298,8 +298,8 @@ private static void constructColumnStatsMetadataPayload(HoodieMetadataPayload pa
           String.format("Valid %s record expected for type: %s", SCHEMA_FIELD_ID_COLUMN_STATS, MetadataPartitionType.COLUMN_STATS.getRecordType()));
     } else {
       payload.columnStatMetadata = HoodieMetadataColumnStats.newBuilder(METADATA_COLUMN_STATS_BUILDER_STUB.get())
-          .setFileName((String) columnStatsRecord.get(COLUMN_STATS_FIELD_FILE_NAME))
-          .setColumnName((String) columnStatsRecord.get(COLUMN_STATS_FIELD_COLUMN_NAME))
+          .setFileName(columnStatsRecord.get(COLUMN_STATS_FIELD_FILE_NAME).toString())
+          .setColumnName(columnStatsRecord.get(COLUMN_STATS_FIELD_COLUMN_NAME).toString())
           // AVRO-2377 1.9.2 Modified the type of org.apache.avro.Schema#FIELD_RESERVED to Collections.unmodifiableSet.
           // This causes Kryo to fail when deserializing a GenericRecord, See HUDI-5484.
           // We should avoid using GenericRecord and convert GenericRecord into a serializable type.

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java
Patch:
@@ -115,7 +115,7 @@ private static boolean checkIfHudiTable(final InputSplit split, final JobConf jo
   public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf job,
                                                                    final Reporter reporter) throws IOException {
     HoodieRealtimeInputFormatUtils.addProjectionField(job, job.get(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, "").split("/"));
-    if (shouldUseFilegroupReader(job)) {
+    if (shouldUseFilegroupReader(job, split)) {
       try {
         if (!(split instanceof FileSplit) || !checkIfHudiTable(split, job)) {
           return super.getRecordReader(split, job, reporter);

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieCombineRealtimeRecordReader.java
Patch:
@@ -54,7 +54,7 @@ public class HoodieCombineRealtimeRecordReader implements RecordReader<NullWrita
 
   public HoodieCombineRealtimeRecordReader(JobConf jobConf, CombineFileSplit split,
       List<RecordReader> readers) {
-    useFileGroupReader = shouldUseFilegroupReader(jobConf);
+    useFileGroupReader = shouldUseFilegroupReader(jobConf, split);
     try {
       ValidationUtils.checkArgument(((HoodieCombineRealtimeFileSplit) split).getRealtimeFileSplits().size() == readers
           .size(), "Num Splits does not match number of unique RecordReaders!");

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java
Patch:
@@ -73,7 +73,7 @@ public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSpli
         "HoodieRealtimeRecordReader can only work on RealtimeSplit and not with " + split);
     RealtimeSplit realtimeSplit = (RealtimeSplit) split;
 
-    if (shouldUseFilegroupReader(jobConf)) {
+    if (shouldUseFilegroupReader(jobConf, split)) {
       return super.getRecordReader(realtimeSplit, jobConf, reporter);
     }
 

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java
Patch:
@@ -126,8 +126,7 @@ public void testParquetDemo() throws Exception {
 
     // batch 2
     ingestSecondBatchAndHiveSync();
-    // TODO(HUDI-8275): fix MOR queries on Hive in integration tests
-    // testHiveAfterSecondBatch();
+    testHiveAfterSecondBatch();
     // testPrestoAfterSecondBatch();
     // testTrinoAfterSecondBatch();
     testSparkSQLAfterSecondBatch();
@@ -138,7 +137,7 @@ public void testParquetDemo() throws Exception {
     scheduleAndRunCompaction();
 
     testIncrementalSparkSQLQuery();
-    // testHiveAfterSecondBatchAfterCompaction();
+    testHiveAfterSecondBatchAfterCompaction();
     // testPrestoAfterSecondBatchAfterCompaction();
     // testTrinoAfterSecondBatchAfterCompaction();
     testIncrementalHiveQueryAfterCompaction();

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java
Patch:
@@ -131,6 +131,7 @@ public class HoodieMetadataPayload implements HoodieRecordPayload<HoodieMetadata
   public static final String COLUMN_STATS_FIELD_COLUMN_NAME = "columnName";
   public static final String COLUMN_STATS_FIELD_TOTAL_UNCOMPRESSED_SIZE = "totalUncompressedSize";
   public static final String COLUMN_STATS_FIELD_IS_DELETED = FIELD_IS_DELETED;
+  public static final String COLUMN_STATS_FIELD_IS_TIGHT_BOUND = "isTightBound";
 
   /**
    * HoodieMetadata record index payload field ids

File: hudi-common/src/main/java/org/apache/hudi/metadata/MetadataPartitionType.java
Patch:
@@ -56,6 +56,7 @@
 import static org.apache.hudi.metadata.HoodieMetadataPayload.COLUMN_STATS_FIELD_COLUMN_NAME;
 import static org.apache.hudi.metadata.HoodieMetadataPayload.COLUMN_STATS_FIELD_FILE_NAME;
 import static org.apache.hudi.metadata.HoodieMetadataPayload.COLUMN_STATS_FIELD_IS_DELETED;
+import static org.apache.hudi.metadata.HoodieMetadataPayload.COLUMN_STATS_FIELD_IS_TIGHT_BOUND;
 import static org.apache.hudi.metadata.HoodieMetadataPayload.COLUMN_STATS_FIELD_MAX_VALUE;
 import static org.apache.hudi.metadata.HoodieMetadataPayload.COLUMN_STATS_FIELD_MIN_VALUE;
 import static org.apache.hudi.metadata.HoodieMetadataPayload.COLUMN_STATS_FIELD_NULL_COUNT;
@@ -309,6 +310,7 @@ private static void constructColumnStatsMetadataPayload(HoodieMetadataPayload pa
           .setTotalSize((Long) columnStatsRecord.get(COLUMN_STATS_FIELD_TOTAL_SIZE))
           .setTotalUncompressedSize((Long) columnStatsRecord.get(COLUMN_STATS_FIELD_TOTAL_UNCOMPRESSED_SIZE))
           .setIsDeleted((Boolean) columnStatsRecord.get(COLUMN_STATS_FIELD_IS_DELETED))
+          .setIsTightBound((Boolean) columnStatsRecord.get(COLUMN_STATS_FIELD_IS_TIGHT_BOUND))
           .build();
     }
   }

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -214,7 +214,7 @@ public HoodieData<HoodieRecord<HoodieMetadataPayload>> getRecordsByKeyPrefixes(L
     checkState(!partitionFileSlices.isEmpty(), "Number of file slices for partition " + partitionName + " should be > 0");
 
     return (shouldLoadInMemory ? HoodieListData.lazy(partitionFileSlices) :
-        engineContext.parallelize(partitionFileSlices))
+        getEngineContext().parallelize(partitionFileSlices))
         .flatMap(
             (SerializableFunction<FileSlice, Iterator<HoodieRecord<HoodieMetadataPayload>>>) fileSlice -> {
               // NOTE: Since this will be executed by executors, we can't access previously cached

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java
Patch:
@@ -519,7 +519,7 @@ private static HoodieAvroRecord<HoodieMetadataPayload> createColumnStatsRecord(S
 
   public static Stream<HoodieRecord> createPartitionStatsRecords(String partitionPath,
                                                                  Collection<HoodieColumnRangeMetadata<Comparable>> columnRangeMetadataList,
-                                                                 boolean isDeleted) {
+                                                                 boolean isDeleted, boolean isTightBound) {
     return columnRangeMetadataList.stream().map(columnRangeMetadata -> {
       HoodieKey key = new HoodieKey(getPartitionStatsIndexKey(partitionPath, columnRangeMetadata.getColumnName()),
           MetadataPartitionType.PARTITION_STATS.getPartitionPath());
@@ -535,6 +535,7 @@ public static Stream<HoodieRecord> createPartitionStatsRecords(String partitionP
               .setTotalSize(columnRangeMetadata.getTotalSize())
               .setTotalUncompressedSize(columnRangeMetadata.getTotalUncompressedSize())
               .setIsDeleted(isDeleted)
+              .setIsTightBound(isTightBound)
               .build(),
           MetadataPartitionType.PARTITION_STATS.getRecordType());
 

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/utils/TestAvroSchemaConverter.java
Patch:
@@ -48,7 +48,8 @@ void testUnionSchemaWithMultipleRecordTypes() {
         + "`nullCount` BIGINT, "
         + "`totalSize` BIGINT, "
         + "`totalUncompressedSize` BIGINT, "
-        + "`isDeleted` BOOLEAN NOT NULL>";
+        + "`isDeleted` BOOLEAN NOT NULL, "
+        + "`isTightBound` BOOLEAN NOT NULL>";
     assertThat(dataType.getChildren().get(pos).toString(), is(expected));
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java
Patch:
@@ -1009,7 +1009,8 @@ private HoodieData<HoodieMetadataColumnStats> getPartitionStatsUsingColStats(Hoo
           .getSortedColumnStatsList(partitionPath, latestFileNames);
 
       TreeSet<HoodieColumnRangeMetadata<Comparable>> aggregatedColumnStats = aggregateColumnStats(partitionPath, colStats);
-      List<HoodieRecord> partitionStatRecords = HoodieMetadataPayload.createPartitionStatsRecords(partitionPath, new ArrayList<>(aggregatedColumnStats), false)
+      // TODO: fix `isTightBound` flag when stats based on log files are available
+      List<HoodieRecord> partitionStatRecords = HoodieMetadataPayload.createPartitionStatsRecords(partitionPath, new ArrayList<>(aggregatedColumnStats), false, false)
           .collect(Collectors.toList());
       return partitionStatRecords.stream()
           .map(record -> {
@@ -1719,7 +1720,7 @@ public List<HoodieColumnRangeMetadata<Comparable>> getSortedColumnStatsList(
         return allColumnNameList.stream()
             .flatMap(columnName ->
                 tableMetadata.getColumnStats(partitionFileNameList, columnName).values().stream()
-                    .map(HoodieTableMetadataUtil::convertColumnStatsRecordToColumnRangeMetadata)
+                    .map(HoodieColumnRangeMetadata::fromColumnStats)
                     .collect(Collectors.toList())
                     .stream())
             .sorted(new HoodieColumnRangeMetadataComparator())

File: hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/HoodieFlinkQuickstart.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.examples.quickstart.utils.QuickstartConfigurations;
 
 import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.JobManagerOptions;
 import org.apache.flink.core.execution.JobClient;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.api.EnvironmentSettings;
@@ -41,6 +42,7 @@
 import org.apache.flink.types.Row;
 import org.jetbrains.annotations.NotNull;
 
+import java.time.Duration;
 import java.util.Collection;
 import java.util.List;
 import java.util.concurrent.ExecutionException;
@@ -177,7 +179,7 @@ public static void execInsertSql(TableEnvironment tEnv, String insert) {
     TableResult tableResult = tEnv.executeSql(insert);
     // wait to finish
     try {
-      tableResult.getJobClient().get().getJobExecutionResult().get();
+      tableResult.await();
     } catch (InterruptedException | ExecutionException ex) {
       // ignored
     }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieSparkClientTestHarness.java
Patch:
@@ -133,7 +133,6 @@ public static void tearDownAll() throws IOException {
   protected SparkSession sparkSession;
   protected SQLContext sqlContext;
   protected ExecutorService executorService;
-  protected HoodieTableMetaClient metaClient;
   protected SparkRDDWriteClient writeClient;
   protected SparkRDDReadClient readClient;
   protected HoodieTableFileSystemView tableView;

File: hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/io/storage/row/parquet/TestParquetSchemaConverter.java
Patch:
@@ -50,7 +50,7 @@ void testConvertComplexTypes() {
     assertThat(messageType.getColumns().size(), is(7));
     final String expected = "message converted {\n"
         + "  optional group f_array (LIST) {\n"
-        + "    repeated group list {\n"
+        + "    repeated group array {\n"
         + "      optional binary element (STRING);\n"
         + "    }\n"
         + "  }\n"

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java
Patch:
@@ -241,7 +241,7 @@ private static <R> HoodieData<HoodieRecord<R>> getExistingRecords(
       HoodieData<Pair<String, String>> partitionLocations, HoodieWriteConfig config, HoodieTable hoodieTable) {
     final Option<String> instantTime = hoodieTable
         .getMetaClient()
-        .getCommitsTimeline()
+        .getActiveTimeline() // we need to include all actions and completed
         .filterCompletedInstants()
         .lastInstant()
         .map(HoodieInstant::getTimestamp);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestSparkSampleWritesUtils.java
Patch:
@@ -35,7 +35,6 @@
 import org.apache.spark.api.java.JavaRDD;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
-import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 
 import java.io.IOException;
@@ -86,7 +85,6 @@ public void skipOverwriteRecordSizeEstimateWhenTimelineNonEmpty() throws Excepti
     assertEquals(originalRecordSize, originalWriteConfig.getCopyOnWriteRecordSizeEstimate(), "Original record size estimate should not be changed.");
   }
 
-  @Disabled("HUDI-8082")
   @Test
   public void overwriteRecordSizeEstimateForEmptyTable() {
     int originalRecordSize = 100;

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -1170,7 +1170,7 @@ public TableBuilder fromProperties(Properties properties) {
             hoodieConfig.getString(HoodieTableConfig.BOOTSTRAP_INDEX_CLASS_NAME));
       }
       if (hoodieConfig.contains(HoodieTableConfig.BOOTSTRAP_INDEX_TYPE)) {
-        setPayloadClassName(BootstrapIndexType.valueOf(hoodieConfig.getString(HoodieTableConfig.BOOTSTRAP_INDEX_TYPE)).getClassName());
+        setBootstrapIndexClass(BootstrapIndexType.valueOf(hoodieConfig.getString(HoodieTableConfig.BOOTSTRAP_INDEX_TYPE)).getClassName());
       }
       if (hoodieConfig.contains(HoodieTableConfig.BOOTSTRAP_BASE_PATH)) {
         setBootstrapBasePath(hoodieConfig.getString(HoodieTableConfig.BOOTSTRAP_BASE_PATH));

File: hudi-aws/src/test/java/org/apache/hudi/aws/sync/ITTestGluePartitionPushdown.java
Patch:
@@ -90,7 +90,7 @@ public void setUp() throws Exception {
     fileSystem = hiveSyncConfig.getHadoopFileSystem();
     fileSystem.mkdirs(new Path(tablePath));
     StorageConfiguration<?> configuration = HadoopFSUtils.getStorageConf(new Configuration());
-    HoodieTableMetaClient metaClient = HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient metaClient = HoodieTableMetaClient.newTableBuilder()
         .setTableType(HoodieTableType.COPY_ON_WRITE)
         .setTableName(TABLE_NAME)
         .setPayloadClass(HoodieAvroPayload.class)

File: hudi-aws/src/test/java/org/apache/hudi/aws/testutils/GlueTestUtil.java
Patch:
@@ -104,7 +104,7 @@ public static void createHoodieTable() throws IOException {
     if (fileSystem.exists(path)) {
       fileSystem.delete(path, true);
     }
-    metaClient = HoodieTableMetaClient.withPropertyBuilder()
+    metaClient = HoodieTableMetaClient.newTableBuilder()
         .setTableType(HoodieTableType.COPY_ON_WRITE)
         .setTableName(TABLE_NAME)
         .setPayloadClass(HoodieAvroPayload.class)

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -267,10 +267,10 @@ protected static void clean(JavaSparkContext jsc, String basePath, String propsF
 
   protected static int deleteMarker(JavaSparkContext jsc, String instantTime, String basePath) {
     try (SparkRDDWriteClient client = createHoodieClient(jsc, basePath, false)) {
-
       HoodieWriteConfig config = client.getConfig();
       HoodieEngineContext context = client.getEngineContext();
       HoodieSparkTable table = HoodieSparkTable.create(config, context);
+      client.validateAgainstTableProperties(table.getMetaClient().getTableConfig(), config);
       WriteMarkersFactory.get(config.getMarkersType(), table, instantTime)
           .quietDeleteMarkerDir(context, config.getMarkersDeleteParallelism());
       return 0;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/UpgradeOrDowngradeCommand.java
Patch:
@@ -89,7 +89,7 @@ static String getHoodieTableVersionName(String versionOption, boolean overrideWi
 
     try {
       int versionCode = Integer.parseInt(versionOption);
-      return HoodieTableVersion.versionFromCode(versionCode).name();
+      return HoodieTableVersion.fromVersionCode(versionCode).name();
     } catch (NumberFormatException e) {
       // The version option from the CLI is not a number, returns the original String
       return versionOption;

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestMetadataCommand.java
Patch:
@@ -68,7 +68,7 @@ public void init() throws IOException {
 
   @Test
   public void testMetadataDelete() throws Exception {
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .setTableType(HoodieTableType.COPY_ON_WRITE.name())
         .setTableName(tableName())
         .setArchiveLogFolder(HoodieTableConfig.ARCHIVELOG_FOLDER.defaultValue())

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java
Patch:
@@ -318,7 +318,7 @@ public void testShowFailedCommits() {
   @Test
   public void testRepairDeprecatedPartition() throws IOException {
     tablePath = tablePath + "/repair_test/";
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .setTableType(HoodieTableType.COPY_ON_WRITE.name())
         .setTableName(tableName())
         .setArchiveLogFolder(HoodieTableConfig.ARCHIVELOG_FOLDER.defaultValue())
@@ -386,7 +386,7 @@ public void testRepairDeprecatedPartition() throws IOException {
   @Test
   public void testRenamePartition() throws IOException {
     tablePath = tablePath + "/rename_partition_test/";
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .setTableType(HoodieTableType.COPY_ON_WRITE.name())
         .setTableName(tableName())
         .setArchiveLogFolder(HoodieTableConfig.ARCHIVELOG_FOLDER.defaultValue())

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestClusteringCommand.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hudi.common.model.HoodieAvroPayload;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
+import org.apache.hudi.common.table.HoodieTableVersion;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
@@ -77,7 +78,8 @@ public void init() throws IOException {
     // Create table and connect
     new TableCommand().createTable(
         basePath, tableName, HoodieTableType.COPY_ON_WRITE.name(),
-        "", TimelineLayoutVersion.VERSION_1, "org.apache.hudi.common.model.HoodieAvroPayload");
+        "", TimelineLayoutVersion.VERSION_1, HoodieTableVersion.current().versionCode(),
+        "org.apache.hudi.common.model.HoodieAvroPayload");
 
     initMetaClient();
   }

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestCommitsCommand.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.cli.testutils.ShellEvaluationResultUtil;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
+import org.apache.hudi.common.table.HoodieTableVersion;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.testutils.HoodieTestTable;
@@ -76,7 +77,8 @@ public void init() throws IOException {
     // Create table and connect
     new TableCommand().createTable(
         basePath, tableName, HoodieTableType.COPY_ON_WRITE.name(),
-        "", TimelineLayoutVersion.VERSION_1, "org.apache.hudi.common.model.HoodieAvroPayload");
+        "", TimelineLayoutVersion.VERSION_1, HoodieTableVersion.current().versionCode(),
+        "org.apache.hudi.common.model.HoodieAvroPayload");
 
     initMetaClient();
   }

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestCompactionCommand.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
+import org.apache.hudi.common.table.HoodieTableVersion;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
@@ -83,7 +84,8 @@ public void init() throws IOException {
     // Create table and connect
     new TableCommand().createTable(
         basePath, tableName, HoodieTableType.MERGE_ON_READ.name(),
-        "", TimelineLayoutVersion.VERSION_1, "org.apache.hudi.common.model.HoodieAvroPayload");
+        "", TimelineLayoutVersion.VERSION_1, HoodieTableVersion.current().versionCode(),
+        "org.apache.hudi.common.model.HoodieAvroPayload");
 
     initMetaClient();
   }

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestMarkersCommand.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.cli.testutils.ShellEvaluationResultUtil;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.IOType;
+import org.apache.hudi.common.table.HoodieTableVersion;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.testutils.FileCreateUtils;
 import org.apache.hudi.storage.StoragePath;
@@ -59,7 +60,8 @@ public void init() throws IOException {
     // Create table and connect
     new TableCommand().createTable(
         tablePath, "test_table", HoodieTableType.COPY_ON_WRITE.name(),
-        "", TimelineLayoutVersion.VERSION_1, "org.apache.hudi.common.model.HoodieAvroPayload");
+        "", TimelineLayoutVersion.VERSION_1, HoodieTableVersion.current().versionCode(),
+        "org.apache.hudi.common.model.HoodieAvroPayload");
   }
 
   /**

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestSavepointsCommand.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieTableType;
+import org.apache.hudi.common.table.HoodieTableVersion;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -71,7 +72,8 @@ public void init() throws IOException {
     // Create table and connect
     new TableCommand().createTable(
         tablePath, "test_table", HoodieTableType.COPY_ON_WRITE.name(),
-        "", TimelineLayoutVersion.VERSION_1, "org.apache.hudi.common.model.HoodieAvroPayload");
+        "", TimelineLayoutVersion.VERSION_1, HoodieTableVersion.current().versionCode(),
+        "org.apache.hudi.common.model.HoodieAvroPayload");
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/exception/HoodieUpgradeDowngradeException.java
Patch:
@@ -20,8 +20,8 @@
 
 public class HoodieUpgradeDowngradeException extends HoodieException {
 
-  public HoodieUpgradeDowngradeException(String msg, Throwable t) {
-    super(msg, t);
+  public HoodieUpgradeDowngradeException(String msg) {
+    super(msg);
   }
 
   public HoodieUpgradeDowngradeException(int fromVersion, int toVersion, boolean upgrade) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataWriteUtils.java
Patch:
@@ -106,6 +106,7 @@ public static HoodieWriteConfig createMetadataWriteConfig(
     HoodieWriteConfig.Builder builder = HoodieWriteConfig.newBuilder()
         .withEngineType(writeConfig.getEngineType())
         .withTimelineLayoutVersion(TimelineLayoutVersion.CURR_VERSION)
+        .withWriteTableVersion(writeConfig.getWriteVersion().versionCode())
         .withMergeAllowDuplicateOnInserts(false)
         .withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder()
             .withConsistencyCheckEnabled(writeConfig.getConsistencyGuardConfig().isConsistencyCheckEnabled())

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -820,7 +820,7 @@ private void waitForAllFiles(HoodieEngineContext context, Map<String, List<Pair<
   }
 
   private boolean waitForCondition(String partitionPath, Stream<Pair<String, String>> partitionFilePaths, FileVisibility visibility) {
-    final HoodieStorage storage = metaClient.getRawHoodieStorage();
+    final HoodieStorage storage = metaClient.getRawStorage();
     List<String> fileList = partitionFilePaths.map(Pair::getValue).collect(Collectors.toList());
     try {
       getConsistencyGuard(storage, config.getConsistencyGuardConfig())

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/common/testutils/HoodieMetadataTestTable.java
Patch:
@@ -65,7 +65,7 @@ public static HoodieTestTable of(HoodieTableMetaClient metaClient,
                                    HoodieTableMetadataWriter writer,
                                    Option<HoodieEngineContext> context) {
     testTableState = HoodieTestTableState.of();
-    return new HoodieMetadataTestTable(metaClient.getBasePath().toString(), metaClient.getRawHoodieStorage(),
+    return new HoodieMetadataTestTable(metaClient.getBasePath().toString(), metaClient.getRawStorage(),
         metaClient,
         writer, context);
   }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -114,12 +114,12 @@ public boolean commit(String instantTime, List<WriteStatus> writeStatuses, Optio
 
   @Override
   protected HoodieTable createTable(HoodieWriteConfig config) {
-    return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);
+    return createTableAndValidate(config, HoodieFlinkTable::create);
   }
 
   @Override
   protected HoodieTable createTable(HoodieWriteConfig config, HoodieTableMetaClient metaClient) {
-    return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context, metaClient);
+    return createTableAndValidate(config, metaClient, HoodieFlinkTable::create);
   }
 
   @Override

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -195,7 +195,7 @@ protected HoodieData<HoodieRecord> getFunctionalIndexRecords(List<Pair<String, F
   }
 
   @Override
-  protected HoodieTable getHoodieTable(HoodieWriteConfig writeConfig, HoodieTableMetaClient metaClient) {
+  protected HoodieTable getTable(HoodieWriteConfig writeConfig, HoodieTableMetaClient metaClient) {
     return HoodieFlinkTable.create(writeConfig, engineContext, metaClient);
   }
 

File: hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkWriteableTestTable.java
Patch:
@@ -66,7 +66,7 @@ private HoodieFlinkWriteableTestTable(String basePath, HoodieStorage storage,
   public static HoodieFlinkWriteableTestTable of(HoodieTableMetaClient metaClient, Schema schema,
                                                  BloomFilter filter) {
     return new HoodieFlinkWriteableTestTable(metaClient.getBasePath().toString(),
-        metaClient.getRawHoodieStorage(), metaClient, schema, filter);
+        metaClient.getRawStorage(), metaClient, schema, filter);
   }
 
   public static HoodieFlinkWriteableTestTable of(HoodieTableMetaClient metaClient, Schema schema) {

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaTableServiceClient.java
Patch:
@@ -70,7 +70,7 @@ protected HoodieData<WriteStatus> convertToWriteStatus(HoodieWriteMetadata<List<
   }
 
   @Override
-  protected HoodieTable<?, List<HoodieRecord<T>>, ?, List<WriteStatus>> createTable(HoodieWriteConfig config, StorageConfiguration<?> storageConf) {
-    return HoodieJavaTable.create(config, context);
+  protected HoodieTable<?, List<HoodieRecord<T>>, ?, List<WriteStatus>> createTable(HoodieWriteConfig config, StorageConfiguration<?> storageConf, boolean skipValidation) {
+    return createTableAndValidate(config, HoodieJavaTable::create, skipValidation);
   }
 }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java
Patch:
@@ -94,12 +94,12 @@ public boolean commit(String instantTime,
 
   @Override
   protected HoodieTable createTable(HoodieWriteConfig config) {
-    return HoodieJavaTable.create(config, context);
+    return createTableAndValidate(config, HoodieJavaTable::create);
   }
 
   @Override
   protected HoodieTable createTable(HoodieWriteConfig config, HoodieTableMetaClient metaClient) {
-    return HoodieJavaTable.create(config, context, metaClient);
+    return createTableAndValidate(config, metaClient, HoodieJavaTable::create);
   }
 
   @Override

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/client/common/TestMultipleHoodieJavaWriteClient.java
Patch:
@@ -78,7 +78,7 @@ void testOccWithMultipleWriters(HoodieTableType tableType) throws IOException {
     final String tableName = "hudiTestTable";
     final String basePath = tablePath.toAbsolutePath().toString() + "/" + tableTypeName;
 
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .setTableType(tableTypeName)
         .setTableName(tableName)
         .setPayloadClassName(HoodieAvroPayload.class.getName())

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/hadoop/TestHoodieFileGroupReaderOnHive.java
Patch:
@@ -193,7 +193,7 @@ public void commitToTable(List<HoodieRecord> recordList, String operation, Map<S
             recordMergerStrategy = HoodieRecordMerger.DEFAULT_MERGER_STRATEGY_UUID;
           }
           Map<String, Object> initConfigs = new HashMap<>(writeConfigs);
-          HoodieTableMetaClient.withPropertyBuilder()
+          HoodieTableMetaClient.newTableBuilder()
               .setTableType(writeConfigs.getOrDefault("hoodie.datasource.write.table.type", "MERGE_ON_READ"))
               .setTableName(writeConfigs.get("hoodie.table.name"))
               .setPartitionFields(writeConfigs.getOrDefault("hoodie.datasource.write.partitionpath.field", ""))

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDTableServiceClient.java
Patch:
@@ -70,8 +70,8 @@ protected HoodieData<WriteStatus> convertToWriteStatus(HoodieWriteMetadata<Hoodi
   }
 
   @Override
-  protected HoodieTable<?, HoodieData<HoodieRecord<T>>, ?, HoodieData<WriteStatus>> createTable(HoodieWriteConfig config, StorageConfiguration<?> storageConf) {
-    return HoodieSparkTable.create(config, context);
+  protected HoodieTable<?, HoodieData<HoodieRecord<T>>, ?, HoodieData<WriteStatus>> createTable(HoodieWriteConfig config, StorageConfiguration<?> storageConf, boolean skipValidation) {
+    return createTableAndValidate(config, HoodieSparkTable::create, skipValidation);
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -216,13 +216,13 @@ protected HoodieData<HoodieRecord> getFunctionalIndexRecords(List<Pair<String, F
   }
 
   @Override
-  protected HoodieTable getHoodieTable(HoodieWriteConfig writeConfig, HoodieTableMetaClient metaClient) {
+  protected HoodieTable getTable(HoodieWriteConfig writeConfig, HoodieTableMetaClient metaClient) {
     return HoodieSparkTable.create(writeConfig, engineContext, metaClient);
   }
 
   @Override
   public BaseHoodieWriteClient<?, JavaRDD<HoodieRecord>, ?, ?> initializeWriteClient() {
-    return new SparkRDDWriteClient(engineContext, metadataWriteConfig, true);
+    return new SparkRDDWriteClient(engineContext, metadataWriteConfig, Option.empty());
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestMultiFS.java
Patch:
@@ -103,7 +103,7 @@ protected HoodieWriteConfig getHoodieWriteConfig(String basePath) {
   @Test
   public void readLocalWriteHDFS() throws Exception {
     // Initialize table and filesystem
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .setTableType(TABLE_TYPE)
         .setTableName(TABLE_NAME)
         .setPayloadClass(HoodieAvroPayload.class)
@@ -113,7 +113,7 @@ public void readLocalWriteHDFS() throws Exception {
     HoodieWriteConfig cfg = getHoodieWriteConfig(dfsBasePath);
     HoodieWriteConfig localConfig = getHoodieWriteConfig(tablePath);
 
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .setTableType(TABLE_TYPE)
         .setTableName(TABLE_NAME)
         .setPayloadClass(HoodieAvroPayload.class)
@@ -140,7 +140,7 @@ public void readLocalWriteHDFS() throws Exception {
       assertEquals(readRecords.count(), records.size());
 
       // Write to local
-      HoodieTableMetaClient.withPropertyBuilder()
+      HoodieTableMetaClient.newTableBuilder()
           .setTableType(TABLE_TYPE)
           .setTableName(TABLE_NAME)
           .setPayloadClass(HoodieAvroPayload.class)

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java
Patch:
@@ -165,7 +165,7 @@ public void testMORTable(boolean shouldAllowDroppedColumns) throws Exception {
     tableType = HoodieTableType.MERGE_ON_READ;
 
     // Create the table
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .fromMetaClient(metaClient)
         .setTableType(HoodieTableType.MERGE_ON_READ)
         .setRecordMergeMode(RecordMergeMode.valueOf(RECORD_MERGE_MODE.defaultValue()))
@@ -255,7 +255,7 @@ public void testMORTable(boolean shouldAllowDroppedColumns) throws Exception {
   @ValueSource(booleans = {false, true})
   public void testCopyOnWriteTable(boolean shouldAllowDroppedColumns) throws Exception {
     // Create the table
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .fromMetaClient(metaClient)
         .setTimelineLayoutVersion(VERSION_1)
         .initTable(metaClient.getStorageConf().newInstance(), metaClient.getBasePath());

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -513,7 +513,7 @@ public void testRestoreWithSavepointBeyondArchival() throws Exception {
         .withProps(config.getProps()).withTimelineLayoutVersion(
             VERSION_0).build();
 
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .fromMetaClient(metaClient)
         .setTimelineLayoutVersion(VERSION_0)
         .setPopulateMetaFields(config.populateMetaFields())

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/FunctionalTestHarness.java
Patch:
@@ -123,13 +123,12 @@ public HoodieTableMetaClient getHoodieMetaClient(StorageConfiguration<?> storage
 
   @Override
   public HoodieTableMetaClient getHoodieMetaClient(StorageConfiguration<?> storageConf, String basePath, Properties props) throws IOException {
-    props = HoodieTableMetaClient.withPropertyBuilder()
+    return HoodieTableMetaClient.newTableBuilder()
       .setTableName(RAW_TRIPS_TEST_NAME)
       .setTableType(COPY_ON_WRITE)
       .setPayloadClass(HoodieAvroPayload.class)
       .fromProperties(props)
-      .build();
-    return HoodieTableMetaClient.initTableAndGetMetaClient(storageConf.newInstance(), basePath, props);
+      .initTable(storageConf.newInstance(), basePath);
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieSparkWriteableTestTable.java
Patch:
@@ -67,7 +67,7 @@ public static HoodieSparkWriteableTestTable of(HoodieTableMetaClient metaClient,
 
   public static HoodieSparkWriteableTestTable of(HoodieTableMetaClient metaClient, Schema schema, BloomFilter filter, Option<HoodieEngineContext> context) {
     return new HoodieSparkWriteableTestTable(metaClient.getBasePath().toString(),
-        metaClient.getRawHoodieStorage(),
+        metaClient.getRawStorage(),
         metaClient, schema, filter, null, context);
   }
 
@@ -79,7 +79,7 @@ public static HoodieSparkWriteableTestTable of(HoodieTableMetaClient metaClient,
   public static HoodieSparkWriteableTestTable of(HoodieTableMetaClient metaClient, Schema schema, BloomFilter filter,
                                                  HoodieTableMetadataWriter metadataWriter, Option<HoodieEngineContext> context) {
     return new HoodieSparkWriteableTestTable(metaClient.getBasePath().toString(),
-        metaClient.getRawHoodieStorage(),
+        metaClient.getRawStorage(),
         metaClient, schema, filter, metadataWriter, context);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java
Patch:
@@ -311,7 +311,7 @@ public Schema readSchemaFromLastCompaction(Option<HoodieInstant> lastCompactionC
   }
 
   private Schema readSchemaFromLogFile(StoragePath path) throws IOException {
-    return readSchemaFromLogFile(metaClient.getRawHoodieStorage(), path);
+    return readSchemaFromLogFile(metaClient.getRawStorage(), path);
   }
 
   /**

File: hudi-examples/hudi-examples-java/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java
Patch:
@@ -82,7 +82,7 @@ public static void main(String[] args) throws Exception {
     Path path = new Path(tablePath);
     FileSystem fs = HadoopFSUtils.getFs(tablePath, storageConf);
     if (!fs.exists(path)) {
-      HoodieTableMetaClient.withPropertyBuilder()
+      HoodieTableMetaClient.newTableBuilder()
         .setTableType(tableType)
         .setTableName(tableName)
           .setPayloadClassName(HoodieAvroPayload.class.getName())

File: hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/spark/HoodieWriteClientExample.java
Patch:
@@ -86,7 +86,7 @@ public static void main(String[] args) throws Exception {
       Path path = new Path(tablePath);
       FileSystem fs = HadoopFSUtils.getFs(tablePath, jsc.hadoopConfiguration());
       if (!fs.exists(path)) {
-        HoodieTableMetaClient.withPropertyBuilder()
+        HoodieTableMetaClient.newTableBuilder()
             .setTableType(tableType)
             .setTableName(tableName)
             .setPayloadClass(HoodieAvroPayload.class)

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java
Patch:
@@ -718,7 +718,7 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor
           HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setBasePath(location)
               .setConf(HadoopFSUtils.getStorageConfWithCopy(hiveConf)).build();
           //Init table with new name
-          HoodieTableMetaClient.withPropertyBuilder().fromProperties(metaClient.getTableConfig().getProps())
+          HoodieTableMetaClient.newTableBuilder().fromProperties(metaClient.getTableConfig().getProps())
               .setTableName(newTableName)
               .initTable(HadoopFSUtils.getStorageConfWithCopy(hiveConf), location);
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/FlinkWriteClients.java
Patch:
@@ -160,6 +160,7 @@ public static HoodieWriteConfig getHoodieClientConfig(
             .withEngineType(EngineType.FLINK)
             .withPath(conf.getString(FlinkOptions.PATH))
             .combineInput(conf.getBoolean(FlinkOptions.PRE_COMBINE), true)
+            .withWriteTableVersion(conf.getInteger(FlinkOptions.WRITE_TABLE_VERSION))
             .withMergeAllowDuplicateOnInserts(OptionsResolver.insertClustering(conf))
             .withClusteringConfig(
                 HoodieClusteringConfig.newBuilder()

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java
Patch:
@@ -249,10 +249,11 @@ public static HoodieTableMetaClient initTableIfNotExists(
       org.apache.hadoop.conf.Configuration hadoopConf) throws IOException {
     final String basePath = conf.getString(FlinkOptions.PATH);
     if (!tableExists(basePath, hadoopConf)) {
-      HoodieTableMetaClient.withPropertyBuilder()
+      HoodieTableMetaClient.newTableBuilder()
           .setTableCreateSchema(conf.getString(FlinkOptions.SOURCE_AVRO_SCHEMA))
           .setTableType(conf.getString(FlinkOptions.TABLE_TYPE))
           .setTableName(conf.getString(FlinkOptions.TABLE_NAME))
+          .setTableVersion(conf.getInteger(FlinkOptions.WRITE_TABLE_VERSION))
           .setDatabaseName(conf.getString(FlinkOptions.DATABASE_NAME))
           .setRecordKeyFields(conf.getString(FlinkOptions.RECORD_KEY_FIELD, null))
           .setPayloadClassName(conf.getString(FlinkOptions.PAYLOAD_CLASS_NAME))

File: hudi-gcp/src/test/java/org/apache/hudi/gcp/bigquery/TestHoodieBigQuerySyncClient.java
Patch:
@@ -78,7 +78,7 @@ public class TestHoodieBigQuerySyncClient {
   @BeforeAll
   static void setupOnce() throws Exception {
     basePath = tempDir.toString();
-    metaClient = HoodieTableMetaClient.withPropertyBuilder()
+    metaClient = HoodieTableMetaClient.newTableBuilder()
         .setTableType(HoodieTableType.COPY_ON_WRITE)
         .setTableName(TEST_TABLE)
         .setPayloadClass(HoodieAvroPayload.class)

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/fs/TestFSUtilsWithRetryWrapperEnable.java
Patch:
@@ -78,7 +78,7 @@ public void setUp() throws IOException {
     HoodieWrapperFileSystem fs =
         new HoodieWrapperFileSystem(fileSystem, new NoOpConsistencyGuard());
     HoodieStorage storage = new HoodieHadoopStorage(fs);
-    metaClient.setHoodieStorage(storage);
+    metaClient.setStorage(storage);
   }
 
   // Test the scenario that fs keeps retrying until it fails.
@@ -92,7 +92,7 @@ public void testProcessFilesWithExceptions() throws Exception {
     HoodieWrapperFileSystem fs =
         new HoodieWrapperFileSystem(fileSystem, new NoOpConsistencyGuard());
     HoodieStorage storage = new HoodieHadoopStorage(fs);
-    metaClient.setHoodieStorage(storage);
+    metaClient.setStorage(storage);
     List<String> folders =
         Arrays.asList("2016/04/15", ".hoodie/.temp/2/2016/04/15");
     folders.forEach(f -> assertThrows(RuntimeException.class, () -> metaClient.getStorage()

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/table/timeline/TestHoodieActiveTimeline.java
Patch:
@@ -138,7 +138,7 @@ public void testLoadingInstantsFromFiles() throws IOException {
         "Check the instants stream");
 
     // Backwards compatibility testing for reading compaction plans
-    metaClient = HoodieTableMetaClient.withPropertyBuilder()
+    metaClient = HoodieTableMetaClient.newTableBuilder()
         .fromMetaClient(metaClient)
         .setTimelineLayoutVersion(VERSION_0)
         .initTable(metaClient.getStorageConf().newInstance(), metaClient.getBasePath());
@@ -773,11 +773,11 @@ private void shouldAllowTempCommit(boolean allowTempCommit, Consumer<HoodieTable
       HoodieStorage storage = metaClient.getStorage();
       FileSystem fs = (FileSystem) storage.getFileSystem();
       HoodieWrapperFileSystem newFs = new HoodieWrapperFileSystem(fs, new NoOpConsistencyGuard());
-      metaClient.setHoodieStorage(new HoodieHadoopStorage(newFs));
+      metaClient.setStorage(new HoodieHadoopStorage(newFs));
       try {
         fun.accept(metaClient);
       } finally {
-        metaClient.setHoodieStorage(storage);
+        metaClient.setStorage(storage);
       }
       return;
     }

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestTable.java
Patch:
@@ -169,7 +169,7 @@ protected HoodieTestTable(String basePath, HoodieStorage storage,
                             HoodieTableMetaClient metaClient, Option<HoodieEngineContext> context) {
     ValidationUtils.checkArgument(Objects.equals(basePath, metaClient.getBasePath().toString()));
     ValidationUtils.checkArgument(Objects.equals(
-        storage.getFileSystem(), metaClient.getRawHoodieStorage().getFileSystem()));
+        storage.getFileSystem(), metaClient.getRawStorage().getFileSystem()));
     this.basePath = basePath;
     this.storage = storage;
     this.fs = (FileSystem) storage.getFileSystem();
@@ -180,7 +180,7 @@ protected HoodieTestTable(String basePath, HoodieStorage storage,
 
   public static HoodieTestTable of(HoodieTableMetaClient metaClient) {
     testTableState = HoodieTestTableState.of();
-    return new HoodieTestTable(metaClient.getBasePath().toString(), metaClient.getRawHoodieStorage(), metaClient);
+    return new HoodieTestTable(metaClient.getBasePath().toString(), metaClient.getRawStorage(), metaClient);
   }
 
   public void setNonPartitioned() {

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestInputPathHandler.java
Patch:
@@ -163,8 +163,9 @@ static HoodieTableMetaClient initTableType(Configuration hadoopConf, String base
     properties.setProperty(HoodieTableConfig.TYPE.key(), tableType.name());
     properties.setProperty(HoodieTableConfig.PAYLOAD_CLASS_NAME.key(), HoodieAvroPayload.class.getName());
     properties.setProperty(HoodieTableConfig.RECORD_MERGER_STRATEGY.key(), HoodieRecordMerger.DEFAULT_MERGER_STRATEGY_UUID);
-    return HoodieTableMetaClient.initTableAndGetMetaClient(
-        HadoopFSUtils.getStorageConfWithCopy(hadoopConf), basePath, properties);
+    return HoodieTableMetaClient.newTableBuilder()
+        .fromProperties(properties)
+        .initTable(HadoopFSUtils.getStorageConfWithCopy(hadoopConf), basePath);
   }
 
   static List<Path> generatePartitions(DistributedFileSystem dfs, String basePath)

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteJob.java
Patch:
@@ -121,7 +121,7 @@ public HoodieTestSuiteJob(HoodieTestSuiteConfig cfg, JavaSparkContext jsc, boole
         (BuiltinKeyGenerator) HoodieSparkKeyGeneratorFactory.createKeyGenerator(props);
 
     if (!fs.exists(new Path(cfg.targetBasePath))) {
-      metaClient = HoodieTableMetaClient.withPropertyBuilder()
+      metaClient = HoodieTableMetaClient.newTableBuilder()
           .setTableType(cfg.tableType)
           .setTableName(cfg.targetTableName)
           .setRecordKeyFields(this.props.getString(DataSourceWriteOptions.RECORDKEY_FIELD().key()))

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/KafkaConnectTransactionServices.java
Patch:
@@ -94,12 +94,13 @@ public KafkaConnectTransactionServices(KafkaConnectConfigs connectConfigs) throw
       LOG.info(String.format("Setting record key %s and partition fields %s for table %s",
           recordKeyFields, partitionColumns, tableBasePath + tableName));
 
-      tableMetaClient = Option.of(HoodieTableMetaClient.withPropertyBuilder()
+      tableMetaClient = Option.of(HoodieTableMetaClient.newTableBuilder()
           .setTableType(HoodieTableType.COPY_ON_WRITE.name())
           .setTableName(tableName)
           .setPayloadClassName(HoodieAvroPayload.class.getName())
           .setRecordKeyFields(recordKeyFields)
           .setPartitionFields(partitionColumns)
+          .setTableVersion(writeConfig.getWriteVersion())
           .setKeyGeneratorClassProp(writeConfig.getKeyGeneratorClass())
           .fromProperties(connectConfigs.getProps())
           .initTable(storageConf.newInstance(), tableBasePath));

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/DataSourceInternalWriterHelper.java
Patch:
@@ -70,7 +70,7 @@ public DataSourceInternalWriterHelper(String instantTime, HoodieWriteConfig writ
 
     this.metaClient = HoodieTableMetaClient.builder()
         .setConf(storageConf.newInstance()).setBasePath(writeConfig.getBasePath()).build();
-    this.metaClient.validateTableProperties(writeConfig.getProps());
+    this.writeClient.validateAgainstTableProperties(this.metaClient.getTableConfig(), writeConfig);
     this.writeClient.preWrite(instantTime, WriteOperationType.BULK_INSERT, metaClient);
   }
 

File: hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/cli/BootstrapExecutorUtils.java
Patch:
@@ -234,11 +234,12 @@ private void initializeTable() throws IOException {
     Map<String, Object> timestampKeyGeneratorConfigs =
         extractConfigsRelatedToTimestampBasedKeyGenerator(keyGenClassAndParColsForKeyGenerator.getLeft(), props);
 
-    HoodieTableMetaClient.PropertyBuilder builder = HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.TableBuilder builder = HoodieTableMetaClient.newTableBuilder()
         .fromProperties(props)
         .setTableType(cfg.tableType)
         .setDatabaseName(cfg.database)
         .setTableName(cfg.tableName)
+        .setTableVersion(bootstrapConfig.getWriteVersion())
         .setRecordKeyFields(props.getString(RECORDKEY_FIELD_NAME.key()))
         .setPreCombineField(props.getString(PRECOMBINE_FIELD_NAME.key(), null))
         .setPopulateMetaFields(props.getBoolean(

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveSyncFunctionalTestHarness.java
Patch:
@@ -108,7 +108,7 @@ public HiveSyncConfig hiveSyncConf() throws IOException {
   }
 
   public HoodieHiveSyncClient hiveClient(HiveSyncConfig hiveSyncConfig) throws IOException {
-    HoodieTableMetaClient metaClient = HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient metaClient = HoodieTableMetaClient.newTableBuilder()
         .setTableType(HoodieTableType.COPY_ON_WRITE)
         .setTableName(hiveSyncConfig.getString(META_SYNC_TABLE_NAME))
         .setPayloadClass(HoodieAvroPayload.class)

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestCluster.java
Patch:
@@ -161,7 +161,7 @@ public void createCOWTable(String commitTime, int numberOfPartitions, String dbN
     String tablePathStr = tablePath(dbName, tableName);
     Path path = new Path(tablePathStr);
     FileIOUtils.deleteDirectory(new File(path.toString()));
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .setTableType(HoodieTableType.COPY_ON_WRITE)
         .setTableName(tableName)
         .setPayloadClass(HoodieAvroPayload.class)

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java
Patch:
@@ -261,6 +261,7 @@ private int doScheduleAndCluster(JavaSparkContext jsc) throws Exception {
 
       if (cfg.retryLastFailedClusteringJob) {
         HoodieSparkTable<HoodieRecordPayload> table = HoodieSparkTable.create(client.getConfig(), client.getEngineContext());
+        client.validateAgainstTableProperties(table.getMetaClient().getTableConfig(), client.getConfig());
         Option<HoodieInstant> lastClusterOpt = table.getActiveTimeline().getLastPendingClusterInstant();
 
         if (lastClusterOpt.isPresent()) {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java
Patch:
@@ -323,6 +323,7 @@ public void run() {
   public void dryRun() {
     try (SparkRDDWriteClient<HoodieRecordPayload> client =  UtilHelpers.createHoodieClient(jsc, cfg.basePath, "", cfg.parallelism, Option.empty(), props)) {
       HoodieSparkTable<HoodieRecordPayload> table = HoodieSparkTable.create(client.getConfig(), client.getEngineContext());
+      client.validateAgainstTableProperties(table.getMetaClient().getTableConfig(), client.getConfig());
       List<String> parts = Arrays.asList(cfg.partitions.split(","));
       Map<String, List<String>> partitionToReplaceFileIds = jsc.parallelize(parts, parts.size()).distinct()
           .mapToPair(partitionPath -> new Tuple2<>(partitionPath, table.getSliceView().getLatestFileSlices(partitionPath).map(fg -> fg.getFileId()).distinct().collect(Collectors.toList())))

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/BootstrapExecutor.java
Patch:
@@ -59,6 +59,7 @@
 import static org.apache.hudi.common.table.HoodieTableConfig.POPULATE_META_FIELDS;
 import static org.apache.hudi.common.table.HoodieTableConfig.TIMELINE_TIMEZONE;
 import static org.apache.hudi.config.HoodieWriteConfig.PRECOMBINE_FIELD_NAME;
+import static org.apache.hudi.config.HoodieWriteConfig.WRITE_TABLE_VERSION;
 import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_SYNC_BUCKET_SYNC;
 import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_SYNC_BUCKET_SYNC_SPEC;
 import static org.apache.hudi.keygen.constant.KeyGeneratorOptions.HIVE_STYLE_PARTITIONING_ENABLE;
@@ -204,13 +205,14 @@ private void initializeTable() throws IOException {
       throw new IllegalArgumentException("Bootstrap source base path and Hudi table base path must be different");
     }
 
-    HoodieTableMetaClient.PropertyBuilder builder = HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.TableBuilder builder = HoodieTableMetaClient.newTableBuilder()
         .fromProperties(props)
         .setTableType(cfg.tableType)
         .setTableName(cfg.targetTableName)
         .setRecordKeyFields(props.getString(RECORDKEY_FIELD_NAME.key()))
         .setPreCombineField(props.getString(
             PRECOMBINE_FIELD_NAME.key(), PRECOMBINE_FIELD_NAME.defaultValue()))
+        .setTableVersion(props.getInteger(WRITE_TABLE_VERSION.key(), WRITE_TABLE_VERSION.defaultValue()))
         .setPopulateMetaFields(props.getBoolean(
             POPULATE_META_FIELDS.key(), POPULATE_META_FIELDS.defaultValue()))
         .setArchiveLogFolder(props.getString(

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/SparkSampleWritesUtils.java
Patch:
@@ -93,7 +93,7 @@ public static Option<HoodieWriteConfig> getWriteConfigWithRecordSizeEstimate(Jav
   private static Pair<Boolean, String> doSampleWrites(JavaSparkContext jsc, Option<JavaRDD<HoodieRecord>> recordsOpt, HoodieWriteConfig writeConfig, String instantTime)
       throws IOException {
     final String sampleWritesBasePath = getSampleWritesBasePath(jsc, writeConfig, instantTime);
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .setTableType(HoodieTableType.COPY_ON_WRITE)
         .setTableName(String.format("%s_samples_%s", writeConfig.getTableName(), instantTime))
         .setCDCEnabled(false)

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieSnapshotExporter.java
Patch:
@@ -91,7 +91,7 @@ public void init() throws Exception {
     targetPath = Paths.get(basePath(), "target").toString();
     storage = HoodieStorageUtils.getStorage(basePath(), HadoopFSUtils.getStorageConf(jsc().hadoopConfiguration()));
 
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .setTableType(HoodieTableType.COPY_ON_WRITE)
         .setTableName(TABLE_NAME)
         .setPayloadClass(HoodieAvroPayload.class)

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestHoodieIncrSource.java
Patch:
@@ -106,13 +106,12 @@ public void setUp() throws IOException {
 
   @Override
   public HoodieTableMetaClient getHoodieMetaClient(StorageConfiguration<?> storageConf, String basePath, Properties props) throws IOException {
-    props = HoodieTableMetaClient.withPropertyBuilder()
+    return HoodieTableMetaClient.newTableBuilder()
         .setTableName(RAW_TRIPS_TEST_NAME)
         .setTableType(tableType)
         .setPayloadClass(HoodieAvroPayload.class)
         .fromProperties(props)
-        .build();
-    return HoodieTableMetaClient.initTableAndGetMetaClient(storageConf.newInstance(), basePath, props);
+        .initTable(storageConf.newInstance(), basePath);
   }
 
   @Test

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/testutils/UtilitiesTestBase.java
Patch:
@@ -309,7 +309,7 @@ private static void clearHiveDb(String tempWriteablePath) throws Exception {
     // Create Dummy hive sync config
     HiveSyncConfig hiveSyncConfig = getHiveSyncConfig(tempWriteablePath, "dummy");
     hiveSyncConfig.setHadoopConf(hiveTestService.getHiveConf());
-    HoodieTableMetaClient.withPropertyBuilder()
+    HoodieTableMetaClient.newTableBuilder()
         .setTableType(HoodieTableType.COPY_ON_WRITE)
         .setTableName(hiveSyncConfig.getString(META_SYNC_TABLE_NAME))
         .initTable(storage.getConf().newInstance(), hiveSyncConfig.getString(META_SYNC_BASE_PATH));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java
Patch:
@@ -239,8 +239,8 @@ protected List<Pair<String, BloomIndexFileInfo>> loadColumnRangesFromMetaIndex(
           new BloomIndexFileInfo(
               partitionAndFileNameToFileId.get(entry.getKey()),
               // NOTE: Here we assume that the type of the primary key field is string
-              (String) unwrapAvroValueWrapper(entry.getValue().getMinValue()),
-              (String) unwrapAvroValueWrapper(entry.getValue().getMaxValue())
+              unwrapAvroValueWrapper(entry.getValue().getMinValue()).toString(),
+              unwrapAvroValueWrapper(entry.getValue().getMaxValue()).toString()
           )));
     }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieLogBlock.java
Patch:
@@ -320,6 +320,7 @@ protected void inflate() throws HoodieIOException {
     } catch (IOException e) {
       // TODO : fs.open() and return inputstream again, need to pass FS configuration
       // because the inputstream might close/timeout for large number of log blocks to be merged
+      deflate();
       inflate();
     }
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/HoodieMultiTableStreamer.java
Patch:
@@ -140,6 +140,7 @@ private void populateTableExecutionContextList(TypedProperties properties, Strin
       //copy all the values from config to cfg
       String targetBasePath = resetTarget(config, database, currentTable);
       Helpers.deepCopyConfigs(config, cfg);
+      cfg.propsFilePath = configFilePath;
       String overriddenTargetBasePath = getStringWithAltKeys(tableProperties, HoodieStreamerConfig.TARGET_BASE_PATH, true);
       cfg.targetBasePath = StringUtils.isNullOrEmpty(overriddenTargetBasePath) ? targetBasePath : overriddenTargetBasePath;
       if (cfg.enableMetaSync && StringUtils.isNullOrEmpty(tableProperties.getString(HoodieSyncConfig.META_SYNC_TABLE_NAME.key(), ""))) {
@@ -255,6 +256,7 @@ static void deepCopyConfigs(Config globalConfig, HoodieStreamer.Config tableConf
       tableConfig.clusterSchedulingWeight = globalConfig.clusterSchedulingWeight;
       tableConfig.clusterSchedulingMinShare = globalConfig.clusterSchedulingMinShare;
       tableConfig.sparkMaster = globalConfig.sparkMaster;
+      tableConfig.configs.addAll(globalConfig.configs);
     }
   }
 

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/util/Parquet2SparkSchemaUtils.java
Patch:
@@ -141,7 +141,7 @@ private static String convertGroupField(GroupType field) {
         ValidationUtils.checkArgument(field.getFieldCount() == 1, "Illegal List type: " + field);
         Type repeatedType = field.getType(0);
         if (isElementType(repeatedType, field.getName())) {
-          return arrayType(repeatedType, false);
+          return arrayType(repeatedType, true);
         } else {
           Type elementType = repeatedType.asGroupType().getType(0);
           boolean optional = elementType.isRepetition(OPTIONAL);

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/utils/BucketStreamWriteFunctionWrapper.java
Patch:
@@ -150,15 +150,15 @@ public void checkpointFunction(long checkpointId) throws Exception {
     // checkpoint the coordinator first
     this.coordinator.checkpointCoordinator(checkpointId, new CompletableFuture<>());
     writeFunction.snapshotState(new MockFunctionSnapshotContext(checkpointId));
-    stateInitializationContext.getOperatorStateStore().checkpointBegin(checkpointId);
+    stateInitializationContext.checkpointBegin(checkpointId);
   }
 
   public void endInput() {
     writeFunction.endInput();
   }
 
   public void checkpointComplete(long checkpointId) {
-    stateInitializationContext.getOperatorStateStore().checkpointSuccess(checkpointId);
+    stateInitializationContext.checkpointSuccess(checkpointId);
     coordinator.notifyCheckpointComplete(checkpointId);
     if (asyncCompaction) {
       try {

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/utils/ConsistentBucketStreamWriteFunctionWrapper.java
Patch:
@@ -76,6 +76,6 @@ public void checkpointFunction(long checkpointId) throws Exception {
     this.coordinator.checkpointCoordinator(checkpointId, new CompletableFuture<>());
     writeFunction.snapshotState(functionSnapshotContext);
     assignFunction.snapshotState(functionSnapshotContext);
-    stateInitializationContext.getOperatorStateStore().checkpointBegin(checkpointId);
+    stateInitializationContext.checkpointBegin(checkpointId);
   }
 }

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/utils/InsertFunctionWrapper.java
Patch:
@@ -126,7 +126,7 @@ public void checkpointFunction(long checkpointId) throws Exception {
     this.coordinator.checkpointCoordinator(checkpointId, new CompletableFuture<>());
 
     writeFunction.snapshotState(new MockFunctionSnapshotContext(checkpointId));
-    stateInitializationContext.getOperatorStateStore().checkpointBegin(checkpointId);
+    stateInitializationContext.checkpointBegin(checkpointId);
   }
 
   @Override
@@ -135,7 +135,7 @@ public void endInput() {
   }
 
   public void checkpointComplete(long checkpointId) {
-    stateInitializationContext.getOperatorStateStore().checkpointSuccess(checkpointId);
+    stateInitializationContext.checkpointSuccess(checkpointId);
     coordinator.notifyCheckpointComplete(checkpointId);
     if (asyncClustering) {
       try {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/StreamSync.java
Patch:
@@ -316,7 +316,7 @@ public StreamSync(HoodieStreamer.Config cfg, SparkSession sparkSession,
     this.hoodieMetrics = new HoodieMetrics(hoodieWriteConfig, storage);
     if (props.getBoolean(ERROR_TABLE_ENABLED.key(), ERROR_TABLE_ENABLED.defaultValue())) {
       this.errorTableWriter = ErrorTableUtils.getErrorTableWriter(
-          cfg, sparkSession, props, hoodieSparkContext, fs);
+          cfg, sparkSession, props, hoodieSparkContext, fs, Option.of(metrics));
       this.errorWriteFailureStrategy = ErrorTableUtils.getErrorWriteFailureStrategy(props);
     }
     refreshTimeline();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowParquetWriteSupport.java
Patch:
@@ -46,7 +46,7 @@ public class HoodieRowParquetWriteSupport extends ParquetWriteSupport {
 
   public HoodieRowParquetWriteSupport(Configuration conf, StructType structType, Option<BloomFilter> bloomFilterOpt, HoodieConfig config) {
     Configuration hadoopConf = new Configuration(conf);
-    hadoopConf.set("spark.sql.parquet.writeLegacyFormat", config.getStringOrDefault(HoodieStorageConfig.PARQUET_WRITE_LEGACY_FORMAT_ENABLED));
+    hadoopConf.set("spark.sql.parquet.writeLegacyFormat", config.getStringOrDefault(HoodieStorageConfig.PARQUET_WRITE_LEGACY_FORMAT_ENABLED, "false"));
     hadoopConf.set("spark.sql.parquet.outputTimestampType", config.getStringOrDefault(HoodieStorageConfig.PARQUET_OUTPUT_TIMESTAMP_TYPE));
     hadoopConf.set("spark.sql.parquet.fieldId.write.enabled", config.getStringOrDefault(PARQUET_FIELD_ID_WRITE_ENABLED));
     setSchema(structType, hadoopConf);

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieStorageConfig.java
Patch:
@@ -129,7 +129,7 @@ public class HoodieStorageConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> PARQUET_WRITE_LEGACY_FORMAT_ENABLED = ConfigProperty
       .key("hoodie.parquet.writelegacyformat.enabled")
-      .defaultValue("false")
+      .noDefaultValue()
       .markAdvanced()
       .withDocumentation("Sets spark.sql.parquet.writeLegacyFormat. If true, data will be written in a way of Spark 1.4 and earlier. "
           + "For example, decimal values will be written in Parquet's fixed-length byte array format which other systems such as Apache Hive and Apache Impala use. "

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieDeltaStreamerWrapper.java
Patch:
@@ -86,7 +86,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> fetchSource() t
         .setBasePath(service.getCfg().targetBasePath)
         .build();
     String instantTime = InProcessTimeGenerator.createNewInstantTime();
-    InputBatch inputBatch = service.readFromSource(instantTime, metaClient);
+    InputBatch inputBatch = service.readFromSource(instantTime, metaClient).getLeft();
     return Pair.of(inputBatch.getSchemaProvider(), Pair.of(inputBatch.getCheckpointForNextBatch(), (JavaRDD<HoodieRecord>) inputBatch.getBatch().get()));
   }
 

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestHoodieSparkMergeOnReadTableClustering.java
Patch:
@@ -241,9 +241,7 @@ private void doClusteringAndValidate(SparkRDDWriteClient client,
                                        HoodieWriteConfig cfg,
                                        HoodieTestDataGenerator dataGen,
                                        boolean clusteringAsRow) {
-    if (clusteringAsRow) {
-      client.getConfig().setValue(DataSourceWriteOptions.ENABLE_ROW_WRITER(), "true");
-    }
+    client.getConfig().setValue(DataSourceWriteOptions.ENABLE_ROW_WRITER(), Boolean.toString(clusteringAsRow));
 
     client.cluster(clusteringCommitTime, true);
     metaClient = HoodieTableMetaClient.reload(metaClient);

File: hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/DefaultSource.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.config.HoodieInternalConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 
+import org.apache.spark.sql.HoodieDataTypeUtils;
 import org.apache.spark.sql.SaveMode;
 import org.apache.spark.sql.sources.DataSourceRegister;
 import org.apache.spark.sql.sources.v2.DataSourceOptions;
@@ -36,8 +37,6 @@
 import java.util.Map;
 import java.util.Optional;
 
-import static org.apache.hudi.DataSourceUtils.tryOverrideParquetWriteLegacyFormatProperty;
-
 /**
  * DataSource V2 implementation for managing internal write logic. Only called internally.
  */
@@ -69,7 +68,7 @@ public Optional<DataSourceWriter> createWriter(String writeUUID, StructType sche
         HoodieTableConfig.POPULATE_META_FIELDS.defaultValue());
     Map<String, String> properties = options.asMap();
     // Auto set the value of "hoodie.parquet.writelegacyformat.enabled"
-    tryOverrideParquetWriteLegacyFormatProperty(properties, schema);
+    HoodieDataTypeUtils.tryOverrideParquetWriteLegacyFormatProperty(properties, schema);
     // 1st arg to createHoodieConfig is not really required to be set. but passing it anyways.
     HoodieWriteConfig config = DataSourceUtils.createHoodieConfig(options.get(HoodieWriteConfig.AVRO_SCHEMA_STRING.key()).get(), path, tblName, properties);
     boolean arePartitionRecordsSorted = HoodieInternalConfig.getBulkInsertIsPartitionRecordsSorted(

File: hudi-spark-datasource/hudi-spark3-common/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java
Patch:
@@ -35,8 +35,6 @@
 import java.util.HashMap;
 import java.util.Map;
 
-import static org.apache.hudi.DataSourceUtils.tryOverrideParquetWriteLegacyFormatProperty;
-
 /**
  * DataSource V2 implementation for managing internal write logic. Only called internally.
  * This class is only compatible with datasource V2 API in Spark 3.
@@ -61,7 +59,7 @@ public Table getTable(StructType schema, Transform[] partitioning, Map<String, S
     // Create a new map as the properties is an unmodifiableMap on Spark 3.2.0
     Map<String, String> newProps = new HashMap<>(properties);
     // Auto set the value of "hoodie.parquet.writelegacyformat.enabled"
-    tryOverrideParquetWriteLegacyFormatProperty(newProps, schema);
+    HoodieDataTypeUtils.tryOverrideParquetWriteLegacyFormatProperty(newProps, schema);
     // 1st arg to createHoodieConfig is not really required to be set. but passing it anyways.
     HoodieWriteConfig config = DataSourceUtils.createHoodieConfig(newProps.get(HoodieWriteConfig.AVRO_SCHEMA_STRING.key()), path, tblName, newProps);
     return new HoodieDataSourceInternalTable(instantTime, config, schema, getSparkSession(),

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieReaderConfig.java
Patch:
@@ -68,7 +68,7 @@ public class HoodieReaderConfig extends HoodieConfig {
 
   public static final ConfigProperty<Boolean> MERGE_USE_RECORD_POSITIONS = ConfigProperty
       .key("hoodie.merge.use.record.positions")
-      .defaultValue(false)
+      .defaultValue(true)
       .markAdvanced()
       .sinceVersion("1.0.0")
       .withDocumentation("Whether to use positions in the block header for data blocks containing updates and delete blocks for merging.");

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/metadata/JavaHoodieBackedTableMetadataWriter.java
Patch:
@@ -87,7 +87,7 @@ protected void initRegistry() {
   }
 
   @Override
-  protected void commit(String instantTime, Map<MetadataPartitionType, HoodieData<HoodieRecord>> partitionRecordsMap) {
+  protected void commit(String instantTime, Map<String, HoodieData<HoodieRecord>> partitionRecordsMap) {
     commitInternal(instantTime, partitionRecordsMap, false, Option.empty());
   }
 
@@ -97,8 +97,8 @@ protected List<HoodieRecord> convertHoodieDataToEngineSpecificData(HoodieData<Ho
   }
 
   @Override
-  protected void bulkCommit(String instantTime, MetadataPartitionType partitionType, HoodieData<HoodieRecord> records, int fileGroupCount) {
-    commitInternal(instantTime, Collections.singletonMap(partitionType, records), true, Option.of(new JavaHoodieMetadataBulkInsertPartitioner()));
+  protected void bulkCommit(String instantTime, String partitionName, HoodieData<HoodieRecord> records, int fileGroupCount) {
+    commitInternal(instantTime, Collections.singletonMap(partitionName, records), true, Option.of(new JavaHoodieMetadataBulkInsertPartitioner()));
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -130,7 +130,7 @@ protected void initRegistry() {
   }
 
   @Override
-  protected void commit(String instantTime, Map<MetadataPartitionType, HoodieData<HoodieRecord>> partitionRecordsMap) {
+  protected void commit(String instantTime, Map<String, HoodieData<HoodieRecord>> partitionRecordsMap) {
     commitInternal(instantTime, partitionRecordsMap, false, Option.empty());
   }
 
@@ -141,10 +141,10 @@ protected JavaRDD<HoodieRecord> convertHoodieDataToEngineSpecificData(HoodieData
 
   @Override
   protected void bulkCommit(
-      String instantTime, MetadataPartitionType partitionType, HoodieData<HoodieRecord> records,
+      String instantTime, String partitionName, HoodieData<HoodieRecord> records,
       int fileGroupCount) {
     SparkHoodieMetadataBulkInsertPartitioner partitioner = new SparkHoodieMetadataBulkInsertPartitioner(fileGroupCount);
-    commitInternal(instantTime, Collections.singletonMap(partitionType, records), true, Option.of(partitioner));
+    commitInternal(instantTime, Collections.singletonMap(partitionName, records), true, Option.of(partitioner));
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/metadata/BaseTableMetadata.java
Patch:
@@ -343,12 +343,12 @@ public Map<String, List<HoodieRecordGlobalLocation>> readSecondaryIndex(List<Str
   /**
    * Returns a map of (record-key -> secondary-key) for the provided record keys.
    */
-  public Map<String, String> getSecondaryKeys(List<String> recordKeys) {
+  public Map<String, String> getSecondaryKeys(List<String> recordKeys, String secondaryIndexName) {
     ValidationUtils.checkState(dataMetaClient.getTableConfig().isMetadataPartitionAvailable(MetadataPartitionType.RECORD_INDEX),
         "Record index is not initialized in MDT");
-    ValidationUtils.checkState(dataMetaClient.getTableConfig().isMetadataPartitionAvailable(MetadataPartitionType.SECONDARY_INDEX),
+    ValidationUtils.checkState(dataMetaClient.getTableConfig().getMetadataPartitions().contains(secondaryIndexName),
         "Secondary index is not initialized in MDT");
-    return getSecondaryKeysForRecordKeys(recordKeys, MetadataPartitionType.SECONDARY_INDEX.getPartitionPath());
+    return getSecondaryKeysForRecordKeys(recordKeys, secondaryIndexName);
   }
 
   /**

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/HoodieSparkFunctionalIndexClient.java
Patch:
@@ -21,6 +21,7 @@
 
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.common.config.HoodieIndexingConfig;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.HoodieIndexDefinition;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteConcurrencyMode;
@@ -128,7 +129,8 @@ private static Map<String, String> buildWriteConfig(HoodieTableMetaClient metaCl
     Map<String, String> writeConfig = new HashMap<>();
     if (metaClient.getTableConfig().isMetadataTableAvailable()) {
       writeConfig.put(HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(), WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.name());
-      writeConfig.putAll(JavaConverters.mapAsJavaMapConverter(HoodieCLIUtils.getLockOptions(metaClient.getBasePath().toString())).asJava());
+      writeConfig.putAll(JavaConverters.mapAsJavaMapConverter(HoodieCLIUtils.getLockOptions(metaClient.getBasePath().toString(),
+              metaClient.getBasePath().toUri().getScheme(), new TypedProperties())).asJava());
 
       // [HUDI-7472] Ensure write-config contains the existing MDT partition to prevent those from getting deleted
       metaClient.getTableConfig().getMetadataPartitions().forEach(partitionPath -> {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java
Patch:
@@ -70,7 +70,7 @@ public HoodieClusteringJob(JavaSparkContext jsc, Config cfg, TypedProperties pro
     this.props.put(HoodieCleanConfig.ASYNC_CLEAN.key(), false);
     if (this.metaClient.getTableConfig().isMetadataTableAvailable()) {
       // add default lock config options if MDT is enabled.
-      UtilHelpers.addLockOptions(cfg.basePath, this.props);
+      UtilHelpers.addLockOptions(cfg.basePath, this.metaClient.getBasePath().toUri().getScheme(), this.props);
     }
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java
Patch:
@@ -73,7 +73,7 @@ public HoodieCompactor(JavaSparkContext jsc, Config cfg, TypedProperties props,
     this.props.put(HoodieCleanConfig.ASYNC_CLEAN.key(), false);
     if (this.metaClient.getTableConfig().isMetadataTableAvailable()) {
       // add default lock config options if MDT is enabled.
-      UtilHelpers.addLockOptions(cfg.basePath, this.props);
+      UtilHelpers.addLockOptions(cfg.basePath, this.metaClient.getBasePath().toUri().getScheme(),  this.props);
     }
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieTTLJob.java
Patch:
@@ -64,7 +64,7 @@ public HoodieTTLJob(JavaSparkContext jsc, Config cfg, TypedProperties props, Hoo
     this.props.put(HoodieCleanConfig.ASYNC_CLEAN.key(), false);
     if (this.metaClient.getTableConfig().isMetadataTableAvailable()) {
       // add default lock config options if MDT is enabled.
-      UtilHelpers.addLockOptions(cfg.basePath, this.props);
+      UtilHelpers.addLockOptions(cfg.basePath, this.metaClient.getBasePath().toUri().getScheme(),  this.props);
     }
   }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestUtilHelpers.java
Patch:
@@ -33,12 +33,12 @@ public class TestUtilHelpers {
   @Test
   void testAddLockOptions() {
     TypedProperties props1 = new TypedProperties();
-    UtilHelpers.addLockOptions("path1", props1);
+    UtilHelpers.addLockOptions("path1", "file", props1);
     assertEquals(FileSystemBasedLockProvider.class.getName(), props1.getString(HoodieLockConfig.LOCK_PROVIDER_CLASS_NAME.key()));
 
     TypedProperties props2 = new TypedProperties();
     props2.put(HoodieLockConfig.LOCK_PROVIDER_CLASS_NAME.key(), "Dummy");
-    UtilHelpers.addLockOptions("path2", props2);
+    UtilHelpers.addLockOptions("path2", "file", props2);
     assertEquals(1, props2.size(), "Should not add lock options if the lock provider is already there.");
   }
 }

File: hudi-aws/src/main/java/org/apache/hudi/config/GlueCatalogSyncClientConfig.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.common.config.ConfigGroups;
 import org.apache.hudi.common.config.ConfigProperty;
 import org.apache.hudi.common.config.HoodieConfig;
-import org.apache.hudi.common.util.HoodieTableConfigUtils;
+import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 
@@ -83,7 +83,7 @@ public class GlueCatalogSyncClientConfig extends HoodieConfig {
   public static final ConfigProperty<String> META_SYNC_PARTITION_INDEX_FIELDS = ConfigProperty
       .key(GLUE_CLIENT_PROPERTY_PREFIX + "partition_index_fields")
       .noDefaultValue()
-      .withInferFunction(cfg -> HoodieTableConfigUtils.getPartitionFieldProp(cfg)
+      .withInferFunction(cfg -> HoodieTableConfig.getPartitionFieldProp(cfg)
           .or(() -> Option.ofNullable(cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME))))
       .sinceVersion("0.15.0")
       .withDocumentation(String.join(" ", "Specify the partitions fields to index on aws glue. Separate the fields by semicolon.",

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/CustomKeyGenerator.java
Patch:
@@ -83,7 +83,7 @@ private static List<BuiltinKeyGenerator> getPartitionKeyGenerators(List<String>
       return Collections.emptyList();
     } else {
       return partitionPathFields.stream().map(field -> {
-        String[] fieldWithType = field.split(CustomAvroKeyGenerator.SPLIT_REGEX);
+        String[] fieldWithType = field.split(CUSTOM_KEY_GENERATOR_SPLIT_REGEX);
         if (fieldWithType.length != 2) {
           throw new HoodieKeyGeneratorException("Unable to find field names for partition path in proper format");
         }

File: hudi-gcp/src/main/java/org/apache/hudi/gcp/bigquery/BigQuerySyncConfig.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.common.config.ConfigProperty;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.config.TypedProperties;
-import org.apache.hudi.common.util.HoodieTableConfigUtils;
+import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
@@ -103,7 +103,7 @@ public class BigQuerySyncConfig extends HoodieSyncConfig implements Serializable
   public static final ConfigProperty<String> BIGQUERY_SYNC_PARTITION_FIELDS = ConfigProperty
       .key("hoodie.gcp.bigquery.sync.partition_fields")
       .noDefaultValue()
-      .withInferFunction(cfg -> HoodieTableConfigUtils.getPartitionFieldProp(cfg)
+      .withInferFunction(cfg -> HoodieTableConfig.getPartitionFieldProp(cfg)
           .or(() -> Option.ofNullable(cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME))))
       .markAdvanced()
       .withDocumentation("Comma-delimited partition fields. Default to non-partitioned.");

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/HoodieSyncConfig.java
Patch:
@@ -24,10 +24,10 @@
 import org.apache.hudi.common.config.HoodieConfig;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.config.TypedProperties;
+import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.util.HadoopConfigUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
-import org.apache.hudi.common.util.HoodieTableConfigUtils;
 import org.apache.hudi.config.metrics.HoodieMetricsConfig;
 import org.apache.hudi.hadoop.fs.HadoopFSUtils;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
@@ -109,7 +109,7 @@ public class HoodieSyncConfig extends HoodieConfig {
   public static final ConfigProperty<String> META_SYNC_PARTITION_FIELDS = ConfigProperty
       .key("hoodie.datasource.hive_sync.partition_fields")
       .defaultValue("")
-      .withInferFunction(cfg -> HoodieTableConfigUtils.getPartitionFieldProp(cfg)
+      .withInferFunction(cfg -> HoodieTableConfig.getPartitionFieldProp(cfg)
           .or(() -> Option.ofNullable(cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME))))
       .markAdvanced()
       .withDocumentation("Field in the table to use for determining hive partition columns.");
@@ -122,7 +122,7 @@ public class HoodieSyncConfig extends HoodieConfig {
         if (StringUtils.nonEmpty(cfg.getString(META_SYNC_PARTITION_FIELDS))) {
           partitionFieldsOpt = Option.ofNullable(cfg.getString(META_SYNC_PARTITION_FIELDS));
         } else {
-          partitionFieldsOpt = HoodieTableConfigUtils.getPartitionFieldProp(cfg)
+          partitionFieldsOpt = HoodieTableConfig.getPartitionFieldProp(cfg)
               .or(() -> Option.ofNullable(cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME)));
         }
         if (!partitionFieldsOpt.isPresent()) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java
Patch:
@@ -65,7 +65,7 @@ public interface HoodieTimeline extends Serializable {
   String SCHEMA_COMMIT_ACTION = "schemacommit";
   String[] VALID_ACTIONS_IN_TIMELINE = {COMMIT_ACTION, DELTA_COMMIT_ACTION,
       CLEAN_ACTION, SAVEPOINT_ACTION, RESTORE_ACTION, ROLLBACK_ACTION,
-      COMPACTION_ACTION, REPLACE_COMMIT_ACTION, CLUSTERING_ACTION, INDEXING_ACTION};
+      COMPACTION_ACTION, LOG_COMPACTION_ACTION, REPLACE_COMMIT_ACTION, CLUSTERING_ACTION, INDEXING_ACTION};
 
   String COMMIT_EXTENSION = "." + COMMIT_ACTION;
   String DELTA_COMMIT_EXTENSION = "." + DELTA_COMMIT_ACTION;

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/util/WriteStatMerger.java
Patch:
@@ -45,7 +45,7 @@ private static HoodieDeltaWriteStat mergeDeltaWriteStat(
     HoodieDeltaWriteStat merged = new HoodieDeltaWriteStat();
     mergeWriteStat(merged, stat1, stat2);
     merged.setLogVersion(stat2.getLogVersion());
-    merged.setLogOffset(stat2.getLogOffset());
+    merged.setLogOffset(maxLong(stat1.getLogOffset(), stat2.getLogOffset()));
     merged.setBaseFile(stat2.getBaseFile());
     // log files
     List<String> mergedLogFiles = new ArrayList<>(stat1.getLogFiles());

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java
Patch:
@@ -554,6 +554,7 @@ public RecordReader getRecordReader(InputSplit split, JobConf job, Reporter repo
     if (inputFormatClass.getName().equals(getParquetRealtimeInputFormatClassName())) {
       HoodieCombineFileInputFormatShim shims = createInputFormatShim();
       IOContextMap.get(job).setInputPath(((CombineHiveInputSplit) split).getPath(0));
+      job.set("hudi.hive.realtime","true");
       return shims.getRecordReader(job, ((CombineHiveInputSplit) split).getInputSplitShim(),
           reporter, CombineHiveRecordReader.class);
     } else {

File: hudi-common/src/main/java/org/apache/hudi/common/model/OverwriteNonDefaultsWithLatestAvroPayload.java
Patch:
@@ -33,8 +33,8 @@
  *
  * <ol>
  * <li>preCombine - Picks the latest delta record for a key, based on an ordering field;
- * <li>combineAndGetUpdateValue/getInsertValue - overwrite storage for specified fields
- * that doesn't equal defaultValue.
+ * <li>combineAndGetUpdateValue/getInsertValue - overwrite the storage for the specified fields
+ * with the fields from the latest delta record that doesn't equal defaultValue.
  * </ol>
  */
 public class OverwriteNonDefaultsWithLatestAvroPayload extends OverwriteWithLatestAvroPayload {

File: hudi-common/src/main/java/org/apache/hudi/common/config/ConfigGroups.java
Patch:
@@ -100,7 +100,7 @@ public static String getDescription(Names names) {
         break;
       case ENVIRONMENT_CONFIG:
         description = "Hudi supports passing configurations via a configuration file "
-            + "`hudi-default.conf` in which each line consists of a key and a value "
+            + "`hudi-defaults.conf` in which each line consists of a key and a value "
             + "separated by whitespace or = sign. For example:\n"
             + "```\n"
             + "hoodie.datasource.hive_sync.mode               jdbc\n"

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java
Patch:
@@ -104,7 +104,7 @@ public static SparkConf getSparkConfForTest(String appName) {
       sparkConf.set("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension");
     }
 
-    if (canLoadClass("org.apache.spark.sql.hudi.catalog.HoodieCatalog") && HoodieSparkUtils.gteqSpark3_2()) {
+    if (canLoadClass("org.apache.spark.sql.hudi.catalog.HoodieCatalog") && HoodieSparkUtils.gteqSpark3_3()) {
       sparkConf.set("spark.sql.catalog.spark_catalog",
           "org.apache.spark.sql.hudi.catalog.HoodieCatalog");
     }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/SparkClientFunctionalTestHarness.java
Patch:
@@ -117,7 +117,7 @@ public static Map<String, String> getSparkSqlConf() {
     Map<String, String> sqlConf = new HashMap<>();
     sqlConf.put("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension");
 
-    if (HoodieSparkUtils.gteqSpark3_2()) {
+    if (HoodieSparkUtils.gteqSpark3_3()) {
       sqlConf.put("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.hudi.catalog.HoodieCatalog");
     }
 

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestHiveTableSchemaEvolution.java
Patch:
@@ -92,7 +92,7 @@ private void initSparkContexts(String appName) {
   @ParameterizedTest
   @ValueSource(strings = {"cow", "mor"})
   public void testHiveReadSchemaEvolutionTable(String tableType) throws Exception {
-    if (HoodieSparkUtils.gteqSpark3_1()) {
+    if (HoodieSparkUtils.gteqSpark3_3()) {
       String tableName = "hudi_test" + new Date().getTime();
       String path = new Path(basePath.toAbsolutePath().toString()).toUri().toString();
 

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestOrcBootstrap.java
Patch:
@@ -191,7 +191,7 @@ private void testBootstrapCommon(boolean partitioned, boolean deltaCommit, Effec
   private void testBootstrapCommon(boolean partitioned, boolean deltaCommit, EffectiveMode mode, BootstrapMode modeForRegexMatch) throws Exception {
     // NOTE: Hudi doesn't support Orc in Spark < 3.0
     //       Please check HUDI-4496 for more details
-    if (!HoodieSparkUtils.gteqSpark3_0()) {
+    if (!HoodieSparkUtils.gteqSpark3_3()) {
       return;
     }
     String keyGeneratorClass = partitioned ? SimpleKeyGenerator.class.getCanonicalName()

File: hudi-spark-datasource/hudi-spark3-common/src/main/java/org/apache/spark/sql/execution/datasources/parquet/Spark3HoodieVectorizedParquetRecordReader.java
Patch:
@@ -36,7 +36,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
-public class Spark32PlusHoodieVectorizedParquetRecordReader extends VectorizedParquetRecordReader {
+public class Spark3HoodieVectorizedParquetRecordReader extends VectorizedParquetRecordReader {
 
   // save the col type change info.
   private Map<Integer, Pair<DataType, DataType>> typeChangeInfos;
@@ -63,7 +63,7 @@ public class Spark32PlusHoodieVectorizedParquetRecordReader extends VectorizedPa
   private int batchIdx = 0;
   private int numBatched = 0;
 
-  public Spark32PlusHoodieVectorizedParquetRecordReader(
+  public Spark3HoodieVectorizedParquetRecordReader(
       ZoneId convertTz,
       String datetimeRebaseMode,
       String datetimeRebaseTz,

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/SchedulerConfGenerator.java
Patch:
@@ -135,7 +135,7 @@ private static String generateAndStoreConfig(Integer deltaSyncWeight, Integer co
       bw.write(generateConfig(deltaSyncWeight, compactionWeight, deltaSyncMinShare, compactionMinShare, clusteringWeight, clusteringMinShare));
     }
     // SPARK-35083 introduces remote scheduler pool files, so the file must include scheme since Spark 3.2
-    String path = HoodieSparkUtils.gteqSpark3_2() ? tempConfigFile.toURI().toString() : tempConfigFile.getAbsolutePath();
+    String path = HoodieSparkUtils.gteqSpark3_3() ? tempConfigFile.toURI().toString() : tempConfigFile.getAbsolutePath();
     LOG.info("Configs written to file " + path);
     return path;
   }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -2231,7 +2231,7 @@ public void testParquetDFSSourceWithSchemaFilesAndTransformer() throws Exception
   public void testORCDFSSourceWithoutSchemaProviderAndNoTransformer() throws Exception {
     // NOTE: Hudi doesn't support Orc in Spark < 3.0
     //       Please check HUDI-4496 for more details
-    if (HoodieSparkUtils$.MODULE$.gteqSpark3_0()) {
+    if (HoodieSparkUtils$.MODULE$.gteqSpark3_3()) {
       testORCDFSSource(false, null);
     }
   }
@@ -2241,7 +2241,7 @@ public void testORCDFSSourceWithoutSchemaProviderAndNoTransformer() throws Excep
   public void testORCDFSSourceWithSchemaProviderAndWithTransformer() throws Exception {
     // NOTE: Hudi doesn't support Orc in Spark < 3.0
     //       Please check HUDI-4496 for more details
-    if (HoodieSparkUtils$.MODULE$.gteqSpark3_0()) {
+    if (HoodieSparkUtils$.MODULE$.gteqSpark3_3()) {
       testORCDFSSource(true, Collections.singletonList(TripsWithDistanceTransformer.class.getName()));
     }
   }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestSchedulerConfGenerator.java
Patch:
@@ -91,7 +91,7 @@ public void testGeneratedConfigFileScheme() throws Exception {
     Map<String, String> configs = SchedulerConfGenerator.getSparkSchedulingConfigs(cfg);
 
     URI schedulerFile = URI.create(configs.get(SparkConfigs.SPARK_SCHEDULER_ALLOCATION_FILE_KEY()));
-    if (HoodieSparkUtils.gteqSpark3_2()) {
+    if (HoodieSparkUtils.gteqSpark3_3()) {
       assertNotNull(schedulerFile.getScheme());
     } else {
       assertNull(schedulerFile.getScheme());

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieSnapshotExporter.java
Patch:
@@ -267,7 +267,7 @@ public class TestHoodieSnapshotExporterForNonHudi {
     private HoodieSnapshotExporter.Config createConfig(String format) {
       // NOTE: Hudi doesn't support Orc in Spark < 3.0
       //       Please check HUDI-4496 for more details
-      if ("orc".equals(format) && !HoodieSparkUtils.gteqSpark3_0()) {
+      if ("orc".equals(format) && !HoodieSparkUtils.gteqSpark3_3()) {
         return null;
       }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java
Patch:
@@ -53,6 +53,7 @@
 import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;
 import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.EXTRA_TYPE_SCHEMA;
 import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.FARE_NESTED_SCHEMA;
+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.HOODIE_IS_DELETED_SCHEMA;
 import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.MAP_TYPE_SCHEMA;
 import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.TIP_NESTED_SCHEMA;
 import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;
@@ -74,7 +75,7 @@ public class TestTableSchemaEvolution extends HoodieClientTestBase {
   public static final String EXTRA_FIELD_WITHOUT_DEFAULT_SCHEMA =
       "{\"name\": \"new_field_without_default\", \"type\": \"boolean\"},";
   public static final String EXTRA_FIELD_NULLABLE_SCHEMA =
-      ",{\"name\": \"new_field_without_default\", \"type\": [\"boolean\", \"null\"]}";
+      ",{\"name\": \"new_nullable_field\", \"type\": [\"null\", \"boolean\"], \"default\": null} ]}";
 
   // TRIP_EXAMPLE_SCHEMA with a new_field added
   public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED_COL_ADDED = TRIP_SCHEMA_PREFIX + EXTRA_TYPE_SCHEMA + MAP_TYPE_SCHEMA
@@ -154,7 +155,7 @@ public void testSchemaCompatibilityBasic() {
         "Added field without default and not nullable is not compatible (Evolved Schema)");
 
     assertTrue(isSchemaCompatible(TRIP_EXAMPLE_SCHEMA, TRIP_SCHEMA_PREFIX + EXTRA_TYPE_SCHEMA + MAP_TYPE_SCHEMA
-            + FARE_NESTED_SCHEMA + TIP_NESTED_SCHEMA + TRIP_SCHEMA_SUFFIX + EXTRA_FIELD_NULLABLE_SCHEMA, false),
+            + FARE_NESTED_SCHEMA + TIP_NESTED_SCHEMA + HOODIE_IS_DELETED_SCHEMA + EXTRA_FIELD_NULLABLE_SCHEMA, false),
         "Added nullable field is compatible (Evolved Schema)");
   }
 

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java
Patch:
@@ -121,7 +121,8 @@ public class HoodieTestDataGenerator implements AutoCloseable {
       + "{\"name\": \"rider\", \"type\": \"string\"}," + "{\"name\": \"driver\", \"type\": \"string\"},"
       + "{\"name\": \"begin_lat\", \"type\": \"double\"}," + "{\"name\": \"begin_lon\", \"type\": \"double\"},"
       + "{\"name\": \"end_lat\", \"type\": \"double\"}," + "{\"name\": \"end_lon\", \"type\": \"double\"},";
-  public static final String TRIP_SCHEMA_SUFFIX = "{\"name\": \"_hoodie_is_deleted\", \"type\": \"boolean\", \"default\": false} ]}";
+  public static final String HOODIE_IS_DELETED_SCHEMA = "{\"name\": \"_hoodie_is_deleted\", \"type\": \"boolean\", \"default\": false}";
+  public static final String TRIP_SCHEMA_SUFFIX = HOODIE_IS_DELETED_SCHEMA + " ]}";
   public static final String FARE_NESTED_SCHEMA = "{\"name\": \"fare\",\"type\": {\"type\":\"record\", \"name\":\"fare\",\"fields\": ["
       + "{\"name\": \"amount\",\"type\": \"double\"},{\"name\": \"currency\", \"type\": \"string\"}]}},";
   public static final String FARE_FLATTENED_SCHEMA = "{\"name\": \"fare\", \"type\": \"double\"},"

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestSparkSampleWritesUtils.java
Patch:
@@ -35,6 +35,7 @@
 import org.apache.spark.api.java.JavaRDD;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 
 import java.io.IOException;
@@ -85,6 +86,7 @@ public void skipOverwriteRecordSizeEstimateWhenTimelineNonEmpty() throws Excepti
     assertEquals(originalRecordSize, originalWriteConfig.getCopyOnWriteRecordSizeEstimate(), "Original record size estimate should not be changed.");
   }
 
+  @Disabled("HUDI-8082")
   @Test
   public void overwriteRecordSizeEstimateForEmptyTable() {
     int originalRecordSize = 100;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/RunCompactionActionExecutor.java
Patch:
@@ -125,6 +125,7 @@ public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
       compactionMetadata.setWriteStatuses(statuses);
       compactionMetadata.setCommitted(false);
       compactionMetadata.setCommitMetadata(Option.of(metadata));
+      compactionMetadata.setWriteStats(updateStatusMap);
     } catch (Exception e) {
       throw new HoodieCompactionException("Could not compact " + config.getBasePath(), e);
     }

File: hudi-aws/src/test/java/org/apache/hudi/aws/sync/ITTestGluePartitionPushdown.java
Patch:
@@ -72,7 +72,8 @@ public class ITTestGluePartitionPushdown {
   private Column[] partitionsColumn = {Column.builder().name("part1").type("int").build(), Column.builder().name("part2").type("string").build()};
   List<FieldSchema> partitionsFieldSchema = Arrays.asList(new FieldSchema("part1", "int"), new FieldSchema("part2", "string"));
 
-  public ITTestGluePartitionPushdown() throws IOException {}
+  public ITTestGluePartitionPushdown() throws IOException {
+  }
 
   @BeforeEach
   public void setUp() throws Exception {

File: hudi-cli/src/main/java/org/apache/hudi/cli/HoodiePrintHelper.java
Patch:
@@ -29,7 +29,6 @@
 import java.util.function.Function;
 import java.util.stream.Collectors;
 
-
 /**
  * Helper class to render table for hoodie-cli.
  */

File: hudi-cli/src/test/java/org/apache/hudi/cli/testutils/ShellEvaluationResultUtil.java
Patch:
@@ -23,7 +23,9 @@
 
 public class ShellEvaluationResultUtil {
   private static final Logger LOGGER = LoggerFactory.getLogger(ShellEvaluationResultUtil.class);
-  private ShellEvaluationResultUtil() {}
+
+  private ShellEvaluationResultUtil() {
+  }
 
   public static boolean isSuccess(Object shellEvaluationResult) {
     boolean hasError = shellEvaluationResult instanceof Throwable;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/CompactionAdminClient.java
Patch:
@@ -327,7 +327,8 @@ private List<RenameOpResult> runRenamingOps(HoodieTableMetaClient metaClient,
    */
   public static class RenameOpResult extends OperationResult<RenameInfo> {
 
-    public RenameOpResult() {}
+    public RenameOpResult() {
+    }
 
     public RenameOpResult(Pair<HoodieLogFile, HoodieLogFile> op, boolean success, Option<Exception> exception) {
       super(

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/ZookeeperBasedLockProvider.java
Patch:
@@ -115,8 +115,7 @@ private void createNodeIfNotExists(String path) throws Exception {
       }
     }
   }
-
-
+  
   // Only used for testing
   public ZookeeperBasedLockProvider(
       final LockConfiguration lockConfiguration, final CuratorFramework curatorFrameworkClient) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/OperationResult.java
Patch:
@@ -33,7 +33,8 @@ public class OperationResult<T> implements Serializable {
   private boolean success;
   private Option<Exception> exception;
 
-  public OperationResult() {}
+  public OperationResult() {
+  }
 
   public OperationResult(T operation, boolean success, Option<Exception> exception) {
     this.operation = operation;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/EightToSevenDowngradeHandler.java
Patch:
@@ -40,7 +40,6 @@
 
 import static org.apache.hudi.common.table.timeline.HoodieInstant.UNDERSCORE;
 
-
 /**
  * Version 7 is going to be placeholder version for bridge release 0.16.0.
  * Version 8 is the placeholder version to track 1.x.

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/table/action/commit/TestWriterHelperBase.java
Patch:
@@ -30,7 +30,6 @@
 import java.io.IOException;
 import java.util.List;
 
-
 /**
  * Tests for write helpers
  */

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/upgrade/JavaUpgradeDowngradeHelper.java
Patch:
@@ -35,7 +35,8 @@ public class JavaUpgradeDowngradeHelper implements SupportsUpgradeDowngrade {
   private static final JavaUpgradeDowngradeHelper SINGLETON_INSTANCE =
       new JavaUpgradeDowngradeHelper();
 
-  private JavaUpgradeDowngradeHelper() {}
+  private JavaUpgradeDowngradeHelper() {
+  }
 
   public static JavaUpgradeDowngradeHelper getInstance() {
     return SINGLETON_INSTANCE;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndexHelper.java
Patch:
@@ -74,7 +74,8 @@ public class SparkHoodieBloomIndexHelper extends BaseHoodieBloomIndexHelper {
   private static final SparkHoodieBloomIndexHelper SINGLETON_INSTANCE =
       new SparkHoodieBloomIndexHelper();
 
-  private SparkHoodieBloomIndexHelper() {}
+  private SparkHoodieBloomIndexHelper() {
+  }
 
   public static SparkHoodieBloomIndexHelper getInstance() {
     return SINGLETON_INSTANCE;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BulkInsertDataInternalWriterHelper.java
Patch:
@@ -166,7 +166,8 @@ public List<WriteStatus> getWriteStatuses() throws IOException {
     return writeStatusList;
   }
 
-  public void abort() {}
+  public void abort() {
+  }
 
   public void close() throws IOException {
     for (HoodieRowCreateHandle rowCreateHandle : handles.values()) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/common/fs/NonSerializableFileSystem.java
Patch:
@@ -92,7 +92,8 @@ public long getLen() {
   }
 
   @Override
-  public void setWorkingDirectory(Path path) {}
+  public void setWorkingDirectory(Path path) {
+  }
 
   @Override
   public Path getWorkingDirectory() {

File: hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaUtils.java
Patch:
@@ -50,7 +50,8 @@ public class AvroSchemaUtils {
 
   private static final Logger LOG = LoggerFactory.getLogger(AvroSchemaUtils.class);
 
-  private AvroSchemaUtils() {}
+  private AvroSchemaUtils() {
+  }
 
   /**
    * See {@link #isSchemaCompatible(Schema, Schema, boolean, boolean)} doc for more details

File: hudi-common/src/main/java/org/apache/hudi/avro/GenericAvroSerializer.java
Patch:
@@ -40,7 +40,6 @@
 import static org.apache.hudi.common.util.StringUtils.fromUTF8Bytes;
 import static org.apache.hudi.common.util.StringUtils.getUTF8Bytes;
 
-
 /**
  * Custom serializer used for generic Avro containers.
  * <p>

File: hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java
Patch:
@@ -77,7 +77,6 @@ public MercifulJsonConverter() {
     this(false, "__");
   }
 
-
   /**
    * Allows enabling sanitization and allows choice of invalidCharMask for sanitization
    */

File: hudi-common/src/main/java/org/apache/hudi/common/model/ClusteringGroupInfo.java
Patch:
@@ -43,7 +43,8 @@ public static ClusteringGroupInfo create(HoodieClusteringGroup clusteringGroup)
   
   // Only for serialization/de-serialization
   @Deprecated
-  public ClusteringGroupInfo() {}
+  public ClusteringGroupInfo() {
+  }
   
   private ClusteringGroupInfo(final List<ClusteringOperation> operations, final int numOutputGroups) {
     this.operations = operations;

File: hudi-common/src/main/java/org/apache/hudi/common/model/ClusteringOperation.java
Patch:
@@ -45,7 +45,8 @@ public static ClusteringOperation create(HoodieSliceInfo sliceInfo) {
 
   // Only for serialization/de-serialization
   @Deprecated
-  public ClusteringOperation() {}
+  public ClusteringOperation() {
+  }
 
   private ClusteringOperation(final String dataFilePath, final List<String> deltaFilePaths, final String fileId,
                              final String partitionPath, final String bootstrapFilePath, final int version) {

File: hudi-common/src/main/java/org/apache/hudi/common/model/CompactionOperation.java
Patch:
@@ -47,7 +47,8 @@ public class CompactionOperation implements Serializable {
 
   // Only for serialization/de-serialization
   @Deprecated
-  public CompactionOperation() {}
+  public CompactionOperation() {
+  }
 
   public CompactionOperation(String fileId, String partitionPath, String baseInstantTime,
                              Option<String> dataFileCommitTime, List<String> deltaFileNames, Option<String> dataFileName,

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieKey.java
Patch:
@@ -42,7 +42,8 @@ public class HoodieKey implements Serializable {
   private String partitionPath;
 
   // Required for serializer
-  public HoodieKey() {}
+  public HoodieKey() {
+  }
 
   public HoodieKey(String recordKey, String partitionPath) {
     this.recordKey = recordKey;

File: hudi-common/src/main/java/org/apache/hudi/common/model/MetadataValues.java
Patch:
@@ -31,7 +31,8 @@ public class MetadataValues {
 
   private boolean set = false;
 
-  public MetadataValues() {}
+  public MetadataValues() {
+  }
 
   public String getCommitTime() {
     return commitTime;

File: hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java
Patch:
@@ -136,7 +136,7 @@ public PartialUpdateAvroPayload preCombine(OverwriteWithLatestAvroPayload oldVal
       return this;
     }
     // pick the payload with greater ordering value as insert record
-    final boolean shouldPickOldRecord = oldValue.orderingVal.compareTo(orderingVal) > 0 ? true : false;
+    final boolean shouldPickOldRecord = oldValue.orderingVal.compareTo(orderingVal) > 0;
     try {
       GenericRecord oldRecord = HoodieAvroUtils.bytesToAvro(oldValue.recordBytes, schema);
       Option<IndexedRecord> mergedRecord = mergeOldRecord(oldRecord, schema, shouldPickOldRecord, true);

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -801,8 +801,8 @@ private static HoodieTableMetaClient newMetaClient(HoodieStorage storage, String
                                                      String payloadClassName, String recordMergerStrategy,
                                                      HoodieTimeGeneratorConfig timeGeneratorConfig, FileSystemRetryConfig fileSystemRetryConfig,
                                                      HoodieMetaserverConfig metaserverConfig) {
-    return metaserverConfig.isMetaserverEnabled()
-        ? (HoodieTableMetaClient) ReflectionUtils.loadClass("org.apache.hudi.common.table.HoodieTableMetaserverClient",
+    return metaserverConfig.isMetaserverEnabled() ? (HoodieTableMetaClient) ReflectionUtils.loadClass(
+        "org.apache.hudi.common.table.HoodieTableMetaserverClient",
         new Class<?>[] {HoodieStorage.class, String.class, ConsistencyGuardConfig.class, String.class, HoodieTimeGeneratorConfig.class,
             FileSystemRetryConfig.class, Option.class, Option.class, HoodieMetaserverConfig.class},
         storage, basePath, consistencyGuardConfig, recordMergerStrategy, timeGeneratorConfig, fileSystemRetryConfig,

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatReader.java
Patch:
@@ -108,7 +108,8 @@ public HoodieLogFile getLogFile() {
   }
 
   @Override
-  public void remove() {}
+  public void remove() {
+  }
 
   @Override
   public boolean hasPrev() {

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java
Patch:
@@ -429,8 +429,7 @@ public interface HoodieTimeline extends Serializable {
    *
    */
   public Option<HoodieInstant> getLastPendingClusterInstant();
-
-
+  
   /**
    * get the least recent pending cluster commit if present
    */

File: hudi-common/src/main/java/org/apache/hudi/common/util/HoodieTableConfigUtils.java
Patch:
@@ -75,8 +75,7 @@ public static String getPartitionFieldWithoutKeyGenPartitionType(String partitio
         ? partitionField.split(BaseKeyGenerator.CUSTOM_KEY_GENERATOR_SPLIT_REGEX)[0]
         : partitionField;
   }
-
-
+  
   /**
    * This function returns the hoodie.table.version from hoodie.properties file.
    */

File: hudi-common/src/main/java/org/apache/hudi/common/util/TypeUtils.java
Patch:
@@ -29,7 +29,8 @@
  */
 public final class TypeUtils {
 
-  private TypeUtils() {}
+  private TypeUtils() {
+  }
 
   /**
    * Maps values from the provided Enum's {@link Class} into corresponding values,

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/BaseHoodieQueueBasedExecutor.java
Patch:
@@ -98,7 +98,8 @@ protected void doProduce(HoodieMessageQueue<I, O> queue, HoodieProducer<I> produ
 
   protected abstract void doConsume(HoodieMessageQueue<I, O> queue, HoodieConsumer<O, E> consumer);
 
-  protected void setUp() {}
+  protected void setUp() {
+  }
 
   /**
    * Start producing

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/DisruptorExecutor.java
Patch:
@@ -60,5 +60,6 @@ protected void setUp() {
   }
 
   @Override
-  protected void doConsume(HoodieMessageQueue<I, O> queue, HoodieConsumer<O, E> consumer) {}
+  protected void doConsume(HoodieMessageQueue<I, O> queue, HoodieConsumer<O, E> consumer) {
+  }
 }

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/DisruptorMessageQueue.java
Patch:
@@ -106,7 +106,8 @@ public boolean isEmpty() {
   }
 
   @Override
-  public void seal() {}
+  public void seal() {
+  }
 
   @Override
   public void close() {

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/FunctionBasedQueueProducer.java
Patch:
@@ -46,5 +46,7 @@ public void produce(HoodieMessageQueue<I, ?> queue) {
   }
 
   @Override
-  public void close() { /* no-op */ }
+  public void close() {
+    /* no-op */
+  }
 }

File: hudi-common/src/main/java/org/apache/hudi/exception/ExceptionUtil.java
Patch:
@@ -24,7 +24,8 @@
  * Class collecting common utilities helping in handling {@link Exception}s
  */
 public final class ExceptionUtil {
-  private ExceptionUtil() {}
+  private ExceptionUtil() {
+  }
 
   /**
    * Fetches inner-most cause of the provided {@link Throwable}

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestPartialUpdateAvroPayload.java
Patch:
@@ -38,7 +38,6 @@
 import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
-
 /**
  * Unit tests {@link TestPartialUpdateAvroPayload}.
  */

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java
Patch:
@@ -380,7 +380,6 @@ public GenericRecord generateGenericRecord(String rowKey, String partitionPath,
     return generateGenericRecord(rowKey, partitionPath, riderName, driverName, timestamp, false, false);
   }
 
-
   /**
    * Populate rec with values for TRIP_SCHEMA_PREFIX
    */
@@ -468,8 +467,7 @@ private void generateTripSuffixValues(GenericRecord rec, boolean isDeleteRecord)
       rec.put("_hoodie_is_deleted", false);
     }
   }
-
-
+  
   /**
    * Generate record conforming to TRIP_EXAMPLE_SCHEMA or TRIP_FLATTENED_SCHEMA if isFlattened is true
    */

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/SchemaTestUtil.java
Patch:
@@ -71,7 +71,8 @@ public final class SchemaTestUtil {
 
   private final Random random = new Random(0xDEED);
 
-  public SchemaTestUtil() {}
+  public SchemaTestUtil() {
+  }
 
   public static Schema getSimpleSchema() throws IOException {
     return new Schema.Parser().parse(SchemaTestUtil.class.getResourceAsStream("/simple-test.avsc"));

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/FileIndex.java
Patch:
@@ -283,8 +283,8 @@ public List<String> getOrBuildPartitionPaths() {
     if (this.partitionPaths != null) {
       return this.partitionPaths;
     }
-    List<String> allPartitionPaths = this.tableExists
-        ? FSUtils.getAllPartitionPaths(new HoodieFlinkEngineContext(hadoopConf),
+    List<String> allPartitionPaths = this.tableExists ? FSUtils.getAllPartitionPaths(
+        new HoodieFlinkEngineContext(hadoopConf),
         new HoodieHadoopStorage(path, HadoopFSUtils.getStorageConf(hadoopConf)), metadataConfig, path.toString())
         : Collections.emptyList();
     if (this.partitionPruner == null) {

File: hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/EmptyColumnReader.java
Patch:
@@ -32,7 +32,8 @@
  */
 public class EmptyColumnReader implements ColumnReader<WritableColumnVector> {
 
-  public EmptyColumnReader() {}
+  public EmptyColumnReader() {
+  }
 
   @Override
   public void readToVector(int readNumber, WritableColumnVector vector) throws IOException {

File: hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/EmptyColumnReader.java
Patch:
@@ -32,7 +32,8 @@
  */
 public class EmptyColumnReader implements ColumnReader<WritableColumnVector> {
 
-  public EmptyColumnReader() {}
+  public EmptyColumnReader() {
+  }
 
   @Override
   public void readToVector(int readNumber, WritableColumnVector vector) throws IOException {

File: hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/EmptyColumnReader.java
Patch:
@@ -32,7 +32,8 @@
  */
 public class EmptyColumnReader implements ColumnReader<WritableColumnVector> {
 
-  public EmptyColumnReader() {}
+  public EmptyColumnReader() {
+  }
 
   @Override
   public void readToVector(int readNumber, WritableColumnVector vector) throws IOException {

File: hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/EmptyColumnReader.java
Patch:
@@ -32,7 +32,8 @@
  */
 public class EmptyColumnReader implements ColumnReader<WritableColumnVector> {
 
-  public EmptyColumnReader() {}
+  public EmptyColumnReader() {
+  }
 
   @Override
   public void readToVector(int readNumber, WritableColumnVector vector) throws IOException {

File: hudi-flink-datasource/hudi-flink1.18.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/EmptyColumnReader.java
Patch:
@@ -32,7 +32,8 @@
  */
 public class EmptyColumnReader implements ColumnReader<WritableColumnVector> {
 
-  public EmptyColumnReader() {}
+  public EmptyColumnReader() {
+  }
 
   @Override
   public void readToVector(int readNumber, WritableColumnVector vector) throws IOException {

File: hudi-hadoop-common/src/main/java/org/apache/hudi/common/bootstrap/index/HFileBootstrapIndex.java
Patch:
@@ -31,6 +31,7 @@ public class HFileBootstrapIndex {
    * This class is explicitly used as Key Comparator to workaround hard coded
    * legacy format class names inside HBase. Otherwise we will face issues with shading.
    */
-  public static class HoodieKVComparator extends CellComparatorImpl {}
+  public static class HoodieKVComparator extends CellComparatorImpl {
+  }
 }
 

File: hudi-hadoop-common/src/main/java/org/apache/hudi/hadoop/fs/HoodieWrapperFileSystem.java
Patch:
@@ -136,7 +136,8 @@ protected static <R> R executeFuncWithTimeAndByteMetrics(String metricName, Path
     return executeFuncWithTimeMetrics(metricName, p, func);
   }
 
-  public HoodieWrapperFileSystem() {}
+  public HoodieWrapperFileSystem() {
+  }
 
   public HoodieWrapperFileSystem(FileSystem fileSystem, ConsistencyGuard consistencyGuard) {
     this.fileSystem = fileSystem;

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/table/view/TestHoodieTableFileSystemView.java
Patch:
@@ -2344,8 +2344,7 @@ public void testHoodieTableFileSystemViewWithPendingClustering() throws IOExcept
     assertTrue(latestBaseFilesPerPartition.contains(fileId2));
     assertTrue(latestBaseFilesPerPartition.contains(fileId4));
   }
-
-
+  
   // Generate Hoodie WriteStat For Given Partition
   private List<HoodieWriteStat> buildWriteStats(HashMap<String, List<String>> partitionToFileIds, String commitTime) {
     HashMap<String, List<Pair<String, Integer>>> maps = new HashMap<>();

File: hudi-hadoop-common/src/test/java/org/apache/hudi/io/hadoop/TestHoodieHadoopIOFactory.java
Patch:
@@ -38,7 +38,6 @@
 import static org.junit.jupiter.api.Assertions.assertThrows;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
-
 /**
  * Tests {@link HoodieHadoopIOFactory}
  */

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/BootstrapBaseFileSplit.java
Patch:
@@ -35,7 +35,8 @@ public class BootstrapBaseFileSplit extends FileSplit {
    * NOTE: This ctor is necessary for Hive to be able to serialize and
    *       then instantiate it when deserializing back
    */
-  public BootstrapBaseFileSplit() {}
+  public BootstrapBaseFileSplit() {
+  }
 
   public BootstrapBaseFileSplit(FileSplit baseSplit, FileSplit bootstrapFileSplit)
       throws IOException {

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieRealtimeBootstrapBaseFileSplit.java
Patch:
@@ -63,7 +63,8 @@ public class HoodieRealtimeBootstrapBaseFileSplit extends BootstrapBaseFileSplit
    * NOTE: This ctor is necessary for Hive to be able to serialize and
    *       then instantiate it when deserializing back
    */
-  public HoodieRealtimeBootstrapBaseFileSplit() {}
+  public HoodieRealtimeBootstrapBaseFileSplit() {
+  }
 
   public HoodieRealtimeBootstrapBaseFileSplit(FileSplit baseSplit,
                                               String basePath,

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/KafkaConnectWriterProvider.java
Patch:
@@ -72,9 +72,9 @@ public KafkaConnectWriterProvider(
         KafkaConnectUtils.getDefaultStorageConf(connectConfigs);
 
     try {
-      this.schemaProvider = StringUtils.isNullOrEmpty(connectConfigs.getSchemaProviderClass()) ? null
-          : (SchemaProvider) ReflectionUtils.loadClass(connectConfigs.getSchemaProviderClass(),
-          new TypedProperties(connectConfigs.getProps()));
+      this.schemaProvider = StringUtils.isNullOrEmpty(connectConfigs.getSchemaProviderClass()) ? null :
+          (SchemaProvider) ReflectionUtils.loadClass(
+              connectConfigs.getSchemaProviderClass(), new TypedProperties(connectConfigs.getProps()));
 
       this.keyGenerator = HoodieAvroKeyGeneratorFactory.createKeyGenerator(
           new TypedProperties(connectConfigs.getProps()));

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/util/PartitionFilterGenerator.java
Patch:
@@ -76,8 +76,7 @@ private Literal buildLiteralExpression(String fieldValue, String fieldType) {
         throw new IllegalArgumentException(String.format(UNSUPPORTED_TYPE_ERROR, fieldType));
     }
   }
-
-
+  
   /**
    * Build expression from the Partition list. Here we're trying to match all partitions.
    *

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deser/KafkaAvroSchemaDeserializer.java
Patch:
@@ -39,7 +39,8 @@ public class KafkaAvroSchemaDeserializer extends KafkaAvroDeserializer {
 
   private Schema sourceSchema;
 
-  public KafkaAvroSchemaDeserializer() {}
+  public KafkaAvroSchemaDeserializer() {
+  }
 
   public KafkaAvroSchemaDeserializer(SchemaRegistryClient client, Map<String, ?> props) {
     super(client, props);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/HoodieStreamerUtils.java
Patch:
@@ -64,7 +64,6 @@
 import static org.apache.hudi.common.table.HoodieTableConfig.DROP_PARTITION_COLUMNS;
 import static org.apache.hudi.config.HoodieErrorTableConfig.ERROR_ENABLE_VALIDATE_RECORD_CREATION;
 
-
 /**
  * Util class for HoodieStreamer.
  */

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/testutils/UtilitiesTestBase.java
Patch:
@@ -310,9 +310,9 @@ private static void clearHiveDb(String tempWriteablePath) throws Exception {
     HiveSyncConfig hiveSyncConfig = getHiveSyncConfig(tempWriteablePath, "dummy");
     hiveSyncConfig.setHadoopConf(hiveTestService.getHiveConf());
     HoodieTableMetaClient.withPropertyBuilder()
-      .setTableType(HoodieTableType.COPY_ON_WRITE)
-      .setTableName(hiveSyncConfig.getString(META_SYNC_TABLE_NAME))
-      .initTable(storage.getConf().newInstance(), hiveSyncConfig.getString(META_SYNC_BASE_PATH));
+        .setTableType(HoodieTableType.COPY_ON_WRITE)
+        .setTableName(hiveSyncConfig.getString(META_SYNC_TABLE_NAME))
+        .initTable(storage.getConf().newInstance(), hiveSyncConfig.getString(META_SYNC_BASE_PATH));
 
     QueryBasedDDLExecutor ddlExecutor = new JDBCExecutor(hiveSyncConfig);
     ddlExecutor.runSQL("drop database if exists " + hiveSyncConfig.getString(META_SYNC_DATABASE_NAME));

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java
Patch:
@@ -215,7 +215,7 @@ public IndexedRecord next() {
         this.dis.skipBytes(recordLength);
         this.readRecords++;
         if (this.promotedSchema.isPresent()) {
-          return  HoodieAvroUtils.rewriteRecordWithNewSchema(record, this.promotedSchema.get());
+          return HoodieAvroUtils.rewriteRecordWithNewSchema(record, this.promotedSchema.get());
         }
         return record;
       } catch (IOException e) {

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamerSchemaEvolutionExtensive.java
Patch:
@@ -195,7 +195,7 @@ protected void testBase(String updateFile, String updateColumn, Map<String,Integ
   protected static Stream<Arguments> testArgs() {
     Stream.Builder<Arguments> b = Stream.builder();
     //only testing row-writer enabled for now
-    for (Boolean rowWriterEnable : new Boolean[]{true}) {
+    for (Boolean rowWriterEnable : new Boolean[]{false, true}) {
       for (Boolean addFilegroups : new Boolean[]{false, true}) {
         for (Boolean multiLogFiles : new Boolean[]{false, true}) {
           for (Boolean shouldCluster : new Boolean[]{false, true}) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -229,7 +229,7 @@ public List<MetadataPartitionType> getEnabledPartitionTypes() {
   protected boolean initializeIfNeeded(HoodieTableMetaClient dataMetaClient,
                                        Option<String> inflightInstantTimestamp) throws IOException {
     HoodieTimer timer = HoodieTimer.start();
-    List<MetadataPartitionType> partitionsToInit = new ArrayList<>(MetadataPartitionType.values().length);
+    List<MetadataPartitionType> partitionsToInit = new ArrayList<>(MetadataPartitionType.getValidValues().length);
 
     try {
       boolean exists = metadataTableExists(dataMetaClient);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -1002,7 +1002,7 @@ public void maybeDeleteMetadataTable() {
    * Deletes the metadata partition if the writer disables any metadata index.
    */
   public void deleteMetadataIndexIfNecessary() {
-    Stream.of(MetadataPartitionType.values()).forEach(partitionType -> {
+    Stream.of(MetadataPartitionType.getValidValues()).forEach(partitionType -> {
       if (shouldDeleteMetadataPartition(partitionType)) {
         try {
           LOG.info("Deleting metadata partition because it is disabled in writer: " + partitionType.name());

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java
Patch:
@@ -544,10 +544,10 @@ public void testDowngradeSixToFiveShouldDeleteRecordIndexPartition() throws Exce
             .withMetadataIndexBloomFilter(true)
             .withEnableRecordIndex(true).build())
         .build();
-    for (MetadataPartitionType partitionType : MetadataPartitionType.values()) {
+    for (MetadataPartitionType partitionType : MetadataPartitionType.getValidValues()) {
       metaClient.getTableConfig().setMetadataPartitionState(metaClient, partitionType.getPartitionPath(), true);
     }
-    metaClient.getTableConfig().setMetadataPartitionsInflight(metaClient, MetadataPartitionType.values());
+    metaClient.getTableConfig().setMetadataPartitionsInflight(metaClient, MetadataPartitionType.getValidValues());
     String metadataTableBasePath = Paths.get(basePath, METADATA_TABLE_FOLDER_PATH).toString();
     HoodieTableMetaClient metadataTableMetaClient = HoodieTestUtils.init(metadataTableBasePath, MERGE_ON_READ);
     HoodieMetadataTestTable.of(metadataTableMetaClient)

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -750,7 +750,7 @@ public HoodieTableFileSystemView getMetadataFileSystemView() {
   }
 
   public Map<String, String> stats() {
-    Set<String> allMetadataPartitionPaths = Arrays.stream(MetadataPartitionType.values()).map(MetadataPartitionType::getPartitionPath).collect(Collectors.toSet());
+    Set<String> allMetadataPartitionPaths = Arrays.stream(MetadataPartitionType.getValidValues()).map(MetadataPartitionType::getPartitionPath).collect(Collectors.toSet());
     return metrics.map(m -> m.getStats(true, metadataMetaClient, this, allMetadataPartitionPaths)).orElseGet(HashMap::new);
   }
 

File: hudi-aws/src/main/java/org/apache/hudi/config/GlueCatalogSyncClientConfig.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.common.config.ConfigGroups;
 import org.apache.hudi.common.config.ConfigProperty;
 import org.apache.hudi.common.config.HoodieConfig;
-import org.apache.hudi.common.table.HoodieTableConfig;
+import org.apache.hudi.common.util.HoodieTableConfigUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 
@@ -83,7 +83,7 @@ public class GlueCatalogSyncClientConfig extends HoodieConfig {
   public static final ConfigProperty<String> META_SYNC_PARTITION_INDEX_FIELDS = ConfigProperty
       .key(GLUE_CLIENT_PROPERTY_PREFIX + "partition_index_fields")
       .noDefaultValue()
-      .withInferFunction(cfg -> Option.ofNullable(cfg.getString(HoodieTableConfig.PARTITION_FIELDS))
+      .withInferFunction(cfg -> HoodieTableConfigUtils.getPartitionFieldProp(cfg)
           .or(() -> Option.ofNullable(cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME))))
       .sinceVersion("0.15.0")
       .withDocumentation(String.join(" ", "Specify the partitions fields to index on aws glue. Separate the fields by semicolon.",

File: hudi-common/src/main/java/org/apache/hudi/keygen/BaseKeyGenerator.java
Patch:
@@ -33,6 +33,8 @@
 public abstract class BaseKeyGenerator extends KeyGenerator {
 
   public static final String EMPTY_PARTITION = "";
+  public static final String CUSTOM_KEY_GENERATOR_SPLIT_REGEX = ":";
+  public static final String FIELD_SEPARATOR = ",";
   protected List<String> recordKeyFields;
   protected List<String> partitionPathFields;
   protected final boolean encodePartitionPath;

File: hudi-gcp/src/main/java/org/apache/hudi/gcp/bigquery/BigQuerySyncConfig.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.common.config.ConfigProperty;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.config.TypedProperties;
-import org.apache.hudi.common.table.HoodieTableConfig;
+import org.apache.hudi.common.util.HoodieTableConfigUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
@@ -103,7 +103,7 @@ public class BigQuerySyncConfig extends HoodieSyncConfig implements Serializable
   public static final ConfigProperty<String> BIGQUERY_SYNC_PARTITION_FIELDS = ConfigProperty
       .key("hoodie.gcp.bigquery.sync.partition_fields")
       .noDefaultValue()
-      .withInferFunction(cfg -> Option.ofNullable(cfg.getString(HoodieTableConfig.PARTITION_FIELDS))
+      .withInferFunction(cfg -> HoodieTableConfigUtils.getPartitionFieldProp(cfg)
           .or(() -> Option.ofNullable(cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME))))
       .markAdvanced()
       .withDocumentation("Comma-delimited partition fields. Default to non-partitioned.");

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/KafkaConnectTransactionServices.java
Patch:
@@ -88,7 +88,7 @@ public KafkaConnectTransactionServices(KafkaConnectConfigs connectConfigs) throw
       KeyGenerator keyGenerator = HoodieAvroKeyGeneratorFactory.createAvroKeyGeneratorByType(
           new TypedProperties(connectConfigs.getProps()));
       String recordKeyFields = KafkaConnectUtils.getRecordKeyColumns(keyGenerator);
-      String partitionColumns = KafkaConnectUtils.getPartitionColumns(keyGenerator,
+      String partitionColumns = KafkaConnectUtils.getPartitionColumnsForKeyGenerator(keyGenerator,
           new TypedProperties(connectConfigs.getProps()));
 
       LOG.info(String.format("Setting record key %s and partition fields %s for table %s",

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/HoodieSyncConfig.java
Patch:
@@ -24,10 +24,10 @@
 import org.apache.hudi.common.config.HoodieConfig;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.config.TypedProperties;
-import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.util.HadoopConfigUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
+import org.apache.hudi.common.util.HoodieTableConfigUtils;
 import org.apache.hudi.config.metrics.HoodieMetricsConfig;
 import org.apache.hudi.hadoop.fs.HadoopFSUtils;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
@@ -109,7 +109,7 @@ public class HoodieSyncConfig extends HoodieConfig {
   public static final ConfigProperty<String> META_SYNC_PARTITION_FIELDS = ConfigProperty
       .key("hoodie.datasource.hive_sync.partition_fields")
       .defaultValue("")
-      .withInferFunction(cfg -> Option.ofNullable(cfg.getString(HoodieTableConfig.PARTITION_FIELDS))
+      .withInferFunction(cfg -> HoodieTableConfigUtils.getPartitionFieldProp(cfg)
           .or(() -> Option.ofNullable(cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME))))
       .markAdvanced()
       .withDocumentation("Field in the table to use for determining hive partition columns.");
@@ -122,7 +122,7 @@ public class HoodieSyncConfig extends HoodieConfig {
         if (StringUtils.nonEmpty(cfg.getString(META_SYNC_PARTITION_FIELDS))) {
           partitionFieldsOpt = Option.ofNullable(cfg.getString(META_SYNC_PARTITION_FIELDS));
         } else {
-          partitionFieldsOpt = Option.ofNullable(cfg.getString(HoodieTableConfig.PARTITION_FIELDS))
+          partitionFieldsOpt = HoodieTableConfigUtils.getPartitionFieldProp(cfg)
               .or(() -> Option.ofNullable(cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME)));
         }
         if (!partitionFieldsOpt.isPresent()) {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/BootstrapExecutor.java
Patch:
@@ -233,9 +233,9 @@ private void initializeTable() throws IOException {
         .setPartitionMetafileUseBaseFormat(props.getBoolean(
             PARTITION_METAFILE_USE_BASE_FORMAT.key(),
             PARTITION_METAFILE_USE_BASE_FORMAT.defaultValue()));
-    String partitionColumns = SparkKeyGenUtils.getPartitionColumns(props);
-    if (!StringUtils.isNullOrEmpty(partitionColumns)) {
-      builder.setPartitionFields(partitionColumns).setKeyGeneratorClassProp(
+    String partitionColumnsForKeyGenerator = SparkKeyGenUtils.getPartitionColumnsForKeyGenerator(props);
+    if (!StringUtils.isNullOrEmpty(partitionColumnsForKeyGenerator)) {
+      builder.setPartitionFields(partitionColumnsForKeyGenerator).setKeyGeneratorClassProp(
           props.getString(HoodieWriteConfig.KEYGENERATOR_CLASS_NAME.key(), SimpleKeyGenerator.class.getName()));
     } else {
       builder.setKeyGeneratorClassProp(props.getString(

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/StreamSync.java
Patch:
@@ -398,7 +398,7 @@ public void refreshTimeline() throws IOException {
   private void initializeEmptyTable() throws IOException {
     this.commitsTimelineOpt = Option.empty();
     this.allCommitsTimelineOpt = Option.empty();
-    String partitionColumns = SparkKeyGenUtils.getPartitionColumns(props);
+    String partitionColumns = SparkKeyGenUtils.getPartitionColumnsForKeyGenerator(props);
     HoodieTableMetaClient.withPropertyBuilder()
         .setTableType(cfg.tableType)
         .setTableName(cfg.targetTableName)

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/SimpleConcurrentFileWritesConflictResolutionStrategy.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.util.ClusteringUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieWriteConflictException;
@@ -62,6 +63,8 @@ public Stream<HoodieInstant> getCandidateInstants(HoodieTableMetaClient metaClie
 
     Stream<HoodieInstant> compactionAndClusteringPendingTimeline = activeTimeline
         .filterPendingReplaceClusteringAndCompactionTimeline()
+        .filter(instant -> ClusteringUtils.isClusteringInstant(activeTimeline, instant)
+            || HoodieTimeline.COMPACTION_ACTION.equals(instant.getAction()))
         .findInstantsAfter(currentInstant.getTimestamp())
         .getInstantsAsStream();
     return Stream.concat(completedCommitsInstantStream, compactionAndClusteringPendingTimeline);

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -641,7 +641,7 @@ public static void initializeBootstrapDirsIfNotExists(StoragePath basePath, Hood
     }
 
 
-    // Create bootstrap index by partition folder if it does not exist
+    // Create bootstrap index by file-id folder if it does not exist
     final StoragePath bootstrap_index_folder_by_fileids =
         new StoragePath(basePath, HoodieTableMetaClient.BOOTSTRAP_INDEX_BY_FILE_ID_FOLDER_PATH);
     if (!storage.exists(bootstrap_index_folder_by_fileids)) {

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java
Patch:
@@ -88,6 +88,7 @@ public AbstractRealtimeRecordReader(RealtimeSplit split, JobConf job) {
     try {
       metaClient = HoodieTableMetaClient.builder()
           .setConf(HadoopFSUtils.getStorageConfWithCopy(jobConf)).setBasePath(split.getBasePath()).build();
+      payloadProps.putAll(metaClient.getTableConfig().getProps(true));
       if (metaClient.getTableConfig().getPreCombineField() != null) {
         this.payloadProps.setProperty(HoodiePayloadProps.PAYLOAD_ORDERING_FIELD_PROP_KEY, metaClient.getTableConfig().getPreCombineField());
       }

File: hudi-aws/src/test/java/org/apache/hudi/aws/sync/TestAWSGlueSyncClient.java
Patch:
@@ -115,6 +115,7 @@ void testCreateOrReplaceTable_TableExists() {
     CompletableFuture<DeleteTableResponse> deleteTableResponse = CompletableFuture.completedFuture(DeleteTableResponse.builder().build());
     Mockito.when(mockAwsGlue.deleteTable(any(DeleteTableRequest.class))).thenReturn(deleteTableResponse);
 
+    Mockito.when(mockAwsGlue.updateTable(any(UpdateTableRequest.class))).thenReturn(CompletableFuture.completedFuture(null));
     awsGlueSyncClient.createOrReplaceTable(tableName, storageSchema, inputFormatClass, outputFormatClass, serdeClass, serdeProperties, tableProperties);
 
     // Verify that awsGlue.updateTable() is called exactly once

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/AvroConvertor.java
Patch:
@@ -117,10 +117,11 @@ public GenericRecord fromJson(String json) {
       initJsonConvertor();
       return jsonConverter.convert(json, schema);
     } catch (Exception e) {
+      String errorMessage = "Failed to convert JSON string to Avro record: ";
       if (json != null) {
-        throw new HoodieSchemaException("Failed to convert schema from json to avro: " + json, e);
+        throw new HoodieSchemaException(errorMessage + json + "; schema: " + schemaStr, e);
       } else {
-        throw new HoodieSchemaException("Failed to convert schema from json to avro. Schema string was null.", e);
+        throw new HoodieSchemaException(errorMessage + "JSON string was null.", e);
       }
     }
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java
Patch:
@@ -92,7 +92,7 @@ public static class Config implements Serializable {
     public String sparkMemory = null;
     @Parameter(names = {"--retry", "-rt"}, description = "number of retries")
     public int retry = 0;
-    @Parameter(names = {"--skip-clean", "-sc"}, description = "do not trigger clean after compaction", required = false)
+    @Parameter(names = {"--skip-clean", "-sc"}, description = "do not trigger clean after clustering", required = false)
     public Boolean skipClean = true;
 
     @Parameter(names = {"--schedule", "-sc"}, description = "Schedule clustering @desperate soon please use \"--mode schedule\" instead")

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDBDAO.java
Patch:
@@ -414,7 +414,7 @@ public <T extends Serializable> void prefixDelete(String columnFamilyName, Strin
         // This will not delete the last entry
         getRocksDB().deleteRange(managedHandlesMap.get(columnFamilyName), getUTF8Bytes(firstEntry), getUTF8Bytes(lastEntry));
         // Delete the last entry
-        getRocksDB().delete(getUTF8Bytes(lastEntry));
+        getRocksDB().delete(managedHandlesMap.get(columnFamilyName), getUTF8Bytes(lastEntry));
       } catch (RocksDBException e) {
         LOG.error("Got exception performing range delete");
         throw new HoodieException(e);

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HiveAvroSerializer.java
Patch:
@@ -304,8 +304,7 @@ private Object serializePrimitive(PrimitiveObjectInspector fieldOI, Object struc
       case DATE:
         return HoodieHiveUtils.getDays(structFieldData);
       case TIMESTAMP:
-        Object timestamp = HoodieHiveUtils.getTimestamp(structFieldData);
-        return HoodieHiveUtils.getMills(timestamp);
+        return HoodieHiveUtils.getMills(structFieldData);
       case INT:
         if (schema.getLogicalType() != null && schema.getLogicalType().getName().equals("date")) {
           return new WritableDateObjectInspector().getPrimitiveWritableObject(structFieldData).getDays();

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/shims/HiveShim.java
Patch:
@@ -27,11 +27,9 @@ public interface HiveShim {
 
   Writable getTimestampWriteable(long value, boolean timestampMillis);
 
-  Object unwrapTimestampAsPrimitive(Object o);
-
   Writable getDateWriteable(int value);
 
   int getDays(Object dateWritable);
 
-  long getMills(Object timestamp);
+  long getMills(Object timestampWritable);
 }

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -76,6 +76,7 @@
 import static org.apache.hudi.common.config.TimestampKeyGeneratorConfig.TIMESTAMP_OUTPUT_DATE_FORMAT;
 import static org.apache.hudi.common.config.TimestampKeyGeneratorConfig.TIMESTAMP_OUTPUT_TIMEZONE_FORMAT;
 import static org.apache.hudi.common.config.TimestampKeyGeneratorConfig.TIMESTAMP_TIMEZONE_FORMAT;
+import static org.apache.hudi.common.config.TimestampKeyGeneratorConfig.TIMESTAMP_TYPE_FIELD;
 import static org.apache.hudi.common.util.ConfigUtils.fetchConfigs;
 import static org.apache.hudi.common.util.ConfigUtils.recoverIfNeeded;
 import static org.apache.hudi.common.util.StringUtils.getUTF8Bytes;
@@ -284,6 +285,7 @@ public class HoodieTableConfig extends HoodieConfig {
   public static final ConfigProperty<String> HIVE_STYLE_PARTITIONING_ENABLE = KeyGeneratorOptions.HIVE_STYLE_PARTITIONING_ENABLE;
 
   public static final List<ConfigProperty<String>> PERSISTED_CONFIG_LIST = Arrays.asList(
+      TIMESTAMP_TYPE_FIELD,
       INPUT_TIME_UNIT,
       TIMESTAMP_INPUT_DATE_FORMAT_LIST_DELIMITER_REGEX,
       TIMESTAMP_INPUT_DATE_FORMAT,

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bucket/BucketStreamWriteFunction.java
Patch:
@@ -22,11 +22,11 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.util.Functions;
+import org.apache.hudi.common.util.hash.BucketIndexUtil;
 import org.apache.hudi.configuration.FlinkOptions;
 import org.apache.hudi.configuration.OptionsResolver;
 import org.apache.hudi.index.bucket.BucketIdentifier;
 import org.apache.hudi.sink.StreamWriteFunction;
-import org.apache.hudi.sink.utils.BucketIndexUtil;
 
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.state.FunctionInitializationContext;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/partitioner/BucketIndexPartitioner.java
Patch:
@@ -20,8 +20,8 @@
 
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.util.Functions;
+import org.apache.hudi.common.util.hash.BucketIndexUtil;
 import org.apache.hudi.index.bucket.BucketIdentifier;
-import org.apache.hudi.sink.utils.BucketIndexUtil;
 
 import org.apache.flink.api.common.functions.Partitioner;
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -2043,7 +2043,7 @@ public static HoodieData<HoodieRecord> convertFilesToPartitionStatsRecords(Hoodi
           .collect(Collectors.groupingBy(HoodieColumnRangeMetadata::getColumnName, toList())); // Group by column name
       // Step 3: Aggregate Column Ranges
       Stream<HoodieColumnRangeMetadata<Comparable>> partitionStatsRangeMetadata = columnMetadataMap.entrySet().stream()
-          .map(entry -> FileFormatUtils.getColumnRangeInPartition(entry.getValue()));
+          .map(entry -> FileFormatUtils.getColumnRangeInPartition(partitionPath, entry.getValue()));
       return HoodieMetadataPayload.createPartitionStatsRecords(partitionPath, partitionStatsRangeMetadata.collect(toList()), false).iterator();
     });
   }
@@ -2107,7 +2107,7 @@ public static HoodieData<HoodieRecord> convertMetadataToPartitionStatsRecords(Ho
             .collect(Collectors.groupingBy(HoodieColumnRangeMetadata::getColumnName, toList())); // Group by column name
         // Step 3: Aggregate Column Ranges
         Stream<HoodieColumnRangeMetadata<Comparable>> partitionStatsRangeMetadata = columnMetadataMap.entrySet().stream()
-            .map(entry -> FileFormatUtils.getColumnRangeInPartition(entry.getValue()));
+            .map(entry -> FileFormatUtils.getColumnRangeInPartition(partitionName, entry.getValue()));
         return HoodieMetadataPayload.createPartitionStatsRecords(partitionName, partitionStatsRangeMetadata.collect(toList()), false).iterator();
       });
     } catch (Exception e) {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java
Patch:
@@ -357,7 +357,7 @@ private static Database alterDatabase(Database hiveDB, CatalogDatabase newDataba
   private Table isHoodieTable(Table hiveTable) {
     if (!hiveTable.getParameters().getOrDefault(SPARK_SOURCE_PROVIDER, "").equalsIgnoreCase("hudi")
         && !isFlinkHoodieTable(hiveTable)) {
-      throw new HoodieCatalogException(String.format("the %s is not hoodie table", hiveTable.getTableName()));
+      throw new HoodieCatalogException(String.format("Table %s is not a hoodie table", hiveTable.getTableName()));
     }
     return hiveTable;
   }
@@ -374,7 +374,7 @@ public Table getHiveTable(ObjectPath tablePath) throws TableNotExistException {
     } catch (NoSuchObjectException e) {
       throw new TableNotExistException(getName(), tablePath);
     } catch (TException e) {
-      throw new HoodieCatalogException(String.format("Failed to get table %s from Hive metastore", tablePath.getObjectName()));
+      throw new HoodieCatalogException(String.format("Failed to get table %s from Hive metastore", tablePath.getObjectName()), e);
     }
   }
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bucket/ConsistentBucketAssignFunction.java
Patch:
@@ -66,7 +66,7 @@ public class ConsistentBucketAssignFunction extends ProcessFunction<HoodieRecord
   private final int bucketNum;
   private transient HoodieFlinkWriteClient writeClient;
   private transient Map<String, ConsistentBucketIdentifier> partitionToIdentifier;
-  private transient String lastRefreshInstant = HoodieTimeline.INIT_INSTANT_TS;
+  private transient String lastRefreshInstant;
   private final int maxRetries = 10;
   private final long maxWaitTimeInMs = 1000;
 
@@ -81,6 +81,7 @@ public void open(Configuration parameters) throws Exception {
     try {
       this.writeClient = FlinkWriteClients.createWriteClient(this.config, getRuntimeContext());
       this.partitionToIdentifier = new HashMap<>();
+      this.lastRefreshInstant = HoodieTimeline.INIT_INSTANT_TS;
     } catch (Throwable e) {
       LOG.error("Fail to initialize consistent bucket assigner", e);
       throw new RuntimeException(e);

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/partitioner/BucketAssigner.java
Patch:
@@ -291,17 +291,17 @@ public String getFileId() {
    */
   private static class SmallFileAssignState {
     long assigned;
-    long totalUnassigned;
+    long total;
     final String fileId;
 
     SmallFileAssignState(long parquetMaxFileSize, SmallFile smallFile, long averageRecordSize) {
       this.assigned = 0;
-      this.totalUnassigned = (parquetMaxFileSize - smallFile.sizeBytes) / averageRecordSize;
+      this.total = (parquetMaxFileSize - smallFile.sizeBytes) / averageRecordSize;
       this.fileId = smallFile.location.getFileId();
     }
 
     public boolean canAssign() {
-      return this.totalUnassigned > 0 && this.totalUnassigned > this.assigned;
+      return this.total > 0 && this.total > this.assigned;
     }
 
     /**

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java
Patch:
@@ -68,6 +68,7 @@
 
 import static org.apache.hudi.avro.HoodieAvroUtils.unwrapAvroValueWrapper;
 import static org.apache.hudi.avro.HoodieAvroUtils.wrapValueIntoAvro;
+import static org.apache.hudi.common.util.StringUtils.nonEmpty;
 import static org.apache.hudi.common.util.TypeUtils.unsafeCast;
 import static org.apache.hudi.common.util.ValidationUtils.checkArgument;
 import static org.apache.hudi.common.util.ValidationUtils.checkState;
@@ -686,10 +687,11 @@ public static Stream<HoodieRecord> createPartitionStatsRecords(String partitionP
     return columnRangeMetadataList.stream().map(columnRangeMetadata -> {
       HoodieKey key = new HoodieKey(getPartitionStatsIndexKey(partitionPath, columnRangeMetadata.getColumnName()),
           MetadataPartitionType.PARTITION_STATS.getPartitionPath());
+      String fileName = nonEmpty(columnRangeMetadata.getFilePath()) ? new StoragePath(columnRangeMetadata.getFilePath()).getName() : null;
 
       HoodieMetadataPayload payload = new HoodieMetadataPayload(key.getRecordKey(),
           HoodieMetadataColumnStats.newBuilder()
-              .setFileName(null)
+              .setFileName(fileName)
               .setColumnName(columnRangeMetadata.getColumnName())
               .setMinValue(wrapValueIntoAvro(columnRangeMetadata.getMinValue()))
               .setMaxValue(wrapValueIntoAvro(columnRangeMetadata.getMaxValue()))
@@ -713,7 +715,6 @@ public static String getPartitionStatsIndexKey(String partitionPath, String colu
   @SuppressWarnings({"rawtypes", "unchecked"})
   private static HoodieMetadataColumnStats mergeColumnStatsRecords(HoodieMetadataColumnStats prevColumnStats,
                                                                    HoodieMetadataColumnStats newColumnStats) {
-    checkArgument(Objects.equals(prevColumnStats.getFileName(), newColumnStats.getFileName()));
     checkArgument(Objects.equals(prevColumnStats.getColumnName(), newColumnStats.getColumnName()));
 
     // We're handling 2 cases in here

File: hudi-aws/src/main/java/org/apache/hudi/aws/credentials/HoodieConfigAWSAssumedRoleCredentialsProvider.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.config.HoodieAWSConfig;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import software.amazon.awssdk.auth.credentials.AwsCredentials;

File: hudi-aws/src/main/java/org/apache/hudi/aws/sync/AWSGlueCatalogSyncClient.java
Patch:
@@ -24,12 +24,12 @@
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.table.TableSchemaResolver;
 import org.apache.hudi.common.util.CollectionUtils;
-import org.apache.hudi.common.util.MapUtils;
 import org.apache.hudi.common.util.CustomizedThreadFactory;
 import org.apache.hudi.common.util.HoodieTimer;
+import org.apache.hudi.common.util.MapUtils;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;
 import org.apache.hudi.config.GlueCatalogSyncClientConfig;
+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;
 import org.apache.hudi.hive.HiveSyncConfig;
 import org.apache.hudi.sync.common.HoodieSyncClient;
 import org.apache.hudi.sync.common.model.FieldSchema;
@@ -111,9 +111,9 @@
 import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_SUPPORT_TIMESTAMP_TYPE;
 import static org.apache.hudi.hive.util.HiveSchemaUtil.getPartitionKeyType;
 import static org.apache.hudi.hive.util.HiveSchemaUtil.parquetSchemaToMapSchema;
+import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_BASE_FILE_FORMAT;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_DATABASE_NAME;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_PARTITION_FIELDS;
-import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_BASE_FILE_FORMAT;
 import static org.apache.hudi.sync.common.util.TableUtils.tableId;
 
 /**

File: hudi-aws/src/test/java/org/apache/hudi/aws/sync/TestGluePartitionPushdown.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.aws.sync.util.GluePartitionFilterGenerator;
 import org.apache.hudi.hive.HiveSyncConfig;
 import org.apache.hudi.sync.common.model.FieldSchema;
+
 import org.junit.jupiter.api.Test;
 
 import java.util.ArrayList;

File: hudi-cli/src/main/java/org/apache/hudi/cli/HoodiePrompt.java
Patch:
@@ -18,13 +18,12 @@
 
 package org.apache.hudi.cli;
 
+import org.jline.utils.AttributedString;
 import org.springframework.core.Ordered;
 import org.springframework.core.annotation.Order;
 import org.springframework.shell.jline.PromptProvider;
 import org.springframework.stereotype.Component;
 
-import org.jline.utils.AttributedString;
-
 /**
  * This class deals with displaying prompt on CLI based on the state.
  */

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/BootstrapCommand.java
Patch:
@@ -30,12 +30,12 @@
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.utilities.UtilHelpers;
+
 import org.apache.spark.launcher.SparkLauncher;
 import org.apache.spark.util.Utils;
 import org.springframework.shell.standard.ShellComponent;
 import org.springframework.shell.standard.ShellMethod;
 import org.springframework.shell.standard.ShellOption;
-import scala.collection.JavaConverters;
 
 import java.io.IOException;
 import java.net.URISyntaxException;
@@ -45,6 +45,8 @@
 import java.util.List;
 import java.util.stream.Collectors;
 
+import scala.collection.JavaConverters;
+
 /**
  * CLI command to perform bootstrap action & display bootstrap index.
  */

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CleansCommand.java
Patch:
@@ -34,12 +34,12 @@
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
 import org.apache.hudi.utilities.UtilHelpers;
+
 import org.apache.spark.launcher.SparkLauncher;
 import org.apache.spark.util.Utils;
 import org.springframework.shell.standard.ShellComponent;
 import org.springframework.shell.standard.ShellMethod;
 import org.springframework.shell.standard.ShellOption;
-import scala.collection.JavaConverters;
 
 import java.io.IOException;
 import java.net.URISyntaxException;
@@ -49,6 +49,8 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
+import scala.collection.JavaConverters;
+
 /**
  * CLI command to show cleans options.
  */

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/ClusteringCommand.java
Patch:
@@ -24,11 +24,13 @@
 import org.apache.hudi.cli.utils.SparkUtil;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.utilities.UtilHelpers;
+
 import org.apache.spark.launcher.SparkLauncher;
 import org.apache.spark.util.Utils;
 import org.springframework.shell.standard.ShellComponent;
 import org.springframework.shell.standard.ShellMethod;
 import org.springframework.shell.standard.ShellOption;
+
 import scala.collection.JavaConverters;
 
 @ShellComponent

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieSyncValidateCommand.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.exception.HoodieException;
+
 import org.springframework.shell.standard.ShellComponent;
 import org.springframework.shell.standard.ShellMethod;
 import org.springframework.shell.standard.ShellOption;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/MarkersCommand.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.cli.HoodieCLI;
 import org.apache.hudi.cli.utils.InputStreamConsumer;
 import org.apache.hudi.cli.utils.SparkUtil;
+
 import org.apache.spark.launcher.SparkLauncher;
 import org.springframework.shell.standard.ShellComponent;
 import org.springframework.shell.standard.ShellMethod;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RestoresCommand.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
 import org.apache.hudi.common.util.Option;
+
 import org.springframework.shell.standard.ShellComponent;
 import org.springframework.shell.standard.ShellMethod;
 import org.springframework.shell.standard.ShellOption;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RollbacksCommand.java
Patch:
@@ -33,7 +33,6 @@
 import org.apache.hudi.common.util.collection.Pair;
 
 import org.apache.spark.launcher.SparkLauncher;
-
 import org.springframework.shell.standard.ShellComponent;
 import org.springframework.shell.standard.ShellMethod;
 import org.springframework.shell.standard.ShellOption;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SavepointsCommand.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.exception.HoodieException;
+
 import org.apache.spark.launcher.SparkLauncher;
 import org.springframework.shell.standard.ShellComponent;
 import org.springframework.shell.standard.ShellMethod;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkEnvCommand.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.cli.commands;
 
 import org.apache.hudi.cli.HoodiePrintHelper;
+
 import org.springframework.shell.standard.ShellComponent;
 import org.springframework.shell.standard.ShellMethod;
 import org.springframework.shell.standard.ShellOption;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/StatsCommand.java
Patch:
@@ -28,9 +28,9 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.NumericUtils;
-import org.apache.hudi.storage.StoragePathInfo;
-import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.storage.HoodieStorage;
+import org.apache.hudi.storage.StoragePath;
+import org.apache.hudi.storage.StoragePathInfo;
 
 import com.codahale.metrics.Histogram;
 import com.codahale.metrics.Snapshot;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/TempViewCommand.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.cli.HoodieCLI;
 import org.apache.hudi.exception.HoodieException;
+
 import org.springframework.shell.standard.ShellComponent;
 import org.springframework.shell.standard.ShellMethod;
 import org.springframework.shell.standard.ShellOption;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/TimelineCommand.java
Patch:
@@ -32,9 +32,9 @@
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.metadata.HoodieTableMetadata;
-import org.apache.hudi.storage.StoragePathInfo;
-import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.storage.HoodieStorage;
+import org.apache.hudi.storage.StoragePath;
+import org.apache.hudi.storage.StoragePathInfo;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/UpgradeOrDowngradeCommand.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.cli.utils.SparkUtil;
 import org.apache.hudi.common.table.HoodieTableVersion;
 import org.apache.hudi.common.util.StringUtils;
+
 import org.apache.spark.launcher.SparkLauncher;
 import org.springframework.shell.standard.ShellComponent;
 import org.springframework.shell.standard.ShellMethod;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/UtilsCommand.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.cli.commands;
 
 import org.apache.hudi.common.util.StringUtils;
+
 import org.springframework.shell.standard.ShellComponent;
 import org.springframework.shell.standard.ShellMethod;
 import org.springframework.shell.standard.ShellOption;

File: hudi-cli/src/test/java/org/apache/hudi/cli/TestSparkUtil.java
Patch:
@@ -18,10 +18,10 @@
 
 package org.apache.hudi.cli;
 
-import org.apache.hudi.common.util.Option;
 import org.apache.hudi.cli.utils.SparkUtil;
-import org.apache.spark.SparkConf;
+import org.apache.hudi.common.util.Option;
 
+import org.apache.spark.SparkConf;
 import org.apache.spark.launcher.SparkLauncher;
 import org.junit.jupiter.api.Test;
 

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestSparkEnvCommand.java
Patch:
@@ -20,8 +20,8 @@
 
 import org.apache.hudi.cli.HoodiePrintHelper;
 import org.apache.hudi.cli.functional.CLIFunctionalTestHarness;
-
 import org.apache.hudi.cli.testutils.ShellEvaluationResultUtil;
+
 import org.junit.jupiter.api.Tag;
 import org.junit.jupiter.api.Test;
 import org.springframework.beans.factory.annotation.Autowired;

File: hudi-cli/src/test/java/org/apache/hudi/cli/testutils/HoodieCLIIntegrationTestBase.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.cli.testutils;
 
 import org.apache.hudi.common.model.HoodieTableType;
+
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
 

File: hudi-cli/src/test/java/org/apache/hudi/cli/testutils/HoodieCLIIntegrationTestHarness.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.cli.testutils;
 
 import org.apache.hudi.testutils.HoodieSparkClientTestHarness;
+
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java
Patch:
@@ -22,8 +22,8 @@
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieHeartbeatException;
-import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.storage.HoodieStorage;
+import org.apache.hudi.storage.StoragePath;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/ConcurrentOperation.java
Patch:
@@ -19,13 +19,13 @@
 package org.apache.hudi.client.transaction;
 
 import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;
-import org.apache.hudi.common.table.timeline.MetadataConversionUtils;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieMetadataWrapper;
 import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
+import org.apache.hudi.common.table.timeline.MetadataConversionUtils;
 import org.apache.hudi.common.util.CommitUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/LockInfo.java
Patch:
@@ -19,9 +19,10 @@
 
 package org.apache.hudi.client.transaction.lock;
 
-import com.fasterxml.jackson.core.JsonProcessingException;
 import org.apache.hudi.common.util.JsonUtils;
 
+import com.fasterxml.jackson.core.JsonProcessingException;
+
 import java.util.ArrayList;
 
 public class LockInfo {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/LegacyArchivedMetaEntryReader.java
Patch:
@@ -36,8 +36,8 @@
 import org.apache.hudi.common.util.collection.ClosableIterator;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieIOException;
-import org.apache.hudi.storage.StoragePathInfo;
 import org.apache.hudi.storage.StoragePath;
+import org.apache.hudi.storage.StoragePathInfo;
 
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/CopyOnWriteInsertHandler.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.io.HoodieWriteHandle;
 import org.apache.hudi.io.WriteHandleFactory;
 import org.apache.hudi.table.HoodieTable;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/HoodieLazyInsertIterable.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.execution;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.utils.LazyIterableIterator;
 import org.apache.hudi.common.engine.TaskContextSupplier;
@@ -29,6 +28,8 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.util.ExecutorFactory;
 
+import org.apache.avro.Schema;
+
 import java.util.Iterator;
 import java.util.List;
 import java.util.function.Function;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieBootstrapHandle.java
Patch:
@@ -18,14 +18,15 @@
 
 package org.apache.hudi.io;
 
-import org.apache.avro.JsonProperties;
-import org.apache.avro.Schema;
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
+import org.apache.avro.JsonProperties;
+import org.apache.avro.Schema;
+
 import java.util.List;
 import java.util.stream.Collectors;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandleFactory.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.keygen.BaseKeyGenerator;
 import org.apache.hudi.table.HoodieTable;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.io;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieBaseFile;
@@ -29,6 +28,8 @@
 import org.apache.hudi.keygen.BaseKeyGenerator;
 import org.apache.hudi.table.HoodieTable;
 
+import org.apache.avro.Schema;
+
 import javax.annotation.concurrent.NotThreadSafe;
 
 import java.io.IOException;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandleWithChangeLog.java
Patch:
@@ -30,9 +30,9 @@
 import org.apache.avro.Schema;
 
 import java.io.IOException;
-import java.util.Properties;
 import java.util.Iterator;
 import java.util.Map;
+import java.util.Properties;
 
 /**
  * A sorted merge handle that supports logging change logs.

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/ComplexAvroKeyGenerator.java
Patch:
@@ -17,10 +17,11 @@
 
 package org.apache.hudi.keygen;
 
-import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 
+import org.apache.avro.generic.GenericRecord;
+
 import java.util.Arrays;
 import java.util.stream.Collectors;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/GlobalAvroDeleteKeyGenerator.java
Patch:
@@ -17,9 +17,10 @@
 
 package org.apache.hudi.keygen;
 
-import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.common.config.TypedProperties;
 
+import org.apache.avro.generic.GenericRecord;
+
 import java.util.ArrayList;
 import java.util.List;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/SimpleAvroKeyGenerator.java
Patch:
@@ -17,11 +17,12 @@
 
 package org.apache.hudi.keygen;
 
-import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 
+import org.apache.avro.generic.GenericRecord;
+
 import java.util.Collections;
 
 /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/factory/HoodieAvroKeyGeneratorFactory.java
Patch:
@@ -21,11 +21,11 @@
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieKeyGeneratorException;
+import org.apache.hudi.keygen.AutoRecordGenWrapperAvroKeyGenerator;
 import org.apache.hudi.keygen.BaseKeyGenerator;
 import org.apache.hudi.keygen.ComplexAvroKeyGenerator;
 import org.apache.hudi.keygen.CustomAvroKeyGenerator;
 import org.apache.hudi.keygen.GlobalAvroDeleteKeyGenerator;
-import org.apache.hudi.keygen.AutoRecordGenWrapperAvroKeyGenerator;
 import org.apache.hudi.keygen.KeyGenUtils;
 import org.apache.hudi.keygen.KeyGenerator;
 import org.apache.hudi.keygen.NonpartitionedAvroKeyGenerator;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataWriteUtils.java
Patch:
@@ -35,11 +35,11 @@
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.config.metrics.HoodieMetricsConfig;
+import org.apache.hudi.config.metrics.HoodieMetricsDatadogConfig;
 import org.apache.hudi.config.metrics.HoodieMetricsGraphiteConfig;
 import org.apache.hudi.config.metrics.HoodieMetricsJmxConfig;
 import org.apache.hudi.config.metrics.HoodieMetricsM3Config;
 import org.apache.hudi.config.metrics.HoodieMetricsPrometheusConfig;
-import org.apache.hudi.config.metrics.HoodieMetricsDatadogConfig;
 import org.apache.hudi.exception.HoodieMetadataException;
 import org.apache.hudi.table.action.compact.strategy.UnBoundedCompactionStrategy;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataKeyGenerator.java
Patch:
@@ -19,11 +19,12 @@
 
 package org.apache.hudi.metadata;
 
-import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.keygen.BaseKeyGenerator;
 import org.apache.hudi.keygen.KeyGenUtils;
 
+import org.apache.avro.generic.GenericRecord;
+
 /**
  * Custom key generator for the Hoodie table metadata. The metadata table record payload
  * has an internal schema with a known key field HoodieMetadataPayload.SCHEMA_FIELD_ID_KEY.

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/BucketIndexBulkInsertPartitioner.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.io.AppendHandleFactory;
 import org.apache.hudi.io.SingleFileHandleCreateFactory;
 import org.apache.hudi.io.WriteHandleFactory;
+
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.Logger;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/util/ConsistentHashingUpdateStrategyUtils.java
Patch:
@@ -26,8 +26,8 @@
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.ClusteringUtils;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.common.util.ValidationUtils;
+import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.index.bucket.ConsistentBucketIdentifier;
 import org.apache.hudi.index.bucket.ConsistentBucketIndexUtils;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/SmallFile.java
Patch:
@@ -18,9 +18,10 @@
 
 package org.apache.hudi.table.action.commit;
 
-import java.io.Serializable;
 import org.apache.hudi.common.model.HoodieRecordLocation;
 
+import java.io.Serializable;
+
 /**
  * Helper class for a small file's location and its actual size on disk.
  */

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/CompactionStrategy.java
Patch:
@@ -20,9 +20,9 @@
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
+import org.apache.hudi.client.utils.FileSliceMetricUtils;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.util.CompactionUtils;
-import org.apache.hudi.client.utils.FileSliceMetricUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 
 import java.io.Serializable;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/ZeroToOneUpgradeHandler.java
Patch:
@@ -31,8 +31,8 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieRollbackException;
-import org.apache.hudi.storage.StoragePathInfo;
 import org.apache.hudi.storage.StoragePath;
+import org.apache.hudi.storage.StoragePathInfo;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.rollback.BaseRollbackHelper;
 import org.apache.hudi.table.action.rollback.ListingBasedRollbackStrategy;

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/DummyActiveAction.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi;
 
-import org.apache.hudi.common.table.timeline.ActiveAction;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
+import org.apache.hudi.common.table.timeline.ActiveAction;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/client/heartbeat/TestHoodieHeartbeatClient.java
Patch:
@@ -19,8 +19,8 @@
 package org.apache.hudi.client.heartbeat;
 
 import org.apache.hudi.common.testutils.HoodieCommonTestHarness;
-import org.apache.hudi.storage.StoragePathInfo;
 import org.apache.hudi.storage.StoragePath;
+import org.apache.hudi.storage.StoragePathInfo;
 
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/keygen/factory/TestCreateAvroKeyGeneratorByTypeWithFactory.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hudi.keygen.TimestampBasedAvroKeyGenerator;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 import org.apache.hudi.keygen.constant.KeyGeneratorType;
+
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.BeforeEach;

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/metrics/datadog/TestHoodieMetricsDatadogConfig.java
Patch:
@@ -19,8 +19,8 @@
 
 package org.apache.hudi.metrics.datadog;
 
-import org.apache.hudi.config.metrics.HoodieMetricsDatadogConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
+import org.apache.hudi.config.metrics.HoodieMetricsDatadogConfig;
 
 import org.junit.jupiter.api.Test;
 

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/table/action/TestCleanPlanner.java
Patch:
@@ -75,7 +75,6 @@
 import static org.apache.hudi.table.action.clean.CleanPlanner.SAVEPOINTED_TIMESTAMPS;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.mockito.Mockito.mock;
-
 import static org.mockito.Mockito.when;
 
 public class TestCleanPlanner {

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/utils/HoodieWriterClientTestHarness.java
Patch:
@@ -18,9 +18,6 @@
 
 package org.apache.hudi.utils;
 
-import org.apache.avro.generic.GenericRecord;
-import org.apache.hadoop.fs.Path;
-
 import org.apache.hudi.avro.model.HoodieCleanMetadata;
 import org.apache.hudi.avro.model.HoodieClusteringPlan;
 import org.apache.hudi.client.BaseHoodieWriteClient;
@@ -86,6 +83,9 @@
 import org.apache.hudi.table.action.commit.HoodieWriteHelper;
 import org.apache.hudi.table.marker.WriteMarkersFactory;
 import org.apache.hudi.testutils.MetadataMergeWriteStatus;
+
+import org.apache.avro.generic.GenericRecord;
+import org.apache.hadoop.fs.Path;
 import org.jetbrains.annotations.NotNull;
 
 import java.io.IOException;

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/utils/TestConcatenatingIterator.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.utils;
 
 import org.apache.hudi.client.utils.ConcatenatingIterator;
+
 import org.junit.jupiter.api.Test;
 
 import java.util.ArrayList;

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/execution/FlinkLazyInsertIterable.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.execution;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -30,6 +29,8 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.util.ExecutorFactory;
 
+import org.apache.avro.Schema;
+
 import java.util.Iterator;
 import java.util.List;
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkPartitionTTLActionExecutor.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.hudi.table.action.ttl.strategy.HoodiePartitionTTLStrategyFactory;
 import org.apache.hudi.table.action.ttl.strategy.PartitionTTLStrategy;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkWriteHelper.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.table.action.commit;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.data.HoodieListData;
@@ -34,6 +33,8 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
+import org.apache.avro.Schema;
+
 import java.io.IOException;
 import java.time.Duration;
 import java.time.Instant;

File: hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/table/action/cluster/strategy/TestFlinkSizeBasedClusteringPlanStrategy.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieFlinkCopyOnWriteTable;
 import org.apache.hudi.table.action.cluster.ClusteringPlanPartitionFilterMode;
+
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
 import org.mockito.Mock;

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/execution/JavaLazyInsertIterable.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.execution;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -29,6 +28,8 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.util.ExecutorFactory;
 
+import org.apache.avro.Schema;
+
 import java.util.Iterator;
 import java.util.List;
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertPreppedCommitActionExecutor.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
-
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
 import java.util.List;

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPreppedCommitActionExecutor.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
-
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
 import java.util.List;

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaWriteHelper.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.table.action.commit;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.data.HoodieListData;
@@ -31,6 +30,8 @@
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.table.HoodieTable;
 
+import org.apache.avro.Schema;
+
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/model/HoodieInternalRow.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.client.model;
 
 import org.apache.hudi.common.model.HoodieRecord;
+
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.catalyst.expressions.UnsafeRow;
 import org.apache.spark.sql.catalyst.util.ArrayData;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/SparkLazyInsertIterable.java
Patch:
@@ -27,9 +27,9 @@
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.io.WriteHandleFactory;
 import org.apache.hudi.table.HoodieTable;
+import org.apache.hudi.util.ExecutorFactory;
 
 import org.apache.avro.Schema;
-import org.apache.hudi.util.ExecutorFactory;
 
 import java.util.Iterator;
 import java.util.List;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDSimpleBucketBulkInsertPartitioner.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.index.bucket.BucketIdentifier;
 import org.apache.hudi.index.bucket.HoodieSimpleBucketIndex;
 import org.apache.hudi.table.HoodieTable;
+
 import org.apache.spark.Partitioner;
 import org.apache.spark.api.java.JavaRDD;
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RowSpatialCurveSortPartitioner.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.config.HoodieClusteringConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
+
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/HoodieSparkFileWriter.java
Patch:
@@ -18,9 +18,10 @@
 
 package org.apache.hudi.io.storage;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
+
+import org.apache.avro.Schema;
 import org.apache.spark.sql.catalyst.InternalRow;
 
 import java.io.IOException;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowParquetWriteSupport.java
Patch:
@@ -18,14 +18,14 @@
 
 package org.apache.hudi.io.storage.row;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hudi.avro.HoodieBloomFilterWriteSupport;
 import org.apache.hudi.common.bloom.BloomFilter;
 import org.apache.hudi.common.config.HoodieConfig;
 import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ReflectionUtils;
 
+import org.apache.hadoop.conf.Configuration;
 import org.apache.parquet.hadoop.api.WriteSupport;
 import org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport;
 import org.apache.spark.sql.types.StructType;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/ComplexKeyGenerator.java
Patch:
@@ -17,9 +17,10 @@
 
 package org.apache.hudi.keygen;
 
-import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
+
+import org.apache.avro.generic.GenericRecord;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.types.StructType;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/SimpleKeyGenerator.java
Patch:
@@ -18,10 +18,11 @@
 
 package org.apache.hudi.keygen;
 
-import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
+
+import org.apache.avro.generic.GenericRecord;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.types.StructType;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/SparkKeyGeneratorInterface.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.ApiMaturityLevel;
 import org.apache.hudi.PublicAPIMethod;
+
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.types.StructType;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java
Patch:
@@ -18,11 +18,12 @@
 
 package org.apache.hudi.keygen;
 
-import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieKeyGeneratorException;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
+
+import org.apache.avro.generic.GenericRecord;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.types.StructType;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/UTF8StringPartitionPathFormatter.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.hudi.keygen;
 
 import org.apache.hudi.common.util.PartitionPathEncodeUtils;
+
 import org.apache.spark.unsafe.types.UTF8String;
 
 import java.util.function.Supplier;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BucketBulkInsertDataInternalWriterHelper.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.io.storage.row.HoodieRowCreateHandle;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 import org.apache.hudi.table.HoodieTable;
+
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.types.StructType;
 import org.apache.spark.unsafe.types.UTF8String;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkHoodiePartitioner.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.WorkloadProfile;
+
 import org.apache.spark.Partitioner;
 
 /**

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/bootstrap/TestUniformBootstrapModeSelector.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.client.bootstrap.selector.UniformBootstrapModeSelector;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
+
 import org.junit.jupiter.api.Test;
 
 import java.util.ArrayList;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/clustering/plan/strategy/TestSparkSizeBasedClusteringPlanStrategy.java
Patch:
@@ -29,7 +29,6 @@
 
 import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.Test;
-
 import org.mockito.Mock;
 
 import java.util.ArrayList;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestBoundedInMemoryExecutorInSpark.java
Patch:
@@ -19,8 +19,8 @@
 package org.apache.hudi.execution;
 
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.queue.BoundedInMemoryExecutor;
 import org.apache.hudi.common.util.queue.ExecutorType;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestBoundedInMemoryQueue.java
Patch:
@@ -20,8 +20,8 @@
 
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.util.DefaultSizeEstimator;
 import org.apache.hudi.common.util.FileIOUtils;
 import org.apache.hudi.common.util.Option;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestDisruptorExecutionInSpark.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.testutils.HoodieSparkClientTestHarness;
+
 import org.apache.spark.TaskContext;
 import org.apache.spark.TaskContext$;
 import org.junit.jupiter.api.AfterEach;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestSimpleExecutionInSpark.java
Patch:
@@ -20,8 +20,8 @@
 
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.queue.HoodieConsumer;
 import org.apache.hudi.common.util.queue.SimpleExecutor;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/hbase/TestHBaseQPSResourceAllocator.java
Patch:
@@ -18,12 +18,12 @@
 
 package org.apache.hudi.index.hbase;
 
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieHBaseIndexConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestConsistencyGuard.java
Patch:
@@ -18,12 +18,12 @@
 
 package org.apache.hudi.table;
 
+import org.apache.hudi.common.fs.ConsistencyGuard;
 import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.fs.FailSafeConsistencyGuard;
 import org.apache.hudi.common.fs.OptimisticConsistencyGuard;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.testutils.FileCreateUtils;
-import org.apache.hudi.common.fs.ConsistencyGuard;
 import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.testutils.HoodieSparkClientTestHarness;
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/rollback/TestRollbackUtils.java
Patch:
@@ -24,8 +24,8 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.CollectionUtils;
-import org.apache.hudi.storage.StoragePathInfo;
 import org.apache.hudi.storage.StoragePath;
+import org.apache.hudi.storage.StoragePathInfo;
 
 import org.junit.jupiter.api.Test;
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieSparkWriteableTestTable.java
Patch:
@@ -29,8 +29,8 @@
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.metadata.HoodieTableMetadataWriter;
-import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.storage.HoodieStorage;
+import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.table.HoodieTable;
 
 import org.apache.avro.Schema;

File: hudi-common/src/main/java/org/apache/hudi/common/conflict/detection/DirectMarkerBasedDetectionStrategy.java
Patch:
@@ -26,9 +26,9 @@
 import org.apache.hudi.common.util.MarkerUtils;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.exception.HoodieIOException;
-import org.apache.hudi.storage.StoragePathInfo;
-import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.storage.HoodieStorage;
+import org.apache.hudi.storage.StoragePath;
+import org.apache.hudi.storage.StoragePathInfo;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: hudi-common/src/main/java/org/apache/hudi/common/fs/OptimisticConsistencyGuard.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.common.fs;
 
-import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.storage.HoodieStorage;
+import org.apache.hudi.storage.StoragePath;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: hudi-common/src/main/java/org/apache/hudi/common/model/BootstrapIndexType.java
Patch:
@@ -19,8 +19,8 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.hudi.common.bootstrap.index.hfile.HFileBootstrapIndex;
 import org.apache.hudi.common.bootstrap.index.NoOpBootstrapIndex;
+import org.apache.hudi.common.bootstrap.index.hfile.HFileBootstrapIndex;
 import org.apache.hudi.common.config.EnumDescription;
 import org.apache.hudi.common.config.EnumFieldDescription;
 import org.apache.hudi.common.config.HoodieConfig;

File: hudi-common/src/main/java/org/apache/hudi/common/model/FirstValueAvroPayload.java
Patch:
@@ -18,13 +18,14 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.avro.generic.GenericRecord;
-import org.apache.avro.generic.IndexedRecord;
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.ConfigUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 
+import org.apache.avro.generic.GenericRecord;
+import org.apache.avro.generic.IndexedRecord;
+
 import java.util.Properties;
 
 /**

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroRecordMerger.java
Patch:
@@ -18,14 +18,14 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.avro.Schema;
-import org.apache.avro.generic.IndexedRecord;
-
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.HoodieRecord.HoodieRecordType;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
 
+import org.apache.avro.Schema;
+import org.apache.avro.generic.IndexedRecord;
+
 import java.io.IOException;
 import java.util.Properties;
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieLogFile.java
Patch:
@@ -21,9 +21,9 @@
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.table.cdc.HoodieCDCUtils;
 import org.apache.hudi.exception.InvalidHoodiePathException;
-import org.apache.hudi.storage.StoragePathInfo;
-import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.storage.HoodieStorage;
+import org.apache.hudi.storage.StoragePath;
+import org.apache.hudi.storage.StoragePathInfo;
 
 import java.io.Serializable;
 import java.util.Comparator;

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java
Patch:
@@ -18,13 +18,14 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.avro.Schema;
-import org.apache.avro.generic.IndexedRecord;
 import org.apache.hudi.ApiMaturityLevel;
 import org.apache.hudi.PublicAPIClass;
 import org.apache.hudi.PublicAPIMethod;
 import org.apache.hudi.common.util.Option;
 
+import org.apache.avro.Schema;
+import org.apache.avro.generic.IndexedRecord;
+
 import java.io.IOException;
 import java.io.Serializable;
 import java.util.Map;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieUnMergedLogRecordScanner.java
Patch:
@@ -29,9 +29,9 @@
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.internal.schema.InternalSchema;
 import org.apache.hudi.storage.HoodieStorage;
+import org.apache.hudi.storage.StoragePath;
 
 import org.apache.avro.Schema;
-import org.apache.hudi.storage.StoragePath;
 
 import java.util.List;
 import java.util.stream.Collectors;

File: hudi-common/src/main/java/org/apache/hudi/common/util/HoodieCommonKryoRegistrar.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.common.util;
 
-import com.esotericsoftware.kryo.Kryo;
 import org.apache.hudi.common.HoodieJsonPayload;
 import org.apache.hudi.common.model.AWSDmsAvroPayload;
 import org.apache.hudi.common.model.DefaultHoodieRecordPayload;
@@ -27,9 +26,9 @@
 import org.apache.hudi.common.model.HoodieAvroPayload;
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieEmptyRecord;
+import org.apache.hudi.common.model.HoodieRecordDelegate;
 import org.apache.hudi.common.model.HoodieRecordGlobalLocation;
 import org.apache.hudi.common.model.HoodieRecordLocation;
-import org.apache.hudi.common.model.HoodieRecordDelegate;
 import org.apache.hudi.common.model.OverwriteNonDefaultsWithLatestAvroPayload;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.model.PartialUpdateAvroPayload;
@@ -38,6 +37,8 @@
 import org.apache.hudi.common.model.debezium.PostgresDebeziumAvroPayload;
 import org.apache.hudi.metadata.HoodieMetadataPayload;
 
+import com.esotericsoftware.kryo.Kryo;
+
 import java.util.Arrays;
 
 /**

File: hudi-common/src/main/java/org/apache/hudi/common/util/SerializationUtils.java
Patch:
@@ -18,12 +18,13 @@
 
 package org.apache.hudi.common.util;
 
+import org.apache.hudi.avro.GenericAvroSerializer;
+
 import com.esotericsoftware.kryo.Kryo;
 import com.esotericsoftware.kryo.Serializer;
 import com.esotericsoftware.kryo.io.Input;
 import com.esotericsoftware.kryo.io.Output;
 import org.apache.avro.generic.GenericData;
-import org.apache.hudi.avro.GenericAvroSerializer;
 import org.apache.avro.util.Utf8;
 import org.objenesis.strategy.StdInstantiatorStrategy;
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/DisruptorMessageQueue.java
Patch:
@@ -18,13 +18,13 @@
 
 package org.apache.hudi.common.util.queue;
 
-import com.lmax.disruptor.TimeoutException;
 import org.apache.hudi.common.util.CustomizedThreadFactory;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieException;
 
 import com.lmax.disruptor.EventTranslator;
 import com.lmax.disruptor.RingBuffer;
+import com.lmax.disruptor.TimeoutException;
 import com.lmax.disruptor.WaitStrategy;
 import com.lmax.disruptor.dsl.Disruptor;
 import com.lmax.disruptor.dsl.ProducerType;

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/WaitStrategyFactory.java
Patch:
@@ -18,12 +18,13 @@
 
 package org.apache.hudi.common.util.queue;
 
+import org.apache.hudi.exception.HoodieException;
+
 import com.lmax.disruptor.BlockingWaitStrategy;
 import com.lmax.disruptor.BusySpinWaitStrategy;
 import com.lmax.disruptor.SleepingWaitStrategy;
 import com.lmax.disruptor.WaitStrategy;
 import com.lmax.disruptor.YieldingWaitStrategy;
-import org.apache.hudi.exception.HoodieException;
 
 import static org.apache.hudi.common.util.queue.DisruptorWaitStrategyType.BLOCKING_WAIT;
 

File: hudi-common/src/main/java/org/apache/hudi/expression/Literal.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.internal.schema.Types;
 
 import javax.xml.bind.DatatypeConverter;
+
 import java.math.BigDecimal;
 import java.nio.ByteBuffer;
 import java.util.UUID;

File: hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieFileWriter.java
Patch:
@@ -18,11 +18,11 @@
 
 package org.apache.hudi.io.storage;
 
-import org.apache.avro.Schema;
-
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 
+import org.apache.avro.Schema;
+
 import java.io.IOException;
 import java.util.Properties;
 

File: hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieSeekingFileReader.java
Patch:
@@ -18,10 +18,11 @@
 
 package org.apache.hudi.io.storage;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.collection.ClosableIterator;
 
+import org.apache.avro.Schema;
+
 import java.io.IOException;
 import java.io.UnsupportedEncodingException;
 import java.util.List;

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataLogRecordReader.java
Patch:
@@ -28,9 +28,9 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.ExternalSpillableMap;
 import org.apache.hudi.storage.HoodieStorage;
+import org.apache.hudi.storage.StoragePath;
 
 import org.apache.avro.Schema;
-import org.apache.hudi.storage.StoragePath;
 
 import javax.annotation.concurrent.ThreadSafe;
 

File: hudi-common/src/main/java/org/apache/hudi/metrics/MetricUtils.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
+
 import java.util.Arrays;
 import java.util.List;
 import java.util.Map;

File: hudi-common/src/main/java/org/apache/hudi/metrics/m3/M3ScopeReporterAdaptor.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hudi.metrics.m3;
 
+import org.apache.hudi.common.util.collection.Pair;
+
 import com.codahale.metrics.Counter;
 import com.codahale.metrics.Gauge;
 import com.codahale.metrics.Histogram;
@@ -29,13 +31,13 @@
 import com.codahale.metrics.Snapshot;
 import com.codahale.metrics.Timer;
 import com.uber.m3.tally.Scope;
+
 import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.SortedMap;
 import java.util.concurrent.TimeUnit;
-import org.apache.hudi.common.util.collection.Pair;
 
 /**
  * Implementation of com.codahale.metrics.ScheduledReporter, to emit metrics from

File: hudi-common/src/main/java/org/apache/hudi/util/Transient.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.common.function.ThrowingConsumer;
 
 import javax.annotation.concurrent.ThreadSafe;
+
 import java.io.Serializable;
 
 import static org.apache.hudi.common.util.ValidationUtils.checkArgument;

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestFirstValueAvroPayload.java
Patch:
@@ -18,10 +18,11 @@
 
 package org.apache.hudi.common.model;
 
+import org.apache.hudi.common.testutils.PreCombineTestUtils;
+
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericData;
 import org.apache.avro.generic.GenericRecord;
-import org.apache.hudi.common.testutils.PreCombineTestUtils;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.MethodSource;

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodieLSMTimelineManifest.java
Patch:
@@ -23,8 +23,8 @@
 import java.util.Arrays;
 import java.util.stream.Collectors;
 
-import static org.hamcrest.MatcherAssert.assertThat;
 import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
 
 /**
  * Test cases for {@link HoodieLSMTimelineManifest}.

File: hudi-common/src/test/java/org/apache/hudi/common/table/timeline/TestLSMTimeline.java
Patch:
@@ -23,8 +23,8 @@
 
 import org.junit.jupiter.api.Test;
 
-import static org.hamcrest.MatcherAssert.assertThat;
 import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
 import static org.junit.jupiter.api.Assertions.assertThrows;
 
 /**

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestHoodieRecordUtils.java
Patch:
@@ -18,14 +18,13 @@
 
 package org.apache.hudi.common.util;
 
-import org.apache.avro.generic.GenericRecord;
-
 import org.apache.hudi.common.model.DefaultHoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieAvroRecordMerger;
 import org.apache.hudi.common.model.HoodieRecordMerger;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.exception.HoodieException;
 
+import org.apache.avro.generic.GenericRecord;
 import org.junit.jupiter.api.Test;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestRetryHelper.java
Patch:
@@ -23,8 +23,8 @@
 import java.io.IOException;
 import java.lang.reflect.Method;
 
-import static org.junit.jupiter.api.Assertions.assertTrue;
 import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertTrue;
 
 /**
  * Test retry helper.

File: hudi-common/src/test/java/org/apache/hudi/expression/TestPartialBindVisitor.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.expression;
 
 import org.apache.hudi.internal.schema.Types;
+
 import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.BeforeAll;
 import org.junit.jupiter.api.Test;

File: hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/factory/ContinuousFileSourceFactory.java
Patch:
@@ -18,6 +18,9 @@
 
 package org.apache.hudi.examples.quickstart.factory;
 
+import org.apache.hudi.configuration.FlinkOptions;
+import org.apache.hudi.examples.quickstart.source.ContinuousFileSource;
+
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;
 import org.apache.flink.configuration.Configuration;
@@ -29,8 +32,6 @@
 
 import java.util.Collections;
 import java.util.Set;
-import org.apache.hudi.configuration.FlinkOptions;
-import org.apache.hudi.examples.quickstart.source.ContinuousFileSource;
 
 /**
  * Factory for ContinuousFileSource.

File: hudi-examples/hudi-examples-flink/src/test/java/org/apache/hudi/examples/quickstart/TestHoodieFlinkQuickstart.java
Patch:
@@ -18,9 +18,10 @@
 
 package org.apache.hudi.examples.quickstart;
 
+import org.apache.hudi.common.model.HoodieTableType;
+
 import org.apache.flink.test.util.AbstractTestBase;
 import org.apache.flink.types.Row;
-import org.apache.hudi.common.model.HoodieTableType;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.io.TempDir;
 import org.junit.jupiter.params.ParameterizedTest;

File: hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/common/ExampleDataSchemaProvider.java
Patch:
@@ -18,9 +18,10 @@
 
 package org.apache.hudi.examples.common;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.schema.SchemaProvider;
+
+import org.apache.avro.Schema;
 import org.apache.spark.api.java.JavaSparkContext;
 
 /**

File: hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/common/IdentityTransformer.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.transform.Transformer;
+
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java
Patch:
@@ -23,8 +23,8 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieOperation;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordMerger;
 import org.apache.hudi.common.model.HoodieRecordLocation;
+import org.apache.hudi.common.model.HoodieRecordMerger;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.HoodieRecordUtils;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bucket/BucketStreamWriteFunction.java
Patch:
@@ -26,12 +26,12 @@
 import org.apache.hudi.configuration.OptionsResolver;
 import org.apache.hudi.index.bucket.BucketIdentifier;
 import org.apache.hudi.sink.StreamWriteFunction;
+import org.apache.hudi.sink.utils.BucketIndexUtil;
 
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.state.FunctionInitializationContext;
 import org.apache.flink.streaming.api.functions.ProcessFunction;
 import org.apache.flink.util.Collector;
-import org.apache.hudi.sink.utils.BucketIndexUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bulk/sort/SortOperator.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hudi.sink.bulk.sort;
 
+import org.apache.hudi.adapter.Utils;
+
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.metrics.Gauge;
 import org.apache.flink.runtime.memory.MemoryManager;
@@ -36,7 +38,6 @@
 import org.apache.flink.table.runtime.typeutils.BinaryRowDataSerializer;
 import org.apache.flink.table.runtime.util.StreamRecordCollector;
 import org.apache.flink.util.MutableObjectIterator;
-import org.apache.hudi.adapter.Utils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/partitioner/BucketIndexPartitioner.java
Patch:
@@ -21,9 +21,9 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.util.Functions;
 import org.apache.hudi.index.bucket.BucketIdentifier;
+import org.apache.hudi.sink.utils.BucketIndexUtil;
 
 import org.apache.flink.api.common.functions.Partitioner;
-import org.apache.hudi.sink.utils.BucketIndexUtil;
 
 /**
  * Bucket index input partitioner.

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/ExpressionPredicates.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hudi.source;
 
+import org.apache.hudi.util.ImplicitTypeConverter;
+
 import org.apache.flink.table.expressions.CallExpression;
 import org.apache.flink.table.expressions.Expression;
 import org.apache.flink.table.expressions.FieldReferenceExpression;
@@ -26,7 +28,6 @@
 import org.apache.flink.table.functions.BuiltInFunctionDefinitions;
 import org.apache.flink.table.functions.FunctionDefinition;
 import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.hudi.util.ImplicitTypeConverter;
 import org.apache.parquet.filter2.predicate.FilterPredicate;
 import org.apache.parquet.filter2.predicate.Operators;
 import org.slf4j.Logger;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java
Patch:
@@ -46,13 +46,13 @@
 import org.apache.hudi.source.IncrementalInputSplits;
 import org.apache.hudi.source.StreamReadMonitoringFunction;
 import org.apache.hudi.source.StreamReadOperator;
+import org.apache.hudi.source.prune.DataPruner;
+import org.apache.hudi.source.prune.PartitionPruners;
+import org.apache.hudi.source.prune.PrimaryKeyPruners;
 import org.apache.hudi.source.rebalance.partitioner.StreamReadAppendPartitioner;
 import org.apache.hudi.source.rebalance.partitioner.StreamReadBucketIndexPartitioner;
 import org.apache.hudi.source.rebalance.selector.StreamReadAppendKeySelector;
 import org.apache.hudi.source.rebalance.selector.StreamReadBucketIndexKeySelector;
-import org.apache.hudi.source.prune.DataPruner;
-import org.apache.hudi.source.prune.PartitionPruners;
-import org.apache.hudi.source.prune.PrimaryKeyPruners;
 import org.apache.hudi.storage.StorageConfiguration;
 import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.storage.StoragePathInfo;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/RecordIterators.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.table.format;
 
-import org.apache.hudi.common.util.collection.ClosableIterator;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.collection.ClosableIterator;
 import org.apache.hudi.internal.schema.InternalSchema;
 import org.apache.hudi.source.ExpressionPredicates.Predicate;
 import org.apache.hudi.table.format.cow.ParquetSplitReaderUtil;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java
Patch:
@@ -27,9 +27,9 @@
 import org.apache.hudi.common.model.HoodieRecordMerger;
 import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;
 import org.apache.hudi.common.table.log.InstantRange;
-import org.apache.hudi.common.util.collection.ClosableIterator;
 import org.apache.hudi.common.util.HoodieRecordUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.collection.ClosableIterator;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.configuration.FlinkOptions;
 import org.apache.hudi.configuration.HadoopConfigurations;
@@ -68,13 +68,13 @@
 
 import java.io.IOException;
 import java.util.Arrays;
-import java.util.stream.Collectors;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Set;
 import java.util.function.Function;
+import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
 import static org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_COMMIT_TIME_COL_POS;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/RowDataCastProjection.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.flink.table.types.logical.LogicalType;
 
 import javax.annotation.Nullable;
+
 import java.util.stream.IntStream;
 
 /**

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/RowDataProjection.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.flink.table.types.logical.RowType;
 
 import javax.annotation.Nullable;
+
 import java.io.Serializable;
 import java.util.Arrays;
 import java.util.List;

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/TestWriterWithPartitionTTl.java
Patch:
@@ -20,7 +20,6 @@
 
 package org.apache.hudi.sink;
 
-import org.apache.flink.configuration.Configuration;
 import org.apache.hudi.avro.model.HoodieReplaceCommitMetadata;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
@@ -31,6 +30,8 @@
 import org.apache.hudi.table.action.ttl.strategy.KeepByTimeStrategy;
 import org.apache.hudi.util.StreamerUtil;
 import org.apache.hudi.utils.TestData;
+
+import org.apache.flink.configuration.Configuration;
 import org.junit.jupiter.api.Test;
 
 import static org.apache.hudi.common.table.timeline.HoodieInstantTimeGenerator.fixInstantTimeCompatibility;

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/cluster/ITTestFlinkConsistentHashingClustering.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.avro.model.HoodieClusteringPlan;
 import org.apache.hudi.client.HoodieFlinkWriteClient;
+import org.apache.hudi.client.clustering.plan.strategy.FlinkConsistentBucketClusteringPlanStrategy;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -33,7 +34,6 @@
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.sink.clustering.FlinkClusteringConfig;
 import org.apache.hudi.table.HoodieFlinkTable;
-import org.apache.hudi.client.clustering.plan.strategy.FlinkConsistentBucketClusteringPlanStrategy;
 import org.apache.hudi.util.CompactionUtil;
 import org.apache.hudi.util.FlinkWriteClients;
 import org.apache.hudi.util.StreamerUtil;

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/cluster/ITTestHoodieFlinkClustering.java
Patch:
@@ -81,8 +81,8 @@
 import java.util.concurrent.TimeUnit;
 import java.util.stream.Collectors;
 
-import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;
+import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertThrows;
 import static org.junit.jupiter.api.Assertions.assertTrue;

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/source/TestExpressionEvaluators.java
Patch:
@@ -33,7 +33,6 @@
 import org.apache.flink.table.functions.FunctionDefinition;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.RowType;
-
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.MethodSource;
@@ -44,8 +43,8 @@
 import java.util.Map;
 import java.util.stream.Stream;
 
-import static org.apache.hudi.source.prune.DataPruner.convertColumnStats;
 import static org.apache.hudi.source.ExpressionEvaluators.fromExpression;
+import static org.apache.hudi.source.prune.DataPruner.convertColumnStats;
 import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/source/TestExpressionPredicates.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.source;
 
-import org.apache.flink.table.types.DataType;
 import org.apache.hudi.source.ExpressionPredicates.And;
 import org.apache.hudi.source.ExpressionPredicates.Equals;
 import org.apache.hudi.source.ExpressionPredicates.GreaterThan;
@@ -37,6 +36,7 @@
 import org.apache.flink.table.expressions.ResolvedExpression;
 import org.apache.flink.table.expressions.ValueLiteralExpression;
 import org.apache.flink.table.functions.BuiltInFunctionDefinitions;
+import org.apache.flink.table.types.DataType;
 import org.apache.parquet.filter2.predicate.Operators.Eq;
 import org.apache.parquet.filter2.predicate.Operators.Gt;
 import org.apache.parquet.filter2.predicate.Operators.IntColumn;
@@ -46,10 +46,10 @@
 import org.junit.jupiter.params.provider.Arguments;
 import org.junit.jupiter.params.provider.MethodSource;
 
+import java.math.BigDecimal;
 import java.time.LocalDate;
 import java.time.LocalDateTime;
 import java.time.LocalTime;
-import java.math.BigDecimal;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/utils/FlinkMiniCluster.java
Patch:
@@ -22,12 +22,10 @@
 import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;
 import org.apache.flink.test.util.AbstractTestBase;
 import org.apache.flink.test.util.MiniClusterWithClientResource;
-
 import org.junit.jupiter.api.extension.AfterAllCallback;
 import org.junit.jupiter.api.extension.AfterEachCallback;
 import org.junit.jupiter.api.extension.BeforeAllCallback;
 import org.junit.jupiter.api.extension.ExtensionContext;
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/utils/TestClusteringUtil.java
Patch:
@@ -39,12 +39,11 @@
 import org.apache.hudi.util.FlinkWriteClients;
 import org.apache.hudi.util.StreamerUtil;
 
+import org.apache.flink.configuration.Configuration;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.io.TempDir;
 
-import org.apache.flink.configuration.Configuration;
-
 import java.io.File;
 import java.io.IOException;
 import java.util.Collections;

File: hudi-gcp/src/main/java/org/apache/hudi/gcp/bigquery/HoodieBigQuerySyncClient.java
Patch:
@@ -40,11 +40,11 @@
 import com.google.cloud.bigquery.QueryJobConfiguration;
 import com.google.cloud.bigquery.Schema;
 import com.google.cloud.bigquery.StandardSQLTypeName;
+import com.google.cloud.bigquery.StandardTableDefinition;
 import com.google.cloud.bigquery.Table;
 import com.google.cloud.bigquery.TableId;
 import com.google.cloud.bigquery.TableInfo;
 import com.google.cloud.bigquery.ViewDefinition;
-import com.google.cloud.bigquery.StandardTableDefinition;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: hudi-gcp/src/test/java/org/apache/hudi/gcp/bigquery/TestBigQuerySyncToolArgs.java
Patch:
@@ -29,10 +29,10 @@
 import static org.apache.hudi.gcp.bigquery.BigQuerySyncConfig.BIGQUERY_SYNC_PARTITION_FIELDS;
 import static org.apache.hudi.gcp.bigquery.BigQuerySyncConfig.BIGQUERY_SYNC_PROJECT_ID;
 import static org.apache.hudi.gcp.bigquery.BigQuerySyncConfig.BIGQUERY_SYNC_REQUIRE_PARTITION_FILTER;
-import static org.apache.hudi.gcp.bigquery.BigQuerySyncConfig.BIGQUERY_SYNC_USE_BQ_MANIFEST_FILE;
 import static org.apache.hudi.gcp.bigquery.BigQuerySyncConfig.BIGQUERY_SYNC_SOURCE_URI;
 import static org.apache.hudi.gcp.bigquery.BigQuerySyncConfig.BIGQUERY_SYNC_SOURCE_URI_PREFIX;
 import static org.apache.hudi.gcp.bigquery.BigQuerySyncConfig.BIGQUERY_SYNC_TABLE_NAME;
+import static org.apache.hudi.gcp.bigquery.BigQuerySyncConfig.BIGQUERY_SYNC_USE_BQ_MANIFEST_FILE;
 import static org.apache.hudi.gcp.bigquery.BigQuerySyncConfig.BIGQUERY_SYNC_USE_FILE_LISTING_FROM_METADATA;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 

File: hudi-hadoop-common/src/main/java/org/apache/hudi/hadoop/fs/HoodieSerializableFileStatus.java
Patch:
@@ -23,8 +23,8 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.permission.FsPermission;
 
-import java.io.Serializable;
 import java.io.IOException;
+import java.io.Serializable;
 import java.util.Arrays;
 import java.util.stream.Collectors;
 

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/table/timeline/TestHoodieInstant.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.common.model.HoodieTimelineTimeZone;
 import org.apache.hudi.common.testutils.HoodieCommonTestHarness;
 import org.apache.hudi.common.util.Option;
+
 import org.junit.jupiter.api.Test;
 
 import java.io.IOException;

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java
Patch:
@@ -18,17 +18,17 @@
 
 package org.apache.hudi.common.testutils;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.view.HoodieTableFileSystemView;
 import org.apache.hudi.common.table.view.SyncableFileSystemView;
 import org.apache.hudi.exception.HoodieIOException;
-
 import org.apache.hudi.storage.HoodieStorage;
 import org.apache.hudi.storage.StorageConfiguration;
+
+import org.apache.hadoop.conf.Configuration;
 import org.junit.jupiter.api.io.TempDir;
 
 import java.io.IOException;

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/util/collection/TestBitCaskDiskMap.java
Patch:
@@ -24,9 +24,9 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.AvroBinaryTestPayload;
 import org.apache.hudi.common.testutils.HoodieCommonTestHarness;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.SchemaTestUtil;
 import org.apache.hudi.common.testutils.SpillableMapTestUtils;
 import org.apache.hudi.common.util.HoodieRecordSizeEstimator;

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/util/collection/TestExternalSpillableMap.java
Patch:
@@ -25,8 +25,8 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.HoodieCommonTestHarness;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.SchemaTestUtil;
 import org.apache.hudi.common.testutils.SpillableMapTestUtils;
 import org.apache.hudi.common.util.DefaultSizeEstimator;

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/util/collection/TestRocksDbDiskMap.java
Patch:
@@ -24,8 +24,8 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.HoodieCommonTestHarness;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.SchemaTestUtil;
 import org.apache.hudi.common.testutils.SpillableMapTestUtils;
 import org.apache.hudi.common.util.Option;

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HiveHoodieTableFileIndex.java
Patch:
@@ -25,8 +25,8 @@
 import org.apache.hudi.common.model.HoodieTableQueryType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.storage.StoragePathInfo;
 import org.apache.hudi.storage.StoragePath;
+import org.apache.hudi.storage.StoragePathInfo;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieCopyOnWriteTableInputFormat.java
Patch:
@@ -65,7 +65,6 @@
 import java.util.stream.Collectors;
 
 import static org.apache.hudi.common.config.HoodieMetadataConfig.ENABLE;
-import static org.apache.hudi.hadoop.fs.HadoopFSUtils.convertToStoragePath;
 
 /**
  * Base implementation of the Hive's {@link FileInputFormat} allowing for reading of Hudi's

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieHFileInputFormat.java
Patch:
@@ -18,6 +18,9 @@
 
 package org.apache.hudi.hadoop;
 
+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;
+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;
+
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.ArrayWritable;
@@ -26,8 +29,6 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
-import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;
-import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;
 
 import java.io.IOException;
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormatBase.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hudi.hadoop;
 
+import org.apache.hudi.hadoop.realtime.HoodieMergeOnReadTableInputFormat;
+
 import org.apache.hadoop.conf.Configurable;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
@@ -28,7 +30,6 @@
 import org.apache.hadoop.mapred.FileSplit;
 import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
-import org.apache.hudi.hadoop.realtime.HoodieMergeOnReadTableInputFormat;
 
 import java.io.IOException;
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/RealtimeFileStatus.java
Patch:
@@ -18,13 +18,14 @@
 
 package org.apache.hudi.hadoop;
 
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.Path;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.hadoop.realtime.HoodieRealtimePath;
 import org.apache.hudi.hadoop.realtime.HoodieVirtualKeyInfo;
 
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+
 import java.io.IOException;
 import java.util.List;
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieRealtimeBootstrapBaseFileSplit.java
Patch:
@@ -18,11 +18,12 @@
 
 package org.apache.hudi.hadoop.realtime;
 
-import org.apache.hadoop.mapred.FileSplit;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.hadoop.BootstrapBaseFileSplit;
 
+import org.apache.hadoop.mapred.FileSplit;
+
 import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.IOException;

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieRealtimePath.java
Patch:
@@ -18,11 +18,12 @@
 
 package org.apache.hudi.hadoop.realtime;
 
-import org.apache.hadoop.fs.Path;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.hadoop.PathWithBootstrapFileStatus;
 
+import org.apache.hadoop.fs.Path;
+
 import java.util.List;
 
 /**

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HiveAvroSerializer.java
Patch:
@@ -49,7 +49,6 @@
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo;
 import org.apache.hadoop.io.ArrayWritable;
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieHiveUtils.java
Patch:
@@ -28,7 +28,6 @@
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapreduce.JobContext;
-
 import org.apache.hive.common.util.HiveVersionInfo;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestAnnotation.java
Patch:
@@ -18,8 +18,10 @@
 
 package org.apache.hudi.hadoop;
 
-import org.junit.jupiter.api.Test;
 import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;
+
+import org.junit.jupiter.api.Test;
+
 import java.lang.annotation.Annotation;
 
 import static org.junit.jupiter.api.Assertions.assertTrue;

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/utils/TestHiveAvroSerializer.java
Patch:
@@ -18,11 +18,11 @@
 
 package org.apache.hudi.hadoop.utils;
 
-import org.apache.avro.generic.GenericArray;
 import org.apache.hudi.avro.HoodieAvroUtils;
 
 import org.apache.avro.LogicalTypes;
 import org.apache.avro.Schema;
+import org.apache.avro.generic.GenericArray;
 import org.apache.avro.generic.GenericData;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector;

File: hudi-kafka-connect/src/test/java/org/apache/hudi/connect/TestConnectTransactionParticipant.java
Patch:
@@ -24,9 +24,9 @@
 import org.apache.hudi.connect.transaction.TransactionCoordinator;
 import org.apache.hudi.connect.writers.KafkaConnectConfigs;
 import org.apache.hudi.exception.HoodieException;
+import org.apache.hudi.helper.MockKafkaConnect;
 import org.apache.hudi.helper.MockKafkaControlAgent;
 import org.apache.hudi.helper.TestHudiWriterProvider;
-import org.apache.hudi.helper.MockKafkaConnect;
 
 import org.apache.kafka.common.TopicPartition;
 import org.junit.jupiter.api.BeforeEach;

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -40,8 +40,8 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.TableNotFoundException;
-import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.storage.HoodieStorage;
+import org.apache.hudi.storage.StoragePath;
 import org.apache.hudi.table.BulkInsertPartitioner;
 
 import org.apache.avro.generic.GenericRecord;

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/commit/BaseDatasetBulkInsertCommitActionExecutor.java
Patch:
@@ -40,6 +40,7 @@
 import org.apache.hudi.table.BulkInsertPartitioner;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
+
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/commit/DatasetBulkInsertCommitActionExecutor.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.internal.DataSourceInternalWriterHelper;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
+
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/commit/DatasetBulkInsertOverwriteCommitActionExecutor.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hudi.config.HoodieInternalConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.data.HoodieJavaPairRDD;
+
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
 

File: hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/payload/AWSDmsAvroPayload.java
Patch:
@@ -18,9 +18,10 @@
 
 package org.apache.hudi.payload;
 
-import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.common.util.Option;
 
+import org.apache.avro.generic.GenericRecord;
+
 /**
  * Provides support for seamlessly applying changes captured via Amazon Database Migration Service onto S3.
  *

File: hudi-hadoop-common/src/main/java/org/apache/hudi/io/hadoop/HoodieAvroParquetReader.java
Patch:
@@ -166,7 +166,7 @@ private ClosableIterator<IndexedRecord> getIndexedRecordIteratorInternal(Schema
     // NOTE: We have to set both Avro read-schema and projection schema to make
     //       sure that in case the file-schema is not equal to read-schema we'd still
     //       be able to read that file (in case projection is a proper one)
-    Configuration hadoopConf = storage.getConf().unwrapAs(Configuration.class);
+    Configuration hadoopConf = storage.getConf().unwrapCopyAs(Configuration.class);
     if (!requestedSchema.isPresent()) {
       AvroReadSupport.setAvroReadSchema(hadoopConf, schema);
       AvroReadSupport.setRequestedProjection(hadoopConf, schema);

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -269,6 +269,7 @@ public void deletePending(HoodieInstant instant) {
 
   public void deleteCompletedRollback(HoodieInstant instant) {
     ValidationUtils.checkArgument(instant.isCompleted());
+    ValidationUtils.checkArgument(Objects.equals(instant.getAction(), HoodieTimeline.ROLLBACK_ACTION));
     deleteInstantFile(instant);
   }
 

File: hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/HoodieFlinkQuickstart.java
Patch:
@@ -40,6 +40,7 @@
 import org.apache.flink.table.catalog.exceptions.TableNotExistException;
 import org.apache.flink.types.Row;
 import org.apache.hudi.common.model.HoodieTableType;
+import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.configuration.FlinkOptions;
 import org.apache.hudi.examples.quickstart.factory.CollectSinkTableFactory;
 import org.apache.hudi.examples.quickstart.utils.QuickstartConfigurations;
@@ -138,6 +139,7 @@ public void createHudiTable(String tablePath, String tableName,
         .option(FlinkOptions.PATH, tablePath)
         .option(FlinkOptions.READ_AS_STREAMING, true)
         .option(FlinkOptions.TABLE_TYPE, tableType)
+        .option(HoodieWriteConfig.ALLOW_EMPTY_COMMIT.key(), false)
         .end();
     streamTableEnv.executeSql(hoodieTableDDL);
   }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/OptionsResolver.java
Patch:
@@ -388,7 +388,7 @@ public static ConflictResolutionStrategy getConflictResolutionStrategy(Configura
    * Returns whether to commit even when current batch has no data, for flink defaults false
    */
   public static boolean allowCommitOnEmptyBatch(Configuration conf) {
-    return conf.getBoolean(HoodieWriteConfig.ALLOW_EMPTY_COMMIT.key(), false);
+    return conf.getBoolean(HoodieWriteConfig.ALLOW_EMPTY_COMMIT.key(), HoodieWriteConfig.ALLOW_EMPTY_COMMIT.defaultValue());
   }
 
   /**

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/ITTestHoodieDataSource.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.common.table.cdc.HoodieCDCSupplementalLoggingMode;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.CollectionUtils;
+import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.configuration.FlinkOptions;
 import org.apache.hudi.metadata.HoodieTableMetadata;
 import org.apache.hudi.table.catalog.HoodieCatalogTestUtils;
@@ -206,6 +207,7 @@ void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {
         .option(FlinkOptions.READ_AS_STREAMING, true)
         .option(FlinkOptions.READ_STREAMING_SKIP_COMPACT, false)
         .option(FlinkOptions.TABLE_TYPE, tableType)
+        .option(HoodieWriteConfig.ALLOW_EMPTY_COMMIT.key(), false)
         .end();
     streamTableEnv.executeSql(hoodieTableDDL);
     String insertInto = "insert into t1 select * from source";
@@ -798,6 +800,7 @@ void testInsertOverwrite(String indexType, HoodieTableType tableType) {
         .option(FlinkOptions.PATH, tempFile.getAbsolutePath())
         .option(FlinkOptions.TABLE_TYPE, tableType)
         .option(FlinkOptions.INDEX_TYPE, indexType)
+        .option(HoodieWriteConfig.ALLOW_EMPTY_COMMIT.key(), false)
         .end();
     tableEnv.executeSql(hoodieTableDDL);
 

File: hudi-client/hudi-spark-client/src/main/scala/org/apache/hudi/BaseSparkInternalRowReaderContext.java
Patch:
@@ -48,6 +48,7 @@
 
 import static org.apache.hudi.common.model.HoodieRecord.RECORD_KEY_METADATA_FIELD;
 import static org.apache.hudi.common.model.HoodieRecordMerger.DEFAULT_MERGER_STRATEGY_UUID;
+import static org.apache.hudi.common.model.HoodieRecordMerger.OVERWRITE_MERGER_STRATEGY_UUID;
 import static org.apache.spark.sql.HoodieInternalRowUtils.getCachedSchema;
 
 /**
@@ -65,6 +66,8 @@ public HoodieRecordMerger getRecordMerger(String mergerStrategy) {
     switch (mergerStrategy) {
       case DEFAULT_MERGER_STRATEGY_UUID:
         return new HoodieSparkRecordMerger();
+      case OVERWRITE_MERGER_STRATEGY_UUID:
+        return new OverwriteWithLatestSparkMerger();
       default:
         throw new HoodieException("The merger strategy UUID is not supported: " + mergerStrategy);
     }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/partitioner/profile/DeltaWriteProfile.java
Patch:
@@ -34,7 +34,7 @@
 import java.util.stream.Collectors;
 
 /**
- * WriteProfile for MERGE_ON_READ table type, this allows auto correction of small parquet files to larger ones
+ * DeltaWriteProfile for MERGE_ON_READ table type, this allows auto correction of small parquet files to larger ones
  * without the need for an index in the logFile.
  *
  * <p>Note: assumes the index can always index log files for Flink write.

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/visitor/NameToIDVisitor.java
Patch:
@@ -95,14 +95,14 @@ public Map<String, Integer> field(Types.Field field, Map<String, Integer> fieldR
 
   @Override
   public Map<String, Integer> array(Types.ArrayType array, Map<String, Integer> elementResult) {
-    nameToId.put(createFullName("element", fieldNames), array.elementId());
+    nameToId.put(createFullName(InternalSchema.ARRAY_ELEMENT, fieldNames), array.elementId());
     return nameToId;
   }
 
   @Override
   public Map<String, Integer> map(Types.MapType map, Map<String, Integer> keyResult, Map<String, Integer> valueResult) {
-    nameToId.put(createFullName("key", fieldNames), map.keyId());
-    nameToId.put(createFullName("value", fieldNames), map.valueId());
+    nameToId.put(createFullName(InternalSchema.MAP_KEY, fieldNames), map.keyId());
+    nameToId.put(createFullName(InternalSchema.MAP_VALUE, fieldNames), map.valueId());
     return nameToId;
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java
Patch:
@@ -97,7 +97,7 @@
  */
 public abstract class BaseHoodieTableServiceClient<I, T, O> extends BaseHoodieClient implements RunsTableService {
 
-  private static final Logger LOG = LoggerFactory.getLogger(BaseHoodieWriteClient.class);
+  private static final Logger LOG = LoggerFactory.getLogger(BaseHoodieTableServiceClient.class);
 
   protected transient Timer.Context compactionTimer;
   protected transient Timer.Context clusteringTimer;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java
Patch:
@@ -396,8 +396,8 @@ public static <R> HoodieRecord<R> createNewTaggedHoodieRecord(HoodieRecord<R> ol
    */
   public static String getPartitionNameFromPartitionType(MetadataPartitionType partitionType, HoodieTableMetaClient metaClient, String indexName) {
     if (MetadataPartitionType.FUNCTIONAL_INDEX.equals(partitionType)) {
-      checkArgument(metaClient.getFunctionalIndexMetadata().isPresent(), "Index definition is not present");
-      return metaClient.getFunctionalIndexMetadata().get().getIndexDefinitions().get(indexName).getIndexName();
+      checkArgument(metaClient.getIndexMetadata().isPresent(), "Index definition is not present");
+      return metaClient.getIndexMetadata().get().getIndexDefinitions().get(indexName).getIndexName();
     }
     return partitionType.getPartitionPath();
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/ScheduleIndexActionExecutor.java
Patch:
@@ -132,7 +132,7 @@ public Option<HoodieIndexPlan> execute() {
 
   private HoodieIndexPartitionInfo buildIndexPartitionInfo(MetadataPartitionType partitionType, HoodieInstant indexUptoInstant) {
     // for functional index, we need to pass the index name as the partition name
-    String partitionName = MetadataPartitionType.FUNCTIONAL_INDEX.equals(partitionType) ? config.getFunctionalIndexConfig().getIndexName() : partitionType.getPartitionPath();
+    String partitionName = MetadataPartitionType.FUNCTIONAL_INDEX.equals(partitionType) ? config.getIndexingConfig().getIndexName() : partitionType.getPartitionPath();
     return new HoodieIndexPartitionInfo(LATEST_INDEX_PLAN_VERSION, partitionName, indexUptoInstant.getTimestamp(), Collections.emptyMap());
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/functional/BaseHoodieFunctionalIndexClient.java
Patch:
@@ -49,7 +49,7 @@ public void register(HoodieTableMetaClient metaClient, String indexName, String
             + StoragePath.SEPARATOR + HoodieTableMetaClient.INDEX_DEFINITION_FOLDER_NAME
             + StoragePath.SEPARATOR + HoodieTableMetaClient.INDEX_DEFINITION_FILE_NAME);
     // build HoodieFunctionalIndexMetadata and then add to index definition file
-    metaClient.buildFunctionalIndexDefinition(indexMetaPath, indexName, indexType, columns, options);
+    metaClient.buildIndexDefinition(indexMetaPath, indexName, indexType, columns, options);
     // update table config if necessary
     if (!metaClient.getTableConfig().getProps().containsKey(HoodieTableConfig.INDEX_DEFINITION_PATH) || !metaClient.getTableConfig().getIndexDefinitionPath().isPresent()) {
       metaClient.getTableConfig().setValue(HoodieTableConfig.INDEX_DEFINITION_PATH, indexMetaPath);

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hudi.common.model.HoodieRecordMerger;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
-import org.apache.hudi.exception.HoodieClusteringException;
+import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.io.storage.HoodieFileReader;
 
 import org.apache.avro.Schema;
@@ -84,7 +84,7 @@ private boolean hasNextInternal() {
           return true;
         }
       } catch (IOException e) {
-        throw new HoodieClusteringException("Failed to wrapIntoHoodieRecordPayloadWithParams: " + e.getMessage());
+        throw new HoodieIOException("Failed to wrapIntoHoodieRecordPayloadWithParams: " + e.getMessage());
       }
     }
     return super.doHasNext();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGenUtils.java
Patch:
@@ -268,6 +268,8 @@ public static List<String> getRecordKeyFields(TypedProperties props) {
    * @return true if record keys need to be auto generated. false otherwise.
    */
   public static boolean isAutoGeneratedRecordKeysEnabled(TypedProperties props) {
-    return !props.containsKey(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key());
+    return !props.containsKey(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key())
+        || props.getProperty(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key()).equals(StringUtils.EMPTY_STRING);
+    // spark-sql sets record key config to empty string for update, and couple of other statements.
   }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/factory/HoodieSparkKeyGeneratorFactory.java
Patch:
@@ -88,6 +88,9 @@ public static KeyGenerator createKeyGenerator(String keyGeneratorClass, TypedPro
         //Need to prevent overwriting the keygen for spark sql merge into because we need to extract
         //the recordkey from the meta cols if it exists. Sql keygen will use pkless keygen if needed.
         && !props.getBoolean(SPARK_SQL_MERGE_INTO_PREPPED_KEY, false);
+    if (autoRecordKeyGen) {
+      props.remove(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key());
+    }
     KeyGenerator keyGenerator = (KeyGenerator) ReflectionUtils.loadClass(keyGeneratorClass, props);
     if (autoRecordKeyGen) {
       return new AutoRecordGenWrapperKeyGenerator(props, (BuiltinKeyGenerator) keyGenerator);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java
Patch:
@@ -129,7 +129,7 @@ private HoodieCompactionPlan scheduleCompaction() {
 
   private Option<Pair<Integer, String>> getLatestDeltaCommitInfo() {
     Option<Pair<HoodieTimeline, HoodieInstant>> deltaCommitsInfo =
-        CompactionUtils.getDeltaCommitsSinceLatestCompaction(table.getActiveTimeline());
+        CompactionUtils.getCompletedDeltaCommitsSinceLatestCompaction(table.getActiveTimeline());
     if (deltaCommitsInfo.isPresent()) {
       return Option.of(Pair.of(
           deltaCommitsInfo.get().getLeft().countInstants(),

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/lookup/HoodieLookupFunction.java
Patch:
@@ -47,7 +47,7 @@
 import java.util.Map;
 
 /**
- * Lookup function for filesystem connector tables.
+ * Lookup function for Hoodie dimension table.
  *
  * <p>Note: reference Flink FileSystemLookupFunction to avoid additional connector jar dependencies.
  */

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java
Patch:
@@ -616,7 +616,7 @@ private int[] getLookupKeys(int[][] keys) {
     for (int[] key : keys) {
       if (key.length > 1) {
         throw new UnsupportedOperationException(
-            "Hive lookup can not support nested key now.");
+            "Hoodie lookup can not support nested key now.");
       }
       keyIndices[i] = key[0];
       i++;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/AverageRecordSizeUtils.java
Patch:
@@ -29,7 +29,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.io.IOException;
 import java.util.Iterator;
 import java.util.concurrent.atomic.AtomicLong;
 
@@ -80,9 +79,9 @@ static long averageBytesPerRecord(HoodieTimeline commitTimeline, HoodieWriteConf
               break;
             }
           }
-        } catch (IOException ioe) {
+        } catch (Throwable t) {
           // make this fail safe.
-          LOG.error("Error trying to compute average bytes/record ", ioe);
+          LOG.error("Error trying to compute average bytes/record ", t);
         }
       }
     }

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java
Patch:
@@ -320,7 +320,7 @@ private static String printAllCompactions(HoodieDefaultTimeline timeline,
         .filter(pair -> pair.getRight() != null)
         .collect(Collectors.toList());
 
-    Set<String> committedInstants = timeline.getCommitTimeline().filterCompletedInstants()
+    Set<String> committedInstants = timeline.getCommitAndReplaceTimeline().filterCompletedInstants()
         .getInstantsAsStream().map(HoodieInstant::getTimestamp).collect(Collectors.toSet());
 
     List<Comparable[]> rows = new ArrayList<>();

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/FileSystemViewCommand.java
Patch:
@@ -247,7 +247,7 @@ private HoodieTableFileSystemView buildFileSystemView(String globRegex, String m
 
     HoodieTimeline timeline;
     if (basefileOnly) {
-      timeline = metaClient.getActiveTimeline().getCommitTimeline();
+      timeline = metaClient.getActiveTimeline().getCommitAndReplaceTimeline();
     } else if (excludeCompaction) {
       timeline = metaClient.getActiveTimeline().getCommitsTimeline();
     } else {

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java
Patch:
@@ -232,7 +232,7 @@ storage, new StoragePath(logFilePathPattern)).stream()
               .withReaderSchema(readerSchema)
               .withLatestInstantTime(
                   client.getActiveTimeline()
-                      .getCommitTimeline().lastInstant().get().getTimestamp())
+                      .getCommitAndReplaceTimeline().lastInstant().get().getTimestamp())
               .withReverseReader(
                   Boolean.parseBoolean(
                       HoodieReaderConfig.COMPACTION_REVERSE_LOG_READ_ENABLE.defaultValue()))

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java
Patch:
@@ -118,7 +118,7 @@ public String addPartitionMeta(
 
     HoodieTableMetaClient client = HoodieCLI.getTableMetaClient();
     String latestCommit =
-        client.getActiveTimeline().getCommitTimeline().lastInstant().get().getTimestamp();
+        client.getActiveTimeline().getCommitAndReplaceTimeline().lastInstant().get().getTimestamp();
     List<String> partitionPaths =
         FSUtils.getAllPartitionFoldersThreeLevelsDown(HoodieCLI.storage, client.getBasePath());
     StoragePath basePath = client.getBasePathV2();
@@ -240,7 +240,7 @@ public String migratePartitionMeta(
       Option<StoragePath> baseFormatFile =
           HoodiePartitionMetadata.baseFormatMetaPathIfExists(HoodieCLI.storage, partition);
       String latestCommit =
-          client.getActiveTimeline().getCommitTimeline().lastInstant().get().getTimestamp();
+          client.getActiveTimeline().getCommitAndReplaceTimeline().lastInstant().get().getTimestamp();
 
       String[] row = new String[] {
           partitionPath,

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/StatsCommand.java
Patch:
@@ -69,7 +69,7 @@ public String writeAmplificationStats(
     long totalRecordsWritten = 0;
 
     HoodieActiveTimeline activeTimeline = HoodieCLI.getTableMetaClient().getActiveTimeline();
-    HoodieTimeline timeline = activeTimeline.getCommitTimeline().filterCompletedInstants();
+    HoodieTimeline timeline = activeTimeline.getCommitAndReplaceTimeline().filterCompletedInstants();
 
     List<Comparable[]> rows = new ArrayList<>();
     DecimalFormat df = new DecimalFormat("#.00");

File: hudi-cli/src/main/java/org/apache/hudi/cli/utils/CommitUtil.java
Patch:
@@ -36,7 +36,7 @@ public class CommitUtil {
 
   public static long countNewRecords(HoodieTableMetaClient metaClient, List<String> commitsToCatchup) throws IOException {
     long totalNew = 0;
-    HoodieTimeline timeline = metaClient.reloadActiveTimeline().getCommitTimeline().filterCompletedInstants();
+    HoodieTimeline timeline = metaClient.reloadActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants();
     for (String commit : commitsToCatchup) {
       HoodieCommitMetadata c = HoodieCommitMetadata.fromBytes(
           timeline.getInstantDetails(new HoodieInstant(false, HoodieTimeline.COMMIT_ACTION, commit)).get(),

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestTableCommand.java
Patch:
@@ -207,7 +207,7 @@ private void testRefreshCommand(String command) throws IOException {
     assertTrue(prepareTable());
 
     HoodieTimeline timeline =
-        HoodieCLI.getTableMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants();
+        HoodieCLI.getTableMetaClient().getActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants();
     assertEquals(0, timeline.countInstants(), "There should have no instant at first");
 
     // generate four savepoints
@@ -218,14 +218,14 @@ private void testRefreshCommand(String command) throws IOException {
 
     // Before refresh, no instant
     timeline =
-        HoodieCLI.getTableMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants();
+        HoodieCLI.getTableMetaClient().getActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants();
     assertEquals(0, timeline.countInstants(), "there should have no instant");
 
     Object result = shell.evaluate(() -> command);
     assertTrue(ShellEvaluationResultUtil.isSuccess(result));
 
     timeline =
-        HoodieCLI.getTableMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants();
+        HoodieCLI.getTableMetaClient().getActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants();
 
     // After refresh, there are 4 instants
     assertEquals(4, timeline.countInstants(), "there should have 4 instants");

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestSavepointsCommand.java
Patch:
@@ -137,7 +137,7 @@ public void testRollbackToSavepoint() throws IOException {
     assertEquals(1, timeline.getRestoreTimeline().countInstants());
 
     // 103 instant had rollback
-    assertFalse(timeline.getCommitTimeline().containsInstant(
+    assertFalse(timeline.getCommitAndReplaceTimeline().containsInstant(
         new HoodieInstant(HoodieInstant.State.COMPLETED, "commit", "103")));
   }
 
@@ -182,9 +182,9 @@ public void testRollbackToSavepointWithMetadataTableEnable() throws Exception {
     assertEquals(1, timeline.getRestoreTimeline().countInstants());
 
     // 103 and 104 instant had rollback
-    assertFalse(timeline.getCommitTimeline().containsInstant(
+    assertFalse(timeline.getCommitAndReplaceTimeline().containsInstant(
         new HoodieInstant(HoodieInstant.State.COMPLETED, "commit", "103")));
-    assertFalse(timeline.getCommitTimeline().containsInstant(
+    assertFalse(timeline.getCommitAndReplaceTimeline().containsInstant(
         new HoodieInstant(HoodieInstant.State.COMPLETED, "commit", "104")));
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bucket/ConsistentBucketIndexUtils.java
Patch:
@@ -143,7 +143,7 @@ public static Option<HoodieConsistentHashingMetadata> loadMetadata(HoodieTable t
           && maxCommitMetaFileTs.equals(HoodieConsistentHashingMetadata.getTimestampFromFile(maxMetadataFile.getPath().getName()))) {
         return loadMetadataFromGivenFile(table, maxMetadataFile);
       }
-      HoodieTimeline completedCommits = metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants();
+      HoodieTimeline completedCommits = metaClient.getActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants();
 
       // fix the in-consistency between un-committed and committed hashing metadata files.
       List<FileStatus> fixed = new ArrayList<>();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -1429,7 +1429,7 @@ protected void compactIfNecessary(BaseHoodieWriteClient writeClient) {
 
   protected void cleanIfNecessary(BaseHoodieWriteClient writeClient) {
     Option<HoodieInstant> lastCompletedCompactionInstant = metadataMetaClient.reloadActiveTimeline()
-        .getCommitTimeline().filterCompletedInstants().lastInstant();
+        .getCommitAndReplaceTimeline().filterCompletedInstants().lastInstant();
     if (lastCompletedCompactionInstant.isPresent()
         && metadataMetaClient.getActiveTimeline().filterCompletedInstants()
         .findInstantsAfter(lastCompletedCompactionInstant.get().getTimestamp()).countInstants() < 3) {

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java
Patch:
@@ -132,7 +132,7 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)
     // for new inserts, compute buckets depending on how many records we have for each partition
     Set<String> partitionPaths = profile.getPartitionPaths();
     long averageRecordSize =
-        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),
+        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants(),
             config);
     LOG.info("AvgRecordSize => " + averageRecordSize);
 

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/client/functional/TestHoodieJavaClientOnCopyOnWriteStorage.java
Patch:
@@ -517,7 +517,7 @@ private void testUpsertsInternal(HoodieWriteConfig config,
         0, 150);
 
     HoodieActiveTimeline activeTimeline = new HoodieActiveTimeline(metaClient, false);
-    List<HoodieInstant> instants = activeTimeline.getCommitTimeline().getInstants();
+    List<HoodieInstant> instants = activeTimeline.getCommitAndReplaceTimeline().getInstants();
     assertEquals(5, instants.size());
     assertEquals(new HoodieInstant(COMPLETED, COMMIT_ACTION, "001"),
         instants.get(0));

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/testutils/HoodieJavaClientTestHarness.java
Patch:
@@ -867,7 +867,7 @@ private List<WriteStatus> getWriteStatusAndVerifyDeleteOperation(String newCommi
 
     // verify that there is a commit
     HoodieTableMetaClient metaClient = createMetaClient();
-    HoodieTimeline timeline = new HoodieActiveTimeline(metaClient).getCommitTimeline();
+    HoodieTimeline timeline = new HoodieActiveTimeline(metaClient).getCommitAndReplaceTimeline();
 
     if (assertForCommit) {
       assertEquals(3, timeline.findInstantsAfter(initCommitTime, Integer.MAX_VALUE).countInstants(),

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestMultiFS.java
Patch:
@@ -135,7 +135,7 @@ public void readLocalWriteHDFS() throws Exception {
       // Read from hdfs
       FileSystem fs = HadoopFSUtils.getFs(dfsBasePath, HoodieTestUtils.getDefaultStorageConf());
       HoodieTableMetaClient metaClient = HoodieTestUtils.createMetaClient(HadoopFSUtils.getStorageConf(fs.getConf()), dfsBasePath);
-      HoodieTimeline timeline = new HoodieActiveTimeline(metaClient).getCommitTimeline();
+      HoodieTimeline timeline = new HoodieActiveTimeline(metaClient).getCommitAndReplaceTimeline();
       Dataset<Row> readRecords = HoodieClientTestUtils.readCommit(dfsBasePath, sqlContext, timeline, readCommitTime);
       assertEquals(readRecords.count(), records.size());
 
@@ -156,7 +156,7 @@ public void readLocalWriteHDFS() throws Exception {
       LOG.info("Reading from path: " + tablePath);
       fs = HadoopFSUtils.getFs(tablePath, HoodieTestUtils.getDefaultStorageConf());
       metaClient = HoodieTestUtils.createMetaClient(new HadoopStorageConfiguration(fs.getConf()), tablePath);
-      timeline = new HoodieActiveTimeline(metaClient).getCommitTimeline();
+      timeline = new HoodieActiveTimeline(metaClient).getCommitAndReplaceTimeline();
       Dataset<Row> localReadRecords =
           HoodieClientTestUtils.readCommit(tablePath, sqlContext, timeline, writeCommitTime);
       assertEquals(localReadRecords.count(), localRecords.size());

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java
Patch:
@@ -309,7 +309,7 @@ public void testCopyOnWriteTable(boolean shouldAllowDroppedColumns) throws Excep
         (String s, Integer a) -> evolvedRecords, SparkRDDWriteClient::insert, true, numRecords, 3 * numRecords, 6, false);
 
     // new commit
-    HoodieTimeline curTimeline = metaClient.reloadActiveTimeline().getCommitTimeline().filterCompletedInstants();
+    HoodieTimeline curTimeline = metaClient.reloadActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants();
     assertTrue(curTimeline.lastInstant().get().getTimestamp().equals("006"));
     checkReadRecords("000", 3 * numRecords);
 
@@ -333,7 +333,7 @@ public void testCopyOnWriteTable(boolean shouldAllowDroppedColumns) throws Excep
 
   private void checkReadRecords(String instantTime, int numExpectedRecords) throws IOException {
     if (tableType == HoodieTableType.COPY_ON_WRITE) {
-      HoodieTimeline timeline = metaClient.reloadActiveTimeline().getCommitTimeline();
+      HoodieTimeline timeline = metaClient.reloadActiveTimeline().getCommitAndReplaceTimeline();
       assertEquals(numExpectedRecords, HoodieClientTestUtils.countRecordsOptionallySince(jsc, basePath, sqlContext, timeline, Option.of(instantTime)));
     } else {
       // TODO: This code fails to read records under the following conditions:

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -683,7 +683,7 @@ private void testUpsertsInternal(HoodieWriteConfig config,
         0, 150);
 
     HoodieActiveTimeline activeTimeline = new HoodieActiveTimeline(metaClient, false);
-    List<HoodieInstant> instants = activeTimeline.getCommitTimeline().getInstants();
+    List<HoodieInstant> instants = activeTimeline.getCommitAndReplaceTimeline().getInstants();
     assertEquals(5, instants.size());
     assertEquals(new HoodieInstant(COMPLETED, COMMIT_ACTION, "001"),
         instants.get(0));

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java
Patch:
@@ -152,7 +152,7 @@ public static Pair<String, JavaRDD<WriteStatus>> insertFirstBigBatchForClientCle
     assertNoWriteErrors(statuses.collect());
     // verify that there is a commit
     metaClient = HoodieTableMetaClient.reload(metaClient);
-    HoodieTimeline timeline = new HoodieActiveTimeline(metaClient).getCommitTimeline();
+    HoodieTimeline timeline = new HoodieActiveTimeline(metaClient).getCommitAndReplaceTimeline();
     assertEquals(1, timeline.findInstantsAfter("000", Integer.MAX_VALUE).countInstants(), "Expecting a single commit.");
     // Should have 100 records in table (check using Index), all in locations marked at commit
     HoodieTable table = HoodieSparkTable.create(client.getConfig(), context, metaClient);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestHoodieMergeOnReadTable.java
Patch:
@@ -162,7 +162,7 @@ public void testUpsertPartitioner(boolean populateMetaFields) throws Exception {
       assertTrue(deltaCommit.isPresent());
       assertEquals("001", deltaCommit.get().getTimestamp(), "Delta commit should be 001");
 
-      Option<HoodieInstant> commit = metaClient.getActiveTimeline().getCommitTimeline().firstInstant();
+      Option<HoodieInstant> commit = metaClient.getActiveTimeline().getCommitAndReplaceTimeline().firstInstant();
       assertFalse(commit.isPresent());
 
       List<StoragePathInfo> allFiles = listAllBaseFilesInPath(hoodieTable);
@@ -196,7 +196,7 @@ public void testUpsertPartitioner(boolean populateMetaFields) throws Exception {
       assertTrue(deltaCommit.isPresent());
       assertEquals("002", deltaCommit.get().getTimestamp(), "Latest Delta commit should be 002");
 
-      commit = metaClient.getActiveTimeline().getCommitTimeline().firstInstant();
+      commit = metaClient.getActiveTimeline().getCommitAndReplaceTimeline().firstInstant();
       assertFalse(commit.isPresent());
 
       allFiles = listAllBaseFilesInPath(hoodieTable);
@@ -652,7 +652,7 @@ public void testHandleUpdateWithMultiplePartitions() throws Exception {
       assertTrue(deltaCommit.isPresent());
       assertEquals("001", deltaCommit.get().getTimestamp(), "Delta commit should be 001");
 
-      Option<HoodieInstant> commit = metaClient.getActiveTimeline().getCommitTimeline().firstInstant();
+      Option<HoodieInstant> commit = metaClient.getActiveTimeline().getCommitAndReplaceTimeline().firstInstant();
       assertFalse(commit.isPresent());
 
       List<StoragePathInfo> allFiles = listAllBaseFilesInPath(hoodieTable);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java
Patch:
@@ -270,7 +270,7 @@ public void testCompactionRetryOnFailureBasedOnNumCommits() throws Exception {
     // Then: 1 delta commit is done, the failed compaction is retried
     metaClient = createMetaClient(cfg.getBasePath());
     assertEquals(4, metaClient.getActiveTimeline().getWriteTimeline().countInstants());
-    assertEquals(instantTime2, metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants().firstInstant().get().getTimestamp());
+    assertEquals(instantTime2, metaClient.getActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants().firstInstant().get().getTimestamp());
   }
 
   @Test
@@ -308,7 +308,7 @@ public void testCompactionRetryOnFailureBasedOnTime() throws Exception {
     metaClient = createMetaClient(cfg.getBasePath());
     // 2 delta commits at the beginning. 1 compaction, 1 delta commit following it.
     assertEquals(4, metaClient.getActiveTimeline().getWriteTimeline().countInstants());
-    assertEquals(instantTime, metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants().firstInstant().get().getTimestamp());
+    assertEquals(instantTime, metaClient.getActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants().firstInstant().get().getTimestamp());
   }
 
   @Test
@@ -345,6 +345,6 @@ public void testCompactionRetryOnFailureBasedOnNumAndTime() throws Exception {
     // Then: 1 delta commit is done, the failed compaction is retried
     metaClient = createMetaClient(cfg.getBasePath());
     assertEquals(4, metaClient.getActiveTimeline().getWriteTimeline().countInstants());
-    assertEquals(instantTime, metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants().firstInstant().get().getTimestamp());
+    assertEquals(instantTime, metaClient.getActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants().firstInstant().get().getTimestamp());
   }
 }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/rollback/TestCopyOnWriteRollbackActionExecutor.java
Patch:
@@ -289,7 +289,7 @@ private void performRollbackAndValidate(boolean isUsingMarkers, HoodieWriteConfi
     //2. rollback
     HoodieInstant commitInstant;
     if (isUsingMarkers) {
-      commitInstant = table.getActiveTimeline().getCommitTimeline().filterInflights().lastInstant().get();
+      commitInstant = table.getActiveTimeline().getCommitAndReplaceTimeline().filterInflights().lastInstant().get();
     } else {
       commitInstant = table.getCompletedCommitTimeline().lastInstant().get();
     }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableInsertUpdateDelete.java
Patch:
@@ -291,7 +291,7 @@ public void testSimpleInsertUpdateAndDelete(boolean populateMetaFields) throws E
       assertTrue(deltaCommit.isPresent());
       assertEquals("001", deltaCommit.get().getTimestamp(), "Delta commit should be 001");
 
-      Option<HoodieInstant> commit = metaClient.getActiveTimeline().getCommitTimeline().firstInstant();
+      Option<HoodieInstant> commit = metaClient.getActiveTimeline().getCommitAndReplaceTimeline().firstInstant();
       assertFalse(commit.isPresent());
 
       List<StoragePathInfo> allFiles = listAllBaseFilesInPath(hoodieTable);
@@ -334,7 +334,7 @@ public void testSimpleInsertUpdateAndDelete(boolean populateMetaFields) throws E
       assertTrue(deltaCommit.isPresent());
       assertEquals("004", deltaCommit.get().getTimestamp(), "Latest Delta commit should be 004");
 
-      commit = metaClient.getActiveTimeline().getCommitTimeline().firstInstant();
+      commit = metaClient.getActiveTimeline().getCommitAndReplaceTimeline().firstInstant();
       assertFalse(commit.isPresent());
 
       allFiles = listAllBaseFilesInPath(hoodieTable);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableRollback.java
Patch:
@@ -113,7 +113,7 @@ void testCOWToMORConvertedTableRollback(boolean rollbackUsingMarkers) throws Exc
       client.commit(newCommitTime, jsc().parallelize(statuses));
 
       metaClient = HoodieTableMetaClient.reload(metaClient);
-      Option<HoodieInstant> commit = metaClient.getActiveTimeline().getCommitTimeline().firstInstant();
+      Option<HoodieInstant> commit = metaClient.getActiveTimeline().getCommitAndReplaceTimeline().firstInstant();
       assertTrue(commit.isPresent());
       assertEquals("001", commit.get().getTimestamp(), "commit should be 001");
 
@@ -189,7 +189,7 @@ void testRollbackWithDeltaAndCompactionCommit(boolean rollbackUsingMarkers) thro
       assertTrue(deltaCommit.isPresent());
       assertEquals("000000001", deltaCommit.get().getTimestamp(), "Delta commit should be 000000001");
 
-      Option<HoodieInstant> commit = metaClient.getActiveTimeline().getCommitTimeline().firstInstant();
+      Option<HoodieInstant> commit = metaClient.getActiveTimeline().getCommitAndReplaceTimeline().firstInstant();
       assertFalse(commit.isPresent());
 
       List<StoragePathInfo> allFiles = listAllBaseFilesInPath(hoodieTable);
@@ -377,7 +377,7 @@ void testMultiRollbackWithDeltaAndCompactionCommit() throws Exception {
       assertEquals(200, getTotalRecordsWritten(instantCommitMetadataPairOpt.get().getValue()));
 
       Option<HoodieInstant> commit =
-          metaClient.getActiveTimeline().getCommitTimeline().firstInstant();
+          metaClient.getActiveTimeline().getCommitAndReplaceTimeline().firstInstant();
       assertFalse(commit.isPresent());
 
       HoodieTable hoodieTable = HoodieSparkTable.create(cfg, context(), metaClient);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestBase.java
Patch:
@@ -530,7 +530,7 @@ private JavaRDD<WriteStatus> getWriteStatusAndVerifyDeleteOperation(String newCo
 
     // verify that there is a commit
     HoodieTableMetaClient metaClient = HoodieTestUtils.createMetaClient(storageConf, basePath);
-    HoodieTimeline timeline = new HoodieActiveTimeline(metaClient).getCommitTimeline();
+    HoodieTimeline timeline = new HoodieActiveTimeline(metaClient).getCommitAndReplaceTimeline();
 
     if (assertForCommit) {
       assertEquals(3, timeline.findInstantsAfter(initCommitTime, Integer.MAX_VALUE).countInstants(),

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/SparkClientFunctionalTestHarness.java
Patch:
@@ -289,7 +289,7 @@ protected Stream<HoodieBaseFile> insertRecordsToMORTable(HoodieTableMetaClient m
         "Delta commit should be specified value");
 
     Option<HoodieInstant> commit =
-        reloadedMetaClient.getActiveTimeline().getCommitTimeline().lastInstant();
+        reloadedMetaClient.getActiveTimeline().getCommitAndReplaceTimeline().lastInstant();
     assertFalse(commit.isPresent());
 
     List<StoragePathInfo> allFiles = listAllBaseFilesInPath(hoodieTable);
@@ -337,7 +337,7 @@ protected void updateRecordsInMORTable(HoodieTableMetaClient metaClient, List<Ho
         "Latest Delta commit should match specified time");
 
     Option<HoodieInstant> commit =
-        reloadedMetaClient.getActiveTimeline().getCommitTimeline().firstInstant();
+        reloadedMetaClient.getActiveTimeline().getCommitAndReplaceTimeline().firstInstant();
     assertFalse(commit.isPresent());
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -665,7 +665,7 @@ public boolean isTimelineNonEmpty() {
   public HoodieTimeline getCommitsTimeline() {
     switch (this.getTableType()) {
       case COPY_ON_WRITE:
-        return getActiveTimeline().getCommitTimeline();
+        return getActiveTimeline().getCommitAndReplaceTimeline();
       case MERGE_ON_READ:
         // We need to include the parquet files written out in delta commits
         // Include commit action to be able to start doing a MOR over a COW table - no
@@ -685,7 +685,7 @@ public HoodieTimeline getCommitsTimeline() {
   public HoodieTimeline getCommitsAndCompactionTimeline() {
     switch (this.getTableType()) {
       case COPY_ON_WRITE:
-        return getActiveTimeline().getCommitTimeline();
+        return getActiveTimeline().getCommitAndReplaceTimeline();
       case MERGE_ON_READ:
         return getActiveTimeline().getWriteTimeline();
       default:
@@ -701,7 +701,7 @@ public HoodieTimeline getCommitTimeline() {
       case COPY_ON_WRITE:
       case MERGE_ON_READ:
         // We need to include the parquet files written out in delta commits in tagging
-        return getActiveTimeline().getCommitTimeline();
+        return getActiveTimeline().getCommitAndReplaceTimeline();
       default:
         throw new HoodieException("Unsupported table type :" + this.getTableType());
     }

File: hudi-common/src/main/java/org/apache/hudi/common/util/CompactionUtils.java
Patch:
@@ -285,8 +285,7 @@ public static List<HoodieInstant> getPendingCompactionInstantTimes(HoodieTableMe
    */
   public static Option<Pair<HoodieTimeline, HoodieInstant>> getDeltaCommitsSinceLatestCompaction(
       HoodieActiveTimeline activeTimeline) {
-    Option<HoodieInstant> lastCompaction = activeTimeline.getCommitTimeline()
-        .filterCompletedInstants().lastInstant();
+    Option<HoodieInstant> lastCompaction = activeTimeline.getCommitTimeline().filterCompletedInstants().lastInstant();
     HoodieTimeline deltaCommits = activeTimeline.getDeltaCommitTimeline();
 
     final HoodieInstant latestInstant;

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -753,7 +753,7 @@ public Option<String> getSyncedInstantTime() {
   @Override
   public Option<String> getLatestCompactionTime() {
     if (metadataMetaClient != null) {
-      Option<HoodieInstant> latestCompaction = metadataMetaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants().lastInstant();
+      Option<HoodieInstant> latestCompaction = metadataMetaClient.getActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants().lastInstant();
       if (latestCompaction.isPresent()) {
         return Option.of(latestCompaction.get().getTimestamp());
       }

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/table/TestHoodieTableMetaClient.java
Patch:
@@ -86,7 +86,7 @@ public void checkSerDe() {
   @Test
   public void checkCommitTimeline() {
     HoodieActiveTimeline activeTimeline = metaClient.getActiveTimeline();
-    HoodieTimeline activeCommitTimeline = activeTimeline.getCommitTimeline();
+    HoodieTimeline activeCommitTimeline = activeTimeline.getCommitAndReplaceTimeline();
     assertTrue(activeCommitTimeline.empty(), "Should be empty commit timeline");
 
     HoodieInstant instant = new HoodieInstant(true, HoodieTimeline.COMMIT_ACTION, "1");
@@ -95,12 +95,12 @@ public void checkCommitTimeline() {
 
     // Commit timeline should not auto-reload every time getActiveCommitTimeline(), it should be cached
     activeTimeline = metaClient.getActiveTimeline();
-    activeCommitTimeline = activeTimeline.getCommitTimeline();
+    activeCommitTimeline = activeTimeline.getCommitAndReplaceTimeline();
     assertTrue(activeCommitTimeline.empty(), "Should be empty commit timeline");
 
     activeTimeline = activeTimeline.reload();
     HoodieInstant completedInstant = activeTimeline.getCommitsTimeline().getInstantsAsStream().findFirst().get();
-    activeCommitTimeline = activeTimeline.getCommitTimeline();
+    activeCommitTimeline = activeTimeline.getCommitAndReplaceTimeline();
     assertFalse(activeCommitTimeline.empty(), "Should be the 1 commit we made");
     assertTrue(completedInstant.isCompleted());
     assertTrue(completedInstant.getTimestamp().equals(instant.getTimestamp()));

File: hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java
Patch:
@@ -202,9 +202,9 @@ public void run() throws Exception {
     HoodieTableMetaClient metaClient = HoodieClientTestUtils.createMetaClient(jssc, tablePath);
     if (tableType.equals(HoodieTableType.MERGE_ON_READ.name())) {
       // Ensure we have successfully completed one compaction commit
-      ValidationUtils.checkArgument(metaClient.getActiveTimeline().getCommitTimeline().countInstants() == 1);
+      ValidationUtils.checkArgument(metaClient.getActiveTimeline().getCommitAndReplaceTimeline().countInstants() == 1);
     } else {
-      ValidationUtils.checkArgument(metaClient.getActiveTimeline().getCommitTimeline().countInstants() >= 1);
+      ValidationUtils.checkArgument(metaClient.getActiveTimeline().getCommitAndReplaceTimeline().countInstants() >= 1);
     }
 
     // Deletes Stream

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerTestBase.java
Patch:
@@ -636,7 +636,7 @@ static HoodieDeltaStreamer.Config makeConfigForHudiIncrSrc(String srcBasePath, S
 
     static void assertAtleastNCompactionCommits(int minExpected, String tablePath) {
       HoodieTableMetaClient meta = createMetaClient(storage, tablePath);
-      HoodieTimeline timeline = meta.getActiveTimeline().getCommitTimeline().filterCompletedInstants();
+      HoodieTimeline timeline = meta.getActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants();
       LOG.info("Timeline Instants=" + meta.getActiveTimeline().getInstants());
       int numCompactionCommits = timeline.countInstants();
       assertTrue(minExpected <= numCompactionCommits, "Got=" + numCompactionCommits + ", exp >=" + minExpected);
@@ -652,7 +652,7 @@ static void assertAtleastNDeltaCommits(int minExpected, String tablePath) {
 
     static void assertAtleastNCompactionCommitsAfterCommit(int minExpected, String lastSuccessfulCommit, String tablePath) {
       HoodieTableMetaClient meta = createMetaClient(storage.getConf(), tablePath);
-      HoodieTimeline timeline = meta.getActiveTimeline().getCommitTimeline().findInstantsAfter(lastSuccessfulCommit).filterCompletedInstants();
+      HoodieTimeline timeline = meta.getActiveTimeline().getCommitAndReplaceTimeline().findInstantsAfter(lastSuccessfulCommit).filterCompletedInstants();
       LOG.info("Timeline Instants=" + meta.getActiveTimeline().getInstants());
       int numCompactionCommits = timeline.countInstants();
       assertTrue(minExpected <= numCompactionCommits, "Got=" + numCompactionCommits + ", exp >=" + minExpected);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -857,7 +857,7 @@ public void testDeltaSyncWithPendingCompaction() throws Exception {
 
     // delete compaction commit
     HoodieTableMetaClient meta = HoodieTestUtils.createMetaClient(storage, tableBasePath);
-    HoodieTimeline timeline = meta.getActiveTimeline().getCommitTimeline().filterCompletedInstants();
+    HoodieTimeline timeline = meta.getActiveTimeline().getCommitAndReplaceTimeline().filterCompletedInstants();
     HoodieInstant commitInstant = timeline.lastInstant().get();
     String commitFileName = tableBasePath + "/.hoodie/" + commitInstant.getFileName();
     fs.delete(new Path(commitFileName), false);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java
Patch:
@@ -67,7 +67,6 @@
 import static org.apache.hudi.common.testutils.HoodieTestUtils.DEFAULT_PARTITION_PATHS;
 import static org.apache.hudi.common.testutils.HoodieTestUtils.generateFakeHoodieWriteStat;
 import static org.apache.hudi.common.testutils.SchemaTestUtil.getSchemaFromResource;
-import static org.apache.hudi.table.action.commit.UpsertPartitioner.averageBytesPerRecord;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 import static org.mockito.ArgumentMatchers.any;
@@ -175,7 +174,7 @@ public void testAverageBytesPerRecordForNonEmptyCommitTimeLine() throws Exceptio
     LinkedList<Option<byte[]>> commits = generateCommitMetadataList();
     when(commitTimeLine.getInstantDetails(any(HoodieInstant.class))).thenAnswer(invocationOnMock -> commits.pop());
     long expectAvgSize = (long) Math.ceil((1.0 * 7500) / 1500);
-    long actualAvgSize = averageBytesPerRecord(commitTimeLine, config);
+    long actualAvgSize = AverageRecordSizeUtils.averageBytesPerRecord(commitTimeLine, config);
     assertEquals(expectAvgSize, actualAvgSize);
   }
 
@@ -185,7 +184,7 @@ public void testAverageBytesPerRecordForEmptyCommitTimeLine() throws Exception {
     HoodieWriteConfig config = makeHoodieClientConfigBuilder().build();
     when(commitTimeLine.empty()).thenReturn(true);
     long expectAvgSize = config.getCopyOnWriteRecordSizeEstimate();
-    long actualAvgSize = averageBytesPerRecord(commitTimeLine, config);
+    long actualAvgSize = AverageRecordSizeUtils.averageBytesPerRecord(commitTimeLine, config);
     assertEquals(expectAvgSize, actualAvgSize);
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -511,9 +511,6 @@ public void preWrite(String instantTime, WriteOperationType writeOperationType,
    * @return Write Status
    */
   public O postWrite(HoodieWriteMetadata<O> result, String instantTime, HoodieTable hoodieTable) {
-    if (result.getIndexLookupDuration().isPresent()) {
-      metrics.updateIndexMetrics(getOperationType().name(), result.getIndexUpdateDuration().get().toMillis());
-    }
     if (result.isCommitted()) {
       // Perform post commit operations.
       if (result.getFinalizeDuration().isPresent()) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java
Patch:
@@ -43,6 +43,7 @@
 import org.apache.hudi.metadata.MetadataPartitionType;
 import org.apache.hudi.metadata.SparkHoodieBackedTableMetadataWriter;
 import org.apache.hudi.metrics.DistributedRegistry;
+import org.apache.hudi.metrics.HoodieMetrics;
 import org.apache.hudi.table.BulkInsertPartitioner;
 import org.apache.hudi.table.HoodieSparkTable;
 import org.apache.hudi.table.HoodieTable;
@@ -141,8 +142,8 @@ public JavaRDD<WriteStatus> upsert(JavaRDD<HoodieRecord<T>> records, String inst
     preWrite(instantTime, WriteOperationType.UPSERT, table.getMetaClient());
     HoodieWriteMetadata<HoodieData<WriteStatus>> result = table.upsert(context, instantTime, HoodieJavaRDD.of(records));
     HoodieWriteMetadata<JavaRDD<WriteStatus>> resultRDD = result.clone(HoodieJavaRDD.getJavaRDD(result.getWriteStatuses()));
-    if (result.getIndexLookupDuration().isPresent()) {
-      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());
+    if (result.getSourceReadAndIndexDurationMs().isPresent()) {
+      metrics.updateSourceReadAndIndexMetrics(HoodieMetrics.DURATION_STR, result.getSourceReadAndIndexDurationMs().get());
     }
     return postWrite(resultRDD, instantTime, table);
   }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkUpsertDeltaCommitActionExecutor.java
Patch:
@@ -43,6 +43,6 @@ public SparkUpsertDeltaCommitActionExecutor(HoodieSparkEngineContext context,
   @Override
   public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
     return HoodieWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
-        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(),this, operationType);
+        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, operationType);
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieIOHandle.java
Patch:
@@ -30,9 +30,9 @@ public abstract class HoodieIOHandle<T, I, K, O> {
 
   protected final String instantTime;
   protected final HoodieWriteConfig config;
-  protected final HoodieStorage storage;
-  protected final FileSystem fs;
   protected final HoodieTable<T, I, K, O> hoodieTable;
+  protected FileSystem fs;
+  protected HoodieStorage storage;
 
   HoodieIOHandle(HoodieWriteConfig config, Option<String> instantTime, HoodieTable<T, I, K, O> hoodieTable) {
     this.instantTime = instantTime.orElse(StringUtils.EMPTY_STRING);

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -96,7 +96,7 @@ public static String getTablePath(HoodieStorage storage,
    *
    * @see HoodieWriteConfig#getUserDefinedBulkInsertPartitionerClass()
    */
-  private static Option<BulkInsertPartitioner> createUserDefinedBulkInsertPartitioner(HoodieWriteConfig config)
+  public static Option<BulkInsertPartitioner> createUserDefinedBulkInsertPartitioner(HoodieWriteConfig config)
       throws HoodieException {
     String bulkInsertPartitionerClass = config.getUserDefinedBulkInsertPartitionerClass();
     try {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/StreamSync.java
Patch:
@@ -134,6 +134,7 @@
 
 import scala.Tuple2;
 
+import static org.apache.hudi.DataSourceUtils.createUserDefinedBulkInsertPartitioner;
 import static org.apache.hudi.avro.AvroSchemaUtils.getAvroRecordQualifiedName;
 import static org.apache.hudi.common.table.HoodieTableConfig.ARCHIVELOG_FOLDER;
 import static org.apache.hudi.common.table.HoodieTableConfig.HIVE_STYLE_PARTITIONING_ENABLE;
@@ -988,7 +989,7 @@ private WriteClientWriteResult writeToSink(InputBatch inputBatch, String instant
           writeClientWriteResult = new WriteClientWriteResult(writeClient.upsert(records, instantTime));
           break;
         case BULK_INSERT:
-          writeClientWriteResult = new WriteClientWriteResult(writeClient.bulkInsert(records, instantTime));
+          writeClientWriteResult = new WriteClientWriteResult(writeClient.bulkInsert(records, instantTime, createUserDefinedBulkInsertPartitioner(writeClient.getConfig())));
           break;
         case INSERT_OVERWRITE:
           writeResult = writeClient.insertOverwrite(records, instantTime);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieClient.java
Patch:
@@ -102,7 +102,7 @@ protected BaseHoodieClient(HoodieEngineContext context, HoodieWriteConfig client
     this.heartbeatClient = new HoodieHeartbeatClient(storage, this.basePath,
         clientConfig.getHoodieClientHeartbeatIntervalInMs(),
         clientConfig.getHoodieClientHeartbeatTolerableMisses());
-    this.metrics = new HoodieMetrics(config);
+    this.metrics = new HoodieMetrics(config, context.getStorageConf());
     this.txnManager = new TransactionManager(config, storage);
     this.timeGenerator = TimeGenerators.getTimeGenerator(
         config.getTimeGeneratorConfig(), HadoopFSUtils.getStorageConf(hadoopConf));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/timeline/HoodieTimelineArchiver.java
Patch:
@@ -86,7 +86,7 @@ public HoodieTimelineArchiver(HoodieWriteConfig config, HoodieTable<T, I, K, O>
     Pair<Integer, Integer> minAndMaxInstants = getMinAndMaxInstantsToKeep(table, metaClient);
     this.minInstantsToKeep = minAndMaxInstants.getLeft();
     this.maxInstantsToKeep = minAndMaxInstants.getRight();
-    this.metrics = new HoodieMetrics(config);
+    this.metrics = new HoodieMetrics(config, table.getStorageConf());
   }
 
   public int archiveIfRequired(HoodieEngineContext context) throws IOException {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/LockManager.java
Patch:
@@ -69,7 +69,7 @@ public LockManager(HoodieWriteConfig writeConfig, FileSystem fs, TypedProperties
         Integer.parseInt(HoodieLockConfig.LOCK_ACQUIRE_CLIENT_NUM_RETRIES.defaultValue()));
     maxWaitTimeInMs = lockConfiguration.getConfig().getLong(LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP_KEY,
         Long.parseLong(HoodieLockConfig.LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS.defaultValue()));
-    metrics = new HoodieLockMetrics(writeConfig);
+    metrics = new HoodieLockMetrics(writeConfig, storageConf);
     lockRetryHelper = new RetryHelper<>(maxWaitTimeInMs, maxRetries, maxWaitTimeInMs,
         Arrays.asList(HoodieLockException.class, InterruptedException.class), "acquire lock");
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/metrics/HoodieLockMetrics.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.common.util.HoodieTimer;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.metrics.Metrics;
+import org.apache.hudi.storage.StorageConfiguration;
 
 import java.util.concurrent.TimeUnit;
 
@@ -49,12 +50,12 @@ public class HoodieLockMetrics {
   private static final Object REGISTRY_LOCK = new Object();
   private Metrics metrics;
 
-  public HoodieLockMetrics(HoodieWriteConfig writeConfig) {
+  public HoodieLockMetrics(HoodieWriteConfig writeConfig, StorageConfiguration<?> storageConf) {
     this.isMetricsEnabled = writeConfig.isLockingMetricsEnabled();
     this.writeConfig = writeConfig;
 
     if (isMetricsEnabled) {
-      metrics = Metrics.getInstance(writeConfig.getMetricsConfig());
+      metrics = Metrics.getInstance(writeConfig.getMetricsConfig(), storageConf);
       MetricRegistry registry = metrics.getRegistry();
 
       lockAttempts = registry.counter(getMetricsName(LOCK_ACQUIRE_ATTEMPTS_COUNTER_NAME));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.common.util.VisibleForTesting;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
+import org.apache.hudi.storage.StorageConfiguration;
 
 import com.codahale.metrics.Counter;
 import com.codahale.metrics.Timer;
@@ -106,11 +107,11 @@ public class HoodieMetrics {
   private Counter compactionRequestedCounter = null;
   private Counter compactionCompletedCounter = null;
 
-  public HoodieMetrics(HoodieWriteConfig config) {
+  public HoodieMetrics(HoodieWriteConfig config, StorageConfiguration<?> storageConf) {
     this.config = config;
     this.tableName = config.getTableName();
     if (config.isMetricsOn()) {
-      metrics = Metrics.getInstance(config.getMetricsConfig());
+      metrics = Metrics.getInstance(config.getMetricsConfig(), storageConf);
       this.rollbackTimerName = getMetricsName(TIMER_ACTION, HoodieTimeline.ROLLBACK_ACTION);
       this.cleanTimerName = getMetricsName(TIMER_ACTION, HoodieTimeline.CLEAN_ACTION);
       this.archiveTimerName = getMetricsName(TIMER_ACTION, ARCHIVE_ACTION);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/RunCompactionActionExecutor.java
Patch:
@@ -73,7 +73,7 @@ public RunCompactionActionExecutor(HoodieEngineContext context,
     this.operationType = operationType;
     checkArgument(operationType == WriteOperationType.COMPACT || operationType == WriteOperationType.LOG_COMPACT,
         "Only COMPACT and LOG_COMPACT is supported");
-    metrics = new HoodieMetrics(config);
+    metrics = new HoodieMetrics(config, context.getStorageConf());
   }
 
   @Override

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java
Patch:
@@ -101,7 +101,7 @@ public RunIndexActionExecutor(HoodieEngineContext context, HoodieWriteConfig con
     super(context, config, table, instantTime);
     this.txnManager = new TransactionManager(config, table.getMetaClient().getStorage());
     if (config.getMetadataConfig().isMetricsEnabled()) {
-      this.metrics = Option.of(new HoodieMetadataMetrics(config.getMetricsConfig()));
+      this.metrics = Option.of(new HoodieMetadataMetrics(config.getMetricsConfig(), context.getStorageConf()));
     } else {
       this.metrics = Option.empty();
     }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -89,7 +89,7 @@ public static HoodieTableMetadataWriter create(StorageConfiguration<?> conf,
   protected void initRegistry() {
     if (metadataWriteConfig.isMetricsOn()) {
       // should support executor metrics
-      this.metrics = Option.of(new HoodieMetadataMetrics(metadataWriteConfig.getMetricsConfig()));
+      this.metrics = Option.of(new HoodieMetadataMetrics(metadataWriteConfig.getMetricsConfig(), storageConf));
     } else {
       this.metrics = Option.empty();
     }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/metadata/JavaHoodieBackedTableMetadataWriter.java
Patch:
@@ -79,7 +79,7 @@ public static HoodieTableMetadataWriter create(StorageConfiguration<?> conf,
   @Override
   protected void initRegistry() {
     if (metadataWriteConfig.isMetricsOn()) {
-      this.metrics = Option.of(new HoodieMetadataMetrics(metadataWriteConfig.getMetricsConfig()));
+      this.metrics = Option.of(new HoodieMetadataMetrics(metadataWriteConfig.getMetricsConfig(), storageConf));
     } else {
       this.metrics = Option.empty();
     }

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/client/TestJavaHoodieBackedMetadata.java
Patch:
@@ -2393,7 +2393,7 @@ public void testMetadataMetrics() throws Exception {
       assertNoWriteErrors(writeStatuses);
       validateMetadata(client);
 
-      Metrics metrics = Metrics.getInstance(writeConfig.getMetricsConfig());
+      Metrics metrics = Metrics.getInstance(writeConfig.getMetricsConfig(), storageConf);
       assertTrue(metrics.getRegistry().getGauges().containsKey(HoodieMetadataMetrics.INITIALIZE_STR + ".count"));
       assertTrue(metrics.getRegistry().getGauges().containsKey(HoodieMetadataMetrics.INITIALIZE_STR + ".totalDuration"));
       assertTrue((Long) metrics.getRegistry().getGauges().get(HoodieMetadataMetrics.INITIALIZE_STR + ".count").getValue() >= 1L);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SparkPreCommitValidator.java
Patch:
@@ -59,7 +59,7 @@ protected SparkPreCommitValidator(HoodieSparkTable<T> table, HoodieEngineContext
     this.table = table;
     this.engineContext = engineContext;
     this.writeConfig = writeConfig;
-    this.metrics = new HoodieMetrics(writeConfig);
+    this.metrics = new HoodieMetrics(writeConfig, engineContext.getStorageConf());
   }
   
   protected Set<String> getPartitionsModified(HoodieWriteMetadata<O> writeResult) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -119,7 +119,7 @@ protected void initRegistry() {
       } else {
         registry = Registry.getRegistry("HoodieMetadata");
       }
-      this.metrics = Option.of(new HoodieMetadataMetrics(metadataWriteConfig.getMetricsConfig()));
+      this.metrics = Option.of(new HoodieMetadataMetrics(metadataWriteConfig.getMetricsConfig(), storageConf));
     } else {
       this.metrics = Option.empty();
     }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -3114,7 +3114,7 @@ public void testMetadataMetrics() throws Exception {
       assertNoWriteErrors(writeStatuses);
       validateMetadata(client);
 
-      Metrics metrics = Metrics.getInstance(writeConfig.getMetricsConfig());
+      Metrics metrics = Metrics.getInstance(writeConfig.getMetricsConfig(), storageConf);
       assertTrue(metrics.getRegistry().getGauges().containsKey(HoodieMetadataMetrics.INITIALIZE_STR + ".count"));
       assertTrue(metrics.getRegistry().getGauges().containsKey(HoodieMetadataMetrics.INITIALIZE_STR + ".totalDuration"));
       assertTrue((Long) metrics.getRegistry().getGauges().get(HoodieMetadataMetrics.INITIALIZE_STR + ".count").getValue() >= 1L);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/TestHoodieTimelineArchiver.java
Patch:
@@ -1000,7 +1000,7 @@ public void testArchiveMetrics() throws Exception {
                 .build())
             .withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)
             .withParallelism(2, 2).forTable("test-trip-table").build();
-    HoodieMetrics metrics = new HoodieMetrics(cfg);
+    HoodieMetrics metrics = new HoodieMetrics(cfg, storageConf);
     BaseHoodieWriteClient client = getHoodieWriteClient(cfg);
     client.archive();
     assertTrue(metrics.getMetrics().getRegistry().getNames().contains(metrics.getMetricsName(ARCHIVE_ACTION, DURATION_STR)));

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java
Patch:
@@ -147,7 +147,7 @@ public AppendResult appendBlocks(List<HoodieLogBlock> blocks) throws IOException
       // bytes for header
       byte[] headerBytes = HoodieLogBlock.getLogMetadataBytes(block.getLogBlockHeader());
       // content bytes
-      byte[] content = block.getContentBytes();
+      byte[] content = block.getContentBytes(storage.getConf());
       // bytes for footer
       byte[] footerBytes = HoodieLogBlock.getLogMetadataBytes(block.getLogBlockFooter());
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.internal.schema.InternalSchema;
 import org.apache.hudi.io.SeekableDataInputStream;
+import org.apache.hudi.storage.StorageConfiguration;
 
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericDatumReader;
@@ -100,7 +101,7 @@ public HoodieLogBlockType getBlockType() {
   }
 
   @Override
-  protected byte[] serializeRecords(List<HoodieRecord> records) throws IOException {
+  protected byte[] serializeRecords(List<HoodieRecord> records, StorageConfiguration<?> storageConf) throws IOException {
     Schema schema = new Schema.Parser().parse(super.getLogBlockHeader().get(HeaderMetadataType.SCHEMA));
     GenericDatumWriter<IndexedRecord> writer = new GenericDatumWriter<>(schema);
     ByteArrayOutputStream baos = new ByteArrayOutputStream();

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieCommandBlock.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.io.SeekableDataInputStream;
+import org.apache.hudi.storage.StorageConfiguration;
 
 import java.util.HashMap;
 import java.util.Map;
@@ -61,7 +62,7 @@ public HoodieLogBlockType getBlockType() {
   }
 
   @Override
-  public byte[] getContentBytes() {
+  public byte[] getContentBytes(StorageConfiguration<?> storageConf) {
     return new byte[0];
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieCorruptBlock.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.io.SeekableDataInputStream;
+import org.apache.hudi.storage.StorageConfiguration;
 
 import java.io.IOException;
 import java.util.Map;
@@ -38,7 +39,7 @@ public HoodieCorruptBlock(Option<byte[]> corruptedBytes, Supplier<SeekableDataIn
   }
 
   @Override
-  public byte[] getContentBytes() throws IOException {
+  public byte[] getContentBytes(StorageConfiguration<?> storageConf) throws IOException {
     if (!getContent().isPresent() && readBlockLazily) {
       // read content from disk
       inflate();

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDeleteBlock.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.io.SeekableDataInputStream;
+import org.apache.hudi.storage.StorageConfiguration;
 import org.apache.hudi.util.Lazy;
 
 import org.apache.avro.io.BinaryDecoder;
@@ -112,7 +113,7 @@ public HoodieDeleteBlock(Option<byte[]> content, Supplier<SeekableDataInputStrea
   }
 
   @Override
-  public byte[] getContentBytes() throws IOException {
+  public byte[] getContentBytes(StorageConfiguration<?> storageConf) throws IOException {
     Option<byte[]> content = getContent();
 
     // In case this method is called before realizing keys from content

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java
Patch:
@@ -118,14 +118,14 @@ public HoodieLogBlockType getBlockType() {
   }
 
   @Override
-  protected byte[] serializeRecords(List<HoodieRecord> records) throws IOException {
+  protected byte[] serializeRecords(List<HoodieRecord> records, StorageConfiguration<?> storageConf) throws IOException {
     HFileContext context = new HFileContextBuilder()
         .withBlockSize(DEFAULT_BLOCK_SIZE)
         .withCompression(compressionAlgorithm.get())
         .withCellComparator(ReflectionUtils.loadClass(KV_COMPARATOR_CLASS_NAME))
         .build();
 
-    Configuration conf = new Configuration();
+    Configuration conf = storageConf.unwrapAs(Configuration.class);
     CacheConfig cacheConfig = new CacheConfig(conf);
     ByteArrayOutputStream baos = new ByteArrayOutputStream();
     FSDataOutputStream ostream = new FSDataOutputStream(baos, null);

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieLogBlock.java
Patch:
@@ -88,7 +88,7 @@ public HoodieLogBlock(
   }
 
   // Return the bytes representation of the data belonging to a LogBlock
-  public byte[] getContentBytes() throws IOException {
+  public byte[] getContentBytes(StorageConfiguration<?> storageConf) throws IOException {
     throw new HoodieException("No implementation was provided");
   }
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/BaseTableMetadata.java
Patch:
@@ -93,7 +93,8 @@ protected BaseTableMetadata(HoodieEngineContext engineContext, HoodieMetadataCon
     this.isMetadataTableInitialized = dataMetaClient.getTableConfig().isMetadataTableAvailable();
 
     if (metadataConfig.isMetricsEnabled()) {
-      this.metrics = Option.of(new HoodieMetadataMetrics(HoodieMetricsConfig.newBuilder().fromProperties(metadataConfig.getProps()).build()));
+      this.metrics = Option.of(new HoodieMetadataMetrics(HoodieMetricsConfig.newBuilder()
+          .fromProperties(metadataConfig.getProps()).build(), getStorageConf()));
     } else {
       this.metrics = Option.empty();
     }

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMetrics.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.metrics.HoodieGauge;
 import org.apache.hudi.metrics.Metrics;
+import org.apache.hudi.storage.StorageConfiguration;
 
 import com.codahale.metrics.MetricRegistry;
 import org.slf4j.Logger;
@@ -80,8 +81,8 @@ public class HoodieMetadataMetrics implements Serializable {
   private final transient MetricRegistry metricsRegistry;
   private final transient Metrics metrics;
 
-  public HoodieMetadataMetrics(HoodieMetricsConfig metricsConfig) {
-    this.metrics = Metrics.getInstance(metricsConfig);
+  public HoodieMetadataMetrics(HoodieMetricsConfig metricsConfig, StorageConfiguration<?> storageConf) {
+    this.metrics = Metrics.getInstance(metricsConfig, storageConf);
     this.metricsRegistry = metrics.getRegistry();
   }
 

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -455,7 +455,7 @@ public void testHugeLogFileWrite() throws IOException, URISyntaxException, Inter
     Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
-    byte[] dataBlockContentBytes = getDataBlock(DEFAULT_DATA_BLOCK_TYPE, records, header).getContentBytes();
+    byte[] dataBlockContentBytes = getDataBlock(DEFAULT_DATA_BLOCK_TYPE, records, header).getContentBytes(storage.getConf());
     HoodieLogBlock.HoodieLogBlockContentLocation logBlockContentLoc = new HoodieLogBlock.HoodieLogBlockContentLocation(
         HoodieTestUtils.getDefaultStorageConfWithDefaults(), null, 0, dataBlockContentBytes.length, 0);
     HoodieDataBlock reusableDataBlock = new HoodieAvroDataBlock(null, Option.ofNullable(dataBlockContentBytes), false,

File: hudi-hadoop-common/src/test/java/org/apache/hudi/common/table/log/block/TestHoodieDeleteBlock.java
Patch:
@@ -20,6 +20,7 @@
 package org.apache.hudi.common.table.log.block;
 
 import org.apache.hudi.common.model.DeleteRecord;
+import org.apache.hudi.common.testutils.HoodieTestUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
 
@@ -124,7 +125,7 @@ public void testDeleteBlockWithValidation(DeleteRecord[] deleteRecords) throws I
       deleteRecordList.add(Pair.of(dr, -1L));
     }
     HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deleteRecordList, false, new HashMap<>());
-    byte[] contentBytes = deleteBlock.getContentBytes();
+    byte[] contentBytes = deleteBlock.getContentBytes(HoodieTestUtils.getDefaultStorageConf());
     HoodieDeleteBlock deserializeDeleteBlock = new HoodieDeleteBlock(
         Option.of(contentBytes), null, true, Option.empty(), new HashMap<>(), new HashMap<>());
     DeleteRecord[] deserializedDeleteRecords = deserializeDeleteBlock.getRecordsToDelete();

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -2827,7 +2827,7 @@ public void testAutoGenerateRecordKeys() throws Exception {
     deltaStreamer.sync();
     assertRecordCount(parquetRecordsCount, tableBasePath, sqlContext);
     // validate that auto record keys are enabled.
-    HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setBasePath(tableBasePath).setConf(jsc.hadoopConfiguration()).build();
+    HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setBasePath(tableBasePath).setConf(HoodieTestUtils.getDefaultStorageConf()).build();
     assertFalse(metaClient.getTableConfig().getRecordKeyFields().isPresent());
 
     prepareParquetDFSFiles(200, PARQUET_SOURCE_ROOT, "2.parquet", false, null, null);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamerSchemaEvolutionBase.java
Patch:
@@ -206,7 +206,7 @@ protected HoodieDeltaStreamer.Config getDeltaStreamerConfig(String[] transformer
           transformerClassNames, PROPS_FILENAME_TEST_AVRO_KAFKA, false,  useSchemaProvider, 100000, false, null, tableType, "timestamp", null);
     } else {
       prepareParquetDFSSource(false, hasTransformer, sourceSchemaFile, targetSchemaFile, PROPS_FILENAME_TEST_PARQUET,
-          PARQUET_SOURCE_ROOT, false, "partition_path", "", extraProps);
+          PARQUET_SOURCE_ROOT, false, "partition_path", "", extraProps, false);
       cfg = TestHoodieDeltaStreamer.TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT, ParquetDFSSource.class.getName(),
           transformerClassNames, PROPS_FILENAME_TEST_PARQUET, false,
           useSchemaProvider, 100000, false, null, tableType, "timestamp", null);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -164,6 +164,7 @@ public BaseHoodieWriteClient(HoodieEngineContext context,
     super(context, writeConfig, timelineService);
     this.index = createIndex(writeConfig);
     this.upgradeDowngradeHelper = upgradeDowngradeHelper;
+    this.metrics.emitIndexTypeMetrics(config.getIndexType().ordinal());
   }
 
   protected abstract HoodieIndex<?, ?> createIndex(HoodieWriteConfig writeConfig);
@@ -1230,7 +1231,7 @@ protected void doInitTable(WriteOperationType operationType, HoodieTableMetaClie
     this.txnManager.beginTransaction(ownerInstant, Option.empty());
     try {
       tryUpgrade(metaClient, instantTime);
-      initMetadataTable(instantTime);
+      initMetadataTable(instantTime, metaClient);
     } finally {
       this.txnManager.endTransaction(ownerInstant);
     }
@@ -1241,7 +1242,7 @@ protected void doInitTable(WriteOperationType operationType, HoodieTableMetaClie
    *
    * @param instantTime current inflight instant time
    */
-  protected void initMetadataTable(Option<String> instantTime) {
+  protected void initMetadataTable(Option<String> instantTime, HoodieTableMetaClient metaClient) {
     // by default do nothing.
   }
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java
Patch:
@@ -209,7 +209,7 @@ public List<WriteStatus> deletePrepped(List<HoodieRecord<T>> preppedRecords, fin
   }
 
   @Override
-  protected void initMetadataTable(Option<String> instantTime) {
+  protected void initMetadataTable(Option<String> instantTime, HoodieTableMetaClient metaClient) {
     // Initialize Metadata Table to make sure it's bootstrapped _before_ the operation,
     // if it didn't exist before
     // See https://issues.apache.org/jira/browse/HUDI-3343 for more details

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -554,8 +554,7 @@ protected void postCommit(HoodieTable table, HoodieCommitMetadata metadata, Stri
    */
   protected void mayBeCleanAndArchive(HoodieTable table) {
     autoCleanOnCommit();
-    // reload table to that timeline reflects the clean commit
-    autoArchiveOnCommit(createTable(config, hadoopConf));
+    autoArchiveOnCommit(table);
   }
 
   protected void runTableServicesInline(HoodieTable table, HoodieCommitMetadata metadata, Option<Map<String, String>> extraMetadata) {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkTableServiceClient.java
Patch:
@@ -211,7 +211,9 @@ public void initMetadataTable() {
         // guard the metadata writer with concurrent lock
         this.txnManager.getLockManager().lock();
         try (HoodieBackedTableMetadataWriter metadataWriter = initMetadataWriter(latestPendingInstant)) {
-          metadataWriter.performTableServices(Option.empty());
+          if (metadataWriter.isInitialized()) {
+            metadataWriter.performTableServices(Option.empty());
+          }
         }
       } catch (Exception e) {
         throw new HoodieException("Failed to initialize metadata table", e);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -124,7 +124,8 @@ protected void commitInternal(String instantTime, Map<MetadataPartitionType, Hoo
     if (writeClient.rollbackFailedWrites()) {
       metadataMetaClient = HoodieTableMetaClient.reload(metadataMetaClient);
     }
-    metadataMetaClient.getActiveTimeline().getDeltaCommitTimeline().filterCompletedInstants().lastInstant().ifPresent(instant -> compactIfNecessary(writeClient, instant.getTimestamp()));
+
+    compactIfNecessary(writeClient);
 
     if (!metadataMetaClient.getActiveTimeline().containsInstant(instantTime)) {
       // if this is a new commit being applied to metadata for the first time

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -189,9 +189,9 @@ private FlinkOptions() {
   public static final ConfigOption<Boolean> METADATA_ENABLED = ConfigOptions
       .key("metadata.enabled")
       .booleanType()
-      .defaultValue(false)
+      .defaultValue(true)
       .withFallbackKeys(HoodieMetadataConfig.ENABLE.key())
-      .withDescription("Enable the internal metadata table which serves table metadata like level file listings, default disabled");
+      .withDescription("Enable the internal metadata table which serves table metadata like level file listings, default enabled");
 
   public static final ConfigOption<Integer> METADATA_COMPACTION_DELTA_COMMITS = ConfigOptions
       .key("metadata.compaction.delta_commits")

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/TestWriteWithTimelineBasedCkpMetadata.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.configuration.FlinkOptions;
+import org.apache.hudi.sink.utils.TestWriteBase;
 import org.apache.hudi.utils.TestData;
 
 import org.apache.flink.configuration.Configuration;
@@ -28,7 +29,7 @@
 /**
  * Test cases for timeline based checkpoint metadata.
  */
-public class TestWriteWithTimelineBasedCkpMetadata extends TestWriteCopyOnWrite {
+public class TestWriteWithTimelineBasedCkpMetadata extends TestWriteBase {
 
   @Override
   protected void setUp(Configuration conf) {
@@ -59,5 +60,4 @@ public void testTimelineBasedCkpMetadataFailover() throws Exception {
         .checkWrittenDataCOW(EXPECTED5)
         .end();
   }
-
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/DeletePartitionUtils.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.client.utils;
 
 import org.apache.hudi.common.table.view.SyncableFileSystemView;
-import org.apache.hudi.exception.HoodieDeletePartitionException;
+import org.apache.hudi.exception.HoodieDeletePartitionPendingTableServiceException;
 import org.apache.hudi.table.HoodieTable;
 
 import java.util.ArrayList;
@@ -67,7 +67,7 @@ public static void checkForPendingTableServiceActions(HoodieTable table, List<St
         .forEach(x -> instantsOfOffendingPendingTableServiceAction.add(x.getRight().getTimestamp()));
 
     if (instantsOfOffendingPendingTableServiceAction.size() > 0) {
-      throw new HoodieDeletePartitionException("Failed to drop partitions. "
+      throw new HoodieDeletePartitionPendingTableServiceException("Failed to drop partitions. "
           + "Please ensure that there are no pending table service actions (clustering/compaction) for the partitions to be deleted: " + partitionsToDrop + ". "
           + "Instant(s) of offending pending table service action: "
           + instantsOfOffendingPendingTableServiceAction.stream().distinct().collect(Collectors.toList()));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/ttl/strategy/KeepByCreationTimeStrategy.java
Patch:
@@ -28,7 +28,7 @@
 import java.util.stream.Collectors;
 
 /**
- * KeepByTimeStrategy will return expired partitions by their lastCommitTime.
+ * KeepByTimeStrategy will return expired partitions by their create time.
  */
 public class KeepByCreationTimeStrategy extends KeepByTimeStrategy {
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/ttl/strategy/KeepByTimeStrategy.java
Patch:
@@ -103,7 +103,7 @@ private Map<String, Option<String>> getLastCommitTimeForPartitions(List<String>
    * @param referenceTime last commit time or creation time for partition
    */
   protected boolean isPartitionExpired(String referenceTime) {
-    String expiredTime = instantTimePlusMillis(fixInstantTimeCompatibility(referenceTime), ttlInMilis);
+    String expiredTime = instantTimePlusMillis(referenceTime, ttlInMilis);
     return fixInstantTimeCompatibility(instantTime).compareTo(expiredTime) > 0;
   }
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMetrics.java
Patch:
@@ -153,7 +153,7 @@ public void updateSizeMetrics(HoodieTableMetaClient metaClient, HoodieBackedTabl
   }
 
   protected void incrementMetric(String action, long value) {
-    LOG.info(String.format("Updating metadata metrics (%s=%d) in %s", action, value, metricsRegistry));
+    LOG.debug(String.format("Updating metadata metrics (%s=%d) in %s", action, value, metricsRegistry));
     Option<HoodieGauge<Long>> gaugeOpt = metrics.registerGauge(action);
     gaugeOpt.ifPresent(gauge -> gauge.setValue(gauge.getValue() + value));
   }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieDeltaStreamerWrapper.java
Patch:
@@ -83,7 +83,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> fetchSource() t
     StreamSync service = getDeltaSync();
     service.refreshTimeline();
     HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder()
-        .setConf((Configuration) service.getStorage().getConf().newCopy())
+        .setConf((Configuration) service.getStorage().getConf().unwrapCopy())
         .setBasePath(service.getCfg().targetBasePath)
         .build();
     String instantTime = InProcessTimeGenerator.createNewInstantTime();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/HoodieStreamer.java
Patch:
@@ -691,7 +691,7 @@ public StreamSyncService(Config cfg, HoodieSparkEngineContext hoodieSparkContext
       if (this.storage.exists(new StoragePath(cfg.targetBasePath))) {
         try {
           HoodieTableMetaClient meta = HoodieTableMetaClient.builder()
-              .setConf((Configuration) this.storage.getConf().newCopy())
+              .setConf((Configuration) this.storage.getConf().unwrapCopy())
               .setBasePath(cfg.targetBasePath).setLoadActiveTimelineOnLoad(false).build();
           tableType = meta.getTableType();
           // This will guarantee there is no surprise with table type

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HiveSchemaUtils.java
Patch:
@@ -68,8 +68,8 @@ public static org.apache.flink.table.api.Schema convertTableSchema(Table hiveTab
     allCols.addAll(hiveTable.getPartitionKeys());
 
     String pkConstraintName = hiveTable.getParameters().get(TableOptionProperties.PK_CONSTRAINT_NAME);
-    String pkColumnStr = hiveTable.getParameters().getOrDefault(FlinkOptions.RECORD_KEY_FIELD.key(), FlinkOptions.RECORD_KEY_FIELD.defaultValue());
-    List<String> pkColumns = StringUtils.split(pkColumnStr, ",");
+    String pkColumnStr = hiveTable.getParameters().get(FlinkOptions.RECORD_KEY_FIELD.key());
+    List<String> pkColumns = pkColumnStr == null ? new ArrayList<>() : StringUtils.split(pkColumnStr, ",");
 
     String[] colNames = new String[allCols.size()];
     DataType[] colTypes = new DataType[allCols.size()];
@@ -88,7 +88,7 @@ public static org.apache.flink.table.api.Schema convertTableSchema(Table hiveTab
     org.apache.flink.table.api.Schema.Builder builder = org.apache.flink.table.api.Schema.newBuilder().fromFields(colNames, colTypes);
     if (!StringUtils.isNullOrEmpty(pkConstraintName)) {
       builder.primaryKeyNamed(pkConstraintName, pkColumns);
-    } else {
+    } else if (!pkColumns.isEmpty()) {
       builder.primaryKey(pkColumns);
     }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/testutils/UtilitiesTestBase.java
Patch:
@@ -138,7 +138,6 @@ public static void initTestServices() throws Exception {
 
   public static void initTestServices(boolean needsHdfs, boolean needsHive, boolean needsZookeeper) throws Exception {
     hadoopConf = HoodieTestUtils.getDefaultHadoopConf();
-    hadoopConf.set("hive.exec.scratchdir", System.getenv("java.io.tmpdir") + "/hive");
 
     if (needsHdfs) {
       hdfsTestService = new HdfsTestService(hadoopConf);
@@ -152,6 +151,7 @@ public static void initTestServices(boolean needsHdfs, boolean needsHive, boolea
     }
     storage = HoodieStorageUtils.getStorage(fs);
 
+    hadoopConf.set("hive.exec.scratchdir", basePath + "/.tmp/hive");
     if (needsHive) {
       hiveTestService = new HiveTestService(hadoopConf);
       hiveServer = hiveTestService.start();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/GcsEventsHoodieIncrSource.java
Patch:
@@ -177,8 +177,8 @@ public Pair<Option<Dataset<Row>>, String> fetchNextBatch(Option<String> lastChec
         IncrSourceHelper.filterAndGenerateCheckpointBasedOnSourceLimit(
             filteredSourceData, sourceLimit, queryInfo, cloudObjectIncrCheckpoint);
     if (!checkPointAndDataset.getRight().isPresent()) {
-      LOG.info("Empty source, returning endpoint:" + queryInfo.getEndInstant());
-      return Pair.of(Option.empty(), queryInfo.getEndInstant());
+      LOG.info("Empty source, returning endpoint:" + checkPointAndDataset.getLeft());
+      return Pair.of(Option.empty(), checkPointAndDataset.getLeft().toString());
     }
     LOG.info("Adjusted end checkpoint :" + checkPointAndDataset.getLeft());
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java
Patch:
@@ -152,8 +152,8 @@ public Pair<Option<Dataset<Row>>, String> fetchNextBatch(Option<String> lastChec
         IncrSourceHelper.filterAndGenerateCheckpointBasedOnSourceLimit(
             filteredSourceData, sourceLimit, queryInfo, cloudObjectIncrCheckpoint);
     if (!checkPointAndDataset.getRight().isPresent()) {
-      LOG.info("Empty source, returning endpoint:" + queryInfo.getEndInstant());
-      return Pair.of(Option.empty(), queryInfo.getEndInstant());
+      LOG.info("Empty source, returning endpoint:" + checkPointAndDataset.getLeft());
+      return Pair.of(Option.empty(), checkPointAndDataset.getLeft().toString());
     }
     LOG.info("Adjusted end checkpoint :" + checkPointAndDataset.getLeft());
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestGcsEventsHoodieIncrSource.java
Patch:
@@ -242,7 +242,7 @@ public void testTwoFilesAndContinueAcrossCommits(String extension) throws IOExce
   @CsvSource({
       "1,1#path/to/file2.json,3#path/to/file4.json,1#path/to/file1.json,1",
       "2,1#path/to/file2.json,3#path/to/file4.json,1#path/to/file1.json,2",
-      "3,3#path/to/file5.json,3,1#path/to/file1.json,3"
+      "3,3#path/to/file5.json,3#path/to/file5.json,1#path/to/file1.json,3"
   })
   public void testSplitSnapshotLoad(String snapshotCheckPoint, String exptected1, String exptected2, String exptected3, String exptected4) throws IOException {
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestS3EventsHoodieIncrSource.java
Patch:
@@ -358,8 +358,8 @@ public void testEmptyDataAfterFilter() throws IOException {
 
     readAndAssert(READ_UPTO_LATEST_COMMIT, Option.of("1"), 1000L, "2", typedProperties);
     readAndAssert(READ_UPTO_LATEST_COMMIT, Option.of("1#path/to/file3.json"), 1000L, "2", typedProperties);
-    readAndAssert(READ_UPTO_LATEST_COMMIT, Option.of("2#path/to/skip4.json"), 1000L, "2", typedProperties);
-    readAndAssert(READ_UPTO_LATEST_COMMIT, Option.of("2#path/to/skip5.json"), 1000L, "2", typedProperties);
+    readAndAssert(READ_UPTO_LATEST_COMMIT, Option.of("2#path/to/skip4.json"), 1000L, "2#path/to/skip4.json", typedProperties);
+    readAndAssert(READ_UPTO_LATEST_COMMIT, Option.of("2#path/to/skip5.json"), 1000L, "2#path/to/skip5.json", typedProperties);
     readAndAssert(READ_UPTO_LATEST_COMMIT, Option.of("2"), 1000L, "2", typedProperties);
   }
 
@@ -434,7 +434,7 @@ public void testFilterAnEntireMiddleCommit() throws IOException {
   @CsvSource({
       "1,1#path/to/file2.json,3#path/to/file4.json,1#path/to/file1.json,1",
       "2,1#path/to/file2.json,3#path/to/file4.json,1#path/to/file1.json,2",
-      "3,3#path/to/file5.json,3,1#path/to/file1.json,3"
+      "3,3#path/to/file5.json,3#path/to/file5.json,1#path/to/file1.json,3"
   })
   public void testSplitSnapshotLoad(String snapshotCheckPoint, String exptected1, String exptected2, String exptected3, String exptected4) throws IOException {
 

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/JDBCExecutor.java
Patch:
@@ -47,7 +47,7 @@
  */
 public class JDBCExecutor extends QueryBasedDDLExecutor {
 
-  private static final Logger LOG = LoggerFactory.getLogger(QueryBasedDDLExecutor.class);
+  private static final Logger LOG = LoggerFactory.getLogger(JDBCExecutor.class);
 
   private Connection connection;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -2508,7 +2508,7 @@ public boolean isLogCompactionEnabledOnMetadata() {
   }
 
   public boolean isRecordIndexEnabled() {
-    return metadataConfig.enableRecordIndex();
+    return metadataConfig.isRecordIndexEnabled();
   }
 
   public int getRecordIndexMinFileGroupCount() {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java
Patch:
@@ -99,7 +99,7 @@ public class RunIndexActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I,
   public RunIndexActionExecutor(HoodieEngineContext context, HoodieWriteConfig config, HoodieTable<T, I, K, O> table, String instantTime) {
     super(context, config, table, instantTime);
     this.txnManager = new TransactionManager(config, table.getMetaClient().getStorage());
-    if (config.getMetadataConfig().enableMetrics()) {
+    if (config.getMetadataConfig().isMetricsEnabled()) {
       this.metrics = Option.of(new HoodieMetadataMetrics(config.getMetricsConfig()));
     } else {
       this.metrics = Option.empty();

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/testutils/HoodieJavaClientTestHarness.java
Patch:
@@ -251,7 +251,7 @@ protected HoodieJavaWriteClient getHoodieWriteClient(HoodieWriteConfig cfg) {
   }
 
   public void syncTableMetadata(HoodieWriteConfig writeConfig) {
-    if (!writeConfig.getMetadataConfig().enabled()) {
+    if (!writeConfig.getMetadataConfig().isEnabled()) {
       return;
     }
     // Open up the metadata table again, for syncing

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieSparkClientTestHarness.java
Patch:
@@ -531,7 +531,7 @@ public void validateMetadata(HoodieTestTable testTable, List<String> inflightCom
   }
 
   public void syncTableMetadata(HoodieWriteConfig writeConfig) {
-    if (!writeConfig.getMetadataConfig().enabled()) {
+    if (!writeConfig.getMetadataConfig().isEnabled()) {
       return;
     }
     // Open up the metadata table again, for syncing

File: hudi-common/src/main/java/org/apache/hudi/metadata/BaseTableMetadata.java
Patch:
@@ -92,7 +92,7 @@ protected BaseTableMetadata(HoodieEngineContext engineContext, HoodieMetadataCon
     this.metadataConfig = metadataConfig;
     this.isMetadataTableInitialized = dataMetaClient.getTableConfig().isMetadataTableAvailable();
 
-    if (metadataConfig.enableMetrics()) {
+    if (metadataConfig.isMetricsEnabled()) {
       this.metrics = Option.of(new HoodieMetadataMetrics(HoodieMetricsConfig.newBuilder().fromProperties(metadataConfig.getProps()).build()));
     } else {
       this.metrics = Option.empty();
@@ -412,7 +412,7 @@ Map<String, List<StoragePathInfo>> fetchAllFilesInPartitionPaths(List<StoragePat
    */
   private void checkForSpuriousDeletes(HoodieMetadataPayload metadataPayload, String partitionName) {
     if (!metadataPayload.getDeletions().isEmpty()) {
-      if (metadataConfig.ignoreSpuriousDeletes()) {
+      if (metadataConfig.shouldIgnoreSpuriousDeletes()) {
         LOG.warn("Metadata record for " + partitionName + " encountered some files to be deleted which was not added before. "
             + "Ignoring the spurious deletes as the `" + HoodieMetadataConfig.IGNORE_SPURIOUS_DELETES.key() + "` config is set to true");
       } else {

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -655,7 +655,7 @@ public Pair<HoodieMetadataLogRecordReader, Long> getLogRecordScanner(List<Hoodie
         .withLogBlockTimestamps(validInstantTimestamps)
         .enableFullScan(allowFullScan)
         .withPartition(partitionName)
-        .withEnableOptimizedLogBlocksScan(metadataConfig.doEnableOptimizedLogBlocksScan())
+        .withEnableOptimizedLogBlocksScan(metadataConfig.isOptimizedLogBlocksScanEnabled())
         .withTableMetaClient(metadataMetaClient)
         .build();
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadata.java
Patch:
@@ -117,7 +117,7 @@ static HoodieTableMetadata create(HoodieEngineContext engineContext, HoodieMetad
   }
 
   static HoodieTableMetadata create(HoodieEngineContext engineContext, HoodieMetadataConfig metadataConfig, String datasetBasePath, boolean reuse) {
-    if (metadataConfig.enabled()) {
+    if (metadataConfig.isEnabled()) {
       HoodieBackedTableMetadata metadata = createHoodieBackedTableMetadata(engineContext, metadataConfig, datasetBasePath, reuse);
       // If the MDT is not initialized then we fallback to FSBackedTableMetadata
       if (metadata.isMetadataTableInitialized()) {

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -1685,7 +1685,7 @@ public static boolean getMetadataPartitionsNeedingWriteStatusTracking(HoodieMeta
     }
 
     // Does any enabled partition being enabled need to track the written records
-    return config.enableRecordIndex();
+    return config.isRecordIndexEnabled();
   }
 
   /**

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/FileIndex.java
Patch:
@@ -306,7 +306,7 @@ private boolean isDataSkippingFeasible(boolean dataSkippingEnabled) {
     //          - Any expression not directly referencing top-level column (for ex, sub-queries, since there's
     //          nothing CSI in particular could be applied for)
     if (dataSkippingEnabled) {
-      if (metadataConfig.enabled()) {
+      if (metadataConfig.isEnabled()) {
         return true;
       } else {
         LOG.warn("Data skipping requires Metadata Table to be enabled! Disable the data skipping");

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java
Patch:
@@ -364,8 +364,7 @@ private HoodieData<HoodieRecord<T>> readRecordsForGroupBaseFiles(JavaSparkContex
               Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(writeConfig.getSchema()));
               HoodieFileReader baseFileReader = getBaseOrBootstrapFileReader(hadoopConf, bootstrapBasePath, partitionFields, clusteringOp);
 
-              Option<BaseKeyGenerator> keyGeneratorOp =
-                  writeConfig.populateMetaFields() ? Option.empty() : Option.of((BaseKeyGenerator) HoodieSparkKeyGeneratorFactory.createKeyGenerator(writeConfig.getProps()));
+              Option<BaseKeyGenerator> keyGeneratorOp = HoodieSparkKeyGeneratorFactory.createBaseKeyGenerator(writeConfig);
               // NOTE: Record have to be cloned here to make sure if it holds low-level engine-specific
               //       payload pointing into a shared, mutable (underlying) buffer we get a clean copy of
               //       it since these records will be shuffled later.

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SingleSparkJobExecutionStrategy.java
Patch:
@@ -148,8 +148,7 @@ private Iterator<HoodieRecord<T>> readRecordsForGroupBaseFiles(List<ClusteringOp
         try {
           HoodieFileReader baseFileReader = HoodieFileReaderFactory.getReaderFactory(recordType)
               .getFileReader(writeConfig, getHoodieTable().getHadoopConf(), new StoragePath(clusteringOp.getDataFilePath()));
-          Option<BaseKeyGenerator> keyGeneratorOp =
-              writeConfig.populateMetaFields() ? Option.empty() : Option.of((BaseKeyGenerator) HoodieSparkKeyGeneratorFactory.createKeyGenerator(writeConfig.getProps()));
+          Option<BaseKeyGenerator> keyGeneratorOp = HoodieSparkKeyGeneratorFactory.createBaseKeyGenerator(writeConfig);
           // NOTE: Record have to be cloned here to make sure if it holds low-level engine-specific
           //       payload pointing into a shared, mutable (underlying) buffer we get a clean copy of
           //       it since these records will be shuffled later.

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestCustomKeyGenerator.java
Patch:
@@ -377,6 +377,6 @@ public void testComplexRecordKeysWithComplexPartitionPath(TypedProperties props)
 
   private static Throwable getNestedConstructorErrorCause(Exception e) {
     // custom key generator will fail in the constructor, and we must unwrap the cause for asserting error messages
-    return e.getCause().getCause().getCause();
+    return e.getCause().getCause();
   }
 }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -372,7 +372,7 @@ public void testKafkaConnectCheckpointProvider() throws IOException {
 
   @Test
   public void testPropsWithInvalidKeyGenerator() {
-    Exception e = assertThrows(IOException.class, () -> {
+    Exception e = assertThrows(HoodieException.class, () -> {
       String tableBasePath = basePath + "/test_table_invalid_key_gen";
       HoodieDeltaStreamer deltaStreamer =
           new HoodieDeltaStreamer(TestHelpers.makeConfig(tableBasePath, WriteOperationType.BULK_INSERT,
@@ -381,7 +381,7 @@ public void testPropsWithInvalidKeyGenerator() {
     }, "Should error out when setting the key generator class property to an invalid value");
     // expected
     LOG.warn("Expected error during getting the key generator", e);
-    assertTrue(e.getMessage().contains("Could not load key generator class invalid"));
+    assertTrue(e.getMessage().contains("Unable to load class"));
   }
 
   private static Stream<Arguments> provideInferKeyGenArgs() {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieErrorTableConfig.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.common.config.ConfigClassProperty;
 import org.apache.hudi.common.config.ConfigGroups;
 import org.apache.hudi.common.config.ConfigProperty;
+import org.apache.hudi.common.config.HoodieConfig;
 
 import javax.annotation.concurrent.Immutable;
 
@@ -30,7 +31,7 @@
 @ConfigClassProperty(name = "Error table Configs",
     groupName = ConfigGroups.Names.WRITE_CLIENT,
     description = "Configurations that are required for Error table configs")
-public class HoodieErrorTableConfig {
+public class HoodieErrorTableConfig extends HoodieConfig {
   public static final ConfigProperty<Boolean> ERROR_TABLE_ENABLED = ConfigProperty
       .key("hoodie.errortable.enable")
       .defaultValue(false)

File: hudi-common/src/main/java/org/apache/hudi/common/config/TimestampKeyGeneratorConfig.java
Patch:
@@ -31,7 +31,7 @@
         + "the partition field. The field values are interpreted as timestamps and not just "
         + "converted to string while generating partition path value for records. Record key is "
         + "same as before where it is chosen by field name.")
-public class TimestampKeyGeneratorConfig {
+public class TimestampKeyGeneratorConfig extends HoodieConfig {
   private static final String TIMESTAMP_KEYGEN_CONFIG_PREFIX = "hoodie.keygen.timebased.";
   @Deprecated
   private static final String OLD_TIMESTAMP_KEYGEN_CONFIG_PREFIX = "hoodie.deltastreamer.keygen.timebased.";

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/config/SqlFileBasedSourceConfig.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.common.config.ConfigClassProperty;
 import org.apache.hudi.common.config.ConfigGroups;
 import org.apache.hudi.common.config.ConfigProperty;
+import org.apache.hudi.common.config.HoodieConfig;
 
 import javax.annotation.concurrent.Immutable;
 
@@ -33,7 +34,7 @@
     groupName = ConfigGroups.Names.HUDI_STREAMER,
     subGroupName = ConfigGroups.SubGroupNames.DELTA_STREAMER_SOURCE,
     description = "Configurations controlling the behavior of File-based SQL Source in Hudi Streamer.")
-public class SqlFileBasedSourceConfig {
+public class SqlFileBasedSourceConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> SOURCE_SQL_FILE = ConfigProperty
       .key(STREAMER_CONFIG_PREFIX + "source.sql.file")

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java
Patch:
@@ -139,7 +139,7 @@ public String addPartitionMeta(
           HoodiePartitionMetadata partitionMetadata =
               new HoodiePartitionMetadata(HoodieCLI.storage, latestCommit, basePath, partitionPath,
                   client.getTableConfig().getPartitionMetafileFormat());
-          partitionMetadata.trySave(0);
+          partitionMetadata.trySave();
           row[2] = "Repaired";
         }
       }
@@ -257,7 +257,7 @@ public String migratePartitionMeta(
           HoodiePartitionMetadata partitionMetadata =
               new HoodiePartitionMetadata(HoodieCLI.storage, latestCommit, basePath, partition,
                   Option.of(client.getTableConfig().getBaseFileFormat()));
-          partitionMetadata.trySave(0);
+          partitionMetadata.trySave();
         }
 
         // delete it, in case we failed midway last time.

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -209,7 +209,7 @@ private void init(HoodieRecord record) {
           new StoragePath(config.getBasePath()),
           FSUtils.getPartitionPath(config.getBasePath(), partitionPath),
           hoodieTable.getPartitionMetafileFormat());
-      partitionMetadata.trySave(getPartitionId());
+      partitionMetadata.trySave();
 
       this.writer = createLogWriter(getFileInstant(record));
     } catch (Exception e) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -100,7 +100,7 @@ public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTa
           new StoragePath(config.getBasePath()),
           FSUtils.getPartitionPath(config.getBasePath(), partitionPath),
           hoodieTable.getPartitionMetafileFormat());
-      partitionMetadata.trySave(getPartitionId());
+      partitionMetadata.trySave();
       createMarkerFile(partitionPath,
           FSUtils.makeBaseFileName(this.instantTime, this.writeToken, this.fileId, hoodieTable.getBaseFileExtension()));
       this.fileWriter =

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -177,7 +177,7 @@ private void init(String fileId, String partitionPath, HoodieBaseFile baseFileTo
           new StoragePath(config.getBasePath()),
           FSUtils.getPartitionPath(config.getBasePath(), partitionPath),
           hoodieTable.getPartitionMetafileFormat());
-      partitionMetadata.trySave(getPartitionId());
+      partitionMetadata.trySave();
 
       String newFileName = FSUtils.makeBaseFileName(instantTime, writeToken, fileId, hoodieTable.getBaseFileExtension());
       makeOldAndNewFilePaths(partitionPath, latestValidFilePath, newFileName);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataCreateHandle.java
Patch:
@@ -105,7 +105,7 @@ public HoodieRowDataCreateHandle(HoodieTable table, HoodieWriteConfig writeConfi
               new StoragePath(writeConfig.getBasePath()),
               FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath),
               table.getPartitionMetafileFormat());
-      partitionMetadata.trySave(taskPartitionId);
+      partitionMetadata.trySave();
       createMarkerFile(partitionPath, FSUtils.makeBaseFileName(this.instantTime, getWriteToken(), this.fileId, table.getBaseFileExtension()));
       this.fileWriter = createNewFileWriter(path, table, writeConfig, rowType);
     } catch (IOException e) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowCreateHandle.java
Patch:
@@ -143,7 +143,7 @@ public HoodieRowCreateHandle(HoodieTable table,
               new StoragePath(writeConfig.getBasePath()),
               FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath),
               table.getPartitionMetafileFormat());
-      partitionMetadata.trySave(taskPartitionId);
+      partitionMetadata.trySave();
 
       createMarkerFile(partitionPath, fileName, instantTime, table, writeConfig);
 

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodiePartitionMetadata.java
Patch:
@@ -77,7 +77,7 @@ public void testTextFormatMetaFile(Option<HoodieFileFormat> format) throws IOExc
     HoodiePartitionMetadata writtenMetadata = new HoodiePartitionMetadata(
         metaClient.getStorage(), commitTime, new StoragePath(basePath), partitionPath,
         format);
-    writtenMetadata.trySave(0);
+    writtenMetadata.trySave();
 
     // when
     HoodiePartitionMetadata readMetadata = new HoodiePartitionMetadata(

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java
Patch:
@@ -253,7 +253,8 @@ public static String getCommitTimeAtUTC(long epochSecond) {
    * @deprecated please use non-static version
    */
   public static void writePartitionMetadataDeprecated(HoodieStorage storage,
-                                                      String[] partitionPaths, String basePath) {
+                                                      String[] partitionPaths,
+                                                      String basePath) {
     new HoodieTestDataGenerator().writePartitionMetadata(storage, partitionPaths, basePath);
   }
 
@@ -268,7 +269,7 @@ public void writePartitionMetadata(HoodieStorage storage,
                                      String basePath) {
     for (String partitionPath : partitionPaths) {
       new HoodiePartitionMetadata(storage, "000", new StoragePath(basePath),
-          new StoragePath(basePath, partitionPath), Option.empty()).trySave(0);
+          new StoragePath(basePath, partitionPath), Option.empty()).trySave();
     }
   }
 

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestTablePathUtils.java
Patch:
@@ -82,11 +82,11 @@ private void setup(Option<HoodieFileFormat> partitionMetafileFormat) throws IOEx
     HoodiePartitionMetadata partitionMetadata1 = new HoodiePartitionMetadata(
         storage, Instant.now().toString(), tablePath,
         partitionPath1, partitionMetafileFormat);
-    partitionMetadata1.trySave(1);
+    partitionMetadata1.trySave();
     HoodiePartitionMetadata partitionMetadata2 = new HoodiePartitionMetadata(
         storage, Instant.now().toString(), tablePath,
         partitionPath2, partitionMetafileFormat);
-    partitionMetadata2.trySave(2);
+    partitionMetadata2.trySave();
 
     // Create files
     URI filePathURI1 =

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/testutils/InputFormatTestUtil.java
Patch:
@@ -524,7 +524,7 @@ public static void setupPartition(java.nio.file.Path basePath, java.nio.file.Pat
               new StoragePath(partitionPath.toAbsolutePath().toString()),
               Option.of(HoodieFileFormat.PARQUET));
 
-      partitionMetadata.trySave((int) (Math.random() * 1000));
+      partitionMetadata.trySave();
     }
   }
 

File: hudi-io/src/main/java/org/apache/hudi/storage/HoodieStorage.java
Patch:
@@ -257,7 +257,7 @@ public abstract boolean rename(StoragePath oldPath,
    */
   @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)
   public final void createImmutableFileInPath(StoragePath path,
-                                              Option<byte[]> content) throws IOException {
+                                              Option<byte[]> content) throws HoodieIOException {
     OutputStream fsout = null;
     StoragePath tmpPath = null;
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/streamer/TestStreamSyncUnitTests.java
Patch:
@@ -141,7 +141,7 @@ void testFetchNextBatchFromSource(Boolean useRowWriter, Boolean hasTransformer,
   @MethodSource("getCheckpointToResumeCases")
   void testGetCheckpointToResume(HoodieStreamer.Config cfg, HoodieCommitMetadata commitMetadata, Option<String> expectedResumeCheckpoint) throws IOException {
     HoodieSparkEngineContext hoodieSparkEngineContext = mock(HoodieSparkEngineContext.class);
-    FileSystem fs = mock(FileSystem.class);
+    HoodieStorage storage = HoodieStorageUtils.getStorage(mock(FileSystem.class));
     TypedProperties props = new TypedProperties();
     SparkSession sparkSession = mock(SparkSession.class);
     Configuration configuration = mock(Configuration.class);
@@ -152,7 +152,7 @@ void testGetCheckpointToResume(HoodieStreamer.Config cfg, HoodieCommitMetadata c
     when(commitsTimeline.lastInstant()).thenReturn(Option.of(hoodieInstant));
 
     StreamSync streamSync = new StreamSync(cfg, sparkSession, props, hoodieSparkEngineContext,
-        fs, configuration, client -> true, null,Option.empty(),null,Option.empty(),true,true);
+        storage, configuration, client -> true, null,Option.empty(),null,Option.empty(),true,true);
     StreamSync spy = spy(streamSync);
     doReturn(Option.of(commitMetadata)).when(spy).getLatestCommitMetadataWithValidCheckpointInfo(any());
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -38,7 +38,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.data.HoodieJavaRDD;
 import org.apache.hudi.index.functional.HoodieFunctionalIndex;
-import org.apache.hudi.index.functional.HoodieSparkFunctionalIndex;
+import org.apache.hudi.HoodieSparkFunctionalIndex;
 import org.apache.hudi.metrics.DistributedRegistry;
 import org.apache.hudi.metrics.MetricsReporterType;
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/functional/TestHoodieSparkFunctionalIndex.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.hudi.index.functional;
 
+import org.apache.hudi.HoodieSparkFunctionalIndex;
 import org.apache.hudi.testutils.HoodieSparkClientTestHarness;
 
 import org.apache.spark.sql.Column;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/WriteMarkers.java
Patch:
@@ -86,6 +86,7 @@ public Option<Path> create(String partitionPath, String fileName, IOType type, H
       HoodieTimeline pendingReplaceTimeline = activeTimeline.filterPendingReplaceTimeline();
       // TODO If current is compact or clustering then create marker directly without early conflict detection.
       // Need to support early conflict detection between table service and common writers.
+      // ok to use filterPendingReplaceTimeline().containsInstant because early conflict detection is not relevant for insert overwrite as well
       if (pendingCompactionTimeline.containsInstant(instantTime) || pendingReplaceTimeline.containsInstant(instantTime)) {
         return create(partitionPath, fileName, type, false);
       }
@@ -126,6 +127,7 @@ public Option<Path> createIfNotExists(String partitionPath, String fileName, IOT
       HoodieTimeline pendingReplaceTimeline = activeTimeline.filterPendingReplaceTimeline();
       // TODO If current is compact or clustering then create marker directly without early conflict detection.
       // Need to support early conflict detection between table service and common writers.
+      // ok to use filterPendingReplaceTimeline().containsInstant because early conflict detection is not relevant for insert overwrite as well
       if (pendingCompactionTimeline.containsInstant(instantTime) || pendingReplaceTimeline.containsInstant(instantTime)) {
         return create(partitionPath, fileName, type, true);
       }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/ClusteringPlanSourceFunction.java
Patch:
@@ -75,7 +75,7 @@ public void open(Configuration parameters) throws Exception {
 
   @Override
   public void run(SourceContext<ClusteringPlanEvent> sourceContext) throws Exception {
-    boolean isPending = StreamerUtil.createMetaClient(conf).getActiveTimeline().filterPendingReplaceTimeline().containsInstant(clusteringInstantTime);
+    boolean isPending = StreamerUtil.createMetaClient(conf).getActiveTimeline().isPendingClusterInstant(clusteringInstantTime);
     if (isPending) {
       for (HoodieClusteringGroup clusteringGroup : clusteringPlan.getInputGroups()) {
         LOG.info("Execute clustering plan for instant {} as {} file slices", clusteringInstantTime, clusteringGroup.getSlices().size());

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/ClusteringUtil.java
Patch:
@@ -105,7 +105,7 @@ public static void rollbackClustering(HoodieFlinkTable<?> table, HoodieFlinkWrit
    */
   public static void rollbackClustering(HoodieFlinkTable<?> table, HoodieFlinkWriteClient<?> writeClient, String instantTime) {
     HoodieInstant inflightInstant = HoodieTimeline.getReplaceCommitInflightInstant(instantTime);
-    if (table.getMetaClient().reloadActiveTimeline().filterPendingReplaceTimeline().containsInstant(inflightInstant)) {
+    if (table.getMetaClient().reloadActiveTimeline().isPendingClusterInstant(instantTime)) {
       LOG.warn("Rollback failed clustering instant: [" + instantTime + "]");
       table.rollbackInflightClustering(inflightInstant,
           commitToRollback -> writeClient.getTableServiceClient().getPendingRollbackInfo(table.getMetaClient(), commitToRollback, false));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataWriter.java
Patch:
@@ -43,8 +43,9 @@ public interface HoodieTableMetadataWriter extends Serializable, AutoCloseable {
    *
    * @param engineContext
    * @param indexPartitionInfos - information about partitions to build such as partition type and base instant time
+   * @param instantTime The async index instant time from data table
    */
-  void buildMetadataPartitions(HoodieEngineContext engineContext, List<HoodieIndexPartitionInfo> indexPartitionInfos) throws IOException;
+  void buildMetadataPartitions(HoodieEngineContext engineContext, List<HoodieIndexPartitionInfo> indexPartitionInfos, String instantTime) throws IOException;
 
   /**
    * Drop the given metadata partitions.

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java
Patch:
@@ -149,7 +149,7 @@ public Option<HoodieIndexCommitMetadata> execute() {
           String indexUptoInstant = indexPartitionInfos.get(0).getIndexUptoInstant();
           LOG.info("Starting Index Building with base instant: " + indexUptoInstant);
           HoodieTimer timer = HoodieTimer.start();
-          metadataWriter.buildMetadataPartitions(context, indexPartitionInfos);
+          metadataWriter.buildMetadataPartitions(context, indexPartitionInfos, indexInstant.getTimestamp());
           metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.INITIALIZE_STR, timer.endTimer()));
 
           // get remaining instants to catchup

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -164,7 +164,7 @@ protected void commitInternal(String instantTime, Map<MetadataPartitionType, Hoo
 
     // reload timeline
     metadataMetaClient.reloadActiveTimeline();
-    metadataMetaClient.getActiveTimeline().getDeltaCommitTimeline().filterCompletedInstants().lastInstant().ifPresent(instant -> cleanIfNecessary(writeClient, instant.getTimestamp()));
+    cleanIfNecessary(writeClient);
     writeClient.archive();
 
     // Update total size of the metadata and count of base/log files

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/TestStreamWriteOperatorCoordinator.java
Patch:
@@ -64,6 +64,7 @@
 
 import static org.hamcrest.CoreMatchers.instanceOf;
 import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.CoreMatchers.not;
 import static org.hamcrest.CoreMatchers.startsWith;
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;
@@ -400,7 +401,8 @@ void testSyncMetadataTableWithLogCompaction() throws Exception {
     metadataTableMetaClient.reloadActiveTimeline();
     completedTimeline = metadataTableMetaClient.reloadActiveTimeline().filterCompletedAndCompactionInstants();
     assertThat("One instant need to sync to metadata table", completedTimeline.countInstants(), is(7));
-    assertThat(completedTimeline.nthFromLastInstant(1).get().getTimestamp(), is(instant + "005"));
+    assertThat("The log compaction instant time should be new generated",
+        completedTimeline.nthFromLastInstant(1).get().getTimestamp(), not(instant));
     // log compaction is another delta commit
     assertThat(completedTimeline.nthFromLastInstant(1).get().getAction(), is(HoodieTimeline.DELTA_COMMIT_ACTION));
   }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieIndexer.java
Patch:
@@ -207,7 +207,7 @@ public void testIndexerWithWriterFinishingFirst() throws IOException {
     HoodieBackedTableMetadata metadata = new HoodieBackedTableMetadata(
         context(), metadataConfig, metaClient.getBasePathV2().toString());
     HoodieTableMetaClient metadataMetaClient = metadata.getMetadataMetaClient();
-    String mdtCommitTime = HoodieTableMetadataUtil.createAsyncIndexerTimestamp(indexUptoInstantTime);
+    String mdtCommitTime = indexingInstant.getTimestamp();
     assertTrue(metadataMetaClient.getActiveTimeline().containsInstant(mdtCommitTime));
 
     // Reverts both instants to inflight state, to simulate inflight indexing instants

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java
Patch:
@@ -278,7 +278,7 @@ public void run() {
         try {
           LOG.warn("running logformatwriter hook");
           if (output != null) {
-            close();
+            closeStream();
           }
         } catch (Exception e) {
           LOG.warn("unable to close output stream for log file " + logFile, e);

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -224,7 +224,7 @@ public void buildFunctionalIndexDefinition(String indexMetaPath,
    * Returns Option of {@link HoodieFunctionalIndexMetadata} from index definition file if present, else returns empty Option.
    */
   public Option<HoodieFunctionalIndexMetadata> getFunctionalIndexMetadata() {
-    if (functionalIndexMetadata.isPresent()) {
+    if (functionalIndexMetadata.isPresent() && !functionalIndexMetadata.get().getIndexDefinitions().isEmpty()) {
       return functionalIndexMetadata;
     }
     if (tableConfig.getIndexDefinitionPath().isPresent() && StringUtils.nonEmpty(tableConfig.getIndexDefinitionPath().get())) {

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -125,6 +125,7 @@
 import static org.apache.hudi.avro.AvroSchemaUtils.resolveNullableSchema;
 import static org.apache.hudi.avro.HoodieAvroUtils.addMetadataFields;
 import static org.apache.hudi.avro.HoodieAvroUtils.getNestedFieldSchemaFromWriteSchema;
+import static org.apache.hudi.avro.HoodieAvroUtils.getSchemaForFields;
 import static org.apache.hudi.avro.HoodieAvroUtils.unwrapAvroValueWrapper;
 import static org.apache.hudi.common.config.HoodieCommonConfig.DEFAULT_MAX_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES;
 import static org.apache.hudi.common.config.HoodieCommonConfig.DISK_MAP_BITCASK_COMPRESSION_ENABLED;
@@ -1081,7 +1082,7 @@ public static List<FileSlice> getPartitionLatestFileSlicesIncludingInflight(Hood
                                                                               Option<HoodieTableFileSystemView> fileSystemView,
                                                                               String partition) {
     HoodieTableFileSystemView fsView = fileSystemView.orElseGet(() -> getFileSystemView(metaClient));
-    Stream<FileSlice> fileSliceStream = fsView.fetchLatestFileSlicesIncludingInflight(partition);
+    Stream<FileSlice> fileSliceStream = fsView.getLatestFileSlicesIncludingInflight(partition);
     return fileSliceStream
         .sorted(Comparator.comparing(FileSlice::getFileId))
         .collect(Collectors.toList());
@@ -1919,7 +1920,7 @@ public HoodieRecord next() {
   public static Schema getProjectedSchemaForFunctionalIndex(HoodieFunctionalIndexDefinition indexDefinition, HoodieTableMetaClient metaClient) throws Exception {
     TableSchemaResolver schemaResolver = new TableSchemaResolver(metaClient);
     Schema tableSchema = schemaResolver.getTableAvroSchema();
-    return HoodieAvroUtils.getSchemaForFields(tableSchema, indexDefinition.getSourceFields());
+    return addMetadataFields(getSchemaForFields(tableSchema, indexDefinition.getSourceFields()));
   }
 
   private static Path filePath(String basePath, String partition, String filename) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodiePayloadConfig.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.common.config.ConfigGroups;
 import org.apache.hudi.common.config.ConfigProperty;
 import org.apache.hudi.common.config.HoodieConfig;
-import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
+import org.apache.hudi.common.model.DefaultHoodieRecordPayload;
 
 import java.io.File;
 import java.io.FileReader;
@@ -50,7 +50,7 @@ public class HoodiePayloadConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> PAYLOAD_CLASS_NAME = ConfigProperty
       .key("hoodie.compaction.payload.class")
-      .defaultValue(OverwriteWithLatestAvroPayload.class.getName())
+      .defaultValue(DefaultHoodieRecordPayload.class.getName())
       .markAdvanced()
       .withDocumentation("This needs to be same as class used during insert/upserts. Just like writing, compaction also uses "
         + "the record payload class to merge records in the log against each other, merge again with the base file and "

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -39,13 +39,13 @@
 import org.apache.hudi.common.engine.EngineType;
 import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.fs.FileSystemRetryConfig;
+import org.apache.hudi.common.model.DefaultHoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieAvroRecordMerger;
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieRecordMerger;
 import org.apache.hudi.common.model.HoodieTableType;
-import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.model.RecordPayloadType;
 import org.apache.hudi.common.model.WriteConcurrencyMode;
 import org.apache.hudi.common.model.WriteOperationType;
@@ -154,14 +154,14 @@ public class HoodieWriteConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> WRITE_PAYLOAD_CLASS_NAME = ConfigProperty
       .key("hoodie.datasource.write.payload.class")
-      .defaultValue(OverwriteWithLatestAvroPayload.class.getName())
+      .defaultValue(DefaultHoodieRecordPayload.class.getName())
       .markAdvanced()
       .withDocumentation("Payload class used. Override this, if you like to roll your own merge logic, when upserting/inserting. "
           + "This will render any value set for PRECOMBINE_FIELD_OPT_VAL in-effective");
 
   public static final ConfigProperty<String> WRITE_PAYLOAD_TYPE = ConfigProperty
       .key("hoodie.datasource.write.payload.type")
-      .defaultValue(RecordPayloadType.OVERWRITE_LATEST_AVRO.name())
+      .defaultValue(RecordPayloadType.HOODIE_AVRO_DEFAULT.name())
       .markAdvanced()
       .withDocumentation(RecordPayloadType.class);
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/DefaultHoodieRecordPayload.java
Patch:
@@ -37,9 +37,11 @@
 import java.util.concurrent.atomic.AtomicBoolean;
 
 /**
+ * Default payload.
  * {@link HoodieRecordPayload} impl that honors ordering field in both preCombine and combineAndGetUpdateValue.
  * <p>
- * 1. preCombine - Picks the latest delta record for a key, based on an ordering field 2. combineAndGetUpdateValue/getInsertValue - Chooses the latest record based on ordering field value.
+ * 1. preCombine - Picks the latest delta record for a key, based on an ordering field
+ * 2. combineAndGetUpdateValue/getInsertValue - Chooses the latest record based on ordering field value.
  */
 public class DefaultHoodieRecordPayload extends OverwriteWithLatestAvroPayload {
   public static final String METADATA_EVENT_TIME_KEY = "metadata.event_time.key";

File: hudi-common/src/main/java/org/apache/hudi/common/model/OverwriteWithLatestAvroPayload.java
Patch:
@@ -30,8 +30,6 @@
 import java.util.Objects;
 
 /**
- * Default payload.
- *
  * <ol>
  * <li> preCombine - Picks the latest delta record for a key, based on an ordering field;
  * <li> combineAndGetUpdateValue/getInsertValue - Simply overwrites storage with latest delta record

File: hudi-common/src/main/java/org/apache/hudi/common/model/RecordPayloadType.java
Patch:
@@ -49,7 +49,7 @@ public enum RecordPayloadType {
   @EnumFieldDescription("Subclass of OVERWRITE_LATEST_AVRO used for delta streamer.")
   OVERWRITE_NON_DEF_LATEST_AVRO(OverwriteNonDefaultsWithLatestAvroPayload.class.getName()),
 
-  @EnumFieldDescription("Default payload used for delta streamer.")
+  @EnumFieldDescription("Honors ordering field in preCombine and overwrites storage with latest delta record in combineAndGetUpdateValue")
   OVERWRITE_LATEST_AVRO(OverwriteWithLatestAvroPayload.class.getName()),
 
   @EnumFieldDescription("Used for partial update to Hudi Table.")

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -24,12 +24,12 @@
 import org.apache.hudi.common.config.OrderedProperties;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.BootstrapIndexType;
+import org.apache.hudi.common.model.DefaultHoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordMerger;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.HoodieTimelineTimeZone;
-import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.model.RecordPayloadType;
 import org.apache.hudi.common.table.cdc.HoodieCDCSupplementalLoggingMode;
 import org.apache.hudi.common.table.timeline.HoodieInstantTimeGenerator;
@@ -167,14 +167,14 @@ public class HoodieTableConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> PAYLOAD_CLASS_NAME = ConfigProperty
       .key("hoodie.compaction.payload.class")
-      .defaultValue(OverwriteWithLatestAvroPayload.class.getName())
+      .defaultValue(DefaultHoodieRecordPayload.class.getName())
       .deprecatedAfter("1.0.0")
       .withDocumentation("Payload class to use for performing compactions, i.e merge delta logs with current base file and then "
           + " produce a new base file.");
 
   public static final ConfigProperty<String> PAYLOAD_TYPE = ConfigProperty
       .key("hoodie.compaction.payload.type")
-      .defaultValue(RecordPayloadType.OVERWRITE_LATEST_AVRO.name())
+      .defaultValue(RecordPayloadType.HOODIE_AVRO_DEFAULT.name())
       .sinceVersion("1.0.0")
       .withDocumentation(RecordPayloadType.class);
 

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestHiveTableSchemaEvolution.java
Patch:
@@ -97,7 +97,8 @@ public void testHiveReadSchemaEvolutionTable(String tableType) throws Exception
 
       spark.sql("set hoodie.schema.on.read.enable=true");
       spark.sql(String.format("create table %s (col0 int, col1 float, col2 string) using hudi "
-              + "tblproperties (type='%s', primaryKey='col0', preCombineField='col1') location '%s'",
+              + "tblproperties (type='%s', primaryKey='col0', preCombineField='col1', "
+              + "hoodie.compaction.payload.class='org.apache.hudi.common.model.OverwriteWithLatestAvroPayload') location '%s'",
           tableName, tableType, path));
       spark.sql(String.format("insert into %s values(1, 1.1, 'text')", tableName));
       spark.sql(String.format("update %s set col2 = 'text2' where col0 = 1", tableName));

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestAvroDFSSource.java
Patch:
@@ -39,8 +39,7 @@ public void setup() throws Exception {
   }
 
   @Override
-  protected Source prepareDFSSource() {
-    TypedProperties props = new TypedProperties();
+  protected Source prepareDFSSource(TypedProperties props) {
     props.setProperty("hoodie.streamer.source.dfs.root", dfsRoot);
     try {
       return new AvroDFSSource(props, jsc, sparkSession, schemaProvider);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestCsvDFSSource.java
Patch:
@@ -46,8 +46,7 @@ public void setup() throws Exception {
   }
 
   @Override
-  public Source prepareDFSSource() {
-    TypedProperties props = new TypedProperties();
+  public Source prepareDFSSource(TypedProperties props) {
     props.setProperty("hoodie.streamer.source.dfs.root", dfsRoot);
     props.setProperty("hoodie.streamer.csv.header", Boolean.toString(true));
     props.setProperty("hoodie.streamer.csv.sep", "\t");

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestParquetDFSSource.java
Patch:
@@ -41,8 +41,7 @@ public void setup() throws Exception {
   }
 
   @Override
-  public Source prepareDFSSource() {
-    TypedProperties props = new TypedProperties();
+  public Source prepareDFSSource(TypedProperties props) {
     props.setProperty("hoodie.streamer.source.dfs.root", dfsRoot);
     return new ParquetDFSSource(props, jsc, sparkSession, schemaProvider);
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java
Patch:
@@ -245,6 +245,7 @@ private List<String> getPartitionsFromDeletedSavepoint(HoodieCleanMetadata clean
       Option<HoodieInstant> instantOption = hoodieTable.getCompletedCommitsTimeline().filter(instant -> instant.getTimestamp().equals(savepointCommit)).firstInstant();
       if (!instantOption.isPresent()) {
         LOG.warn("Skipping to process a commit for which savepoint was removed as the instant moved to archived timeline already");
+        return Stream.empty();
       }
       HoodieInstant instant = instantOption.get();
       return getPartitionsForInstants(instant);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/checkpointing/TestKafkaConnectHdfsProvider.java
Patch:
@@ -62,7 +62,7 @@ public void testValidKafkaConnectPath() throws Exception {
     new File(topicPath + "/year=2016/month=05/day=02/"
         + "random_snappy_2" + BASE_FILE_EXTENSION).createNewFile();
     final TypedProperties props = new TypedProperties();
-    props.put("hoodie.deltastreamer.checkpoint.provider.path", topicPath.toString());
+    props.put("hoodie.streamer.checkpoint.provider.path", topicPath.toString());
     final InitialCheckPointProvider provider = new KafkaConnectHdfsProvider(props);
     provider.init(HoodieTestUtils.getDefaultHadoopConf());
     assertEquals("topic1,0:300,1:200", provider.getCheckpoint());
@@ -83,7 +83,7 @@ public void testMissingPartition() throws Exception {
     new File(topicPath + "/year=2016/month=05/day=02/"
         + "topic1+0+201+300" + BASE_FILE_EXTENSION).createNewFile();
     final TypedProperties props = new TypedProperties();
-    props.put("hoodie.deltastreamer.checkpoint.provider.path", topicPath.toString());
+    props.put("hoodie.streamer.checkpoint.provider.path", topicPath.toString());
     final InitialCheckPointProvider provider = new KafkaConnectHdfsProvider(props);
     provider.init(HoodieTestUtils.getDefaultHadoopConf());
     assertThrows(HoodieException.class, provider::getCheckpoint);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamerWithMultiWriter.java
Patch:
@@ -320,8 +320,8 @@ private static TypedProperties prepareMultiWriterProps(FileSystem fs, String bas
     props.setProperty("hoodie.datasource.write.keygenerator.class", TestHoodieDeltaStreamer.TestGenerator.class.getName());
     props.setProperty("hoodie.datasource.write.recordkey.field", "_row_key");
     props.setProperty("hoodie.datasource.write.partitionpath.field", "partition_path");
-    props.setProperty("hoodie.deltastreamer.schemaprovider.source.schema.file", basePath + "/source.avsc");
-    props.setProperty("hoodie.deltastreamer.schemaprovider.target.schema.file", basePath + "/target.avsc");
+    props.setProperty("hoodie.streamer.schemaprovider.source.schema.file", basePath + "/source.avsc");
+    props.setProperty("hoodie.streamer.schemaprovider.target.schema.file", basePath + "/target.avsc");
 
     props.setProperty("include", "base.properties");
     props.setProperty("hoodie.write.concurrency.mode", "optimistic_concurrency_control");

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/BaseTestKafkaSource.java
Patch:
@@ -169,7 +169,7 @@ public void testProtoKafkaSourceInsertRecordsLessSourceLimit() {
     testUtils.createTopic(topic, 2);
     TypedProperties props = createPropsForKafkaSource(topic, Long.MAX_VALUE, "earliest");
     SourceFormatAdapter kafkaSource = createSource(props);
-    props.setProperty("hoodie.deltastreamer.kafka.source.maxEvents", "500");
+    props.setProperty("hoodie.streamer.kafka.source.maxEvents", "500");
 
     /*
      1. maxEventsFromKafkaSourceProp set to more than generated insert records

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestAvroDFSSource.java
Patch:
@@ -41,7 +41,7 @@ public void setup() throws Exception {
   @Override
   protected Source prepareDFSSource() {
     TypedProperties props = new TypedProperties();
-    props.setProperty("hoodie.deltastreamer.source.dfs.root", dfsRoot);
+    props.setProperty("hoodie.streamer.source.dfs.root", dfsRoot);
     try {
       return new AvroDFSSource(props, jsc, sparkSession, schemaProvider);
     } catch (IOException e) {

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestCsvDFSSource.java
Patch:
@@ -48,9 +48,9 @@ public void setup() throws Exception {
   @Override
   public Source prepareDFSSource() {
     TypedProperties props = new TypedProperties();
-    props.setProperty("hoodie.deltastreamer.source.dfs.root", dfsRoot);
-    props.setProperty("hoodie.deltastreamer.csv.header", Boolean.toString(true));
-    props.setProperty("hoodie.deltastreamer.csv.sep", "\t");
+    props.setProperty("hoodie.streamer.source.dfs.root", dfsRoot);
+    props.setProperty("hoodie.streamer.csv.header", Boolean.toString(true));
+    props.setProperty("hoodie.streamer.csv.sep", "\t");
     return new CsvDFSSource(props, jsc, sparkSession, schemaProvider);
   }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestHoodieIncrSource.java
Patch:
@@ -337,8 +337,8 @@ private void readAndAssert(IncrSourceHelper.MissingCheckpointStrategy missingChe
                              String expectedCheckpoint, Option<String> snapshotCheckPointImplClassOpt) {
 
     Properties properties = new Properties();
-    properties.setProperty("hoodie.deltastreamer.source.hoodieincr.path", basePath());
-    properties.setProperty("hoodie.deltastreamer.source.hoodieincr.missing.checkpoint.strategy", missingCheckpointStrategy.name());
+    properties.setProperty("hoodie.streamer.source.hoodieincr.path", basePath());
+    properties.setProperty("hoodie.streamer.source.hoodieincr.missing.checkpoint.strategy", missingCheckpointStrategy.name());
     // TODO: [HUDI-7081] get rid of this
     properties.setProperty(HoodieReaderConfig.FILE_GROUP_READER_ENABLED.key(), "false");
     snapshotCheckPointImplClassOpt.map(className ->

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestJsonDFSSource.java
Patch:
@@ -44,7 +44,7 @@ public void setup() throws Exception {
   @Override
   public Source prepareDFSSource() {
     TypedProperties props = new TypedProperties();
-    props.setProperty("hoodie.deltastreamer.source.dfs.root", dfsRoot);
+    props.setProperty("hoodie.streamer.source.dfs.root", dfsRoot);
     return new JsonDFSSource(props, jsc, sparkSession, schemaProvider);
   }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestJsonKafkaSource.java
Patch:
@@ -82,7 +82,7 @@ public class TestJsonKafkaSource extends BaseTestKafkaSource {
   public void init() throws Exception {
     String schemaFilePath = Objects.requireNonNull(SCHEMA_FILE_URL).toURI().getPath();
     TypedProperties props = new TypedProperties();
-    props.put("hoodie.deltastreamer.schemaprovider.source.schema.file", schemaFilePath);
+    props.put("hoodie.streamer.schemaprovider.source.schema.file", schemaFilePath);
     schemaProvider = new FilebasedSchemaProvider(props, jsc());
   }
 
@@ -93,11 +93,11 @@ TypedProperties createPropsForKafkaSource(String topic, Long maxEventsToReadFrom
 
   static TypedProperties createPropsForJsonKafkaSource(String brokerAddress, String topic, Long maxEventsToReadFromKafkaSource, String resetStrategy) {
     TypedProperties props = new TypedProperties();
-    props.setProperty("hoodie.deltastreamer.source.kafka.topic", topic);
+    props.setProperty("hoodie.streamer.source.kafka.topic", topic);
     props.setProperty("bootstrap.servers", brokerAddress);
     props.setProperty("auto.offset.reset", resetStrategy);
     props.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
-    props.setProperty("hoodie.deltastreamer.kafka.source.maxEvents",
+    props.setProperty("hoodie.streamer.kafka.source.maxEvents",
         maxEventsToReadFromKafkaSource != null ? String.valueOf(maxEventsToReadFromKafkaSource) :
             String.valueOf(KafkaSourceConfig.MAX_EVENTS_FROM_KAFKA_SOURCE.defaultValue()));
     props.setProperty(ConsumerConfig.GROUP_ID_CONFIG, UUID.randomUUID().toString());

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestJsonKafkaSourcePostProcessor.java
Patch:
@@ -80,7 +80,7 @@ public static void cleanupClass() {
   public void init() throws Exception {
     String schemaFilePath = Objects.requireNonNull(TestJsonKafkaSource.SCHEMA_FILE_URL).toURI().getPath();
     TypedProperties props = new TypedProperties();
-    props.put("hoodie.deltastreamer.schemaprovider.source.schema.file", schemaFilePath);
+    props.put("hoodie.streamer.schemaprovider.source.schema.file", schemaFilePath);
     schemaProvider = new FilebasedSchemaProvider(props, jsc());
   }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestParquetDFSSource.java
Patch:
@@ -43,7 +43,7 @@ public void setup() throws Exception {
   @Override
   public Source prepareDFSSource() {
     TypedProperties props = new TypedProperties();
-    props.setProperty("hoodie.deltastreamer.source.dfs.root", dfsRoot);
+    props.setProperty("hoodie.streamer.source.dfs.root", dfsRoot);
     return new ParquetDFSSource(props, jsc, sparkSession, schemaProvider);
   }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestProtoKafkaSource.java
Patch:
@@ -75,11 +75,11 @@ public class TestProtoKafkaSource extends BaseTestKafkaSource {
 
   protected TypedProperties createPropsForKafkaSource(String topic, Long maxEventsToReadFromKafkaSource, String resetStrategy) {
     TypedProperties props = new TypedProperties();
-    props.setProperty("hoodie.deltastreamer.source.kafka.topic", topic);
+    props.setProperty("hoodie.streamer.source.kafka.topic", topic);
     props.setProperty("bootstrap.servers", testUtils.brokerAddress());
     props.setProperty("auto.offset.reset", resetStrategy);
     props.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
-    props.setProperty("hoodie.deltastreamer.kafka.source.maxEvents",
+    props.setProperty("hoodie.streamer.kafka.source.maxEvents",
         maxEventsToReadFromKafkaSource != null ? String.valueOf(maxEventsToReadFromKafkaSource) :
             String.valueOf(KafkaSourceConfig.MAX_EVENTS_FROM_KAFKA_SOURCE.defaultValue()));
     props.setProperty(ConsumerConfig.GROUP_ID_CONFIG, UUID.randomUUID().toString());

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestSqlFileBasedSource.java
Patch:
@@ -51,8 +51,8 @@
 public class TestSqlFileBasedSource extends UtilitiesTestBase {
 
   private final boolean useFlattenedSchema = false;
-  private final String sqlFileSourceConfig = "hoodie.deltastreamer.source.sql.file";
-  private final String sqlFileSourceConfigEmitChkPointConf = "hoodie.deltastreamer.source.sql.checkpoint.emit";
+  private final String sqlFileSourceConfig = "hoodie.streamer.source.sql.file";
+  private final String sqlFileSourceConfigEmitChkPointConf = "hoodie.streamer.source.sql.checkpoint.emit";
   protected FilebasedSchemaProvider schemaProvider;
   protected HoodieTestDataGenerator dataGenerator = new HoodieTestDataGenerator();
   private String dfsRoot;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestSqlSource.java
Patch:
@@ -50,7 +50,7 @@
 public class TestSqlSource extends UtilitiesTestBase {
 
   private final boolean useFlattenedSchema = false;
-  private final String sqlSourceConfig = "hoodie.deltastreamer.source.sql.sql.query";
+  private final String sqlSourceConfig = "hoodie.streamer.source.sql.sql.query";
   protected FilebasedSchemaProvider schemaProvider;
   protected HoodieTestDataGenerator dataGenerator = new HoodieTestDataGenerator();
   private String dfsRoot;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/debezium/TestAbstractDebeziumSource.java
Patch:
@@ -86,12 +86,12 @@ public static void cleanupClass() throws IOException {
 
   private TypedProperties createPropsForJsonSource() {
     TypedProperties props = new TypedProperties();
-    props.setProperty("hoodie.deltastreamer.source.kafka.topic", testTopicName);
+    props.setProperty("hoodie.streamer.source.kafka.topic", testTopicName);
     props.setProperty("bootstrap.servers", testUtils.brokerAddress());
     props.setProperty("auto.offset.reset", "earliest");
     props.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
-    props.setProperty("hoodie.deltastreamer.schemaprovider.registry.url", "localhost");
-    props.setProperty("hoodie.deltastreamer.source.kafka.value.deserializer.class", StringDeserializer.class.getName());
+    props.setProperty("hoodie.streamer.schemaprovider.registry.url", "localhost");
+    props.setProperty("hoodie.streamer.source.kafka.value.deserializer.class", StringDeserializer.class.getName());
     props.setProperty(ConsumerConfig.GROUP_ID_CONFIG, UUID.randomUUID().toString());
 
     return props;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/helpers/TestKafkaOffsetGen.java
Patch:
@@ -65,9 +65,9 @@ public void teardown() throws Exception {
 
   private TypedProperties getConsumerConfigs(String autoOffsetReset, String kafkaCheckpointType) {
     TypedProperties props = new TypedProperties();
-    props.put("hoodie.deltastreamer.source.kafka.checkpoint.type", kafkaCheckpointType);
+    props.put("hoodie.streamer.source.kafka.checkpoint.type", kafkaCheckpointType);
     props.put("auto.offset.reset", autoOffsetReset);
-    props.put("hoodie.deltastreamer.source.kafka.topic", testTopicName);
+    props.put("hoodie.streamer.source.kafka.topic", testTopicName);
     props.setProperty("bootstrap.servers", testUtils.brokerAddress());
     props.setProperty("key.deserializer", StringDeserializer.class.getName());
     props.setProperty("value.deserializer", StringDeserializer.class.getName());
@@ -250,7 +250,7 @@ public void testCheckTopicExists() {
     testUtils.createTopic(testTopicName, 1);
     boolean topicExists = kafkaOffsetGen.checkTopicExists(new KafkaConsumer(props));
     assertTrue(topicExists);
-    props.put("hoodie.deltastreamer.source.kafka.topic", "random");
+    props.put("hoodie.streamer.source.kafka.topic", "random");
     kafkaOffsetGen = new KafkaOffsetGen(props);
     topicExists = kafkaOffsetGen.checkTopicExists(new KafkaConsumer(props));
     assertFalse(topicExists);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/testutils/UtilitiesTestBase.java
Patch:
@@ -450,14 +450,14 @@ public static TypedProperties setupSchemaOnDFS() throws IOException {
     public static TypedProperties setupSchemaOnDFS(String scope, String filename) throws IOException {
       UtilitiesTestBase.Helpers.copyToDFS(scope + "/" + filename, fs, basePath + "/" + filename);
       TypedProperties props = new TypedProperties();
-      props.setProperty("hoodie.deltastreamer.schemaprovider.source.schema.file", basePath + "/" + filename);
+      props.setProperty("hoodie.streamer.schemaprovider.source.schema.file", basePath + "/" + filename);
       return props;
     }
 
     public static TypedProperties setupSchemaOnDFSWithAbsoluteScope(String scope, String filename) throws IOException {
       UtilitiesTestBase.Helpers.copyToDFSFromAbsolutePath(scope + "/" + filename, fs, basePath + "/" + filename);
       TypedProperties props = new TypedProperties();
-      props.setProperty("hoodie.deltastreamer.schemaprovider.source.schema.file", basePath + "/" + filename);
+      props.setProperty("hoodie.streamer.schemaprovider.source.schema.file", basePath + "/" + filename);
       return props;
     }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/transform/TestSqlQueryBasedTransformer.java
Patch:
@@ -78,7 +78,7 @@ public void testSqlQuery() {
         + "from\n"
         + "\t<SRC>";
     TypedProperties props = new TypedProperties();
-    props.put("hoodie.deltastreamer.transformer.sql", transSql);
+    props.put("hoodie.streamer.transformer.sql", transSql);
 
     // transform
     SqlQueryBasedTransformer transformer = new SqlQueryBasedTransformer();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/metrics/HoodieMetricsConfig.java
Patch:
@@ -220,6 +220,8 @@ public HoodieMetricsConfig build() {
           HoodieMetricsGraphiteConfig.newBuilder().fromProperties(hoodieMetricsConfig.getProps()).build());
       hoodieMetricsConfig.setDefaultOnCondition(reporterType == MetricsReporterType.CLOUDWATCH,
             HoodieMetricsCloudWatchConfig.newBuilder().fromProperties(hoodieMetricsConfig.getProps()).build());
+      hoodieMetricsConfig.setDefaultOnCondition(reporterType == MetricsReporterType.M3,
+              HoodieMetricsM3Config.newBuilder().fromProperties(hoodieMetricsConfig.getProps()).build());
       return hoodieMetricsConfig;
     }
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/SourceFormatAdapter.java
Patch:
@@ -62,7 +62,7 @@
 /**
  * Adapts data-format provided by the source to the data-format required by the client (DeltaStreamer).
  */
-public final class SourceFormatAdapter implements Closeable {
+public class SourceFormatAdapter implements Closeable {
 
   private final Source source;
   private boolean shouldSanitize = SANITIZE_SCHEMA_FIELD_NAMES.defaultValue();

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -2134,8 +2134,8 @@ public void testEmptyBatchWithNullSchemaFirstBatch() throws Exception {
 
     String tableBasePath = basePath + "/test_parquet_table" + testNum;
     HoodieDeltaStreamer.Config config = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT, ParquetDFSSource.class.getName(),
-        null, PROPS_FILENAME_TEST_PARQUET, false,
-        false, 100000, false, null, null, "timestamp", null);
+        Collections.singletonList(TestIdentityTransformer.class.getName()), PROPS_FILENAME_TEST_PARQUET, false,
+        false, 100000, false, null, "MERGE_ON_READ", "timestamp", null);
 
     config.schemaProviderClassName = NullValueSchemaProvider.class.getName();
     config.sourceClassName = TestParquetDFSSourceEmptyBatch.class.getName();

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java
Patch:
@@ -502,6 +502,9 @@ private void initTableIfNotExists(ObjectPath tablePath, CatalogTable catalogTabl
     if (catalogTable.isPartitioned() && !flinkConf.contains(FlinkOptions.PARTITION_PATH_FIELD)) {
       final String partitions = String.join(",", catalogTable.getPartitionKeys());
       flinkConf.setString(FlinkOptions.PARTITION_PATH_FIELD, partitions);
+      final String[] pks = flinkConf.getString(FlinkOptions.RECORD_KEY_FIELD).split(",");
+      boolean complexHoodieKey = pks.length > 1 || catalogTable.getPartitionKeys().size() > 1;
+      StreamerUtil.checkKeygenGenerator(complexHoodieKey, flinkConf);
     }
 
     if (!catalogTable.isPartitioned()) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java
Patch:
@@ -54,7 +54,7 @@ public abstract class ClusteringPlanStrategy<T,I,K,O> implements Serializable {
 
   public static final int CLUSTERING_PLAN_VERSION_1 = 1;
 
-  private final HoodieTable<T,I,K,O> hoodieTable;
+  protected final HoodieTable<T,I,K,O> hoodieTable;
   private final transient HoodieEngineContext engineContext;
   private final HoodieWriteConfig writeConfig;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -101,7 +101,7 @@
 import static org.apache.hudi.common.config.HoodieMetadataConfig.DEFAULT_METADATA_POPULATE_META_FIELDS;
 import static org.apache.hudi.common.table.HoodieTableConfig.ARCHIVELOG_FOLDER;
 import static org.apache.hudi.common.table.timeline.HoodieInstant.State.REQUESTED;
-import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;
+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMMIT_ACTION;
 import static org.apache.hudi.common.table.timeline.HoodieTimeline.LESSER_THAN_OR_EQUALS;
 import static org.apache.hudi.common.table.timeline.HoodieTimeline.getIndexInflightInstant;
 import static org.apache.hudi.common.table.timeline.TimelineMetadataUtils.deserializeIndexPlan;
@@ -830,7 +830,7 @@ private static void deletePendingIndexingInstant(HoodieTableMetaClient metaClien
   protected static void checkNumDeltaCommits(HoodieTableMetaClient metaClient, int maxNumDeltaCommitsWhenPending) {
     final HoodieActiveTimeline activeTimeline = metaClient.reloadActiveTimeline();
     Option<HoodieInstant> lastCompaction = activeTimeline.filterCompletedInstants()
-        .filter(s -> s.getAction().equals(COMPACTION_ACTION)).lastInstant();
+        .filter(s -> s.getAction().equals(COMMIT_ACTION)).lastInstant();
     int numDeltaCommits = lastCompaction.isPresent()
         ? activeTimeline.getDeltaCommitTimeline().findInstantsAfter(lastCompaction.get().getTimestamp()).countInstants()
         : activeTimeline.getDeltaCommitTimeline().countInstants();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -34,7 +34,6 @@
 import org.apache.hudi.exception.HoodieInsertException;
 import org.apache.hudi.io.storage.HoodieFileWriter;
 import org.apache.hudi.io.storage.HoodieFileWriterFactory;
-import org.apache.hudi.metadata.HoodieTableMetadata;
 import org.apache.hudi.table.HoodieTable;
 
 import org.apache.avro.Schema;
@@ -115,8 +114,7 @@ public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTa
   public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T, I, K, O> hoodieTable,
       String partitionPath, String fileId, Map<String, HoodieRecord<T>> recordMap,
       TaskContextSupplier taskContextSupplier) {
-    // preserveMetadata is disabled by default for MDT but enabled otherwise
-    this(config, instantTime, hoodieTable, partitionPath, fileId, taskContextSupplier, !HoodieTableMetadata.isMetadataTable(config.getBasePath()));
+    this(config, instantTime, hoodieTable, partitionPath, fileId, taskContextSupplier, true);
     this.recordMap = recordMap;
     this.useWriterSchema = true;
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -47,7 +47,6 @@
 import org.apache.hudi.io.storage.HoodieFileWriter;
 import org.apache.hudi.io.storage.HoodieFileWriterFactory;
 import org.apache.hudi.keygen.BaseKeyGenerator;
-import org.apache.hudi.metadata.HoodieTableMetadata;
 import org.apache.hudi.table.HoodieTable;
 
 import org.apache.avro.Schema;
@@ -145,8 +144,7 @@ public HoodieMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTab
     super(config, instantTime, partitionPath, fileId, hoodieTable, taskContextSupplier);
     this.keyToNewRecords = keyToNewRecords;
     this.useWriterSchemaForCompaction = true;
-    // preserveMetadata is disabled by default for MDT but enabled otherwise
-    this.preserveMetadata = !HoodieTableMetadata.isMetadataTable(config.getBasePath());
+    this.preserveMetadata = true;
     init(fileId, this.partitionPath, dataFileToBeMerged);
     validateAndSetAndKeyGenProps(keyGeneratorOpt, config.populateMetaFields());
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java
Patch:
@@ -234,8 +234,8 @@ private List<String> getPartitionPathsForIncrementalCleaning(HoodieCleanMetadata
   }
 
   private List<String> getPartitionsFromDeletedSavepoint(HoodieCleanMetadata cleanMetadata) {
-    List<String> savepointedTimestampsFromLastClean = Arrays.stream(cleanMetadata.getExtraMetadata()
-            .getOrDefault(SAVEPOINTED_TIMESTAMPS, StringUtils.EMPTY_STRING).split(","))
+    List<String> savepointedTimestampsFromLastClean = cleanMetadata.getExtraMetadata() == null ? Collections.emptyList()
+        : Arrays.stream(cleanMetadata.getExtraMetadata().getOrDefault(SAVEPOINTED_TIMESTAMPS, StringUtils.EMPTY_STRING).split(","))
         .filter(partition -> !StringUtils.isNullOrEmpty(partition)).collect(Collectors.toList());
     if (savepointedTimestampsFromLastClean.isEmpty()) {
       return Collections.emptyList();

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/table/action/TestCleanPlanner.java
Patch:
@@ -507,7 +507,7 @@ private static Pair<HoodieCleanMetadata, Option<byte[]>> getCleanCommitMetadata(
         extraMetadata.put(SAVEPOINTED_TIMESTAMPS, savepointsToTrack.stream().collect(Collectors.joining(",")));
       }
       HoodieCleanMetadata cleanMetadata = new HoodieCleanMetadata(instantTime, 100L, 10, earliestCommitToRetain, lastCompletedTime, partitionMetadata,
-          CLEAN_METADATA_VERSION_2, Collections.EMPTY_MAP, extraMetadata);
+          CLEAN_METADATA_VERSION_2, Collections.EMPTY_MAP, extraMetadata.isEmpty() ?  null : extraMetadata);
       return Pair.of(cleanMetadata, TimelineMetadataUtils.serializeCleanMetadata(cleanMetadata));
     } catch (IOException ex) {
       throw new UncheckedIOException(ex);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java
Patch:
@@ -234,7 +234,7 @@ private HoodieCleanMetadata runClean(HoodieTable<T, I, K, O> table, HoodieInstan
       throw new HoodieIOException("Failed to clean up after commit", e);
     } finally {
       if (!skipLocking) {
-        this.txnManager.endTransaction(Option.of(inflightInstant));
+        this.txnManager.endTransaction(Option.ofNullable(inflightInstant));
       }
     }
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java
Patch:
@@ -285,6 +285,7 @@ protected HoodieWriteMetadata<HoodieData<WriteStatus>> executeClustering(HoodieC
     writeMetadata.setPartitionToReplaceFileIds(getPartitionToReplacedFileIds(clusteringPlan, writeMetadata));
     commitOnAutoCommit(writeMetadata);
     if (!writeMetadata.getCommitMetadata().isPresent()) {
+      LOG.info("Found empty commit metadata for clustering with instant time " + instantTime);
       HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeMetadata.getWriteStats().get(), writeMetadata.getPartitionToReplaceFileIds(),
           extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());
       writeMetadata.setCommitMetadata(Option.of(commitMetadata));

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
Patch:
@@ -627,6 +627,7 @@ public static int retry(int maxRetryCount, CheckedSupplier<Integer> supplier, St
       } while (ret != 0 && maxRetryCount-- > 0);
     } catch (Throwable t) {
       LOG.error(errorMessage, t);
+      throw new RuntimeException("Failed in retry", t);
     }
     return ret;
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java
Patch:
@@ -37,8 +37,8 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
-import org.apache.hudi.exception.HoodieIncompatibleSchemaException;
 import org.apache.hudi.exception.InvalidTableException;
+import org.apache.hudi.internal.schema.HoodieSchemaException;
 import org.apache.hudi.internal.schema.InternalSchema;
 import org.apache.hudi.internal.schema.io.FileBasedInternalSchemaStorageManager;
 import org.apache.hudi.internal.schema.utils.SerDeHelper;
@@ -571,7 +571,7 @@ public static Schema appendPartitionColumns(Schema dataSchema, Option<String[]>
     boolean hasPartitionColNotInSchema = Arrays.stream(partitionFields.get()).anyMatch(pf -> !containsFieldInSchema(dataSchema, pf));
     boolean hasPartitionColInSchema = Arrays.stream(partitionFields.get()).anyMatch(pf -> containsFieldInSchema(dataSchema, pf));
     if (hasPartitionColNotInSchema && hasPartitionColInSchema) {
-      throw new HoodieIncompatibleSchemaException("Partition columns could not be partially contained w/in the data schema");
+      throw new HoodieSchemaException("Partition columns could not be partially contained w/in the data schema");
     }
 
     if (hasPartitionColNotInSchema) {

File: hudi-common/src/main/java/org/apache/hudi/exception/SchemaCompatibilityException.java
Patch:
@@ -18,10 +18,12 @@
 
 package org.apache.hudi.exception;
 
+import org.apache.hudi.internal.schema.HoodieSchemaException;
+
 /**
  * An exception thrown when schema has compatibility problems.
  */
-public class SchemaCompatibilityException extends HoodieException {
+public class SchemaCompatibilityException extends HoodieSchemaException {
 
   public SchemaCompatibilityException(String message) {
     super(message);

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestTableSchemaResolver.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.avro.AvroSchemaUtils;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.exception.HoodieIncompatibleSchemaException;
+import org.apache.hudi.internal.schema.HoodieSchemaException;
 
 import org.apache.avro.Schema;
 import org.junit.jupiter.api.Test;
@@ -61,7 +61,7 @@ public void testRecreateSchemaWhenDropPartitionColumns() {
     String[] pts4 = {"user_partition", "partition_path"};
     try {
       TableSchemaResolver.appendPartitionColumns(originSchema, Option.of(pts3));
-    } catch (HoodieIncompatibleSchemaException e) {
+    } catch (HoodieSchemaException e) {
       assertTrue(e.getMessage().contains("Partial partition fields are still in the schema"));
     }
   }

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/ITTestDataStreamWrite.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.configuration.FlinkOptions;
 import org.apache.hudi.configuration.HadoopConfigurations;
 import org.apache.hudi.configuration.OptionsInference;
-import org.apache.hudi.exception.SchemaCompatibilityException;
+import org.apache.hudi.exception.MissingSchemaFieldException;
 import org.apache.hudi.sink.transform.ChainedTransformer;
 import org.apache.hudi.sink.transform.Transformer;
 import org.apache.hudi.sink.utils.Pipelines;
@@ -557,13 +557,13 @@ public void testColumnDroppingIsNotAllowed() throws Exception {
     } catch (JobExecutionException e) {
       Throwable actualException = e;
       while (actualException != null) {
-        if (actualException.getClass() == SchemaCompatibilityException.class) {
+        if (actualException.getClass() == MissingSchemaFieldException.class) {
           // test is passed
           return;
         }
         actualException = actualException.getCause();
       }
     }
-    throw new AssertionError(String.format("Excepted exception %s is not found", SchemaCompatibilityException.class));
+    throw new AssertionError(String.format("Excepted exception %s is not found", MissingSchemaFieldException.class));
   }
 }

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java
Patch:
@@ -151,7 +151,7 @@ public String delete(@ShellOption(value = "--backup", help = "Backup the metadat
   public String deleteRecordIndex(@ShellOption(value = "--backup", help = "Backup the record index before delete", defaultValue = "true", arity = 1) final boolean backup) throws Exception {
     HoodieTableMetaClient dataMetaClient = HoodieCLI.getTableMetaClient();
     String backupPath = HoodieTableMetadataUtil.deleteMetadataTablePartition(dataMetaClient, new HoodieSparkEngineContext(jsc),
-        MetadataPartitionType.RECORD_INDEX, backup);
+        MetadataPartitionType.RECORD_INDEX.getPartitionPath(), backup);
     if (backup) {
       return "Record Index has been deleted from the Metadata Table and backed up to " + backupPath;
     } else {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -987,10 +987,10 @@ public boolean scheduleCompactionAtInstant(String instantTime, Option<Map<String
    * @param partitionTypes - list of {@link MetadataPartitionType} which needs to be indexed
    * @return instant time for the requested INDEX action
    */
-  public Option<String> scheduleIndexing(List<MetadataPartitionType> partitionTypes) {
+  public Option<String> scheduleIndexing(List<MetadataPartitionType> partitionTypes, List<String> partitionPaths) {
     String instantTime = createNewInstantTime();
     Option<HoodieIndexPlan> indexPlan = createTable(config, hadoopConf)
-        .scheduleIndexing(context, instantTime, partitionTypes);
+        .scheduleIndexing(context, instantTime, partitionTypes, partitionPaths);
     return indexPlan.isPresent() ? Option.of(instantTime) : Option.empty();
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/ThreeToFourUpgradeHandler.java
Patch:
@@ -49,7 +49,7 @@ public Map<ConfigProperty, String> upgrade(HoodieWriteConfig config, HoodieEngin
     tablePropsToAdd.put(TABLE_CHECKSUM, String.valueOf(HoodieTableConfig.generateChecksum(config.getProps())));
     // if metadata is enabled and files partition exist then update TABLE_METADATA_INDEX_COMPLETED
     // schema for the files partition is same between the two versions
-    if (config.isMetadataTableEnabled() && metadataPartitionExists(config.getBasePath(), context, MetadataPartitionType.FILES)) {
+    if (config.isMetadataTableEnabled() && metadataPartitionExists(config.getBasePath(), context, MetadataPartitionType.FILES.getPartitionPath())) {
       tablePropsToAdd.put(TABLE_METADATA_PARTITIONS, MetadataPartitionType.FILES.getPartitionPath());
     }
     return tablePropsToAdd;

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java
Patch:
@@ -378,7 +378,7 @@ public HoodieRollbackMetadata rollback(HoodieEngineContext context, String rollb
   }
 
   @Override
-  public Option<HoodieIndexPlan> scheduleIndexing(HoodieEngineContext context, String indexInstantTime, List<MetadataPartitionType> partitionsToIndex) {
+  public Option<HoodieIndexPlan> scheduleIndexing(HoodieEngineContext context, String indexInstantTime, List<MetadataPartitionType> partitionsToIndex, List<String> partitionPaths) {
     throw new HoodieNotSupportedException("Metadata indexing is not supported for a Flink table yet.");
   }
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java
Patch:
@@ -247,8 +247,8 @@ public HoodieRollbackMetadata rollback(HoodieEngineContext context,
   }
 
   @Override
-  public Option<HoodieIndexPlan> scheduleIndexing(HoodieEngineContext context, String indexInstantTime, List<MetadataPartitionType> partitionsToIndex) {
-    return new ScheduleIndexActionExecutor<>(context, config, this, indexInstantTime, partitionsToIndex).execute();
+  public Option<HoodieIndexPlan> scheduleIndexing(HoodieEngineContext context, String indexInstantTime, List<MetadataPartitionType> partitionsToIndex, List<String> partitionPaths) {
+    return new ScheduleIndexActionExecutor<>(context, config, this, indexInstantTime, partitionsToIndex, partitionPaths).execute();
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java
Patch:
@@ -310,8 +310,8 @@ public HoodieRollbackMetadata rollback(HoodieEngineContext context, String rollb
   }
 
   @Override
-  public Option<HoodieIndexPlan> scheduleIndexing(HoodieEngineContext context, String indexInstantTime, List<MetadataPartitionType> partitionsToIndex) {
-    return new ScheduleIndexActionExecutor<>(context, config, this, indexInstantTime, partitionsToIndex).execute();
+  public Option<HoodieIndexPlan> scheduleIndexing(HoodieEngineContext context, String indexInstantTime, List<MetadataPartitionType> partitionsToIndex, List<String> partitionPaths) {
+    return new ScheduleIndexActionExecutor<>(context, config, this, indexInstantTime, partitionsToIndex, partitionPaths).execute();
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -1098,7 +1098,7 @@ private void revertTableToInflightState(HoodieWriteConfig writeConfig) throws IO
     // Transition the second init commit for record_index partition to inflight in MDT
     deleteMetaFile(metaClient.getFs(), mdtBasePath, mdtInitCommit2, DELTA_COMMIT_EXTENSION);
     metaClient.getTableConfig().setMetadataPartitionState(
-        metaClient, MetadataPartitionType.RECORD_INDEX, false);
+        metaClient, MetadataPartitionType.RECORD_INDEX.getPartitionPath(), false);
     metaClient.getTableConfig().setMetadataPartitionsInflight(
         metaClient, MetadataPartitionType.RECORD_INDEX);
     timeline = metaClient.getActiveTimeline().reload();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieIndex.java
Patch:
@@ -329,11 +329,11 @@ public void testLookupIndexWithAndWithoutColumnStats() throws Exception {
 
     // check column_stats partition exists
     metaClient = HoodieTableMetaClient.reload(metaClient);
-    assertTrue(metadataPartitionExists(metaClient.getBasePath(), context, COLUMN_STATS));
+    assertTrue(metadataPartitionExists(metaClient.getBasePath(), context, COLUMN_STATS.getPartitionPath()));
     assertTrue(metaClient.getTableConfig().getMetadataPartitions().contains(COLUMN_STATS.getPartitionPath()));
 
     // delete the column_stats partition
-    deleteMetadataPartition(metaClient.getBasePath(), context, COLUMN_STATS);
+    deleteMetadataPartition(metaClient.getBasePath(), context, COLUMN_STATS.getPartitionPath());
 
     // Now tagLocation for these records, they should be tagged correctly despite column_stats being enabled but not present
     hoodieTable = HoodieSparkTable.create(config, context, metaClient);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java
Patch:
@@ -540,7 +540,7 @@ public void testDowngradeSixToFiveShouldDeleteRecordIndexPartition() throws Exce
             .withEnableRecordIndex(true).build())
         .build();
     for (MetadataPartitionType partitionType : MetadataPartitionType.values()) {
-      metaClient.getTableConfig().setMetadataPartitionState(metaClient, partitionType, true);
+      metaClient.getTableConfig().setMetadataPartitionState(metaClient, partitionType.getPartitionPath(), true);
     }
     metaClient.getTableConfig().setMetadataPartitionsInflight(metaClient, MetadataPartitionType.values());
     String metadataTableBasePath = Paths.get(basePath, METADATA_TABLE_FOLDER_PATH).toString();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieIndexer.java
Patch:
@@ -43,6 +43,7 @@
 import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 import java.util.Locale;
 import java.util.Set;
@@ -240,7 +241,8 @@ private Option<String> doSchedule(SparkRDDWriteClient<HoodieRecordPayload> clien
     if (indexExists(partitionTypes)) {
       return Option.empty();
     }
-    Option<String> indexingInstant = client.scheduleIndexing(partitionTypes);
+
+    Option<String> indexingInstant = client.scheduleIndexing(partitionTypes, Collections.emptyList());
     if (!indexingInstant.isPresent()) {
       LOG.error("Scheduling of index action did not return any instant.");
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -562,7 +562,7 @@ public class HoodieWriteConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> MERGE_ALLOW_DUPLICATE_ON_INSERTS_ENABLE = ConfigProperty
       .key("hoodie.merge.allow.duplicate.on.inserts")
-      .defaultValue("false")
+      .defaultValue("true")
       .markAdvanced()
       .withDocumentation("When enabled, we allow duplicate keys even if inserts are routed to merge with an existing file (for ensuring file sizing)."
           + " This is only relevant for insert operation, since upsert, delete operations will ensure unique key constraints are maintained.");

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataWriteUtils.java
Patch:
@@ -86,6 +86,7 @@ public static HoodieWriteConfig createMetadataWriteConfig(
     HoodieWriteConfig.Builder builder = HoodieWriteConfig.newBuilder()
         .withEngineType(writeConfig.getEngineType())
         .withTimelineLayoutVersion(TimelineLayoutVersion.CURR_VERSION)
+        .withMergeAllowDuplicateOnInserts(false)
         .withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder()
             .withConsistencyCheckEnabled(writeConfig.getConsistencyGuardConfig().isConsistencyCheckEnabled())
             .withInitialConsistencyCheckIntervalMs(writeConfig.getConsistencyGuardConfig().getInitialConsistencyCheckIntervalMs())

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/config/TestHoodieWriteConfig.java
Patch:
@@ -89,6 +89,7 @@ public void testPropertyLoading(boolean withAlternative) throws IOException {
     assertEquals(5, config.getMaxCommitsToKeep());
     assertEquals(2, config.getMinCommitsToKeep());
     assertTrue(config.shouldUseExternalSchemaTransformation());
+    assertTrue(config.allowDuplicateInserts());
   }
 
   @Test

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -1137,6 +1137,7 @@ private void testAsyncClusteringService(HoodieRecordType recordType) throws Exce
     cfg.tableType = HoodieTableType.COPY_ON_WRITE.name();
     cfg.configs.addAll(getTableServicesConfigs(totalRecords, "false", "", "", "true", "3"));
     cfg.configs.add(String.format("%s=%s", "hoodie.datasource.write.row.writer.enable", "false"));
+    cfg.configs.add(String.format("%s=%s", "hoodie.merge.allow.duplicate.on.inserts", "false"));
     HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);
     deltaStreamerTestRunner(ds, cfg, (r) -> {
       TestHelpers.assertAtLeastNReplaceCommits(1, tableBasePath, fs);
@@ -1200,6 +1201,7 @@ public void testAsyncClusteringServiceWithCompaction(HoodieRecordType recordType
     cfg.continuousMode = true;
     cfg.tableType = HoodieTableType.MERGE_ON_READ.name();
     cfg.configs.addAll(getTableServicesConfigs(totalRecords, "false", "", "", "true", "3"));
+    cfg.configs.add(String.format("%s=%s", "hoodie.merge.allow.duplicate.on.inserts", "false"));
     HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);
     deltaStreamerTestRunner(ds, cfg, (r) -> {
       TestHelpers.assertAtleastNCompactionCommits(2, tableBasePath, fs);

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/utils/TestWriteBase.java
Patch:
@@ -517,8 +517,8 @@ public TestHarness checkLastPendingInstantCompleted() {
      * Used to simulate the use case that the coordinator has not finished a new instant initialization,
      * while the write task fails intermittently.
      */
-    public TestHarness coordinatorFails() throws Exception {
-      this.pipeline.coordinatorFails();
+    public TestHarness restartCoordinator() throws Exception {
+      this.pipeline.restartCoordinator();
       return this;
     }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java
Patch:
@@ -551,7 +551,7 @@ public int getBatchSize(int numRegionServersForTable, int maxQpsPerRegionServer,
       int maxReqPerSec = getMaxReqPerSec(numRSAlive, maxQpsPerRegionServer, qpsFraction);
       int numTasks = numTasksDuringPut;
       int maxParallelPutsTask = Math.max(1, Math.min(numTasks, maxExecutors));
-      int multiPutBatchSizePerSecPerTask = Math.max(1, (int) Math.ceil(maxReqPerSec / maxParallelPutsTask));
+      int multiPutBatchSizePerSecPerTask = Math.max(1, (int) Math.ceil((double) maxReqPerSec / maxParallelPutsTask));
       LOG.info("HbaseIndexThrottling: qpsFraction :" + qpsFraction);
       LOG.info("HbaseIndexThrottling: numRSAlive :" + numRSAlive);
       LOG.info("HbaseIndexThrottling: maxReqPerSec :" + maxReqPerSec);

File: hudi-common/src/main/java/org/apache/hudi/common/fs/SizeAwareDataOutputStream.java
Patch:
@@ -55,7 +55,7 @@ public void write(byte[] v) throws IOException {
   }
 
   public void write(byte[] v, int offset, int len) throws IOException {
-    size.addAndGet(len + offset);
+    size.addAndGet((long) len + offset);
     outputStream.write(v, offset, len);
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieWithTimelineServer.java
Patch:
@@ -102,7 +102,7 @@ public String sendRequest(String driverHost, int port) {
     try (CloseableHttpClient client = HttpClientBuilder.create().build()) {
 
       System.out.println("Sleeping for " + cfg.delaySecs + " secs ");
-      Thread.sleep(cfg.delaySecs * 1000);
+      Thread.sleep(cfg.delaySecs * 1000L);
       System.out.println("Woke up after sleeping for " + cfg.delaySecs + " secs ");
 
       HttpGet request = new HttpGet(url);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JsonKafkaSource.java
Patch:
@@ -93,7 +93,9 @@ protected  JavaRDD<String> maybeAppendKafkaOffsets(JavaRDD<ConsumerRecord<Object
             jsonNode.put(KAFKA_SOURCE_OFFSET_COLUMN, consumerRecord.offset());
             jsonNode.put(KAFKA_SOURCE_PARTITION_COLUMN, consumerRecord.partition());
             jsonNode.put(KAFKA_SOURCE_TIMESTAMP_COLUMN, consumerRecord.timestamp());
-            jsonNode.put(KAFKA_SOURCE_KEY_COLUMN, recordKey);
+            if (recordKey != null) {
+              jsonNode.put(KAFKA_SOURCE_KEY_COLUMN, recordKey);
+            }
             stringList.add(om.writeValueAsString(jsonNode));
           } catch (Throwable e) {
             stringList.add(recordValue);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestJsonKafkaSource.java
Patch:
@@ -352,7 +352,10 @@ public void testAppendKafkaOffset() {
     jsonSource = new JsonKafkaSource(props, jsc(), spark(), schemaProvider, metrics);
     kafkaSource = new SourceFormatAdapter(jsonSource);
     Dataset<Row> dfWithOffsetInfoAndNullKafkaKey = kafkaSource.fetchNewDataInRowFormat(Option.empty(), Long.MAX_VALUE).getBatch().get().cache();
+    // total of 2 * numMessages are in the topic at this point, half with a key and half with a null key. All should have the source offset.
     assertEquals(numMessages, dfWithOffsetInfoAndNullKafkaKey.toDF().filter("_hoodie_kafka_source_key is null").count());
+    assertEquals(numMessages, dfWithOffsetInfoAndNullKafkaKey.toDF().filter("_hoodie_kafka_source_key is not null").count());
+    assertEquals(numMessages * 2, dfWithOffsetInfoAndNullKafkaKey.toDF().filter("_hoodie_kafka_source_offset is not null").count());
 
     dfNoOffsetInfo.unpersist();
     dfWithOffsetInfo.unpersist();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java
Patch:
@@ -166,12 +166,12 @@ public static OffsetRange[] computeOffsetRanges(Map<TopicPartition, Long> fromOf
           if (toOffset == range.untilOffset()) {
             exhaustedPartitions.add(range.partition());
           }
-          allocedEvents += toOffset - range.fromOffset();
           // We need recompute toOffset if allocedEvents larger than actualNumEvents.
-          if (allocedEvents > actualNumEvents) {
+          if (allocedEvents + (toOffset - range.fromOffset()) > actualNumEvents) {
             long offsetsToAdd = Math.min(eventsPerPartition, (actualNumEvents - allocedEvents));
-            toOffset = Math.min(range.untilOffset(), toOffset + offsetsToAdd);
+            toOffset = Math.min(range.untilOffset(), range.fromOffset() + offsetsToAdd);
           }
+          allocedEvents = allocedEvents + (toOffset - range.fromOffset());
           OffsetRange thisRange = OffsetRange.create(range.topicPartition(), range.fromOffset(), toOffset);
           finalRanges.add(thisRange);
           ranges[i] = OffsetRange.create(range.topicPartition(), range.fromOffset() + thisRange.count(), range.untilOffset());

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java
Patch:
@@ -217,7 +217,7 @@ public HoodieMetadataPayload(Option<GenericRecord> recordOpt) {
         //       Otherwise, it has to be present or the record would be considered invalid
         if (bloomFilterRecord == null) {
           checkArgument(record.getSchema().getField(SCHEMA_FIELD_ID_BLOOM_FILTER) == null,
-              String.format("Valid %s record expected for type: %s", SCHEMA_FIELD_ID_BLOOM_FILTER, METADATA_TYPE_COLUMN_STATS));
+              String.format("Valid %s record expected for type: %s", SCHEMA_FIELD_ID_BLOOM_FILTER, METADATA_TYPE_BLOOM_FILTER));
         } else {
           bloomFilterMetadata = new HoodieMetadataBloomFilter(
               (String) bloomFilterRecord.get(BLOOM_FILTER_FIELD_TYPE),

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -22,7 +22,9 @@
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.config.TypedProperties;
+import org.apache.hudi.common.util.Option;
 import org.apache.hudi.utilities.schema.SchemaProvider;
+import org.apache.hudi.utilities.streamer.DefaultStreamContext;
 import org.apache.hudi.utilities.streamer.HoodieStreamer;
 import org.apache.hudi.utilities.streamer.StreamSync;
 
@@ -49,6 +51,6 @@ public DeltaSync(HoodieStreamer.Config cfg, SparkSession sparkSession, SchemaPro
   public DeltaSync(HoodieDeltaStreamer.Config cfg, SparkSession sparkSession, SchemaProvider schemaProvider,
                    TypedProperties props, HoodieSparkEngineContext hoodieSparkContext, FileSystem fs, Configuration conf,
                    Function<SparkRDDWriteClient, Boolean> onInitializingHoodieWriteClient) throws IOException {
-    super(cfg, sparkSession, schemaProvider, props, hoodieSparkContext, fs, conf, onInitializingHoodieWriteClient);
+    super(cfg, sparkSession, props, hoodieSparkContext, fs, conf, onInitializingHoodieWriteClient, new DefaultStreamContext(schemaProvider, Option.empty()));
   }
 }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestProtoKafkaSource.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.utilities.config.ProtoClassBasedSchemaProviderConfig;
 import org.apache.hudi.utilities.schema.ProtoClassBasedSchemaProvider;
 import org.apache.hudi.utilities.schema.SchemaProvider;
+import org.apache.hudi.utilities.streamer.DefaultStreamContext;
 import org.apache.hudi.utilities.streamer.SourceFormatAdapter;
 import org.apache.hudi.utilities.test.proto.Nested;
 import org.apache.hudi.utilities.test.proto.Sample;
@@ -89,7 +90,7 @@ protected TypedProperties createPropsForKafkaSource(String topic, Long maxEvents
   @Override
   SourceFormatAdapter createSource(TypedProperties props) {
     this.schemaProvider = new ProtoClassBasedSchemaProvider(props, jsc());
-    Source protoKafkaSource = new ProtoKafkaSource(props, jsc(), spark(), schemaProvider, metrics);
+    Source protoKafkaSource = new ProtoKafkaSource(props, jsc(), spark(), metrics, new DefaultStreamContext(schemaProvider, sourceProfile));
     return new SourceFormatAdapter(protoKafkaSource);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaUtils.java
Patch:
@@ -59,7 +59,7 @@ public static boolean isSchemaCompatible(Schema prevSchema, Schema newSchema, bo
   /**
    * Establishes whether {@code newSchema} is compatible w/ {@code prevSchema}, as
    * defined by Avro's {@link AvroSchemaCompatibility}.
-   * From avro's compatability standpoint, prevSchema is writer schema and new schema is reader schema.
+   * From avro's compatibility standpoint, prevSchema is writer schema and new schema is reader schema.
    * {@code newSchema} is considered compatible to {@code prevSchema}, iff data written using {@code prevSchema}
    * could be read by {@code newSchema}
    *

File: hudi-common/src/main/java/org/apache/hudi/common/bloom/Key.java
Patch:
@@ -136,7 +136,7 @@ public int hashCode() {
   /**
    * Serialize the fields of this object to <code>out</code>.
    *
-   * @param out <code>DataOuput</code> to serialize this object into.
+   * @param out <code>DataOutput</code> to serialize this object into.
    * @throws IOException
    */
   public void write(DataOutput out) throws IOException {

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/BaseHoodieQueueBasedExecutor.java
Patch:
@@ -215,7 +215,7 @@ public E execute() {
         // to be interrupted as well
         Thread.currentThread().interrupt();
       }
-      // throw if we have any other exception seen already. There is a chance that cancellation/closing of producers with CompeletableFuture wins before the actual exception
+      // throw if we have any other exception seen already. There is a chance that cancellation/closing of producers with CompletableFuture wins before the actual exception
       // is thrown.
       if (this.queue.getThrowable() != null) {
         throw new HoodieException(queue.getThrowable());

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -1640,7 +1640,7 @@ public static String createLogCompactionTimestamp(String timestamp) {
    *
    * @param partitionType         Type of the partition for which the file group count is to be estimated.
    * @param recordCount           The number of records expected to be written.
-   * @param averageRecordSize     Average size of each record to be writen.
+   * @param averageRecordSize     Average size of each record to be written.
    * @param minFileGroupCount     Minimum number of file groups to use.
    * @param maxFileGroupCount     Maximum number of file groups to use.
    * @param growthFactor          By what factor are the records (recordCount) expected to grow?

File: hudi-common/src/main/java/org/apache/hudi/common/data/HoodieBaseListData.java
Patch:
@@ -53,7 +53,7 @@ protected Stream<T> asStream() {
 
   protected boolean isEmpty() {
     if (lazy) {
-      return data.asLeft().findAny().isPresent();
+      return !data.asLeft().findAny().isPresent();
     } else {
       return data.asRight().isEmpty();
     }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -22,7 +22,9 @@
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.config.TypedProperties;
+import org.apache.hudi.common.util.Option;
 import org.apache.hudi.utilities.schema.SchemaProvider;
+import org.apache.hudi.utilities.streamer.DefaultStreamContext;
 import org.apache.hudi.utilities.streamer.HoodieStreamer;
 import org.apache.hudi.utilities.streamer.StreamSync;
 
@@ -49,6 +51,6 @@ public DeltaSync(HoodieStreamer.Config cfg, SparkSession sparkSession, SchemaPro
   public DeltaSync(HoodieDeltaStreamer.Config cfg, SparkSession sparkSession, SchemaProvider schemaProvider,
                    TypedProperties props, HoodieSparkEngineContext hoodieSparkContext, FileSystem fs, Configuration conf,
                    Function<SparkRDDWriteClient, Boolean> onInitializingHoodieWriteClient) throws IOException {
-    super(cfg, sparkSession, schemaProvider, props, hoodieSparkContext, fs, conf, onInitializingHoodieWriteClient);
+    super(cfg, sparkSession, props, hoodieSparkContext, fs, conf, onInitializingHoodieWriteClient, new DefaultStreamContext(schemaProvider, Option.empty()));
   }
 }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestProtoKafkaSource.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.utilities.config.ProtoClassBasedSchemaProviderConfig;
 import org.apache.hudi.utilities.schema.ProtoClassBasedSchemaProvider;
 import org.apache.hudi.utilities.schema.SchemaProvider;
+import org.apache.hudi.utilities.streamer.DefaultStreamContext;
 import org.apache.hudi.utilities.streamer.SourceFormatAdapter;
 import org.apache.hudi.utilities.test.proto.Nested;
 import org.apache.hudi.utilities.test.proto.Sample;
@@ -89,7 +90,7 @@ protected TypedProperties createPropsForKafkaSource(String topic, Long maxEvents
   @Override
   SourceFormatAdapter createSource(TypedProperties props) {
     this.schemaProvider = new ProtoClassBasedSchemaProvider(props, jsc());
-    Source protoKafkaSource = new ProtoKafkaSource(props, jsc(), spark(), schemaProvider, metrics);
+    Source protoKafkaSource = new ProtoKafkaSource(props, jsc(), spark(), metrics, new DefaultStreamContext(schemaProvider, sourceProfile));
     return new SourceFormatAdapter(protoKafkaSource);
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/StreamSync.java
Patch:
@@ -414,7 +414,7 @@ public Pair<Option<String>, JavaRDD<WriteStatus>> syncOnce() throws IOException
             || (newTargetSchema != null && !processedSchema.isSchemaPresent(newTargetSchema))) {
           String sourceStr = newSourceSchema == null ? NULL_PLACEHOLDER : newSourceSchema.toString(true);
           String targetStr = newTargetSchema == null ? NULL_PLACEHOLDER : newTargetSchema.toString(true);
-          LOG.info("Seeing new schema. Source: {0}, Target: {1}", sourceStr, targetStr);
+          LOG.info("Seeing new schema. Source: {}, Target: {}", sourceStr, targetStr);
           // We need to recreate write client with new schema and register them.
           reInitWriteClient(newSourceSchema, newTargetSchema, inputBatch.getBatch());
           if (newSourceSchema != null) {
@@ -988,7 +988,7 @@ public void runMetaSync() {
           SyncUtilHelpers.runHoodieMetaSync(impl.trim(), metaProps, conf, fs, cfg.targetBasePath, cfg.baseFileFormat);
           success = true;
         } catch (HoodieMetaSyncException e) {
-          LOG.error("SyncTool class {0} failed with exception {1}",  impl.trim(), e);
+          LOG.error("SyncTool class {} failed with exception {}", impl.trim(), e);
           failedMetaSyncs.put(impl, e);
         }
         long metaSyncTimeNanos = syncContext != null ? syncContext.stop() : 0;

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestComplexKeyGenerator.java
Patch:
@@ -78,7 +78,7 @@ public void testNullPartitionPathFields() {
   @Test
   public void testNullRecordKeyFields() {
     GenericRecord record = getRecord();
-    Assertions.assertThrows(StringIndexOutOfBoundsException.class, () ->   {
+    Assertions.assertThrows(HoodieKeyException.class, () ->   {
       ComplexKeyGenerator keyGenerator = new ComplexKeyGenerator(getPropertiesWithoutRecordKeyProp());
       keyGenerator.getRecordKey(record);
     });

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestGlobalDeleteRecordGenerator.java
Patch:
@@ -62,7 +62,7 @@ private TypedProperties getProps() {
   @Test
   public void testNullRecordKeyFields() {
     GenericRecord record = getRecord();
-    Assertions.assertThrows(StringIndexOutOfBoundsException.class, () ->  {
+    Assertions.assertThrows(HoodieKeyException.class, () ->  {
       BaseKeyGenerator keyGenerator = new GlobalDeleteKeyGenerator(getPropertiesWithoutRecordKeyProp());
       keyGenerator.getRecordKey(record);
     });

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestNonpartitionedKeyGenerator.java
Patch:
@@ -69,7 +69,7 @@ private TypedProperties getWrongRecordKeyFieldProps() {
   @Test
   public void testNullRecordKeyFields() {
     GenericRecord record = getRecord();
-    Assertions.assertThrows(StringIndexOutOfBoundsException.class, () ->  {
+    Assertions.assertThrows(HoodieKeyException.class, () ->  {
       BaseKeyGenerator keyGenerator = new NonpartitionedKeyGenerator(getPropertiesWithoutRecordKeyProp());
       keyGenerator.getRecordKey(record);
     });

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java
Patch:
@@ -549,6 +549,7 @@ private Table instantiateHiveTable(ObjectPath tablePath, CatalogBaseTable table,
     hiveTable.setCreateTime((int) (System.currentTimeMillis() / 1000));
 
     Map<String, String> properties = new HashMap<>(table.getOptions());
+    hiveConf.getAllProperties().forEach((k, v) -> properties.put("hadoop." + k, String.valueOf(v)));
 
     if (external) {
       hiveTable.setTableType(TableType.EXTERNAL_TABLE.toString());

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/HoodieSyncConfig.java
Patch:
@@ -174,7 +174,7 @@ public class HoodieSyncConfig extends HoodieConfig {
       .withDocumentation("The spark version used when syncing with a metastore.");
   public static final ConfigProperty<String> META_SYNC_SNAPSHOT_WITH_TABLE_NAME = ConfigProperty
           .key("hoodie.meta.sync.sync_snapshot_with_table_name")
-          .defaultValue("false")
+          .defaultValue("true")
           .markAdvanced()
           .sinceVersion("0.14.0")
           .withDocumentation("sync meta info to origin table if enable");

File: hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.common.table;
 
 import org.apache.hudi.avro.HoodieAvroUtils;
+import org.apache.hudi.common.HoodieSchemaNotFoundException;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieLogFile;
@@ -588,6 +589,6 @@ public static Schema appendPartitionColumns(Schema dataSchema, Option<String[]>
   }
 
   private Supplier<Exception> schemaNotFoundError() {
-    return () -> new IllegalArgumentException("No schema found for table at " + metaClient.getBasePathV2().toString());
+    return () -> new HoodieSchemaNotFoundException("No schema found for table at " + metaClient.getBasePathV2().toString());
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/convert/AvroInternalSchemaConverter.java
Patch:
@@ -83,7 +83,9 @@ public static Schema convert(InternalSchema internalSchema, String name) {
    * @return an avro Schema where null is the first.
    */
   public static Schema fixNullOrdering(Schema schema) {
-    if (schema.getType() == Schema.Type.NULL) {
+    if (schema == null) {
+      return Schema.create(Schema.Type.NULL);
+    } else if (schema.getType() == Schema.Type.NULL) {
       return schema;
     }
     return convert(convert(schema), schema.getFullName());

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/TestDFSHoodieTestSuiteWriterAdapter.java
Patch:
@@ -69,7 +69,7 @@ public static void initClass() throws Exception {
   }
 
   @AfterAll
-  public static void cleanupClass() {
+  public static void cleanupClass() throws IOException {
     UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/TestFileDeltaInputWriter.java
Patch:
@@ -63,7 +63,7 @@ public static void initClass() throws Exception {
   }
 
   @AfterAll
-  public static void cleanupClass() {
+  public static void cleanupClass() throws IOException {
     UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/job/TestHoodieTestSuiteJob.java
Patch:
@@ -49,6 +49,7 @@
 import org.junit.jupiter.params.provider.Arguments;
 import org.junit.jupiter.params.provider.MethodSource;
 
+import java.io.IOException;
 import java.util.UUID;
 import java.util.stream.Stream;
 
@@ -134,7 +135,7 @@ public static void initClass() throws Exception {
   }
 
   @AfterAll
-  public static void cleanupClass() {
+  public static void cleanupClass() throws IOException {
     UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/reader/TestDFSAvroDeltaInputReader.java
Patch:
@@ -48,7 +48,7 @@ public static void initClass() throws Exception {
   }
 
   @AfterAll
-  public static void cleanupClass() {
+  public static void cleanupClass() throws IOException {
     UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/reader/TestDFSHoodieDatasetInputReader.java
Patch:
@@ -38,6 +38,7 @@
 import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 
+import java.io.IOException;
 import java.util.HashSet;
 import java.util.List;
 
@@ -55,7 +56,7 @@ public static void initClass() throws Exception {
   }
 
   @AfterAll
-  public static void cleanupClass() {
+  public static void cleanupClass() throws IOException {
     UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -1705,11 +1705,11 @@ public void testDistributedTestDataSource() {
     assertEquals(1000, c);
   }
 
-  private static void prepareJsonKafkaDFSFiles(int numRecords, boolean createTopic, String topicName) {
+  private void prepareJsonKafkaDFSFiles(int numRecords, boolean createTopic, String topicName) {
     prepareJsonKafkaDFSFiles(numRecords, createTopic, topicName, 2);
   }
 
-  private static void prepareJsonKafkaDFSFiles(int numRecords, boolean createTopic, String topicName, int numPartitions) {
+  private void prepareJsonKafkaDFSFiles(int numRecords, boolean createTopic, String topicName, int numPartitions) {
     if (createTopic) {
       try {
         testUtils.createTopic(topicName, numPartitions);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamerSchemaEvolutionBase.java
Patch:
@@ -136,7 +136,6 @@ public void teardown() throws Exception {
   @AfterAll
   static void teardownAll() {
     defaultSchemaProviderClassName = FilebasedSchemaProvider.class.getName();
-    HoodieDeltaStreamerTestBase.cleanupKafkaTestUtils();
   }
 
   protected HoodieStreamer deltaStreamer;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/schema/TestFilebasedSchemaProvider.java
Patch:
@@ -51,7 +51,7 @@ public static void initClass() throws Exception {
   }
 
   @AfterAll
-  public static void cleanUpUtilitiesTestServices() {
+  public static void cleanUpUtilitiesTestServices() throws IOException {
     UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestSqlSource.java
Patch:
@@ -64,7 +64,7 @@ public static void initClass() throws Exception {
   }
 
   @AfterAll
-  public static void cleanupClass() {
+  public static void cleanupClass() throws IOException {
     UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/testutils/sources/AbstractCloudObjectsSourceTestBase.java
Patch:
@@ -58,7 +58,7 @@ public static void initClass() throws Exception {
   }
 
   @AfterAll
-  public static void cleanupClass() {
+  public static void cleanupClass() throws IOException {
     UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 

File: hudi-aws/src/main/java/org/apache/hudi/aws/sync/AWSGlueCatalogSyncClient.java
Patch:
@@ -199,7 +199,7 @@ public void addPartitionsToTable(String tableName, List<String> partitionsToAdd)
       Table table = getTable(awsGlue, databaseName, tableName);
       StorageDescriptor sd = table.storageDescriptor();
       List<PartitionInput> partitionInputs = partitionsToAdd.stream().map(partition -> {
-        String fullPartitionPath = FSUtils.getPartitionPath(getBasePath(), partition).toString();
+        String fullPartitionPath = FSUtils.getPartitionPath(s3aToS3(getBasePath()), partition).toString();
         List<String> partitionValues = partitionValueExtractor.extractPartitionValuesInPath(partition);
         StorageDescriptor partitionSD = sd.copy(copySd -> copySd.location(fullPartitionPath));
         return PartitionInput.builder().values(partitionValues).storageDescriptor(partitionSD).build();
@@ -242,7 +242,7 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit
       Table table = getTable(awsGlue, databaseName, tableName);
       StorageDescriptor sd = table.storageDescriptor();
       List<BatchUpdatePartitionRequestEntry> updatePartitionEntries = changedPartitions.stream().map(partition -> {
-        String fullPartitionPath = FSUtils.getPartitionPath(getBasePath(), partition).toString();
+        String fullPartitionPath = FSUtils.getPartitionPath(s3aToS3(getBasePath()), partition).toString();
         List<String> partitionValues = partitionValueExtractor.extractPartitionValuesInPath(partition);
         StorageDescriptor partitionSD = sd.copy(copySd -> copySd.location(fullPartitionPath));
         PartitionInput partitionInput = PartitionInput.builder().values(partitionValues).storageDescriptor(partitionSD).build();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java
Patch:
@@ -219,7 +219,8 @@ private HoodieCleanMetadata runClean(HoodieTable<T, I, K, O> table, HoodieInstan
       HoodieCleanMetadata metadata = CleanerUtils.convertCleanMetadata(
           inflightInstant.getTimestamp(),
           Option.of(timer.endTimer()),
-          cleanStats
+          cleanStats,
+          cleanerPlan.getExtraMetadata()
       );
       if (!skipLocking) {
         this.txnManager.beginTransaction(Option.of(inflightInstant), Option.empty());

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/utils/TestMetadataConversionUtils.java
Patch:
@@ -392,7 +392,7 @@ private void createReplace(String instantTime, WriteOperationType writeOperation
 
   private void createCleanMetadata(String instantTime) throws IOException {
     HoodieCleanerPlan cleanerPlan = new HoodieCleanerPlan(new HoodieActionInstant("", "", ""),
-        "", "", new HashMap<>(), CleanPlanV2MigrationHandler.VERSION, new HashMap<>(), new ArrayList<>());
+        "", "", new HashMap<>(), CleanPlanV2MigrationHandler.VERSION, new HashMap<>(), new ArrayList<>(), Collections.EMPTY_MAP);
     HoodieCleanStat cleanStats = new HoodieCleanStat(
         HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS,
         HoodieTestUtils.DEFAULT_PARTITION_PATHS[new Random().nextInt(HoodieTestUtils.DEFAULT_PARTITION_PATHS.length)],
@@ -401,7 +401,7 @@ HoodieTestUtils.DEFAULT_PARTITION_PATHS[new Random().nextInt(HoodieTestUtils.DEF
         Collections.emptyList(),
         instantTime,
         "");
-    HoodieCleanMetadata cleanMetadata = convertCleanMetadata(instantTime, Option.of(0L), Collections.singletonList(cleanStats));
+    HoodieCleanMetadata cleanMetadata = convertCleanMetadata(instantTime, Option.of(0L), Collections.singletonList(cleanStats), Collections.EMPTY_MAP);
     HoodieTestTable.of(metaClient).addClean(instantTime, cleanerPlan, cleanMetadata);
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestExternalPathHandling.java
Patch:
@@ -154,7 +154,8 @@ public void testFlow(FileIdAndNameGenerator fileIdAndNameGenerator, List<String>
     HoodieCleanMetadata cleanMetadata = CleanerUtils.convertCleanMetadata(
         cleanTime,
         Option.empty(),
-        cleanStats);
+        cleanStats,
+        Collections.EMPTY_MAP);
     try (HoodieTableMetadataWriter hoodieTableMetadataWriter = (HoodieTableMetadataWriter) writeClient.initTable(WriteOperationType.UPSERT, Option.of(cleanTime)).getMetadataWriter(cleanTime).get()) {
       hoodieTableMetadataWriter.update(cleanMetadata, cleanTime);
       metaClient.getActiveTimeline().transitionCleanInflightToComplete(true, inflightClean,
@@ -292,6 +293,6 @@ private HoodieCleanerPlan cleanerPlan(HoodieActionInstant earliestInstantToRetai
     return new HoodieCleanerPlan(earliestInstantToRetain,
         latestCommit,
         writeConfig.getCleanerPolicy().name(), Collections.emptyMap(),
-        CleanPlanner.LATEST_CLEAN_PLAN_VERSION, filePathsToBeDeletedPerPartition, Collections.emptyList());
+        CleanPlanner.LATEST_CLEAN_PLAN_VERSION, filePathsToBeDeletedPerPartition, Collections.emptyList(), Collections.EMPTY_MAP);
   }
 }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieSparkClientTestHarness.java
Patch:
@@ -643,7 +643,7 @@ public HoodieInstant createEmptyCleanMetadata(String instantTime, boolean inflig
 
   public HoodieInstant createCleanMetadata(String instantTime, boolean inflightOnly, boolean isEmptyForAll, boolean isEmptyCompleted) throws IOException {
     HoodieCleanerPlan cleanerPlan = new HoodieCleanerPlan(new HoodieActionInstant("", "", ""), "", "",
-            new HashMap<>(), CleanPlanV2MigrationHandler.VERSION, new HashMap<>(), new ArrayList<>());
+            new HashMap<>(), CleanPlanV2MigrationHandler.VERSION, new HashMap<>(), new ArrayList<>(), Collections.EMPTY_MAP);
     if (inflightOnly) {
       HoodieTestTable.of(metaClient).addInflightClean(instantTime, cleanerPlan);
     } else {
@@ -655,7 +655,7 @@ HoodieTestUtils.DEFAULT_PARTITION_PATHS[new Random().nextInt(HoodieTestUtils.DEF
               Collections.emptyList(),
               instantTime,
               "");
-      HoodieCleanMetadata cleanMetadata = convertCleanMetadata(instantTime, Option.of(0L), Collections.singletonList(cleanStats));
+      HoodieCleanMetadata cleanMetadata = convertCleanMetadata(instantTime, Option.of(0L), Collections.singletonList(cleanStats), Collections.EMPTY_MAP);
       HoodieTestTable.of(metaClient).addClean(instantTime, cleanerPlan, cleanMetadata, isEmptyForAll, isEmptyCompleted);
     }
     return new HoodieInstant(inflightOnly, "clean", instantTime);

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanPlanV1MigrationHandler.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hadoop.fs.Path;
 
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -63,6 +64,6 @@ public HoodieCleanerPlan downgradeFrom(HoodieCleanerPlan plan) {
         .map(e -> Pair.of(e.getKey(), e.getValue().stream().map(v -> new Path(v.getFilePath()).getName())
             .collect(Collectors.toList()))).collect(Collectors.toMap(Pair::getKey, Pair::getValue));
     return new HoodieCleanerPlan(plan.getEarliestInstantToRetain(), plan.getLastCompletedCommitTimestamp(),
-        plan.getPolicy(), filesPerPartition, VERSION, new HashMap<>(), new ArrayList<>());
+        plan.getPolicy(), filesPerPartition, VERSION, new HashMap<>(), new ArrayList<>(), Collections.EMPTY_MAP);
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanPlanV2MigrationHandler.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hadoop.fs.Path;
 
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -57,7 +58,7 @@ public HoodieCleanerPlan upgradeFrom(HoodieCleanerPlan plan) {
                 new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), e.getKey()), v).toString(), false))
             .collect(Collectors.toList()))).collect(Collectors.toMap(Pair::getKey, Pair::getValue));
     return new HoodieCleanerPlan(plan.getEarliestInstantToRetain(), plan.getLastCompletedCommitTimestamp(),
-        plan.getPolicy(), new HashMap<>(), VERSION, filePathsPerPartition, new ArrayList<>());
+        plan.getPolicy(), new HashMap<>(), VERSION, filePathsPerPartition, new ArrayList<>(), Collections.emptyMap());
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java
Patch:
@@ -64,7 +64,8 @@ public class CleanerUtils {
 
   public static HoodieCleanMetadata convertCleanMetadata(String startCleanTime,
                                                          Option<Long> durationInMs,
-                                                         List<HoodieCleanStat> cleanStats) {
+                                                         List<HoodieCleanStat> cleanStats,
+                                                         Map<String, String> extraMetadatafromCleanPlan) {
     Map<String, HoodieCleanPartitionMetadata> partitionMetadataMap = new HashMap<>();
     Map<String, HoodieCleanPartitionMetadata> partitionBootstrapMetadataMap = new HashMap<>();
 
@@ -92,7 +93,7 @@ public static HoodieCleanMetadata convertCleanMetadata(String startCleanTime,
     }
 
     return new HoodieCleanMetadata(startCleanTime, durationInMs.orElseGet(() -> -1L), totalDeleted, earliestCommitToRetain,
-        lastCompletedCommitTimestamp, partitionMetadataMap, CLEAN_METADATA_VERSION_2, partitionBootstrapMetadataMap);
+        lastCompletedCommitTimestamp, partitionMetadataMap, CLEAN_METADATA_VERSION_2, partitionBootstrapMetadataMap, extraMetadatafromCleanPlan);
   }
 
   /**

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java
Patch:
@@ -631,7 +631,7 @@ private void performClean(String instant, List<String> files, String cleanInstan
 
     HoodieInstant cleanInflightInstant = new HoodieInstant(true, HoodieTimeline.CLEAN_ACTION, cleanInstant);
     metaClient.getActiveTimeline().createNewInstant(cleanInflightInstant);
-    HoodieCleanMetadata cleanMetadata = CleanerUtils.convertCleanMetadata(cleanInstant, Option.empty(), cleanStats);
+    HoodieCleanMetadata cleanMetadata = CleanerUtils.convertCleanMetadata(cleanInstant, Option.empty(), cleanStats, Collections.EMPTY_MAP);
     metaClient.getActiveTimeline().saveAsComplete(cleanInflightInstant,
         TimelineMetadataUtils.serializeCleanMetadata(cleanMetadata));
   }

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestClusteringUtils.java
Patch:
@@ -181,7 +181,7 @@ public void testGetOldestInstantToRetainForClustering() throws IOException {
     metaClient.getActiveTimeline().saveToCleanRequested(requestedInstant4, TimelineMetadataUtils.serializeCleanerPlan(cleanerPlan1));
     HoodieInstant inflightInstant4 = metaClient.getActiveTimeline().transitionCleanRequestedToInflight(requestedInstant4, Option.empty());
     HoodieCleanMetadata cleanMetadata = new HoodieCleanMetadata(cleanTime1, 1L, 1,
-        completedInstant3.getTimestamp(), "", Collections.emptyMap(), 0, Collections.emptyMap());
+        completedInstant3.getTimestamp(), "", Collections.emptyMap(), 0, Collections.emptyMap(), Collections.emptyMap());
     metaClient.getActiveTimeline().transitionCleanInflightToComplete(true, inflightInstant4,
         TimelineMetadataUtils.serializeCleanMetadata(cleanMetadata));
     metaClient.reloadActiveTimeline();
@@ -205,11 +205,11 @@ public void testGetOldestInstantToRetainForClusteringKeepFileVersion() throws IO
     HoodieInstant requestedInstant2 = new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.CLEAN_ACTION, cleanTime1);
     HoodieCleanerPlan cleanerPlan1 = new HoodieCleanerPlan(null, clusterTime1,
         HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS.name(), Collections.emptyMap(),
-        CleanPlanV2MigrationHandler.VERSION, Collections.emptyMap(), Collections.emptyList());
+        CleanPlanV2MigrationHandler.VERSION, Collections.emptyMap(), Collections.emptyList(), Collections.EMPTY_MAP);
     metaClient.getActiveTimeline().saveToCleanRequested(requestedInstant2, TimelineMetadataUtils.serializeCleanerPlan(cleanerPlan1));
     HoodieInstant inflightInstant2 = metaClient.getActiveTimeline().transitionCleanRequestedToInflight(requestedInstant2, Option.empty());
     HoodieCleanMetadata cleanMetadata = new HoodieCleanMetadata(cleanTime1, 1L, 1,
-        "", "", Collections.emptyMap(), 0, Collections.emptyMap());
+        "", "", Collections.emptyMap(), 0, Collections.emptyMap(), Collections.emptyMap());
     metaClient.getActiveTimeline().transitionCleanInflightToComplete(true, inflightInstant2,
         TimelineMetadataUtils.serializeCleanMetadata(cleanMetadata));
     metaClient.reloadActiveTimeline();

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerTestBase.java
Patch:
@@ -67,7 +67,7 @@
 import java.util.concurrent.TimeUnit;
 import java.util.function.Function;
 
-import static org.apache.hudi.common.config.HoodieCommonConfig.HANDLE_MISSING_COLUMNS_WITH_LOSSLESS_TYPE_PROMOTIONS;
+import static org.apache.hudi.common.config.HoodieCommonConfig.SET_NULL_FOR_MISSING_COLUMNS;
 import static org.apache.hudi.common.table.timeline.TimelineMetadataUtils.serializeCommitMetadata;
 import static org.apache.hudi.common.util.StringUtils.nonEmpty;
 import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_URL;
@@ -610,7 +610,7 @@ static HoodieDeltaStreamer.Config makeConfigForHudiIncrSrc(String srcBasePath, S
         cfg.schemaProviderClassName = schemaProviderClassName;
       }
       List<String> cfgs = new ArrayList<>();
-      cfgs.add(HANDLE_MISSING_COLUMNS_WITH_LOSSLESS_TYPE_PROMOTIONS.key() + "=true");
+      cfgs.add(SET_NULL_FOR_MISSING_COLUMNS.key() + "=true");
       cfgs.add("hoodie.deltastreamer.source.hoodieincr.read_latest_on_missing_ckpt=" + addReadLatestOnMissingCkpt);
       cfgs.add("hoodie.deltastreamer.source.hoodieincr.path=" + srcBasePath);
       // No partition

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamerSchemaEvolutionBase.java
Patch:
@@ -160,7 +160,7 @@ protected HoodieDeltaStreamer.Config getDeltaStreamerConfig(String[] transformer
     extraProps.setProperty(HoodieReaderConfig.FILE_GROUP_READER_ENABLED.key(), "false");
     extraProps.setProperty("hoodie.datasource.write.table.type", tableType);
     extraProps.setProperty("hoodie.datasource.write.row.writer.enable", rowWriterEnable.toString());
-    extraProps.setProperty(DataSourceWriteOptions.HANDLE_MISSING_COLUMNS_WITH_LOSSLESS_TYPE_PROMOTIONS().key(), Boolean.toString(nullForDeletedCols));
+    extraProps.setProperty(DataSourceWriteOptions.SET_NULL_FOR_MISSING_COLUMNS().key(), Boolean.toString(nullForDeletedCols));
 
     //we set to 0 so that we create new base files on insert instead of adding inserts to existing filegroups via small file handling
     extraProps.setProperty("hoodie.parquet.small.file.limit", "0");

File: hudi-common/src/main/java/org/apache/hudi/common/bloom/InternalFilter.java
Patch:
@@ -192,7 +192,7 @@ public void write(DataOutput out) throws IOException {
    * <p>For efficiency, implementations should attempt to re-use storage in the
    * existing object where possible.</p>
    *
-   * @param in <code>DataInput</code> to deseriablize this object from.
+   * @param in <code>DataInput</code> to deserialize this object from.
    * @throws IOException
    */
   public void readFields(DataInput in) throws IOException {

File: hudi-common/src/test/java/org/apache/hudi/io/storage/TestHoodieHFileReaderWriterBase.java
Patch:
@@ -472,7 +472,7 @@ public void testHoodieHFileCompatibility(String hfilePrefix) throws IOException
         content, hfilePrefix, false, HFileBootstrapIndex.HoodieKVComparator.class, 4);
   }
 
-  private Set<String> getRandomKeys(int count, List<String> keys) {
+  Set<String> getRandomKeys(int count, List<String> keys) {
     Set<String> rowKeys = new HashSet<>();
     int totalKeys = keys.size();
     while (rowKeys.size() < count) {

File: hudi-io/src/main/java/org/apache/hudi/storage/HoodieLocation.java
Patch:
@@ -108,7 +108,8 @@ public HoodieLocation(HoodieLocation parent, String child) {
           parentUri.getAuthority(),
           parentPathWithSeparator,
           null,
-          parentUri.getFragment()).resolve(normalizedChild);
+          parentUri.getFragment())
+          .resolve(new URI(null, null, normalizedChild, null, null));
       this.uri = new URI(
           parentUri.getScheme(),
           parentUri.getAuthority(),

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -82,6 +82,7 @@
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeAll;
 import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.TestInfo;
 import org.junit.jupiter.api.io.TempDir;
@@ -1946,6 +1947,7 @@ public void testAvroLogRecordReaderWithInsertsDeleteAndRollback(ExternalSpillabl
         0, 0, Option.empty());
   }
 
+  @Disabled("HUDI-7375")
   @ParameterizedTest
   @MethodSource("testArguments")
   public void testLogReaderWithDifferentVersionsOfDeleteBlocks(ExternalSpillableMap.DiskMapType diskMapType,

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java
Patch:
@@ -517,7 +517,7 @@ public Option<HoodieInstant> getLastClusterCommit() {
   }
 
   @Override
-  public Option<HoodieInstant> getLastPendingClusterCommit() {
+  public Option<HoodieInstant> getLastPendingClusterInstant() {
     return  Option.fromJavaOptional(filterPendingReplaceTimeline()
         .getReverseOrderedInstants()
         .filter(i -> ClusteringUtils.isPendingClusteringInstant(this, i)).findFirst());

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java
Patch:
@@ -405,7 +405,7 @@ public interface HoodieTimeline extends Serializable {
    * get the most recent pending cluster commit if present
    *
    */
-  public Option<HoodieInstant> getLastPendingClusterCommit();
+  public Option<HoodieInstant> getLastPendingClusterInstant();
 
   /**
    * Read the completed instant details.

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestClusteringUtils.java
Patch:
@@ -104,20 +104,20 @@ public void testClusteringPlanMultipleInstants() throws Exception {
     validateClusteringInstant(fileIds1, partitionPath1, clusterTime1, fileGroupToInstantMap);
     validateClusteringInstant(fileIds2, partitionPath1, clusterTime, fileGroupToInstantMap);
     validateClusteringInstant(fileIds3, partitionPath1, clusterTime, fileGroupToInstantMap);
-    Option<HoodieInstant> lastPendingClustering = metaClient.getActiveTimeline().getLastPendingClusterCommit();
+    Option<HoodieInstant> lastPendingClustering = metaClient.getActiveTimeline().getLastPendingClusterInstant();
     assertTrue(lastPendingClustering.isPresent());
     assertEquals("2", lastPendingClustering.get().getTimestamp());
 
     //check that it still gets picked if it is inflight
     HoodieInstant inflight = metaClient.getActiveTimeline().transitionReplaceRequestedToInflight(lastPendingClustering.get(), Option.empty());
     assertEquals(HoodieInstant.State.INFLIGHT, inflight.getState());
-    lastPendingClustering = metaClient.reloadActiveTimeline().getLastPendingClusterCommit();
+    lastPendingClustering = metaClient.reloadActiveTimeline().getLastPendingClusterInstant();
     assertEquals("2", lastPendingClustering.get().getTimestamp());
 
     //now that it is complete, the first instant should be picked
     HoodieInstant complete = metaClient.getActiveTimeline().transitionReplaceInflightToComplete(false, inflight, Option.empty());
     assertEquals(HoodieInstant.State.COMPLETED, complete.getState());
-    lastPendingClustering = metaClient.reloadActiveTimeline().getLastPendingClusterCommit();
+    lastPendingClustering = metaClient.reloadActiveTimeline().getLastPendingClusterInstant();
     assertEquals("1", lastPendingClustering.get().getTimestamp());
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/StreamSync.java
Patch:
@@ -455,7 +455,7 @@ public Pair<Option<String>, JavaRDD<WriteStatus>> syncOnce() throws IOException
 
   private Option<String> getLastPendingClusteringInstant(Option<HoodieTimeline> commitTimelineOpt) {
     if (commitTimelineOpt.isPresent()) {
-      Option<HoodieInstant> pendingClusteringInstant = commitTimelineOpt.get().getLastPendingClusterCommit();
+      Option<HoodieInstant> pendingClusteringInstant = commitTimelineOpt.get().getLastPendingClusterInstant();
       return pendingClusteringInstant.isPresent() ? Option.of(pendingClusteringInstant.get().getTimestamp()) : Option.empty();
     }
     return Option.empty();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCDCLogger.java
Patch:
@@ -235,7 +235,7 @@ public void close() {
       throw new HoodieIOException("Failed to close HoodieCDCLogger", e);
     } finally {
       // in case that crash when call `flushIfNeeded`, do the cleanup again.
-      cdcData.clear();
+      cdcData.close();
     }
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -57,6 +57,7 @@
 
 import javax.annotation.concurrent.NotThreadSafe;
 
+import java.io.Closeable;
 import java.io.IOException;
 import java.util.Collections;
 import java.util.HashSet;
@@ -426,8 +427,8 @@ public List<WriteStatus> close() {
       markClosed();
       writeIncomingRecords();
 
-      if (keyToNewRecords instanceof ExternalSpillableMap) {
-        ((ExternalSpillableMap) keyToNewRecords).close();
+      if (keyToNewRecords instanceof Closeable) {
+        ((Closeable) keyToNewRecords).close();
       }
 
       keyToNewRecords = null;

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java
Patch:
@@ -218,8 +218,8 @@ private synchronized R put(T key, R value, boolean flush) {
     try {
       byte[] val = isCompressionEnabled ? DISK_COMPRESSION_REF.get().compressBytes(SerializationUtils.serialize(value)) :
           SerializationUtils.serialize(value);
-      Integer valueSize = val.length;
-      Long timestamp = System.currentTimeMillis();
+      int valueSize = val.length;
+      long timestamp = System.currentTimeMillis();
       this.valueMetadataMap.put(key,
           new BitCaskDiskMap.ValueMetadata(this.filePath, valueSize, filePosition.get(), timestamp));
       byte[] serializedKey = SerializationUtils.serialize(key);
@@ -271,6 +271,7 @@ public void close() {
         fileOutputStream.getChannel().force(false);
         writeOnlyFileHandle.close();
       }
+      fileOutputStream.close();
 
       while (!openedAccessFiles.isEmpty()) {
         BufferedRandomAccessFile file = openedAccessFiles.poll();

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieMergeOnReadSnapshotReader.java
Patch:
@@ -217,5 +217,8 @@ public void close() throws Exception {
     if (logRecordScanner != null) {
       logRecordScanner.close();
     }
+    if (mergedRecordsByKey != null) {
+      mergedRecordsByKey.close();
+    }
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java
Patch:
@@ -59,6 +59,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
+import java.util.function.Supplier;
 import java.util.zip.DeflaterOutputStream;
 import java.util.zip.InflaterInputStream;
 
@@ -74,15 +75,15 @@ public class HoodieAvroDataBlock extends HoodieDataBlock {
 
   private final ThreadLocal<BinaryEncoder> encoderCache = new ThreadLocal<>();
 
-  public HoodieAvroDataBlock(FSDataInputStream inputStream,
+  public HoodieAvroDataBlock(Supplier<FSDataInputStream> inputStreamSupplier,
                              Option<byte[]> content,
                              boolean readBlockLazily,
                              HoodieLogBlockContentLocation logBlockContentLocation,
                              Option<Schema> readerSchema,
                              Map<HeaderMetadataType, String> header,
                              Map<HeaderMetadataType, String> footer,
                              String keyField) {
-    super(content, inputStream, readBlockLazily, Option.of(logBlockContentLocation), readerSchema, header, footer, keyField, false);
+    super(content, inputStreamSupplier, readBlockLazily, Option.of(logBlockContentLocation), readerSchema, header, footer, keyField, false);
   }
 
   public HoodieAvroDataBlock(@Nonnull List<HoodieRecord> records,

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieCDCDataBlock.java
Patch:
@@ -27,21 +27,22 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.function.Supplier;
 
 /**
  * Change log supplemental log data block.
  */
 public class HoodieCDCDataBlock extends HoodieAvroDataBlock {
 
   public HoodieCDCDataBlock(
-      FSDataInputStream inputStream,
+      Supplier<FSDataInputStream> inputStreamSupplier,
       Option<byte[]> content,
       boolean readBlockLazily,
       HoodieLogBlockContentLocation logBlockContentLocation,
       Schema readerSchema,
       Map<HeaderMetadataType, String> header,
       String keyField) {
-    super(inputStream, content, readBlockLazily, logBlockContentLocation,
+    super(inputStreamSupplier, content, readBlockLazily, logBlockContentLocation,
         Option.of(readerSchema), header, new HashMap<>(), keyField);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieCommandBlock.java
Patch:
@@ -24,6 +24,7 @@
 
 import java.util.HashMap;
 import java.util.Map;
+import java.util.function.Supplier;
 
 /**
  * Command block issues a specific command to the scanner.
@@ -43,10 +44,10 @@ public HoodieCommandBlock(Map<HeaderMetadataType, String> header) {
     this(Option.empty(), null, false, Option.empty(), header, new HashMap<>());
   }
 
-  public HoodieCommandBlock(Option<byte[]> content, FSDataInputStream inputStream, boolean readBlockLazily,
+  public HoodieCommandBlock(Option<byte[]> content, Supplier<FSDataInputStream> inputStreamSupplier, boolean readBlockLazily,
                             Option<HoodieLogBlockContentLocation> blockContentLocation, Map<HeaderMetadataType, String> header,
                             Map<HeaderMetadataType, String> footer) {
-    super(header, footer, blockContentLocation, content, inputStream, readBlockLazily);
+    super(header, footer, blockContentLocation, content, inputStreamSupplier, readBlockLazily);
     this.type =
         HoodieCommandBlockTypeEnum.values()[Integer.parseInt(header.get(HeaderMetadataType.COMMAND_BLOCK_TYPE))];
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieCorruptBlock.java
Patch:
@@ -24,17 +24,18 @@
 
 import java.io.IOException;
 import java.util.Map;
+import java.util.function.Supplier;
 
 /**
  * Corrupt block is emitted whenever the scanner finds the length of the block written at the beginning does not match
  * (did not find a EOF or a sync marker after the length).
  */
 public class HoodieCorruptBlock extends HoodieLogBlock {
 
-  public HoodieCorruptBlock(Option<byte[]> corruptedBytes, FSDataInputStream inputStream, boolean readBlockLazily,
+  public HoodieCorruptBlock(Option<byte[]> corruptedBytes, Supplier<FSDataInputStream> inputStreamSupplier, boolean readBlockLazily,
                             Option<HoodieLogBlockContentLocation> blockContentLocation, Map<HeaderMetadataType, String> header,
                             Map<HeaderMetadataType, String> footer) {
-    super(header, footer, blockContentLocation, corruptedBytes, inputStream, readBlockLazily);
+    super(header, footer, blockContentLocation, corruptedBytes, inputStreamSupplier, readBlockLazily);
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDataBlock.java
Patch:
@@ -38,6 +38,7 @@
 import java.util.Set;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.function.Function;
+import java.util.function.Supplier;
 import java.util.stream.Collectors;
 
 import static org.apache.hudi.common.model.HoodieRecordLocation.isPositionValid;
@@ -109,15 +110,15 @@ public HoodieDataBlock(List<HoodieRecord> records,
    * NOTE: This ctor is used on the write-path (ie when records ought to be written into the log)
    */
   protected HoodieDataBlock(Option<byte[]> content,
-                            FSDataInputStream inputStream,
+                            Supplier<FSDataInputStream> inputStreamSupplier,
                             boolean readBlockLazily,
                             Option<HoodieLogBlockContentLocation> blockContentLocation,
                             Option<Schema> readerSchema,
                             Map<HeaderMetadataType, String> headers,
                             Map<HeaderMetadataType, String> footer,
                             String keyFieldName,
                             boolean enablePointLookups) {
-    super(headers, footer, blockContentLocation, content, inputStream, readBlockLazily);
+    super(headers, footer, blockContentLocation, content, inputStreamSupplier, readBlockLazily);
     // Setting `shouldWriteRecordPositions` to false as this constructor is only used by the reader
     this.shouldWriteRecordPositions = false;
     this.records = Option.empty();

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java
Patch:
@@ -57,6 +57,7 @@
 import java.util.Map;
 import java.util.Properties;
 import java.util.TreeMap;
+import java.util.function.Supplier;
 
 import static org.apache.hudi.common.util.StringUtils.getUTF8Bytes;
 import static org.apache.hudi.common.util.TypeUtils.unsafeCast;
@@ -75,7 +76,7 @@ public class HoodieHFileDataBlock extends HoodieDataBlock {
   // interpreted as the actual file path for the HFile data blocks
   private final Path pathForReader;
 
-  public HoodieHFileDataBlock(FSDataInputStream inputStream,
+  public HoodieHFileDataBlock(Supplier<FSDataInputStream> inputStreamSupplier,
                               Option<byte[]> content,
                               boolean readBlockLazily,
                               HoodieLogBlockContentLocation logBlockContentLocation,
@@ -84,7 +85,7 @@ public HoodieHFileDataBlock(FSDataInputStream inputStream,
                               Map<HeaderMetadataType, String> footer,
                               boolean enablePointLookups,
                               Path pathForReader) {
-    super(content, inputStream, readBlockLazily, Option.of(logBlockContentLocation), readerSchema, header, footer, HoodieAvroHFileReader.KEY_FIELD_NAME, enablePointLookups);
+    super(content, inputStreamSupplier, readBlockLazily, Option.of(logBlockContentLocation), readerSchema, header, footer, HoodieAvroHFileReader.KEY_FIELD_NAME, enablePointLookups);
     this.compressionAlgorithm = Option.empty();
     this.pathForReader = pathForReader;
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieParquetDataBlock.java
Patch:
@@ -44,6 +44,7 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.function.Supplier;
 
 import static org.apache.hudi.common.config.HoodieStorageConfig.PARQUET_BLOCK_SIZE;
 import static org.apache.hudi.common.config.HoodieStorageConfig.PARQUET_COMPRESSION_CODEC_NAME;
@@ -62,15 +63,15 @@ public class HoodieParquetDataBlock extends HoodieDataBlock {
   private final Option<Double> expectedCompressionRatio;
   private final Option<Boolean> useDictionaryEncoding;
 
-  public HoodieParquetDataBlock(FSDataInputStream inputStream,
+  public HoodieParquetDataBlock(Supplier<FSDataInputStream> inputStreamSupplier,
                                 Option<byte[]> content,
                                 boolean readBlockLazily,
                                 HoodieLogBlockContentLocation logBlockContentLocation,
                                 Option<Schema> readerSchema,
                                 Map<HeaderMetadataType, String> header,
                                 Map<HeaderMetadataType, String> footer,
                                 String keyField) {
-    super(content, inputStream, readBlockLazily, Option.of(logBlockContentLocation), readerSchema, header, footer, keyField, false);
+    super(content, inputStreamSupplier, readBlockLazily, Option.of(logBlockContentLocation), readerSchema, header, footer, keyField, false);
 
     this.compressionCodecName = Option.empty();
     this.expectedCompressionRatio = Option.empty();

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/ExpressionPredicates.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.flink.table.functions.BuiltInFunctionDefinitions;
 import org.apache.flink.table.functions.FunctionDefinition;
 import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.hudi.util.ImplicitTypeConverter;
 import org.apache.parquet.filter2.predicate.FilterPredicate;
 import org.apache.parquet.filter2.predicate.Operators;
 import org.slf4j.Logger;
@@ -223,7 +224,8 @@ public ColumnPredicate bindValueLiteral(ValueLiteralExpression valueLiteral) {
 
     @Override
     public FilterPredicate filter() {
-      return toParquetPredicate(getFunctionDefinition(), literalType, columnName, literal);
+      Serializable convertedLiteral = ImplicitTypeConverter.convertImplicitly(literalType, literal);
+      return toParquetPredicate(getFunctionDefinition(), literalType, columnName, convertedLiteral);
     }
 
     /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergedReadHandle.java
Patch:
@@ -132,6 +132,7 @@ private HoodieMergedLogRecordScanner getLogRecordScanner(FileSlice fileSlice) {
         .withDiskMapType(config.getCommonConfig().getSpillableDiskMapType())
         .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())
         .withRecordMerger(config.getRecordMerger())
+        .withTableMetaClient(hoodieTable.getMetaClient())
         .build();
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java
Patch:
@@ -207,6 +207,7 @@ public List<WriteStatus> compact(HoodieCompactionHandler compactionHandler,
         .withOptimizedLogBlocksScan(executionHelper.enableOptimizedLogBlockScan(config))
         .withRecordMerger(config.getRecordMerger())
         .withInstantRange(instantRange)
+        .withTableMetaClient(metaClient)
         .build();
 
     Option<HoodieBaseFile> oldDataFileOpt =

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/plan/generators/HoodieLogCompactionPlanGenerator.java
Patch:
@@ -98,6 +98,7 @@ private boolean isFileSliceEligibleForLogCompaction(FileSlice fileSlice, String
         .withBufferSize(writeConfig.getMaxDFSStreamBufferSize())
         .withOptimizedLogBlocksScan(true)
         .withRecordMerger(writeConfig.getRecordMerger())
+        .withTableMetaClient(metaClient)
         .build();
     scanner.scan(true);
     int totalBlocks = scanner.getCurrentInstantLogBlocks().size();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java
Patch:
@@ -321,6 +321,7 @@ private HoodieData<HoodieRecord<T>> readRecordsForGroupWithLogs(JavaSparkContext
               .withDiskMapType(config.getCommonConfig().getSpillableDiskMapType())
               .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())
               .withRecordMerger(config.getRecordMerger())
+              .withTableMetaClient(table.getMetaClient())
               .build();
 
           Option<HoodieFileReader> baseFileReader = StringUtils.isNullOrEmpty(clusteringOp.getDataFilePath())

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnMergeOnReadStorage.java
Patch:
@@ -444,6 +444,7 @@ private void validateBlockInstantsBeforeAndAfterRollback(HoodieWriteConfig confi
             .withLatestInstantTime(instant)
             .withBufferSize(config.getMaxDFSStreamBufferSize())
             .withOptimizedLogBlocksScan(true)
+            .withTableMetaClient(metaClient)
             .build();
         scanner.scan(true);
         List<String> prevInstants = scanner.getValidBlockInstants();
@@ -457,6 +458,7 @@ private void validateBlockInstantsBeforeAndAfterRollback(HoodieWriteConfig confi
             .withLatestInstantTime(currentInstant)
             .withBufferSize(config.getMaxDFSStreamBufferSize())
             .withOptimizedLogBlocksScan(true)
+            .withTableMetaClient(table.getMetaClient())
             .build();
         scanner2.scan(true);
         List<String> currentInstants = scanner2.getValidBlockInstants();

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -498,6 +498,7 @@ public Pair<HoodieMetadataLogRecordReader, Long> getLogRecordScanner(List<Hoodie
         .enableFullScan(allowFullScan)
         .withPartition(partitionName)
         .withEnableOptimizedLogBlocksScan(metadataConfig.doEnableOptimizedLogBlocksScan())
+        .withTableMetaClient(metadataMetaClient)
         .build();
 
     Long logScannerOpenMs = timer.endTimer();

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -1837,6 +1837,7 @@ public static HoodieData<HoodieRecord> readRecordKeysFromFileSlices(HoodieEngine
                 engineType,
                 Collections.emptyList(), // TODO: support different merger classes, which is currently only known to write config
                 metaClient.getTableConfig().getRecordMergerStrategy()))
+            .withTableMetaClient(metaClient)
             .build();
         ClosableIterator<String> recordKeyIterator = ClosableIterator.wrap(mergedLogRecordScanner.getRecords().keySet().iterator());
         return new ClosableIterator<HoodieRecord>() {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/ExpressionPredicates.java
Patch:
@@ -616,10 +616,10 @@ private static FilterPredicate toParquetPredicate(FunctionDefinition functionDef
       case TINYINT:
       case SMALLINT:
       case INTEGER:
+      case DATE:
       case TIME_WITHOUT_TIME_ZONE:
         return predicateSupportsLtGt(functionDefinition, intColumn(columnName), (Integer) literal);
       case BIGINT:
-      case DATE:
       case TIMESTAMP_WITHOUT_TIME_ZONE:
         return predicateSupportsLtGt(functionDefinition, longColumn(columnName), (Long) literal);
       case FLOAT:

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/ExpressionUtils.java
Patch:
@@ -160,7 +160,7 @@ public static Object getValueFromLiteral(ValueLiteralExpression expr) {
             .orElse(null);
       case DATE:
         return expr.getValueAs(LocalDate.class)
-            .map(LocalDate::toEpochDay)
+            .map(date -> (int) date.toEpochDay())
             .orElse(null);
       // NOTE: All integral types of size less than Int are encoded as Ints in MT
       case BOOLEAN:
@@ -212,7 +212,7 @@ public static Object getKeyFromLiteral(ValueLiteralExpression expr, boolean logi
       case TIMESTAMP_WITHOUT_TIME_ZONE:
         return logicalTimestamp ? new Timestamp((long) val) : val;
       case DATE:
-        return LocalDate.ofEpochDay((long) val);
+        return LocalDate.ofEpochDay((int) val);
       default:
         return val;
     }

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/util/TestExpressionUtils.java
Patch:
@@ -140,7 +140,7 @@ void getValueFromLiteralForNonNull() {
       if (dataList.get(i) instanceof LocalTime) {
         assertEquals(((LocalTime) dataList.get(i)).get(ChronoField.MILLI_OF_DAY), ExpressionUtils.getValueFromLiteral((ValueLiteralExpression) childExprs.get(1)));
       } else if (dataList.get(i) instanceof LocalDate) {
-        assertEquals(((LocalDate) dataList.get(i)).toEpochDay(), ExpressionUtils.getValueFromLiteral((ValueLiteralExpression) childExprs.get(1)));
+        assertEquals((int) ((LocalDate) dataList.get(i)).toEpochDay(), ExpressionUtils.getValueFromLiteral((ValueLiteralExpression) childExprs.get(1)));
       } else if (dataList.get(i) instanceof LocalDateTime) {
         assertEquals(((LocalDateTime) dataList.get(i)).toInstant(ZoneOffset.UTC).toEpochMilli(), ExpressionUtils.getValueFromLiteral((ValueLiteralExpression) childExprs.get(1)));
       } else {

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/client/TestJavaHoodieBackedMetadata.java
Patch:
@@ -1534,8 +1534,8 @@ public void testEagerRollbackinMDT() throws IOException {
         fileStatus.getPath().getName().equals(rollbackInstant.getFileName())).collect(Collectors.toList());
 
     // ensure commit3's delta commit in MDT has last mod time > the actual rollback for previous failed commit i.e. commit2.
-    // if rollback wasn't eager, rollback's last mod time will be lower than the commit3'd delta commit last mod time.
-    assertTrue(commit3Files.get(0).getModificationTime() > rollbackFiles.get(0).getModificationTime());
+    // if rollback wasn't eager, rollback's last mod time will be not larger than the commit3'd delta commit last mod time.
+    assertTrue(commit3Files.get(0).getModificationTime() >= rollbackFiles.get(0).getModificationTime());
     client.close();
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/SparkClientFunctionalTestHarness.java
Patch:
@@ -201,6 +201,7 @@ public synchronized void runBeforeEach() {
       SparkRDDReadClient.addHoodieSupport(sparkConf);
       spark = SparkSession.builder().config(sparkConf).getOrCreate();
       sqlContext = spark.sqlContext();
+      HoodieClientTestUtils.overrideSparkHadoopConfiguration(spark.sparkContext());
       jsc = new JavaSparkContext(spark.sparkContext());
       context = new HoodieSparkEngineContext(jsc);
       timelineService = HoodieClientTestUtils.initTimelineService(

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/OptionsResolver.java
Patch:
@@ -340,7 +340,7 @@ public static boolean isReadByTxnCompletionTime(Configuration conf) {
    * Returns the index type.
    */
   public static HoodieIndex.IndexType getIndexType(Configuration conf) {
-    return HoodieIndex.IndexType.valueOf(conf.getString(FlinkOptions.INDEX_TYPE));
+    return HoodieIndex.IndexType.valueOf(conf.getString(FlinkOptions.INDEX_TYPE).toUpperCase());
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/FourToFiveUpgradeHandler.java
Patch:
@@ -77,7 +77,7 @@ public Map<ConfigProperty, String> upgrade(HoodieWriteConfig config, HoodieEngin
 
   private boolean hasDefaultPartitionPath(HoodieWriteConfig config, HoodieTable  table) throws IOException {
     HoodieTableConfig tableConfig = table.getMetaClient().getTableConfig();
-    if (!tableConfig.getPartitionFields().isPresent()) {
+    if (!tableConfig.isTablePartitioned()) {
       return false;
     }
     String checkPartitionPath = DEPRECATED_DEFAULT_PARTITION_PATH;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java
Patch:
@@ -495,7 +495,7 @@ private void completeClustering(HoodieReplaceCommitMetadata metadata,
         preCommit(metadata);
       }
       // Update table's metadata (table)
-      writeTableMetadata(table, clusteringInstant.getTimestamp(), metadata, writeStatuses.orElse(context.emptyHoodieData()));
+      writeTableMetadata(table, clusteringInstant.getTimestamp(), metadata, writeStatuses.orElseGet(context::emptyHoodieData));
 
       LOG.info("Committing Clustering " + clusteringCommitTime + ". Finished with result " + metadata);
 
@@ -1016,7 +1016,7 @@ private List<String> getInstantsToRollbackForLazyCleanPolicy(HoodieTableMetaClie
   @Deprecated
   public boolean rollback(final String commitInstantTime, Option<HoodiePendingRollbackInfo> pendingRollbackInfo, boolean skipLocking) throws HoodieRollbackException {
     final String rollbackInstantTime = pendingRollbackInfo.map(entry -> entry.getRollbackInstant().getTimestamp())
-        .orElse(createNewInstantTime(!skipLocking));
+        .orElseGet(() -> createNewInstantTime(!skipLocking));
     return rollback(commitInstantTime, pendingRollbackInfo, rollbackInstantTime, skipLocking);
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -297,7 +297,7 @@ private void saveInternalSchema(HoodieTable table, String instantTime, HoodieCom
       InternalSchema internalSchema;
       Schema avroSchema = HoodieAvroUtils.createHoodieWriteSchema(config.getSchema(), config.allowOperationMetadataField());
       if (historySchemaStr.isEmpty()) {
-        internalSchema = SerDeHelper.fromJson(config.getInternalSchema()).orElse(AvroInternalSchemaConverter.convert(avroSchema));
+        internalSchema = SerDeHelper.fromJson(config.getInternalSchema()).orElseGet(() -> AvroInternalSchemaConverter.convert(avroSchema));
         internalSchema.setSchemaId(Long.parseLong(instantTime));
       } else {
         internalSchema = InternalSchemaUtils.searchSchema(Long.parseLong(instantTime),

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java
Patch:
@@ -81,7 +81,7 @@ public static Option<HoodieCommitMetadata> resolveWriteConflictIfAny(
           table.getMetaClient(), currentTxnOwnerInstant.get(), lastCompletedTxnOwnerInstant),
               completedInstantsDuringCurrentWriteOperation);
 
-      final ConcurrentOperation thisOperation = new ConcurrentOperation(currentTxnOwnerInstant.get(), thisCommitMetadata.orElse(new HoodieCommitMetadata()));
+      final ConcurrentOperation thisOperation = new ConcurrentOperation(currentTxnOwnerInstant.get(), thisCommitMetadata.orElseGet(HoodieCommitMetadata::new));
       instantStream.forEach(instant -> {
         try {
           ConcurrentOperation otherOperation = new ConcurrentOperation(instant, table.getMetaClient());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -641,7 +641,7 @@ private void rollbackInflightInstant(HoodieInstant inflightInstant,
                                        Function<String, Option<HoodiePendingRollbackInfo>> getPendingRollbackInstantFunc) {
     final String commitTime = getPendingRollbackInstantFunc.apply(inflightInstant.getTimestamp()).map(entry
         -> entry.getRollbackInstant().getTimestamp())
-        .orElse(getMetaClient().createNewInstantTime());
+        .orElseGet(() -> getMetaClient().createNewInstantTime());
     scheduleRollback(context, commitTime, inflightInstant, false, config.shouldRollbackUsingMarkers(),
         false);
     rollback(context, commitTime, inflightInstant, false, false);
@@ -657,7 +657,7 @@ private void rollbackInflightInstant(HoodieInstant inflightInstant,
   public void rollbackInflightLogCompaction(HoodieInstant inflightInstant, Function<String, Option<HoodiePendingRollbackInfo>> getPendingRollbackInstantFunc) {
     final String commitTime = getPendingRollbackInstantFunc.apply(inflightInstant.getTimestamp()).map(entry
         -> entry.getRollbackInstant().getTimestamp())
-        .orElse(getMetaClient().createNewInstantTime());
+        .orElseGet(() -> getMetaClient().createNewInstantTime());
     scheduleRollback(context, commitTime, inflightInstant, false, config.shouldRollbackUsingMarkers(),
         false);
     rollback(context, commitTime, inflightInstant, true, false);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/functional/BaseHoodieFunctionalIndexClient.java
Patch:
@@ -45,7 +45,7 @@ public BaseHoodieFunctionalIndexClient() {
   public void register(HoodieTableMetaClient metaClient, String indexName, String indexType, Map<String, Map<String, String>> columns, Map<String, String> options) {
     LOG.info("Registering index {} of using {}", indexName, indexType);
     String indexMetaPath = metaClient.getTableConfig().getIndexDefinitionPath()
-        .orElse(metaClient.getMetaPath() + Path.SEPARATOR + HoodieTableMetaClient.INDEX_DEFINITION_FOLDER_NAME + Path.SEPARATOR + HoodieTableMetaClient.INDEX_DEFINITION_FILE_NAME);
+        .orElseGet(() -> metaClient.getMetaPath() + Path.SEPARATOR + HoodieTableMetaClient.INDEX_DEFINITION_FOLDER_NAME + Path.SEPARATOR + HoodieTableMetaClient.INDEX_DEFINITION_FILE_NAME);
     // build HoodieFunctionalIndexMetadata and then add to index definition file
     metaClient.buildFunctionalIndexDefinition(indexMetaPath, indexName, indexType, columns, options);
     // update table config if necessary

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/savepoint/SavepointActionExecutor.java
Patch:
@@ -90,7 +90,7 @@ public HoodieSavepointMetadata execute() {
         } catch (IOException e) {
           throw new HoodieSavepointException("Failed to savepoint " + instantTime, e);
         }
-      }).orElse(table.getCompletedCommitsTimeline().firstInstant().get().getTimestamp());
+      }).orElseGet(() -> table.getCompletedCommitsTimeline().firstInstant().get().getTimestamp());
 
       // Cannot allow savepoint time on a commit that could have been cleaned
       ValidationUtils.checkArgument(HoodieTimeline.compareTimestamps(instantTime, HoodieTimeline.GREATER_THAN_OR_EQUALS, lastCommitRetained),

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkTableServiceClient.java
Patch:
@@ -133,7 +133,7 @@ protected void completeClustering(
       // commit to data table after committing to metadata table.
       // We take the lock here to ensure all writes to metadata table happens within a single lock (single writer).
       // Because more than one write to metadata table will result in conflicts since all of them updates the same partition.
-      writeTableMetadata(table, clusteringCommitTime, metadata, writeStatuses.orElse(context.emptyHoodieData()));
+      writeTableMetadata(table, clusteringCommitTime, metadata, writeStatuses.orElseGet(context::emptyHoodieData));
 
       LOG.info("Committing Clustering {} finished with result {}.", clusteringCommitTime, metadata);
       table.getActiveTimeline().transitionReplaceInflightToComplete(

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java
Patch:
@@ -78,7 +78,7 @@ public HoodieWriteMetadata<List<WriteStatus>> bulkInsert(final List<HoodieRecord
           config.shouldAllowMultiWriteOnSameInstant());
     }
 
-    BulkInsertPartitioner partitioner = userDefinedBulkInsertPartitioner.orElse(JavaBulkInsertInternalPartitionerFactory.get(config.getBulkInsertSortMode()));
+    BulkInsertPartitioner partitioner = userDefinedBulkInsertPartitioner.orElseGet(() -> JavaBulkInsertInternalPartitionerFactory.get(config.getBulkInsertSortMode()));
 
     // write new files
     List<WriteStatus> writeStatuses = bulkInsert(inputRecords, instantTime, table, config, performDedupe, partitioner, false,

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java
Patch:
@@ -220,7 +220,7 @@ private <I> BulkInsertPartitioner<I> getPartitioner(Map<String, String> strategy
         default:
           throw new UnsupportedOperationException(String.format("Layout optimization strategy '%s' is not supported", layoutOptStrategy));
       }
-    }).orElse(isRowPartitioner
+    }).orElseGet(() -> isRowPartitioner
         ? BulkInsertInternalPartitionerWithRowsFactory.get(getWriteConfig(), getHoodieTable().isPartitioned(), true)
         : BulkInsertInternalPartitionerFactory.get(getHoodieTable(), getWriteConfig(), true));
   }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java
Patch:
@@ -74,7 +74,7 @@ public HoodieWriteMetadata<HoodieData<WriteStatus>> bulkInsert(final HoodieData<
             executor.getCommitActionType(), instantTime), Option.empty(),
         config.shouldAllowMultiWriteOnSameInstant());
 
-    BulkInsertPartitioner partitioner = userDefinedBulkInsertPartitioner.orElse(BulkInsertInternalPartitionerFactory.get(table, config));
+    BulkInsertPartitioner partitioner = userDefinedBulkInsertPartitioner.orElseGet(() -> BulkInsertInternalPartitionerFactory.get(table, config));
 
     // Write new files
     HoodieData<WriteStatus> writeStatuses =

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteCommitActionExecutor.java
Patch:
@@ -71,7 +71,7 @@ public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
   protected Partitioner getPartitioner(WorkloadProfile profile) {
     return table.getStorageLayout().layoutPartitionerClass()
         .map(c -> getLayoutPartitioner(profile, c))
-        .orElse(new SparkInsertOverwritePartitioner(profile, context, table, config));
+        .orElseGet(() -> new SparkInsertOverwritePartitioner(profile, context, table, config));
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/BaseHoodieTableFileIndex.java
Patch:
@@ -144,7 +144,7 @@ public BaseHoodieTableFileIndex(HoodieEngineContext engineContext,
                                   Option<String> beginInstantTime,
                                   Option<String> endInstantTime) {
     this.partitionColumns = metaClient.getTableConfig().getPartitionFields()
-        .orElse(new String[0]);
+        .orElseGet(() -> new String[0]);
 
     this.metadataConfig = HoodieMetadataConfig.newBuilder()
         .fromProperties(configProperties)
@@ -284,7 +284,7 @@ private Map<PartitionPath, List<FileSlice>> loadFileSlicesForPartitions(List<Par
                 queryInstant.map(instant ->
                         fileSystemView.getLatestMergedFileSlicesBeforeOrOn(partitionPath.path, queryInstant.get())
                     )
-                    .orElse(fileSystemView.getLatestFileSlices(partitionPath.path))
+                    .orElseGet(() -> fileSystemView.getLatestFileSlices(partitionPath.path))
                     .collect(Collectors.toList())
         ));
   }

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieConfig.java
Patch:
@@ -164,7 +164,7 @@ public <T> Integer getInt(ConfigProperty<T> configProperty) {
   public <T> Integer getIntOrDefault(ConfigProperty<T> configProperty) {
     Option<Object> rawValue = getRawValue(configProperty);
     return rawValue.map(v -> Integer.parseInt(v.toString()))
-        .orElse(Integer.parseInt(configProperty.defaultValue().toString()));
+        .orElseGet(() -> Integer.parseInt(configProperty.defaultValue().toString()));
   }
 
   public <T> Boolean getBoolean(ConfigProperty<T> configProperty) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java
Patch:
@@ -956,7 +956,7 @@ private Pair<ClosableIterator<HoodieRecord>, Schema> getRecordsIterator(
             .orElse(Function.identity());
 
     Schema schema = schemaEvolutionTransformerOpt.map(Pair::getRight)
-        .orElse(dataBlock.getSchema());
+        .orElseGet(dataBlock::getSchema);
 
     return Pair.of(new CloseableMappingIterator<>(blockRecordsIterator, transformer), schema);
   }

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/BaseHoodieQueueBasedExecutor.java
Patch:
@@ -131,7 +131,7 @@ private CompletableFuture<Void> startConsumingAsync() {
               return (Void) null;
             }, consumerExecutorService)
         )
-        .orElse(CompletableFuture.completedFuture(null));
+        .orElseGet(() -> CompletableFuture.completedFuture(null));
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/expression/PartialBindVisitor.java
Patch:
@@ -108,14 +108,14 @@ public Expression visitPredicate(Predicate predicate) {
       Predicates.IsNull isNull = (Predicates.IsNull) predicate;
       return Option.ofNullable(isNull.child.accept(this))
           .map(expr -> (Expression)Predicates.isNull(expr))
-          .orElse(alwaysTrue());
+          .orElseGet(this::alwaysTrue);
     }
 
     if (predicate instanceof Predicates.IsNotNull) {
       Predicates.IsNotNull isNotNull = (Predicates.IsNotNull) predicate;
       return Option.ofNullable(isNotNull.child.accept(this))
           .map(expr -> (Expression)Predicates.isNotNull(expr))
-          .orElse(alwaysTrue());
+          .orElseGet(this::alwaysTrue);
     }
 
     if (predicate instanceof Predicates.StringStartsWith) {

File: hudi-common/src/main/java/org/apache/hudi/index/secondary/SecondaryIndexManager.java
Patch:
@@ -116,7 +116,7 @@ public void create(
     List<HoodieSecondaryIndex> newSecondaryIndexes = secondaryIndexes.map(h -> {
       h.add(secondaryIndexToAdd);
       return h;
-    }).orElse(Collections.singletonList(secondaryIndexToAdd));
+    }).orElseGet(() -> Collections.singletonList(secondaryIndexToAdd));
     newSecondaryIndexes.sort(new HoodieSecondaryIndex.HoodieIndexCompactor());
 
     // Persistence secondary indexes' metadata to hoodie.properties file

File: hudi-common/src/main/java/org/apache/hudi/metadata/BaseTableMetadata.java
Patch:
@@ -358,7 +358,7 @@ FileStatus[] fetchAllFilesInPartition(Path partitionPath) throws IOException {
         throw new HoodieIOException("Failed to extract file-statuses from the payload", e);
       }
     })
-        .orElse(new FileStatus[0]);
+        .orElseGet(() -> new FileStatus[0]);
 
     LOG.info("Listed file in partition from metadata: partition=" + relativePartitionPath + ", #files=" + statuses.length);
     return statuses;

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -577,7 +577,7 @@ public HoodieTableFileSystemView getMetadataFileSystemView() {
 
   public Map<String, String> stats() {
     Set<String> allMetadataPartitionPaths = Arrays.stream(MetadataPartitionType.values()).map(MetadataPartitionType::getPartitionPath).collect(Collectors.toSet());
-    return metrics.map(m -> m.getStats(true, metadataMetaClient, this, allMetadataPartitionPaths)).orElse(new HashMap<>());
+    return metrics.map(m -> m.getStats(true, metadataMetaClient, this, allMetadataPartitionPaths)).orElseGet(HashMap::new);
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -1029,7 +1029,7 @@ private static List<FileSlice> getPartitionFileSlices(HoodieTableMetaClient meta
                                                         Option<HoodieTableFileSystemView> fileSystemView,
                                                         String partition,
                                                         boolean mergeFileSlices) {
-    HoodieTableFileSystemView fsView = fileSystemView.orElse(getFileSystemView(metaClient));
+    HoodieTableFileSystemView fsView = fileSystemView.orElseGet(() -> getFileSystemView(metaClient));
     Stream<FileSlice> fileSliceStream;
     if (mergeFileSlices) {
       if (metaClient.getActiveTimeline().filterCompletedInstants().lastInstant().isPresent()) {
@@ -1057,7 +1057,7 @@ private static List<FileSlice> getPartitionFileSlices(HoodieTableMetaClient meta
   public static List<FileSlice> getPartitionLatestFileSlicesIncludingInflight(HoodieTableMetaClient metaClient,
                                                                               Option<HoodieTableFileSystemView> fileSystemView,
                                                                               String partition) {
-    HoodieTableFileSystemView fsView = fileSystemView.orElse(getFileSystemView(metaClient));
+    HoodieTableFileSystemView fsView = fileSystemView.orElseGet(() -> getFileSystemView(metaClient));
     Stream<FileSlice> fileSliceStream = fsView.fetchLatestFileSlicesIncludingInflight(partition);
     return fileSliceStream
         .sorted(Comparator.comparing(FileSlice::getFileId))

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java
Patch:
@@ -176,12 +176,12 @@ public HoodieTableSource(
     this.dataPruner = dataPruner;
     this.partitionPruner = partitionPruner;
     this.dataBucket = dataBucket;
-    this.requiredPos = Optional.ofNullable(requiredPos).orElse(IntStream.range(0, this.tableRowType.getFieldCount()).toArray());
+    this.requiredPos = Optional.ofNullable(requiredPos).orElseGet(() -> IntStream.range(0, this.tableRowType.getFieldCount()).toArray());
     this.limit = Optional.ofNullable(limit).orElse(NO_LIMIT_CONSTANT);
     this.hadoopConf = HadoopConfigurations.getHadoopConf(conf);
-    this.metaClient = Optional.ofNullable(metaClient).orElse(StreamerUtil.metaClientForReader(conf, hadoopConf));
+    this.metaClient = Optional.ofNullable(metaClient).orElseGet(() -> StreamerUtil.metaClientForReader(conf, hadoopConf));
     this.maxCompactionMemoryInBytes = StreamerUtil.getMaxCompactionMemoryInBytes(conf);
-    this.internalSchemaManager = Optional.ofNullable(internalSchemaManager).orElse(InternalSchemaManager.get(this.conf, this.metaClient));
+    this.internalSchemaManager = Optional.ofNullable(internalSchemaManager).orElseGet(() -> InternalSchemaManager.get(this.conf, this.metaClient));
   }
 
   @Override

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/realtime/TestHoodieRealtimeRecordReader.java
Patch:
@@ -289,7 +289,7 @@ private File getLogTempFile(long startTime, long endTime, String diskType) {
     return Arrays.stream(new File("/tmp").listFiles())
         .filter(f -> f.isDirectory() && f.getName().startsWith("hudi-" + diskType) && f.lastModified() > startTime && f.lastModified() < endTime)
         .findFirst()
-        .orElse(new File(""));
+        .orElseGet(() -> new File(""));
   }
 
   @Test

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/utils/KafkaConnectUtils.java
Patch:
@@ -189,7 +189,7 @@ public static String getPartitionColumns(KeyGenerator keyGenerator, TypedPropert
     if (keyGenerator instanceof CustomAvroKeyGenerator) {
       return ((BaseKeyGenerator) keyGenerator).getPartitionPathFields().stream().map(
           pathField -> Arrays.stream(pathField.split(CustomAvroKeyGenerator.SPLIT_REGEX))
-              .findFirst().orElse("Illegal partition path field format: '$pathField' for ${c.getClass.getSimpleName}"))
+              .findFirst().orElseGet(() -> "Illegal partition path field format: '$pathField' for ${c.getClass.getSimpleName}"))
           .collect(Collectors.joining(","));
     }
 

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/commit/BaseDatasetBulkInsertCommitActionExecutor.java
Patch:
@@ -82,7 +82,7 @@ private HoodieWriteMetadata<JavaRDD<WriteStatus>> buildHoodieWriteMetadata(Optio
       hoodieWriteMetadata.setWriteStatuses(HoodieJavaRDD.getJavaRDD(statuses));
       hoodieWriteMetadata.setPartitionToReplaceFileIds(getPartitionToReplacedFileIds(statuses));
       return hoodieWriteMetadata;
-    }).orElse(new HoodieWriteMetadata<>());
+    }).orElseGet(HoodieWriteMetadata::new);
   }
 
   public final HoodieWriteResult execute(Dataset<Row> records, boolean isTablePartitioned) {

File: hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/cli/HDFSParquetImporterUtils.java
Patch:
@@ -277,7 +277,7 @@ public static SparkRDDWriteClient<HoodieRecordPayload> createHoodieClient(JavaSp
     HoodieCompactionConfig compactionConfig = compactionStrategyClass
         .map(strategy -> HoodieCompactionConfig.newBuilder().withInlineCompaction(false)
             .withCompactionStrategy(ReflectionUtils.loadClass(strategy)).build())
-        .orElse(HoodieCompactionConfig.newBuilder().withInlineCompaction(false).build());
+        .orElseGet(() -> HoodieCompactionConfig.newBuilder().withInlineCompaction(false).build());
     HoodieWriteConfig config =
         HoodieWriteConfig.newBuilder().withPath(basePath)
             .withParallelism(parallelism, parallelism)

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/FileSliceHandler.java
Patch:
@@ -31,8 +31,8 @@
 import org.apache.hadoop.fs.FileSystem;
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.stream.Collectors;
@@ -97,7 +97,7 @@ public List<FileSliceDTO> getLatestFileSlicesStateless(String basePath, String p
 
   public List<FileSliceDTO> getLatestFileSlice(String basePath, String partitionPath, String fileId) {
     return viewManager.getFileSystemView(basePath).getLatestFileSlice(partitionPath, fileId)
-        .map(FileSliceDTO::fromFileSlice).map(Arrays::asList).orElse(new ArrayList<>());
+        .map(FileSliceDTO::fromFileSlice).map(Arrays::asList).orElse(Collections.emptyList());
   }
 
   public List<CompactionOpDTO> getPendingCompactionOperations(String basePath) {

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/TimelineHandler.java
Patch:
@@ -27,8 +27,8 @@
 import org.apache.hadoop.fs.FileSystem;
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 
 /**
@@ -43,7 +43,7 @@ public TimelineHandler(Configuration conf, TimelineService.Config timelineServic
 
   public List<InstantDTO> getLastInstant(String basePath) {
     return viewManager.getFileSystemView(basePath).getLastInstant().map(InstantDTO::fromInstant)
-        .map(Arrays::asList).orElse(new ArrayList<>());
+        .map(Arrays::asList).orElse(Collections.emptyList());
   }
 
   public TimelineDTO getTimeline(String basePath) {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JsonDFSSource.java
Patch:
@@ -47,7 +47,7 @@ protected InputBatch<JavaRDD<String>> fetchNewData(Option<String> lastCkptStr, l
         pathSelector.getNextFilePathsAndMaxModificationTime(sparkContext, lastCkptStr, sourceLimit);
     return selPathsWithMaxModificationTime.getLeft()
         .map(pathStr -> new InputBatch<>(Option.of(fromFiles(pathStr)), selPathsWithMaxModificationTime.getRight()))
-        .orElse(new InputBatch<>(Option.empty(), selPathsWithMaxModificationTime.getRight()));
+        .orElseGet(() -> new InputBatch<>(Option.empty(), selPathsWithMaxModificationTime.getRight()));
   }
 
   private JavaRDD<String> fromFiles(String pathStr) {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/StreamSync.java
Patch:
@@ -615,7 +615,7 @@ private InputBatch fetchNextBatchFromSource(Option<String> resumeCheckpointStr,
             AvroConversionUtils.convertStructTypeToAvroSchema(df.schema(), getAvroRecordQualifiedName(cfg.targetTableName)));
 
         schemaProvider = incomingSchemaOpt.map(incomingSchema -> getDeducedSchemaProvider(incomingSchema, dataAndCheckpoint.getSchemaProvider(), metaClient))
-            .orElse(dataAndCheckpoint.getSchemaProvider());
+            .orElseGet(dataAndCheckpoint::getSchemaProvider);
 
         if (useRowWriter) {
           inputBatchForWriter = new InputBatch(transformed, checkpointStr, schemaProvider);
@@ -903,12 +903,12 @@ private WriteClientWriteResult writeToSink(InputBatch inputBatch, String instant
     instantTime = startCommit(instantTime, !autoGenerateRecordKeys);
 
     if (useRowWriter) {
-      Dataset<Row> df = (Dataset<Row>) inputBatch.getBatch().orElse(hoodieSparkContext.getSqlContext().emptyDataFrame());
+      Dataset<Row> df = (Dataset<Row>) inputBatch.getBatch().orElseGet(() -> hoodieSparkContext.getSqlContext().emptyDataFrame());
       HoodieWriteConfig hoodieWriteConfig = prepareHoodieConfigForRowWriter(inputBatch.getSchemaProvider().getTargetSchema());
       BaseDatasetBulkInsertCommitActionExecutor executor = new HoodieStreamerDatasetBulkInsertCommitActionExecutor(hoodieWriteConfig, writeClient, instantTime);
       writeClientWriteResult = new WriteClientWriteResult(executor.execute(df, !HoodieStreamerUtils.getPartitionColumns(props).isEmpty()).getWriteStatuses());
     } else {
-      JavaRDD<HoodieRecord> records = (JavaRDD<HoodieRecord>) inputBatch.getBatch().orElse(hoodieSparkContext.emptyRDD());
+      JavaRDD<HoodieRecord> records = (JavaRDD<HoodieRecord>) inputBatch.getBatch().orElseGet(() -> hoodieSparkContext.emptyRDD());
       // filter dupes if needed
       if (cfg.filterDupes) {
         records = DataSourceUtils.dropDuplicates(hoodieSparkContext.jsc(), records, writeClient.getConfig());

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -2291,7 +2291,9 @@ public void testCsvDFSSourceNoHeaderWithoutSchemaProviderAndWithTransformer() th
       testCsvDFSSource(false, '\t', false, Collections.singletonList(TripsWithDistanceTransformer.class.getName()));
     }, "Should error out when doing the transformation.");
     LOG.debug("Expected error during transformation", e);
-    assertTrue(e.getMessage().contains("cannot resolve 'begin_lat' given input columns:"));
+    // first version for Spark >= 3.3, the second one is for Spark < 3.3
+    assertTrue(e.getMessage().contains("Column 'begin_lat' does not exist. Did you mean one of the following?")
+        || e.getMessage().contains("cannot resolve 'begin_lat' given input columns:"));
   }
 
   @Test

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/util/ExecutorFactory.java
Patch:
@@ -48,8 +48,8 @@ public static <I, O, E> HoodieExecutor<E> create(HoodieWriteConfig config,
     ExecutorType executorType = config.getExecutorType();
     switch (executorType) {
       case BOUNDED_IN_MEMORY:
-        return new BoundedInMemoryExecutor<>(config.getWriteBufferLimitBytes(), inputItr, consumer,
-            transformFunction, preExecuteRunnable);
+        return new BoundedInMemoryExecutor<>(config.getWriteBufferLimitBytes(), config.getWriteBufferRecordSamplingRate(), config.getWriteBufferRecordCacheLimit(),
+            inputItr, consumer, transformFunction, preExecuteRunnable);
       case DISRUPTOR:
         return new DisruptorExecutor<>(config.getWriteExecutorDisruptorWriteBufferLimitBytes(), inputItr, consumer,
             transformFunction, config.getWriteExecutorDisruptorWaitStrategy(), preExecuteRunnable);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/ArchivalUtils.java
Patch:
@@ -20,7 +20,6 @@
 
 package org.apache.hudi.client.utils;
 
-import org.apache.hudi.client.timeline.HoodieTimelineArchiver;
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
@@ -51,7 +50,7 @@
  */
 public class ArchivalUtils {
 
-  private static final Logger LOG = LoggerFactory.getLogger(HoodieTimelineArchiver.class);
+  private static final Logger LOG = LoggerFactory.getLogger(ArchivalUtils.class);
 
   /**
    *  getMinAndMaxInstantsToKeep is used by archival service to find the

File: hudi-aws/src/main/java/org/apache/hudi/aws/transaction/lock/DynamoDBBasedLockProvider.java
Patch:
@@ -159,7 +159,7 @@ private DynamoDbClient getDynamoDBClient() {
             ? this.dynamoDBLockConfiguration.getString(DynamoDbBasedLockConfig.DYNAMODB_ENDPOINT_URL)
             : DynamoDbClient.serviceMetadata().endpointFor(Region.of(region)).toString();
 
-    if (!endpointURL.startsWith("https://") || !endpointURL.startsWith("http://")) {
+    if (!endpointURL.startsWith("https://") && !endpointURL.startsWith("http://")) {
       endpointURL = "https://" + endpointURL;
     }
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java
Patch:
@@ -86,7 +86,7 @@ private static Configuration addProjectionField(Configuration conf, String field
 
   public static void addProjectionField(Configuration conf, String[] fieldName) {
     if (fieldName.length > 0) {
-      List<String> columnNameList = Arrays.stream(conf.get(serdeConstants.LIST_COLUMNS).split(",")).collect(Collectors.toList());
+      List<String> columnNameList = Arrays.stream(conf.get(serdeConstants.LIST_COLUMNS, "").split(",")).collect(Collectors.toList());
       Arrays.stream(fieldName).forEach(field -> {
         int index = columnNameList.indexOf(field);
         if (index != -1) {

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -494,7 +494,7 @@ private static void rewriteRecordsToNewPartition(String basePath, String newPart
     StructType structType = recordsToRewrite.schema();
     int partitionIndex = structType.fieldIndex(partitionFieldProp);
 
-    recordsToRewrite.withColumn(metaClient.getTableConfig().getPartitionFieldProp(), functions.lit(null).cast(structType.apply(partitionIndex).dataType()))
+    recordsToRewrite.withColumn(metaClient.getTableConfig().getPartitionFieldProp(), functions.lit(newPartition).cast(structType.apply(partitionIndex).dataType()))
         .write()
         .options(propsMap)
         .option("hoodie.datasource.write.operation", WriteOperationType.BULK_INSERT.value())

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java
Patch:
@@ -424,7 +424,7 @@ public void testRenamePartition() throws IOException {
 
       // all records from old partition should have been migrated to new partition
       totalRecs = sqlContext.read().format("hudi").load(tablePath)
-          .filter(HoodieRecord.PARTITION_PATH_METADATA_FIELD + " == '" + "2016/03/18" + "'").count();
+          .filter(HoodieRecord.PARTITION_PATH_METADATA_FIELD + " == \"" + "2016/03/18" + "\"").count();
       assertEquals(totalRecs, totalRecsInOldPartition);
     }
   }

File: hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaUtils.java
Patch:
@@ -150,7 +150,7 @@ private static boolean isAtomicSchemasCompatibleEvolution(Schema newReaderSchema
    * Validate whether the {@code targetSchema} is a "compatible" projection of {@code sourceSchema}.
    * Only difference of this method from {@link #isStrictProjectionOf(Schema, Schema)} is
    * the fact that it allows some legitimate type promotions (like {@code int -> long},
-   * {@code decimal(3, 2) -> decimal(5, 2)}, etc) that allows projection to have a "wider"
+   * {@code decimal(3, 2) -> decimal(5, 2)}, etc.) that allows projection to have a "wider"
    * atomic type (whereas strict projection requires atomic type to be identical)
    */
   public static boolean isCompatibleProjectionOf(Schema sourceSchema, Schema targetSchema) {

File: hudi-common/src/main/java/org/apache/hudi/common/model/debezium/AbstractDebeziumAvroPayload.java
Patch:
@@ -72,7 +72,7 @@ public Option<IndexedRecord> combineAndGetUpdateValue(IndexedRecord currentValue
     if (shouldPickCurrentRecord(currentValue, insertValue.get(), schema)) {
       return Option.of(currentValue);
     }
-    // Step 2: Pick the insert record (as a delete record if its a deleted event)
+    // Step 2: Pick the insert record (as a delete record if it is a deleted event)
     return getInsertValue(schema);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -1270,7 +1270,7 @@ protected abstract Option<Pair<String, CompactionOperation>> getPendingLogCompac
   abstract Stream<Pair<String, CompactionOperation>> fetchPendingLogCompactionOperations();
 
   /**
-   * Check if there is an bootstrap base file present for this file.
+   * Check if there is a bootstrap base file present for this file.
    *
    * @param fgId File-Group Id
    * @return true if there is associated bootstrap base-file, false otherwise

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/convert/AvroInternalSchemaConverter.java
Patch:
@@ -160,7 +160,7 @@ public static Type buildTypeFromAvroSchema(Schema schema) {
    * @param schema a avro schema.
    * @param visited track the visit node when do traversal for avro schema; used to check if the name of avro record schema is correct.
    * @param firstVisitRoot track whether the current visited schema node is a root node.
-   * @param nextId a initial id which used to create id for all fields.
+   * @param nextId an initial id which used to create id for all fields.
    * @return a hudi type match avro schema.
    */
   private static Type visitAvroSchemaToBuildType(Schema schema, Deque<String> visited, Boolean firstVisitRoot, AtomicInteger nextId) {
@@ -275,7 +275,7 @@ private static Type visitAvroPrimitiveToBuildInternalType(Schema primitive) {
    *
    * @param type a hudi type.
    * @param recordName the record name
-   * @return a Avro schema match this type
+   * @return an Avro schema match this type
    */
   public static Schema buildAvroSchemaFromType(Type type, String recordName) {
     Map<Type, Schema> cache = new HashMap<>();
@@ -301,7 +301,7 @@ public static Schema buildAvroSchemaFromInternalSchema(InternalSchema schema, St
    * @param cache use to cache intermediate convert result to save cost.
    * @param recordName auto-generated record name used as a fallback, in case
    * {@link org.apache.hudi.internal.schema.Types.RecordType} doesn't bear original record-name
-   * @return a Avro schema match this type
+   * @return an Avro schema match this type
    */
   private static Schema visitInternalSchemaToBuildAvroSchema(Type type, Map<Type, Schema> cache, String recordName) {
     switch (type.typeId()) {

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/AvroSchemaEvolutionUtils.java
Patch:
@@ -77,7 +77,7 @@ public static InternalSchema reconcileSchema(Schema incomingSchema, InternalSche
 
     // Remove redundancy from diffFromEvolutionSchema.
     // for example, now we add a struct col in evolvedSchema, the struct col is " user struct<name:string, age:int> "
-    // when we do diff operation: user, user.name, user.age will appeared in the resultSet which is redundancy, user.name and user.age should be excluded.
+    // when we do diff operation: user, user.name, user.age will appear in the resultSet which is redundancy, user.name and user.age should be excluded.
     // deal with add operation
     TreeMap<Integer, String> finalAddAction = new TreeMap<>();
     for (int i = 0; i < diffFromEvolutionColumns.size(); i++) {

File: hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/AbstractColumnReader.java
Patch:
@@ -98,7 +98,7 @@ public abstract class AbstractColumnReader<V extends WritableColumnVector>
    * Input streams:
    * 1.Run length encoder to encode every data, so we have run length stream to get
    *  run length information.
-   * 2.Data maybe is real data, maybe is dictionary ids which need be decode to real
+   * 2.Data maybe is real data, maybe is dictionary ids which need be decoded to real
    *  data from Dictionary.
    *
    * Run length stream ------> Data stream

File: hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/AbstractColumnReader.java
Patch:
@@ -98,7 +98,7 @@ public abstract class AbstractColumnReader<V extends WritableColumnVector>
    * Input streams:
    * 1.Run length encoder to encode every data, so we have run length stream to get
    *  run length information.
-   * 2.Data maybe is real data, maybe is dictionary ids which need be decode to real
+   * 2.Data maybe is real data, maybe is dictionary ids which need be decoded to real
    *  data from Dictionary.
    *
    * Run length stream ------> Data stream

File: hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/AbstractColumnReader.java
Patch:
@@ -98,7 +98,7 @@ public abstract class AbstractColumnReader<V extends WritableColumnVector>
    * Input streams:
    * 1.Run length encoder to encode every data, so we have run length stream to get
    *  run length information.
-   * 2.Data maybe is real data, maybe is dictionary ids which need be decode to real
+   * 2.Data maybe is real data, maybe is dictionary ids which need be decoded to real
    *  data from Dictionary.
    *
    * Run length stream ------> Data stream

File: hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/AbstractColumnReader.java
Patch:
@@ -98,7 +98,7 @@ public abstract class AbstractColumnReader<V extends WritableColumnVector>
    * Input streams:
    * 1.Run length encoder to encode every data, so we have run length stream to get
    *  run length information.
-   * 2.Data maybe is real data, maybe is dictionary ids which need be decode to real
+   * 2.Data maybe is real data, maybe is dictionary ids which need be decoded to real
    *  data from Dictionary.
    *
    * Run length stream ------> Data stream

File: hudi-flink-datasource/hudi-flink1.18.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/AbstractColumnReader.java
Patch:
@@ -98,7 +98,7 @@ public abstract class AbstractColumnReader<V extends WritableColumnVector>
    * Input streams:
    * 1.Run length encoder to encode every data, so we have run length stream to get
    *  run length information.
-   * 2.Data maybe is real data, maybe is dictionary ids which need be decode to real
+   * 2.Data maybe is real data, maybe is dictionary ids which need be decoded to real
    *  data from Dictionary.
    *
    * Run length stream ------> Data stream

File: hudi-sync/hudi-adb-sync/src/main/java/org/apache/hudi/sync/adb/AdbSyncTool.java
Patch:
@@ -189,7 +189,7 @@ private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat, b
     syncPartitions(tableName, writtenPartitionsSince);
 
     // Update sync commit time
-    // whether to skip syncing commit time stored in tbl properties, since it is time consuming.
+    // whether to skip syncing commit time stored in tbl properties, since it is time-consuming.
     if (!config.getBoolean(ADB_SYNC_SKIP_LAST_COMMIT_TIME_SYNC)) {
       syncClient.updateLastCommitTimeSynced(tableName);
     }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -60,7 +60,6 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
-import org.apache.hudi.exception.HoodieIncrementalPathNotFoundException;
 import org.apache.hudi.exception.TableNotFoundException;
 import org.apache.hudi.hive.HiveSyncConfig;
 import org.apache.hudi.hive.HoodieHiveSyncClient;
@@ -105,6 +104,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.RemoteIterator;
 import org.apache.kafka.common.errors.TopicExistsException;
+import org.apache.spark.SparkException;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.AnalysisException;
@@ -2390,7 +2390,7 @@ public void testHoodieIncrFallback() throws Exception {
 
     insertInTable(tableBasePath, 9, WriteOperationType.UPSERT);
     //No change as this fails with Path not exist error
-    assertThrows(HoodieIncrementalPathNotFoundException.class, () -> new HoodieDeltaStreamer(downstreamCfg, jsc).sync());
+    assertThrows(SparkException.class, () -> new HoodieDeltaStreamer(downstreamCfg, jsc).sync());
     assertRecordCount(1000, downstreamTableBasePath, sqlContext);
 
     if (downstreamCfg.configs == null) {

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java
Patch:
@@ -405,7 +405,7 @@ private List<Partition> getTablePartitions(String tableName, List<String> writte
    */
   private boolean syncAllPartitions(String tableName) {
     try {
-      if (config.getSplitStrings(META_SYNC_PARTITION_FIELDS).isEmpty()) {
+      if (config.shouldNotSyncPartitionMetadata() || config.getSplitStrings(META_SYNC_PARTITION_FIELDS).isEmpty()) {
         return false;
       }
 
@@ -431,7 +431,7 @@ private boolean syncAllPartitions(String tableName) {
    */
   private boolean syncPartitions(String tableName, List<String> writtenPartitionsSince, Set<String> droppedPartitions) {
     try {
-      if (writtenPartitionsSince.isEmpty() || config.getSplitStrings(META_SYNC_PARTITION_FIELDS).isEmpty()) {
+      if (config.shouldNotSyncPartitionMetadata() || writtenPartitionsSince.isEmpty() || config.getSplitStrings(META_SYNC_PARTITION_FIELDS).isEmpty()) {
         return false;
       }
 

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
Patch:
@@ -191,7 +191,7 @@ private static Iterable<Object[]> syncDataSourceTableParams() {
 
   @BeforeEach
   public void setUp() throws Exception {
-    HiveTestUtil.setUp();
+    HiveTestUtil.setUp(Option.empty(), true);
   }
 
   @AfterEach

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHiveIncrementalPuller.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.utilities;
 
 import org.apache.hudi.common.config.TypedProperties;
+import org.apache.hudi.common.util.Option;
 import org.apache.hudi.hive.HiveSyncConfig;
 import org.apache.hudi.hive.HiveSyncTool;
 import org.apache.hudi.hive.HoodieHiveSyncClient;
@@ -64,7 +65,7 @@ public static void cleanUpClass() throws Exception {
 
   @BeforeEach
   public void setUp() throws Exception {
-    HiveTestUtil.setUp();
+    HiveTestUtil.setUp(Option.empty(), true);
   }
 
   @AfterEach

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/commit/BaseDatasetBulkInsertCommitActionExecutor.java
Patch:
@@ -95,8 +95,7 @@ public final HoodieWriteResult execute(Dataset<Row> records, boolean isTablePart
     table = writeClient.initTable(getWriteOperationType(), Option.ofNullable(instantTime));
 
     BulkInsertPartitioner<Dataset<Row>> bulkInsertPartitionerRows = getPartitioner(populateMetaFields, isTablePartitioned);
-    boolean shouldDropPartitionColumns = writeConfig.getBoolean(DataSourceWriteOptions.DROP_PARTITION_COLUMNS());
-    Dataset<Row> hoodieDF = HoodieDatasetBulkInsertHelper.prepareForBulkInsert(records, writeConfig, bulkInsertPartitionerRows, shouldDropPartitionColumns, instantTime);
+    Dataset<Row> hoodieDF = HoodieDatasetBulkInsertHelper.prepareForBulkInsert(records, writeConfig, bulkInsertPartitionerRows, instantTime);
 
     preExecute();
     HoodieWriteMetadata<JavaRDD<WriteStatus>> result = buildHoodieWriteMetadata(doExecute(hoodieDF, bulkInsertPartitionerRows.arePartitionRecordsSorted()));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -1014,8 +1014,10 @@ private boolean shouldExecuteMetadataTableDeletion() {
     // Only execute metadata table deletion when all the following conditions are met
     // (1) This is data table
     // (2) Metadata table is disabled in HoodieWriteConfig for the writer
+    // (3) if mdt is already enabled.
     return !metaClient.isMetadataTable()
-        && !config.isMetadataTableEnabled();
+        && !config.isMetadataTableEnabled()
+        && !metaClient.getTableConfig().getMetadataPartitions().isEmpty();
   }
 
   /**

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/client/TestJavaHoodieBackedMetadata.java
Patch:
@@ -1417,7 +1417,7 @@ public void testColStatsPrefixLookup() throws IOException {
               .forEach(partitionWriteStat -> {
                 String partitionStatName = partitionWriteStat.getKey();
                 List<HoodieWriteStat> writeStats = partitionWriteStat.getValue();
-                String partition = HoodieTableMetadataUtil.getPartitionIdentifier(partitionStatName);
+                String partition = HoodieTableMetadataUtil.getColumnStatsIndexPartitionIdentifier(partitionStatName);
                 if (!commitToPartitionsToFiles.get(commitTime).containsKey(partition)) {
                   commitToPartitionsToFiles.get(commitTime).put(partition, new ArrayList<>());
                 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndexHelper.java
Patch:
@@ -38,6 +38,7 @@
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.io.HoodieKeyLookupResult;
+import org.apache.hudi.metadata.HoodieTableMetadataUtil;
 import org.apache.hudi.table.HoodieTable;
 
 import org.apache.hadoop.fs.FileStatus;
@@ -284,7 +285,7 @@ public int getPartition(Object key) {
       }
 
       String bloomIndexEncodedKey =
-          getBloomFilterIndexKey(new PartitionIndexID(partitionPath), new FileIndexID(baseFileName));
+          getBloomFilterIndexKey(new PartitionIndexID(HoodieTableMetadataUtil.getBloomFilterIndexPartitionIdentifier(partitionPath)), new FileIndexID(baseFileName));
 
       // NOTE: It's crucial that [[targetPartitions]] be congruent w/ the number of
       //       actual file-groups in the Bloom Index in MT

File: hudi-common/src/main/java/org/apache/hudi/metadata/BaseTableMetadata.java
Patch:
@@ -197,7 +197,7 @@ public Map<Pair<String, String>, BloomFilter> getBloomFilters(final List<Pair<St
     Map<String, Pair<String, String>> fileToKeyMap = new HashMap<>();
     partitionNameFileNameList.forEach(partitionNameFileNamePair -> {
       final String bloomFilterIndexKey = HoodieMetadataPayload.getBloomFilterIndexKey(
-          new PartitionIndexID(partitionNameFileNamePair.getLeft()), new FileIndexID(partitionNameFileNamePair.getRight()));
+          new PartitionIndexID(HoodieTableMetadataUtil.getBloomFilterIndexPartitionIdentifier(partitionNameFileNamePair.getLeft())), new FileIndexID(partitionNameFileNamePair.getRight()));
       partitionIDFileIDStrings.add(bloomFilterIndexKey);
       fileToKeyMap.put(bloomFilterIndexKey, partitionNameFileNamePair);
     });
@@ -245,7 +245,7 @@ public Map<Pair<String, String>, HoodieMetadataColumnStats> getColumnStats(final
     final ColumnIndexID columnIndexID = new ColumnIndexID(columnName);
     for (Pair<String, String> partitionNameFileNamePair : partitionNameFileNameList) {
       final String columnStatsIndexKey = HoodieMetadataPayload.getColumnStatsIndexKey(
-          new PartitionIndexID(partitionNameFileNamePair.getLeft()),
+          new PartitionIndexID(HoodieTableMetadataUtil.getColumnStatsIndexPartitionIdentifier(partitionNameFileNamePair.getLeft())),
           new FileIndexID(partitionNameFileNamePair.getRight()),
           columnIndexID);
       columnStatKeyset.add(columnStatsIndexKey);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/ZookeeperBasedLockProvider.java
Patch:
@@ -155,8 +155,6 @@ private void acquireLock(long time, TimeUnit unit) throws Exception {
   private void checkRequiredProps(final LockConfiguration config) {
     ValidationUtils.checkArgument(config.getConfig().getString(ZK_CONNECT_URL_PROP_KEY) != null);
     ValidationUtils.checkArgument(config.getConfig().getString(ZK_BASE_PATH_PROP_KEY) != null);
-    ValidationUtils.checkArgument(config.getConfig().getString(ZK_SESSION_TIMEOUT_MS_PROP_KEY) != null);
-    ValidationUtils.checkArgument(config.getConfig().getString(ZK_CONNECTION_TIMEOUT_MS_PROP_KEY) != null);
     ValidationUtils.checkArgument(config.getConfig().getString(ZK_LOCK_KEY_PROP_KEY) != null);
   }
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/prune/PartitionPruners.java
Patch:
@@ -94,7 +94,7 @@ private boolean evaluate(String partition) {
       Map<String, ColumnStats> partStats = new LinkedHashMap<>();
       for (int idx = 0; idx < partitionKeys.length; idx++) {
         String partKey = partitionKeys[idx];
-        Object partVal = partKey.equals(defaultParName)
+        Object partVal = partStrArray[idx].equals(defaultParName)
             ? null : DataTypeUtils.resolvePartition(partStrArray[idx], partitionTypes.get(idx));
         ColumnStats columnStats = new ColumnStats(partVal, partVal, partVal == null ? 1 : 0);
         partStats.put(partKey, columnStats);

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/utils/TestConfigurations.java
Patch:
@@ -110,12 +110,12 @@ private TestConfigurations() {
           DataTypes.FIELD("salary", DataTypes.DOUBLE()), // new field
           DataTypes.FIELD("ts", DataTypes.TIMESTAMP(6)),
           DataTypes.FIELD("f_struct", DataTypes.ROW(
-              DataTypes.FIELD("f0", DataTypes.INT()),
               DataTypes.FIELD("f2", DataTypes.INT()), // new field added in the middle of struct
               DataTypes.FIELD("f1", DataTypes.STRING()),
               DataTypes.FIELD("renamed_change_type", DataTypes.BIGINT()),
               DataTypes.FIELD("f3", DataTypes.STRING()),
-              DataTypes.FIELD("drop_add", DataTypes.STRING()))), // new field added at the end of struct
+              DataTypes.FIELD("drop_add", DataTypes.STRING()),
+              DataTypes.FIELD("f0", DataTypes.DECIMAL(20, 0)))),
           DataTypes.FIELD("f_map", DataTypes.MAP(DataTypes.STRING(), DataTypes.DOUBLE())),
           DataTypes.FIELD("f_array", DataTypes.ARRAY(DataTypes.DOUBLE())),
           DataTypes.FIELD("new_row_col", DataTypes.ROW(

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieCatalog.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.hudi.configuration.OptionsResolver;
 import org.apache.hudi.exception.HoodieMetadataException;
 import org.apache.hudi.exception.HoodieValidationException;
+import org.apache.hudi.keygen.NonpartitionedAvroKeyGenerator;
 import org.apache.hudi.util.AvroSchemaConverter;
 import org.apache.hudi.util.DataTypeUtils;
 import org.apache.hudi.util.FlinkWriteClients;
@@ -349,6 +350,8 @@ public void createTable(ObjectPath tablePath, CatalogBaseTable catalogTable, boo
       final String partitions = String.join(",", resolvedTable.getPartitionKeys());
       conf.setString(FlinkOptions.PARTITION_PATH_FIELD, partitions);
       options.put(TableOptionProperties.PARTITION_COLUMNS, partitions);
+    } else {
+      conf.setString(FlinkOptions.KEYGEN_CLASS_NAME.key(), NonpartitionedAvroKeyGenerator.class.getName());
     }
     conf.setString(FlinkOptions.TABLE_NAME, tablePath.getObjectName());
     try {

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteWriter.java
Patch:
@@ -76,9 +76,7 @@ public HoodieTestSuiteWriter(JavaSparkContext jsc, Properties props, HoodieTestS
     this.deltaStreamerWrapper = new HoodieDeltaStreamerWrapper(cfg, jsc);
     this.hoodieReadClient = new HoodieReadClient(context, cfg.targetBasePath);
     this.writeConfig = getHoodieClientConfig(cfg, props, schema);
-    if (!cfg.useDeltaStreamer) {
-      this.writeClient = new SparkRDDWriteClient(context, writeConfig);
-    }
+    this.writeClient = new SparkRDDWriteClient(context, writeConfig);
     this.cfg = cfg;
     this.configuration = jsc.hadoopConfiguration();
     this.sparkContext = jsc;

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/DummyActiveAction.java
Patch:
@@ -23,6 +23,8 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 
+import java.util.Collections;
+
 /**
  * Instant triple for testing.
  */
@@ -35,7 +37,7 @@ public class DummyActiveAction extends ActiveAction {
   public DummyActiveAction(HoodieInstant completed, byte[] commitMetadata) {
     super(new HoodieInstant(HoodieInstant.State.REQUESTED, completed.getAction(), completed.getTimestamp()),
         new HoodieInstant(HoodieInstant.State.INFLIGHT, completed.getAction(), completed.getTimestamp()),
-        completed);
+        Collections.singletonList(completed));
     this.commitMetadata = commitMetadata;
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/MetadataConversionUtils.java
Patch:
@@ -154,7 +154,7 @@ public static HoodieLSMTimelineInstant createLSMTimelineInstant(ActiveAction act
     lsmTimelineInstant.setVersion(LSMTimeline.LSM_TIMELINE_INSTANT_VERSION_1);
     switch (activeAction.getPendingAction()) {
       case HoodieTimeline.CLEAN_ACTION: {
-        lsmTimelineInstant.setPlan(ByteBuffer.wrap(activeAction.getCleanPlan(metaClient)));
+        activeAction.getCleanPlan(metaClient).ifPresent(plan -> lsmTimelineInstant.setPlan(ByteBuffer.wrap(plan)));
         break;
       }
       case HoodieTimeline.REPLACE_COMMIT_ACTION: {
@@ -168,11 +168,11 @@ public static HoodieLSMTimelineInstant createLSMTimelineInstant(ActiveAction act
         break;
       }
       case HoodieTimeline.COMPACTION_ACTION: {
-        lsmTimelineInstant.setPlan(ByteBuffer.wrap(activeAction.getCompactionPlan(metaClient)));
+        activeAction.getCompactionPlan(metaClient).ifPresent(plan -> lsmTimelineInstant.setPlan(ByteBuffer.wrap(plan)));
         break;
       }
       case HoodieTimeline.LOG_COMPACTION_ACTION: {
-        lsmTimelineInstant.setPlan(ByteBuffer.wrap(activeAction.getLogCompactionPlan(metaClient)));
+        activeAction.getLogCompactionPlan(metaClient).ifPresent(plan -> lsmTimelineInstant.setPlan(ByteBuffer.wrap(plan)));
         break;
       }
       default:

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/embedded/EmbeddedTimelineService.java
Patch:
@@ -182,7 +182,7 @@ private void startServer(TimelineServiceCreator timelineServiceCreator) throws I
     this.serviceConfig = timelineServiceConfBuilder.build();
 
     server = timelineServiceCreator.create(context, hadoopConf.newCopy(), serviceConfig,
-        FSUtils.getFs(writeConfig.getBasePath(), hadoopConf.newCopy()), createViewManager());
+        FSUtils.getFs(writeConfig.getBasePath(), hadoopConf.newCopy()), viewManager);
     serverPort = server.startService();
     LOG.info("Started embedded timeline server at " + hostAddr + ":" + serverPort);
   }

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/MarkerHandler.java
Patch:
@@ -126,8 +126,8 @@ public void stop() {
     if (dispatchingThreadFuture != null) {
       dispatchingThreadFuture.cancel(true);
     }
-    dispatchingExecutorService.shutdown();
-    batchingExecutorService.shutdown();
+    dispatchingExecutorService.shutdownNow();
+    batchingExecutorService.shutdownNow();
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java
Patch:
@@ -323,6 +323,7 @@ public static <R> HoodieData<HoodieRecord<R>> mergeForPartitionUpdatesIfNeeded(
           } else {
             // merged record has a different partition: issue a delete to the old partition and insert the merged record to the new partition
             HoodieRecord<R> deleteRecord = createDeleteRecord(config, existing.getKey());
+            deleteRecord.setIgnoreIndexUpdate(true);
             return Arrays.asList(tagRecord(deleteRecord, existing.getCurrentLocation()), merged).iterator();
           }
         });

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/StreamSync.java
Patch:
@@ -757,7 +757,8 @@ private HoodieWriteConfig prepareHoodieConfigForRowWriter(Schema writerSchema) {
     hoodieConfig.setValue(DataSourceWriteOptions.PAYLOAD_CLASS_NAME().key(), cfg.payloadClassName);
     hoodieConfig.setValue(HoodieWriteConfig.KEYGENERATOR_CLASS_NAME.key(), HoodieSparkKeyGeneratorFactory.getKeyGeneratorClassName(props));
     hoodieConfig.setValue("path", cfg.targetBasePath);
-    return HoodieSparkSqlWriter.getBulkInsertRowConfig(writerSchema, hoodieConfig, cfg.targetBasePath, cfg.targetTableName);
+    return HoodieSparkSqlWriter.getBulkInsertRowConfig(writerSchema != InputBatch.NULL_SCHEMA ? Option.of(writerSchema) : Option.empty(),
+        hoodieConfig, cfg.targetBasePath, cfg.targetTableName);
   }
 
   /**
@@ -899,7 +900,7 @@ private WriteClientWriteResult writeToSink(InputBatch inputBatch, String instant
     instantTime = startCommit(instantTime, !autoGenerateRecordKeys);
 
     if (useRowWriter) {
-      Dataset<Row> df = (Dataset<Row>) inputBatch.getBatch().orElse(hoodieSparkContext.emptyRDD());
+      Dataset<Row> df = (Dataset<Row>) inputBatch.getBatch().orElse(hoodieSparkContext.getSqlContext().emptyDataFrame());
       HoodieWriteConfig hoodieWriteConfig = prepareHoodieConfigForRowWriter(inputBatch.getSchemaProvider().getTargetSchema());
       BaseDatasetBulkInsertCommitActionExecutor executor = new HoodieStreamerDatasetBulkInsertCommitActionExecutor(hoodieWriteConfig, writeClient, instantTime);
       writeClientWriteResult = new WriteClientWriteResult(executor.execute(df, !HoodieStreamerUtils.getPartitionColumns(props).isEmpty()).getWriteStatuses());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -265,7 +265,8 @@ public boolean commitStats(String instantTime, HoodieData<WriteStatus> writeStat
       if (null == commitCallback) {
         commitCallback = HoodieCommitCallbackFactory.create(config);
       }
-      commitCallback.call(new HoodieWriteCommitCallbackMessage(instantTime, config.getTableName(), config.getBasePath(), stats));
+      commitCallback.call(new HoodieWriteCommitCallbackMessage(
+          instantTime, config.getTableName(), config.getBasePath(), stats, Option.of(commitActionType), extraMetadata));
     }
     return true;
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java
Patch:
@@ -35,7 +35,6 @@
 import org.apache.hudi.utilities.sources.helpers.QueryInfo;
 import org.apache.hudi.utilities.sources.helpers.QueryRunner;
 
-import org.apache.parquet.Strings;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Encoders;
@@ -141,7 +140,7 @@ public S3EventsHoodieIncrSource(
 
     // This is to ensure backward compatibility where we were using the
     // config SOURCE_FILE_FORMAT for file format in previous versions.
-    this.fileFormat = Strings.isNullOrEmpty(getStringWithAltKeys(props, DATAFILE_FORMAT, EMPTY_STRING))
+    this.fileFormat = StringUtils.isNullOrEmpty(getStringWithAltKeys(props, DATAFILE_FORMAT, EMPTY_STRING))
         ? getStringWithAltKeys(props, SOURCE_FILE_FORMAT, true)
         : getStringWithAltKeys(props, DATAFILE_FORMAT, EMPTY_STRING);
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/BaseErrorTableWriter.java
Patch:
@@ -29,6 +29,8 @@
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.sql.SparkSession;
 
+import java.io.Serializable;
+
 /**
  * The class which handles error events while processing write records. All the
  * records which have a processing/write failure are triggered as error events to
@@ -38,7 +40,7 @@
  *
  * The writer can use the configs defined in HoodieErrorTableConfig to manage the error table.
  */
-public abstract class BaseErrorTableWriter<T extends ErrorEvent> {
+public abstract class BaseErrorTableWriter<T extends ErrorEvent> implements Serializable {
 
   // The column name passed to Spark for option `columnNameOfCorruptRecord`. The record
   // is set to this column in case of an error

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/GcsEventsSource.java
Patch:
@@ -49,7 +49,7 @@
 import static org.apache.hudi.common.util.ConfigUtils.getStringWithAltKeys;
 import static org.apache.hudi.utilities.config.CloudSourceConfig.ACK_MESSAGES;
 import static org.apache.hudi.utilities.config.CloudSourceConfig.BATCH_SIZE_CONF;
-import static org.apache.hudi.utilities.config.CloudSourceConfig.MAX_FETCH_TIME_PER_SYNC_MS;
+import static org.apache.hudi.utilities.config.CloudSourceConfig.MAX_FETCH_TIME_PER_SYNC_SECS;
 import static org.apache.hudi.utilities.config.CloudSourceConfig.MAX_NUM_MESSAGES_PER_SYNC;
 import static org.apache.hudi.utilities.config.GCSEventsSourceConfig.GOOGLE_PROJECT_ID;
 import static org.apache.hudi.utilities.config.GCSEventsSourceConfig.PUBSUB_SUBSCRIPTION_ID;
@@ -121,7 +121,7 @@ public GcsEventsSource(TypedProperties props, JavaSparkContext jsc, SparkSession
                 getStringWithAltKeys(props, PUBSUB_SUBSCRIPTION_ID),
                 getIntWithAltKeys(props, BATCH_SIZE_CONF),
                 getIntWithAltKeys(props, MAX_NUM_MESSAGES_PER_SYNC),
-                getIntWithAltKeys(props, MAX_FETCH_TIME_PER_SYNC_MS))
+                getIntWithAltKeys(props, MAX_FETCH_TIME_PER_SYNC_SECS))
     );
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/BaseHoodieLogRecordReader.java
Patch:
@@ -260,7 +260,7 @@ private void scanInternalV1(Option<KeySpec> keySpecOpt) {
             && !HoodieTimeline.compareTimestamps(logBlock.getLogBlockHeader().get(INSTANT_TIME), HoodieTimeline.LESSER_THAN_OR_EQUALS, this.latestInstantTime
         )) {
           // hit a block with instant time greater than should be processed, stop processing further
-          break;
+          continue;
         }
         if (logBlock.getBlockType() != CORRUPT_BLOCK && logBlock.getBlockType() != COMMAND_BLOCK) {
           if (!completedInstantsTimeline.containsOrBeforeTimelineStarts(instantTime)

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java
Patch:
@@ -96,7 +96,7 @@ public interface SchemaConverter {
   public Schema parseSchemaFromRegistry(String registryUrl) {
     String schema = fetchSchemaFromRegistry(registryUrl);
     try {
-      String schemaConverter = getStringWithAltKeys(config, HoodieSchemaProviderConfig.SCHEMA_CONVERTER);
+      String schemaConverter = getStringWithAltKeys(config, HoodieSchemaProviderConfig.SCHEMA_CONVERTER, true);
       SchemaConverter converter = !StringUtils.isNullOrEmpty(schemaConverter)
           ? ReflectionUtils.loadClass(schemaConverter)
           : s -> s;

File: hudi-common/src/main/java/org/apache/hudi/BaseHoodieTableFileIndex.java
Patch:
@@ -428,6 +428,8 @@ private void doRefresh() {
 
     // Reset it to null to trigger re-loading of all partition path
     this.cachedAllPartitionPaths = null;
+    // Reset to force reload file slices inside partitions
+    this.cachedAllInputFileSlices = new HashMap<>();
     if (!shouldListLazily) {
       ensurePreloadedPartitions(getAllQueryPartitionPaths());
     }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/config/CloudSourceConfig.java
Patch:
@@ -108,7 +108,7 @@ public class CloudSourceConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> DATAFILE_FORMAT = ConfigProperty
       .key(STREAMER_CONFIG_PREFIX + "source.cloud.data.datafile.format")
-      .defaultValue("parquet")
+      .defaultValue(HoodieIncrSourceConfig.SOURCE_FILE_FORMAT.defaultValue())
       .withAlternatives(DELTA_STREAMER_CONFIG_PREFIX + "source.cloud.data.datafile.format")
       .markAdvanced()
       .withDocumentation("Format of the data file. By default, this will be the same as hoodie.streamer.source.hoodieincr.file.format");

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -379,16 +379,16 @@ public void testKafkaConnectCheckpointProvider() throws IOException {
 
   @Test
   public void testPropsWithInvalidKeyGenerator() {
-    Exception e = assertThrows(IllegalArgumentException.class, () -> {
+    Exception e = assertThrows(IOException.class, () -> {
       String tableBasePath = basePath + "/test_table_invalid_key_gen";
       HoodieDeltaStreamer deltaStreamer =
           new HoodieDeltaStreamer(TestHelpers.makeConfig(tableBasePath, WriteOperationType.BULK_INSERT,
               Collections.singletonList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_INVALID, false), jsc);
       deltaStreamer.sync();
     }, "Should error out when setting the key generator class property to an invalid value");
     // expected
-    LOG.debug("Expected error during getting the key generator", e);
-    assertTrue(e.getMessage().contains("No KeyGeneratorType found for class name"));
+    LOG.warn("Expected error during getting the key generator", e);
+    assertTrue(e.getMessage().contains("Could not load key generator class invalid"));
   }
 
   private static Stream<Arguments> provideInferKeyGenArgs() {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/CompactionUtil.java
Patch:
@@ -113,7 +113,7 @@ public static void setPreCombineField(Configuration conf, HoodieTableMetaClient
    * @param conf The configuration
    * @param metaClient The meta client
    */
-  public static void inferChangelogMode(Configuration conf, HoodieTableMetaClient metaClient) {
+  public static void inferChangelogMode(Configuration conf, HoodieTableMetaClient metaClient) throws Exception {
     TableSchemaResolver tableSchemaResolver = new TableSchemaResolver(metaClient);
     Schema tableAvroSchema = tableSchemaResolver.getTableAvroSchemaFromDataFile();
     if (tableAvroSchema.getField(HoodieRecord.OPERATION_METADATA_FIELD) != null) {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/ErrorTableUtils.java
Patch:
@@ -64,7 +64,7 @@ public static Option<BaseErrorTableWriter> getErrorTableWriter(HoodieStreamer.Co
 
   public static HoodieErrorTableConfig.ErrorWriteFailureStrategy getErrorWriteFailureStrategy(
       TypedProperties props) {
-    String writeFailureStrategy = props.getString(ERROR_TABLE_WRITE_FAILURE_STRATEGY.key());
+    String writeFailureStrategy = props.getString(ERROR_TABLE_WRITE_FAILURE_STRATEGY.key(), ERROR_TABLE_WRITE_FAILURE_STRATEGY.defaultValue());
     return HoodieErrorTableConfig.ErrorWriteFailureStrategy.valueOf(writeFailureStrategy);
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/StreamSync.java
Patch:
@@ -119,12 +119,11 @@
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
-import java.util.Set;
 import java.util.function.Function;
+import java.util.stream.Collectors;
 
 import scala.Tuple2;
 import scala.collection.JavaConversions;
@@ -982,7 +981,7 @@ private String getSyncClassShortName(String syncClassName) {
   }
 
   public void runMetaSync() {
-    Set<String> syncClientToolClasses = new HashSet<>(Arrays.asList(cfg.syncClientToolClassNames.split(",")));
+    List<String> syncClientToolClasses = Arrays.stream(cfg.syncClientToolClassNames.split(",")).distinct().collect(Collectors.toList());
     // for backward compatibility
     if (cfg.enableHiveSync) {
       cfg.enableMetaSync = true;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/HoodieStreamer.java
Patch:
@@ -747,6 +747,8 @@ protected Pair<CompletableFuture, ExecutorService> startService() {
           while (!isShutdownRequested()) {
             try {
               long start = System.currentTimeMillis();
+              // Send a heartbeat metrics event to track the active ingestion job for this table.
+              streamSync.getMetrics().updateStreamerHeartbeatTimestamp(start);
               // check if deltastreamer need to update the configuration before the sync
               if (configurationHotUpdateStrategyOpt.isPresent()) {
                 Option<TypedProperties> newProps = configurationHotUpdateStrategyOpt.get().updateProperties(props);

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/append/AppendWriteFunction.java
Patch:
@@ -112,7 +112,7 @@ protected void sendBootstrapEvent() {
     int attemptId = getRuntimeContext().getAttemptNumber();
     if (attemptId > 0) {
       // either a partial or global failover, reuses the current inflight instant
-      if (this.currentInstant != null) {
+      if (this.currentInstant != null && !metaClient.getActiveTimeline().filterCompletedInstants().containsInstant(currentInstant)) {
         LOG.info("Recover task[{}] for instant [{}] with attemptId [{}]", taskID, this.currentInstant, attemptId);
         this.currentInstant = null;
         return;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamerSchemaEvolutionBase.java
Patch:
@@ -150,7 +150,7 @@ protected HoodieDeltaStreamer.Config getDeltaStreamerConfig(String[] transformer
     extraProps.setProperty(HoodieReaderConfig.FILE_GROUP_READER_ENABLED.key(), "false");
     extraProps.setProperty("hoodie.datasource.write.table.type", tableType);
     extraProps.setProperty("hoodie.datasource.write.row.writer.enable", rowWriterEnable.toString());
-    extraProps.setProperty(DataSourceWriteOptions.SET_NULL_FOR_MISSING_COLUMNS().key(), Boolean.toString(nullForDeletedCols));
+    extraProps.setProperty(DataSourceWriteOptions.HANDLE_MISSING_COLUMNS_WITH_LOSSLESS_TYPE_PROMOTIONS().key(), Boolean.toString(nullForDeletedCols));
 
     //we set to 0 so that we create new base files on insert instead of adding inserts to existing filegroups via small file handling
     extraProps.setProperty("hoodie.parquet.small.file.limit", "0");

File: hudi-common/src/main/java/org/apache/hudi/common/table/cdc/HoodieCDCExtractor.java
Patch:
@@ -266,8 +266,7 @@ private HoodieCDCFileSplit parseWriteStat(
           );
           FileSlice beforeFileSlice = new FileSlice(fileGroupId, writeStat.getPrevCommit(), beforeBaseFile, Collections.emptyList());
           cdcFileSplit = new HoodieCDCFileSplit(instantTs, BASE_FILE_DELETE, new ArrayList<>(), Option.of(beforeFileSlice), Option.empty());
-        } else if (writeStat.getNumUpdateWrites() == 0L && writeStat.getNumDeletes() == 0
-            && writeStat.getNumWrites() == writeStat.getNumInserts()) {
+        } else if ((writeStat.getNumUpdateWrites() == 0L && writeStat.getNumWrites() == writeStat.getNumInserts())) {
           // all the records in this file are new.
           cdcFileSplit = new HoodieCDCFileSplit(instantTs, BASE_FILE_INSERT, path);
         } else {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -751,7 +751,7 @@ public class HoodieWriteConfig extends HoodieConfig {
 
   public static final ConfigProperty<Boolean> WRITE_RECORD_POSITIONS = ConfigProperty
       .key("hoodie.write.record.positions")
-      .defaultValue(false)
+      .defaultValue(true)
       .markAdvanced()
       .sinceVersion("1.0.0")
       .withDocumentation("Whether to write record positions to the block header for data blocks containing updates and delete blocks. "

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieReaderConfig.java
Patch:
@@ -54,7 +54,7 @@ public class HoodieReaderConfig extends HoodieConfig {
 
   public static final ConfigProperty<Boolean> FILE_GROUP_READER_ENABLED = ConfigProperty
       .key("hoodie.file.group.reader.enabled")
-      .defaultValue(false)
+      .defaultValue(true)
       .markAdvanced()
       .sinceVersion("1.0.0")
       .withDocumentation("Use engine agnostic file group reader if enabled");

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestHoodieIncrSource.java
Patch:
@@ -50,6 +50,7 @@
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
 import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.EnumSource;
 
@@ -214,6 +215,7 @@ public void testHoodieIncrSourceInflightCommitBeforeCompletedCommit(HoodieTableT
     }
   }
 
+  @Disabled("HUDI-7080")
   @ParameterizedTest
   @EnumSource(HoodieTableType.class)
   public void testHoodieIncrSourceWithPendingTableServices(HoodieTableType tableType) throws IOException {

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
+import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
 import org.apache.hudi.common.util.FileIOUtils;
@@ -359,7 +360,8 @@ public Option<Pair<HoodieInstant, HoodieCommitMetadata>> getLastCommitMetadataWi
     return Option.fromJavaOptional(
         getCommitMetadataStream()
             .filter(instantCommitMetadataPair ->
-                !StringUtils.isNullOrEmpty(instantCommitMetadataPair.getValue().getMetadata(HoodieCommitMetadata.SCHEMA_KEY)))
+                WriteOperationType.canUpdateSchema(instantCommitMetadataPair.getRight().getOperationType())
+                    && !StringUtils.isNullOrEmpty(instantCommitMetadataPair.getValue().getMetadata(HoodieCommitMetadata.SCHEMA_KEY)))
             .findFirst()
     );
   }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -1460,8 +1460,8 @@ private void testBulkInsertRowWriterContinuousMode(Boolean useSchemaProvider, Li
   @ParameterizedTest
   @EnumSource(value = HoodieRecordType.class, names = {"AVRO","SPARK"})
   public void testBulkInsertsAndUpsertsWithSQLBasedTransformerFor2StepPipeline(HoodieRecordType recordType) throws Exception {
-    String tableBasePath = basePath + "/test_table2";
-    String downstreamTableBasePath = basePath + "/test_downstream_table2";
+    String tableBasePath = basePath + "/" + recordType.toString() +  "/test_table2";
+    String downstreamTableBasePath = basePath + "/" + recordType.toString() + "/test_downstream_table2";
 
     // Initial bulk insert to ingest to first hudi table
     HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.BULK_INSERT,

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestTransformer.java
Patch:
@@ -80,6 +80,7 @@ public void testMultipleTransformersWithIdentifiers() throws Exception {
 
     assertRecordCount(parquetRecordsCount, tableBasePath, sqlContext);
     assertEquals(0, sqlContext.read().format("org.apache.hudi").load(tableBasePath).where("timestamp != 110").count());
+    testNum++;
   }
 
   /**

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/merge/SparkRecordMergingUtils.java
Patch:
@@ -115,6 +115,7 @@ public static Pair<HoodieRecord, Schema> mergePartialRecords(HoodieSparkRecord o
       InternalRow newPartialRow = newer.getData();
 
       Map<Integer, StructField> mergedIdToFieldMapping = mergedSchemaPair.getLeft();
+      Map<String, Integer> oldNameToIdMapping = getCachedFieldNameToIdMapping(oldSchema);
       Map<String, Integer> newPartialNameToIdMapping = getCachedFieldNameToIdMapping(newSchema);
       List<Object> values = new ArrayList<>(mergedIdToFieldMapping.size());
       for (int fieldId = 0; fieldId < mergedIdToFieldMapping.size(); fieldId++) {
@@ -125,7 +126,7 @@ public static Pair<HoodieRecord, Schema> mergePartialRecords(HoodieSparkRecord o
           values.add(newPartialRow.get(ordInPartialUpdate, structField.dataType()));
         } else {
           // The field does not exist in the newer record; picks the value from older record
-          values.add(oldRow.get(fieldId, structField.dataType()));
+          values.add(oldRow.get(oldNameToIdMapping.get(structField.name()), structField.dataType()));
         }
       }
       InternalRow mergedRow = new GenericInternalRow(values.toArray());

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java
Patch:
@@ -440,7 +440,7 @@ public boolean containsInstant(String ts) {
     // Check for older timestamp which have sec granularity and an extension of DEFAULT_MILLIS_EXT may have been added via Timeline operations
     if (ts.length() == HoodieInstantTimeGenerator.MILLIS_INSTANT_TIMESTAMP_FORMAT_LENGTH && ts.endsWith(HoodieInstantTimeGenerator.DEFAULT_MILLIS_EXT)) {
       final String actualOlderFormatTs = ts.substring(0, ts.length() - HoodieInstantTimeGenerator.DEFAULT_MILLIS_EXT.length());
-      return containsOrBeforeTimelineStarts(actualOlderFormatTs);
+      return containsInstant(actualOlderFormatTs);
     }
 
     return false;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/common/model/HoodieSparkRecord.java
Patch:
@@ -40,6 +40,7 @@
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;
 import org.apache.spark.sql.catalyst.expressions.JoinedRow;
+import org.apache.spark.sql.catalyst.expressions.SpecificInternalRow;
 import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;
 import org.apache.spark.sql.catalyst.expressions.UnsafeRow;
 import org.apache.spark.sql.types.DataType;
@@ -447,6 +448,7 @@ private static void validateRow(InternalRow data, StructType schema) {
         || schema != null && (
         data instanceof HoodieInternalRow
             || data instanceof GenericInternalRow
+            || data instanceof SpecificInternalRow
             || SparkAdapterSupport$.MODULE$.sparkAdapter().isColumnarBatchRow(data));
 
     ValidationUtils.checkState(isValid);

File: hudi-common/src/main/java/org/apache/hudi/common/table/read/HoodieBaseFileGroupRecordBuffer.java
Patch:
@@ -80,7 +80,7 @@ public HoodieBaseFileGroupRecordBuffer(HoodieReaderContext<T> readerContext,
   }
 
   @Override
-  public void setBaseFileIteraotr(ClosableIterator<T> baseFileIterator) {
+  public void setBaseFileIterator(ClosableIterator<T> baseFileIterator) {
     this.baseFileIterator = baseFileIterator;
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/read/HoodieFileGroupReader.java
Patch:
@@ -154,7 +154,7 @@ public void initRecordIterators() {
             baseFilePath.get().getHadoopPath(), start, length, readerState.baseFileAvroSchema, readerState.baseFileAvroSchema, hadoopConf)
         : new EmptyIterator<>();
     scanLogFiles();
-    recordBuffer.setBaseFileIteraotr(baseFileIterator);
+    recordBuffer.setBaseFileIterator(baseFileIterator);
   }
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/table/read/HoodieFileGroupRecordBuffer.java
Patch:
@@ -100,7 +100,7 @@ enum BufferType {
    *
    * @param baseFileIterator
    */
-  void setBaseFileIteraotr(ClosableIterator<T> baseFileIterator);
+  void setBaseFileIterator(ClosableIterator<T> baseFileIterator);
 
   /**
    * Check if next merged record exists.

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/streamer/StreamSync.java
Patch:
@@ -697,8 +697,9 @@ private Option<String> getCheckpointToResume(Option<HoodieTimeline> commitsTimel
     // try get checkpoint from commits(including commit and deltacommit)
     // in COW migrating to MOR case, the first batch of the deltastreamer will lost the checkpoint from COW table, cause the dataloss
     HoodieTimeline deltaCommitTimeline = commitsTimelineOpt.get().filter(instant -> instant.getAction().equals(HoodieTimeline.DELTA_COMMIT_ACTION));
-    // has deltacommit means this is a MOR table, we should get .deltacommit as before
-    if (!deltaCommitTimeline.empty()) {
+    // has deltacommit and this is a MOR table, then we should get checkpoint from .deltacommit
+    // if changing from mor to cow, before changing we must do a full compaction, so we can only consider .commit in such case
+    if (cfg.tableType.equals(HoodieTableType.MERGE_ON_READ.name()) && !deltaCommitTimeline.empty()) {
       commitsTimelineOpt = Option.of(deltaCommitTimeline);
     }
     Option<HoodieInstant> lastCommit = commitsTimelineOpt.get().lastInstant();

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieDeltaStreamerWrapper.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer;
 import org.apache.hudi.utilities.schema.SchemaProvider;
+import org.apache.hudi.utilities.sources.InputBatch;
 import org.apache.hudi.utilities.streamer.StreamSync;
 
 import org.apache.spark.api.java.JavaRDD;
@@ -80,7 +81,8 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> fetchSource() t
     StreamSync service = getDeltaSync();
     service.refreshTimeline();
     String instantTime = InProcessTimeGenerator.createNewInstantTime();
-    return service.readFromSource(instantTime);
+    InputBatch inputBatch = service.readFromSource(instantTime).getLeft();
+    return Pair.of(inputBatch.getSchemaProvider(), Pair.of(inputBatch.getCheckpointForNextBatch(), (JavaRDD<HoodieRecord>) inputBatch.getBatch().get()));
   }
 
   public StreamSync getDeltaSync() {

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerTestBase.java
Patch:
@@ -399,7 +399,7 @@ protected static void prepareORCDFSFiles(int numRecords, String baseORCPath, Str
     }
   }
 
-  static List<String> getAsyncServicesConfigs(int totalRecords, String autoClean, String inlineCluster,
+  static List<String> getTableServicesConfigs(int totalRecords, String autoClean, String inlineCluster,
                                               String inlineClusterMaxCommit, String asyncCluster, String asyncClusterMaxCommit) {
     List<String> configs = new ArrayList<>();
     configs.add(String.format("%s=%d", SourceTestConfig.MAX_UNIQUE_RECORDS_PROP.key(), totalRecords));
@@ -634,7 +634,7 @@ static void waitTillCondition(Function<Boolean, Boolean> condition, Future dsFut
         boolean ret = false;
         while (!ret && !dsFuture.isDone()) {
           try {
-            Thread.sleep(3000);
+            Thread.sleep(2000);
             ret = condition.apply(true);
           } catch (Throwable error) {
             LOG.warn("Got error :", error);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamerDAGExecution.java
Patch:
@@ -61,7 +61,7 @@ public void testClusteringDoesNotTriggerRepeatedDAG() throws Exception {
     // Configure 3 transformers of same type. 2nd transformer has no suffix
     StageListener stageListener = new StageListener("org.apache.hudi.table.action.commit.BaseCommitActionExecutor.executeClustering");
     sparkSession.sparkContext().addSparkListener(stageListener);
-    List<String> configs = getAsyncServicesConfigs(100, "false", "true", "1", "", "");
+    List<String> configs = getTableServicesConfigs(100, "false", "true", "1", "", "");
     runDeltaStreamer(WriteOperationType.UPSERT, false, Option.of(configs));
     assertEquals(1, stageListener.triggerCount);
   }

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -1738,7 +1738,7 @@ public static HoodieData<HoodieRecord> readRecordKeysFromBaseFiles(HoodieEngineC
       final String partition = partitionAndBaseFile.getKey();
       final HoodieBaseFile baseFile = partitionAndBaseFile.getValue();
       final String filename = baseFile.getFileName();
-      Path dataFilePath = new Path(basePath, partition + Path.SEPARATOR + filename);
+      Path dataFilePath = new Path(basePath, StringUtils.isNullOrEmpty(partition) ? filename : (partition + Path.SEPARATOR) + filename);
 
       final String fileId = baseFile.getFileId();
       final String instantTime = baseFile.getCommitTime();

File: hudi-aws/src/main/java/org/apache/hudi/config/DynamoDbBasedLockConfig.java
Patch:
@@ -127,7 +127,7 @@ public static DynamoDbBasedLockConfig.Builder newBuilder() {
 
   public static final ConfigProperty<Integer> LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY = ConfigProperty
       .key(LockConfiguration.LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY)
-      .defaultValue(LockConfiguration.DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS)
+      .defaultValue(LockConfiguration.DEFAULT_LOCK_ACQUIRE_WAIT_TIMEOUT_MS)
       .markAdvanced()
       .sinceVersion("0.10.0")
       .withDocumentation("Lock Acquire Wait Timeout in milliseconds");

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java
Patch:
@@ -36,6 +36,7 @@
 
 import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_LOCK_ACQUIRE_NUM_RETRIES;
 import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS;
+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_LOCK_ACQUIRE_WAIT_TIMEOUT_MS;
 import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;
 import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;
 import static org.apache.hudi.common.config.LockConfiguration.FILESYSTEM_LOCK_EXPIRE_PROP_KEY;
@@ -106,7 +107,7 @@ public class HoodieLockConfig extends HoodieConfig {
 
   public static final ConfigProperty<Integer> LOCK_ACQUIRE_WAIT_TIMEOUT_MS = ConfigProperty
       .key(LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY)
-      .defaultValue(60 * 1000)
+      .defaultValue(DEFAULT_LOCK_ACQUIRE_WAIT_TIMEOUT_MS)
       .markAdvanced()
       .sinceVersion("0.8.0")
       .withDocumentation("Timeout in ms, to wait on an individual lock acquire() call, at the lock provider.");

File: hudi-common/src/main/java/org/apache/hudi/client/transaction/lock/InProcessLockProvider.java
Patch:
@@ -61,7 +61,7 @@ public InProcessLockProvider(final LockConfiguration lockConfiguration, final Co
     ValidationUtils.checkArgument(basePath != null);
     lock = LOCK_INSTANCE_PER_BASEPATH.computeIfAbsent(basePath, (ignore) -> new ReentrantReadWriteLock());
     maxWaitTimeMillis = typedProperties.getLong(LockConfiguration.LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY,
-        LockConfiguration.DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS);
+        LockConfiguration.DEFAULT_LOCK_ACQUIRE_WAIT_TIMEOUT_MS);
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/common/config/LockConfiguration.java
Patch:
@@ -43,7 +43,7 @@ public class LockConfiguration implements Serializable {
   public static final String LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP_KEY = LOCK_PREFIX + "client.num_retries";
 
   public static final String LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY = LOCK_PREFIX + "wait_time_ms";
-  public static final String DEFAULT_LOCK_ACQUIRE_WAIT_TIMEOUT_MS = String.valueOf(60 * 1000);
+  public static final int DEFAULT_LOCK_ACQUIRE_WAIT_TIMEOUT_MS = 60 * 1000;
 
   // configs for file system based locks. NOTE: This only works for DFS with atomic create/delete operation
   public static final String FILESYSTEM_BASED_LOCK_PROPERTY_PREFIX = LOCK_PREFIX + "filesystem.";
@@ -100,8 +100,6 @@ public class LockConfiguration implements Serializable {
   /** @deprecated Use {@link #LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY} */
   @Deprecated
   public static final String LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP = LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY;
-  @Deprecated
-  public static final int DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS = 60 * 1000;
   /** @deprecated Use {@link #HIVE_DATABASE_NAME_PROP_KEY} */
   @Deprecated
   public static final String HIVE_DATABASE_NAME_PROP = HIVE_DATABASE_NAME_PROP_KEY;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimeGeneratorBase.java
Patch:
@@ -82,7 +82,7 @@ public TimeGeneratorBase(HoodieTimeGeneratorConfig config, SerializableConfigura
     maxRetries = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP_KEY,
         Integer.parseInt(DEFAULT_LOCK_ACQUIRE_NUM_RETRIES));
     lockAcquireWaitTimeInMs = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY,
-        Integer.parseInt(DEFAULT_LOCK_ACQUIRE_WAIT_TIMEOUT_MS));
+        DEFAULT_LOCK_ACQUIRE_WAIT_TIMEOUT_MS);
     maxWaitTimeInMs = lockConfiguration.getConfig().getLong(LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP_KEY,
         Long.parseLong(DEFAULT_LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS));
     lockRetryHelper = new RetryHelper<>(maxWaitTimeInMs, maxRetries, maxWaitTimeInMs,

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkSortAndSizeExecutionStrategy.java
Patch:
@@ -68,7 +68,7 @@ public HoodieData<WriteStatus> performClusteringWithRecordsAsRow(Dataset<Row> in
         .withBulkInsertParallelism(numOutputGroups)
         .withProps(getWriteConfig().getProps()).build();
 
-    newConfig.setValue(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE, String.valueOf(getWriteConfig().getClusteringMaxBytesInGroup()));
+    newConfig.setValue(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE, String.valueOf(getWriteConfig().getClusteringTargetFileMaxBytes()));
 
     BulkInsertPartitioner<Dataset<Row>> partitioner = getRowPartitioner(strategyParams, schema);
     Dataset<Row> repartitionedRecords = partitioner.repartitionRecords(inputRecords, numOutputGroups);
@@ -92,7 +92,7 @@ public HoodieData<WriteStatus> performClusteringWithRecordsRDD(final HoodieData<
         .withBulkInsertParallelism(numOutputGroups)
         .withProps(getWriteConfig().getProps()).build();
 
-    newConfig.setValue(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE, String.valueOf(getWriteConfig().getClusteringMaxBytesInGroup()));
+    newConfig.setValue(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE, String.valueOf(getWriteConfig().getClusteringTargetFileMaxBytes()));
 
     return (HoodieData<WriteStatus>) SparkBulkInsertHelper.newInstance().bulkInsert(inputRecords, instantTime, getHoodieTable(),
         newConfig, false, getRDDPartitioner(strategyParams, schema), true, numOutputGroups, new CreateHandleFactory(shouldPreserveHoodieMetadata));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -383,7 +383,7 @@ protected void writeTableMetadata(HoodieTable table, String instantTime, HoodieC
    */
   public void bootstrap(Option<Map<String, String>> extraMetadata) {
     // TODO : MULTIWRITER -> check if failed bootstrap files can be cleaned later
-    if (config.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {
+    if (config.getWriteConcurrencyMode().supportsMultiWriter()) {
       throw new HoodieException("Cannot bootstrap the table in multi-writer mode");
     }
     HoodieTable<T, I, K, O> table = initTable(WriteOperationType.UPSERT, Option.ofNullable(HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCleanConfig.java
Patch:
@@ -143,7 +143,7 @@ public class HoodieCleanConfig extends HoodieConfig {
       .withInferFunction(cfg -> {
         Option<String> writeConcurrencyModeOpt = Option.ofNullable(cfg.getString(HoodieWriteConfig.WRITE_CONCURRENCY_MODE));
         if (!writeConcurrencyModeOpt.isPresent()
-            || !writeConcurrencyModeOpt.get().equalsIgnoreCase(WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.name())) {
+            || !WriteConcurrencyMode.supportsMultiWriter(writeConcurrencyModeOpt.get())) {
           return Option.empty();
         }
         return Option.of(HoodieFailedWritesCleaningPolicy.LAZY.name());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/WriteMarkers.java
Patch:
@@ -81,8 +81,7 @@ public Option<Path> create(String partitionPath, String fileName, IOType type) {
    */
   public Option<Path> create(String partitionPath, String fileName, IOType type, HoodieWriteConfig writeConfig,
                              String fileId, HoodieActiveTimeline activeTimeline) {
-    if (writeConfig.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()
-        && writeConfig.isEarlyConflictDetectionEnable()) {
+    if (writeConfig.getWriteConcurrencyMode().isOptimisticConcurrencyControl() && writeConfig.isEarlyConflictDetectionEnable()) {
       HoodieTimeline pendingCompactionTimeline = activeTimeline.filterPendingCompactionTimeline();
       HoodieTimeline pendingReplaceTimeline = activeTimeline.filterPendingReplaceTimeline();
       // TODO If current is compact or clustering then create marker directly without early conflict detection.
@@ -122,7 +121,7 @@ public Option<Path> createIfNotExists(String partitionPath, String fileName, IOT
   public Option<Path> createIfNotExists(String partitionPath, String fileName, IOType type, HoodieWriteConfig writeConfig,
                              String fileId, HoodieActiveTimeline activeTimeline) {
     if (writeConfig.isEarlyConflictDetectionEnable()
-        && writeConfig.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {
+        && writeConfig.getWriteConcurrencyMode().isOptimisticConcurrencyControl()) {
       HoodieTimeline pendingCompactionTimeline = activeTimeline.filterPendingCompactionTimeline();
       HoodieTimeline pendingReplaceTimeline = activeTimeline.filterPendingReplaceTimeline();
       // TODO If current is compact or clustering then create marker directly without early conflict detection.

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/OptionsInference.java
Patch:
@@ -80,7 +80,7 @@ public static void setupSinkTasks(Configuration conf, int envTasks) {
    * @see ClientIds
    */
   public static void setupClientId(Configuration conf) {
-    if (OptionsResolver.isOptimisticConcurrencyControl(conf)) {
+    if (OptionsResolver.isMultiWriter(conf)) {
       // explicit client id always has higher priority
       if (!conf.contains(FlinkOptions.WRITE_CLIENT_ID)) {
         try (ClientIds clientIds = ClientIds.builder().conf(conf).build()) {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteOperatorCoordinator.java
Patch:
@@ -202,7 +202,7 @@ public void start() throws Exception {
       initHiveSync();
     }
     // start client id heartbeats for optimistic concurrency control
-    if (OptionsResolver.isOptimisticConcurrencyControl(conf)) {
+    if (OptionsResolver.isMultiWriter(conf)) {
       initClientIds(conf);
     }
   }

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/utils/TestWriteBase.java
Patch:
@@ -551,7 +551,7 @@ private void checkInstantState(HoodieInstant.State state, String instantStr) {
     protected String lastCompleteInstant() {
       // If using optimistic concurrency control, fetch last complete instant of current writer from ckp metadata
       // because there are multiple write clients commit to the timeline.
-      if (OptionsResolver.isOptimisticConcurrencyControl(conf)) {
+      if (OptionsResolver.isMultiWriter(conf)) {
         return this.ckpMetadata.lastCompleteInstant();
       } else {
         // fetch the instant from timeline.

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -304,7 +304,7 @@ public void deleteInstantFileIfExists(HoodieInstant instant) {
         if (result) {
           LOG.info("Removed instant " + instant);
         } else {
-          throw new HoodieIOException("Could not delete instant " + instant);
+          throw new HoodieIOException("Could not delete instant " + instant + " with path " + commitFilePath);
         }
       } else {
         LOG.warn("The commit " + commitFilePath + " to remove does not exist");
@@ -322,7 +322,7 @@ protected void deleteInstantFile(HoodieInstant instant) {
       if (result) {
         LOG.info("Removed instant " + instant);
       } else {
-        throw new HoodieIOException("Could not delete instant " + instant);
+        throw new HoodieIOException("Could not delete instant " + instant + " with path " + inFlightCommitFilePath);
       }
     } catch (IOException e) {
       throw new HoodieIOException("Could not remove inflight commit " + inFlightCommitFilePath, e);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/FilebasedSchemaProvider.java
Patch:
@@ -53,7 +53,7 @@ public FilebasedSchemaProvider(TypedProperties props, JavaSparkContext jssc) {
     super(props, jssc);
     checkRequiredConfigProperties(props, Collections.singletonList(FilebasedSchemaProviderConfig.SOURCE_SCHEMA_FILE));
     String sourceFile = getStringWithAltKeys(props, FilebasedSchemaProviderConfig.SOURCE_SCHEMA_FILE);
-    boolean shouldSanitize = SanitizationUtils.getShouldSanitize(props);
+    boolean shouldSanitize = SanitizationUtils.shouldSanitize(props);
     String invalidCharMask = SanitizationUtils.getInvalidCharMask(props);
     this.fs = FSUtils.getFs(sourceFile, jssc.hadoopConfiguration(), true);
     this.sourceSchema = readAvroSchemaFromFile(sourceFile, this.fs, shouldSanitize, invalidCharMask);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.client.transaction.ConcurrentOperation;
 import org.apache.hudi.client.transaction.ConflictResolutionStrategy;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
+import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -67,8 +68,8 @@ public static Option<HoodieCommitMetadata> resolveWriteConflictIfAny(
       Option<HoodieInstant> lastCompletedTxnOwnerInstant,
       boolean reloadActiveTimeline,
       Set<String> pendingInstants) throws HoodieWriteConflictException {
-    // Skip to resolve conflict if using non-blocking concurrency control
-    if (config.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl() && !config.isNonBlockingConcurrencyControl()) {
+    WriteOperationType operationType = thisCommitMetadata.map(HoodieCommitMetadata::getOperationType).orElse(null);
+    if (config.needResolveWriteConflict(operationType)) {
       // deal with pendingInstants
       Stream<HoodieInstant> completedInstantsDuringCurrentWriteOperation = getCompletedInstantsDuringCurrentWriteOperation(table.getMetaClient(), pendingInstants);
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -302,8 +302,8 @@ public void preWrite(String instantTime, WriteOperationType writeOperationType,
    * Refresh the last transaction metadata,
    * should be called before the Driver starts a new transaction.
    */
-  public void preTxn(HoodieTableMetaClient metaClient) {
-    if (txnManager.isLockRequired() && !config.isNonBlockingConcurrencyControl()) {
+  public void preTxn(WriteOperationType operationType, HoodieTableMetaClient metaClient) {
+    if (txnManager.isLockRequired() && config.needResolveWriteConflict(operationType)) {
       // refresh the meta client which is reused
       metaClient.reloadActiveTimeline();
       this.lastCompletedTxnAndMetadata = TransactionUtils.getLastCompletedTxnInstantAndMetadata(metaClient);

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteOperatorCoordinator.java
Patch:
@@ -387,7 +387,7 @@ private void addEventToBuffer(WriteMetadataEvent event) {
 
   private void startInstant() {
     // refresh the last txn metadata
-    this.writeClient.preTxn(this.metaClient);
+    this.writeClient.preTxn(tableState.operationType, this.metaClient);
     // put the assignment in front of metadata generation,
     // because the instant request from write task is asynchronous.
     this.instant = this.writeClient.startCommit(tableState.commitAction, this.metaClient);

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/utils/Pipelines.java
Patch:
@@ -124,10 +124,11 @@ public static DataStreamSink<Object> bulkInsert(Configuration conf, RowType rowT
       RowDataKeyGen keyGen = RowDataKeyGen.instance(conf, rowType);
       RowType rowTypeWithFileId = BucketBulkInsertWriterHelper.rowTypeWithFileId(rowType);
       InternalTypeInfo<RowData> typeInfo = InternalTypeInfo.of(rowTypeWithFileId);
+      boolean needFixedFileIdSuffix = OptionsResolver.isNonBlockingConcurrencyControl(conf);
 
       Map<String, String> bucketIdToFileId = new HashMap<>();
       dataStream = dataStream.partitionCustom(partitioner, keyGen::getHoodieKey)
-          .map(record -> BucketBulkInsertWriterHelper.rowWithFileId(bucketIdToFileId, keyGen, record, indexKeys, numBuckets), typeInfo)
+          .map(record -> BucketBulkInsertWriterHelper.rowWithFileId(bucketIdToFileId, keyGen, record, indexKeys, numBuckets, needFixedFileIdSuffix), typeInfo)
           .setParallelism(conf.getInteger(FlinkOptions.WRITE_TASKS)); // same parallelism as write task to avoid shuffle
       if (conf.getBoolean(FlinkOptions.WRITE_BULK_INSERT_SORT_INPUT)) {
         SortOperatorGen sortOperatorGen = BucketBulkInsertWriterHelper.getFileIdSorterGen(rowTypeWithFileId);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/SparkLazyInsertIterable.java
Patch:
@@ -34,8 +34,6 @@
 import java.util.Iterator;
 import java.util.List;
 
-import static org.apache.hudi.common.util.ValidationUtils.checkState;
-
 public class SparkLazyInsertIterable<T> extends HoodieLazyInsertIterable<T> {
 
   private final boolean useWriterSchema;
@@ -78,7 +76,6 @@ protected List<WriteStatus> computeNext() {
           getTransformer(schema, hoodieConfig), hoodieTable.getPreExecuteRunnable());
 
       final List<WriteStatus> result = bufferedIteratorExecutor.execute();
-      checkState(result != null && !result.isEmpty());
       return result;
     } catch (Exception e) {
       throw new HoodieException(e);

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/ClusteringCommand.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.cli.utils.InputStreamConsumer;
 import org.apache.hudi.cli.utils.SparkUtil;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.utilities.UtilHelpers;
 import org.apache.spark.launcher.SparkLauncher;
 import org.apache.spark.util.Utils;
@@ -59,7 +58,7 @@ public String scheduleClustering(
     SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
 
     // First get a clustering instant time and pass it to spark launcher for scheduling clustering
-    String clusteringInstantTime = HoodieActiveTimeline.createNewInstantTime();
+    String clusteringInstantTime = client.createNewInstantTime();
 
     sparkLauncher.addAppArgs(SparkCommand.CLUSTERING_SCHEDULE.toString(), master, sparkMemory,
         client.getBasePath(), client.getTableConfig().getTableName(), clusteringInstantTime, propsFilePath);

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java
Patch:
@@ -197,7 +197,7 @@ public String scheduleCompact(
     HoodieCLI.initFS(initialized);
 
     // First get a compaction instant time and pass it to spark launcher for scheduling compaction
-    String compactionInstantTime = HoodieActiveTimeline.createNewInstantTime();
+    String compactionInstantTime = client.createNewInstantTime();
 
     String sparkPropertiesPath =
         Utils.getDefaultPropertiesFile(scala.collection.JavaConversions.propertiesAsScalaMap(System.getProperties()));

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCompactionCommand.java
Patch:
@@ -159,7 +159,7 @@ private void generateCompactionInstances() throws IOException {
     HoodieActiveTimeline activeTimeline = HoodieCLI.getTableMetaClient().reloadActiveTimeline();
     // Create six commits
     Arrays.asList("001", "003", "005", "007").forEach(timestamp -> {
-      activeTimeline.transitionCompactionInflightToComplete(
+      activeTimeline.transitionCompactionInflightToComplete(true,
           new HoodieInstant(HoodieInstant.State.INFLIGHT, COMPACTION_ACTION, timestamp), Option.empty());
     });
     // Simulate a compaction commit in metadata table timeline

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/AsyncCleanerService.java
Patch:
@@ -20,7 +20,6 @@
 package org.apache.hudi.async;
 
 import org.apache.hudi.client.BaseHoodieWriteClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
@@ -49,7 +48,7 @@ protected AsyncCleanerService(BaseHoodieWriteClient writeClient) {
 
   @Override
   protected Pair<CompletableFuture, ExecutorService> startService() {
-    String instantTime = HoodieActiveTimeline.createNewInstantTime();
+    String instantTime = writeClient.createNewInstantTime();
     LOG.info(String.format("Starting async clean service with instant time %s...", instantTime));
     return Pair.of(CompletableFuture.supplyAsync(() -> {
       writeClient.clean(instantTime);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/FileSystemBasedLockProvider.java
Patch:
@@ -201,7 +201,7 @@ protected String generateLogStatement(LockState state) {
 
   private void checkRequiredProps(final LockConfiguration config) {
     ValidationUtils.checkArgument(config.getConfig().getString(FILESYSTEM_LOCK_PATH_PROP_KEY, null) != null
-          || config.getConfig().getString(HoodieWriteConfig.BASE_PATH.key(), null) != null);
+        || config.getConfig().getString(HoodieWriteConfig.BASE_PATH.key(), null) != null);
     ValidationUtils.checkArgument(config.getConfig().getInteger(FILESYSTEM_LOCK_EXPIRE_PROP_KEY) >= 0);
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java
Patch:
@@ -225,8 +225,8 @@ private HoodieCleanMetadata runClean(HoodieTable<T, I, K, O> table, HoodieInstan
         this.txnManager.beginTransaction(Option.of(inflightInstant), Option.empty());
       }
       writeTableMetadata(metadata, inflightInstant.getTimestamp());
-      table.getActiveTimeline().transitionCleanInflightToComplete(inflightInstant,
-          TimelineMetadataUtils.serializeCleanMetadata(metadata));
+      table.getActiveTimeline().transitionCleanInflightToComplete(false,
+          inflightInstant, TimelineMetadataUtils.serializeCleanMetadata(metadata));
       LOG.info("Marked clean started on " + inflightInstant.getTimestamp() + " as complete");
       return metadata;
     } catch (IOException e) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java
Patch:
@@ -223,7 +223,7 @@ protected void commit(HoodieData<WriteStatus> writeStatuses, HoodieWriteMetadata
       writeTableMetadata(metadata, writeStatuses, actionType);
       // cannot serialize maps with null values
       metadata.getExtraMetadata().entrySet().removeIf(entry -> entry.getValue() == null);
-      activeTimeline.saveAsComplete(
+      activeTimeline.saveAsComplete(false,
           new HoodieInstant(true, actionType, instantTime),
           serializeCommitMetadata(metadata));
       LOG.info("Committed " + instantTime);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java
Patch:
@@ -266,8 +266,7 @@ private void updateTableConfigAndTimeline(HoodieInstant indexInstant,
       txnManager.beginTransaction(Option.of(indexInstant), Option.empty());
       updateMetadataPartitionsTableConfig(table.getMetaClient(),
           finalIndexPartitionInfos.stream().map(HoodieIndexPartitionInfo::getMetadataPartitionPath).collect(Collectors.toSet()));
-      table.getActiveTimeline().saveAsComplete(
-          new HoodieInstant(true, INDEXING_ACTION, indexInstant.getTimestamp()),
+      table.getActiveTimeline().saveAsComplete(false, new HoodieInstant(true, INDEXING_ACTION, indexInstant.getTimestamp()),
           TimelineMetadataUtils.serializeIndexCommitMetadata(indexCommitMetadata));
     } finally {
       txnManager.endTransaction(Option.of(indexInstant));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/CopyOnWriteRestoreActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 
 import org.apache.hudi.avro.model.HoodieRollbackMetadata;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -46,7 +45,7 @@ protected HoodieRollbackMetadata rollbackInstant(HoodieInstant instantToRollback
       throw new HoodieRollbackException("Unsupported action in rollback instant:" + instantToRollback);
     }
     table.getMetaClient().reloadActiveTimeline();
-    String newInstantTime = HoodieActiveTimeline.createNewInstantTime();
+    String newInstantTime = table.getMetaClient().createNewInstantTime();
     table.scheduleRollback(context, newInstantTime, instantToRollback, false, false, true);
     table.getMetaClient().reloadActiveTimeline();
     CopyOnWriteRollbackActionExecutor rollbackActionExecutor = new CopyOnWriteRollbackActionExecutor(

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/MergeOnReadRestoreActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 
 import org.apache.hudi.avro.model.HoodieRollbackMetadata;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -50,7 +49,7 @@ protected HoodieRollbackMetadata rollbackInstant(HoodieInstant instantToRollback
         throw new IllegalArgumentException("invalid action name " + instantToRollback.getAction());
     }
     table.getMetaClient().reloadActiveTimeline();
-    String instantTime = HoodieActiveTimeline.createNewInstantTime();
+    String instantTime = table.getMetaClient().createNewInstantTime();
     table.scheduleRollback(context, instantTime, instantToRollback, false, false, true);
     table.getMetaClient().reloadActiveTimeline();
     MergeOnReadRollbackActionExecutor rollbackActionExecutor = new MergeOnReadRollbackActionExecutor(

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/HoodieTestCommitGenerator.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.collection.ImmutablePair;
 import org.apache.hudi.common.util.collection.Pair;
@@ -120,7 +121,7 @@ public static String getLogFilename(String instantTime, String fileId) {
   public static void createCommitAndDataFiles(
       String basePath, String instantTime,
       Map<String, List<Pair<String, String>>> partitionPathToFileIdAndNameMap) throws IOException {
-    String commitFilename = HoodieTimeline.makeCommitFileName(instantTime);
+    String commitFilename = HoodieTimeline.makeCommitFileName(instantTime + "_" + InProcessTimeGenerator.createNewInstantTime());
     HoodieCommitMetadata commitMetadata =
         generateCommitMetadata(partitionPathToFileIdAndNameMap, Collections.emptyMap());
     createCommitFileWithMetadata(basePath, new Configuration(), commitFilename, serializeCommitMetadata(commitMetadata).get());

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/client/utils/TestFileSliceMetricUtils.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieLogFile;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 
 import org.junit.jupiter.api.Test;
 
@@ -89,7 +89,7 @@ public void testFileSliceMetricUtilsWithLogFile() {
   private FileSlice buildFileSlice(long baseFileLen, List<Long> logFileLens) {
     final String baseFilePath = ".b5068208-e1a4-11e6-bf01-fe55135034f3_20170101134598.log.1";
     FileSlice slice = new FileSlice("partition_0",
-        HoodieActiveTimeline.createNewInstantTime(),
+        InProcessTimeGenerator.createNewInstantTime(),
         UUID.randomUUID().toString());
     HoodieBaseFile baseFile = new HoodieBaseFile(baseFilePath);
     baseFile.setFileLen(baseFileLen);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkTableServiceClient.java
Patch:
@@ -137,6 +137,7 @@ protected void completeClustering(
 
       LOG.info("Committing Clustering {} finished with result {}.", clusteringCommitTime, metadata);
       table.getActiveTimeline().transitionReplaceInflightToComplete(
+          false,
           HoodieTimeline.getReplaceCommitInflightInstant(clusteringCommitTime),
           serializeCommitMetadata(metadata));
     } catch (IOException e) {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkTable.java
Patch:
@@ -57,6 +57,7 @@ public static <T> HoodieFlinkTable<T> create(HoodieWriteConfig config, HoodieEng
         HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(config.getBasePath())
             .setLoadActiveTimelineOnLoad(true).setConsistencyGuardConfig(config.getConsistencyGuardConfig())
             .setLayoutVersion(Option.of(new TimelineLayoutVersion(config.getTimelineLayoutVersion())))
+            .setTimeGeneratorConfig(config.getTimeGeneratorConfig())
             .setFileSystemRetryConfig(config.getFileSystemRetryConfig()).build();
     return HoodieFlinkTable.create(config, context, metaClient);
   }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaTable.java
Patch:
@@ -54,7 +54,8 @@ public static <T> HoodieJavaTable<T> create(HoodieWriteConfig config, HoodieEngi
     HoodieTableMetaClient metaClient =
         HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(config.getBasePath())
             .setLoadActiveTimelineOnLoad(true).setConsistencyGuardConfig(config.getConsistencyGuardConfig())
-            .setLayoutVersion(Option.of(new TimelineLayoutVersion(config.getTimelineLayoutVersion()))).build();
+            .setLayoutVersion(Option.of(new TimelineLayoutVersion(config.getTimelineLayoutVersion())))
+            .setTimeGeneratorConfig(config.getTimeGeneratorConfig()).build();
     return HoodieJavaTable.create(config, context, metaClient);
   }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkTable.java
Patch:
@@ -55,6 +55,7 @@ public static <T> HoodieSparkTable<T> create(HoodieWriteConfig config, HoodieEng
         HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(config.getBasePath())
             .setLoadActiveTimelineOnLoad(true).setConsistencyGuardConfig(config.getConsistencyGuardConfig())
             .setLayoutVersion(Option.of(new TimelineLayoutVersion(config.getTimelineLayoutVersion())))
+            .setTimeGeneratorConfig(config.getTimeGeneratorConfig())
             .setFileSystemRetryConfig(config.getFileSystemRetryConfig())
             .setMetaserverConfig(config.getProps()).build();
     return HoodieSparkTable.create(config, context, metaClient);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java
Patch:
@@ -31,7 +31,6 @@
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.WriteOperationType;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.CommitUtils;
 import org.apache.hudi.common.util.Option;
@@ -143,7 +142,7 @@ private HoodieData<HoodieRecord<T>> clusteringHandleUpdate(HoodieData<HoodieReco
           .map(Map.Entry::getValue)
           .collect(Collectors.toSet());
       pendingClusteringInstantsToRollback.forEach(instant -> {
-        String commitTime = HoodieActiveTimeline.createNewInstantTime();
+        String commitTime = table.getMetaClient().createNewInstantTime();
         table.scheduleRollback(context, commitTime, instant, false, config.shouldRollbackUsingMarkers(), false);
         table.rollback(context, commitTime, instant, true, true);
       });

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestClientRollback.java
Patch:
@@ -192,14 +192,15 @@ public void testSavepointAndRollback(Boolean testFailedRestore, Boolean failedRe
       if (testFailedRestore) {
         //test to make sure that restore commit is reused when the restore fails and is re-ran
         HoodieInstant inst =  table.getActiveTimeline().getRestoreTimeline().getInstants().get(0);
-        String restoreFileName = table.getMetaClient().getBasePathV2().toString() + "/.hoodie/" +  inst.getFileName();
+        String restoreFileName = table.getMetaClient().getBasePathV2().toString() + "/.hoodie/" + inst.getFileName();
 
         //delete restore commit file
         assertTrue((new File(restoreFileName)).delete());
 
         if (!failedRestoreInflight) {
           //delete restore inflight file
-          assertTrue((new File(restoreFileName + ".inflight")).delete());
+          HoodieInstant inflightInst = new HoodieInstant(true, inst.getAction(), inst.getTimestamp());
+          assertTrue((new File(table.getMetaClient().getBasePathV2().toString() + "/.hoodie/" + inflightInst.getFileName())).delete());
         }
         try (SparkRDDWriteClient newClient = getHoodieWriteClient(cfg)) {
           //restore again

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.testutils.HoodieTestUtils;
 import org.apache.hudi.common.testutils.RawTripTestPayload;
@@ -98,7 +99,8 @@ private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IO
       return createHandle.close().get(0);
     }).collect();
 
-    final Path commitFile = new Path(config.getBasePath() + "/.hoodie/" + HoodieTimeline.makeCommitFileName("100"));
+    final Path commitFile = new Path(config.getBasePath() + "/.hoodie/"
+        + HoodieTimeline.makeCommitFileName("100" + "_" + InProcessTimeGenerator.createNewInstantTime()));
     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);
     return statuses.get(0);
   }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestConsistentBucketIndex.java
Patch:
@@ -26,7 +26,6 @@
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.table.view.FileSystemViewStorageType;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
@@ -200,12 +199,12 @@ public void testWriteData(boolean populateMetaFields, boolean partitioned) throw
   @MethodSource("configParams")
   public void testWriteDataWithCompaction(boolean populateMetaFields, boolean partitioned) throws Exception {
     setUp(populateMetaFields, partitioned);
-    writeData(HoodieActiveTimeline.createNewInstantTime(), 200, true);
+    writeData(writeClient.createNewInstantTime(), 200, true);
     config.setValue(INLINE_COMPACT_NUM_DELTA_COMMITS, "1");
     config.setValue(INLINE_COMPACT_TRIGGER_STRATEGY, CompactionTriggerStrategy.NUM_COMMITS.name());
     String compactionTime = (String) writeClient.scheduleCompaction(Option.empty()).get();
     Assertions.assertEquals(200, readRecordsNum(dataGen.getPartitionPaths(), populateMetaFields));
-    writeData(HoodieActiveTimeline.createNewInstantTime(), 200, true);
+    writeData(writeClient.createNewInstantTime(), 200, true);
     Assertions.assertEquals(400, readRecordsNum(dataGen.getPartitionPaths(), populateMetaFields));
     HoodieWriteMetadata<JavaRDD<WriteStatus>> compactionMetadata = writeClient.compact(compactionTime);
     writeClient.commitCompaction(compactionTime, compactionMetadata.getCommitMetadata().get(), Option.empty());

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestDataValidationCheckForLogCompactionActions.java
Patch:
@@ -29,7 +29,6 @@
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.table.view.FileSystemViewStorageType;
@@ -198,7 +197,7 @@ private void scheduleLogCompactionOnExperimentTable(TestTableContents experiment
   }
 
   private boolean writeOnMainTable(TestTableContents mainTable, int curr) throws IOException {
-    String commitTime = HoodieActiveTimeline.createNewInstantTime();
+    String commitTime = mainTable.client.createNewInstantTime();
     mainTable.client.startCommitWithTime(commitTime);
 
     int actionType = pickAWriteAction();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestRemoteFileSystemViewWithMetadataTable.java
Patch:
@@ -254,7 +254,7 @@ private SparkRDDWriteClient createWriteClient(String basePath, String tableName,
   }
 
   private void writeToTable(int round, SparkRDDWriteClient writeClient) throws IOException {
-    String instantTime = HoodieActiveTimeline.createNewInstantTime();
+    String instantTime = writeClient.createNewInstantTime();
     writeClient.startCommitWithTime(instantTime);
     List<HoodieRecord> records = round == 0
         ? dataGen.generateInserts(instantTime, 100)

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestBoundedInMemoryExecutorInSpark.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.execution;
 
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.queue.BoundedInMemoryExecutor;
@@ -49,7 +49,7 @@
 
 public class TestBoundedInMemoryExecutorInSpark extends HoodieSparkClientTestHarness {
 
-  private final String instantTime = HoodieActiveTimeline.createNewInstantTime();
+  private final String instantTime = InProcessTimeGenerator.createNewInstantTime();
 
   private final HoodieWriteConfig writeConfig = HoodieWriteConfig.newBuilder()
       .withExecutorType(ExecutorType.BOUNDED_IN_MEMORY.name())

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestBoundedInMemoryQueue.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.DefaultSizeEstimator;
 import org.apache.hudi.common.util.FileIOUtils;
@@ -65,7 +65,7 @@
 
 public class TestBoundedInMemoryQueue extends HoodieSparkClientTestHarness {
 
-  private final String instantTime = HoodieActiveTimeline.createNewInstantTime();
+  private final String instantTime = InProcessTimeGenerator.createNewInstantTime();
 
   private final HoodieWriteConfig writeConfig = HoodieWriteConfig.newBuilder()
       .withExecutorType(ExecutorType.BOUNDED_IN_MEMORY.name())

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestDisruptorExecutionInSpark.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.execution;
 
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.util.queue.DisruptorExecutor;
 import org.apache.hudi.common.util.queue.ExecutorType;
 import org.apache.hudi.common.util.queue.HoodieConsumer;
@@ -45,8 +45,7 @@
 
 public class TestDisruptorExecutionInSpark extends HoodieSparkClientTestHarness {
 
-  private final String instantTime = HoodieActiveTimeline.createNewInstantTime();
-
+  private final String instantTime = InProcessTimeGenerator.createNewInstantTime();
 
   private final HoodieWriteConfig writeConfig = HoodieWriteConfig.newBuilder()
       .withExecutorType(ExecutorType.DISRUPTOR.name())

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestDisruptorMessageQueue.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
@@ -64,7 +64,7 @@
 
 public class TestDisruptorMessageQueue extends HoodieSparkClientTestHarness {
 
-  private final String instantTime = HoodieActiveTimeline.createNewInstantTime();
+  private final String instantTime = InProcessTimeGenerator.createNewInstantTime();
 
   private final HoodieWriteConfig writeConfig = HoodieWriteConfig.newBuilder()
       .withExecutorType(ExecutorType.DISRUPTOR.name())

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestSimpleExecutionInSpark.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.queue.HoodieConsumer;
@@ -47,7 +47,7 @@
 
 public class  TestSimpleExecutionInSpark extends HoodieSparkClientTestHarness {
 
-  private final String instantTime = HoodieActiveTimeline.createNewInstantTime();
+  private final String instantTime = InProcessTimeGenerator.createNewInstantTime();
 
   @BeforeEach
   public void setUp() throws Exception {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/hbase/TestSparkHoodieHBaseIndex.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.testutils.HoodieTestUtils;
 import org.apache.hudi.common.util.Option;
@@ -186,7 +185,7 @@ public void testSimpleTagLocationAndUpdate(HoodieTableType tableType) throws Exc
       writeClient.commit(newCommitTime, writeStatues);
 
       // Create new commit time.
-      String secondCommitTime = HoodieActiveTimeline.createNewInstantTime();
+      String secondCommitTime = writeClient.createNewInstantTime();
       // Now tagLocation for these records, index should tag them correctly
       metaClient = HoodieTableMetaClient.reload(metaClient);
       hoodieTable = HoodieSparkTable.create(config, context, metaClient);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/cluster/ClusteringTestUtils.java
Patch:
@@ -26,7 +26,6 @@
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.table.view.FileSystemViewStorageType;
@@ -109,7 +108,7 @@ public static HoodieWriteConfig getClusteringConfig(String basePath, String sche
 
   public static String runClustering(SparkRDDWriteClient clusteringClient, boolean skipExecution, boolean shouldCommit) {
     // Schedule and execute clustering.
-    String clusteringCommitTime = HoodieActiveTimeline.createNewInstantTime();
+    String clusteringCommitTime = clusteringClient.createNewInstantTime();
     return runClusteringOnInstant(clusteringClient, skipExecution, shouldCommit, clusteringCommitTime);
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java
Patch:
@@ -30,7 +30,6 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -125,7 +124,7 @@ public void testCompactionOnCopyOnWriteFail() throws Exception {
     metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.COPY_ON_WRITE);
     try (SparkRDDWriteClient writeClient = getHoodieWriteClient(getConfig());) {
       HoodieTable table = HoodieSparkTable.create(getConfig(), context, metaClient);
-      String compactionInstantTime = HoodieActiveTimeline.createNewInstantTime();
+      String compactionInstantTime = writeClient.createNewInstantTime();
       assertThrows(HoodieNotSupportedException.class, () -> {
         table.scheduleCompaction(context, compactionInstantTime, Option.empty());
         table.compact(context, compactionInstantTime);
@@ -149,7 +148,7 @@ public void testCompactionEmpty() {
       JavaRDD<HoodieRecord> recordsRDD = jsc.parallelize(records, 1);
       writeClient.insert(recordsRDD, newCommitTime).collect();
 
-      String compactionInstantTime = HoodieActiveTimeline.createNewInstantTime();
+      String compactionInstantTime = writeClient.createNewInstantTime();
       Option<HoodieCompactionPlan> plan = table.scheduleCompaction(context, compactionInstantTime, Option.empty());
       assertFalse(plan.isPresent(), "If there is nothing to compact, result will be empty");
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieCleanerTestBase.java
Patch:
@@ -136,7 +136,7 @@ protected List<HoodieCleanStat> runCleaner(
             .setBasePath(HoodieTableMetadata.getMetadataTableBasePath(metaClient.getBasePath()))
             .setConf(metaClient.getHadoopConf())
             .build();
-        HoodieInstant deltaCommit = new HoodieInstant(HoodieInstant.State.COMPLETED, HoodieTimeline.DELTA_COMMIT_ACTION, cleanInstantTs);
+        HoodieInstant deltaCommit = new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, cleanInstantTs);
         metadataMetaClient.reloadActiveTimeline().revertToInflight(deltaCommit);
       }
 

File: hudi-common/src/main/java/org/apache/hudi/common/config/LockConfiguration.java
Patch:
@@ -35,13 +35,15 @@ public class LockConfiguration implements Serializable {
   public static final String LOCK_ACQUIRE_RETRY_MAX_WAIT_TIME_IN_MILLIS_PROP_KEY = LOCK_PREFIX + "max_wait_time_ms_between_retry";
 
   public static final String LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP_KEY = LOCK_PREFIX + "client.wait_time_ms_between_retry";
+  public static final String DEFAULT_LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS = String.valueOf(5000L);
 
   public static final String LOCK_ACQUIRE_NUM_RETRIES_PROP_KEY = LOCK_PREFIX + "num_retries";
   public static final String DEFAULT_LOCK_ACQUIRE_NUM_RETRIES = String.valueOf(15);
 
   public static final String LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP_KEY = LOCK_PREFIX + "client.num_retries";
 
   public static final String LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY = LOCK_PREFIX + "wait_time_ms";
+  public static final String DEFAULT_LOCK_ACQUIRE_WAIT_TIMEOUT_MS = String.valueOf(60 * 1000);
 
   // configs for file system based locks. NOTE: This only works for DFS with atomic create/delete operation
   public static final String FILESYSTEM_BASED_LOCK_PROPERTY_PREFIX = LOCK_PREFIX + "filesystem.";
@@ -87,8 +89,6 @@ public class LockConfiguration implements Serializable {
   /** @deprecated Use {@link #LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP_KEY} */
   @Deprecated
   public static final String LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP = LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP_KEY;
-  @Deprecated
-  public static final String DEFAULT_LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS = String.valueOf(10000L);
   /** @deprecated Use {@link #LOCK_ACQUIRE_NUM_RETRIES_PROP_KEY} */
   @Deprecated
   public static final String LOCK_ACQUIRE_NUM_RETRIES_PROP = LOCK_ACQUIRE_NUM_RETRIES_PROP_KEY;

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
+import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.util.collection.ImmutablePair;
@@ -187,7 +188,7 @@ public static String maskWithoutFileId(String instantTime, int taskPartitionId)
   }
 
   public static String getCommitFromCommitFile(String commitFileName) {
-    return commitFileName.split("\\.")[0];
+    return HoodieInstant.extractTimestamp(commitFileName);
   }
 
   public static String getCommitTime(String fullFileName) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/ActiveAction.java
Patch:
@@ -95,7 +95,7 @@ public String getInstantTime() {
   }
 
   public String getCompletionTime() {
-    return this.completed.getStateTransitionTime();
+    return this.completed.getCompletionTime();
   }
 
   public Option<byte[]> getCommitMetadata(HoodieTableMetaClient metaClient) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/CompletionTimeQueryView.java
Patch:
@@ -177,7 +177,7 @@ private void load() {
     // load active instants first.
     this.metaClient.getActiveTimeline()
         .filterCompletedInstants().getInstantsAsStream()
-        .forEach(instant -> setCompletionTime(instant.getTimestamp(), instant.getStateTransitionTime()));
+        .forEach(instant -> setCompletionTime(instant.getTimestamp(), instant.getCompletionTime()));
     // then load the archived instants.
     HoodieArchivedTimeline.loadInstants(metaClient,
         new HoodieArchivedTimeline.StartTsFilter(this.startInstant),

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/MetadataConversionUtils.java
Patch:
@@ -64,7 +64,7 @@ public static HoodieArchivedMetaEntry createMetaWrapper(HoodieInstant hoodieInst
     HoodieArchivedMetaEntry archivedMetaWrapper = new HoodieArchivedMetaEntry();
     archivedMetaWrapper.setCommitTime(hoodieInstant.getTimestamp());
     archivedMetaWrapper.setActionState(hoodieInstant.getState().name());
-    archivedMetaWrapper.setStateTransitionTime(hoodieInstant.getStateTransitionTime());
+    archivedMetaWrapper.setStateTransitionTime(hoodieInstant.getCompletionTime());
     switch (hoodieInstant.getAction()) {
       case HoodieTimeline.CLEAN_ACTION: {
         if (hoodieInstant.isCompleted()) {
@@ -185,7 +185,7 @@ public static HoodieArchivedMetaEntry createMetaWrapperForEmptyInstant(HoodieIns
     HoodieArchivedMetaEntry archivedMetaWrapper = new HoodieArchivedMetaEntry();
     archivedMetaWrapper.setCommitTime(hoodieInstant.getTimestamp());
     archivedMetaWrapper.setActionState(hoodieInstant.getState().name());
-    archivedMetaWrapper.setStateTransitionTime(hoodieInstant.getStateTransitionTime());
+    archivedMetaWrapper.setStateTransitionTime(hoodieInstant.getCompletionTime());
     switch (hoodieInstant.getAction()) {
       case HoodieTimeline.CLEAN_ACTION: {
         archivedMetaWrapper.setActionType(ActionType.clean.name());

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -972,7 +972,7 @@ public static HoodieTableFileSystemView getFileSystemView(HoodieTableMetaClient
     HoodieTimeline timeline = metaClient.getActiveTimeline();
     if (timeline.empty()) {
       final HoodieInstant instant = new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION,
-          HoodieActiveTimeline.createNewInstantTime());
+          metaClient.createNewInstantTime(false));
       timeline = new HoodieDefaultTimeline(Stream.of(instant), metaClient.getActiveTimeline()::getInstantDetails);
     }
     return new HoodieTableFileSystemView(metaClient, timeline);
@@ -1422,7 +1422,7 @@ public static String deleteMetadataTable(HoodieTableMetaClient dataMetaClient, H
     }
 
     if (backup) {
-      final Path metadataBackupPath = new Path(metadataTablePath.getParent(), ".metadata_" + HoodieActiveTimeline.createNewInstantTime());
+      final Path metadataBackupPath = new Path(metadataTablePath.getParent(), ".metadata_" + dataMetaClient.createNewInstantTime(false));
       LOG.info("Backing up metadata directory to " + metadataBackupPath + " before deletion");
       try {
         if (fs.rename(metadataTablePath, metadataBackupPath)) {
@@ -1479,7 +1479,7 @@ public static String deleteMetadataTablePartition(HoodieTableMetaClient dataMeta
 
     if (backup) {
       final Path metadataPartitionBackupPath = new Path(metadataTablePartitionPath.getParent().getParent(),
-          String.format(".metadata_%s_%s", partitionType.getPartitionPath(), HoodieActiveTimeline.createNewInstantTime()));
+          String.format(".metadata_%s_%s", partitionType.getPartitionPath(), dataMetaClient.createNewInstantTime(false)));
       LOG.info(String.format("Backing up MDT partition %s to %s before deletion", partitionType, metadataPartitionBackupPath));
       try {
         if (fs.rename(metadataTablePartitionPath, metadataPartitionBackupPath)) {

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodieFileGroup.java
Patch:
@@ -135,7 +135,7 @@ public void testGetBaseInstantTime() {
 
   private CompletionTimeQueryView getMockCompletionTimeQueryView(MockHoodieTimeline activeTimeline) {
     Map<String, String> completionTimeMap = activeTimeline.filterCompletedInstants().getInstantsAsStream()
-        .collect(Collectors.toMap(HoodieInstant::getTimestamp, HoodieInstant::getStateTransitionTime));
+        .collect(Collectors.toMap(HoodieInstant::getTimestamp, HoodieInstant::getCompletionTime));
     CompletionTimeQueryView queryView = mock(CompletionTimeQueryView.class);
     when(queryView.getCompletionTime(any(String.class), any(String.class)))
         .thenAnswer((InvocationOnMock invocationOnMock) -> {

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestHoodieTableFSViewWithClustering.java
Patch:
@@ -180,9 +180,9 @@ public void testReplaceFileIdIsExcludedInView() throws IOException {
     assertEquals(actualReplacedFileIds, allReplacedFileIds);
   }
 
-  private static void saveAsComplete(HoodieActiveTimeline timeline, HoodieInstant inflight, Option<byte[]> data) {
+  private void saveAsComplete(HoodieActiveTimeline timeline, HoodieInstant inflight, Option<byte[]> data) {
     if (inflight.getAction().equals(HoodieTimeline.COMPACTION_ACTION)) {
-      timeline.transitionCompactionInflightToComplete(inflight, data);
+      timeline.transitionCompactionInflightToComplete(true, inflight, data);
     } else {
       HoodieInstant requested = new HoodieInstant(HoodieInstant.State.REQUESTED, inflight.getAction(), inflight.getTimestamp());
       timeline.createNewInstant(requested);

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/MockHoodieTimeline.java
Patch:
@@ -35,7 +35,8 @@ public class MockHoodieTimeline extends HoodieActiveTimeline {
   public MockHoodieTimeline(Stream<String> completed, Stream<String> inflights) {
     super();
     this.setInstants(Stream
-        .concat(completed.map(s -> new HoodieInstant(false, HoodieTimeline.COMMIT_ACTION, s)),
+        .concat(completed.map(s -> new HoodieInstant(HoodieInstant.State.COMPLETED, HoodieTimeline.COMMIT_ACTION, s,
+                InProcessTimeGenerator.createNewInstantTime())),
             inflights.map(s -> new HoodieInstant(true, HoodieTimeline.COMMIT_ACTION, s)))
         .sorted(Comparator.comparing(HoodieInstant::getFileName)).collect(Collectors.toList()));
   }

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/SchemaTestUtil.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieIOException;
 
@@ -156,7 +155,7 @@ public List<String> genRandomUUID(int size, List<String> existingUUIDs) {
 
   public List<IndexedRecord> generateHoodieTestRecords(int from, int limit)
       throws IOException, URISyntaxException {
-    String instantTime = HoodieActiveTimeline.createNewInstantTime();
+    String instantTime = InProcessTimeGenerator.createNewInstantTime();
     List<String> recorKeyList = genRandomUUID(limit, Collections.emptyList());
     return generateHoodieTestRecords(from, recorKeyList, "0000/00/00", instantTime);
   }

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestCompactionUtils.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.table.timeline.versioning.compaction.CompactionPlanMigrator;
 import org.apache.hudi.common.testutils.CompactionTestUtils.DummyHoodieBaseFile;
 import org.apache.hudi.common.testutils.HoodieCommonTestHarness;
@@ -376,8 +377,8 @@ public MockHoodieActiveTimeline(
         Stream<Pair<String, String>> inflights) {
       super();
       this.setInstants(Stream.concat(
-          Stream.concat(completedDeltaCommits.map(s -> new HoodieInstant(false, DELTA_COMMIT_ACTION, s)),
-              completedCompactionCommits.map(s -> new HoodieInstant(false, COMMIT_ACTION, s))),
+          Stream.concat(completedDeltaCommits.map(s -> new HoodieInstant(HoodieInstant.State.COMPLETED, DELTA_COMMIT_ACTION, s, InProcessTimeGenerator.createNewInstantTime())),
+              completedCompactionCommits.map(s -> new HoodieInstant(HoodieInstant.State.COMPLETED, COMMIT_ACTION, s, InProcessTimeGenerator.createNewInstantTime()))),
               inflights.map(s -> new HoodieInstant(true, s.getRight(), s.getLeft())))
           .sorted(Comparator.comparing(HoodieInstant::getFileName)).collect(Collectors.toList()));
     }

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestRetryHelper.java
Patch:
@@ -37,7 +37,7 @@ public class TestRetryHelper {
   @Test
   public void testCheckIfExceptionInRetryList() throws Exception {
     // test default retry exceptions
-    RetryHelper retryHelper = new RetryHelper(INTERVAL_TIME, NUM, INTERVAL_TIME, null);
+    RetryHelper retryHelper = new RetryHelper(INTERVAL_TIME, NUM, INTERVAL_TIME, "");
     Method privateOne = retryHelper.getClass().getDeclaredMethod("checkIfExceptionInRetryList", Exception.class);
     privateOne.setAccessible(true);
     boolean retry = (boolean) privateOne.invoke(retryHelper, new IOException("test"));

File: hudi-common/src/test/java/org/apache/hudi/common/util/collection/TestBitCaskDiskMap.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.AvroBinaryTestPayload;
 import org.apache.hudi.common.testutils.HoodieCommonTestHarness;
 import org.apache.hudi.common.testutils.SchemaTestUtil;
@@ -140,7 +140,7 @@ public void testSimpleUpsert(boolean isCompressionEnabled) throws IOException, U
 
     // generate updates from inserts
     List<IndexedRecord> updatedRecords = SchemaTestUtil.updateHoodieTestRecords(recordKeys,
-        testUtil.generateHoodieTestRecords(0, 100), HoodieActiveTimeline.createNewInstantTime());
+        testUtil.generateHoodieTestRecords(0, 100), InProcessTimeGenerator.createNewInstantTime());
     String newCommitTime =
         ((GenericRecord) updatedRecords.get(0)).get(HoodieRecord.COMMIT_TIME_METADATA_FIELD).toString();
 

File: hudi-common/src/test/java/org/apache/hudi/common/util/collection/TestRocksDbDiskMap.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.HoodieCommonTestHarness;
 import org.apache.hudi.common.testutils.SchemaTestUtil;
 import org.apache.hudi.common.testutils.SpillableMapTestUtils;
@@ -137,7 +137,7 @@ public void testSimpleUpsert() throws IOException, URISyntaxException {
 
     // generate updates from inserts for first 50 keys / subset of keys
     List<IndexedRecord> updatedRecords = SchemaTestUtil.updateHoodieTestRecords(recordKeys.subList(0, 50),
-        testUtil.generateHoodieTestRecords(0, 50), HoodieActiveTimeline.createNewInstantTime());
+        testUtil.generateHoodieTestRecords(0, 50), InProcessTimeGenerator.createNewInstantTime());
     String newCommitTime =
         ((GenericRecord) updatedRecords.get(0)).get(HoodieRecord.COMMIT_TIME_METADATA_FIELD).toString();
 
@@ -205,4 +205,4 @@ private  List<String> setupMapWithRecords(RocksDbDiskMap rocksDBBasedMap, int nu
     assertTrue(rocksDBBasedMap.sizeOfFileOnDiskInBytes() > 0);
     return recordKeys;
   }
-}
\ No newline at end of file
+}

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/CleanFunction.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.sink;
 
 import org.apache.hudi.client.HoodieFlinkWriteClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.configuration.FlinkOptions;
 import org.apache.hudi.sink.utils.NonThrownExecutor;
 import org.apache.hudi.util.FlinkWriteClients;
@@ -62,7 +61,7 @@ public void open(Configuration parameters) throws Exception {
     super.open(parameters);
     this.writeClient = FlinkWriteClients.createWriteClient(conf, getRuntimeContext());
     this.executor = NonThrownExecutor.builder(LOG).waitForTasksFinish(true).build();
-    String instantTime = HoodieActiveTimeline.createNewInstantTime();
+    String instantTime = writeClient.createNewInstantTime();
     LOG.info(String.format("exec clean with instant time %s...", instantTime));
     executor.execute(() -> {
       this.isCleaning = true;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/ClusteringPlanSourceFunction.java
Patch:
@@ -40,8 +40,7 @@
  *
  * <ul>
  *   <li>If the timeline has no inflight instants,
- *   use {@link org.apache.hudi.common.table.timeline.HoodieActiveTimeline#createNewInstantTime()}
- *   as the instant time;</li>
+ *   use {@link org.apache.hudi.common.table.timeline.HoodieActiveTimeline#createNewInstantTime() as the instant time;</li>
  *   <li>If the timeline has inflight instants,
  *   use the median instant time between [last complete instant time, earliest inflight instant time]
  *   as the instant time.</li>

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/HoodieFlinkClusteringJob.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.avro.model.HoodieClusteringPlan;
 import org.apache.hudi.client.HoodieFlinkWriteClient;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.ClusteringUtils;
@@ -248,7 +247,7 @@ private void cluster() throws Exception {
         ClusteringUtil.validateClusteringScheduling(conf);
 
         String clusteringInstantTime = cfg.clusteringInstantTime != null ? cfg.clusteringInstantTime
-            : HoodieActiveTimeline.createNewInstantTime();
+            : writeClient.createNewInstantTime();
 
         LOG.info("Creating a clustering plan for instant [" + clusteringInstantTime + "]");
         boolean scheduled = writeClient.scheduleClusteringAtInstant(clusteringInstantTime, Option.empty());

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/compact/CompactionPlanSourceFunction.java
Patch:
@@ -43,8 +43,7 @@
  *
  * <ul>
  *   <li>If the timeline has no inflight instants,
- *   use {@link org.apache.hudi.common.table.timeline.HoodieActiveTimeline#createNewInstantTime()}
- *   as the instant time;</li>
+ *   use {@link org.apache.hudi.common.table.timeline.HoodieActiveTimeline#createNewInstantTime() as the instant time;</li>
  *   <li>If the timeline has inflight instants,
  *   use the median instant time between [last complete instant time, earliest inflight instant time]
  *   as the instant time.</li>

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/compact/HoodieFlinkCompactor.java
Patch:
@@ -221,7 +221,7 @@ private void compact() throws Exception {
 
       // checks the compaction plan and do compaction.
       if (cfg.schedule) {
-        Option<String> compactionInstantTimeOption = CompactionUtil.getCompactionInstantTime(metaClient);
+        Option<String> compactionInstantTimeOption = CompactionUtil.getCompactionInstantTime(table.getMetaClient());
         if (compactionInstantTimeOption.isPresent()) {
           boolean scheduled = writeClient.scheduleCompactionAtInstant(compactionInstantTimeOption.get(), Option.empty());
           if (!scheduled) {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/IncrementalInputSplits.java
Patch:
@@ -285,7 +285,7 @@ public Result inputSplits(
 
     // version number should be monotonically increasing
     // fetch the instant offset by completion time
-    String offsetToIssue = instants.stream().map(HoodieInstant::getStateTransitionTime).max(String::compareTo).orElse(endInstant);
+    String offsetToIssue = instants.stream().map(HoodieInstant::getCompletionTime).max(String::compareTo).orElse(endInstant);
 
     if (instantRange == null) {
       // reading from the earliest, scans the partitions and files directly.
@@ -377,12 +377,12 @@ private Result getHollowInputSplits(
     // while with smaller txn start time.
     List<HoodieInstant> instants = commitTimeline.getInstantsAsStream()
         .filter(s -> HoodieTimeline.compareTimestamps(s.getTimestamp(), LESSER_THAN, issuedInstant))
-        .filter(s -> HoodieTimeline.compareTimestamps(s.getStateTransitionTime(), GREATER_THAN, issuedOffset))
+        .filter(s -> HoodieTimeline.compareTimestamps(s.getCompletionTime(), GREATER_THAN, issuedOffset))
         .filter(s -> StreamerUtil.isWriteCommit(metaClient.getTableType(), s, commitTimeline)).collect(Collectors.toList());
     if (instants.isEmpty()) {
       return Result.EMPTY;
     }
-    String offsetToIssue = instants.stream().map(HoodieInstant::getStateTransitionTime).max(String::compareTo).orElse(issuedOffset);
+    String offsetToIssue = instants.stream().map(HoodieInstant::getCompletionTime).max(String::compareTo).orElse(issuedOffset);
     List<MergeOnReadInputSplit> inputSplits = instants.stream().map(instant -> {
       String instantTs = instant.getTimestamp();
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieCatalog.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.model.DefaultHoodieRecordPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.TableSchemaResolver;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -476,7 +475,8 @@ public void dropPartition(ObjectPath tablePath, CatalogPartitionSpec catalogPart
     // enable auto-commit though ~
     options.put(HoodieWriteConfig.AUTO_COMMIT_ENABLE.key(), "true");
     try (HoodieFlinkWriteClient<?> writeClient = createWriteClient(options, tablePathStr, tablePath)) {
-      writeClient.deletePartitions(Collections.singletonList(partitionPathStr), HoodieActiveTimeline.createNewInstantTime())
+      writeClient.deletePartitions(Collections.singletonList(partitionPathStr),
+              writeClient.createNewInstantTime())
           .forEach(writeStatus -> {
             if (writeStatus.hasErrors()) {
               throw new HoodieMetadataException(String.format("Failed to commit metadata table records at file id %s.", writeStatus.getFileId()));

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.util.ConfigUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
@@ -826,7 +825,7 @@ public void dropPartition(
       boolean hiveStylePartitioning = Boolean.parseBoolean(table.getOptions().get(FlinkOptions.HIVE_STYLE_PARTITIONING.key()));
       writeClient.deletePartitions(
           Collections.singletonList(HoodieCatalogUtil.inferPartitionPath(hiveStylePartitioning, partitionSpec)),
-              HoodieActiveTimeline.createNewInstantTime())
+              writeClient.createNewInstantTime())
           .forEach(writeStatus -> {
             if (writeStatus.hasErrors()) {
               throw new HoodieMetadataException(String.format("Failed to commit metadata table records at file id %s.", writeStatus.getFileId()));

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/CompactionUtil.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.TableSchemaResolver;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.Option;
@@ -89,7 +88,7 @@ public static Option<String> getCompactionInstantTime(HoodieTableMetaClient meta
       LOG.info("No instants to schedule the compaction plan");
       return Option.empty();
     } else {
-      return Option.of(HoodieActiveTimeline.createNewInstantTime());
+      return Option.of(metaClient.createNewInstantTime());
     }
   }
 
@@ -203,7 +202,7 @@ public static void rollbackEarliestCompaction(HoodieFlinkTable<?> table, Configu
             instant.getState() == HoodieInstant.State.INFLIGHT).firstInstant();
     if (earliestInflight.isPresent()) {
       HoodieInstant instant = earliestInflight.get();
-      String currentTime = HoodieActiveTimeline.createNewInstantTime();
+      String currentTime = table.getMetaClient().createNewInstantTime();
       int timeout = conf.getInteger(FlinkOptions.COMPACTION_TIMEOUT_SECONDS);
       if (StreamerUtil.instantTimeDiffSeconds(currentTime, instant.getTimestamp()) >= timeout) {
         LOG.info("Rollback the inflight compaction instant: " + instant + " for timeout(" + timeout + "s)");

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/bucket/ITTestBucketStreamWrite.java
Patch:
@@ -154,4 +154,4 @@ private static void doWrite(String path, boolean isCow) throws InterruptedExcept
     // wait for the asynchronous commit to finish
     TimeUnit.SECONDS.sleep(3);
   }
-}
\ No newline at end of file
+}

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/bulk/TestBulkInsertWriteHelper.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.sink.bulk;
 
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.table.HoodieFlinkTable;
 import org.apache.hudi.util.FlinkTables;
@@ -66,7 +65,7 @@ public void before() throws IOException {
   @Test
   void testWrite() throws Exception {
     HoodieFlinkTable<?> table = FlinkTables.createTable(conf);
-    String instant = HoodieActiveTimeline.createNewInstantTime();
+    String instant = table.getMetaClient().createNewInstantTime();
     RowType rowType = TestConfigurations.ROW_TYPE;
     BulkInsertWriterHelper writerHelper = new BulkInsertWriterHelper(conf, table, table.getConfig(), instant,
         1, 1, 0, rowType, false);

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.common.table.log.InstantRange;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.configuration.FlinkOptions;
-import org.apache.hudi.configuration.HadoopConfigurations;
 import org.apache.hudi.table.format.mor.MergeOnReadInputSplit;
 import org.apache.hudi.util.StreamerUtil;
 import org.apache.hudi.utils.TestConfigurations;
@@ -219,7 +218,7 @@ public void testConsumingHollowInstants() throws Exception {
     // re-create the metadata file for c2 and c3 so that they have greater completion time than c4.
     // the completion time sequence become: c1, c4, c2, c3,
     // we will test with the same consumption sequence.
-    HoodieTableMetaClient metaClient = StreamerUtil.createMetaClient(tempFile.getAbsolutePath(), HadoopConfigurations.getHadoopConf(conf));
+    HoodieTableMetaClient metaClient = StreamerUtil.createMetaClient(conf);
     List<HoodieInstant> oriInstants = metaClient.getCommitsTimeline().filterCompletedInstants().getInstants();
     assertThat(oriInstants.size(), is(4));
     List<HoodieCommitMetadata> metadataList = new ArrayList<>();

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/format/TestInputFormat.java
Patch:
@@ -559,7 +559,7 @@ void testReadHollowInstants(HoodieTableType tableType) throws Exception {
     // re-create the metadata file for c2 and c3 so that they have greater completion time than c4.
     // the completion time sequence become: c1, c4, c2, c3,
     // we will test with the same consumption sequence.
-    HoodieTableMetaClient metaClient = StreamerUtil.createMetaClient(tempFile.getAbsolutePath(), HadoopConfigurations.getHadoopConf(conf));
+    HoodieTableMetaClient metaClient = StreamerUtil.createMetaClient(conf);
     List<HoodieInstant> oriInstants = metaClient.getCommitsTimeline().filterCompletedInstants().getInstants();
     assertThat(oriInstants.size(), is(4));
     List<HoodieCommitMetadata> metadataList = new ArrayList<>();
@@ -608,7 +608,7 @@ void testReadHollowInstants(HoodieTableType tableType) throws Exception {
     TestData.assertRowDataEquals(result3, TestData.dataSetInsert(3, 4));
 
     // test c2 and c4, c2 completion time > c1, so it is not a hollow instant
-    IncrementalInputSplits.Result splits4 = incrementalInputSplits.inputSplits(metaClient, oriInstants.get(0).getTimestamp(), oriInstants.get(0).getStateTransitionTime(), false);
+    IncrementalInputSplits.Result splits4 = incrementalInputSplits.inputSplits(metaClient, oriInstants.get(0).getTimestamp(), oriInstants.get(0).getCompletionTime(), false);
     assertFalse(splits4.isEmpty());
     List<RowData> result4 = readData(inputFormat, splits4.getInputSplits().toArray(new MergeOnReadInputSplit[0]));
     TestData.assertRowDataEquals(result4, TestData.dataSetInsert(3, 4, 7, 8));
@@ -629,7 +629,7 @@ void testReadHollowInstants(HoodieTableType tableType) throws Exception {
 
     // test c2 and c4, c2 is recognized as a hollow instant
     // the (version_number, completion_time) pair is not consistent, just for test purpose
-    IncrementalInputSplits.Result splits7 = incrementalInputSplits.inputSplits(metaClient, oriInstants.get(2).getTimestamp(), oriInstants.get(3).getStateTransitionTime(), false);
+    IncrementalInputSplits.Result splits7 = incrementalInputSplits.inputSplits(metaClient, oriInstants.get(2).getTimestamp(), oriInstants.get(3).getCompletionTime(), false);
     assertFalse(splits7.isEmpty());
     List<RowData> result7 = readData(inputFormat, splits7.getInputSplits().toArray(new MergeOnReadInputSplit[0]));
     TestData.assertRowDataEquals(result7, TestData.dataSetInsert(3, 4, 7, 8));

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/utils/TestClusteringUtil.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.client.HoodieFlinkWriteClient;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
@@ -124,7 +123,7 @@ private String generateClusteringPlan() {
         HoodieClusteringStrategy.newBuilder().build(), Collections.emptyMap(), 1, false);
     HoodieRequestedReplaceMetadata metadata = new HoodieRequestedReplaceMetadata(WriteOperationType.CLUSTER.name(),
         plan, Collections.emptyMap(), 1);
-    String instantTime = HoodieActiveTimeline.createNewInstantTime();
+    String instantTime = table.getMetaClient().createNewInstantTime();
     HoodieInstant clusteringInstant =
         new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.REPLACE_COMMIT_ACTION, instantTime);
     try {

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/utils/TestCompactionUtil.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.client.HoodieFlinkWriteClient;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
@@ -174,7 +173,7 @@ void testInferMetadataConf(boolean metadataEnabled) throws Exception {
   private String generateCompactionPlan() {
     HoodieCompactionOperation operation = new HoodieCompactionOperation();
     HoodieCompactionPlan plan = new HoodieCompactionPlan(Collections.singletonList(operation), Collections.emptyMap(), 1, null, null);
-    String instantTime = HoodieActiveTimeline.createNewInstantTime();
+    String instantTime = table.getMetaClient().createNewInstantTime();
     HoodieInstant compactionInstant =
         new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, instantTime);
     try {

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestHoodieHFileInputFormat.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
 import org.apache.hudi.common.testutils.HoodieTestUtils;
 import org.apache.hudi.hadoop.testutils.InputFormatTestUtil;
@@ -339,7 +340,8 @@ private void createCommitFile(java.nio.file.Path basePath, String commitNumber,
     List<HoodieWriteStat> writeStats = HoodieTestUtils.generateFakeHoodieWriteStat(1);
     HoodieCommitMetadata commitMetadata = new HoodieCommitMetadata();
     writeStats.forEach(stat -> commitMetadata.addWriteStat(partitionPath, stat));
-    File file = basePath.resolve(".hoodie").resolve(commitNumber + ".commit").toFile();
+    File file = basePath.resolve(".hoodie")
+        .resolve(commitNumber + "_" + InProcessTimeGenerator.createNewInstantTime() + ".commit").toFile();
     file.createNewFile();
     FileOutputStream fileOutputStream = new FileOutputStream(file);
     fileOutputStream.write(serializeCommitMetadata(commitMetadata).get());

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestHoodieParquetInputFormat.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
 import org.apache.hudi.common.testutils.FileCreateUtils;
 import org.apache.hudi.common.testutils.HoodieTestUtils;
@@ -491,7 +492,8 @@ private void createCommitFile(java.nio.file.Path basePath, String commitNumber,
     List<HoodieWriteStat> writeStats = HoodieTestUtils.generateFakeHoodieWriteStat(1);
     HoodieCommitMetadata commitMetadata = new HoodieCommitMetadata();
     writeStats.forEach(stat -> commitMetadata.addWriteStat(partitionPath, stat));
-    File file = basePath.resolve(".hoodie").resolve(commitNumber + ".commit").toFile();
+    File file = basePath.resolve(".hoodie")
+        .resolve(commitNumber + "_" + InProcessTimeGenerator.createNewInstantTime() + ".commit").toFile();
     file.createNewFile();
     FileOutputStream fileOutputStream = new FileOutputStream(file);
     fileOutputStream.write(serializeCommitMetadata(commitMetadata).get());

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/testutils/InputFormatTestUtil.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.hudi.common.table.log.block.HoodieHFileDataBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
 import org.apache.hudi.common.table.log.block.HoodieParquetDataBlock;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.HoodieTestUtils;
 import org.apache.hudi.common.testutils.SchemaTestUtil;
 import org.apache.hudi.common.util.Option;
@@ -148,12 +149,12 @@ public static void simulateUpdates(File directory, String baseFileExtension, fin
 
   public static void commit(java.nio.file.Path basePath, String commitNumber) throws IOException {
     // create the commit
-    Files.createFile(basePath.resolve(Paths.get(".hoodie", commitNumber + ".commit")));
+    Files.createFile(basePath.resolve(Paths.get(".hoodie", commitNumber + "_" + InProcessTimeGenerator.createNewInstantTime() + ".commit")));
   }
 
   public static void deltaCommit(java.nio.file.Path basePath, String commitNumber) throws IOException {
     // create the commit
-    Files.createFile(basePath.resolve(Paths.get(".hoodie", commitNumber + ".deltacommit")));
+    Files.createFile(basePath.resolve(Paths.get(".hoodie", commitNumber + "_" + InProcessTimeGenerator.createNewInstantTime() + ".deltacommit")));
   }
 
   public static void setupIncremental(JobConf jobConf, String startCommit, int numberOfCommitsToPull) {

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieDeltaStreamerWrapper.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.WriteOperationType;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer;
 import org.apache.hudi.utilities.schema.SchemaProvider;
@@ -79,7 +79,7 @@ public JavaRDD<WriteStatus> compact() throws Exception {
   public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> fetchSource() throws Exception {
     StreamSync service = getDeltaSync();
     service.refreshTimeline();
-    String instantTime = HoodieActiveTimeline.createNewInstantTime();
+    String instantTime = InProcessTimeGenerator.createNewInstantTime();
     return service.readFromSource(instantTime);
   }
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieInlineTestSuiteWriter.java
Patch:
@@ -26,7 +26,6 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.data.HoodieJavaRDD;
@@ -90,7 +89,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> fetchSource() t
 
   public Option<String> startCommit() {
     if (cfg.useDeltaStreamer) {
-      return Option.of(HoodieActiveTimeline.createNewInstantTime());
+      return Option.of(writeClient.createNewInstantTime());
     } else {
       return Option.of(writeClient.startCommit());
     }

File: hudi-platform-service/hudi-metaserver/hudi-metaserver-server/src/main/java/org/apache/hudi/metaserver/store/RelationalDBBasedStorage.java
Patch:
@@ -32,6 +32,7 @@
 
 import java.io.Serializable;
 import java.util.Collections;
+import java.util.Date;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -126,7 +127,7 @@ public String createNewTimestamp(long tableId) throws MetaserverStorageException
     do {
       oldTimestamp = getLatestTimestamp(tableId);
       do {
-        newTimestamp = HoodieInstantTimeGenerator.createNewInstantTime(0L);
+        newTimestamp = HoodieInstantTimeGenerator.formatDate(new Date(System.currentTimeMillis()));
       } while (oldTimestamp != null && HoodieTimeline.compareTimestamps(newTimestamp,
           HoodieActiveTimeline.LESSER_THAN_OR_EQUALS, oldTimestamp));
       Map<String, Object> params = new HashMap<>();

File: hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java
Patch:
@@ -20,8 +20,8 @@
 import org.apache.hudi.HoodieDataSourceHelpers;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.hive.HiveSyncConfig;
 import org.apache.hudi.hive.MultiPartKeysValueExtractor;
@@ -162,7 +162,7 @@ private void insert(SparkSession spark) throws IOException {
     JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());
 
     // Generate some input..
-    String instantTime = HoodieActiveTimeline.createNewInstantTime();
+    String instantTime = InProcessTimeGenerator.createNewInstantTime();
     List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime/* ignore */, 100));
     List<String> records1 = recordsToStrings(recordsSoFar);
     Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestDataSkippingWithMORColstats.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieCompactionConfig;
@@ -345,7 +344,7 @@ private void scheduleCompaction() {
         .withKeyGenerator("org.apache.hudi.keygen.NonpartitionedKeyGenerator")
         .build();
     try (SparkRDDWriteClient client = getHoodieWriteClient(cfg)) {
-      client.scheduleCompactionAtInstant(HoodieActiveTimeline.createNewInstantTime(), Option.empty());
+      client.scheduleCompactionAtInstant(client.createNewInstantTime(), Option.empty());
     }
   }
 

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveSyncClient.java
Patch:
@@ -356,10 +356,10 @@ public void updateLastCommitTimeSynced(String tableName) {
     HoodieTimeline activeTimeline = getActiveTimeline();
     Option<String> lastCommitSynced = activeTimeline.lastInstant().map(HoodieInstant::getTimestamp);
     Option<String> lastCommitCompletionSynced = activeTimeline
-        .getInstantsOrderedByStateTransitionTime()
+        .getInstantsOrderedByCompletionTime()
         .skip(activeTimeline.countInstants() - 1)
         .findFirst()
-        .map(i -> Option.of(i.getStateTransitionTime()))
+        .map(i -> Option.of(i.getCompletionTime()))
         .orElse(Option.empty());
     if (lastCommitSynced.isPresent()) {
       try {

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestCluster.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.NetworkTestUtils;
 import org.apache.hudi.common.testutils.SchemaTestUtil;
 import org.apache.hudi.common.testutils.minicluster.HdfsTestService;
@@ -173,7 +174,7 @@ public void createCOWTable(String commitTime, int numberOfPartitions, String dbN
   private void createCommitFile(HoodieCommitMetadata commitMetadata, String commitTime, String basePath) throws IOException {
     byte[] bytes = serializeCommitMetadata(commitMetadata).get();
     Path fullPath = new Path(basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/"
-        + HoodieTimeline.makeCommitFileName(commitTime));
+        + HoodieTimeline.makeCommitFileName(commitTime + "_" + InProcessTimeGenerator.createNewInstantTime()));
     FSDataOutputStream fsout = dfsCluster.getFileSystem().create(fullPath, true);
     fsout.write(bytes);
     fsout.close();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java
Patch:
@@ -115,7 +115,7 @@ public static QueryInfo generateQueryInfo(JavaSparkContext jssc, String srcBaseP
     HoodieTimeline completedCommitTimeline = srcMetaClient.getCommitsAndCompactionTimeline().filterCompletedInstants();
     final HoodieTimeline activeCommitTimeline = handleHollowCommitIfNeeded(completedCommitTimeline, srcMetaClient, handlingMode);
     Function<HoodieInstant, String> timestampForLastInstant = instant -> handlingMode == HollowCommitHandling.USE_TRANSITION_TIME
-        ? instant.getStateTransitionTime() : instant.getTimestamp();
+        ? instant.getCompletionTime() : instant.getTimestamp();
     String beginInstantTime = beginInstant.orElseGet(() -> {
       if (missingCheckpointStrategy != null) {
         if (missingCheckpointStrategy == MissingCheckpointStrategy.READ_LATEST) {

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerTestBase.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
@@ -439,7 +438,7 @@ static void addCommitToTimeline(HoodieTableMetaClient metaClient, WriteOperation
     HoodieCommitMetadata commitMetadata = new HoodieCommitMetadata();
     commitMetadata.setOperationType(writeOperationType);
     extraMetadata.forEach((k, v) -> commitMetadata.getExtraMetadata().put(k, v));
-    String commitTime = HoodieActiveTimeline.createNewInstantTime();
+    String commitTime = metaClient.createNewInstantTime();
     metaClient.getActiveTimeline().createNewInstant(new HoodieInstant(HoodieInstant.State.REQUESTED, commitActiontype, commitTime));
     metaClient.getActiveTimeline().createNewInstant(new HoodieInstant(HoodieInstant.State.INFLIGHT, commitActiontype, commitTime));
     metaClient.getActiveTimeline().saveAsComplete(

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestHoodieIncrSource.java
Patch:
@@ -153,7 +153,7 @@ public void testHoodieIncrSourceInflightCommitBeforeCompletedCommit(HoodieTableT
       List<Pair<String, List<HoodieRecord>>> inserts = new ArrayList<>();
 
       for (int i = 0; i < 6; i++) {
-        inserts.add(writeRecords(writeClient, INSERT, null, HoodieActiveTimeline.createNewInstantTime()));
+        inserts.add(writeRecords(writeClient, INSERT, null, writeClient.createNewInstantTime()));
       }
 
       // Emulates a scenario where an inflight commit is before a completed commit
@@ -254,7 +254,7 @@ public void testHoodieIncrSourceWithPendingTableServices(HoodieTableType tableTy
       for (int i = 0; i < 6; i++) {
         WriteOperationType opType = i < 4 ? BULK_INSERT : UPSERT;
         List<HoodieRecord> recordsForUpdate = i < 4 ? null : dataBatches.get(3).getRight();
-        dataBatches.add(writeRecords(writeClient, opType, recordsForUpdate, HoodieActiveTimeline.createNewInstantTime()));
+        dataBatches.add(writeRecords(writeClient, opType, recordsForUpdate, writeClient.createNewInstantTime()));
         if (tableType == COPY_ON_WRITE) {
           if (i == 2) {
             writeClient.scheduleClustering(Option.empty());
@@ -268,7 +268,7 @@ public void testHoodieIncrSourceWithPendingTableServices(HoodieTableType tableTy
           }
         }
       }
-      dataBatches.add(writeRecords(writeClient, BULK_INSERT, null, HoodieActiveTimeline.createNewInstantTime()));
+      dataBatches.add(writeRecords(writeClient, BULK_INSERT, null, writeClient.createNewInstantTime()));
 
       String latestCommitTimestamp = dataBatches.get(dataBatches.size() - 1).getKey();
       // Pending clustering exists

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestJsonKafkaSource.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
+import org.apache.hudi.common.testutils.InProcessTimeGenerator;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.utilities.config.HoodieStreamerConfig;
@@ -257,7 +257,7 @@ public void testErrorEventsForDataInRowFormat() throws IOException {
     SourceFormatAdapter kafkaSource = new SourceFormatAdapter(jsonSource, errorTableWriter, Option.of(props));
     assertEquals(1000, kafkaSource.fetchNewDataInRowFormat(Option.empty(),Long.MAX_VALUE).getBatch().get().count());
     assertEquals(2,((JavaRDD)errorTableWriter.get().getErrorEvents(
-        HoodieActiveTimeline.createNewInstantTime(), Option.empty()).get()).count());
+        InProcessTimeGenerator.createNewInstantTime(), Option.empty()).get()).count());
   }
 
   @Test
@@ -289,7 +289,7 @@ public void testErrorEventsForDataInAvroFormat() throws IOException {
     InputBatch<JavaRDD<GenericRecord>> fetch1 = kafkaSource.fetchNewDataInAvroFormat(Option.empty(),Long.MAX_VALUE);
     assertEquals(1000,fetch1.getBatch().get().count());
     assertEquals(2, ((JavaRDD)errorTableWriter.get().getErrorEvents(
-        HoodieActiveTimeline.createNewInstantTime(), Option.empty()).get()).count());
+        InProcessTimeGenerator.createNewInstantTime(), Option.empty()).get()).count());
   }
 
   private BaseErrorTableWriter getAnonymousErrorTableWriter(TypedProperties props) {

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.config.HoodieCommonConfig;
 import org.apache.hudi.common.config.HoodieMemoryConfig;
 import org.apache.hudi.common.config.HoodieReaderConfig;
-import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieAvroIndexedRecord;
 import org.apache.hudi.common.model.HoodieAvroRecordMerger;
@@ -198,7 +197,7 @@ private Option<HoodieAvroIndexedRecord> mergeRecord(HoodieRecord<?> newRecord, A
     GenericRecord genericRecord = HiveAvroSerializer.rewriteRecordIgnoreResultCheck(oldRecord, getLogScannerReaderSchema());
     HoodieRecord record = new HoodieAvroIndexedRecord(genericRecord);
     Option<Pair<HoodieRecord, Schema>> mergeResult = HoodieAvroRecordMerger.INSTANCE.merge(record,
-        genericRecord.getSchema(), newRecord, getLogScannerReaderSchema(), new TypedProperties(payloadProps));
+        genericRecord.getSchema(), newRecord, getLogScannerReaderSchema(), payloadProps);
     return mergeResult.map(p -> (HoodieAvroIndexedRecord) p.getLeft());
   }
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaExecutionStrategy.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.table.HoodieTableConfig;
+import org.apache.hudi.common.table.log.HoodieFileSliceReader;
 import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
@@ -61,7 +62,6 @@
 import java.util.Properties;
 import java.util.stream.Collectors;
 
-import static org.apache.hudi.common.table.log.HoodieFileSliceReader.getFileSliceReader;
 import static org.apache.hudi.config.HoodieClusteringConfig.PLAN_STRATEGY_SORT_COLUMNS;
 
 /**
@@ -195,7 +195,7 @@ private List<HoodieRecord<T>> readRecordsForGroupWithLogs(List<ClusteringOperati
             ? Option.empty()
             : Option.of(HoodieFileReaderFactory.getReaderFactory(recordType).getFileReader(table.getHadoopConf(), new Path(clusteringOp.getDataFilePath())));
         HoodieTableConfig tableConfig = table.getMetaClient().getTableConfig();
-        Iterator<HoodieRecord<T>> fileSliceReader = getFileSliceReader(baseFileReader, scanner, readerSchema,
+        Iterator<HoodieRecord<T>> fileSliceReader = new HoodieFileSliceReader(baseFileReader, scanner, readerSchema, tableConfig.getPreCombineField(), writeConfig.getRecordMerger(),
             tableConfig.getProps(),
             tableConfig.populateMetaFields() ? Option.empty() : Option.of(Pair.of(tableConfig.getRecordKeyFieldProp(),
                 tableConfig.getPartitionFieldProp())));

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java
Patch:
@@ -35,6 +35,7 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.table.HoodieTableConfig;
+import org.apache.hudi.common.table.log.HoodieFileSliceReader;
 import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;
 import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.CustomizedThreadFactory;
@@ -91,7 +92,6 @@
 
 import static org.apache.hudi.client.utils.SparkPartitionUtils.getPartitionFieldVals;
 import static org.apache.hudi.common.config.HoodieCommonConfig.TIMESTAMP_AS_OF;
-import static org.apache.hudi.common.table.log.HoodieFileSliceReader.getFileSliceReader;
 import static org.apache.hudi.config.HoodieClusteringConfig.PLAN_STRATEGY_SORT_COLUMNS;
 
 /**
@@ -324,7 +324,7 @@ private HoodieData<HoodieRecord<T>> readRecordsForGroupWithLogs(JavaSparkContext
           Option<HoodieFileReader> baseFileReader = StringUtils.isNullOrEmpty(clusteringOp.getDataFilePath())
               ? Option.empty()
               : Option.of(getBaseOrBootstrapFileReader(hadoopConf, bootstrapBasePath, partitionFields, clusteringOp));
-          recordIterators.add(getFileSliceReader(baseFileReader, scanner, readerSchema,
+          recordIterators.add(new HoodieFileSliceReader(baseFileReader, scanner, readerSchema, tableConfig.getPreCombineField(), config.getRecordMerger(),
               tableConfig.getProps(),
               tableConfig.populateMetaFields() ? Option.empty() : Option.of(Pair.of(tableConfig.getRecordKeyFieldProp(),
                   tableConfig.getPartitionFieldProp()))));

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/ClusteringOperator.java
Patch:
@@ -282,7 +282,8 @@ private Iterator<RowData> readRecordsForGroupWithLogs(List<ClusteringOperation>
             .build();
 
         HoodieTableConfig tableConfig = table.getMetaClient().getTableConfig();
-        HoodieFileSliceReader<? extends IndexedRecord> hoodieFileSliceReader = HoodieFileSliceReader.getFileSliceReader(baseFileReader, scanner, readerSchema,
+        HoodieFileSliceReader<? extends IndexedRecord> hoodieFileSliceReader = new HoodieFileSliceReader(baseFileReader, scanner, readerSchema,
+            tableConfig.getPreCombineField(),writeConfig.getRecordMerger(),
             tableConfig.getProps(),
             tableConfig.populateMetaFields() ? Option.empty() : Option.of(Pair.of(tableConfig.getRecordKeyFieldProp(),
                 tableConfig.getPartitionFieldProp())));

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestHoodieSparkMergeOnReadTableClustering.java
Patch:
@@ -61,7 +61,7 @@ class TestHoodieSparkMergeOnReadTableClustering extends SparkClientFunctionalTes
   private static Stream<Arguments> testClustering() {
     // enableClusteringAsRow, doUpdates, populateMetaFields, preserveCommitMetadata
     return Stream.of(
-        Arguments.of(true, true, true),
+        Arguments.of(false, true, true),
         Arguments.of(true, true, false),
         Arguments.of(true, false, true),
         Arguments.of(true, false, false),

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableInsertUpdateDelete.java
Patch:
@@ -442,7 +442,7 @@ private HoodieDataBlock getLogBlock() throws IOException, URISyntaxException {
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
     List<HoodieRecord> hoodieRecords = records.stream().map(HoodieAvroIndexedRecord::new).collect(Collectors.toList());
-    return new HoodieAvroDataBlock(hoodieRecords, header, HoodieRecord.RECORD_KEY_METADATA_FIELD);
+    return new HoodieAvroDataBlock(hoodieRecords, false, header, HoodieRecord.RECORD_KEY_METADATA_FIELD);
   }
 
   private String generateFakeWriteToken(String correctWriteToken) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -91,7 +91,7 @@ public class HoodieTableConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> DATABASE_NAME = ConfigProperty
       .key("hoodie.database.name")
-      .noDefaultValue()
+      .noDefaultValue("Database name can't have default value as it's used to toggle Hive incremental query feature. See HUDI-2837")
       .withDocumentation("Database name that will be used for incremental query.If different databases have the same table name during incremental query, "
           + "we can set it to limit the table name under a specific database");
 

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/factory/TestHoodieSparkKeyGeneratorFactory.java
Patch:
@@ -72,6 +72,7 @@ public void testKeyGeneratorFactory() throws IOException {
 
     // set KeyGenerator type only
     props.put(KEYGENERATOR_TYPE.key(), KeyGeneratorType.CUSTOM.name());
+    props.put(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME.key(), "field:simple");
     KeyGenerator keyGenerator = HoodieSparkKeyGeneratorFactory.createKeyGenerator(props);
     assertEquals(CustomKeyGenerator.class.getName(), keyGenerator.getClass().getName());
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -1736,8 +1736,7 @@ public boolean isClusteringSortEnabled() {
   }
 
   public HoodieClusteringConfig.LayoutOptimizationStrategy getLayoutOptimizationStrategy() {
-    return HoodieClusteringConfig.LayoutOptimizationStrategy.fromValue(
-        getStringOrDefault(HoodieClusteringConfig.LAYOUT_OPTIMIZE_STRATEGY));
+    return HoodieClusteringConfig.resolveLayoutOptimizationStrategy(getStringOrDefault(HoodieClusteringConfig.LAYOUT_OPTIMIZE_STRATEGY));
   }
 
   public HoodieClusteringConfig.SpatialCurveCompositionStrategyType getLayoutOptimizationCurveBuildMethod() {

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/catalog/TestHoodieHiveCatalog.java
Patch:
@@ -211,11 +211,11 @@ public void testCreateExternalTable() throws TableAlreadyExistException, Databas
   @Test
   public void testCreateNonHoodieTable() throws TableAlreadyExistException, DatabaseNotExistException {
     CatalogTable table =
-        new CatalogTableImpl(schema, Collections.emptyMap(), "hudi table");
+        new CatalogTableImpl(schema, Collections.singletonMap(FactoryUtil.CONNECTOR.key(), "hudi-fake"), "hudi table");
     try {
       hoodieCatalog.createTable(tablePath, table, false);
     } catch (HoodieCatalogException e) {
-      assertEquals(String.format("The %s is not hoodie table", tablePath.getObjectName()), e.getMessage());
+      assertEquals("Unsupported connector identity hudi-fake, supported identity is hudi", e.getMessage());
     }
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/ClusteringPlanActionExecutor.java
Patch:
@@ -57,8 +57,7 @@ public ClusteringPlanActionExecutor(HoodieEngineContext context,
 
   protected Option<HoodieClusteringPlan> createClusteringPlan() {
     LOG.info("Checking if clustering needs to be run on " + config.getBasePath());
-    Option<HoodieInstant> lastClusteringInstant = table.getActiveTimeline()
-        .filter(s -> s.getAction().equalsIgnoreCase(HoodieTimeline.REPLACE_COMMIT_ACTION)).lastInstant();
+    Option<HoodieInstant> lastClusteringInstant = table.getActiveTimeline().getLastClusterCommit();
 
     int commitsSinceLastClustering = table.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()
         .findInstantsAfter(lastClusteringInstant.map(HoodieInstant::getTimestamp).orElse("0"), Integer.MAX_VALUE)

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
Patch:
@@ -67,7 +67,7 @@ public class HoodieCompactionConfig extends HoodieConfig {
       .key("hoodie.log.compaction.enable")
       .defaultValue("false")
       .markAdvanced()
-      .sinceVersion("0.14")
+      .sinceVersion("0.14.0")
       .withDocumentation("By enabling log compaction through this config, log compaction will also get enabled for the metadata table.");
 
   public static final ConfigProperty<String> INLINE_LOG_COMPACT = ConfigProperty

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -571,7 +571,8 @@ public class HoodieWriteConfig extends HoodieConfig {
   public static final ConfigProperty<Integer> NUM_RETRIES_ON_CONFLICT_FAILURES = ConfigProperty
       .key("hoodie.write.num.retries.on.conflict.failures")
       .defaultValue(0)
-      .sinceVersion("0.13.0")
+      .markAdvanced()
+      .sinceVersion("0.14.0")
       .withDocumentation("Maximum number of times to retry a batch on conflict failure.");
 
   public static final ConfigProperty<String> WRITE_SCHEMA_OVERRIDE = ConfigProperty

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java
Patch:
@@ -108,7 +108,7 @@ public MultipleSparkJobExecutionStrategy(HoodieTable table, HoodieEngineContext
   @Override
   public HoodieWriteMetadata<HoodieData<WriteStatus>> performClustering(final HoodieClusteringPlan clusteringPlan, final Schema schema, final String instantTime) {
     JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(getEngineContext());
-    boolean shouldPreserveMetadata = Option.ofNullable(clusteringPlan.getPreserveHoodieMetadata()).orElse(false);
+    boolean shouldPreserveMetadata = Option.ofNullable(clusteringPlan.getPreserveHoodieMetadata()).orElse(true);
     ExecutorService clusteringExecutorService = Executors.newFixedThreadPool(
         Math.min(clusteringPlan.getInputGroups().size(), writeConfig.getClusteringMaxParallelism()),
         new CustomizedThreadFactory("clustering-job-group", true));
@@ -117,7 +117,7 @@ public HoodieWriteMetadata<HoodieData<WriteStatus>> performClustering(final Hood
       Stream<HoodieData<WriteStatus>> writeStatusesStream = FutureUtils.allOf(
               clusteringPlan.getInputGroups().stream()
                   .map(inputGroup -> {
-                    if (getWriteConfig().getBooleanOrDefault("hoodie.datasource.write.row.writer.enable", false)) {
+                    if (getWriteConfig().getBooleanOrDefault("hoodie.datasource.write.row.writer.enable", true)) {
                       return runClusteringForGroupAsyncAsRow(inputGroup,
                           clusteringPlan.getStrategy().getStrategyParams(),
                           shouldPreserveMetadata,

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestMultiWriterWithPreferWriterIngestion.java
Patch:
@@ -201,6 +201,8 @@ public void testHoodieClientMultiWriterWithClustering(HoodieTableType tableType)
       setUpMORTestTable();
     }
     Properties properties = new Properties();
+    // Use RDD API to perform clustering (TODO: Fix row-writer API)
+    properties.put("hoodie.datasource.write.row.writer.enable", String.valueOf(false));
     properties.setProperty(FILESYSTEM_LOCK_PATH_PROP_KEY, basePath + "/.hoodie/.locks");
     HoodieWriteConfig cfg = getConfigBuilder()
         .withCleanConfig(HoodieCleanConfig.newBuilder().withFailedWritesCleaningPolicy(HoodieFailedWritesCleaningPolicy.LAZY)

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/offlinejob/TestHoodieClusteringJob.java
Patch:
@@ -99,6 +99,7 @@ public void testHoodieClusteringJobWithClean() throws Exception {
 
   private HoodieClusteringJob init(String tableBasePath, boolean runSchedule, String scheduleAndExecute, boolean isAutoClean) {
     HoodieClusteringJob.Config clusterConfig = buildHoodieClusteringUtilConfig(tableBasePath, runSchedule, scheduleAndExecute, isAutoClean);
+    clusterConfig.configs.add(String.format("%s=%s", "hoodie.datasource.write.row.writer.enable", "false"));
     return new HoodieClusteringJob(jsc, clusterConfig);
   }
 

File: hudi-gcp/src/main/java/org/apache/hudi/gcp/bigquery/BigQuerySchemaResolver.java
Patch:
@@ -92,9 +92,9 @@ private static String fieldsToSqlString(List<Field> fields) {
       }
       String name = field.getName();
       if (field.getMode() == Field.Mode.REPEATED) {
-        return String.format("%s ARRAY<%s>", name, type);
+        return String.format("`%s` ARRAY<%s>", name, type);
       } else {
-        return String.format("%s %s%s", name, type, mode);
+        return String.format("`%s` %s%s", name, type, mode);
       }
     }).collect(Collectors.joining(", "));
   }

File: hudi-gcp/src/test/java/org/apache/hudi/gcp/bigquery/TestHoodieBigQuerySyncClient.java
Patch:
@@ -94,7 +94,7 @@ void createTableWithManifestFile_partitioned() throws Exception {
 
     QueryJobConfiguration configuration = jobInfoCaptor.getValue().getConfiguration();
     assertEquals(configuration.getQuery(),
-        String.format("CREATE EXTERNAL TABLE `%s.%s.%s` ( field STRING ) WITH PARTITION COLUMNS OPTIONS (enable_list_inference=true, "
+        String.format("CREATE EXTERNAL TABLE `%s.%s.%s` ( `field` STRING ) WITH PARTITION COLUMNS OPTIONS (enable_list_inference=true, "
             + "hive_partition_uri_prefix=\"%s\", uris=[\"%s\"], format=\"PARQUET\", "
             + "file_set_spec_type=\"NEW_LINE_DELIMITED_MANIFEST\")", PROJECT_ID, TEST_DATASET, TEST_TABLE, SOURCE_PREFIX, MANIFEST_FILE_URI));
   }
@@ -114,7 +114,7 @@ void createTableWithManifestFile_nonPartitioned() throws Exception {
 
     QueryJobConfiguration configuration = jobInfoCaptor.getValue().getConfiguration();
     assertEquals(configuration.getQuery(),
-        String.format("CREATE EXTERNAL TABLE `%s.%s.%s` ( field STRING ) OPTIONS (enable_list_inference=true, uris=[\"%s\"], format=\"PARQUET\", "
+        String.format("CREATE EXTERNAL TABLE `%s.%s.%s` ( `field` STRING ) OPTIONS (enable_list_inference=true, uris=[\"%s\"], format=\"PARQUET\", "
             + "file_set_spec_type=\"NEW_LINE_DELIMITED_MANIFEST\")", PROJECT_ID, TEST_DATASET, TEST_TABLE, MANIFEST_FILE_URI));
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/action/InternalSchemaMerger.java
Patch:
@@ -116,9 +116,9 @@ private List<Types.Field> buildRecordType(List<Types.Field> oldFields, List<Type
       Type newType = newTypes.get(i);
       Types.Field oldField = oldFields.get(i);
       int fieldId = oldField.fieldId();
-      String fullName = querySchema.findfullName(fieldId);
+      String fullName = querySchema.findFullName(fieldId);
       if (fileSchema.findField(fieldId) != null) {
-        if (fileSchema.findfullName(fieldId).equals(fullName)) {
+        if (fileSchema.findFullName(fieldId).equals(fullName)) {
           // maybe col type changed, deal with it.
           newFields.add(Types.Field.get(oldField.fieldId(), oldField.isOptional(), oldField.name(), newType, oldField.doc()));
         } else {
@@ -173,7 +173,7 @@ private String normalizeFullName(String fullName) {
       }
       String parentName = sb.toString();
       int parentFieldIdFromQuerySchema = querySchema.findIdByName(parentName);
-      String parentNameFromFileSchema = fileSchema.findfullName(parentFieldIdFromQuerySchema);
+      String parentNameFromFileSchema = fileSchema.findFullName(parentFieldIdFromQuerySchema);
       if (parentNameFromFileSchema.isEmpty()) {
         break;
       }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java
Patch:
@@ -506,7 +506,7 @@ public Pair<Boolean, List<CleanFileInfo>> getDeletePaths(String partitionPath, O
   }
 
   /**
-   * Returns earliest commit to retain based on cleaning policy.
+   * Returns the earliest commit to retain based on cleaning policy.
    */
   public Option<HoodieInstant> getEarliestCommitToRetain() {
     return CleanerUtils.getEarliestCommitToRetain(

File: hudi-common/src/main/java/org/apache/hudi/common/util/CompactionUtils.java
Patch:
@@ -339,7 +339,7 @@ public static Option<Pair<HoodieTimeline, HoodieInstant>> getDeltaCommitsSinceLa
   }
 
   /**
-   * Gets the oldest instant to retain for MOR compaction.
+   * Gets the earliest instant to retain for MOR compaction.
    * If there is no completed compaction,
    * num delta commits >= "hoodie.compact.inline.max.delta.commits"
    * If there is a completed compaction,
@@ -348,9 +348,9 @@ public static Option<Pair<HoodieTimeline, HoodieInstant>> getDeltaCommitsSinceLa
    * @param activeTimeline  Active timeline of a table.
    * @param maxDeltaCommits Maximum number of delta commits that trigger the compaction plan,
    *                        i.e., "hoodie.compact.inline.max.delta.commits".
-   * @return the oldest instant to keep for MOR compaction.
+   * @return the earliest instant to keep for MOR compaction.
    */
-  public static Option<HoodieInstant> getOldestInstantToRetainForCompaction(
+  public static Option<HoodieInstant> getEarliestInstantToRetainForCompaction(
       HoodieActiveTimeline activeTimeline, int maxDeltaCommits) {
     Option<Pair<HoodieTimeline, HoodieInstant>> deltaCommitsInfoOption =
         CompactionUtils.getDeltaCommitsSinceLatestCompaction(activeTimeline);

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestClusteringUtils.java
Patch:
@@ -146,7 +146,7 @@ public void testGetOldestInstantToRetainForClustering() throws IOException {
     HoodieInstant inflightInstant3 = metaClient.getActiveTimeline().transitionReplaceRequestedToInflight(requestedInstant3, Option.empty());
     HoodieInstant completedInstant3 = metaClient.getActiveTimeline().transitionReplaceInflightToComplete(inflightInstant3, Option.empty());
     metaClient.reloadActiveTimeline();
-    Option<HoodieInstant> actual = ClusteringUtils.getOldestInstantToRetainForClustering(metaClient.getActiveTimeline(), metaClient);
+    Option<HoodieInstant> actual = ClusteringUtils.getEarliestInstantToRetainForClustering(metaClient.getActiveTimeline(), metaClient);
     assertTrue(actual.isPresent());
     assertEquals(clusterTime1, actual.get().getTimestamp(), "no clean in timeline, retain first replace commit");
 
@@ -168,7 +168,7 @@ public void testGetOldestInstantToRetainForClustering() throws IOException {
     metaClient.getActiveTimeline().transitionCleanInflightToComplete(inflightInstant4,
         TimelineMetadataUtils.serializeCleanMetadata(cleanMetadata));
     metaClient.reloadActiveTimeline();
-    actual = ClusteringUtils.getOldestInstantToRetainForClustering(metaClient.getActiveTimeline(), metaClient);
+    actual = ClusteringUtils.getEarliestInstantToRetainForClustering(metaClient.getActiveTimeline(), metaClient);
     assertEquals(clusterTime3, actual.get().getTimestamp(),
         "retain the first replace commit after the earliestInstantToRetain ");
   }
@@ -206,7 +206,7 @@ public void testGetOldestInstantToRetainForClusteringKeepFileVersion() throws IO
     metaClient.getActiveTimeline().transitionReplaceInflightToComplete(inflightInstant3, Option.empty());
     metaClient.reloadActiveTimeline();
 
-    Option<HoodieInstant> actual = ClusteringUtils.getOldestInstantToRetainForClustering(metaClient.getActiveTimeline(), metaClient);
+    Option<HoodieInstant> actual = ClusteringUtils.getEarliestInstantToRetainForClustering(metaClient.getActiveTimeline(), metaClient);
     assertEquals(clusterTime2, actual.get().getTimestamp(),
         "retain the first replace commit after the last complete clean ");
   }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -479,12 +479,12 @@ private void testDeduplication(
     // Global dedup should be done based on recordKey only
     HoodieIndex index = mock(HoodieIndex.class);
     when(index.isGlobal()).thenReturn(true);
-    int dedupParallelism = records.getNumPartitions() + 100;
+    int dedupParallelism = records.getNumPartitions() + 2;
     HoodieData<HoodieRecord<RawTripTestPayload>> dedupedRecsRdd =
         (HoodieData<HoodieRecord<RawTripTestPayload>>) HoodieWriteHelper.newInstance()
             .deduplicateRecords(records, index, dedupParallelism, writeConfig.getSchema(), writeConfig.getProps(), HoodiePreCombineAvroRecordMerger.INSTANCE);
     List<HoodieRecord<RawTripTestPayload>> dedupedRecs = dedupedRecsRdd.collectAsList();
-    assertEquals(records.getNumPartitions(), dedupedRecsRdd.getNumPartitions());
+    assertEquals(dedupParallelism, dedupedRecsRdd.getNumPartitions());
     assertEquals(1, dedupedRecs.size());
     assertEquals(dedupedRecs.get(0).getPartitionPath(), recordThree.getPartitionPath());
     assertNodupesWithinPartition(dedupedRecs);
@@ -496,7 +496,7 @@ private void testDeduplication(
         (HoodieData<HoodieRecord<RawTripTestPayload>>) HoodieWriteHelper.newInstance()
             .deduplicateRecords(records, index, dedupParallelism, writeConfig.getSchema(), writeConfig.getProps(), HoodiePreCombineAvroRecordMerger.INSTANCE);
     dedupedRecs = dedupedRecsRdd.collectAsList();
-    assertEquals(records.getNumPartitions(), dedupedRecsRdd.getNumPartitions());
+    assertEquals(dedupParallelism, dedupedRecsRdd.getNumPartitions());
     assertEquals(2, dedupedRecs.size());
     assertNodupesWithinPartition(dedupedRecs);
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java
Patch:
@@ -137,7 +137,7 @@ public SparkHoodieHBaseIndex(HoodieWriteConfig config) {
   }
 
   private void init(HoodieWriteConfig config) {
-    this.multiPutBatchSize = config.getHbaseIndexGetBatchSize();
+    this.multiPutBatchSize = config.getHbaseIndexPutBatchSize();
     this.maxQpsPerRegionServer = config.getHbaseIndexMaxQPSPerRegionServer();
     this.putBatchSizeCalculator = new HBasePutBatchSizeCalculator();
     this.hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java
Patch:
@@ -98,8 +98,9 @@ public HiveSyncConfig(Properties props) {
 
   public HiveSyncConfig(Properties props, Configuration hadoopConf) {
     super(props, hadoopConf);
-    HiveConf hiveConf = hadoopConf instanceof HiveConf
-        ? (HiveConf) hadoopConf : new HiveConf(hadoopConf, HiveConf.class);
+    HiveConf hiveConf = new HiveConf();
+    // HiveConf needs to load Hadoop conf to allow instantiation via AWSGlueClientFactory
+    hiveConf.addResource(hadoopConf);
     setHadoopConf(hiveConf);
     validateParameters();
   }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamerDAGExecution.java
Patch:
@@ -86,14 +86,14 @@ private void runDeltaStreamer(WriteOperationType operationType, boolean shouldGe
         PARQUET_SOURCE_ROOT, false, "partition_path", "");
     String tableBasePath = basePath + "/runDeltaStreamer" + testNum;
     FileIOUtils.deleteDirectory(new File(tableBasePath));
-    HoodieDeltaStreamer.Config config = TestHoodieDeltaStreamer.TestHelpers.makeConfig(tableBasePath, operationType,
+    HoodieDeltaStreamer.Config config = TestHelpers.makeConfig(tableBasePath, operationType,
         ParquetDFSSource.class.getName(), null, PROPS_FILENAME_TEST_PARQUET, false,
         useSchemaProvider, 100000, false, null, HoodieTableType.MERGE_ON_READ.name(), "timestamp", null);
     configsOpt.ifPresent(cfgs -> config.configs.addAll(cfgs));
     HoodieDeltaStreamer deltaStreamer = new HoodieDeltaStreamer(config, jsc);
 
     deltaStreamer.sync();
-    TestHoodieDeltaStreamer.TestHelpers.assertRecordCount(parquetRecordsCount, tableBasePath, sqlContext);
+    assertRecordCount(parquetRecordsCount, tableBasePath, sqlContext);
     testNum++;
 
     if (shouldGenerateUpdates) {

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestTransformer.java
Patch:
@@ -59,7 +59,7 @@ public void testMultipleTransformersWithIdentifiers() throws Exception {
         PARQUET_SOURCE_ROOT, false, "partition_path", "");
     String tableBasePath = basePath + "/testMultipleTransformersWithIdentifiers" + testNum;
     HoodieDeltaStreamer deltaStreamer = new HoodieDeltaStreamer(
-        TestHoodieDeltaStreamer.TestHelpers.makeConfig(tableBasePath, WriteOperationType.INSERT, ParquetDFSSource.class.getName(),
+        HoodieDeltaStreamerTestBase.TestHelpers.makeConfig(tableBasePath, WriteOperationType.INSERT, ParquetDFSSource.class.getName(),
             transformerClassNames, PROPS_FILENAME_TEST_PARQUET, false,
             useSchemaProvider, 100000, false, null, null, "timestamp", null), jsc);
 
@@ -78,7 +78,7 @@ public void testMultipleTransformersWithIdentifiers() throws Exception {
     properties.setProperty("transformer.suffix", ".1,.2,.3");
     deltaStreamer.sync();
 
-    TestHoodieDeltaStreamer.TestHelpers.assertRecordCount(parquetRecordsCount, tableBasePath, sqlContext);
+    assertRecordCount(parquetRecordsCount, tableBasePath, sqlContext);
     assertEquals(0, sqlContext.read().format("org.apache.hudi").load(tableBasePath).where("timestamp != 110").count());
   }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/testutils/UtilitiesTestBase.java
Patch:
@@ -118,6 +118,7 @@ public class UtilitiesTestBase {
   protected static HoodieSparkEngineContext context;
   protected static SparkSession sparkSession;
   protected static SQLContext sqlContext;
+  protected static Configuration hadoopConf;
 
   @BeforeAll
   public static void setLogLevel() {
@@ -131,7 +132,7 @@ public static void initTestServices() throws Exception {
   }
 
   public static void initTestServices(boolean needsHdfs, boolean needsHive, boolean needsZookeeper) throws Exception {
-    final Configuration hadoopConf = HoodieTestUtils.getDefaultHadoopConf();
+    hadoopConf = HoodieTestUtils.getDefaultHadoopConf();
     hadoopConf.set("hive.exec.scratchdir", System.getenv("java.io.tmpdir") + "/hive");
 
     if (needsHdfs) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieLogBlock.java
Patch:
@@ -168,7 +168,7 @@ public static HoodieLogBlockType fromId(String id) {
    * new enums at the end.
    */
   public enum HeaderMetadataType {
-    INSTANT_TIME, TARGET_INSTANT_TIME, SCHEMA, COMMAND_BLOCK_TYPE, COMPACTED_BLOCK_TIMES, RECORD_POSITIONS, BLOCK_SEQUENCE_NUMBER
+    INSTANT_TIME, TARGET_INSTANT_TIME, SCHEMA, COMMAND_BLOCK_TYPE, COMPACTED_BLOCK_TIMES, RECORD_POSITIONS, BLOCK_IDENTIFIER
   }
 
   /**

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -2915,7 +2915,7 @@ private static Set<HoodieLogFile> writeLogFiles(Path partitionPath,
   private static Map<HeaderMetadataType, String> getUpdatedHeader(Map<HeaderMetadataType, String> header, int blockSequenceNumber) {
     Map<HeaderMetadataType, String> updatedHeader = new HashMap<>();
     updatedHeader.putAll(header);
-    updatedHeader.put(HeaderMetadataType.BLOCK_SEQUENCE_NUMBER, String.valueOf(blockSequenceNumber));
+    updatedHeader.put(HeaderMetadataType.BLOCK_IDENTIFIER, String.valueOf(blockSequenceNumber));
     return updatedHeader;
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -172,7 +172,7 @@ protected HoodieBackedTableMetadataWriter(Configuration hadoopConf,
 
     this.dataMetaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(dataWriteConfig.getBasePath()).build();
 
-    if (dataMetaClient.getTableConfig().isMetadataTableAvailable() || writeConfig.isMetadataTableEnabled()) {
+    if (writeConfig.isMetadataTableEnabled()) {
       this.metadataWriteConfig = HoodieMetadataWriteUtils.createMetadataWriteConfig(writeConfig, failedWritesCleaningPolicy);
 
       try {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkTable.java
Patch:
@@ -91,7 +91,7 @@ protected HoodieIndex getIndex(HoodieWriteConfig config, HoodieEngineContext con
   protected Option<HoodieTableMetadataWriter> getMetadataWriter(
       String triggeringInstantTimestamp,
       HoodieFailedWritesCleaningPolicy failedWritesCleaningPolicy) {
-    if (config.isMetadataTableEnabled() || metaClient.getTableConfig().isMetadataTableAvailable()) {
+    if (config.isMetadataTableEnabled()) {
       // if any partition is deleted, we need to reload the metadata table writer so that new table configs are picked up
       // to reflect the delete mdt partitions.
       deleteMetadataIndexIfNecessary();
@@ -112,6 +112,7 @@ protected Option<HoodieTableMetadataWriter> getMetadataWriter(
         throw new HoodieMetadataException("Checking existence of metadata table failed", e);
       }
     } else {
+      // if metadata is not enabled in the write config, we should try and delete it (if present)
       maybeDeleteMetadataTable();
     }
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -605,6 +605,7 @@ public void reset() {
     dataMetaClient.reloadActiveTimeline();
     if (metadataMetaClient != null) {
       metadataMetaClient.reloadActiveTimeline();
+      metadataFileSystemView.close();
       metadataFileSystemView = getFileSystemView(metadataMetaClient);
     }
     // the cached reader has max instant time restriction, they should be cleared

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -1411,8 +1411,8 @@ private HoodieData<HoodieRecord> getRecordIndexUpdates(HoodieData<WriteStatus> w
           .flatMapToPair(Stream::iterator)
           .reduceByKey((recordDelegate1, recordDelegate2) -> {
             if (recordDelegate1.getRecordKey().equals(recordDelegate2.getRecordKey())) {
-              if (recordDelegate1.getNewLocation().isPresent() && recordDelegate2.getNewLocation().isPresent()) {
-                throw new HoodieIOException("Both version of records does not have location set. Record V1 " + recordDelegate1.toString()
+              if (!recordDelegate1.getNewLocation().isPresent() && !recordDelegate2.getNewLocation().isPresent()) {
+                throw new HoodieIOException("Both version of records do not have location set. Record V1 " + recordDelegate1.toString()
                     + ", Record V2 " + recordDelegate2.toString());
               }
               if (recordDelegate1.getNewLocation().isPresent()) {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bootstrap/BootstrapOperator.java
Patch:
@@ -108,7 +108,9 @@ public BootstrapOperator(Configuration conf) {
   @Override
   public void snapshotState(StateSnapshotContext context) throws Exception {
     lastInstantTime = this.ckpMetadata.lastPendingInstant();
-    instantState.update(Collections.singletonList(lastInstantTime));
+    if (null != lastInstantTime) {
+      instantState.update(Collections.singletonList(lastInstantTime));
+    }
   }
 
   @Override

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandleWithChangeLog.java
Patch:
@@ -103,7 +103,7 @@ protected void writeInsertRecord(HoodieRecord<T> newRecord) throws IOException {
     // TODO Remove these unnecessary newInstance invocations
     HoodieRecord<T> savedRecord = newRecord.newInstance();
     super.writeInsertRecord(newRecord);
-    if (!HoodieOperation.isDelete(newRecord.getOperation())) {
+    if (!HoodieOperation.isDelete(newRecord.getOperation()) && !savedRecord.isDelete(schema, config.getPayloadConfig().getProps())) {
       cdcLogger.put(newRecord, null, savedRecord.toIndexedRecord(schema, config.getPayloadConfig().getProps()).map(HoodieAvroIndexedRecord::getData));
     }
   }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.data.HoodieJavaRDD;
 import org.apache.hudi.metrics.DistributedRegistry;
+import org.apache.hudi.metrics.MetricsReporterType;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.spark.api.java.JavaRDD;
@@ -98,7 +99,7 @@ public static HoodieTableMetadataWriter create(Configuration conf, HoodieWriteCo
   protected void initRegistry() {
     if (metadataWriteConfig.isMetricsOn()) {
       Registry registry;
-      if (metadataWriteConfig.isExecutorMetricsEnabled()) {
+      if (metadataWriteConfig.isExecutorMetricsEnabled() && metadataWriteConfig.getMetricsReporterType() != MetricsReporterType.INMEMORY) {
         registry = Registry.getRegistry("HoodieMetadata", DistributedRegistry.class.getName());
         HoodieSparkEngineContext sparkEngineContext = (HoodieSparkEngineContext) engineContext;
         ((DistributedRegistry) registry).register(sparkEngineContext.getJavaSparkContext());

File: hudi-aws/src/main/java/org/apache/hudi/aws/sync/AWSGlueCatalogSyncClient.java
Patch:
@@ -607,6 +607,7 @@ private static boolean updateTableParameters(GlueAsyncClient awsGlue, String dat
 
       UpdateTableRequest request =  UpdateTableRequest.builder().databaseName(databaseName)
           .tableInput(updatedTableInput)
+          .skipArchive(skipTableArchive)
           .build();
       awsGlue.updateTable(request);
       return true;

File: hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java
Patch:
@@ -327,8 +327,9 @@ private MessageType readSchemaFromHFileBaseFile(Path hFilePath) throws IOExcepti
 
     FileSystem fs = metaClient.getRawFs();
     CacheConfig cacheConfig = new CacheConfig(fs.getConf());
-    HoodieAvroHFileReader hFileReader = new HoodieAvroHFileReader(fs.getConf(), hFilePath, cacheConfig);
-    return convertAvroSchemaToParquet(hFileReader.getSchema());
+    try (HoodieAvroHFileReader hFileReader = new HoodieAvroHFileReader(fs.getConf(), hFilePath, cacheConfig)) {
+      return convertAvroSchemaToParquet(hFileReader.getSchema());
+    }
   }
 
   private MessageType readSchemaFromORCBaseFile(Path orcFilePath) throws IOException {

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -375,14 +375,16 @@ private Map<String, HoodieRecord<HoodieMetadataPayload>> fetchBaseFileRecordsByK
         ? reader.getRecordsByKeysIterator(sortedKeys)
         : reader.getRecordsByKeyPrefixIterator(sortedKeys);
 
-    return toStream(records)
+    Map<String, HoodieRecord<HoodieMetadataPayload>> result = toStream(records)
         .map(record -> {
           GenericRecord data = (GenericRecord) record.getData();
           return Pair.of(
               (String) (data).get(HoodieMetadataPayload.KEY_FIELD_NAME),
               composeRecord(data, partitionName));
         })
         .collect(Collectors.toMap(Pair::getKey, Pair::getValue));
+    records.close();
+    return result;
   }
 
   private HoodieRecord<HoodieMetadataPayload> composeRecord(GenericRecord avroRecord, String partitionName) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/TableFileSystemView.java
Patch:
@@ -171,14 +171,14 @@ interface SliceView extends SliceViewWithLatestSlice {
   /**
    * Return Pending Compaction Operations.
    *
-   * @return Pair<Pair<InstantTime,CompactionOperation>>
+   * @return Stream<Pair<InstantTime,CompactionOperation>>
    */
   Stream<Pair<String, CompactionOperation>> getPendingCompactionOperations();
 
   /**
    * Return Pending Compaction Operations.
    *
-   * @return Pair<Pair<InstantTime,CompactionOperation>>
+   * @return Stream<Pair<InstantTime,CompactionOperation>>
    */
   Stream<Pair<String, CompactionOperation>> getPendingLogCompactionOperations();
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java
Patch:
@@ -133,7 +133,6 @@ private void prepareHiveAvroSerializer() {
       LOG.warn("fall to init HiveAvroSerializer to support payload merge", e);
       this.supportPayload = false;
     }
-
   }
 
   /**

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/testutils/TestHoodieMetadataBase.java
Patch:
@@ -303,7 +303,7 @@ protected HoodieWriteConfig.Builder getWriteConfigBuilder(HoodieFailedWritesClea
             .ignoreSpuriousDeletes(validateMetadataPayloadConsistency)
             .build())
         .withMetricsConfig(HoodieMetricsConfig.newBuilder().on(enableMetrics)
-            .withExecutorMetrics(true).build())
+            .withExecutorMetrics(enableMetrics).build())
         .withMetricsGraphiteConfig(HoodieMetricsGraphiteConfig.newBuilder()
             .usePrefix("unit-test").build())
         .withRollbackUsingMarkers(useRollbackUsingMarkers)

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieMetadataBase.java
Patch:
@@ -344,7 +344,7 @@ protected HoodieWriteConfig.Builder getWriteConfigBuilder(HoodieFailedWritesClea
             .ignoreSpuriousDeletes(validateMetadataPayloadConsistency)
             .build())
         .withMetricsConfig(HoodieMetricsConfig.newBuilder().on(enableMetrics)
-            .withExecutorMetrics(true).build())
+            .withExecutorMetrics(enableMetrics).build())
         .withMetricsGraphiteConfig(HoodieMetricsGraphiteConfig.newBuilder()
             .usePrefix("unit-test").build())
         .withRollbackUsingMarkers(useRollbackUsingMarkers)

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestBootstrapRead.java
Patch:
@@ -63,7 +63,7 @@ private static Stream<Arguments> testArgs() {
 
   @ParameterizedTest
   @MethodSource("testArgs")
-  public void runTests(String bootstrapType, Boolean dashPartitions, HoodieTableType tableType, Integer nPartitions) {
+  public void testBootstrapFunctional(String bootstrapType, Boolean dashPartitions, HoodieTableType tableType, Integer nPartitions) {
     this.bootstrapType = bootstrapType;
     this.dashPartitions = dashPartitions;
     this.tableType = tableType;

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestNewHoodieParquetFileFormat.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.SaveMode;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Tag;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.Arguments;
@@ -38,6 +39,7 @@
 import static org.junit.jupiter.api.Assertions.assertEquals;
 
 @Tag("functional")
+@Disabled("HUDI-6756")
 public class TestNewHoodieParquetFileFormat extends TestBootstrapReadBase {
 
   private static Stream<Arguments> testArgs() {
@@ -54,7 +56,7 @@ private static Stream<Arguments> testArgs() {
 
   @ParameterizedTest
   @MethodSource("testArgs")
-  public void runTests(HoodieTableType tableType, Integer nPartitions) {
+  public void testNewParquetFileFormat(HoodieTableType tableType, Integer nPartitions) {
     this.bootstrapType = nPartitions == 0 ? "metadata" : "mixed";
     this.dashPartitions = true;
     this.tableType = tableType;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/embedded/EmbeddedTimelineService.java
Patch:
@@ -70,7 +70,7 @@ private FileSystemViewManager createViewManager() {
       // Reset to default if set to Remote
       builder.withStorageType(FileSystemViewStorageType.MEMORY);
     }
-    return FileSystemViewManager.createViewManager(context, writeConfig.getMetadataConfig(), builder.build(), writeConfig.getCommonConfig(), basePath);
+    return FileSystemViewManager.createViewManagerWithTableMetadata(context, writeConfig.getMetadataConfig(), builder.build(), writeConfig.getCommonConfig());
   }
 
   public void startServer() throws IOException {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -145,7 +145,7 @@ protected HoodieTable(HoodieWriteConfig config, HoodieEngineContext context, Hoo
         .build();
     this.metadata = HoodieTableMetadata.create(context, metadataConfig, config.getBasePath());
 
-    this.viewManager = FileSystemViewManager.createViewManager(context, config.getMetadataConfig(), config.getViewStorageConfig(), config.getCommonConfig(), () -> metadata);
+    this.viewManager = FileSystemViewManager.createViewManager(context, config.getMetadataConfig(), config.getViewStorageConfig(), config.getCommonConfig(), unused -> metadata);
     this.metaClient = metaClient;
     this.index = getIndex(config, context);
     this.storageLayout = getStorageLayout(config);
@@ -164,7 +164,7 @@ protected HoodieStorageLayout getStorageLayout(HoodieWriteConfig config) {
 
   private synchronized FileSystemViewManager getViewManager() {
     if (null == viewManager) {
-      viewManager = FileSystemViewManager.createViewManager(getContext(), config.getMetadataConfig(), config.getViewStorageConfig(), config.getCommonConfig(), () -> metadata);
+      viewManager = FileSystemViewManager.createViewManager(getContext(), config.getMetadataConfig(), config.getViewStorageConfig(), config.getCommonConfig(), unused -> metadata);
     }
     return viewManager;
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/SupportsUpgradeDowngrade.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.hudi.table.upgrade;
 
+import org.apache.hudi.client.BaseHoodieWriteClient;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
@@ -41,4 +42,6 @@ public interface SupportsUpgradeDowngrade extends Serializable {
    * @return partition columns in String.
    */
   String getPartitionColumns(HoodieWriteConfig config);
+
+  BaseHoodieWriteClient getWriteClient(HoodieWriteConfig config, HoodieEngineContext context);
 }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteJob.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.integ.testsuite;
 
+import org.apache.hudi.DataSourceWriteOptions;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.fs.FSUtils;
@@ -120,6 +121,7 @@ public HoodieTestSuiteJob(HoodieTestSuiteConfig cfg, JavaSparkContext jsc, boole
       metaClient = HoodieTableMetaClient.withPropertyBuilder()
           .setTableType(cfg.tableType)
           .setTableName(cfg.targetTableName)
+          .setRecordKeyFields(this.props.getString(DataSourceWriteOptions.RECORDKEY_FIELD().key()))
           .setArchiveLogFolder(ARCHIVELOG_FOLDER.defaultValue())
           .initTable(jsc.hadoopConfiguration(), cfg.targetBasePath);
     } else {

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -71,7 +71,6 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.conf.Configuration;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java
Patch:
@@ -580,7 +580,7 @@ private boolean checkMetadataTableIsAvailable() {
   private List<String> validatePartitions(HoodieSparkEngineContext engineContext, String basePath) {
     // compare partitions
     List<String> allPartitionPathsFromFS = FSUtils.getAllPartitionPaths(engineContext, basePath, false, cfg.assumeDatePartitioning);
-    HoodieTimeline completedTimeline = metaClient.getActiveTimeline().filterCompletedInstants();
+    HoodieTimeline completedTimeline = metaClient.getCommitsTimeline().filterCompletedInstants();
 
     // ignore partitions created by uncommitted ingestion.
     allPartitionPathsFromFS = allPartitionPathsFromFS.stream().parallel().filter(part -> {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java
Patch:
@@ -491,10 +491,10 @@ private boolean checkMetadataTableIsAvailable() {
           .setConf(jsc.hadoopConfiguration()).setBasePath(new Path(cfg.basePath, HoodieTableMetaClient.METADATA_TABLE_FOLDER_PATH).toString())
           .setLoadActiveTimelineOnLoad(true)
           .build();
-      int finishedInstants = mdtMetaClient.getActiveTimeline().filterCompletedInstants().countInstants();
+      int finishedInstants = mdtMetaClient.getCommitsTimeline().filterCompletedInstants().countInstants();
       if (finishedInstants == 0) {
-        if (metaClient.getActiveTimeline().filterCompletedInstants().countInstants() == 0) {
-          LOG.info("There is no completed instant both in metadata table and corresponding data table.");
+        if (metaClient.getCommitsTimeline().filterCompletedInstants().countInstants() == 0) {
+          LOG.info("There is no completed commit in both metadata table and corresponding data table.");
           return false;
         } else {
           throw new HoodieValidationException("There is no completed instant for metadata table.");

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java
Patch:
@@ -217,7 +217,7 @@ public static Pair<CloudObjectIncrCheckpoint, Dataset<Row>> filterAndGenerateChe
       row = collectedRows.select(queryInfo.getOrderColumn(), queryInfo.getKeyColumn(), CUMULATIVE_COLUMN_NAME).orderBy(
           col(queryInfo.getOrderColumn()).desc(), col(queryInfo.getKeyColumn()).desc()).first();
     }
-    LOG.info("Processed batch size: " + row.getLong(2) + " bytes");
+    LOG.info("Processed batch size: " + row.get(row.fieldIndex(CUMULATIVE_COLUMN_NAME)) + " bytes");
     sourceData.unpersist();
     return Pair.of(new CloudObjectIncrCheckpoint(row.getString(0), row.getString(1)), collectedRows);
   }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestConsistentBucketIndex.java
Patch:
@@ -228,8 +228,8 @@ public void testBulkInsertData(boolean populateMetaFields, boolean partitioned)
     Assertions.assertEquals(numFilesCreated,
         Arrays.stream(dataGen.getPartitionPaths()).mapToInt(p -> Objects.requireNonNull(listStatus(p, true)).length).sum());
 
-    // BulkInsert again.
-    writeData(writeRecords, "002", WriteOperationType.BULK_INSERT,true);
+    // Upsert Data
+    writeData(writeRecords, "002", WriteOperationType.UPSERT,true);
     // The total number of file group should be the same, but each file group will have a log file.
     Assertions.assertEquals(numFilesCreated,
         Arrays.stream(dataGen.getPartitionPaths()).mapToInt(p -> Objects.requireNonNull(listStatus(p, true)).length).sum());

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
+import org.apache.hudi.common.util.ConfigUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.util.collection.Pair;
@@ -34,7 +35,6 @@
 import org.apache.hudi.exception.HoodieCatalogException;
 import org.apache.hudi.exception.HoodieMetadataException;
 import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;
-import org.apache.hudi.sync.common.util.ConfigUtils;
 import org.apache.hudi.table.format.FilePathUtils;
 import org.apache.hudi.util.AvroSchemaConverter;
 import org.apache.hudi.util.DataTypeUtils;

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/helpers/DFSTestSuitePathSelector.java
Patch:
@@ -41,6 +41,8 @@
 import java.util.List;
 import java.util.stream.Collectors;
 
+import static org.apache.hudi.common.util.ConfigUtils.getStringWithAltKeys;
+
 /**
  * A custom dfs path selector used only for the hudi test suite. To be used only if workload is not run inline.
  */
@@ -70,7 +72,7 @@ public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(
       // obtain all eligible files for the batch
       List<FileStatus> eligibleFiles = new ArrayList<>();
       FileStatus[] fileStatuses = fs.globStatus(
-          new Path(props.getString(DFSPathSelectorConfig.ROOT_INPUT_PATH.key()), "*"));
+          new Path(getStringWithAltKeys(props, DFSPathSelectorConfig.ROOT_INPUT_PATH), "*"));
       // Say input data is as follow input/1, input/2, input/5 since 3,4 was rolled back and 5 is new generated data
       // checkpoint from the latest commit metadata will be 2 since 3,4 has been rolled back. We need to set the
       // next batch id correctly as 5 instead of 3

File: hudi-sync/hudi-adb-sync/src/main/java/org/apache/hudi/sync/adb/AdbSyncTool.java
Patch:
@@ -19,14 +19,14 @@
 package org.apache.hudi.sync.adb;
 
 import org.apache.hudi.common.model.HoodieFileFormat;
+import org.apache.hudi.common.util.ConfigUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;
 import org.apache.hudi.hive.SchemaDifference;
 import org.apache.hudi.hive.util.HiveSchemaUtil;
 import org.apache.hudi.sync.common.HoodieSyncTool;
 import org.apache.hudi.sync.common.model.PartitionEvent;
 import org.apache.hudi.sync.common.model.PartitionEvent.PartitionEventType;
-import org.apache.hudi.sync.common.util.ConfigUtils;
 import org.apache.hudi.sync.common.util.SparkDataSourceTableUtils;
 
 import com.beust.jcommander.JCommander;

File: hudi-sync/hudi-adb-sync/src/test/java/org/apache/hudi/sync/adb/TestAdbSyncConfig.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.apache.hudi.sync.adb;
 
-import org.apache.hudi.sync.common.util.ConfigUtils;
+import org.apache.hudi.common.util.ConfigUtils;
 
 import org.junit.jupiter.api.Test;
 

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieSyncTableStrategy;
+import org.apache.hudi.common.util.ConfigUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.InvalidTableException;
@@ -30,7 +31,6 @@
 import org.apache.hudi.sync.common.model.Partition;
 import org.apache.hudi.sync.common.model.PartitionEvent;
 import org.apache.hudi.sync.common.model.PartitionEvent.PartitionEventType;
-import org.apache.hudi.sync.common.util.ConfigUtils;
 import org.apache.hudi.sync.common.util.SparkDataSourceTableUtils;
 
 import com.beust.jcommander.JCommander;

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveSyncClient.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.common.table.TableSchemaResolver;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.util.ConfigUtils;
 import org.apache.hudi.common.util.MapUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
@@ -36,7 +37,6 @@
 import org.apache.hudi.sync.common.HoodieSyncClient;
 import org.apache.hudi.sync.common.model.FieldSchema;
 import org.apache.hudi.sync.common.model.Partition;
-import org.apache.hudi.sync.common.util.ConfigUtils;
 
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hudi.common.table.timeline.HoodieInstantTimeGenerator;
 import org.apache.hudi.common.testutils.NetworkTestUtils;
 import org.apache.hudi.common.testutils.SchemaTestUtil;
+import org.apache.hudi.common.util.ConfigUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.util.collection.ImmutablePair;
@@ -45,7 +46,6 @@
 import org.apache.hudi.sync.common.model.Partition;
 import org.apache.hudi.sync.common.model.PartitionEvent;
 import org.apache.hudi.sync.common.model.PartitionEvent.PartitionEventType;
-import org.apache.hudi.sync.common.util.ConfigUtils;
 
 import org.apache.avro.Schema;
 import org.apache.avro.Schema.Field;
@@ -98,8 +98,8 @@
 import static org.apache.hudi.hive.testutils.HiveTestUtil.basePath;
 import static org.apache.hudi.hive.testutils.HiveTestUtil.ddlExecutor;
 import static org.apache.hudi.hive.testutils.HiveTestUtil.getHiveConf;
-import static org.apache.hudi.hive.testutils.HiveTestUtil.hiveSyncProps;
 import static org.apache.hudi.hive.testutils.HiveTestUtil.hiveSyncConfig;
+import static org.apache.hudi.hive.testutils.HiveTestUtil.hiveSyncProps;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_BASE_PATH;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_CONDITIONAL_SYNC;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_DATABASE_NAME;

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/HoodieSyncConfig.java
Patch:
@@ -26,10 +26,10 @@
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.table.HoodieTableConfig;
+import org.apache.hudi.common.util.ConfigUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
-import org.apache.hudi.sync.common.util.ConfigUtils;
 
 import com.beust.jcommander.Parameter;
 import org.apache.hadoop.conf.Configuration;

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/HoodieSyncTool.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.hudi.sync.common;
 
 import org.apache.hudi.common.config.TypedProperties;
-import org.apache.hudi.sync.common.util.ConfigUtils;
+import org.apache.hudi.common.util.ConfigUtils;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/util/SparkDataSourceTableUtils.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.sync.common.util;
 
+import org.apache.hudi.common.util.ConfigUtils;
 import org.apache.hudi.common.util.StringUtils;
 
 import org.apache.parquet.schema.GroupType;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JsonKafkaSource.java
Patch:
@@ -43,6 +43,7 @@
 import java.util.LinkedList;
 import java.util.List;
 
+import static org.apache.hudi.common.util.ConfigUtils.getStringWithAltKeys;
 import static org.apache.hudi.utilities.schema.KafkaOffsetPostProcessor.KAFKA_SOURCE_OFFSET_COLUMN;
 import static org.apache.hudi.utilities.schema.KafkaOffsetPostProcessor.KAFKA_SOURCE_PARTITION_COLUMN;
 import static org.apache.hudi.utilities.schema.KafkaOffsetPostProcessor.KAFKA_SOURCE_TIMESTAMP_COLUMN;
@@ -96,7 +97,8 @@ protected  JavaRDD<String> maybeAppendKafkaOffsets(JavaRDD<ConsumerRecord<Object
   }
 
   private JavaRDD<String> postProcess(JavaRDD<String> jsonStringRDD) {
-    String postProcessorClassName = this.props.getString(JsonKafkaPostProcessorConfig.JSON_KAFKA_PROCESSOR_CLASS.key(), null);
+    String postProcessorClassName = getStringWithAltKeys(
+        this.props, JsonKafkaPostProcessorConfig.JSON_KAFKA_PROCESSOR_CLASS, true);
     // no processor, do nothing
     if (StringUtils.isNullOrEmpty(postProcessorClassName)) {
       return jsonStringRDD;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/KafkaSource.java
Patch:
@@ -34,6 +34,8 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import static org.apache.hudi.common.util.ConfigUtils.getBooleanWithAltKeys;
+
 abstract class KafkaSource<T> extends Source<JavaRDD<T>> {
   private static final Logger LOG = LoggerFactory.getLogger(KafkaSource.class);
   // these are native kafka's config. do not change the config names.
@@ -77,7 +79,7 @@ protected InputBatch<JavaRDD<T>> fetchNewData(Option<String> lastCheckpointStr,
 
   @Override
   public void onCommit(String lastCkptStr) {
-    if (this.props.getBoolean(KafkaSourceConfig.ENABLE_KAFKA_COMMIT_OFFSET.key(), KafkaSourceConfig.ENABLE_KAFKA_COMMIT_OFFSET.defaultValue())) {
+    if (getBooleanWithAltKeys(this.props, KafkaSourceConfig.ENABLE_KAFKA_COMMIT_OFFSET)) {
       offsetGen.commitOffsetToKafka(lastCkptStr);
     }
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/transform/ChainedTransformer.java
Patch:
@@ -171,8 +171,8 @@ protected TypedProperties getProperties(TypedProperties properties, List<Transfo
       TypedProperties transformerProps = properties;
       if (idOpt.isPresent()) {
         // Transformer specific property keys end with the id associated with the transformer.
-        // Ex. For id tr1, key `hoodie.deltastreamer.transformer.sql.tr1` would be converted to
-        // `hoodie.deltastreamer.transformer.sql` and then passed to the transformer.
+        // Ex. For id tr1, key `hoodie.streamer.transformer.sql.tr1` would be converted to
+        // `hoodie.streamer.transformer.sql` and then passed to the transformer.
         String id = idOpt.get();
         transformerProps = new TypedProperties(properties);
         Map<String, Object> overrideKeysMap = new HashMap<>();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/transform/SqlQueryBasedTransformer.java
Patch:
@@ -32,6 +32,8 @@
 
 import java.util.UUID;
 
+import static org.apache.hudi.common.util.ConfigUtils.getStringWithAltKeys;
+
 /**
  * A transformer that allows a sql-query template be used to transform the source before writing to Hudi data-set.
  *
@@ -47,7 +49,7 @@ public class SqlQueryBasedTransformer implements Transformer {
   @Override
   public Dataset<Row> apply(JavaSparkContext jsc, SparkSession sparkSession, Dataset<Row> rowDataset,
       TypedProperties properties) {
-    String transformerSQL = properties.getString(SqlTransformerConfig.TRANSFORMER_SQL.key());
+    String transformerSQL = getStringWithAltKeys(properties, SqlTransformerConfig.TRANSFORMER_SQL);
     if (null == transformerSQL) {
       throw new HoodieTransformException("Missing configuration : (" + SqlTransformerConfig.TRANSFORMER_SQL.key() + ")");
     }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/helpers/TestDatePartitionPathSelector.java
Patch:
@@ -39,6 +39,7 @@
 import java.util.UUID;
 import java.util.stream.Stream;
 
+import static org.apache.hudi.common.util.ConfigUtils.getStringWithAltKeys;
 import static org.apache.hudi.utilities.config.DFSPathSelectorConfig.ROOT_INPUT_PATH;
 import static org.apache.hudi.utilities.config.DatePartitionPathSelectorConfig.CURRENT_DATE;
 import static org.apache.hudi.utilities.config.DatePartitionPathSelectorConfig.DATE_FORMAT;
@@ -202,7 +203,7 @@ public void testPruneDatePartitionPaths(
     TypedProperties props = getProps(basePath + "/" + tableName, dateFormat, datePartitionDepth, numPrevDaysToList, currentDate);
     DatePartitionPathSelector pathSelector = new DatePartitionPathSelector(props, jsc.hadoopConfiguration());
 
-    Path root = new Path(props.getString(ROOT_INPUT_PATH.key()));
+    Path root = new Path(getStringWithAltKeys(props, ROOT_INPUT_PATH));
     int totalDepthBeforeDatePartitions = props.getInteger(DATE_PARTITION_DEPTH.key()) - 1;
 
     // Create parent dir

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/util/ConsistentHashingUpdateStrategyUtils.java
Patch:
@@ -83,7 +83,7 @@ private static void extractHashingMetadataFromClusteringPlan(String instant, Hoo
       ValidationUtils.checkState(p != null, "Clustering plan does not has partition info, plan: " + plan);
       // Skip unrelated clustering group
       if (!recordPartitions.contains(p)) {
-        return;
+        continue;
       }
 
       String preInstant = partitionToInstant.putIfAbsent(p, instant);

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/TestWriteMergeOnRead.java
Patch:
@@ -26,7 +26,6 @@
 import org.apache.hudi.utils.TestData;
 
 import org.apache.flink.configuration.Configuration;
-import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.ValueSource;
@@ -165,7 +164,6 @@ public void testInsertAsyncClustering() {
   }
 
   @Test
-  @Disabled("HUDI-6655")
   public void testConsistentBucketIndex() throws Exception {
     conf.setString(FlinkOptions.INDEX_TYPE, "BUCKET");
     conf.setString(FlinkOptions.BUCKET_INDEX_ENGINE_TYPE, "CONSISTENT_HASHING");

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -189,9 +189,9 @@ private FlinkOptions() {
   public static final ConfigOption<Boolean> METADATA_ENABLED = ConfigOptions
       .key("metadata.enabled")
       .booleanType()
-      .defaultValue(true)
+      .defaultValue(false)
       .withFallbackKeys(HoodieMetadataConfig.ENABLE.key())
-      .withDescription("Enable the internal metadata table which serves table metadata like level file listings, default enabled");
+      .withDescription("Enable the internal metadata table which serves table metadata like level file listings, default disabled");
 
   public static final ConfigOption<Integer> METADATA_COMPACTION_DELTA_COMMITS = ConfigOptions
       .key("metadata.compaction.delta_commits")

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/TestWriteMergeOnRead.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.utils.TestData;
 
 import org.apache.flink.configuration.Configuration;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.ValueSource;
@@ -164,6 +165,7 @@ public void testInsertAsyncClustering() {
   }
 
   @Test
+  @Disabled("HUDI-6655")
   public void testConsistentBucketIndex() throws Exception {
     conf.setString(FlinkOptions.INDEX_TYPE, "BUCKET");
     conf.setString(FlinkOptions.BUCKET_INDEX_ENGINE_TYPE, "CONSISTENT_HASHING");

File: hudi-common/src/main/java/org/apache/hudi/common/config/ConfigGroups.java
Patch:
@@ -140,7 +140,7 @@ public static String getDescription(Names names) {
             + "on both datasource and WriteClient levels.";
         break;
       case METRICS:
-        description = "These set of configs are used to enable monitoring and reporting of key"
+        description = "These set of configs are used to enable monitoring and reporting of key "
             + "Hudi stats and metrics.";
         break;
       case KAFKA_CONNECT:

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -239,7 +239,6 @@ public void testMetadataTableBootstrap(HoodieTableType tableType, boolean addRol
   }
 
   @Test
-  @Disabled("HUDI-6324") // Disabling of MDT partitions might have to be revisited. Might only be an admin operation.
   public void testTurnOffMetadataIndexAfterEnable() throws Exception {
     initPath();
     HoodieWriteConfig cfg = getConfigBuilder(TRIP_EXAMPLE_SCHEMA, HoodieIndex.IndexType.BLOOM, HoodieFailedWritesCleaningPolicy.EAGER)

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -542,7 +542,7 @@ public boolean hasNext() {
         public HoodieRecord next() {
           return forDelete
               ? HoodieMetadataPayload.createRecordIndexDelete(recordKeyIterator.next())
-              : HoodieMetadataPayload.createRecordIndexUpdate(recordKeyIterator.next(), partition, fileId, instantTime);
+              : HoodieMetadataPayload.createRecordIndexUpdate(recordKeyIterator.next(), partition, fileId, instantTime, 0);
         }
       };
     });
@@ -1357,7 +1357,7 @@ private HoodieData<HoodieRecord> getRecordIndexUpdates(HoodieData<WriteStatus> w
 
             hoodieRecord = HoodieMetadataPayload.createRecordIndexUpdate(
                 recordDelegate.getRecordKey(), recordDelegate.getPartitionPath(),
-                newLocation.get().getFileId(), newLocation.get().getInstantTime());
+                newLocation.get().getFileId(), newLocation.get().getInstantTime(), dataWriteConfig.getWritesFileIdEncoding());
           } else {
             // Delete existing index for a deleted record
             hoodieRecord = HoodieMetadataPayload.createRecordIndexDelete(recordDelegate.getRecordKey());

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/KafkaConnectWriterProvider.java
Patch:
@@ -93,6 +93,7 @@ public KafkaConnectWriterProvider(
           .withCleanConfig(HoodieCleanConfig.newBuilder().withAutoClean(false).build())
           .withCompactionConfig(HoodieCompactionConfig.newBuilder().withInlineCompaction(false).build())
           .withClusteringConfig(HoodieClusteringConfig.newBuilder().withInlineClustering(false).build())
+          .withWritesFileIdEncoding(1)
           .build();
 
       context = new HoodieJavaEngineContext(hadoopConf);

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java
Patch:
@@ -408,11 +408,10 @@ public void testFileSizeUpsertRecords() throws Exception {
 
   @Test
   public void testInsertUpsertWithHoodieAvroPayload() throws Exception {
-    Schema schema = getSchemaFromResource(TestJavaCopyOnWriteActionExecutor.class, "/testDataGeneratorSchema.txt");
     HoodieWriteConfig config = HoodieWriteConfig.newBuilder()
         .withEngineType(EngineType.JAVA)
         .withPath(basePath)
-        .withSchema(schema.toString())
+        .withSchema(TRIP_EXAMPLE_SCHEMA)
         .withStorageConfig(HoodieStorageConfig.newBuilder()
             .parquetMaxFileSize(1000 * 1024).hfileMaxFileSize(1000 * 1024).build())
         .build();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java
Patch:
@@ -450,8 +450,7 @@ public void testFileSizeUpsertRecords() throws Exception {
 
   @Test
   public void testInsertUpsertWithHoodieAvroPayload() throws Exception {
-    Schema schema = getSchemaFromResource(TestCopyOnWriteActionExecutor.class, "/testDataGeneratorSchema.txt");
-    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(schema.toString())
+    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(TRIP_EXAMPLE_SCHEMA)
         .withFileSystemViewConfig(FileSystemViewStorageConfig.newBuilder()
             .withRemoteServerPort(timelineServicePort).build())
         .withStorageConfig(HoodieStorageConfig.newBuilder()

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestAvroOrcUtils.java
Patch:
@@ -41,9 +41,10 @@ public static List<Arguments> testCreateOrcSchemaArgs() {
     // the ORC schema is constructed in the order as AVRO_SCHEMA:
     // TRIP_SCHEMA_PREFIX, EXTRA_TYPE_SCHEMA, MAP_TYPE_SCHEMA, FARE_NESTED_SCHEMA, TIP_NESTED_SCHEMA, TRIP_SCHEMA_SUFFIX
     // The following types are tested:
-    // DATE, DECIMAL, LONG, INT, BYTES, ARRAY, RECORD, MAP, STRING, FLOAT, DOUBLE
+    // DATE, DECIMAL, LONG, INT, BYTES, ARRAY, RECORD, MAP, STRING, FLOAT, DOUBLE, ENUM
     TypeDescription orcSchema = TypeDescription.fromString("struct<"
-        + "timestamp:bigint,_row_key:string,partition_path:string,rider:string,driver:string,begin_lat:double,"
+        + "timestamp:bigint,_row_key:string,partition_path:string,"
+        + "trip_type:string,rider:string,driver:string,begin_lat:double,"
         + "begin_lon:double,end_lat:double,end_lon:double,"
         + "distance_in_meters:int,seconds_since_epoch:bigint,weight:float,nation:binary,"
         + "current_date:date,current_ts:bigint,height:decimal(10,6),"

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bulk/BulkInsertWriterHelper.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.configuration.FlinkOptions;
+import org.apache.hudi.configuration.OptionsResolver;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.io.storage.row.HoodieRowDataCreateHandle;
 import org.apache.hudi.table.HoodieTable;
@@ -84,7 +85,7 @@ public BulkInsertWriterHelper(Configuration conf, HoodieTable hoodieTable, Hoodi
     this.taskEpochId = taskEpochId;
     this.rowType = preserveHoodieMetadata ? rowType : addMetadataFields(rowType, writeConfig.allowOperationMetadataField()); // patch up with metadata fields
     this.preserveHoodieMetadata = preserveHoodieMetadata;
-    this.isInputSorted = conf.getBoolean(FlinkOptions.WRITE_BULK_INSERT_SORT_INPUT);
+    this.isInputSorted = OptionsResolver.isBulkInsertOperation(conf) && conf.getBoolean(FlinkOptions.WRITE_BULK_INSERT_SORT_INPUT);
     this.fileIdPrefix = UUID.randomUUID().toString();
     this.keyGen = preserveHoodieMetadata ? null : RowDataKeyGen.instance(conf, rowType);
   }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/streamer/HoodieFlinkStreamer.java
Patch:
@@ -103,7 +103,7 @@ public static void main(String[] args) throws Exception {
     DataStream<Object> pipeline;
     // Append mode
     if (OptionsResolver.isAppendMode(conf)) {
-      pipeline = Pipelines.append(conf, rowType, dataStream, false);
+      pipeline = Pipelines.append(conf, rowType, dataStream);
       if (OptionsResolver.needsAsyncClustering(conf)) {
         Pipelines.cluster(conf, rowType, pipeline);
       } else if (OptionsResolver.isLazyFailedWritesCleanPolicy(conf)) {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSink.java
Patch:
@@ -85,14 +85,13 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
       RowType rowType = (RowType) schema.toSinkRowDataType().notNull().getLogicalType();
 
       // bulk_insert mode
-      final String writeOperation = this.conf.get(FlinkOptions.OPERATION);
-      if (WriteOperationType.fromValue(writeOperation) == WriteOperationType.BULK_INSERT) {
+      if (OptionsResolver.isBulkInsertOperation(conf)) {
         return Pipelines.bulkInsert(conf, rowType, dataStream);
       }
 
       // Append mode
       if (OptionsResolver.isAppendMode(conf)) {
-        DataStream<Object> pipeline = Pipelines.append(conf, rowType, dataStream, context.isBounded());
+        DataStream<Object> pipeline = Pipelines.append(conf, rowType, dataStream);
         if (OptionsResolver.needsAsyncClustering(conf)) {
           return Pipelines.cluster(conf, rowType, pipeline);
         } else if (OptionsResolver.isLazyFailedWritesCleanPolicy(conf)) {

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/ITTestDataStreamWrite.java
Patch:
@@ -314,7 +314,7 @@ private void testWriteToHoodieWithCluster(
         .setParallelism(4);
 
     OptionsInference.setupSinkTasks(conf, execEnv.getParallelism());
-    DataStream<Object> pipeline = Pipelines.append(conf, rowType, dataStream, true);
+    DataStream<Object> pipeline = Pipelines.append(conf, rowType, dataStream);
     execEnv.addOperator(pipeline.getTransformation());
 
     Pipelines.cluster(conf, rowType, pipeline);

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/bucket/ITTestConsistentBucketStreamWrite.java
Patch:
@@ -21,11 +21,11 @@
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
-import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieClusteringConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.configuration.FlinkOptions;
 import org.apache.hudi.configuration.OptionsInference;
+import org.apache.hudi.configuration.OptionsResolver;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.sink.utils.Pipelines;
 import org.apache.hudi.util.AvroSchemaConverter;
@@ -187,8 +187,7 @@ private void testWriteToHoodie(
     OptionsInference.setupSinkTasks(conf, execEnv.getParallelism());
     DataStream<HoodieRecord> hoodieRecordDataStream = Pipelines.bootstrap(conf, rowType, dataStream);
     // bulk_insert mode
-    final String writeOperation = conf.get(FlinkOptions.OPERATION);
-    if (WriteOperationType.fromValue(writeOperation) == WriteOperationType.BULK_INSERT) {
+    if (OptionsResolver.isBulkInsertOperation(conf)) {
       Pipelines.bulkInsert(conf, rowType, dataStream);
     } else {
       DataStream<Object> pipeline = Pipelines.hoodieStreamWrite(conf, hoodieRecordDataStream);

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java
Patch:
@@ -334,6 +334,9 @@ private List<MergeOnReadInputSplit> buildInputSplits() {
     HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,
         // file-slice after pending compaction-requested instant-time is also considered valid
         metaClient.getCommitsAndCompactionTimeline().filterCompletedAndCompactionInstants(), fileStatuses);
+    if (!fsView.getLastInstant().isPresent()) {
+      return Collections.emptyList();
+    }
     String latestCommit = fsView.getLastInstant().get().getTimestamp();
     final String mergeType = this.conf.getString(FlinkOptions.MERGE_TYPE);
     final AtomicInteger cnt = new AtomicInteger(0);

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/source/TestFileIndex.java
Patch:
@@ -137,7 +137,9 @@ void testFileListingWithDataSkipping() throws Exception {
             .dataPruner(DataPruner.newInstance(Collections.singletonList(new CallExpression(
                 FunctionIdentifier.of("greaterThan"),
                 BuiltInFunctionDefinitions.GREATER_THAN,
-                Arrays.asList(new FieldReferenceExpression("uuid", DataTypes.BIGINT(), 0, 0), new ValueLiteralExpression(5L, DataTypes.BIGINT().notNull())),
+                Arrays.asList(
+                    new FieldReferenceExpression("uuid", DataTypes.BIGINT(), 0, 0), 
+                    new ValueLiteralExpression((byte) 5, DataTypes.TINYINT().notNull())),
                 DataTypes.BOOLEAN()
             ))))
             .partitionPruner(null)

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -268,14 +268,14 @@ protected void init(String fileId, Iterator<HoodieRecord<T>> newRecordsItr) {
   protected boolean writeUpdateRecord(HoodieRecord<T> newRecord, HoodieRecord<T> oldRecord, Option<HoodieRecord> combineRecordOpt, Schema writerSchema) throws IOException {
     boolean isDelete = false;
     if (combineRecordOpt.isPresent()) {
-      updatedRecordsWritten++;
       if (oldRecord.getData() != combineRecordOpt.get().getData()) {
         // the incoming record is chosen
         isDelete = HoodieOperation.isDelete(newRecord.getOperation());
       } else {
         // the incoming record is dropped
         return false;
       }
+      updatedRecordsWritten++;
     }
     return writeRecord(newRecord, combineRecordOpt, writerSchema, config.getPayloadConfig().getProps(), isDelete);
   }

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordDelegate.java
Patch:
@@ -23,14 +23,16 @@
 
 import javax.annotation.Nullable;
 
+import java.io.Serializable;
+
 /**
  * Delegate for {@link HoodieRecord}.
  * <p>
  * This is used when write handles report back write operation's info and stats,
  * instead of passing back the full {@link HoodieRecord}, this lean delegate
  * of it will be passed instead.
  */
-public class HoodieRecordDelegate {
+public class HoodieRecordDelegate implements Serializable {
 
   private final HoodieKey hoodieKey;
 

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java
Patch:
@@ -302,7 +302,7 @@ public static Map<String, FileStatus[]> getFilesInPartitions(HoodieEngineContext
                                                                HoodieMetadataConfig metadataConfig,
                                                                String basePathStr,
                                                                String[] partitionPaths) {
-    try (HoodieTableMetadata tableMetadata = HoodieTableMetadata.create(engineContext, metadataConfig, basePathStr, true)) {
+    try (HoodieTableMetadata tableMetadata = HoodieTableMetadata.create(engineContext, metadataConfig, basePathStr)) {
       return tableMetadata.getAllFilesInPartitions(Arrays.asList(partitionPaths));
     } catch (Exception ex) {
       throw new HoodieException("Error get files in partitions: " + String.join(",", partitionPaths), ex);

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java
Patch:
@@ -166,8 +166,8 @@ public static class HiveSyncConfigParams {
     @Parameter(names = {"--sync-comment"}, description = "synchronize table comments to hive")
     public Boolean syncComment;
 
-    @Parameter(names = {"--sync-strategy"}, description = "Hive table synchronization strategy. Available option: ONLY_RO, ONLY_RT, ALL")
-    public Boolean syncStrategy;
+    @Parameter(names = {"--sync-strategy"}, description = "Hive table synchronization strategy. Available option: RO, RT, ALL")
+    public String syncStrategy;
 
     public boolean isHelp() {
       return hoodieSyncConfigParams.isHelp();

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstantTimeGenerator.java
Patch:
@@ -114,7 +114,7 @@ public static String getInstantFromTemporalAccessor(TemporalAccessor temporalAcc
 
   /**
    * Creates an instant string given a valid date-time string.
-   * @param dateString A date-time string in the format yyyy-MM-dd HH:mm:ss[:SSS]
+   * @param dateString A date-time string in the format yyyy-MM-dd HH:mm:ss[.SSS]
    * @return A timeline instant
    * @throws ParseException If we cannot parse the date string
    */
@@ -124,7 +124,7 @@ public static String getInstantForDateString(String dateString) {
     } catch (Exception e) {
       // Attempt to add the milliseconds in order to complete parsing
       return getInstantFromTemporalAccessor(LocalDateTime.parse(
-          String.format("%s:%s", dateString, DEFAULT_MILLIS_EXT), MILLIS_GRANULARITY_DATE_FORMATTER));
+          String.format("%s.%s", dateString, DEFAULT_MILLIS_EXT), MILLIS_GRANULARITY_DATE_FORMATTER));
     }
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanActionExecutor.java
Patch:
@@ -50,7 +50,7 @@
 
 public class CleanPlanActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieCleanerPlan>> {
 
-  private static final Logger LOG = LoggerFactory.getLogger(CleanPlanner.class);
+  private static final Logger LOG = LoggerFactory.getLogger(CleanPlanActionExecutor.class);
 
   private final Option<Map<String, String>> extraMetadata;
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -160,7 +160,7 @@ private void doCommit(String instantTime, Map<MetadataPartitionType, HoodieData<
     }
 
     // Update total size of the metadata and count of base/log files
-    metrics.ifPresent(m -> m.updateSizeMetrics(metadataMetaClient, metadata));
+    metrics.ifPresent(m -> m.updateSizeMetrics(metadataMetaClient, metadata, dataMetaClient.getTableConfig().getMetadataPartitions()));
   }
 
   /**

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -135,7 +135,6 @@ private void commitInternal(String instantTime, Map<MetadataPartitionType, Hoodi
     HoodieData<HoodieRecord> preppedRecords = prepRecords(partitionRecordsMap);
     JavaRDD<HoodieRecord> preppedRecordRDD = HoodieJavaRDD.getJavaRDD(preppedRecords);
 
-    engineContext.setJobStatus(this.getClass().getName(), "Committing " + instantTime + " to metadata table " + metadataWriteConfig.getTableName());
     try (SparkRDDWriteClient writeClient = (SparkRDDWriteClient) getWriteClient()) {
       // rollback partially failed writes if any.
       if (dataWriteConfig.getFailedWritesCleanPolicy().isEager()
@@ -169,8 +168,10 @@ private void commitInternal(String instantTime, Map<MetadataPartitionType, Hoodi
 
       writeClient.startCommitWithTime(instantTime);
       if (bulkInsertPartitioner.isPresent()) {
+        engineContext.setJobStatus(this.getClass().getSimpleName(), String.format("Bulk inserting at %s into metadata table %s", instantTime, metadataWriteConfig.getTableName()));
         writeClient.bulkInsertPreppedRecords(preppedRecordRDD, instantTime, bulkInsertPartitioner).collect();
       } else {
+        engineContext.setJobStatus(this.getClass().getSimpleName(), String.format("Upserting at %s into metadata table %s", instantTime, metadataWriteConfig.getTableName()));
         writeClient.upsertPreppedRecords(preppedRecordRDD, instantTime).collect();
       }
 
@@ -179,7 +180,7 @@ private void commitInternal(String instantTime, Map<MetadataPartitionType, Hoodi
     }
 
     // Update total size of the metadata and count of base/log files
-    metrics.ifPresent(m -> m.updateSizeMetrics(metadataMetaClient, metadata));
+    metrics.ifPresent(m -> m.updateSizeMetrics(metadataMetaClient, metadata, dataMetaClient.getTableConfig().getMetadataPartitions()));
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -778,7 +778,7 @@ public void setMetadataPartitionState(HoodieTableMetaClient metaClient, Metadata
     setValue(TABLE_METADATA_PARTITIONS, partitions.stream().sorted().collect(Collectors.joining(CONFIG_VALUES_DELIMITER)));
     setValue(TABLE_METADATA_PARTITIONS_INFLIGHT, partitionsInflight.stream().sorted().collect(Collectors.joining(CONFIG_VALUES_DELIMITER)));
     update(metaClient.getFs(), new Path(metaClient.getMetaPath()), getProps());
-    LOG.info(String.format("MDT %s partition %s has been %s", metaClient.getBasePathV2(), partitionType, enabled ? "enabled" : "disabled"));
+    LOG.info(String.format("MDT %s partition %s has been %s", metaClient.getBasePathV2(), partitionType.name(), enabled ? "enabled" : "disabled"));
   }
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -341,7 +341,7 @@ private void ensurePartitionsLoadedCorrectly(List<String> partitionList) {
         long beginTs = System.currentTimeMillis();
         // Not loaded yet
         try {
-          LOG.info("Building file system view for partitions " + partitionSet);
+          LOG.debug("Building file system view for partitions: " + partitionSet);
 
           // Pairs of relative partition path and absolute partition path
           List<Pair<String, Path>> absolutePartitionPathList = partitionSet.stream()

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -58,6 +58,7 @@
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
@@ -543,7 +544,8 @@ public HoodieTableFileSystemView getMetadataFileSystemView() {
   }
 
   public Map<String, String> stats() {
-    return metrics.map(m -> m.getStats(true, metadataMetaClient, this)).orElse(new HashMap<>());
+    Set<String> allMetadataPartitionPaths = Arrays.stream(MetadataPartitionType.values()).map(MetadataPartitionType::getPartitionPath).collect(Collectors.toSet());
+    return metrics.map(m -> m.getStats(true, metadataMetaClient, this, allMetadataPartitionPaths)).orElse(new HashMap<>());
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -1634,7 +1634,7 @@ public static int estimateFileGroupCount(MetadataPartitionType partitionType, lo
 
     LOG.info(String.format("Estimated file group count for MDT partition %s is %d "
             + "[recordCount=%d, avgRecordSize=%d, minFileGroupCount=%d, maxFileGroupCount=%d, growthFactor=%f, "
-            + "maxFileGroupSizeBytes=%d]", partitionType, fileGroupCount, recordCount, averageRecordSize, minFileGroupCount,
+            + "maxFileGroupSizeBytes=%d]", partitionType.name(), fileGroupCount, recordCount, averageRecordSize, minFileGroupCount,
         maxFileGroupCount, growthFactor, maxFileGroupSizeBytes));
     return fileGroupCount;
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java
Patch:
@@ -477,6 +477,7 @@ private void scanInternalV2(Option<KeySpec> keySpecOption, boolean skipProcessin
         switch (logBlock.getBlockType()) {
           case HFILE_DATA_BLOCK:
           case AVRO_DATA_BLOCK:
+          case PARQUET_DATA_BLOCK:
           case DELETE_BLOCK:
             List<HoodieLogBlock> logBlocksList = instantToBlocksMap.getOrDefault(instantTime, new ArrayList<>());
             if (logBlocksList.size() == 0) {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/ExpressionEvaluators.java
Patch:
@@ -543,6 +543,7 @@ private static int compare(@NotNull Object val1, @NotNull Object val2, LogicalTy
       case TIMESTAMP_WITHOUT_TIME_ZONE:
       case TIME_WITHOUT_TIME_ZONE:
       case DATE:
+      case BIGINT:
         return ((Long) val1).compareTo((Long) val2);
       case BOOLEAN:
         return ((Boolean) val1).compareTo((Boolean) val2);

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java
Patch:
@@ -189,7 +189,7 @@ protected <T> ClosableIterator<HoodieRecord<T>> lookupRecords(List<String> sorte
 
     Path inlinePath = InLineFSUtils.getInlineFilePath(
         blockContentLoc.getLogFile().getPath(),
-        blockContentLoc.getLogFile().getPath().getFileSystem(inlineConf).getScheme(),
+        blockContentLoc.getLogFile().getPath().toUri().getScheme(),
         blockContentLoc.getContentPositionInLogFile(),
         blockContentLoc.getBlockSize());
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieParquetDataBlock.java
Patch:
@@ -151,7 +151,7 @@ protected <T> ClosableIterator<HoodieRecord<T>> readRecordsFromBlockPayload(Hood
 
     Path inlineLogFilePath = InLineFSUtils.getInlineFilePath(
         blockContentLoc.getLogFile().getPath(),
-        blockContentLoc.getLogFile().getPath().getFileSystem(inlineConf).getScheme(),
+        blockContentLoc.getLogFile().getPath().toUri().getScheme(),
         blockContentLoc.getContentPositionInLogFile(),
         blockContentLoc.getBlockSize());
 

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RollbacksCommand.java
Patch:
@@ -135,10 +135,9 @@ public String rollbackCommit(
           help = "Enabling marker based rollback") final String rollbackUsingMarkers)
       throws Exception {
     HoodieActiveTimeline activeTimeline = HoodieCLI.getTableMetaClient().getActiveTimeline();
-    HoodieTimeline completedTimeline = activeTimeline.getCommitsTimeline().filterCompletedInstants();
-    HoodieTimeline filteredTimeline = completedTimeline.filter(instant -> instant.getTimestamp().equals(instantTime));
+    HoodieTimeline filteredTimeline = activeTimeline.filter(instant -> instant.getTimestamp().equals(instantTime));
     if (filteredTimeline.empty()) {
-      return "Commit " + instantTime + " not found in Commits " + completedTimeline;
+      return "Commit " + instantTime + " not found in Commits " + activeTimeline;
     }
 
     SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java
Patch:
@@ -194,7 +194,7 @@ protected Iterator<List<WriteStatus>> handleUpsertPartition(
         }
       }
     } catch (Throwable t) {
-      String msg = "Error upsetting bucketType " + bucketType + " for partition :" + partitionPath;
+      String msg = "Error upserting bucketType " + bucketType + " for partition :" + partitionPath;
       LOG.error(msg, t);
       throw new HoodieUpsertException(msg, t);
     }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestGcsEventsSource.java
Patch:
@@ -63,7 +63,7 @@ public static void beforeAll() throws Exception {
 
   @BeforeEach
   public void beforeEach() throws Exception {
-    schemaProvider = new FilebasedSchemaProvider(Helpers.setupSchemaOnDFS("delta-streamer-config", "gcs-metadata.avsc"), jsc);
+    schemaProvider = new FilebasedSchemaProvider(Helpers.setupSchemaOnDFS("streamer-config", "gcs-metadata.avsc"), jsc);
     MockitoAnnotations.initMocks(this);
 
     props = new TypedProperties();

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestS3EventsSource.java
Patch:
@@ -51,7 +51,7 @@ public void setup() throws Exception {
     this.dfsRoot = basePath + "/parquetFiles";
     this.fileSuffix = ".parquet";
     fs.mkdirs(new Path(dfsRoot));
-    schemaProvider = new FilebasedSchemaProvider(Helpers.setupSchemaOnDFS("delta-streamer-config", "s3-metadata.avsc"), jsc);
+    schemaProvider = new FilebasedSchemaProvider(Helpers.setupSchemaOnDFS("streamer-config", "s3-metadata.avsc"), jsc);
   }
 
   @AfterEach

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/TypeInfoLogicalTypeVisitor.java
Patch:
@@ -137,7 +137,7 @@ public TypeInfo visit(TimestampType timestampType) {
     int precision = timestampType.getPrecision();
     // see org.apache.hudi.hive.util.HiveSchemaUtil#convertField for details.
     // default supports timestamp
-    if (precision == 6) {
+    if (precision <= 6) {
       return TypeInfoFactory.timestampTypeInfo;
     } else {
       return TypeInfoFactory.longTypeInfo;

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRestoresCommand.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.testutils.HoodieMetadataTestTable;
 import org.apache.hudi.common.testutils.HoodieTestTable;
+import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
@@ -101,7 +102,7 @@ public void init() throws Exception {
             .build();
 
     HoodieTestTable hoodieTestTable = HoodieMetadataTestTable.of(metaClient, SparkHoodieBackedTableMetadataWriter.create(
-                    metaClient.getHadoopConf(), config, context))
+                    metaClient.getHadoopConf(), config, context), Option.of(context))
             .withPartitionMetaFiles(DEFAULT_PARTITION_PATHS)
             .addCommit("100")
             .withBaseFilesInPartitions(partitionAndFileId).getLeft()

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRollbacksCommand.java
Patch:
@@ -35,6 +35,7 @@
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.testutils.HoodieMetadataTestTable;
+import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -101,7 +102,7 @@ public void init() throws Exception {
         .withRollbackUsingMarkers(false)
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();
     HoodieMetadataTestTable.of(metaClient, SparkHoodieBackedTableMetadataWriter.create(
-            metaClient.getHadoopConf(), config, context))
+            metaClient.getHadoopConf(), config, context), Option.of(context))
         .withPartitionMetaFiles(DEFAULT_PARTITION_PATHS)
         .addCommit("100")
         .withBaseFilesInPartitions(partitionAndFileId).getLeft()

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java
Patch:
@@ -26,7 +26,6 @@
 import org.apache.hudi.avro.model.HoodieRestoreMetadata;
 import org.apache.hudi.avro.model.HoodieRollbackMetadata;
 import org.apache.hudi.client.transaction.TransactionManager;
-import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.table.HoodieTableConfig;
@@ -377,7 +376,7 @@ public void run() {
                 }
                 HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(
                     table.getActiveTimeline().getInstantDetails(instant).get(), HoodieCommitMetadata.class);
-                metadataWriter.update(commitMetadata, HoodieListData.eager(Collections.emptyList()), instant.getTimestamp());
+                metadataWriter.update(commitMetadata, context.emptyHoodieData(), instant.getTimestamp());
                 break;
               case CLEAN_ACTION:
                 HoodieCleanMetadata cleanMetadata = CleanerUtils.getCleanerMetadata(table.getMetaClient(), instant);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestSparkRDDWriteClient.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.embedded.EmbeddedTimelineService;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.data.HoodieData.HoodieDataCacheKey;
-import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -134,7 +133,7 @@ void testWriteClientReleaseResourcesShouldOnlyUnpersistRelevantRdds(HoodieTableT
     writeClient.startCommitWithTime(instant1);
     List<WriteStatus> writeStatuses = writeClient.insert(writeRecords, instant1).collect();
     assertNoWriteErrors(writeStatuses);
-    writeClient.commitStats(instant1, HoodieListData.eager(writeStatuses), writeStatuses.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
+    writeClient.commitStats(instant1, context().parallelize(writeStatuses, 1), writeStatuses.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
         Option.empty(), metaClient.getCommitActionType());
     writeClient.close();
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestConsistentBucketIndex.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.client.functional;
 
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -279,7 +278,7 @@ private List<WriteStatus> writeData(JavaRDD<HoodieRecord> records, String commit
     }
     org.apache.hudi.testutils.Assertions.assertNoWriteErrors(writeStatues);
     if (doCommit) {
-      boolean success = writeClient.commitStats(commitTime, HoodieListData.eager(writeStatues), writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
+      boolean success = writeClient.commitStats(commitTime, context.parallelize(writeStatues, 1), writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
           Option.empty(), metaClient.getCommitActionType());
       Assertions.assertTrue(success);
     }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -873,7 +873,7 @@ public void testMetadataTableWithPendingCompaction(boolean simulateFailedCompact
     java.nio.file.Path metaFilePath = parentPath.resolve(metadataCompactionInstant + HoodieTimeline.COMMIT_EXTENSION);
     java.nio.file.Path tempFilePath = FileCreateUtils.renameFileToTemp(metaFilePath, metadataCompactionInstant);
     metaClient.reloadActiveTimeline();
-    testTable = HoodieMetadataTestTable.of(metaClient, metadataWriter);
+    testTable = HoodieMetadataTestTable.of(metaClient, metadataWriter, Option.of(context));
     // this validation will exercise the code path where a compaction is inflight in metadata table, but still metadata based file listing should match non
     // metadata based file listing.
     validateMetadata(testTable);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieMetadataBase.java
Patch:
@@ -119,7 +119,7 @@ protected void initWriteConfigAndMetatableWriter(HoodieWriteConfig writeConfig,
       metadataWriter = SparkHoodieBackedTableMetadataWriter.create(hadoopConf, writeConfig, context);
       // reload because table configs could have been updated
       metaClient = HoodieTableMetaClient.reload(metaClient);
-      testTable = HoodieMetadataTestTable.of(metaClient, metadataWriter);
+      testTable = HoodieMetadataTestTable.of(metaClient, metadataWriter, Option.of(context));
     } else {
       testTable = HoodieTestTable.of(metaClient);
     }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestSparkConsistentBucketClustering.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.client.clustering.run.strategy.SparkConsistentBucketClusteringExecutionStrategy;
 import org.apache.hudi.client.clustering.update.strategy.SparkConsistentBucketDuplicateUpdateStrategy;
 import org.apache.hudi.common.config.HoodieStorageConfig;
-import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieConsistentHashingMetadata;
@@ -295,7 +294,7 @@ public void testConcurrentWrite() throws IOException {
     List<WriteStatus> writeStatues = writeData(writeTime, 2000, false);
     // Cannot schedule clustering if there is in-flight writer
     Assertions.assertFalse(writeClient.scheduleClustering(Option.empty()).isPresent());
-    Assertions.assertTrue(writeClient.commitStats(writeTime, HoodieListData.eager(writeStatues), writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
+    Assertions.assertTrue(writeClient.commitStats(writeTime, context.parallelize(writeStatues, 1), writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
         Option.empty(), metaClient.getCommitActionType()));
     metaClient = HoodieTableMetaClient.reload(metaClient);
 
@@ -331,7 +330,7 @@ private List<WriteStatus> writeData(String commitTime, int totalRecords, boolean
     List<WriteStatus> writeStatues = writeClient.upsert(writeRecords, commitTime).collect();
     org.apache.hudi.testutils.Assertions.assertNoWriteErrors(writeStatues);
     if (doCommit) {
-      Assertions.assertTrue(writeClient.commitStats(commitTime, HoodieListData.eager(writeStatues), writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
+      Assertions.assertTrue(writeClient.commitStats(commitTime, context.parallelize(writeStatues, 1), writeStatues.stream().map(WriteStatus::getStat).collect(Collectors.toList()),
           Option.empty(), metaClient.getCommitActionType()));
     }
     metaClient = HoodieTableMetaClient.reload(metaClient);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/TestHoodieTimelineArchiver.java
Patch:
@@ -140,7 +140,7 @@ private void initWriteConfigAndMetatableWriter(HoodieWriteConfig writeConfig, bo
       metadataWriter = SparkHoodieBackedTableMetadataWriter.create(hadoopConf, writeConfig, context);
       // reload because table configs could have been updated
       metaClient = HoodieTableMetaClient.reload(metaClient);
-      testTable = HoodieMetadataTestTable.of(metaClient, metadataWriter);
+      testTable = HoodieMetadataTestTable.of(metaClient, metadataWriter, Option.of(context));
     } else {
       testTable = HoodieTestTable.of(metaClient);
     }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableCompaction.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
-import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.model.DefaultHoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -128,7 +127,7 @@ public void testWriteDuringCompaction(String payloadClass) throws IOException {
     List<WriteStatus> writeStatuses = writeData(insertTime, 100, false);
     Assertions.assertEquals(200, readTableTotalRecordsNum());
     // commit the write. The records should be visible now even though the compaction does not complete.
-    client.commitStats(insertTime, HoodieListData.eager(writeStatuses), writeStatuses.stream().map(WriteStatus::getStat)
+    client.commitStats(insertTime, context().parallelize(writeStatuses, 1), writeStatuses.stream().map(WriteStatus::getStat)
         .collect(Collectors.toList()), Option.empty(), metaClient.getCommitActionType());
     Assertions.assertEquals(300, readTableTotalRecordsNum());
     // after the compaction, total records should remain the same
@@ -193,7 +192,7 @@ private List<WriteStatus> writeData(String instant, int numRecords, boolean doCo
     org.apache.hudi.testutils.Assertions.assertNoWriteErrors(writeStatuses);
     if (doCommit) {
       List<HoodieWriteStat> writeStats = writeStatuses.stream().map(WriteStatus::getStat).collect(Collectors.toList());
-      boolean committed = client.commitStats(instant, HoodieListData.eager(writeStatuses), writeStats, Option.empty(), metaClient.getCommitActionType());
+      boolean committed = client.commitStats(instant, context().parallelize(writeStatuses, 1), writeStats, Option.empty(), metaClient.getCommitActionType());
       Assertions.assertTrue(committed);
     }
     metaClient = HoodieTableMetaClient.reload(metaClient);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/offlinejob/HoodieOfflineJobTestBase.java
Patch:
@@ -20,7 +20,6 @@
 
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -89,7 +88,7 @@ protected List<WriteStatus> writeData(boolean isUpsert, String instant, int numR
     org.apache.hudi.testutils.Assertions.assertNoWriteErrors(writeStatuses);
     if (doCommit) {
       List<HoodieWriteStat> writeStats = writeStatuses.stream().map(WriteStatus::getStat).collect(Collectors.toList());
-      boolean committed = client.commitStats(instant, HoodieListData.eager(writeStatuses), writeStats, Option.empty(), metaClient.getCommitActionType());
+      boolean committed = client.commitStats(instant, context.parallelize(writeStatuses, 1), writeStats, Option.empty(), metaClient.getCommitActionType());
       Assertions.assertTrue(committed);
     }
     metaClient = HoodieTableMetaClient.reload(metaClient);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java
Patch:
@@ -874,7 +874,8 @@ public boolean rollback(final String commitInstantTime, Option<HoodiePendingRoll
                 + "(exists in active timeline: %s), with rollback plan: %s",
             rollbackInstantTime, commitInstantOpt.isPresent(), pendingRollbackInfo.isPresent()));
         Option<HoodieRollbackPlan> rollbackPlanOption = pendingRollbackInfo.map(entry -> Option.of(entry.getRollbackPlan()))
-            .orElseGet(() -> table.scheduleRollback(context, rollbackInstantTime, commitInstantOpt.get(), false, config.shouldRollbackUsingMarkers()));
+            .orElseGet(() -> table.scheduleRollback(context, rollbackInstantTime, commitInstantOpt.get(), false, config.shouldRollbackUsingMarkers(),
+                false));
         if (rollbackPlanOption.isPresent()) {
           // There can be a case where the inflight rollback failed after the instant files
           // are deleted for commitInstantTime, so that commitInstantOpt is empty as it is

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/CopyOnWriteRestoreActionExecutor.java
Patch:
@@ -47,7 +47,7 @@ protected HoodieRollbackMetadata rollbackInstant(HoodieInstant instantToRollback
     }
     table.getMetaClient().reloadActiveTimeline();
     String newInstantTime = HoodieActiveTimeline.createNewInstantTime();
-    table.scheduleRollback(context, newInstantTime, instantToRollback, false, false);
+    table.scheduleRollback(context, newInstantTime, instantToRollback, false, false, true);
     table.getMetaClient().reloadActiveTimeline();
     CopyOnWriteRollbackActionExecutor rollbackActionExecutor = new CopyOnWriteRollbackActionExecutor(
         context,

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/MergeOnReadRestoreActionExecutor.java
Patch:
@@ -51,7 +51,7 @@ protected HoodieRollbackMetadata rollbackInstant(HoodieInstant instantToRollback
     }
     table.getMetaClient().reloadActiveTimeline();
     String instantTime = HoodieActiveTimeline.createNewInstantTime();
-    table.scheduleRollback(context, instantTime, instantToRollback, false, false);
+    table.scheduleRollback(context, instantTime, instantToRollback, false, false, true);
     table.getMetaClient().reloadActiveTimeline();
     MergeOnReadRollbackActionExecutor rollbackActionExecutor = new MergeOnReadRollbackActionExecutor(
         context,

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/CopyOnWriteRollbackActionExecutor.java
Patch:
@@ -44,7 +44,7 @@ public CopyOnWriteRollbackActionExecutor(HoodieEngineContext context,
                                            HoodieInstant commitInstant,
                                            boolean deleteInstants,
                                            boolean skipLocking) {
-    super(context, config, table, instantTime, commitInstant, deleteInstants, skipLocking);
+    super(context, config, table, instantTime, commitInstant, deleteInstants, skipLocking, false);
   }
 
   public CopyOnWriteRollbackActionExecutor(HoodieEngineContext context,
@@ -55,7 +55,7 @@ public CopyOnWriteRollbackActionExecutor(HoodieEngineContext context,
                                            boolean deleteInstants,
                                            boolean skipTimelinePublish,
                                            boolean skipLocking) {
-    super(context, config, table, instantTime, commitInstant, deleteInstants, skipTimelinePublish, skipLocking);
+    super(context, config, table, instantTime, commitInstant, deleteInstants, skipTimelinePublish, skipLocking, false);
   }
 
   @Override

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/MergeOnReadRollbackActionExecutor.java
Patch:
@@ -44,7 +44,7 @@ public MergeOnReadRollbackActionExecutor(HoodieEngineContext context,
                                            HoodieInstant commitInstant,
                                            boolean deleteInstants,
                                            boolean skipLocking) {
-    super(context, config, table, instantTime, commitInstant, deleteInstants, skipLocking);
+    super(context, config, table, instantTime, commitInstant, deleteInstants, skipLocking, false);
   }
 
   public MergeOnReadRollbackActionExecutor(HoodieEngineContext context,
@@ -55,7 +55,7 @@ public MergeOnReadRollbackActionExecutor(HoodieEngineContext context,
                                            boolean deleteInstants,
                                            boolean skipTimelinePublish,
                                            boolean skipLocking) {
-    super(context, config, table, instantTime, commitInstant, deleteInstants, skipTimelinePublish, skipLocking);
+    super(context, config, table, instantTime, commitInstant, deleteInstants, skipTimelinePublish, skipLocking, false);
   }
 
   @Override

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/ZeroToOneUpgradeHandler.java
Patch:
@@ -116,7 +116,7 @@ protected void recreateMarkers(final String commitInstantTime,
 
   List<HoodieRollbackStat> getListBasedRollBackStats(HoodieTable<?, ?, ?, ?> table, HoodieEngineContext context, Option<HoodieInstant> commitInstantOpt) {
     List<HoodieRollbackRequest> hoodieRollbackRequests =
-        new ListingBasedRollbackStrategy(table, context, table.getConfig(), commitInstantOpt.get().getTimestamp())
+        new ListingBasedRollbackStrategy(table, context, table.getConfig(), commitInstantOpt.get().getTimestamp(), false)
             .getRollbackRequests(commitInstantOpt.get());
     return new BaseRollbackHelper(table.getMetaClient(), table.getConfig())
         .collectRollbackStats(context, commitInstantOpt.get(), hoodieRollbackRequests);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java
Patch:
@@ -334,9 +334,9 @@ public Option<HoodieCleanerPlan> scheduleCleaning(HoodieEngineContext context, S
 
   @Override
   public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context, String instantTime, HoodieInstant instantToRollback,
-                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
+                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers, boolean isRestore) {
     return new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish,
-        shouldRollbackUsingMarkers).execute();
+        shouldRollbackUsingMarkers, isRestore).execute();
   }
 
   @Override

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java
Patch:
@@ -146,9 +146,9 @@ public Iterator<List<WriteStatus>> handleInsertsForLogCompaction(String instantT
 
   @Override
   public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context, String instantTime, HoodieInstant instantToRollback,
-                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
+                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers, boolean isRestore) {
     return new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish,
-        shouldRollbackUsingMarkers).execute();
+        shouldRollbackUsingMarkers, isRestore).execute();
   }
 
   @Override

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java
Patch:
@@ -204,9 +204,9 @@ public void rollbackBootstrap(HoodieEngineContext context,
 
   @Override
   public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context, String instantTime, HoodieInstant instantToRollback,
-                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
+                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers, boolean isRestore) {
     return new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish,
-        shouldRollbackUsingMarkers).execute();
+        shouldRollbackUsingMarkers, isRestore).execute();
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java
Patch:
@@ -203,9 +203,10 @@ public Option<HoodieCleanerPlan> scheduleCleaning(HoodieEngineContext context, S
   @Override
   public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context,
                                                      String instantTime,
-                                                     HoodieInstant instantToRollback, boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
+                                                     HoodieInstant instantToRollback, boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers,
+                                                     boolean isRestore) {
     return new BaseRollbackPlanActionExecutor<>(context, config, this, instantTime, instantToRollback, skipTimelinePublish,
-        shouldRollbackUsingMarkers).execute();
+        shouldRollbackUsingMarkers, isRestore).execute();
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java
Patch:
@@ -169,9 +169,10 @@ public void rollbackBootstrap(HoodieEngineContext context, String instantTime) {
   @Override
   public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context,
                                                      String instantTime,
-                                                     HoodieInstant instantToRollback, boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
+                                                     HoodieInstant instantToRollback, boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers,
+                                                     boolean isRestore) {
     return new BaseRollbackPlanActionExecutor<>(context, config, this, instantTime, instantToRollback, skipTimelinePublish,
-        shouldRollbackUsingMarkers).execute();
+        shouldRollbackUsingMarkers, isRestore).execute();
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java
Patch:
@@ -147,7 +147,7 @@ private HoodieData<HoodieRecord<T>> clusteringHandleUpdate(HoodieData<HoodieReco
           .collect(Collectors.toSet());
       pendingClusteringInstantsToRollback.forEach(instant -> {
         String commitTime = HoodieActiveTimeline.createNewInstantTime();
-        table.scheduleRollback(context, commitTime, instant, false, config.shouldRollbackUsingMarkers());
+        table.scheduleRollback(context, commitTime, instant, false, config.shouldRollbackUsingMarkers(), false);
         table.rollback(context, commitTime, instant, true, true);
       });
       table.getMetaClient().reloadActiveTimeline();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestSavepointRestoreCopyOnWrite.java
Patch:
@@ -158,7 +158,7 @@ void testCleaningRollbackInstants(boolean commitRollback) throws Exception {
         HoodieInstant pendingInstant = metaClient.getActiveTimeline().filterPendingExcludingCompaction()
             .lastInstant().orElseThrow(() -> new HoodieException("Pending instant does not exist"));
         HoodieSparkTable.create(client.getConfig(), context)
-            .scheduleRollback(context, HoodieActiveTimeline.createNewInstantTime(), pendingInstant, false, true);
+            .scheduleRollback(context, HoodieActiveTimeline.createNewInstantTime(), pendingInstant, false, true, false);
       }
       Option<String> rollbackInstant = metaClient.reloadActiveTimeline().getRollbackTimeline().lastInstant().map(HoodieInstant::getTimestamp);
       assertTrue(rollbackInstant.isPresent(), "The latest instant should be a rollback");

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java
Patch:
@@ -828,7 +828,7 @@ public void testCleanMarkerDataFilesOnRollback() throws Exception {
         new HoodieInstant(State.REQUESTED, HoodieTimeline.COMMIT_ACTION, "001"), Option.empty());
     metaClient.reloadActiveTimeline();
     HoodieInstant rollbackInstant = new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, "001");
-    table.scheduleRollback(context, "002", rollbackInstant, false, config.shouldRollbackUsingMarkers());
+    table.scheduleRollback(context, "002", rollbackInstant, false, config.shouldRollbackUsingMarkers(), false);
     table.rollback(context, "002", rollbackInstant, true, false);
     final int numTempFilesAfter = testTable.listAllFilesInTempFolder().length;
     assertEquals(0, numTempFilesAfter, "All temp files are deleted.");

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/rollback/TestMergeOnReadRollbackActionExecutor.java
Patch:
@@ -107,7 +107,7 @@ public void testMergeOnReadRollbackActionExecutor(boolean isUsingMarkers) throws
     HoodieInstant rollBackInstant = new HoodieInstant(isUsingMarkers, HoodieTimeline.DELTA_COMMIT_ACTION, "002");
     BaseRollbackPlanActionExecutor mergeOnReadRollbackPlanActionExecutor =
         new BaseRollbackPlanActionExecutor(context, cfg, table, "003", rollBackInstant, false,
-            cfg.shouldRollbackUsingMarkers());
+            cfg.shouldRollbackUsingMarkers(), false);
     mergeOnReadRollbackPlanActionExecutor.execute().get();
     MergeOnReadRollbackActionExecutor mergeOnReadRollbackActionExecutor = new MergeOnReadRollbackActionExecutor(
         context,
@@ -253,7 +253,7 @@ public void testRollbackForCanIndexLogFile() throws IOException {
     HoodieInstant rollBackInstant = new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.DELTA_COMMIT_ACTION, "002");
     BaseRollbackPlanActionExecutor mergeOnReadRollbackPlanActionExecutor =
         new BaseRollbackPlanActionExecutor(context, cfg, table, "003", rollBackInstant, false,
-            cfg.shouldRollbackUsingMarkers());
+            cfg.shouldRollbackUsingMarkers(), false);
     mergeOnReadRollbackPlanActionExecutor.execute().get();
     MergeOnReadRollbackActionExecutor mergeOnReadRollbackActionExecutor = new MergeOnReadRollbackActionExecutor(
         context,

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/strategy/TestHoodieCompactionStrategy.java
Patch:
@@ -87,7 +87,7 @@ public void testBoundedIOSimple() {
     Long returnedSize = returned.stream().map(s -> s.getMetrics().get(BoundedIOCompactionStrategy.TOTAL_IO_MB))
         .map(Double::longValue).reduce(Long::sum).orElse(0L);
     assertEquals(610, (long) returnedSize,
-        "Should chose the first 2 compactions which should result in a total IO of 690 MB");
+        "Should chose the first 2 compactions which should result in a total IO of 610 MB");
   }
 
   @Test

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/prune/DataPruner.java
Patch:
@@ -116,6 +116,7 @@ private static Object getValAsJavaObj(RowData indexRow, int pos, LogicalType col
         return indexRow.getTimestamp(pos, tsType.getPrecision()).getMillisecond();
       case TIME_WITHOUT_TIME_ZONE:
       case DATE:
+      case BIGINT:
         return indexRow.getLong(pos);
       // NOTE: All integral types of size less than Int are encoded as Ints in MT
       case BOOLEAN:

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestGlobalIndexEnableUpdatePartitions.java
Patch:
@@ -65,8 +65,8 @@ private static Stream<Arguments> getTableTypeAndIndexType() {
         Arguments.of(COPY_ON_WRITE, GLOBAL_BLOOM),
         Arguments.of(COPY_ON_WRITE, RECORD_INDEX),
         Arguments.of(MERGE_ON_READ, GLOBAL_SIMPLE),
-        Arguments.of(MERGE_ON_READ, GLOBAL_BLOOM)
-    // Arguments.of(MERGE_ON_READ, RECORD_INDEX)
+        Arguments.of(MERGE_ON_READ, GLOBAL_BLOOM),
+        Arguments.of(MERGE_ON_READ, RECORD_INDEX)
     );
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordGlobalLocation.java
Patch:
@@ -32,7 +32,8 @@ public final class HoodieRecordGlobalLocation extends HoodieRecordLocation {
 
   private String partitionPath;
 
-  public HoodieRecordGlobalLocation() {}
+  public HoodieRecordGlobalLocation() {
+  }
 
   public HoodieRecordGlobalLocation(String partitionPath, String instantTime, String fileId) {
     super(instantTime, fileId);
@@ -98,7 +99,7 @@ public HoodieRecordGlobalLocation copy(String partitionPath) {
   }
 
   @Override
-  public final void write(Kryo kryo, Output output) {
+  public void write(Kryo kryo, Output output) {
     super.write(kryo, output);
 
     kryo.writeObjectOrNull(output, partitionPath, String.class);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -821,7 +821,7 @@ private Set<String> getMetadataPartitionsToUpdate() {
       return partitionsToUpdate;
     }
     // fallback to all enabled partitions if table config returned no partitions
-    LOG.warn("There are no partitions to update according to table config. Falling back to enabled partition types in the write config.");
+    LOG.debug("There are no partitions to update according to table config. Falling back to enabled partition types in the write config.");
     return getEnabledPartitionTypes().stream().map(MetadataPartitionType::getPartitionPath).collect(Collectors.toSet());
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -76,7 +76,7 @@ public class HoodieActiveTimeline extends HoodieDefaultTimeline {
       REQUESTED_INDEX_COMMIT_EXTENSION, INFLIGHT_INDEX_COMMIT_EXTENSION, INDEX_COMMIT_EXTENSION,
       REQUESTED_SAVE_SCHEMA_ACTION_EXTENSION, INFLIGHT_SAVE_SCHEMA_ACTION_EXTENSION, SAVE_SCHEMA_ACTION_EXTENSION));
 
-  private static final Set<String> NOT_PARSABLE_TIMESTAMPS = new HashSet<String>(3) {{
+  public static final Set<String> NOT_PARSABLE_TIMESTAMPS = new HashSet<String>(3) {{
       add(HoodieTimeline.INIT_INSTANT_TS);
       add(HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS);
       add(HoodieTimeline.FULL_BOOTSTRAP_INSTANT_TS);
@@ -481,7 +481,6 @@ public HoodieInstant transitionLogCompactionInflightToComplete(HoodieInstant inf
     return commitInstant;
   }
 
-
   //-----------------------------------------------------------------
   //      END - COMPACTION RELATED META-DATA MANAGEMENT
   //-----------------------------------------------------------------

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestCleanPlanExecutor.java
Patch:
@@ -41,7 +41,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.metadata.HoodieTableMetadataWriter;
 import org.apache.hudi.metadata.SparkHoodieBackedTableMetadataWriter;
-import org.apache.hudi.table.TestCleaner;
+import org.apache.hudi.testutils.HoodieCleanerTestBase;
 
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.params.ParameterizedTest;
@@ -72,7 +72,7 @@
 /**
  * Tests covering different clean plan policies/strategies.
  */
-public class TestCleanPlanExecutor extends TestCleaner {
+public class TestCleanPlanExecutor extends HoodieCleanerTestBase {
 
   @Test
   public void testInvalidCleaningTriggerStrategy() {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java
Patch:
@@ -521,6 +521,8 @@ private MergeOnReadInputFormat mergeOnReadInputFormat(
         this.schema.getColumnDataTypes().toArray(new DataType[0]),
         this.requiredPos,
         this.conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME),
+        this.conf.getString(FlinkOptions.PARTITION_PATH_FIELD),
+        this.conf.getBoolean(FlinkOptions.HIVE_STYLE_PARTITIONING),
         this.limit == NO_LIMIT_CONSTANT ? Long.MAX_VALUE : this.limit, // ParquetInputFormat always uses the limit value
         getParquetConf(this.conf, this.hadoopConf),
         this.conf.getBoolean(FlinkOptions.UTC_TIMEZONE),

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java
Patch:
@@ -56,11 +56,11 @@ public interface HoodieTimeline extends Serializable {
   String COMPACTION_ACTION = "compaction";
   String LOG_COMPACTION_ACTION = "logcompaction";
   String REQUESTED_EXTENSION = ".requested";
+  String COMPLETED_EXTENSION = ".completed";
   String RESTORE_ACTION = "restore";
   String INDEXING_ACTION = "indexing";
   // only for schema save
   String SCHEMA_COMMIT_ACTION = "schemacommit";
-
   String[] VALID_ACTIONS_IN_TIMELINE = {COMMIT_ACTION, DELTA_COMMIT_ACTION,
       CLEAN_ACTION, SAVEPOINT_ACTION, RESTORE_ACTION, ROLLBACK_ACTION,
       COMPACTION_ACTION, REPLACE_COMMIT_ACTION, INDEXING_ACTION};
@@ -81,6 +81,7 @@ public interface HoodieTimeline extends Serializable {
   String REQUESTED_ROLLBACK_EXTENSION = "." + ROLLBACK_ACTION + REQUESTED_EXTENSION;
   String INFLIGHT_SAVEPOINT_EXTENSION = "." + SAVEPOINT_ACTION + INFLIGHT_EXTENSION;
   String REQUESTED_COMPACTION_SUFFIX = StringUtils.join(COMPACTION_ACTION, REQUESTED_EXTENSION);
+  String COMPLETED_COMPACTION_SUFFIX = StringUtils.join(COMPACTION_ACTION, COMPLETED_EXTENSION);
   String REQUESTED_COMPACTION_EXTENSION = StringUtils.join(".", REQUESTED_COMPACTION_SUFFIX);
   String INFLIGHT_COMPACTION_EXTENSION = StringUtils.join(".", COMPACTION_ACTION, INFLIGHT_EXTENSION);
   String REQUESTED_RESTORE_EXTENSION = "." + RESTORE_ACTION + REQUESTED_EXTENSION;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -28,11 +28,11 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.clustering.plan.strategy.SparkSingleFileSortPlanStrategy;
 import org.apache.hudi.client.clustering.run.strategy.SparkSingleFileSortExecutionStrategy;
-import org.apache.hudi.client.transaction.lock.InProcessLockProvider;
 import org.apache.hudi.client.clustering.update.strategy.SparkRejectUpdateStrategy;
 import org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass;
 import org.apache.hudi.client.transaction.IngestionPrimaryWriterBasedConflictResolutionStrategy;
 import org.apache.hudi.client.transaction.SimpleConcurrentFileWritesConflictResolutionStrategy;
+import org.apache.hudi.client.transaction.lock.InProcessLockProvider;
 import org.apache.hudi.client.validator.SparkPreCommitValidator;
 import org.apache.hudi.client.validator.SqlQueryEqualityPreCommitValidator;
 import org.apache.hudi.client.validator.SqlQuerySingleResultPreCommitValidator;
@@ -100,8 +100,8 @@
 import org.apache.hudi.exception.HoodieInsertException;
 import org.apache.hudi.exception.HoodieUpsertException;
 import org.apache.hudi.exception.HoodieValidationException;
-import org.apache.hudi.execution.bulkinsert.RDDCustomColumnsSortPartitioner;
 import org.apache.hudi.exception.HoodieWriteConflictException;
+import org.apache.hudi.execution.bulkinsert.RDDCustomColumnsSortPartitioner;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.index.HoodieIndex.IndexType;
 import org.apache.hudi.io.HoodieMergeHandle;

File: hudi-common/src/main/java/org/apache/hudi/common/table/cdc/HoodieCDCExtractor.java
Patch:
@@ -265,7 +265,7 @@ private HoodieCDCFileSplit parseWriteStat(
               new HoodieIOException("Can not get the previous version of the base file")
           );
           FileSlice beforeFileSlice = new FileSlice(fileGroupId, writeStat.getPrevCommit(), beforeBaseFile, Collections.emptyList());
-          cdcFileSplit = new HoodieCDCFileSplit(instantTs, BASE_FILE_DELETE, new ArrayList<>(), Option.empty(), Option.of(beforeFileSlice));
+          cdcFileSplit = new HoodieCDCFileSplit(instantTs, BASE_FILE_DELETE, new ArrayList<>(), Option.of(beforeFileSlice), Option.empty());
         } else if (writeStat.getNumUpdateWrites() == 0L && writeStat.getNumDeletes() == 0
             && writeStat.getNumWrites() == writeStat.getNumInserts()) {
           // all the records in this file are new.

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -223,7 +223,7 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti
     HoodieTable table = createTable(config, hadoopConf);
     HoodieCommitMetadata metadata = CommitUtils.buildMetadata(stats, partitionToReplaceFileIds,
         extraMetadata, operationType, config.getWriteSchema(), commitActionType);
-    HoodieInstant inflightInstant = new HoodieInstant(State.INFLIGHT, table.getMetaClient().getCommitActionType(), instantTime);
+    HoodieInstant inflightInstant = new HoodieInstant(State.INFLIGHT, commitActionType, instantTime);
     HeartbeatUtils.abortIfHeartbeatExpired(instantTime, table, heartbeatClient, config);
     this.txnManager.beginTransaction(Option.of(inflightInstant),
         lastCompletedTxnAndMetadata.isPresent() ? Option.of(lastCompletedTxnAndMetadata.get().getLeft()) : Option.empty());

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -190,7 +190,7 @@ private FlinkOptions() {
       .booleanType()
       .defaultValue(true)
       .withFallbackKeys(HoodieMetadataConfig.ENABLE.key())
-      .withDescription("Enable the internal metadata table which serves table metadata like level file listings, default disabled");
+      .withDescription("Enable the internal metadata table which serves table metadata like level file listings, default enabled");
 
   public static final ConfigOption<Integer> METADATA_COMPACTION_DELTA_COMMITS = ConfigOptions
       .key("metadata.compaction.delta_commits")

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -456,7 +456,7 @@ public void performMergeDataValidationCheck(WriteStatus writeStatus) {
     }
 
     long oldNumWrites = 0;
-    try (HoodieFileReader reader = HoodieFileReaderFactory.getReaderFactory(this.config.getRecordMerger().getRecordType()).getFileReader(hoodieTable.getHadoopConf(), oldFilePath)) {
+    try (HoodieFileReader reader = HoodieFileReaderFactory.getReaderFactory(this.recordMerger.getRecordType()).getFileReader(hoodieTable.getHadoopConf(), oldFilePath)) {
       oldNumWrites = reader.getTotalRecords();
     } catch (IOException e) {
       throw new HoodieUpsertException("Failed to check for merge data validation", e);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieWriteHelper.java
Patch:
@@ -56,7 +56,7 @@ protected HoodieData<HoodieRecord<T>> tag(HoodieData<HoodieRecord<T>> dedupedRec
   }
 
   @Override
-  protected HoodieData<HoodieRecord<T>> doDeduplicateRecords(
+  public HoodieData<HoodieRecord<T>> deduplicateRecords(
       HoodieData<HoodieRecord<T>> records, HoodieIndex<?, ?> index, int parallelism, String schemaStr, TypedProperties props, HoodieRecordMerger merger) {
     boolean isIndexingGlobal = index.isGlobal();
     final SerializableSchema schema = new SerializableSchema(schemaStr);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkWriteHelper.java
Patch:
@@ -91,7 +91,7 @@ protected List<HoodieRecord<T>> tag(List<HoodieRecord<T>> dedupedRecords, Hoodie
   }
 
   @Override
-  protected List<HoodieRecord<T>> doDeduplicateRecords(
+  public List<HoodieRecord<T>> deduplicateRecords(
       List<HoodieRecord<T>> records, HoodieIndex<?, ?> index, int parallelism, String schemaStr, TypedProperties props, HoodieRecordMerger merger) {
     // If index used is global, then records are expected to differ in their partitionPath
     Map<Object, List<HoodieRecord<T>>> keyedRecords = records.stream()
@@ -110,7 +110,7 @@ protected List<HoodieRecord<T>> doDeduplicateRecords(
       // we cannot allow the user to change the key or partitionPath, since that will affect
       // everything
       // so pick it from one of the records.
-      boolean choosePrev = rec1 == reducedRecord;
+      boolean choosePrev = rec1.getData() == reducedRecord.getData();
       HoodieKey reducedKey = choosePrev ? rec1.getKey() : rec2.getKey();
       HoodieOperation operation = choosePrev ? rec1.getOperation() : rec2.getOperation();
       HoodieRecord<T> hoodieRecord = reducedRecord.newInstance(reducedKey, operation);

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaWriteHelper.java
Patch:
@@ -58,7 +58,7 @@ protected List<HoodieRecord<T>> tag(List<HoodieRecord<T>> dedupedRecords, Hoodie
   }
 
   @Override
-  protected List<HoodieRecord<T>> doDeduplicateRecords(
+  public List<HoodieRecord<T>> deduplicateRecords(
       List<HoodieRecord<T>> records, HoodieIndex<?, ?> index, int parallelism, String schemaStr, TypedProperties props, HoodieRecordMerger merger) {
     boolean isIndexingGlobal = index.isGlobal();
     Map<Object, List<Pair<Object, HoodieRecord<T>>>> keyedRecords = records.stream().map(record -> {

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java
Patch:
@@ -20,7 +20,6 @@
 
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.DeleteRecord;
-import org.apache.hudi.common.model.HoodieAvroRecordMerger;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodiePayloadProps;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -60,7 +59,6 @@
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
-import java.util.Properties;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.function.Function;
@@ -167,7 +165,7 @@ protected AbstractHoodieLogRecordReader(FileSystem fs, String basePath, List<Str
     this.payloadClassFQN = tableConfig.getPayloadClass();
     this.preCombineField = tableConfig.getPreCombineField();
     // Log scanner merge log with precombine
-    TypedProperties props = HoodieAvroRecordMerger.Config.withLegacyOperatingModePreCombining(new Properties());
+    TypedProperties props = new TypedProperties();
     if (this.preCombineField != null) {
       props.setProperty(HoodiePayloadProps.PAYLOAD_ORDERING_FIELD_PROP_KEY, this.preCombineField);
     }

File: hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroParquetReader.java
Patch:
@@ -47,8 +47,6 @@
 
 /**
  * {@link HoodieFileReader} implementation for parquet format.
- *
- * @param <R> Record implementation that permits field access by integer index.
  */
 public class HoodieAvroParquetReader extends HoodieAvroFileReaderBase {
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataLogRecordReader.java
Patch:
@@ -20,7 +20,6 @@
 
 import org.apache.avro.Schema;
 import org.apache.hadoop.fs.FileSystem;
-import org.apache.hudi.common.model.HoodieAvroRecordMerger;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;
 import org.apache.hudi.common.table.log.InstantRange;
@@ -136,10 +135,9 @@ public static class Builder {
     private final HoodieMergedLogRecordScanner.Builder scannerBuilder =
         new HoodieMergedLogRecordScanner.Builder()
             .withKeyFiledOverride(HoodieMetadataPayload.KEY_FIELD_NAME)
-            // NOTE: Merging of Metadata Table's records is currently handled using {@code HoodieAvroRecordMerger}
+            // NOTE: Merging of Metadata Table's records is currently handled using {@code HoodiePreCombineAvroRecordMerger}
             //       for compatibility purposes; In the future it {@code HoodieMetadataPayload} semantic
             //       will be migrated to its own custom instance of {@code RecordMerger}
-            .withRecordMerger(new HoodieAvroRecordMerger())
             .withReadBlocksLazily(true)
             .withReverseReader(false)
             .withOperationField(false);

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestHoodieRecordUtils.java
Patch:
@@ -27,7 +27,7 @@
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
 
-class HoodieRecordUtilsTest {
+class TestHoodieRecordUtils {
 
   @Test
   void loadHoodieMerge() {

File: hudi-examples/hudi-examples-flink/src/test/java/org/apache/hudi/examples/quickstart/TestQuickstartData.java
Patch:
@@ -38,9 +38,7 @@
 import org.apache.hudi.common.config.HoodieCommonConfig;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieAvroRecord;
-import org.apache.hudi.common.model.HoodieAvroRecordMerger;
 import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;
-import org.apache.hudi.common.util.HoodieRecordUtils;
 import org.apache.hudi.examples.quickstart.utils.QuickstartConfigurations;
 import org.apache.parquet.Strings;
 import org.apache.parquet.avro.AvroParquetReader;
@@ -362,7 +360,6 @@ private static HoodieMergedLogRecordScanner getScanner(
         .withSpillableMapBasePath("/tmp/")
         .withDiskMapType(HoodieCommonConfig.SPILLABLE_DISK_MAP_TYPE.defaultValue())
         .withBitCaskDiskMapCompressionEnabled(HoodieCommonConfig.DISK_MAP_BITCASK_COMPRESSION_ENABLED.defaultValue())
-        .withRecordMerger(HoodieRecordUtils.loadRecordMerger(HoodieAvroRecordMerger.class.getName()))
         .build();
   }
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
+import org.apache.hudi.common.util.HoodieRecordUtils;
 import org.apache.hudi.common.util.ObjectSizeCalculator;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.configuration.FlinkOptions;
@@ -201,7 +202,7 @@ private void initWriteFunction() {
   }
 
   private void initMergeClass() {
-    recordMerger = writeClient.getConfig().getRecordMerger();
+    recordMerger = HoodieRecordUtils.mergerToPreCombineMode(writeClient.getConfig().getRecordMerger());
     LOG.info("init hoodie merge with class [{}]", recordMerger.getClass().getName());
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -1862,7 +1862,7 @@ public void testMultiWriterForDoubleLocking() throws Exception {
     SparkRDDWriteClient writeClient = new SparkRDDWriteClient(engineContext, writeConfig);
     String partitionPath = dataGen.getPartitionPaths()[0];
     for (int j = 0; j < 6; j++) {
-      String newCommitTime = "000000" + j;
+      String newCommitTime = HoodieActiveTimeline.createNewInstantTime();
       List<HoodieRecord> records = dataGen.generateInsertsForPartition(newCommitTime, 100, partitionPath);
       writeClient.startCommitWithTime(newCommitTime);
       JavaRDD writeStatuses = writeClient.insert(jsc.parallelize(records, 1), newCommitTime);

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstant.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.common.table.timeline;
 
 import org.apache.hudi.common.util.StringUtils;
-
 import org.apache.hadoop.fs.FileStatus;
 
 import java.io.Serializable;

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTablePreCommitFileSystemView.java
Patch:
@@ -39,7 +39,7 @@
  *
  */
 public class HoodieTablePreCommitFileSystemView {
-  
+
   private Map<String, List<String>> partitionToReplaceFileIds;
   private List<HoodieWriteStat> filesWritten;
   private String preCommitInstantTime;

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestTable.java
Patch:
@@ -65,8 +65,6 @@
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 import java.io.IOException;
 import java.nio.file.Files;
@@ -133,7 +131,6 @@ public class HoodieTestTable {
   public static final String PHONY_TABLE_SCHEMA =
       "{\"namespace\": \"org.apache.hudi.avro.model\", \"type\": \"record\", \"name\": \"PhonyRecord\", \"fields\": []}";
 
-  private static final Logger LOG = LoggerFactory.getLogger(HoodieTestTable.class);
   private static final Random RANDOM = new Random();
 
   protected static HoodieTestTableState testTableState;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/HoodieSparkFileReaderFactory.java
Patch:
@@ -35,6 +35,9 @@ protected HoodieFileReader newParquetFileReader(Configuration conf, Path path) {
     conf.setIfUnset(SQLConf.CASE_SENSITIVE().key(), SQLConf.CASE_SENSITIVE().defaultValueString());
     // Using string value of this conf to preserve compatibility across spark versions.
     conf.setIfUnset("spark.sql.legacy.parquet.nanosAsLong", "false");
+    // This is a required config since Spark 3.4.0: SQLConf.PARQUET_INFER_TIMESTAMP_NTZ_ENABLED
+    // Using string value of this conf to preserve compatibility across spark versions.
+    conf.setIfUnset("spark.sql.parquet.inferTimestampNTZ.enabled", "true");
     return new HoodieSparkParquetReader(conf, path);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java
Patch:
@@ -341,12 +341,12 @@ protected Stream<HoodieInstant> filterInstantsByAction(String action) {
 
   @Override
   public boolean empty() {
-    return getInstants().isEmpty();
+    return instants.isEmpty();
   }
 
   @Override
   public int countInstants() {
-    return getInstants().size();
+    return instants.size();
   }
 
   @Override

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestTimelineUtils.java
Patch:
@@ -290,7 +290,7 @@ public void testGetCommitsTimelineAfter() throws IOException {
             new HoodieInstant(COMPLETED, COMMIT_ACTION, "009"),
             new HoodieInstant(COMPLETED, COMMIT_ACTION, "011"),
             new HoodieInstant(COMPLETED, COMMIT_ACTION, "012")),
-        TimelineUtils.getCommitsTimelineAfter(mockMetaClient, startTs, startTs));
+        TimelineUtils.getCommitsTimelineAfter(mockMetaClient, startTs, Option.of(startTs)));
     verify(mockMetaClient, never()).getArchivedTimeline(any());
 
     // Should load both archived and active timeline
@@ -311,7 +311,7 @@ public void testGetCommitsTimelineAfter() throws IOException {
             new HoodieInstant(COMPLETED, COMMIT_ACTION, "010", "010"),
             new HoodieInstant(COMPLETED, COMMIT_ACTION, "011", "011"),
             new HoodieInstant(COMPLETED, COMMIT_ACTION, "012", "012")),
-        TimelineUtils.getCommitsTimelineAfter(mockMetaClient, startTs, startTs));
+        TimelineUtils.getCommitsTimelineAfter(mockMetaClient, startTs, Option.of(startTs)));
     verify(mockMetaClient, times(1)).getArchivedTimeline(any());
   }
 

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/HoodieSyncClient.java
Patch:
@@ -90,7 +90,7 @@ public boolean isBootstrap() {
    */
   public Set<String> getDroppedPartitionsSince(Option<String> lastCommitTimeSynced, Option<String> lastCommitCompletionTimeSynced) {
     HoodieTimeline timeline = lastCommitTimeSynced.isPresent()
-        ? TimelineUtils.getCommitsTimelineAfter(metaClient, lastCommitTimeSynced.get(), lastCommitCompletionTimeSynced.get())
+        ? TimelineUtils.getCommitsTimelineAfter(metaClient, lastCommitTimeSynced.get(), lastCommitCompletionTimeSynced)
         : metaClient.getActiveTimeline();
     return new HashSet<>(TimelineUtils.getDroppedPartitions(timeline));
   }
@@ -135,7 +135,7 @@ public List<String> getWrittenPartitionsSince(Option<String> lastCommitTimeSynce
     } else {
       LOG.info("Last commit time synced is " + lastCommitTimeSynced.get() + ", Getting commits since then");
       return TimelineUtils.getWrittenPartitions(
-          TimelineUtils.getCommitsTimelineAfter(metaClient, lastCommitTimeSynced.get(), lastCommitCompletionTimeSynced.get()));
+          TimelineUtils.getCommitsTimelineAfter(metaClient, lastCommitTimeSynced.get(), lastCommitCompletionTimeSynced));
     }
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/RunCompactionActionExecutor.java
Patch:
@@ -108,6 +108,7 @@ public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
         metadata.addMetadata(SerDeHelper.LATEST_SCHEMA, schemaPair.getLeft().get());
         metadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, schemaPair.getRight().get());
       }
+      // Setting operationType, which is compact.
       metadata.setOperationType(operationType);
       compactionMetadata.setWriteStatuses(statuses);
       compactionMetadata.setCommitted(false);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -186,4 +186,4 @@ public BaseHoodieWriteClient getWriteClient() {
     }
     return writeClient;
   }
-}
\ No newline at end of file
+}

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -150,7 +150,7 @@ private void commitInternal(String instantTime, Map<MetadataPartitionType, Hoodi
         metadataMetaClient = HoodieTableMetaClient.reload(metadataMetaClient);
       }
 
-      if (!metadataMetaClient.getActiveTimeline().containsInstant(instantTime)) {
+      if (!metadataMetaClient.getActiveTimeline().getCommitsTimeline().containsInstant(instantTime)) {
         // if this is a new commit being applied to metadata for the first time
         writeClient.startCommitWithTime(instantTime);
       } else {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestDataValidationCheckForLogCompactionActions.java
Patch:
@@ -377,7 +377,7 @@ private TestTableContents setupTestTable2() throws IOException {
     // Create logcompaction client.
     HoodieWriteConfig logCompactionConfig = HoodieWriteConfig.newBuilder().withProps(config2.getProps())
         .withCompactionConfig(HoodieCompactionConfig.newBuilder()
-            .withLogCompactionBlocksThreshold("2").build())
+            .withLogCompactionBlocksThreshold(2).build())
         .build();
     SparkRDDWriteClient logCompactionClient = new SparkRDDWriteClient(context, logCompactionConfig);
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -2778,6 +2778,7 @@ private void validateMetadata(SparkRDDWriteClient testClient, Option<String> ign
       }
     });
 
+    // TODO: include validation for record_index partition here.
     LOG.info("Validation time=" + timer.endTimer());
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestHoodieMergeOnReadTable.java
Patch:
@@ -313,8 +313,8 @@ public void testLogFileCountsAfterCompaction() throws Exception {
   public void testLogBlocksCountsAfterLogCompaction(boolean populateMetaFields) throws Exception {
 
     HoodieCompactionConfig compactionConfig = HoodieCompactionConfig.newBuilder()
-        .withInlineCompaction(false)
-        .withLogCompactionBlocksThreshold("1")
+        .withMaxNumDeltaCommitsBeforeCompaction(1)
+        .withLogCompactionBlocksThreshold(1)
         .build();
     // insert 100 recordsx
     HoodieWriteConfig.Builder cfgBuilder = getConfigBuilder(true)

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieMergedLogRecordScanner.java
Patch:
@@ -332,7 +332,6 @@ public static class Builder extends AbstractHoodieLogRecordReader.Builder {
     private String keyFieldOverride;
     // By default, we're doing a full-scan
     private boolean forceFullScan = true;
-    // Use scanV2 method.
     private boolean enableOptimizedLogBlocksScan = false;
     private HoodieRecordMerger recordMerger;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java
Patch:
@@ -500,7 +500,7 @@ protected void runAnyPendingClustering(HoodieTable table) {
   protected void writeTableMetadata(HoodieTable table, String instantTime, String actionType, HoodieCommitMetadata metadata) {
     checkArgument(table.isTableServiceAction(actionType, instantTime), String.format("Unsupported action: %s.%s is not table service.", actionType, instantTime));
     context.setJobStatus(this.getClass().getSimpleName(), "Committing to metadata table: " + config.getTableName());
-    table.getMetadataWriter(instantTime).ifPresent(w -> ((HoodieTableMetadataWriter) w).update(metadata, instantTime, true));
+    table.getMetadataWriter(instantTime).ifPresent(w -> ((HoodieTableMetadataWriter) w).update(metadata, instantTime));
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/HoodieTimelineArchiver.java
Patch:
@@ -519,7 +519,7 @@ private Stream<HoodieInstant> getInstantsToArchive() throws IOException {
 
     // If metadata table is enabled, do not archive instants which are more recent than the last compaction on the
     // metadata table.
-    if (config.isMetadataTableEnabled()) {
+    if (table.getMetaClient().getTableConfig().isMetadataTableEnabled()) {
       try (HoodieTableMetadata tableMetadata = HoodieTableMetadata.create(table.getContext(), config.getMetadataConfig(),
           config.getBasePath(), FileSystemViewStorageConfig.SPILLABLE_DIR.defaultValue())) {
         Option<String> latestCompactionTime = tableMetadata.getLatestCompactionTime();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/BaseActionExecutor.java
Patch:
@@ -56,8 +56,7 @@ public BaseActionExecutor(HoodieEngineContext context, HoodieWriteConfig config,
    * @param metadata commit metadata of interest.
    */
   protected final void writeTableMetadata(HoodieCommitMetadata metadata, String actionType) {
-    table.getMetadataWriter(instantTime).ifPresent(w -> w.update(
-        metadata, instantTime, table.isTableServiceAction(actionType, instantTime)));
+    table.getMetadataWriter(instantTime).ifPresent(w -> w.update(metadata, instantTime));
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java
Patch:
@@ -377,7 +377,7 @@ public void run() {
                 HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(
                     table.getActiveTimeline().getInstantDetails(instant).get(), HoodieCommitMetadata.class);
                 // do not trigger any table service as partition is not fully built out yet
-                metadataWriter.update(commitMetadata, instant.getTimestamp(), false);
+                metadataWriter.update(commitMetadata, instant.getTimestamp());
                 break;
               case CLEAN_ACTION:
                 HoodieCleanMetadata cleanMetadata = CleanerUtils.getCleanerMetadata(table.getMetaClient(), instant);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -384,7 +384,7 @@ private void completeClustering(
   }
 
   @Override
-  protected void doInitTable(HoodieTableMetaClient metaClient, Option<String> instantTime, boolean initialMetadataTableIfNecessary) {
+  protected void doInitTable(WriteOperationType operationType, HoodieTableMetaClient metaClient, Option<String> instantTime) {
     // do nothing.
 
     // flink executes the upgrade/downgrade once when initializing the first instant on start up,

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDTableServiceClient.java
Patch:
@@ -277,7 +277,7 @@ private void updateTableMetadata(HoodieTable table, HoodieCommitMetadata commitM
     // Do not do any conflict resolution here as we do with regular writes. We take the lock here to ensure all writes to metadata table happens within a
     // single lock (single writer). Because more than one write to metadata table will result in conflicts since all of them updates the same partition.
     table.getMetadataWriter(hoodieInstant.getTimestamp())
-        .ifPresent(writer -> ((HoodieTableMetadataWriter) writer).update(commitMetadata, hoodieInstant.getTimestamp(), isTableServiceAction));
+            .ifPresent(writer -> ((HoodieTableMetadataWriter) writer).update(commitMetadata, hoodieInstant.getTimestamp()));
   }
 
   /**

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestHarness.java
Patch:
@@ -666,10 +666,10 @@ private void runFullValidation(HoodieMetadataConfig metadataConfig,
 
       List<FileSlice> latestSlices = fsView.getLatestFileSlices(partition).collect(Collectors.toList());
 
-      assertTrue(latestSlices.stream().map(FileSlice::getBaseFile).filter(Objects::nonNull).count() <= partitionType.getFileGroupCount(), "Should have a single latest base file");
-      assertTrue(latestSlices.size() <= partitionType.getFileGroupCount(), "Should have a single latest file slice");
+      assertTrue(latestSlices.stream().map(FileSlice::getBaseFile).filter(Objects::nonNull).count() > 0, "Should have a single latest base file");
+      assertTrue(latestSlices.size() > 0, "Should have a single latest file slice");
       assertTrue(latestSlices.size() <= numFileVersions, "Should limit file slice to "
-          + numFileVersions + " but was " + latestSlices.size());
+              + numFileVersions + " but was " + latestSlices.size());
     });
   }
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMetrics.java
Patch:
@@ -59,6 +59,9 @@ public class HoodieMetadataMetrics implements Serializable {
   public static final String STAT_COUNT_LOG_FILES = "logFileCount";
   public static final String STAT_COUNT_PARTITION = "partitionCount";
   public static final String STAT_LAST_COMPACTION_TIMESTAMP = "lastCompactionTimestamp";
+  public static final String SKIP_TABLE_SERVICES = "skip_table_services";
+  public static final String TABLE_SERVICE_EXECUTION_STATUS = "table_service_execution_status";
+  public static final String TABLE_SERVICE_EXECUTION_DURATION = "table_service_execution_duration";
 
   private static final Logger LOG = LoggerFactory.getLogger(HoodieMetadataMetrics.class);
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -1254,8 +1254,9 @@ private HoodieIndexer.Config buildIndexerConfig(String basePath,
     return config;
   }
 
-  @ParameterizedTest
-  @EnumSource(value = HoodieRecordType.class, names = {"AVRO", "SPARK"})
+  //@ParameterizedTest
+  //@EnumSource(value = HoodieRecordType.class, names = {"AVRO", "SPARK"})
+  @Disabled("HUDI-6332")
   public void testHoodieIndexer(HoodieRecordType recordType) throws Exception {
     String tableBasePath = basePath + "/asyncindexer";
     HoodieDeltaStreamer ds = initialHoodieDeltaStreamer(tableBasePath, 1000, "false", recordType);

File: hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java
Patch:
@@ -393,7 +393,7 @@ private static ColumnReader createColumnReader(
           if (fieldIndex < 0) {
             fieldReaders.add(new EmptyColumnReader());
           } else {
-            fieldReaders.add(i,
+            fieldReaders.add(
                 createColumnReader(
                     utcTimestamp,
                     rowType.getTypeAt(i),

File: hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java
Patch:
@@ -393,7 +393,7 @@ private static ColumnReader createColumnReader(
           if (fieldIndex < 0) {
             fieldReaders.add(new EmptyColumnReader());
           } else {
-            fieldReaders.add(i,
+            fieldReaders.add(
                 createColumnReader(
                     utcTimestamp,
                     rowType.getTypeAt(i),

File: hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java
Patch:
@@ -393,7 +393,7 @@ private static ColumnReader createColumnReader(
           if (fieldIndex < 0) {
             fieldReaders.add(new EmptyColumnReader());
           } else {
-            fieldReaders.add(i,
+            fieldReaders.add(
                 createColumnReader(
                     utcTimestamp,
                     rowType.getTypeAt(i),

File: hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java
Patch:
@@ -393,7 +393,7 @@ private static ColumnReader createColumnReader(
           if (fieldIndex < 0) {
             fieldReaders.add(new EmptyColumnReader());
           } else {
-            fieldReaders.add(i,
+            fieldReaders.add(
                 createColumnReader(
                     utcTimestamp,
                     rowType.getTypeAt(i),

File: hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java
Patch:
@@ -393,7 +393,7 @@ private static ColumnReader createColumnReader(
           if (fieldIndex < 0) {
             fieldReaders.add(new EmptyColumnReader());
           } else {
-            fieldReaders.add(i,
+            fieldReaders.add(
                 createColumnReader(
                     utcTimestamp,
                     rowType.getTypeAt(i),

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataWriteUtils.java
Patch:
@@ -93,7 +93,7 @@ public static HoodieWriteConfig createMetadataWriteConfig(
             .withCleanerParallelism(parallelism)
             .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS)
             .withFailedWritesCleaningPolicy(failedWritesCleaningPolicy)
-            .retainCommits(DEFAULT_METADATA_CLEANER_COMMITS_RETAINED)
+            .retainCommits(Math.min(writeConfig.getCleanerCommitsRetained(), DEFAULT_METADATA_CLEANER_COMMITS_RETAINED))
             .build())
         // we will trigger archive manually, to ensure only regular writer invokes it
         .withArchivalConfig(HoodieArchivalConfig.newBuilder()

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieMetadataBase.java
Patch:
@@ -400,7 +400,7 @@ protected HoodieWriteConfig getMetadataWriteConfig(HoodieWriteConfig writeConfig
             .withCleanerParallelism(parallelism)
             .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS)
             .withFailedWritesCleaningPolicy(HoodieFailedWritesCleaningPolicy.LAZY)
-            .retainCommits(DEFAULT_METADATA_CLEANER_COMMITS_RETAINED)
+            .retainCommits(Math.min(writeConfig.getCleanerCommitsRetained(), DEFAULT_METADATA_CLEANER_COMMITS_RETAINED))
             .build())
         // we will trigger archival manually, to control the instant times
         .withArchivalConfig(HoodieArchivalConfig.newBuilder()

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQuerySingleResultPreCommitValidator.java
Patch:
@@ -40,7 +40,7 @@
  * Example configuration: "query1#expectedResult1;query2#expectedResult2;"
  */
 public class SqlQuerySingleResultPreCommitValidator<T, I, K, O extends HoodieData<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
-  private static final Logger LOG = LoggerFactory.getLogger(SqlQueryInequalityPreCommitValidator.class);
+  private static final Logger LOG = LoggerFactory.getLogger(SqlQuerySingleResultPreCommitValidator.class);
 
   public SqlQuerySingleResultPreCommitValidator(HoodieSparkTable<T> table, HoodieEngineContext engineContext, HoodieWriteConfig config) {
     super(table, engineContext, config);
@@ -68,9 +68,9 @@ protected void validateUsingQuery(String query, String prevTableSnapshot, String
     }
     Object result = newRows.get(0).apply(0);
     if (result == null || !expectedResult.equals(result.toString())) {
-      LOG.error("Mismatch query result. Expected: " + expectedResult + " got " + result + "Query: " + query);
+      LOG.error("Mismatch query result. Expected: " + expectedResult + " got " + result + " on Query: " + query);
       throw new HoodieValidationException("Query validation failed for '" + query
-          + "'. Expected " + expectedResult + " rows, Found " + result);
+          + "'. Expected " + expectedResult + " row(s), Found " + result);
     } else {
       LOG.info("Query validation successful. Expected: " + expectedResult + " got " + result);
     }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkValidatorUtils.java
Patch:
@@ -76,8 +76,8 @@ public static void runValidators(HoodieWriteConfig config,
       SQLContext sqlContext = new SQLContext(HoodieSparkEngineContext.getSparkContext(context));
       // Refresh timeline to ensure validator sees the any other operations done on timeline (async operations such as other clustering/compaction/rollback)
       table.getMetaClient().reloadActiveTimeline();
-      Dataset<Row> beforeState = getRecordsFromCommittedFiles(sqlContext, partitionsModified, table).cache();
-      Dataset<Row> afterState  = getRecordsFromPendingCommits(sqlContext, partitionsModified, writeMetadata, table, instantTime).cache();
+      Dataset<Row> beforeState = getRecordsFromCommittedFiles(sqlContext, partitionsModified, table);
+      Dataset<Row> afterState  = getRecordsFromPendingCommits(sqlContext, partitionsModified, writeMetadata, table, instantTime);
 
       Stream<SparkPreCommitValidator> validators = Arrays.stream(config.getPreCommitValidators().split(","))
           .map(validatorClass -> ((SparkPreCommitValidator) ReflectionUtils.loadClass(validatorClass,
@@ -107,7 +107,7 @@ private static CompletableFuture<Boolean> runValidatorAsync(SparkPreCommitValida
         LOG.info("validation complete for " + validator.getClass().getName());
         return true;
       } catch (HoodieValidationException e) {
-        LOG.error("validation failed for " + validator.getClass().getName());
+        LOG.error("validation failed for " + validator.getClass().getName(), e);
         return false;
       }
     });

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstantTimeGenerator.java
Patch:
@@ -55,7 +55,7 @@ public class HoodieInstantTimeGenerator {
   // The default number of milliseconds that we add if they are not present
   // We prefer the max timestamp as it mimics the current behavior with second granularity
   // when performing comparisons such as LESS_THAN_OR_EQUAL_TO
-  private static final String DEFAULT_MILLIS_EXT = "999";
+  public static final String DEFAULT_MILLIS_EXT = "999";
 
   private static HoodieTimelineTimeZone commitTimeZone = HoodieTimelineTimeZone.LOCAL;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -276,7 +276,7 @@ public HoodieTableConfig(FileSystem fs, String metaPath, String payloadClassName
         setValue(PAYLOAD_CLASS_NAME, payloadClassName);
         needStore = true;
       }
-      if (contains(RECORD_MERGER_STRATEGY) && payloadClassName != null
+      if (contains(RECORD_MERGER_STRATEGY) && recordMergerStrategyId != null
           && !getString(RECORD_MERGER_STRATEGY).equals(recordMergerStrategyId)) {
         setValue(RECORD_MERGER_STRATEGY, recordMergerStrategyId);
         needStore = true;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -2183,7 +2183,8 @@ public boolean getPushGatewayRandomJobNameSuffix() {
   }
 
   public String getMetricReporterMetricsNamePrefix() {
-    return getStringOrDefault(HoodieMetricsConfig.METRICS_REPORTER_PREFIX);
+    // Metrics prefixes should not have a dot as this is usually a separator
+    return getStringOrDefault(HoodieMetricsConfig.METRICS_REPORTER_PREFIX).replaceAll("\\.", "_");
   }
 
   public String getMetricReporterFileBasedConfigs() {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
Patch:
@@ -192,7 +192,7 @@ public static SchemaPostProcessor createSchemaPostProcessor(
 
   }
 
-  public static Option<Transformer> createTransformer(Option<List<String>> classNamesOpt, Boolean isErrorTableWriterEnabled) throws IOException {
+  public static Option<Transformer> createTransformer(Option<List<String>> classNamesOpt, boolean isErrorTableWriterEnabled) throws IOException {
     try {
       return classNamesOpt.map(classNames -> classNames.isEmpty() ? null : 
           isErrorTableWriterEnabled ? new ErrorTableAwareChainedTransformer(classNames) : new ChainedTransformer(classNames)

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/transform/ErrorTableAwareChainedTransformer.java
Patch:
@@ -50,7 +50,7 @@ public Dataset<Row> apply(JavaSparkContext jsc, SparkSession sparkSession, Datas
     for (TransformerInfo transformerInfo : transformers) {
       Transformer transformer = transformerInfo.getTransformer();
       dataset = transformer.apply(jsc, sparkSession, dataset, transformerInfo.getProperties(properties));
-      // validate in every stage to ensure it's not dropped by one of the transformer and added by next transformer.
+      // validate in every stage to ensure ErrorRecordColumn not dropped by one of the transformer and added by next transformer.
       ErrorTableUtils.validate(dataset);
     }
     return dataset;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergedReadHandle.java
Patch:
@@ -158,7 +158,9 @@ private List<HoodieRecord<T>> doMergedRead(Option<HoodieFileReader> baseFileRead
           if (!mergeResult.isPresent()) {
             continue;
           }
-          mergedRecords.add(mergeResult.get().getLeft());
+          HoodieRecord<T> r = mergeResult.get().getLeft().wrapIntoHoodieRecordPayloadWithParams(readerSchema,
+              config.getProps(), simpleKeyGenFieldsOpt, logRecordScanner.isWithOperationField(), logRecordScanner.getPartitionNameOverride(), false, Option.empty());
+          mergedRecords.add(r);
         } else {
           mergedRecords.add(record.copy());
         }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/common/HoodieSparkEngineContext.java
Patch:
@@ -40,8 +40,6 @@
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.api.java.function.PairFlatMapFunction;
 import org.apache.spark.sql.SQLContext;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 import javax.annotation.concurrent.ThreadSafe;
 
@@ -63,7 +61,6 @@
 @ThreadSafe
 public class HoodieSparkEngineContext extends HoodieEngineContext {
 
-  private static final Logger LOG = LoggerFactory.getLogger(HoodieSparkEngineContext.class);
   private final JavaSparkContext javaSparkContext;
   private final SQLContext sqlContext;
   private final Map<HoodieDataCacheKey, List<Integer>> cachedRddIds = new HashMap<>();

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -469,7 +469,7 @@ private Pair<HoodieSeekingFileReader<?>, Long> getBaseFileReader(FileSlice slice
     return Pair.of(baseFileReader, baseFileOpenMs);
   }
 
-  private Set<String> getValidInstantTimestamps() {
+  public static Set<String> getValidInstantTimestamps(HoodieTableMetaClient dataMetaClient, HoodieTableMetaClient metadataMetaClient) {
     // Only those log files which have a corresponding completed instant on the dataset should be read
     // This is because the metadata table is updated before the dataset instants are committed.
     HoodieActiveTimeline datasetTimeline = dataMetaClient.getActiveTimeline();
@@ -511,7 +511,7 @@ public Pair<HoodieMetadataLogRecordReader, Long> getLogRecordScanner(List<Hoodie
 
     // Only those log files which have a corresponding completed instant on the dataset should be read
     // This is because the metadata table is updated before the dataset instants are committed.
-    Set<String> validInstantTimestamps = getValidInstantTimestamps();
+    Set<String> validInstantTimestamps = getValidInstantTimestamps(dataMetaClient, metadataMetaClient);
 
     Option<HoodieInstant> latestMetadataInstant = metadataMetaClient.getActiveTimeline().filterCompletedInstants().lastInstant();
     String latestMetadataInstantTime = latestMetadataInstant.map(HoodieInstant::getTimestamp).orElse(SOLO_COMMIT_TIMESTAMP);
@@ -565,7 +565,7 @@ private boolean isFullScanAllowedForPartition(String partitionName) {
    * @param instant  The Rollback operation to read
    * @param timeline instant of timeline from dataset.
    */
-  private List<String> getRollbackedCommits(HoodieInstant instant, HoodieActiveTimeline timeline) {
+  private static List<String> getRollbackedCommits(HoodieInstant instant, HoodieActiveTimeline timeline) {
     try {
       List<String> commitsToRollback = null;
       if (instant.getAction().equals(HoodieTimeline.ROLLBACK_ACTION)) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/hbase/TestSparkHoodieHBaseIndex.java
Patch:
@@ -114,6 +114,7 @@ public class TestSparkHoodieHBaseIndex extends SparkClientFunctionalTestHarness
   @BeforeAll
   public static void init() throws Exception {
     // Initialize HbaseMiniCluster
+    System.setProperty("zookeeper.4lw.commands.whitelist", "*");
     hbaseConfig = HBaseConfiguration.create();
     hbaseConfig.set(ZOOKEEPER_ZNODE_PARENT, "/hudi-hbase-test");
 

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/minicluster/ZookeeperTestService.java
Patch:
@@ -163,6 +163,7 @@ private static void setupTestEnv() {
     // resulting in test failure (client timeout on first session).
     // set env and directly in order to handle static init/gc issues
     System.setProperty("zookeeper.preAllocSize", "100");
+    System.setProperty("zookeeper.4lw.commands.whitelist", "*");
     FileTxnLog.setPreallocSize(100 * 1024);
   }
 

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java
Patch:
@@ -188,7 +188,9 @@ public static void shutdown() throws IOException {
     if (zkServer != null) {
       zkServer.shutdown(true);
     }
-    fileSystem.close();
+    if (fileSystem != null) {
+      fileSystem.close();
+    }
   }
 
   public static void createCOWTable(String instantTime, int numberOfPartitions, boolean useSchemaFromCommitMetadata,

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHiveIncrementalPuller.java
Patch:
@@ -29,6 +29,7 @@
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.condition.EnabledIf;
 import org.junit.jupiter.api.io.TempDir;
 
 import java.io.File;
@@ -166,6 +167,7 @@ public void testPullerWithoutSourceInSql() throws IOException, URISyntaxExceptio
   }
 
   @Test
+  @EnabledIf(value = "org.apache.hudi.HoodieSparkUtils#isSpark2", disabledReason = "Disable due to hive not support avro 1.10.2.")
   public void testPuller() throws IOException, URISyntaxException {
     createTables();
     HiveIncrementalPuller.Config cfg = getHivePullerConfig("select name from testdb.test1 where `_hoodie_commit_time` > '%s'");

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestAvroKafkaSource.java
Patch:
@@ -159,8 +159,8 @@ public void testAppendKafkaOffsetsSourceFormatAdapter() throws IOException {
         UtilHelpers.createSchemaProvider(FilebasedSchemaProvider.class.getName(), props, jsc()), props, jsc(), new ArrayList<>());
 
     props.put("hoodie.deltastreamer.source.kafka.value.deserializer.class", ByteArrayDeserializer.class.getName());
-    int numPartitions = 3;
-    int numMessages = 15;
+    int numPartitions = 2;
+    int numMessages = 30;
     testUtils.createTopic(topic,numPartitions);
     sendMessagesToKafka(topic, numMessages, numPartitions);
     AvroKafkaSource avroKafkaSource = new AvroKafkaSource(props, jsc(), spark(), schemaProvider, metrics);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/helpers/TestKafkaOffsetGen.java
Patch:
@@ -151,7 +151,7 @@ public void testGetNextOffsetRangesFromMultiplePartitions() {
   public void testGetNextOffsetRangesFromGroup() {
     HoodieTestDataGenerator dataGenerator = new HoodieTestDataGenerator();
     testUtils.createTopic(testTopicName, 2);
-    testUtils.sendMessages(testTopicName, Helpers.jsonifyRecords(dataGenerator.generateInserts("000", 1000)));
+    testUtils.sendMessages(testTopicName, Helpers.jsonifyRecordsByPartitions(dataGenerator.generateInserts("000", 1000), 2));
     KafkaOffsetGen kafkaOffsetGen = new KafkaOffsetGen(getConsumerConfigs("group", "string"));
     String lastCheckpointString = testTopicName + ",0:250,1:249";
     kafkaOffsetGen.commitOffsetToKafka(lastCheckpointString);

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java
Patch:
@@ -186,6 +186,7 @@ private void initWriteFunction() {
         this.writeFunction = (records, instantTime) -> this.writeClient.insert(records, instantTime);
         break;
       case UPSERT:
+      case DELETE: // shares the code path with UPSERT
         this.writeFunction = (records, instantTime) -> this.writeClient.upsert(records, instantTime);
         break;
       case INSERT_OVERWRITE:

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableFactory.java
Patch:
@@ -196,8 +196,9 @@ private void checkRecordKey(Configuration conf, ResolvedSchema schema) {
       if (recordKeys.length == 1
           && FlinkOptions.RECORD_KEY_FIELD.defaultValue().equals(recordKeys[0])
           && !fields.contains(recordKeys[0])) {
-        throw new HoodieValidationException("Primary key definition is required, use either PRIMARY KEY syntax "
-            + "or option '" + FlinkOptions.RECORD_KEY_FIELD.key() + "' to specify.");
+        throw new HoodieValidationException("Primary key definition is required, the default primary key field "
+            + "'" + FlinkOptions.RECORD_KEY_FIELD.defaultValue() + "' does not exist in the table schema, "
+            + "use either PRIMARY KEY syntax or option '" + FlinkOptions.RECORD_KEY_FIELD.key() + "' to speciy.");
       }
 
       Arrays.stream(recordKeys)

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -486,7 +486,7 @@ private Stream<HoodieBaseFile> convertFileStatusesToBaseFiles(FileStatus[] statu
   /**
    * Helper to convert file-status to log-files.
    *
-   * @param statuses List of FIle-Status
+   * @param statuses List of File-Status
    */
   private Stream<HoodieLogFile> convertFileStatusesToLogFiles(FileStatus[] statuses) {
     Predicate<FileStatus> rtFilePredicate = fileStatus ->  {

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java
Patch:
@@ -63,7 +63,7 @@ public class HoodieTableFileSystemView extends IncrementalTimelineSyncFileSystem
   protected Map<HoodieFileGroupId, Pair<String, CompactionOperation>> fgIdToPendingCompaction;
 
   /**
-   * PartitionPath + File-Id to pending compaction instant time.
+   * PartitionPath + File-Id to pending logcompaction instant time.
    */
   protected Map<HoodieFileGroupId, Pair<String, CompactionOperation>> fgIdToPendingLogCompaction;
 

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfigHolder.java
Patch:
@@ -155,5 +155,5 @@ public class HiveSyncConfigHolder {
       .defaultValue(HoodieSyncTableStrategy.ALL.name())
       .markAdvanced()
       .sinceVersion("0.13.0")
-      .withDocumentation("Hive table synchronization strategy. Available option: ONLY_RO, ONLY_RT, ALL.");
+      .withDocumentation("Hive table synchronization strategy. Available option: RO, RT, ALL.");
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/bootstrap/BootstrapRecordConsumer.java
Patch:
@@ -41,6 +41,7 @@ public void consume(HoodieRecord record) {
 
   @Override
   public Void finish() {
+    bootstrapHandle.close();
     return null;
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseMergeHelper.java
Patch:
@@ -56,6 +56,7 @@ public void consume(HoodieRecord record) {
 
     @Override
     public Void finish() {
+      upsertHandle.close();
       return null;
     }
   }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeletePartitionCommitActionExecutor.java
Patch:
@@ -66,7 +66,7 @@ public HoodieWriteMetadata<List<WriteStatus>> execute() {
     DeletePartitionUtils.checkForPendingTableServiceActions(table, partitions);
 
     try {
-      HoodieTimer timer = new HoodieTimer().startTimer();
+      HoodieTimer timer = HoodieTimer.start();
       context.setJobStatus(this.getClass().getSimpleName(), "Gather all file ids from all deleting partitions.");
       Map<String, List<String>> partitionToReplaceFileIds =
           context.parallelize(partitions).distinct().collectAsList()

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieMergedLogRecordScanner.java
Patch:
@@ -76,7 +76,7 @@ public class HoodieMergedLogRecordScanner extends AbstractHoodieLogRecordReader
 
   private static final Logger LOG = LoggerFactory.getLogger(HoodieMergedLogRecordScanner.class);
   // A timer for calculating elapsed time in millis
-  public final HoodieTimer timer = new HoodieTimer();
+  public final HoodieTimer timer = HoodieTimer.create();
   // Map of compacted/merged records
   private final ExternalSpillableMap<String, HoodieRecord> records;
   // Set of already scanned prefixes allowing us to avoid scanning same prefixes again

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/ClosableIterator.java
Patch:
@@ -24,8 +24,6 @@
  * An iterator that give a chance to release resources.
  *
  * @param <R> The return type
- *
- * TODO move under common.util.collection
  */
 public interface ClosableIterator<R> extends Iterator<R>, AutoCloseable {
   @Override

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/HoodieMessageQueue.java
Patch:
@@ -50,6 +50,8 @@ public interface HoodieMessageQueue<I, O> extends Closeable {
    */
   void markAsFailed(Throwable e);
 
+  Throwable getThrowable();
+
   boolean isEmpty();
 
   /**

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -289,7 +289,6 @@ public DeltaSync(HoodieDeltaStreamer.Config cfg, SparkSession sparkSession, Sche
     // Register User Provided schema first
     registerAvroSchemas(schemaProvider);
 
-    this.transformer = UtilHelpers.createTransformer(Option.ofNullable(cfg.transformerClassNames));
 
     this.metrics = (HoodieIngestionMetrics) ReflectionUtils.loadClass(cfg.ingestionMetricsClass, getHoodieClientConfig(this.schemaProvider));
     this.hoodieMetrics = new HoodieMetrics(getHoodieClientConfig(this.schemaProvider));
@@ -306,6 +305,9 @@ public DeltaSync(HoodieDeltaStreamer.Config cfg, SparkSession sparkSession, Sche
     this.formatAdapter = new SourceFormatAdapter(
         UtilHelpers.createSource(cfg.sourceClassName, props, jssc, sparkSession, schemaProvider, metrics),
         this.errorTableWriter, Option.of(props));
+
+    this.transformer = UtilHelpers.createTransformer(Option.ofNullable(cfg.transformerClassNames), this.errorTableWriter.isPresent());
+
   }
 
   /**

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java
Patch:
@@ -30,9 +30,10 @@
 import java.util.Arrays;
 
 /**
- * A partitioner that does sort based on specified column values for each RDD partition.
+ * A partitioner that globally sorts a {@link JavaRDD<HoodieRecord>} based on partition path column and custom columns.
  *
- * @param <T> HoodieRecordPayload type
+ * @see GlobalSortPartitioner
+ * @see BulkInsertSortMode#GLOBAL_SORT
  */
 public class RDDCustomColumnsSortPartitioner<T>
     implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java
Patch:
@@ -326,6 +326,8 @@ private Map<BootstrapMode, List<Pair<String, List<HoodieFileStatus>>>> listAndPr
       if (!(selector instanceof FullRecordBootstrapModeSelector)) {
         FullRecordBootstrapModeSelector fullRecordBootstrapModeSelector = new FullRecordBootstrapModeSelector(config);
         result.putAll(fullRecordBootstrapModeSelector.select(folders));
+      } else {
+        result.putAll(selector.select(folders));
       }
     } else {
       result = selector.select(folders);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataCreateHandle.java
Patch:
@@ -166,7 +166,7 @@ public HoodieInternalWriteStatus close() throws IOException {
     long fileSizeInBytes = FSUtils.getFileSize(table.getMetaClient().getFs(), path);
     stat.setTotalWriteBytes(fileSizeInBytes);
     stat.setFileSizeInBytes(fileSizeInBytes);
-    stat.setTotalWriteErrors(writeStatus.getFailedRowsSize());
+    stat.setTotalWriteErrors(writeStatus.getTotalErrorRecords());
     HoodieWriteStat.RuntimeStats runtimeStats = new HoodieWriteStat.RuntimeStats();
     runtimeStats.setTotalCreateTime(currTimer.endTimer());
     stat.setRuntimeStats(runtimeStats);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowCreateHandle.java
Patch:
@@ -232,7 +232,7 @@ public HoodieInternalWriteStatus close() throws IOException {
     long fileSizeInBytes = FSUtils.getFileSize(table.getMetaClient().getFs(), path);
     stat.setTotalWriteBytes(fileSizeInBytes);
     stat.setFileSizeInBytes(fileSizeInBytes);
-    stat.setTotalWriteErrors(writeStatus.getFailedRowsSize());
+    stat.setTotalWriteErrors(writeStatus.getTotalErrorRecords());
     HoodieWriteStat.RuntimeStats runtimeStats = new HoodieWriteStat.RuntimeStats();
     runtimeStats.setTotalCreateTime(currTimer.endTimer());
     stat.setRuntimeStats(runtimeStats);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieInternalWriteStatus.java
Patch:
@@ -44,6 +44,7 @@ public void testFailureFraction() {
     // verification
     assertEquals(fileId, status.getFileId());
     assertEquals(partitionPath, status.getPartitionPath());
+    assertEquals(1000, status.getTotalErrorRecords());
     assertTrue(status.getFailedRecordKeys().size() > 0);
     assertTrue(status.getFailedRecordKeys().size() < 150); // 150 instead of 100, to prevent flaky test
     assertTrue(status.hasErrors());
@@ -66,6 +67,7 @@ public void testSuccessRecordTracking() {
       // verification
       assertEquals(fileId, status.getFileId());
       assertEquals(partitionPath, status.getPartitionPath());
+      assertEquals(1000, status.getTotalErrorRecords());
       assertEquals(1000, status.getFailedRecordKeys().size());
       assertTrue(status.hasErrors());
       if (trackSuccess) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/storage/row/TestHoodieRowCreateHandle.java
Patch:
@@ -218,7 +218,6 @@ private void assertOutput(HoodieInternalWriteStatus writeStatus, int size, Strin
                             String instantTime, Dataset<Row> inputRows, List<String> filenames, List<String> fileAbsPaths, boolean populateMetaFields) {
     assertEquals(writeStatus.getPartitionPath(), partitionPath);
     assertEquals(writeStatus.getTotalRecords(), size);
-    assertEquals(writeStatus.getFailedRowsSize(), 0);
     assertEquals(writeStatus.getTotalErrorRecords(), 0);
     assertFalse(writeStatus.hasErrors());
     assertNull(writeStatus.getGlobalError());

File: hudi-spark-datasource/hudi-spark-common/src/test/java/org/apache/hudi/internal/HoodieBulkInsertInternalWriterTestBase.java
Patch:
@@ -122,7 +122,6 @@ protected void assertWriteStatuses(List<HoodieInternalWriteStatus> writeStatuses
         assertEquals(writeStatus.getTotalRecords(), sizeMap.get(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[counter % 3]));
       }
       assertNull(writeStatus.getGlobalError());
-      assertEquals(writeStatus.getFailedRowsSize(), 0);
       assertEquals(writeStatus.getTotalErrorRecords(), 0);
       assertFalse(writeStatus.hasErrors());
       assertNotNull(writeStatus.getFileId());

File: hudi-sync/hudi-sync-common/src/test/java/org/apache/hudi/sync/common/util/TestSyncUtilHelpers.java
Patch:
@@ -112,8 +112,8 @@ public void syncHoodieTable() {
   }
 
   public static class DummySyncTool2 extends HoodieSyncTool {
-    public DummySyncTool2(Properties props, Configuration hadoopConf) {
-      super(props, hadoopConf);
+    public DummySyncTool2(Properties props) {
+      super(props);
     }
 
     @Override

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/ErrorTableUtils.java
Patch:
@@ -48,7 +48,7 @@ public static Option<BaseErrorTableWriter> getErrorTableWriter(HoodieDeltaStream
     Class<?>[] argClassArr = new Class[] {HoodieDeltaStreamer.Config.class,
         SparkSession.class, TypedProperties.class, JavaSparkContext.class, FileSystem.class};
     String errMsg = "Unable to instantiate ErrorTableWriter with arguments type " + Arrays.toString(argClassArr);
-    ValidationUtils.checkArgument(ReflectionUtils.hasConstructor(BaseErrorTableWriter.class.getName(), argClassArr), errMsg);
+    ValidationUtils.checkArgument(ReflectionUtils.hasConstructor(BaseErrorTableWriter.class.getName(), argClassArr, false), errMsg);
 
     try {
       return Option.of((BaseErrorTableWriter) ReflectionUtils.getClass(errorTableWriterClass).getConstructor(argClassArr)

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java
Patch:
@@ -650,7 +650,7 @@ private static Type convertToParquetType(
             .addField(
                 Types
                     .repeatedGroup()
-                    .addField(convertToParquetType("key", keyType, repetition))
+                    .addField(convertToParquetType("key", keyType, Type.Repetition.REQUIRED))
                     .addField(convertToParquetType("value", valueType, repetition))
                     .named("key_value"))
             .named(name);

File: hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/io/storage/row/parquet/TestParquetSchemaConverter.java
Patch:
@@ -56,7 +56,7 @@ void testConvertComplexTypes() {
         + "  }\n"
         + "  optional group f_map (MAP) {\n"
         + "    repeated group key_value {\n"
-        + "      optional int32 key;\n"
+        + "      required int32 key;\n"
         + "      optional binary value (STRING);\n"
         + "    }\n"
         + "  }\n"

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/HoodieTimelineArchiver.java
Patch:
@@ -536,7 +536,7 @@ private Stream<HoodieInstant> getInstantsToArchive() throws IOException {
       }
     }
 
-    if (HoodieTableMetadata.isMetadataTable(config.getBasePath())) {
+    if (table.isMetadataTable()) {
       HoodieTableMetaClient dataMetaClient = HoodieTableMetaClient.builder()
           .setBasePath(HoodieTableMetadata.getDatasetBasePath(config.getBasePath()))
           .setConf(metaClient.getHadoopConf())

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -269,7 +269,7 @@ private HoodieWriteConfig createMetadataWriteConfig(
         .withWriteConcurrencyMode(WriteConcurrencyMode.SINGLE_WRITER)
         .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).withFileListingParallelism(writeConfig.getFileListingParallelism()).build())
         .withAutoCommit(true)
-        .withAvroSchemaValidate(true)
+        .withAvroSchemaValidate(false)
         .withEmbeddedTimelineServerEnabled(false)
         .withMarkersType(MarkerType.DIRECT.name())
         .withRollbackUsingMarkers(false)

File: hudi-common/src/main/java/org/apache/hudi/common/model/BaseAvroPayload.java
Patch:
@@ -83,7 +83,7 @@ public boolean canProduceSentinel() {
    * @param genericRecord instance of {@link GenericRecord} of interest.
    * @returns {@code true} if record represents a delete record. {@code false} otherwise.
    */
-  protected static boolean isDeleteRecord(GenericRecord genericRecord) {
+  protected boolean isDeleteRecord(GenericRecord genericRecord) {
     final String isDeleteKey = HoodieRecord.HOODIE_IS_DELETED_FIELD;
     // Modify to be compatible with new version Avro.
     // The new version Avro throws for GenericRecord.get if the field name

File: hudi-common/src/test/java/org/apache/hudi/common/model/debezium/TestMySqlDebeziumAvroPayload.java
Patch:
@@ -33,10 +33,12 @@
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.Objects;
+import java.util.Properties;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertThrows;
+import static org.junit.jupiter.api.Assertions.assertTrue;
 
 /**
  * Tests {@link MySqlDebeziumAvroPayload}.
@@ -100,6 +102,7 @@ public void testMergeWithUpdate() throws IOException {
   public void testMergeWithDelete() throws IOException {
     GenericRecord deleteRecord = createRecord(2, Operation.DELETE, "00002.11");
     MySqlDebeziumAvroPayload payload = new MySqlDebeziumAvroPayload(deleteRecord, "00002.11");
+    assertTrue(payload.isDeleted(avroSchema, new Properties()));
 
     GenericRecord existingRecord = createRecord(2, Operation.UPDATE, "00001.111");
     Option<IndexedRecord> mergedRecord = payload.combineAndGetUpdateValue(existingRecord, avroSchema);

File: hudi-common/src/test/java/org/apache/hudi/common/model/debezium/TestPostgresDebeziumAvroPayload.java
Patch:
@@ -41,11 +41,13 @@
 import java.nio.charset.StandardCharsets;
 import java.util.Arrays;
 import java.util.Objects;
+import java.util.Properties;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertNull;
 import static org.junit.jupiter.api.Assertions.assertThrows;
+import static org.junit.jupiter.api.Assertions.assertTrue;
 
 /**
  * Tests {@link PostgresDebeziumAvroPayload}.
@@ -108,6 +110,7 @@ public void testMergeWithUpdate() throws IOException {
   public void testMergeWithDelete() throws IOException {
     GenericRecord deleteRecord = createRecord(2, Operation.DELETE, 100L);
     PostgresDebeziumAvroPayload payload = new PostgresDebeziumAvroPayload(deleteRecord, 100L);
+    assertTrue(payload.isDeleted(avroSchema, new Properties()));
 
     GenericRecord existingRecord = createRecord(2, Operation.UPDATE, 99L);
     Option<IndexedRecord> mergedRecord = payload.combineAndGetUpdateValue(existingRecord, avroSchema);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieMultiTableDeltaStreamer.java
Patch:
@@ -256,12 +256,13 @@ public void testTableLevelProperties() throws IOException {
           String tableLevelKeyGeneratorClass = tableExecutionContext.getProperties().getString(DataSourceWriteOptions.KEYGENERATOR_CLASS_NAME().key());
           assertEquals(TestHoodieDeltaStreamer.TestTableLevelGenerator.class.getName(), tableLevelKeyGeneratorClass);
           List<String> transformerClass = tableExecutionContext.getConfig().transformerClassNames;
-          assertEquals("org.apache.hudi.utilities.transform.SqlFileBasedTransformer", transformerClass.get(0)); // HUDI-4630
+          assertEquals(1, transformerClass.size());
+          assertEquals("org.apache.hudi.utilities.deltastreamer.TestHoodieDeltaStreamer$TestIdentityTransformer", transformerClass.get(0));
           break;
         default:
           String defaultKeyGeneratorClass = tableExecutionContext.getProperties().getString(DataSourceWriteOptions.KEYGENERATOR_CLASS_NAME().key());
           assertEquals(TestHoodieDeltaStreamer.TestGenerator.class.getName(), defaultKeyGeneratorClass);
-          assertNull(tableExecutionContext.getConfig().transformerClassNames);  //HUDI-4630
+          assertNull(tableExecutionContext.getConfig().transformerClassNames);
       }
     });
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/TimestampBasedAvroKeyGenerator.java
Patch:
@@ -76,7 +76,7 @@ public TimestampBasedAvroKeyGenerator(TypedProperties config) throws IOException
   }
 
   TimestampBasedAvroKeyGenerator(TypedProperties config, String recordKeyField, String partitionPathField) throws IOException {
-    super(config, recordKeyField, partitionPathField);
+    super(config, Option.ofNullable(recordKeyField), partitionPathField);
     String dateTimeParserClass = config.getString(KeyGeneratorOptions.Config.DATE_TIME_PARSER_PROP, HoodieDateTimeParser.class.getName());
     this.parser = KeyGenUtils.createDateTimeParser(config, dateTimeParserClass);
     this.inputDateTimeZone = parser.getInputDateTimeZone();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/GlobalDeleteKeyGenerator.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.keygen;
 
 import org.apache.hudi.common.config.TypedProperties;
-import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 
 import org.apache.avro.generic.GenericRecord;
 import org.apache.spark.sql.Row;
@@ -28,7 +27,6 @@
 import org.apache.spark.unsafe.types.UTF8String;
 
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.List;
 
 /**
@@ -40,7 +38,7 @@ public class GlobalDeleteKeyGenerator extends BuiltinKeyGenerator {
   private final GlobalAvroDeleteKeyGenerator globalAvroDeleteKeyGenerator;
   public GlobalDeleteKeyGenerator(TypedProperties config) {
     super(config);
-    this.recordKeyFields = Arrays.asList(config.getString(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key()).split(","));
+    this.recordKeyFields = KeyGenUtils.getRecordKeyFields(config);
     this.globalAvroDeleteKeyGenerator = new GlobalAvroDeleteKeyGenerator(config);
   }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.common.config.TypedProperties;
+import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieKeyGeneratorException;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 import org.apache.spark.sql.Row;
@@ -41,7 +42,7 @@ public class TimestampBasedKeyGenerator extends SimpleKeyGenerator {
   private final TimestampBasedAvroKeyGenerator timestampBasedAvroKeyGenerator;
 
   public TimestampBasedKeyGenerator(TypedProperties config) throws IOException {
-    this(config, config.getString(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key()),
+    this(config, config.getString(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key(), null),
         config.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME.key()));
   }
 
@@ -50,7 +51,7 @@ public TimestampBasedKeyGenerator(TypedProperties config) throws IOException {
   }
 
   TimestampBasedKeyGenerator(TypedProperties config, String recordKeyField, String partitionPathField) throws IOException {
-    super(config, recordKeyField, partitionPathField);
+    super(config, Option.ofNullable(recordKeyField), partitionPathField);
     timestampBasedAvroKeyGenerator = new TimestampBasedAvroKeyGenerator(config, recordKeyField, partitionPathField);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/keygen/BaseKeyGenerator.java
Patch:
@@ -32,6 +32,7 @@
  */
 public abstract class BaseKeyGenerator extends KeyGenerator {
 
+  public static final String EMPTY_PARTITION = "";
   protected List<String> recordKeyFields;
   protected List<String> partitionPathFields;
   protected final boolean encodePartitionPath;

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieDeltaStreamerWrapper.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.WriteOperationType;
+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.utilities.deltastreamer.DeltaSync;
 import org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer;
@@ -78,7 +79,8 @@ public JavaRDD<WriteStatus> compact() throws Exception {
   public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> fetchSource() throws Exception {
     DeltaSync service = getDeltaSync();
     service.refreshTimeline();
-    return service.readFromSource(service.getCommitsTimelineOpt());
+    String instantTime = HoodieActiveTimeline.createNewInstantTime();
+    return service.readFromSource(service.getCommitsTimelineOpt(), instantTime);
   }
 
   public DeltaSync getDeltaSync() {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/InProcessLockProvider.java
Patch:
@@ -124,7 +124,6 @@ public void close() {
       lock.writeLock().unlock();
     }
     LOG.info(getLogMessage(LockState.ALREADY_RELEASED));
-    LOCK_INSTANCE_PER_BASEPATH.remove(basePath);
   }
 
   private String getLogMessage(LockState state) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -537,7 +537,8 @@ protected void postCommit(HoodieTable table, HoodieCommitMetadata metadata, Stri
    */
   protected void mayBeCleanAndArchive(HoodieTable table) {
     autoCleanOnCommit();
-    autoArchiveOnCommit(table);
+    // reload table to that timeline reflects the clean commit
+    autoArchiveOnCommit(createTable(config, hadoopConf));
   }
 
   protected void runTableServicesInline(HoodieTable table, HoodieCommitMetadata metadata, Option<Map<String, String>> extraMetadata) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java
Patch:
@@ -43,7 +43,7 @@ public static HoodieFileSliceReader getFileSliceReader(
       Iterator<HoodieRecord> baseIterator = baseFileReader.get().getRecordIterator(schema);
       while (baseIterator.hasNext()) {
         scanner.processNextRecord(baseIterator.next().wrapIntoHoodieRecordPayloadWithParams(schema, props,
-            simpleKeyGenFieldsOpt, scanner.isWithOperationField(), scanner.getPartitionNameOverride(), false));
+            simpleKeyGenFieldsOpt, scanner.isWithOperationField(), scanner.getPartitionNameOverride(), false, Option.empty()));
       }
     }
     return new HoodieFileSliceReader(scanner.iterator());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java
Patch:
@@ -101,7 +101,7 @@ record -> new ImmutablePair<>(record.getPartitionPath(), record.getRecordKey()))
     }
 
     // Step 3: Tag the incoming records, as inserts or updates, by joining with existing record keys
-    HoodieData<HoodieRecord<R>> taggedRecords = tagLocationBacktoRecords(keyFilenamePairs, records);
+    HoodieData<HoodieRecord<R>> taggedRecords = tagLocationBacktoRecords(keyFilenamePairs, records, hoodieTable);
 
     if (config.getBloomIndexUseCaching()) {
       records.unpersist();
@@ -306,7 +306,8 @@ HoodiePairData<HoodieFileGroupId, String> explodeRecordsWithFileComparisons(
    */
   protected <R> HoodieData<HoodieRecord<R>> tagLocationBacktoRecords(
       HoodiePairData<HoodieKey, HoodieRecordLocation> keyFilenamePair,
-      HoodieData<HoodieRecord<R>> records) {
+      HoodieData<HoodieRecord<R>> records,
+      HoodieTable hoodieTable) {
     HoodiePairData<HoodieKey, HoodieRecord<R>> keyRecordPairs =
         records.mapToPair(record -> new ImmutablePair<>(record.getKey(), record));
     // Here as the records might have more data than keyFilenamePairs (some row keys' fileId is null),

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieKeyLookupHandle.java
Patch:
@@ -100,13 +100,13 @@ public HoodieKeyLookupResult getLookupResult() {
       LOG.debug("#The candidate row keys for " + partitionPathFileIDPair + " => " + candidateRecordKeys);
     }
 
-    HoodieBaseFile dataFile = getLatestDataFile();
-    List<String> matchingKeys = HoodieIndexUtils.filterKeysFromFile(new Path(dataFile.getPath()), candidateRecordKeys,
+    HoodieBaseFile baseFile = getLatestBaseFile();
+    List<String> matchingKeys = HoodieIndexUtils.filterKeysFromFile(new Path(baseFile.getPath()), candidateRecordKeys,
         hoodieTable.getHadoopConf());
     LOG.info(
         String.format("Total records (%d), bloom filter candidates (%d)/fp(%d), actual matches (%d)", totalKeysChecked,
             candidateRecordKeys.size(), candidateRecordKeys.size() - matchingKeys.size(), matchingKeys.size()));
     return new HoodieKeyLookupResult(partitionPathFileIDPair.getRight(), partitionPathFileIDPair.getLeft(),
-        dataFile.getCommitTime(), matchingKeys);
+        baseFile.getCommitTime(), matchingKeys);
   }
 }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java
Patch:
@@ -19,8 +19,8 @@
 package org.apache.hudi.client;
 
 import org.apache.hudi.common.fs.FSUtils;
-import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieAvroIndexedRecord;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
@@ -185,7 +185,7 @@ public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exc
     final HoodieWriteConfig config = makeHoodieClientConfig("/exampleEvolvedSchemaChangeOrder.avsc");
     final HoodieSparkTable table = HoodieSparkTable.create(config, context);
     String recordStr = "{\"_row_key\":\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\","
-        + "\"time\":\"2016-01-31T03:16:41.415Z\",\"added_field\":1},\"number\":12";
+        + "\"time\":\"2016-01-31T03:16:41.415Z\",\"added_field\":1,\"number\":12}";
     List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());
     String assertMsg = "UpdateFunction could not read records written with exampleSchema.avsc using the "
         + "exampleEvolvedSchemaChangeOrder.avsc as column order change";

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java
Patch:
@@ -635,7 +635,8 @@ private void processDataBlock(HoodieDataBlock dataBlock, Option<KeySpec> keySpec
                 recordKeyPartitionPathFieldPair,
                 this.withOperationField,
                 this.partitionNameOverrideOpt,
-                populateMetaFields);
+                populateMetaFields,
+                Option.empty());
         processNextRecord(completedRecord);
         totalLogRecords.incrementAndGet();
       }

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -381,7 +381,7 @@ private HoodieRecord<HoodieMetadataPayload> composeRecord(GenericRecord avroReco
     return SpillableMapUtils.convertToHoodieRecordPayload(avroRecord,
         metadataTableConfig.getPayloadClass(), metadataTableConfig.getPreCombineField(),
         Pair.of(metadataTableConfig.getRecordKeyFieldProp(), metadataTableConfig.getPartitionFieldProp()),
-        false, Option.of(partitionName));
+        false, Option.of(partitionName), Option.empty());
   }
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/config/ConfigProperty.java
Patch:
@@ -171,7 +171,7 @@ public <U extends Enum<U>> ConfigProperty<T> withDocumentation(Class<U> e, Strin
     Objects.requireNonNull(description);
     sb.append(description.value());
     for (Field f: e.getFields()) {
-      if (isValid(f.getName())) {
+      if (f.isEnumConstant() && isValid(f.getName())) {
         EnumFieldDescription fieldDescription = f.getAnnotation(EnumFieldDescription.class);
         Objects.requireNonNull(fieldDescription);
         sb.append("\n    ");

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java
Patch:
@@ -398,7 +398,7 @@ private List<MergeOnReadInputSplit> buildInputSplits() {
             .partitionPruner(partitionPruner)
             .build();
         final boolean cdcEnabled = this.conf.getBoolean(FlinkOptions.CDC_ENABLED);
-        final IncrementalInputSplits.Result result = incrementalInputSplits.inputSplits(metaClient, hadoopConf, cdcEnabled);
+        final IncrementalInputSplits.Result result = incrementalInputSplits.inputSplits(metaClient, cdcEnabled);
         if (result.isEmpty()) {
           // When there is no input splits, just return an empty source.
           LOG.warn("No input splits generate for incremental read, returns empty collection instead");

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -285,7 +285,7 @@ public DeltaSync(HoodieDeltaStreamer.Config cfg, SparkSession sparkSession, Sche
     // Register User Provided schema first
     registerAvroSchemas(schemaProvider);
 
-    this.transformer = UtilHelpers.createTransformer(cfg.transformerClassNames);
+    this.transformer = UtilHelpers.createTransformer(Option.ofNullable(cfg.transformerClassNames));
 
     this.metrics = (HoodieIngestionMetrics) ReflectionUtils.loadClass(cfg.ingestionMetricsClass, getHoodieClientConfig(this.schemaProvider));
     this.hoodieMetrics = new HoodieMetrics(getHoodieClientConfig(this.schemaProvider));

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieDeltaStreamerWrapper.java
Patch:
@@ -78,7 +78,7 @@ public JavaRDD<WriteStatus> compact() throws Exception {
   public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> fetchSource() throws Exception {
     DeltaSync service = getDeltaSync();
     service.refreshTimeline();
-    return service.readFromSource(service.getCommitTimelineOpt());
+    return service.readFromSource(service.getCommitsTimelineOpt());
   }
 
   public DeltaSync getDeltaSync() {

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -160,7 +160,7 @@ public final class HoodieMetadataConfig extends HoodieConfig {
 
   public static final ConfigProperty<Integer> COLUMN_STATS_INDEX_PARALLELISM = ConfigProperty
       .key(METADATA_PREFIX + ".index.column.stats.parallelism")
-      .defaultValue(10)
+      .defaultValue(200)
       .markAdvanced()
       .sinceVersion("0.11.0")
       .withDocumentation("Parallelism to use, when generating column stats index.");

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableFactory.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.table;
 
+import org.apache.hudi.avro.AvroSchemaUtils;
 import org.apache.hudi.common.model.DefaultHoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableConfig;
@@ -425,7 +426,7 @@ private void setupSortOptions(Configuration conf, ReadableConfig contextConfig)
   private static void inferAvroSchema(Configuration conf, LogicalType rowType) {
     if (!conf.getOptional(FlinkOptions.SOURCE_AVRO_SCHEMA_PATH).isPresent()
         && !conf.getOptional(FlinkOptions.SOURCE_AVRO_SCHEMA).isPresent()) {
-      String inferredSchema = AvroSchemaConverter.convertToSchema(rowType).toString();
+      String inferredSchema = AvroSchemaConverter.convertToSchema(rowType, AvroSchemaUtils.getAvroRecordQualifiedName(conf.getString(FlinkOptions.TABLE_NAME))).toString();
       conf.setString(FlinkOptions.SOURCE_AVRO_SCHEMA, inferredSchema);
     }
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java
Patch:
@@ -181,7 +181,7 @@ public static void main(String[] args) throws IOException {
     // Take input configs
     final Config cfg = new Config();
     new JCommander(cfg, null, args);
-    LOG.info(String.format("Snapshot hoodie table from %s targetBasePath to %stargetBasePath", cfg.basePath,
+    LOG.info(String.format("Snapshot hoodie table from %s (source) to %s (target)", cfg.basePath,
         cfg.outputPath));
 
     // Create a spark job to do the snapshot copy

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/config/CloudSourceConfig.java
Patch:
@@ -43,8 +43,8 @@ public class CloudSourceConfig extends HoodieConfig {
        * If batch size is too big, two possible issues can happen:
        * i) Acknowledgement takes too long (given that Hudi needs to commit first).
        * ii) In the case of Google Cloud Pubsub:
-       *   a) it will keep delivering the same message since it wasn't acked in time.
-       *   b) The size of the request that acks outstanding messages may exceed the limit,
+       *   a) it will keep delivering the same message since it wasn't acknowledged in time.
+       *   b) The size of the request that acknowledges outstanding messages may exceed the limit,
        *      which is 512KB as per Google's docs. See: https://cloud.google.com/pubsub/quotas#resource_limits
        */
       .defaultValue(10)

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/config/HoodieDeltaStreamerConfig.java
Patch:
@@ -76,7 +76,7 @@ public class HoodieDeltaStreamerConfig extends HoodieConfig {
       .key(DELTA_STREAMER_CONFIG_PREFIX + "multiwriter.source.checkpoint.id")
       .noDefaultValue()
       .markAdvanced()
-      .withDocumentation("Unique Id to be used for multiwriter deltastreamer scenario. This is the "
+      .withDocumentation("Unique Id to be used for multi-writer deltastreamer scenario. This is the "
           + "scenario when multiple deltastreamers are used to write to the same target table. If you are just using "
           + "a single deltastreamer for a table then you do not need to set this config.");
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltastreamerMultiWriterCkptUpdateFunc.java
Patch:
@@ -38,7 +38,7 @@
 /**
  * This is used as an extraPreCommitFunc in BaseHoodieWriteClient
  * It adds the checkpoint to deltacommit metadata. It must be implemented this way
- * because it needs the lock to ensure that it does not overwrite another deltastreamers
+ * because it needs the lock to ensure that it does not overwrite another deltastreamer's
  * latest checkpoint with an older one.
  */
 public class DeltastreamerMultiWriterCkptUpdateFunc implements BiConsumer<HoodieTableMetaClient, HoodieCommitMetadata> {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -95,7 +95,7 @@
 /**
  * An Utility which can incrementally take the output from {@link HiveIncrementalPuller} and apply it to the target
  * table. Does not maintain any state, queries at runtime to see how far behind the target table is from the source
- * table. This can be overriden to force sync from a timestamp.
+ * table. This can be overridden to force sync from a timestamp.
  * <p>
  * In continuous mode, DeltaStreamer runs in loop-mode going through the below operations (a) pull-from-source (b)
  * write-to-sink (c) Schedule Compactions if needed (d) Conditionally Sync to Hive each cycle. For MOR table with
@@ -386,7 +386,7 @@ public static class Config implements Serializable {
     public Integer maxRetryCount = 3;
 
     @Parameter(names = {"--allow-commit-on-no-checkpoint-change"}, description = "allow commits even if checkpoint has not changed before and after fetch data"
-        + "from souce. This might be useful in sources like SqlSource where there is not checkpoint. And is not recommended to enable in continuous mode.")
+        + "from source. This might be useful in sources like SqlSource where there is not checkpoint. And is not recommended to enable in continuous mode.")
     public Boolean allowCommitOnNoCheckpointChange = false;
 
     @Parameter(names = {"--help", "-h"}, help = true)

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudStoreIngestionConfig.java
Patch:
@@ -37,9 +37,9 @@ public class CloudStoreIngestionConfig {
    * If batch size is too big, two possible issues can happen:
    * i) Acknowledgement takes too long (given that Hudi needs to commit first).
    * ii) In the case of Google Cloud Pubsub:
-   *   a) it will keep delivering the same message since it wasn't acked in time.
-   *   b) The size of the request that acks outstanding messages may exceed the limit,
-   *      which is 512KB as per Google's docs. See: https://cloud.google.com/pubsub/quotas#resource_limits
+   * a) it will keep delivering the same message since it wasn't acknowledged in time.
+   * b) The size of the request that acknowledges outstanding messages may exceed the limit,
+   * which is 512KB as per Google's docs. See: https://cloud.google.com/pubsub/quotas#resource_limits
    */
   @Deprecated
   public static final int DEFAULT_BATCH_SIZE = CloudSourceConfig.BATCH_SIZE_CONF.defaultValue();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/MetadataMessage.java
Patch:
@@ -68,8 +68,8 @@ public MessageValidity shouldBeProcessed() {
     if (isOverwriteOfExistingFile()) {
       return new MessageValidity(DO_SKIP,
       "eventType: " + getEventType()
-              + ". Overwrite of existing objectId: " + getObjectId()
-              + " with generation numner: " + getOverwroteGeneration()
+          + ". Overwrite of existing objectId: " + getObjectId()
+          + " with generation number: " + getOverwroteGeneration()
       );
     }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHiveIncrementalPuller.java
Patch:
@@ -75,7 +75,7 @@ public void tearDown() throws Exception {
   public void testInitHiveIncrementalPuller() {
     assertDoesNotThrow(() -> {
       new HiveIncrementalPuller(config);
-    }, "Unexpected exception while initing HiveIncrementalPuller.");
+    }, "Unexpected exception while initializing HiveIncrementalPuller.");
   }
 
   private HiveIncrementalPuller.Config getHivePullerConfig(String incrementalSql) throws IOException {

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieIndexer.java
Patch:
@@ -403,15 +403,15 @@ private void scheduleAndExecuteIndexing(MetadataPartitionType partitionTypeToInd
     metaClient = reload(metaClient);
   }
 
-  private void indexMetadataPartitionsAndAssert(MetadataPartitionType partitionTypeToIndex, List<MetadataPartitionType> alreadyCompletedPartitions, List<MetadataPartitionType> nonExistantPartitions,
+  private void indexMetadataPartitionsAndAssert(MetadataPartitionType partitionTypeToIndex, List<MetadataPartitionType> alreadyCompletedPartitions, List<MetadataPartitionType> nonExistentPartitions,
                                                 String tableName) {
     scheduleAndExecuteIndexing(partitionTypeToIndex, tableName);
 
     // validate table config
     Set<String> completedPartitions = metaClient.getTableConfig().getMetadataPartitions();
     assertTrue(completedPartitions.contains(partitionTypeToIndex.getPartitionPath()));
     alreadyCompletedPartitions.forEach(entry -> assertTrue(completedPartitions.contains(entry.getPartitionPath())));
-    nonExistantPartitions.forEach(entry -> assertFalse(completedPartitions.contains(entry.getPartitionPath())));
+    nonExistentPartitions.forEach(entry -> assertFalse(completedPartitions.contains(entry.getPartitionPath())));
 
     // validate metadata partitions actually exist
     assertTrue(metadataPartitionExists(basePath(), context(), partitionTypeToIndex));

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -2547,7 +2547,7 @@ public void testFetchingCheckpointFromPreviousCommits() throws IOException {
     assertEquals(testDeltaSync.getLatestCommitMetadataWithValidCheckpointInfo(metaClient.getActiveTimeline()
         .getCommitsTimeline()).get().getMetadata(CHECKPOINT_KEY), "def");
 
-    // add a replace commit which does not have CEHCKPOINT_KEY. Deltastreamer should be able to go back and pick the right checkpoint.
+    // add a replace commit which does not have CHECKPOINT_KEY. Deltastreamer should be able to go back and pick the right checkpoint.
     addReplaceCommitToTimeline(metaClient, Collections.emptyMap());
     metaClient.reloadActiveTimeline();
     assertEquals(testDeltaSync.getLatestCommitMetadataWithValidCheckpointInfo(metaClient.getActiveTimeline()
@@ -2621,7 +2621,7 @@ public static class DistanceUDF implements UDF4<Double, Double, Double, Double,
     /**
      * Returns some random number as distance between the points.
      *
-     * @param lat1 Latitiude of source
+     * @param lat1 Latitude of source
      * @param lat2 Latitude of destination
      * @param lon1 Longitude of source
      * @param lon2 Longitude of destination

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/multisync/TestMultipleMetaSync.java
Patch:
@@ -56,7 +56,7 @@ void testMultipleMetaStore() throws Exception {
 
   @ParameterizedTest
   @MethodSource("withOneException")
-  void testeWithException(String syncClassNames) {
+  void testWithException(String syncClassNames) {
     String tableBasePath = basePath + "/test_multiple_metastore_exception";
     MockSyncTool1.syncSuccess = false;
     MockSyncTool2.syncSuccess = false;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/schema/TestFilebasedSchemaProvider.java
Patch:
@@ -81,7 +81,7 @@ public void renameBadlyFormattedSchemaTest() throws IOException {
   }
 
   @Test
-  public void renameBadlyFormattedSchemaWithProperyDisabledTest() {
+  public void renameBadlyFormattedSchemaWithPropertyDisabledTest() {
     assertThrows(SchemaParseException.class, () -> {
       new FilebasedSchemaProvider(
           Helpers.setupSchemaOnDFS("delta-streamer-config", "file_schema_provider_invalid.avsc"), jsc);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestGcsEventsSource.java
Patch:
@@ -163,7 +163,7 @@ public void shouldSkipInvalidMessages1() {
   }
 
   @Test
-  public void shouldGcsEventsSourceDoesNotDedupeInterally() {
+  public void shouldGcsEventsSourceDoesNotDedupeInternally() {
     ReceivedMessage dupe1 = fileCreateMessage("objectId-1", "{'data':{'bucket':'bucket-1'}}");
     ReceivedMessage dupe2 = fileCreateMessage("objectId-1", "{'data':{'bucket':'bucket-1'}}");
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestJsonKafkaSource.java
Patch:
@@ -223,9 +223,9 @@ void sendJsonSafeMessagesToKafka(String topic, int count, int numPartitions) {
   }
 
   @Test
-  public void testErrorEventsForDataInRowForamt() throws IOException {
+  public void testErrorEventsForDataInRowFormat() throws IOException {
     // topic setup.
-    final String topic = TEST_TOPIC_PREFIX + "testErrorEventsForDataInRowForamt";
+    final String topic = TEST_TOPIC_PREFIX + "testErrorEventsForDataInRowFormat";
 
     testUtils.createTopic(topic, 2);
     List<TopicPartition> topicPartitions = new ArrayList<>();

File: hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/quickstart/HoodieSparkQuickstart.java
Patch:
@@ -198,7 +198,7 @@ public static Dataset<Row> updateData(SparkSession spark, JavaSparkContext jsc,
   }
 
   /**
-   * Deleta data based in data information.
+   * Delete data based in data information.
    */
   public static Dataset<Row> delete(SparkSession spark, String tablePath, String tableName) {
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java
Patch:
@@ -565,7 +565,7 @@ private Table instantiateHiveTable(ObjectPath tablePath, CatalogBaseTable table,
     // the metadata fields should be included to keep sync with the hive sync tool,
     // because since Hive 3.x, there is validation when altering table,
     // when the metadata fields are synced through the hive sync tool,
-    // a compatability issue would be reported.
+    // a compatibility issue would be reported.
     boolean withOperationField = Boolean.parseBoolean(table.getOptions().getOrDefault(FlinkOptions.CHANGELOG_ENABLED.key(), "false"));
     List<FieldSchema> allColumns = HiveSchemaUtils.toHiveFieldSchema(table.getSchema(), withOperationField);
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/HoodiePipeline.java
Patch:
@@ -210,12 +210,12 @@ private static String getCreateHoodieTableDDL(
         .append(") NOT ENFORCED\n")
         .append(")\n");
     if (!partitionField.isEmpty()) {
-      String partitons = partitionField
+      String partitions = partitionField
           .stream()
           .map(partitionName -> "`" + partitionName + "`")
           .collect(Collectors.joining(","));
       builder.append("PARTITIONED BY (")
-          .append(partitons)
+          .append(partitions)
           .append(")\n");
     }
     builder.append("with ('connector' = 'hudi'");

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/bulk/TestRowDataKeyGen.java
Patch:
@@ -190,7 +190,7 @@ void testPrimaryKeylessWrite() {
   }
 
   @Test
-  void testRecoredKeyContainsTimestamp() {
+  void testRecordKeyContainsTimestamp() {
     Configuration conf = TestConfigurations.getDefaultConf("path1");
     conf.setString(FlinkOptions.RECORD_KEY_FIELD, "uuid,ts");
     conf.setString(KeyGeneratorOptions.KEYGENERATOR_CONSISTENT_LOGICAL_TIMESTAMP_ENABLED.key(), "true");

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java
Patch:
@@ -703,7 +703,7 @@ public String[] getLocations() throws IOException {
     }
 
     /**
-     * Prints this obejct as a string.
+     * Prints this object as a string.
      */
     @Override
     public String toString() {

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java
Patch:
@@ -386,8 +386,9 @@ public static List<FileStatus> filterIncrementalFileStatus(Job job, HoodieTableM
   }
 
   /**
-   * Takes in a list of filesStatus and a list of table metadatas. Groups the files status list
+   * Takes in a list of filesStatus and a list of table metadata. Groups the files status list
    * based on given table metadata.
+   *
    * @param fileStatuses
    * @param fileExtension
    * @param metaClientList

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/utils/TestHiveAvroSerializer.java
Patch:
@@ -57,7 +57,7 @@ public class TestHiveAvroSerializer {
       + "{\"name\":\"col8\",\"type\":[\"null\",\"boolean\"],\"default\":null},"
       + "{\"name\":\"col9\",\"type\":[\"null\",\"bytes\"],\"default\":null},"
       + "{\"name\":\"par\",\"type\":[\"null\",{\"type\":\"int\",\"logicalType\":\"date\"}],\"default\":null}]}";
-  private static final String NESTED_CHEMA = "{\"name\":\"MyClass\",\"type\":\"record\",\"namespace\":\"com.acme.avro\",\"fields\":["
+  private static final String NESTED_SCHEMA = "{\"name\":\"MyClass\",\"type\":\"record\",\"namespace\":\"com.acme.avro\",\"fields\":["
       + "{\"name\":\"firstname\",\"type\":\"string\"},"
       + "{\"name\":\"lastname\",\"type\":\"string\"},"
       + "{\"name\":\"student\",\"type\":{\"name\":\"student\",\"type\":\"record\",\"fields\":["
@@ -104,7 +104,7 @@ public void testSerialize() {
 
   @Test
   public void testNestedValueSerialize() {
-    Schema nestedSchema = new Schema.Parser().parse(NESTED_CHEMA);
+    Schema nestedSchema = new Schema.Parser().parse(NESTED_SCHEMA);
     GenericRecord avroRecord = new GenericData.Record(nestedSchema);
     avroRecord.put("firstname", "person1");
     avroRecord.put("lastname", "person2");

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieContinuousTestSuiteWriter.java
Patch:
@@ -75,11 +75,11 @@
  * --test-continuous-mode \
  * --min-sync-interval-seconds 20
  */
-public class HoodieContinousTestSuiteWriter extends HoodieTestSuiteWriter {
+public class HoodieContinuousTestSuiteWriter extends HoodieTestSuiteWriter {
 
-  private static Logger log = LoggerFactory.getLogger(HoodieContinousTestSuiteWriter.class);
+  private static Logger log = LoggerFactory.getLogger(HoodieContinuousTestSuiteWriter.class);
 
-  public HoodieContinousTestSuiteWriter(JavaSparkContext jsc, Properties props, HoodieTestSuiteJob.HoodieTestSuiteConfig cfg, String schema) throws Exception {
+  public HoodieContinuousTestSuiteWriter(JavaSparkContext jsc, Properties props, HoodieTestSuiteJob.HoodieTestSuiteConfig cfg, String schema) throws Exception {
     super(jsc, props, cfg, schema);
   }
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieMultiWriterTestSuiteJob.java
Patch:
@@ -52,7 +52,7 @@
  * Writer 1 DeltaStreamer ingesting data into partitions 0 to 10, Writer 2 Spark datasource ingesting data into partitions 100 to 110.
  * Multiple spark datasource writers, each writing to exclusive set of partitions.
  *
- * Example comamnd
+ * Example command
  * spark-submit
  * --packages org.apache.spark:spark-avro_2.11:2.4.0
  * --conf spark.task.cpus=3

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteJob.java
Patch:
@@ -18,9 +18,9 @@
 
 package org.apache.hudi.integ.testsuite;
 
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.fs.FSUtils;
-import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
@@ -318,7 +318,7 @@ public static class HoodieTestSuiteConfig extends HoodieDeltaStreamer.Config {
     public Boolean useHudiToGenerateUpdates = false;
 
     @Parameter(names = {"--test-continuous-mode"}, description = "Tests continuous mode in deltastreamer.")
-    public Boolean testContinousMode = false;
+    public Boolean testContinuousMode = false;
 
     @Parameter(names = {"--enable-presto-validation"}, description = "Enables presto validation")
     public Boolean enablePrestoValidation = false;
@@ -348,7 +348,7 @@ public static class HoodieTestSuiteConfig extends HoodieDeltaStreamer.Config {
     @Parameter(names = {"--index-type"}, description = "Index type to use for writes")
     public String indexType = "SIMPLE";
 
-    @Parameter(names = {"--enable-metadata-on-read"}, description = "Enable's metadata for queries")
+    @Parameter(names = {"--enable-metadata-on-read"}, description = "Enables metadata for queries")
     public Boolean enableMetadataOnRead = HoodieMetadataConfig.ENABLE.defaultValue();
   }
 }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/configuration/DFSDeltaConfig.java
Patch:
@@ -36,7 +36,7 @@ public class DFSDeltaConfig extends DeltaConfig {
   private final Long maxFileSize;
   // The current batch id
   private Integer batchId;
-  // Paralleism to use when generating input data
+  // Parallelism to use when generating input data
   private int inputParallelism;
   // Whether to delete older input data once it has been ingested
   private boolean deleteOldInputData;

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/configuration/DeltaConfig.java
Patch:
@@ -234,7 +234,7 @@ public int validateOnceEveryIteration() {
       return Integer.valueOf(configsMap.getOrDefault(VALIDATE_ONCE_EVERY_ITR, 1).toString());
     }
 
-    public String inputPartitonsToSkipWithValidate() {
+    public String inputPartitionsToSkipWithValidate() {
       return configsMap.getOrDefault(INPUT_PARTITIONS_TO_SKIP_VALIDATE, "").toString();
     }
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/WriterContext.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.common.config.SerializableConfiguration;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieException;
-import org.apache.hudi.integ.testsuite.HoodieContinousTestSuiteWriter;
+import org.apache.hudi.integ.testsuite.HoodieContinuousTestSuiteWriter;
 import org.apache.hudi.integ.testsuite.HoodieInlineTestSuiteWriter;
 import org.apache.hudi.integ.testsuite.HoodieTestSuiteJob.HoodieTestSuiteConfig;
 import org.apache.hudi.integ.testsuite.HoodieTestSuiteWriter;
@@ -72,7 +72,7 @@ public void initContext(JavaSparkContext jsc) throws HoodieException {
     try {
       this.schemaProvider = UtilHelpers.createSchemaProvider(cfg.schemaProviderClassName, props, jsc);
       String schemaStr = schemaProvider.getSourceSchema().toString();
-      this.hoodieTestSuiteWriter = (cfg.testContinousMode && cfg.useDeltaStreamer) ? new HoodieContinousTestSuiteWriter(jsc, props, cfg, schemaStr)
+      this.hoodieTestSuiteWriter = (cfg.testContinuousMode && cfg.useDeltaStreamer) ? new HoodieContinuousTestSuiteWriter(jsc, props, cfg, schemaStr)
           : new HoodieInlineTestSuiteWriter(jsc, props, cfg, schemaStr);
       int inputParallelism = cfg.inputParallelism > 0 ? cfg.inputParallelism : jsc.defaultParallelism();
       this.deltaGenerator = new DeltaGenerator(
@@ -81,7 +81,7 @@ public void initContext(JavaSparkContext jsc) throws HoodieException {
               schemaStr, cfg.limitFileSize, inputParallelism, cfg.deleteOldInput, cfg.useHudiToGenerateUpdates),
           jsc, sparkSession, schemaStr, keyGenerator);
       log.info(String.format("Initialized writerContext with: %s", schemaStr));
-      if (cfg.testContinousMode) {
+      if (cfg.testContinuousMode) {
         executorService = Executors.newFixedThreadPool(1);
         executorService.execute(new TestSuiteWriterRunnable(hoodieTestSuiteWriter));
       }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/CompactNode.java
Patch:
@@ -41,7 +41,7 @@ public CompactNode(Config config) {
    * if it has one.
    *
    * @param executionContext Execution context to run this compaction
-   * @param curItrCount      cur interation count.
+   * @param curItrCount      cur iteration count.
    * @throws Exception will be thrown if any error occurred.
    */
   @Override

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/DeltaGenerator.java
Patch:
@@ -94,7 +94,7 @@ public Pair<Integer, JavaRDD<DeltaWriteStats>> writeRecords(JavaRDD<GenericRecor
         FileSystem fs = FSUtils.getFs(oldInputDir.toString(), deltaOutputConfig.getConfiguration());
         fs.delete(oldInputDir, true);
       } catch (IOException e) {
-        log.error("Failed to delete older input data direcory " + oldInputDir, e);
+        log.error("Failed to delete older input data directory " + oldInputDir, e);
       }
     }
 

File: hudi-platform-service/hudi-metaserver/hudi-metaserver-server/src/main/java/org/apache/hudi/metaserver/HoodieMetaserverPreparations.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.metaserver.thrift.MetaserverStorageException;
 
 /**
- *  Metaserver storage Initialization that create tables defined in the DDLMappper.
+ * Metaserver storage Initialization that create tables defined in the DDLMapper.
  */
 public class HoodieMetaserverPreparations {
 

File: hudi-sync/hudi-datahub-sync/src/main/java/org/apache/hudi/sync/datahub/DataHubSyncClient.java
Patch:
@@ -19,14 +19,14 @@
 
 package org.apache.hudi.sync.datahub;
 
-import com.linkedin.common.Status;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.TableSchemaResolver;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.sync.common.HoodieSyncClient;
 import org.apache.hudi.sync.common.HoodieSyncException;
 import org.apache.hudi.sync.datahub.config.DataHubSyncConfig;
 
+import com.linkedin.common.Status;
 import com.linkedin.common.urn.DatasetUrn;
 import com.linkedin.data.template.SetMode;
 import com.linkedin.data.template.StringMap;
@@ -52,6 +52,7 @@
 import org.apache.avro.AvroTypeException;
 import org.apache.avro.Schema;
 import org.apache.parquet.schema.MessageType;
+
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
@@ -105,7 +106,7 @@ public void updateTableSchema(String tableName, MessageType schema) {
       MetadataChangeProposalWrapper schemaChange = createSchemaMetadataUpdate(tableName);
       emitter.emit(schemaChange, responseLogger).get();
 
-      // When updating an entity, it is ncessary to set its soft-delete status to false, or else the update won't get
+      // When updating an entity, it is necessary to set its soft-delete status to false, or else the update won't get
       // reflected in the UI.
       MetadataChangeProposalWrapper softDeleteUndoProposal = createUndoSoftDelete();
       emitter.emit(softDeleteUndoProposal, responseLogger).get();

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestService.java
Patch:
@@ -273,7 +273,7 @@ protected TSocket acceptImpl() throws TTransportException {
   private TServer startMetaStore(HiveConf conf) throws IOException {
     try {
       // Server will create new threads up to max as necessary. After an idle
-      // period, it will destory threads to keep the number of threads in the
+      // period, it will destroy threads to keep the number of threads in the
       // pool to min.
       String host = conf.getVar(ConfVars.HIVE_SERVER2_THRIFT_BIND_HOST);
       int port = conf.getIntVar(ConfVars.METASTORE_SERVER_PORT);

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/TimelineService.java
Patch:
@@ -105,7 +105,7 @@ public static class Config implements Serializable {
     @Parameter(names = {"--threads", "-t"}, description = "Number of threads to use for serving requests. The default number is 250")
     public int numThreads = DEFAULT_NUM_THREADS;
 
-    @Parameter(names = {"--async"}, description = "Use asyncronous request processing")
+    @Parameter(names = {"--async"}, description = "Use asynchronous request processing")
     public boolean async = false;
 
     @Parameter(names = {"--compress"}, description = "Compress output using gzip")

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLayoutConfig.java
Patch:
@@ -41,9 +41,9 @@ public class HoodieLayoutConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> LAYOUT_TYPE = ConfigProperty
       .key("hoodie.storage.layout.type")
-      .defaultValue("DEFAULT")
+      .defaultValue(HoodieStorageLayout.LayoutType.DEFAULT.name())
       .markAdvanced()
-      .withDocumentation("Type of storage layout. Possible options are [DEFAULT | BUCKET]");
+      .withDocumentation(HoodieStorageLayout.LayoutType.class);
 
   public static final ConfigProperty<String> LAYOUT_PARTITIONER_CLASS_NAME = ConfigProperty
       .key("hoodie.storage.layout.partitioner.class")

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -894,7 +894,7 @@ public PropertyBuilder setCDCEnabled(boolean cdcEnabled) {
     }
 
     public PropertyBuilder setCDCSupplementalLoggingMode(String cdcSupplementalLoggingMode) {
-      this.cdcSupplementalLoggingMode = cdcSupplementalLoggingMode;
+      this.cdcSupplementalLoggingMode = cdcSupplementalLoggingMode.toUpperCase();
       return this;
     }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/cdc/HoodieCDCInferenceCase.java
Patch:
@@ -24,10 +24,10 @@
  *
  * AS_IS:
  *   For this type, there must be a real cdc log file from which we get the whole/part change data.
- *   when `hoodie.table.cdc.supplemental.logging.mode` is {@link HoodieCDCSupplementalLoggingMode#data_before_after}, it keeps all the fields about the
+ *   when `hoodie.table.cdc.supplemental.logging.mode` is {@link HoodieCDCSupplementalLoggingMode#DATA_BEFORE_AFTER}, it keeps all the fields about the
  *   change data, including `op`, `ts_ms`, `before` and `after`. So read it and return directly,
  *   no more other files need to be loaded.
- *   when `hoodie.table.cdc.supplemental.logging.mode` is {@link HoodieCDCSupplementalLoggingMode#data_before}, it keeps the `op`, the key and the
+ *   when `hoodie.table.cdc.supplemental.logging.mode` is {@link HoodieCDCSupplementalLoggingMode#DATA_BEFORE}, it keeps the `op`, the key and the
  *   `before` of the changing record. When `op` is equal to 'i' or 'u', need to get the current record from the
  *   current base/log file as `after`.
  *   when `hoodie.table.cdc.supplemental.logging.mode` is 'op_key', it just keeps the `op` and the key of

File: hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java
Patch:
@@ -132,7 +132,7 @@ public static Map<HoodieFileGroupId, HoodieInstant> getAllFileGroupsInPendingClu
     } catch (Exception e) {
       if (e instanceof IllegalStateException && e.getMessage().contains("Duplicate key")) {
         throw new HoodieException("Found duplicate file groups pending clustering. If you're running deltastreamer in continuous mode, consider adding delay using --min-sync-interval-seconds. "
-            + "Or consider setting write concurrency mode to optimistic_concurrency_control.", e);
+            + "Or consider setting write concurrency mode to OPTIMISTIC_CONCURRENCY_CONTROL.", e);
       }
       throw new HoodieException("Error getting all file groups in pending clustering", e);
     }

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -551,7 +551,7 @@ public void testCDCBlock() throws IOException, InterruptedException {
         + "]}";
     Schema dataSchema = new Schema.Parser().parse(dataSchemaString);
     Schema cdcSchema = HoodieCDCUtils.schemaBySupplementalLoggingMode(
-        HoodieCDCSupplementalLoggingMode.data_before_after, dataSchema);
+        HoodieCDCSupplementalLoggingMode.DATA_BEFORE_AFTER, dataSchema);
     GenericRecord insertedRecord = new GenericData.Record(dataSchema);
     insertedRecord.put("uuid", 1);
     insertedRecord.put("name", "apple");

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -52,7 +52,7 @@
 import java.util.Map;
 import java.util.Set;
 
-import static org.apache.hudi.common.table.cdc.HoodieCDCSupplementalLoggingMode.data_before_after;
+import static org.apache.hudi.common.table.cdc.HoodieCDCSupplementalLoggingMode.DATA_BEFORE_AFTER;
 import static org.apache.hudi.common.util.PartitionPathEncodeUtils.DEFAULT_PARTITION_PATH;
 
 /**
@@ -175,7 +175,7 @@ private FlinkOptions() {
   public static final ConfigOption<String> SUPPLEMENTAL_LOGGING_MODE = ConfigOptions
       .key("cdc.supplemental.logging.mode")
       .stringType()
-      .defaultValue(data_before_after.name())
+      .defaultValue(DATA_BEFORE_AFTER.name())
       .withFallbackKeys(HoodieTableConfig.CDC_SUPPLEMENTAL_LOGGING_MODE.key())
       .withDescription("Setting 'op_key_only' persists the 'op' and the record key only, "
           + "setting 'data_before' persists the additional 'before' image, "

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/cdc/CdcInputFormat.java
Patch:
@@ -173,11 +173,11 @@ private ClosableIterator<RowData> getRecordIterator(
         Schema dataSchema = HoodieAvroUtils.removeMetadataFields(new Schema.Parser().parse(tableState.getAvroSchema()));
         Schema cdcSchema = HoodieCDCUtils.schemaBySupplementalLoggingMode(mode, dataSchema);
         switch (mode) {
-          case data_before_after:
+          case DATA_BEFORE_AFTER:
             return new BeforeAfterImageIterator(tablePath, tableState, hadoopConf, cdcSchema, fileSplit);
-          case data_before:
+          case DATA_BEFORE:
             return new BeforeImageIterator(conf, hadoopConf, tablePath, tableState, cdcSchema, fileSplit, imageManager);
-          case op_key_only:
+          case OP_KEY_ONLY:
             return new RecordKeyImageIterator(conf, hadoopConf, tablePath, tableState, cdcSchema, fileSplit, imageManager);
           default:
             throw new AssertionError("Unexpected mode" + mode);

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/configuration/TestOptionsInference.java
Patch:
@@ -71,7 +71,7 @@ void testSetupClientId() throws Exception {
 
   private Configuration getConf() {
     Configuration conf = new Configuration();
-    conf.setString(HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(), WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.value());
+    conf.setString(HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(), WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.name());
     conf.setString(FlinkOptions.PATH, tempFile.getAbsolutePath());
     return conf;
   }

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/TestStreamWriteOperatorCoordinator.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieWriteStat;
+import org.apache.hudi.common.model.WriteConcurrencyMode;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
@@ -363,7 +364,7 @@ void testLockForMetadataTable() throws Exception {
     Configuration conf = TestConfigurations.getDefaultConf(tempFile.getAbsolutePath());
     conf.setBoolean(FlinkOptions.METADATA_ENABLED, true);
 
-    conf.setString(HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(), "optimistic_concurrency_control");
+    conf.setString(HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(), WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.name());
     conf.setInteger("hoodie.write.lock.client.num_retries", 1);
 
     OperatorCoordinator.Context context = new MockOperatorCoordinatorContext(new OperatorID(), 1);

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/TestWriteCopyOnWrite.java
Patch:
@@ -468,7 +468,7 @@ public void testWriteExactlyOnce() throws Exception {
   //              | ----- txn2 ----- |
   @Test
   public void testWriteMultiWriterInvolved() throws Exception {
-    conf.setString(HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(), WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.value());
+    conf.setString(HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(), WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.name());
     conf.setString(FlinkOptions.INDEX_TYPE, HoodieIndex.IndexType.BUCKET.name());
     conf.setBoolean(FlinkOptions.PRE_COMBINE, true);
 
@@ -497,7 +497,7 @@ public void testWriteMultiWriterInvolved() throws Exception {
   //                       | ----- txn2 ----- |
   @Test
   public void testWriteMultiWriterPartialOverlapping() throws Exception {
-    conf.setString(HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(), WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.value());
+    conf.setString(HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(), WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.name());
     conf.setString(FlinkOptions.INDEX_TYPE, HoodieIndex.IndexType.BUCKET.name());
     conf.setBoolean(FlinkOptions.PRE_COMBINE, true);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -493,7 +493,7 @@ public abstract O bulkInsertPreppedRecords(I preppedRecords, final String instan
   public void preWrite(String instantTime, WriteOperationType writeOperationType,
       HoodieTableMetaClient metaClient) {
     setOperationType(writeOperationType);
-    this.lastCompletedTxnAndMetadata = txnManager.isNeedsLockGuard()
+    this.lastCompletedTxnAndMetadata = txnManager.isLockRequired()
         ? TransactionUtils.getLastCompletedTxnInstantAndMetadata(metaClient) : Option.empty();
     this.pendingInflightAndRequestedInstants = TransactionUtils.getInflightAndRequestedInstants(metaClient);
     this.pendingInflightAndRequestedInstants.remove(instantTime);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/DirectMarkerTransactionManager.java
Patch:
@@ -42,12 +42,12 @@ public class DirectMarkerTransactionManager extends TransactionManager {
   private final String filePath;
 
   public DirectMarkerTransactionManager(HoodieWriteConfig config, FileSystem fs, String partitionPath, String fileId) {
-    super(new LockManager(config, fs, createUpdatedLockProps(config, partitionPath, fileId)), config.needsLockGuard());
+    super(new LockManager(config, fs, createUpdatedLockProps(config, partitionPath, fileId)), config.isLockRequired());
     this.filePath = partitionPath + "/" + fileId;
   }
 
   public void beginTransaction(String newTxnOwnerInstantTime) {
-    if (needsLockGuard) {
+    if (isLockRequired) {
       LOG.info("Transaction starting for " + newTxnOwnerInstantTime + " and " + filePath);
       lockManager.lock();
 
@@ -57,7 +57,7 @@ public void beginTransaction(String newTxnOwnerInstantTime) {
   }
 
   public void endTransaction(String currentTxnOwnerInstantTime) {
-    if (needsLockGuard) {
+    if (isLockRequired) {
       LOG.info("Transaction ending with transaction owner " + currentTxnOwnerInstantTime
           + " for " + filePath);
       if (reset(Option.of(getInstant(currentTxnOwnerInstantTime)), Option.empty(), Option.empty())) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java
Patch:
@@ -94,7 +94,7 @@ public BaseCommitActionExecutor(HoodieEngineContext context, HoodieWriteConfig c
     this.taskContextSupplier = context.getTaskContextSupplier();
     // TODO : Remove this once we refactor and move out autoCommit method from here, since the TxnManager is held in {@link BaseHoodieWriteClient}.
     this.txnManagerOption = config.shouldAutoCommit() ? Option.of(new TransactionManager(config, table.getMetaClient().getFs())) : Option.empty();
-    if (this.txnManagerOption.isPresent() && this.txnManagerOption.get().isNeedsLockGuard()) {
+    if (this.txnManagerOption.isPresent() && this.txnManagerOption.get().isLockRequired()) {
       // these txn metadata are only needed for auto commit when optimistic concurrent control is also enabled
       this.lastCompletedTxn = TransactionUtils.getLastCompletedTxnInstantAndMetadata(table.getMetaClient());
       this.pendingInflightAndRequestedInstants = TransactionUtils.getInflightAndRequestedInstants(table.getMetaClient());

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -280,7 +280,7 @@ public void preWrite(String instantTime, WriteOperationType writeOperationType,
    * should be called before the Driver starts a new transaction.
    */
   public void preTxn(HoodieTableMetaClient metaClient) {
-    if (txnManager.isNeedsLockGuard()) {
+    if (txnManager.isLockRequired()) {
       // refresh the meta client which is reused
       metaClient.reloadActiveTimeline();
       this.lastCompletedTxnAndMetadata = TransactionUtils.getLastCompletedTxnInstantAndMetadata(metaClient);

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/OptionsResolver.java
Patch:
@@ -243,7 +243,7 @@ public static boolean isConsistentLogicalTimestampEnabled(Configuration conf) {
   /**
    * Returns whether the writer txn should be guarded by lock.
    */
-  public static boolean needsGuardByLock(Configuration conf) {
+  public static boolean isLockRequired(Configuration conf) {
     return conf.getBoolean(FlinkOptions.METADATA_ENABLED)
         || conf.getString(HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(), HoodieWriteConfig.WRITE_CONCURRENCY_MODE.defaultValue())
             .equalsIgnoreCase(WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.value());

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/FlinkWriteClients.java
Patch:
@@ -224,7 +224,7 @@ public static HoodieWriteConfig getHoodieClientConfig(
             .withProps(flinkConf2TypedProperties(conf))
             .withSchema(getSourceSchema(conf).toString());
 
-    if (OptionsResolver.needsGuardByLock(conf) && !conf.containsKey(HoodieLockConfig.LOCK_PROVIDER_CLASS_NAME.key())) {
+    if (OptionsResolver.isLockRequired(conf) && !conf.containsKey(HoodieLockConfig.LOCK_PROVIDER_CLASS_NAME.key())) {
       // configure the fs lock provider by default
       builder.withLockConfig(HoodieLockConfig.newBuilder()
           .withConflictResolutionStrategy(OptionsResolver.getConflictResolutionStrategy(conf))

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -2003,8 +2003,8 @@ public int getParquetPageSize() {
     return getInt(HoodieStorageConfig.PARQUET_PAGE_SIZE);
   }
 
-  public int getLogFileDataBlockMaxSize() {
-    return getInt(HoodieStorageConfig.LOGFILE_DATA_BLOCK_MAX_SIZE);
+  public long getLogFileDataBlockMaxSize() {
+    return getLong(HoodieStorageConfig.LOGFILE_DATA_BLOCK_MAX_SIZE);
   }
 
   public double getParquetCompressionRatio() {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -115,9 +115,9 @@ public class HoodieAppendHandle<T, I, K, O> extends HoodieWriteHandle<T, I, K, O
   // Total number of bytes written during this append phase (an estimation)
   protected long estimatedNumberOfBytesWritten;
   // Number of records that must be written to meet the max block size for a log block
-  private int numberOfRecords = 0;
+  private long numberOfRecords = 0;
   // Max block size to limit to for a log block
-  private final int maxBlockSize = config.getLogFileDataBlockMaxSize();
+  private final long maxBlockSize = config.getLogFileDataBlockMaxSize();
   // Header metadata for a log block
   protected final Map<HeaderMetadataType, String> header = new HashMap<>();
   private SizeEstimator<HoodieRecord> sizeEstimator;
@@ -593,7 +593,7 @@ private void flushToDiskIfRequired(HoodieRecord record, boolean appendDeleteBloc
     }
 
     // Append if max number of records reached to achieve block size
-    if (numberOfRecords >= (int) (maxBlockSize / averageRecordSize)) {
+    if (numberOfRecords >= (long) (maxBlockSize / averageRecordSize)) {
       // Recompute averageRecordSize before writing a new block and update existing value with
       // avg of new and old
       LOG.info("Flush log block to disk, the current avgRecordSize => " + averageRecordSize);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCDCLogger.java
Patch:
@@ -92,7 +92,7 @@ public class HoodieCDCLogger implements Closeable {
   private final CDCTransformer transformer;
 
   // Max block size to limit to for a log block
-  private final int maxBlockSize;
+  private final long maxBlockSize;
 
   // Average cdc record size. This size is updated at the end of every log block flushed to disk
   private long averageCDCRecordSize = 0;

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieStorageConfig.java
Patch:
@@ -330,7 +330,7 @@ public Builder hfileBlockSize(int blockSize) {
       return this;
     }
 
-    public Builder logFileDataBlockMaxSize(int dataBlockSize) {
+    public Builder logFileDataBlockMaxSize(long dataBlockSize) {
       storageConfig.setValue(LOGFILE_DATA_BLOCK_MAX_SIZE, String.valueOf(dataBlockSize));
       return this;
     }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/StreamReadMonitoringFunction.java
Patch:
@@ -242,11 +242,9 @@ public void cancel() {
     if (checkpointLock != null) {
       // this is to cover the case where cancel() is called before the run()
       synchronized (checkpointLock) {
-        issuedInstant = null;
         isRunning = false;
       }
     } else {
-      issuedInstant = null;
       isRunning = false;
     }
   }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/ClusteringOperator.java
Patch:
@@ -138,9 +138,8 @@ public ClusteringOperator(Configuration conf, RowType rowType) {
 
     // target size should larger than small file limit
     this.conf.setLong(FlinkOptions.CLUSTERING_PLAN_STRATEGY_SMALL_FILE_LIMIT.key(),
-        this.conf.getLong(FlinkOptions.CLUSTERING_PLAN_STRATEGY_TARGET_FILE_MAX_BYTES) > this.conf.getLong(FlinkOptions.CLUSTERING_PLAN_STRATEGY_SMALL_FILE_LIMIT)
-          ? this.conf.getLong(FlinkOptions.CLUSTERING_PLAN_STRATEGY_SMALL_FILE_LIMIT)
-            : this.conf.getLong(FlinkOptions.CLUSTERING_PLAN_STRATEGY_TARGET_FILE_MAX_BYTES));
+        Math.min(this.conf.getLong(FlinkOptions.CLUSTERING_PLAN_STRATEGY_TARGET_FILE_MAX_BYTES) / 1024 / 1024,
+            this.conf.getLong(FlinkOptions.CLUSTERING_PLAN_STRATEGY_SMALL_FILE_LIMIT)));
   }
 
   @Override

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaWriteHelper.java
Patch:
@@ -79,7 +79,7 @@ protected List<HoodieRecord<T>> doDeduplicateRecords(
       // we cannot allow the user to change the key or partitionPath, since that will affect
       // everything
       // so pick it from one of the records.
-      return reducedRecord.newInstance(rec1.getKey());
+      return reducedRecord.newInstance(rec1.getKey(), rec1.getOperation());
     }).orElse(null)).filter(Objects::nonNull).collect(Collectors.toList());
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroRecord.java
Patch:
@@ -117,7 +117,7 @@ public HoodieRecordType getRecordType() {
 
   @Override
   public Object[] getColumnValues(Schema recordSchema, String[] columns, boolean consistentLogicalTimestampEnabled) {
-    return new Object[]{HoodieAvroUtils.getRecordColumnValues(this, columns, recordSchema, consistentLogicalTimestampEnabled)};
+    return HoodieAvroUtils.getRecordColumnValues(this, columns, recordSchema, consistentLogicalTimestampEnabled);
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java
Patch:
@@ -79,8 +79,8 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
-import java.util.Set;
 import java.util.Properties;
+import java.util.Set;
 import java.util.TimeZone;
 import java.util.regex.Pattern;
 import java.util.stream.Collectors;
@@ -1116,9 +1116,9 @@ public static HoodieRecord createHoodieRecordFromAvro(
    * Given avro records, rewrites them with new schema.
    *
    * @param oldRecords oldRecords to be rewrite
-   * @param newSchema newSchema used to rewrite oldRecord
+   * @param newSchema  newSchema used to rewrite oldRecord
    * @param renameCols a map store all rename cols, (k, v)-> (colNameFromNewSchema, colNameFromOldSchema)
-   * @return a iterator of rewrote GeneriRcords
+   * @return an iterator of rewrote {@link GenericRecord}
    */
   public static Iterator<GenericRecord> rewriteRecordWithNewSchema(Iterator<GenericRecord> oldRecords, Schema newSchema, Map<String, String> renameCols, boolean validate) {
     if (oldRecords == null || newSchema == null) {

File: hudi-common/src/main/java/org/apache/hudi/common/bloom/InternalDynamicBloomFilter.java
Patch:
@@ -27,7 +27,7 @@
 
 /**
  * Hoodie's internal dynamic Bloom Filter. This is largely based of {@link org.apache.hadoop.util.bloom.DynamicBloomFilter}
- * with bounds on maximum number of entries. Once the max entries is reached, false positive gaurantees are not
+ * with bounds on maximum number of entries. Once the max entries is reached, false positive guarantees are not
  * honored.
  */
 class InternalDynamicBloomFilter extends InternalFilter {

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -242,7 +242,7 @@ public final class HoodieMetadataConfig extends HoodieConfig {
       .defaultValue(false)
       .markAdvanced()
       .sinceVersion("0.13.0")
-      .withDocumentation("Optimized log blocks scanner that addresses all the multiwriter use-cases while appending to log files. "
+      .withDocumentation("Optimized log blocks scanner that addresses all the multi-writer use-cases while appending to log files. "
           + "It also differentiates original blocks written by ingestion writers and compacted blocks written by log compaction.");
 
   private HoodieMetadataConfig() {

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FileSystemRetryConfig.java
Patch:
@@ -69,7 +69,7 @@ public class FileSystemRetryConfig  extends HoodieConfig {
       .defaultValue("")
       .markAdvanced()
       .sinceVersion("0.11.0")
-      .withDocumentation("The class name of the Exception that needs to be re-tryed, separated by commas. "
+      .withDocumentation("The class name of the Exception that needs to be retried, separated by commas. "
           + "Default is empty which means retry all the IOException and RuntimeException from FileSystem");
 
   private FileSystemRetryConfig() {

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodiePartitionMetadata.java
Patch:
@@ -121,7 +121,7 @@ public void trySave(int taskPartitionId) {
         fs.rename(tmpMetaPath, metaPath);
       }
     } catch (IOException ioe) {
-      LOG.warn("Error trying to save partition metadata (this is okay, as long as atleast 1 of these succced), "
+      LOG.warn("Error trying to save partition metadata (this is okay, as long as at least 1 of these succeeded), "
           + partitionPath, ioe);
     } finally {
       if (!metafileExists) {

File: hudi-common/src/main/java/org/apache/hudi/common/model/RewriteAvroPayload.java
Patch:
@@ -27,7 +27,7 @@
 import java.io.IOException;
 
 /**
- * Default payload used for rewrite use cases where we dont change schema. We dont need to serialize/deserialize avro record in payload.
+ * Default payload used for rewrite use cases where we don't change schema. We dont need to serialize/deserialize avro record in payload.
  */
 public class RewriteAvroPayload implements HoodieRecordPayload<RewriteAvroPayload> {
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/debezium/MySqlDebeziumAvroPayload.java
Patch:
@@ -38,7 +38,7 @@
  * - For inserts, op=i
  * - For deletes, op=d
  * - For updates, op=u
- * - For snapshort inserts, op=r
+ * - For snapshot inserts, op=r
  * <p>
  * This payload implementation will issue matching insert, delete, updates against the hudi table
  */

File: hudi-common/src/main/java/org/apache/hudi/common/model/debezium/PostgresDebeziumAvroPayload.java
Patch:
@@ -42,7 +42,7 @@
  * - For inserts, op=i
  * - For deletes, op=d
  * - For updates, op=u
- * - For snapshort inserts, op=r
+ * - For snapshot inserts, op=r
  * <p>
  * This payload implementation will issue matching insert, delete, updates against the hudi table
  */

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java
Patch:
@@ -33,9 +33,9 @@
 import org.apache.hudi.common.table.log.block.HoodieDeleteBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
-import org.apache.hudi.common.util.collection.ClosableIterator;
 import org.apache.hudi.common.util.InternalSchemaCache;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.collection.ClosableIterator;
 import org.apache.hudi.common.util.collection.CloseableMappingIterator;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
@@ -408,7 +408,7 @@ private void scanInternalV2(Option<KeySpec> keySpecOption, boolean skipProcessin
        * First traversal to identify the rollback blocks and valid data and compacted blocks.
        *
        * Scanning blocks is easy to do in single writer mode, where the rollback block is right after the effected data blocks.
-       * With multiwriter mode the blocks can be out of sync. An example scenario.
+       * With multi-writer mode the blocks can be out of sync. An example scenario.
        * B1, B2, B3, B4, R1(B3), B5
        * In this case, rollback block R1 is invalidating the B3 which is not the previous block.
        * This becomes more complicated if we have compacted blocks, which are data blocks created using log compaction.

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java
Patch:
@@ -140,7 +140,7 @@ public interface HoodieTimeline extends Serializable {
   /**
    * Filter this timeline to just include the in-flights excluding major and minor compaction instants.
    *
-   * @return New instance of HoodieTimeline with just in-flights excluding majoe and minor compaction instants
+   * @return New instance of HoodieTimeline with just in-flights excluding major and minor compaction instants
    */
   HoodieTimeline filterPendingExcludingMajorAndMinorCompaction();
 
@@ -535,7 +535,7 @@ static String makeRequestedCompactionFileName(String instantTime) {
     return StringUtils.join(instantTime, HoodieTimeline.REQUESTED_COMPACTION_EXTENSION);
   }
 
-  // Log comaction action
+  // Log compaction action
   static String makeInflightLogCompactionFileName(String instantTime) {
     return StringUtils.join(instantTime, HoodieTimeline.INFLIGHT_LOG_COMPACTION_EXTENSION);
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/MetadataMigrator.java
Patch:
@@ -79,7 +79,7 @@ public T migrateToVersion(T metadata, int metadataVersion, int targetVersion) {
     if (metadataVersion == targetVersion) {
       return metadata;
     } else if (metadataVersion > targetVersion) {
-      return dowgradeToVersion(metadata, metadataVersion, targetVersion);
+      return downgradeToVersion(metadata, metadataVersion, targetVersion);
     } else {
       return upgradeToVersion(metadata, metadataVersion, targetVersion);
     }
@@ -95,7 +95,7 @@ private T upgradeToVersion(T metadata, int metadataVersion, int targetVersion) {
     return metadata;
   }
 
-  private T dowgradeToVersion(T metadata, int metadataVersion, int targetVersion) {
+  private T downgradeToVersion(T metadata, int metadataVersion, int targetVersion) {
     int newVersion = metadataVersion - 1;
     while (newVersion >= targetVersion) {
       VersionMigrator<T> downgrader = migrators.get(newVersion);

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewStorageConfig.java
Patch:
@@ -163,7 +163,7 @@ public class FileSystemViewStorageConfig extends HoodieConfig {
       .defaultValue("")
       .markAdvanced()
       .sinceVersion("0.12.1")
-      .withDocumentation("The class name of the Exception that needs to be re-tryed, separated by commas. "
+      .withDocumentation("The class name of the Exception that needs to be retried, separated by commas. "
           + "Default is empty which means retry all the IOException and RuntimeException from Remote Request.");
 
   public static final ConfigProperty<String> REMOTE_BACKUP_VIEW_ENABLE = ConfigProperty

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java
Patch:
@@ -422,7 +422,7 @@ protected void applyDeltaFileSlicesToPartitionView(String partition, List<Hoodie
         .map(FileSlice::getBaseFile).filter(Option::isPresent).map(Option::get)
         .map(df -> Pair.of(Path.getPathWithoutSchemeAndAuthority(new Path(df.getPath())).toString(), df))
         .collect(Collectors.toMap(Pair::getKey, Pair::getValue));
-    // Note: Delta Log Files and Data FIles can be empty when adding/removing pending compactions
+    // Note: Delta Log Files and Data Files can be empty when adding/removing pending compactions
     Map<String, HoodieBaseFile> deltaDataFiles = deltaFileGroups.stream().flatMap(HoodieFileGroup::getAllRawFileSlices)
         .map(FileSlice::getBaseFile).filter(Option::isPresent).map(Option::get)
         .map(df -> Pair.of(Path.getPathWithoutSchemeAndAuthority(new Path(df.getPath())).toString(), df))

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/SyncableFileSystemView.java
Patch:
@@ -42,7 +42,7 @@ public interface SyncableFileSystemView
 
   /**
    * Read the latest timeline and refresh the file-system view to match the current state of the file-system. The
-   * refresh can either be done incrementally (from reading file-slices in metadata files) or from scratch by reseting
+   * refresh can either be done incrementally (from reading file-slices in metadata files) or from scratch by resetting
    * view storage.
    */
   void sync();

File: hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java
Patch:
@@ -74,7 +74,7 @@ public boolean acquire(int numOps) {
       while (!semaphore.tryAcquire(numOps)) {
         Thread.sleep(WAIT_BEFORE_NEXT_ACQUIRE_PERMIT_IN_MS);
       }
-      LOG.debug(String.format("acquire permits: %s, maxPremits: %s", numOps, maxPermits));
+      LOG.debug(String.format("acquire permits: %s, maxPermits: %s", numOps, maxPermits));
     } catch (InterruptedException e) {
       throw new RuntimeException("Unable to acquire permits", e);
     }
@@ -88,7 +88,7 @@ public void stop() {
   public void releasePermitsPeriodically() {
     scheduler = Executors.newScheduledThreadPool(SCHEDULER_CORE_THREAD_POOL_SIZE);
     scheduler.scheduleAtFixedRate(() -> {
-      LOG.debug(String.format("Release permits: maxPremits: %s, available: %s", maxPermits,
+      LOG.debug(String.format("Release permits: maxPermits: %s, available: %s", maxPermits,
           semaphore.availablePermits()));
       semaphore.release(maxPermits - semaphore.availablePermits());
     }, RELEASE_PERMITS_PERIOD_IN_SECONDS, RELEASE_PERMITS_PERIOD_IN_SECONDS, timePeriod);

File: hudi-common/src/main/java/org/apache/hudi/common/util/ReflectionUtils.java
Patch:
@@ -188,7 +188,7 @@ public static Object invokeStaticMethod(String clazz, String methodName, Object[
     } catch (NoSuchMethodException e) {
       throw new HoodieException(String.format("Unable to find the method %s of the class %s ",  methodName, clazz), e);
     } catch (InvocationTargetException | IllegalAccessException e) {
-      throw new HoodieException(String.format("Unable to invoke the methond %s of the class %s ",  methodName, clazz), e);
+      throw new HoodieException(String.format("Unable to invoke the method %s of the class %s ", methodName, clazz), e);
     }
   }
 

File: hudi-common/src/main/java/org/apache/hudi/exception/HoodieHeartbeatException.java
Patch:
@@ -22,7 +22,7 @@
 
 /**
  * <p>
- * Exception thrown for Hoodie hearbeat failures. The root of the exception hierarchy.
+ * Exception thrown for Hoodie heartbeat failures. The root of the exception hierarchy.
  * </p>
  * <p>
  * Hoodie Write/Read clients will throw this exception if any of its operations fail. This is a runtime (unchecked)

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/InternalSchemaBuilder.java
Patch:
@@ -71,7 +71,7 @@ public Map<String, Integer> buildNameToId(Type type) {
    * Use to traverse all types in internalSchema with visitor.
    *
    * @param schema hoodie internal schema
-   * @return vistor expected result.
+   * @return visitor expected result.
    */
   public <T> T visit(InternalSchema schema, InternalSchemaVisitor<T> visitor) {
     return visitor.schema(schema, visit(schema.getRecord(), visitor));

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChange.java
Patch:
@@ -163,9 +163,9 @@ public BaseColumnChange addPositionChange(String srcName, String dsrName, String
     protected abstract Integer findIdByFullName(String fullName);
 
     // Modify hudi meta columns is prohibited
-    protected void checkColModifyIsLegal(String colNeedToModfiy) {
-      if (HoodieRecord.HOODIE_META_COLUMNS.stream().anyMatch(f -> f.equalsIgnoreCase(colNeedToModfiy))) {
-        throw new IllegalArgumentException(String.format("cannot modify hudi meta col: %s", colNeedToModfiy));
+    protected void checkColModifyIsLegal(String colNeedToModify) {
+      if (HoodieRecord.HOODIE_META_COLUMNS.stream().anyMatch(f -> f.equalsIgnoreCase(colNeedToModify))) {
+        throw new IllegalArgumentException(String.format("cannot modify hudi meta col: %s", colNeedToModify));
       }
     }
 

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/io/AbstractInternalSchemaStorageManager.java
Patch:
@@ -36,7 +36,7 @@ abstract class AbstractInternalSchemaStorageManager {
 
   /**
    * Get latest history schema string.
-   * Using give validCommits to validate all legal histroy Schema files, and return the latest one.
+   * Using give validCommits to validate all legal history Schema files, and return the latest one.
    * If the passed valid commits is null or empty, valid instants will be fetched from the file-system and used.
    */
   public abstract String getHistorySchemaStrByGivenValidCommits(List<String> validCommits);

File: hudi-common/src/main/java/org/apache/hudi/keygen/constant/KeyGeneratorOptions.java
Patch:
@@ -58,7 +58,7 @@ public class KeyGeneratorOptions extends HoodieConfig {
       .key("hoodie.datasource.write.partitionpath.field")
       .noDefaultValue()
       .withDocumentation("Partition path field. Value to be used at the partitionPath component of HoodieKey. "
-          + "Actual value ontained by invoking .toString()");
+          + "Actual value obtained by invoking .toString()");
 
   public static final ConfigProperty<String> KEYGENERATOR_CONSISTENT_LOGICAL_TIMESTAMP_ENABLED = ConfigProperty
       .key("hoodie.datasource.write.keygenerator.consistent.logical.timestamp.enabled")

File: hudi-common/src/main/java/org/apache/hudi/keygen/constant/KeyGeneratorType.java
Patch:
@@ -52,7 +52,7 @@ public enum KeyGeneratorType {
   CUSTOM,
 
   /**
-   * Simple Key generator for unpartitioned Hive Tables.
+   * Simple Key generator for non-partitioned Hive Tables.
    */
   NON_PARTITION,
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/MetadataPartitionType.java
Patch:
@@ -35,7 +35,7 @@ public enum MetadataPartitionType {
   private final String fileIdPrefix;
   // Total file groups
   // TODO fix: enum should not have any mutable aspect as this compromises whole idea
-  //      of the inum being static, immutable entity
+  //      of the enum being static, immutable entity
   private int fileGroupCount = 1;
 
   MetadataPartitionType(final String partitionPath, final String fileIdPrefix) {

File: hudi-common/src/main/java/org/apache/hudi/secondary/index/SecondaryIndexUtils.java
Patch:
@@ -50,7 +50,7 @@ public static Option<List<HoodieSecondaryIndex>> getSecondaryIndexes(HoodieTable
   }
 
   /**
-   * Parse secondary index str to List<HOodieSecondaryIndex>
+   * Parse secondary index str to List<HoodieSecondaryIndex>
    *
    * @param jsonStr Secondary indexes with json format
    * @return List<HoodieSecondaryIndex>

File: hudi-common/src/test/java/org/apache/hudi/common/fs/inline/TestInLineFileSystem.java
Patch:
@@ -242,7 +242,7 @@ private OuterPathInfo generateOuterFileAndGetInfo(int inlineContentSize) throws
   @Test
   public void testOpen() throws IOException {
     Path inlinePath = getRandomInlinePath();
-    // open non existant path
+    // open non-existent path
     assertThrows(FileNotFoundException.class, () -> {
       inlinePath.getFileSystem(conf).open(inlinePath);
     }, "Should have thrown exception");

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/FileSystemTestUtils.java
Patch:
@@ -59,7 +59,7 @@ public static Path getRandomOuterFSPath() {
   }
 
   public static Path getPhantomFile(Path outerPath, long startOffset, long inlineLength) {
-    // Generate phathom inline file
+    // Generate phantom inline file
     return InLineFSUtils.getInlineFilePath(outerPath, FILE_SCHEME, startOffset, inlineLength);
   }
 

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java
Patch:
@@ -62,7 +62,7 @@ protected void initPath() {
   }
 
   /**
-   * Initializes a test data generator which used to generate test datas.
+   * Initializes a test data generator which used to generate test data.
    */
   protected void initTestDataGenerator() {
     dataGen = new HoodieTestDataGenerator();

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestClusteringUtils.java
Patch:
@@ -104,7 +104,7 @@ public void testClusteringPlanMultipleInstants() throws Exception {
     validateClusteringInstant(fileIds3, partitionPath1, clusterTime, fileGroupToInstantMap);
   }
 
-  // replacecommit.inflight doesnt have clustering plan. 
+  // replacecommit.inflight doesn't have clustering plan.
   // Verify that getClusteringPlan fetches content from corresponding requested file.
   @Test
   public void testClusteringPlanInflight() throws Exception {

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -219,6 +219,7 @@ public class HoodieTableConfig extends HoodieConfig {
   public static final ConfigProperty<Boolean> DROP_PARTITION_COLUMNS = ConfigProperty
       .key("hoodie.datasource.write.drop.partition.columns")
       .defaultValue(false)
+      .markAdvanced()
       .withDocumentation("When set to true, will not write the partition columns into hudi. By default, false.");
 
   public static final ConfigProperty<String> URL_ENCODE_PARTITIONING = KeyGeneratorOptions.URL_ENCODE_PARTITIONING;

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/JDBCExecutor.java
Patch:
@@ -154,7 +154,7 @@ public void dropPartitionsToTable(String tableName, List<String> partitionsToDro
       LOG.info("No partitions to add for " + tableName);
       return;
     }
-    LOG.info("Adding partitions " + partitionsToDrop.size() + " to table " + tableName);
+    LOG.info("Dropping partitions " + partitionsToDrop.size() + " from table " + tableName);
     List<String> sqls = constructDropPartitions(tableName, partitionsToDrop);
     sqls.stream().forEach(sql -> runSQL(sql));
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java
Patch:
@@ -92,7 +92,8 @@ record -> new ImmutablePair<>(record.getPartitionPath(), record.getRecordKey()))
 
     // Cache the result, for subsequent stages.
     if (config.getBloomIndexUseCaching()) {
-      keyFilenamePairs.persist("MEMORY_AND_DISK_SER");
+      keyFilenamePairs.persist(new HoodieConfig(config.getProps())
+          .getString(HoodieIndexConfig.BLOOM_INDEX_INPUT_STORAGE_LEVEL_VALUE));
     }
     if (LOG.isDebugEnabled()) {
       long totalTaggedRecords = keyFilenamePairs.count();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java
Patch:
@@ -165,7 +165,8 @@ private List<String> getPartitionPathsForCleanByCommits(Option<HoodieInstant> in
           HoodieCleanMetadata cleanMetadata = TimelineMetadataUtils
                   .deserializeHoodieCleanMetadata(hoodieTable.getActiveTimeline().getInstantDetails(lastClean.get()).get());
           if ((cleanMetadata.getEarliestCommitToRetain() != null)
-                  && (cleanMetadata.getEarliestCommitToRetain().length() > 0)) {
+                  && (cleanMetadata.getEarliestCommitToRetain().length() > 0)
+                  && !hoodieTable.getActiveTimeline().isBeforeTimelineStarts(cleanMetadata.getEarliestCommitToRetain())) {
             return getPartitionPathsForIncrementalCleaning(cleanMetadata, instantToRetain);
           }
         }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/OptionsResolver.java
Patch:
@@ -59,8 +59,7 @@ public static boolean insertClustering(Configuration conf) {
   public static boolean isAppendMode(Configuration conf) {
     // 1. inline clustering is supported for COW table;
     // 2. async clustering is supported for both COW and MOR table
-    return isCowTable(conf) && isInsertOperation(conf) && !conf.getBoolean(FlinkOptions.INSERT_CLUSTER)
-        || needsScheduleClustering(conf);
+    return isInsertOperation(conf) && ((isCowTable(conf) && !conf.getBoolean(FlinkOptions.INSERT_CLUSTER)) || isMorTable(conf));
   }
 
   /**

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/ExpressionUtils.java
Patch:
@@ -253,12 +253,15 @@ private static void splitByAnd(
    */
   private static boolean isPartitionCallExpr(CallExpression expr, Set<Integer> parFieldPos) {
     List<Expression> children = expr.getChildren();
+    // if any child expr reference a non-partition field, returns false.
     return children.stream()
         .allMatch(
             child -> {
               if (child instanceof FieldReferenceExpression) {
                 FieldReferenceExpression refExpr = (FieldReferenceExpression) child;
                 return parFieldPos.contains(refExpr.getFieldIndex());
+              } else if (child instanceof CallExpression) {
+                return isPartitionCallExpr((CallExpression) child, parFieldPos);
               } else {
                 return true;
               }

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/ITTestHoodieDataSource.java
Patch:
@@ -1789,7 +1789,7 @@ void testDynamicPartitionPrune(HoodieTableType tableType, boolean hiveStyleParti
     // write second commit
     TestData.writeData(TestData.DATA_SET_INSERT_SEPARATE_PARTITION, conf);
     // stop the streaming query and get data
-    List<Row> actualResult = stopAndFetchData(streamTableEnv, tableResult, 10);
+    List<Row> actualResult = fetchResult(streamTableEnv, tableResult, 10);
     assertRowsEquals(actualResult, TestData.DATA_SET_INSERT_SEPARATE_PARTITION);
   }
 
@@ -1912,7 +1912,7 @@ private List<Row> execSelectSql(TableEnvironment tEnv, String select, long timeo
   private List<Row> execSelectSql(TableEnvironment tEnv, String select, String sinkDDL, long timeout)
       throws InterruptedException {
     TableResult tableResult = submitSelectSql(tEnv, select, sinkDDL);
-    return stopAndFetchData(tEnv, tableResult, timeout);
+    return fetchResult(tEnv, tableResult, timeout);
   }
 
   private TableResult submitSelectSql(TableEnvironment tEnv, String select, String sinkDDL) {
@@ -1922,7 +1922,7 @@ private TableResult submitSelectSql(TableEnvironment tEnv, String select, String
     return tableResult;
   }
 
-  private List<Row> stopAndFetchData(TableEnvironment tEnv, TableResult tableResult, long timeout)
+  private List<Row> fetchResult(TableEnvironment tEnv, TableResult tableResult, long timeout)
       throws InterruptedException {
     // wait for the timeout then cancels the job
     TimeUnit.SECONDS.sleep(timeout);

File: hudi-aws/src/main/java/org/apache/hudi/config/DynamoDbBasedLockConfig.java
Patch:
@@ -46,7 +46,7 @@ public class DynamoDbBasedLockConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> DYNAMODB_LOCK_TABLE_NAME = ConfigProperty
       .key(DYNAMODB_BASED_LOCK_PROPERTY_PREFIX + "table")
-      .noDefaultValue()
+      .defaultValue("hudi_locks")
       .sinceVersion("0.10.0")
       .withDocumentation("For DynamoDB based lock provider, the name of the DynamoDB table acting as lock table");
 
@@ -98,7 +98,7 @@ public class DynamoDbBasedLockConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> DYNAMODB_LOCK_TABLE_CREATION_TIMEOUT = ConfigProperty
       .key(DYNAMODB_BASED_LOCK_PROPERTY_PREFIX + "table_creation_timeout")
-      .defaultValue(String.valueOf(10 * 60 * 1000))
+      .defaultValue(String.valueOf(2 * 60 * 1000))
       .sinceVersion("0.10.0")
       .withDocumentation("For DynamoDB based lock provider, the maximum number of milliseconds to wait for creating DynamoDB table");
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java
Patch:
@@ -76,7 +76,7 @@ public class HoodieLockConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> LOCK_ACQUIRE_RETRY_MAX_WAIT_TIME_IN_MILLIS = ConfigProperty
       .key(LOCK_ACQUIRE_RETRY_MAX_WAIT_TIME_IN_MILLIS_PROP_KEY)
-      .defaultValue(String.valueOf(5000L))
+      .defaultValue(String.valueOf(16000L))
       .sinceVersion("0.8.0")
       .withDocumentation("Maximum amount of time to wait between retries by lock provider client. This bounds"
           + " the maximum delay from the exponential backoff. Currently used by ZK based lock provider only.");

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteCommitCallbackConfig.java
Patch:
@@ -67,9 +67,9 @@ public class HoodieWriteCommitCallbackConfig extends HoodieConfig {
 
   public static final ConfigProperty<Integer> CALLBACK_HTTP_TIMEOUT_IN_SECONDS = ConfigProperty
       .key(CALLBACK_PREFIX + "http.timeout.seconds")
-      .defaultValue(3)
+      .defaultValue(30)
       .sinceVersion("0.6.0")
-      .withDocumentation("Callback timeout in seconds. 3 by default");
+      .withDocumentation("Callback timeout in seconds.");
 
   /**
    * @deprecated Use {@link #TURN_CALLBACK_ON} and its methods instead

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -35,6 +35,7 @@
 import org.apache.hudi.exception.HoodieInsertException;
 import org.apache.hudi.io.storage.HoodieFileWriter;
 import org.apache.hudi.io.storage.HoodieFileWriterFactory;
+import org.apache.hudi.metadata.HoodieTableMetadata;
 import org.apache.hudi.table.HoodieTable;
 
 import org.apache.avro.Schema;
@@ -115,7 +116,8 @@ public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTa
   public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T, I, K, O> hoodieTable,
       String partitionPath, String fileId, Map<String, HoodieRecord<T>> recordMap,
       TaskContextSupplier taskContextSupplier) {
-    this(config, instantTime, hoodieTable, partitionPath, fileId, taskContextSupplier, config.isPreserveHoodieCommitMetadataForCompaction());
+    // preserveMetadata is disabled by default for MDT but enabled otherwise
+    this(config, instantTime, hoodieTable, partitionPath, fileId, taskContextSupplier, !HoodieTableMetadata.isMetadataTable(config.getBasePath()));
     this.recordMap = recordMap;
     this.useWriterSchema = true;
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -47,6 +47,7 @@
 import org.apache.hudi.io.storage.HoodieFileWriter;
 import org.apache.hudi.io.storage.HoodieFileWriterFactory;
 import org.apache.hudi.keygen.BaseKeyGenerator;
+import org.apache.hudi.metadata.HoodieTableMetadata;
 import org.apache.hudi.table.HoodieTable;
 
 import org.apache.avro.Schema;
@@ -140,7 +141,8 @@ public HoodieMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTab
     super(config, instantTime, partitionPath, fileId, hoodieTable, taskContextSupplier);
     this.keyToNewRecords = keyToNewRecords;
     this.useWriterSchemaForCompaction = true;
-    this.preserveMetadata = config.isPreserveHoodieCommitMetadataForCompaction();
+    // preserveMetadata is disabled by default for MDT but enabled otherwise
+    this.preserveMetadata = !HoodieTableMetadata.isMetadataTable(config.getBasePath());
     init(fileId, this.partitionPath, dataFileToBeMerged);
     validateAndSetAndKeyGenProps(keyGeneratorOpt, config.populateMetaFields());
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -291,9 +291,6 @@ private HoodieWriteConfig createMetadataWriteConfig(
         .withCompactionConfig(HoodieCompactionConfig.newBuilder()
             .withInlineCompaction(false)
             .withMaxNumDeltaCommitsBeforeCompaction(writeConfig.getMetadataCompactDeltaCommitMax())
-            // by default, the HFile does not keep the metadata fields, set up as false
-            // to always use the metadata of the new record.
-            .withPreserveCommitMetadata(false)
             .withEnableOptimizedLogBlocksScan(String.valueOf(writeConfig.enableOptimizedLogBlocksScan()))
             .build())
         .withParallelism(parallelism, parallelism)

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareClusteringPlanStrategy.java
Patch:
@@ -122,7 +122,7 @@ public Option<HoodieClusteringPlan> generateClusteringPlan() {
         .setInputGroups(clusteringGroups)
         .setExtraMetadata(getExtraMetadata())
         .setVersion(getPlanVersion())
-        .setPreserveHoodieMetadata(getWriteConfig().isPreserveHoodieCommitMetadataForClustering())
+        .setPreserveHoodieMetadata(true)
         .build());
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieIndex.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieAvroRecord;
@@ -38,7 +39,6 @@
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieLayoutConfig;
-import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.index.HoodieIndex.IndexType;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableIncrementalRead.java
Patch:
@@ -170,13 +170,13 @@ public void testIncrementalReadsWithCompaction() throws Exception {
 
       // verify new write shows up in snapshot mode after compaction is complete
       snapshotROFiles = getROSnapshotFiles(partitionPath);
-      validateFiles(partitionPath,2, snapshotROFiles, false, roSnapshotJobConf,400, commitTime1, compactionCommitTime,
+      validateFiles(partitionPath,2, snapshotROFiles, false, roSnapshotJobConf,400, commitTime1, updateTime,
           insertsTime);
 
       incrementalROFiles = getROIncrementalFiles(partitionPath, "002", -1, true);
       assertTrue(incrementalROFiles.length == 2);
       // verify 006 shows up because of pending compaction
-      validateFiles(partitionPath, 2, incrementalROFiles, false, roJobConf, 400, commitTime1, compactionCommitTime,
+      validateFiles(partitionPath, 2, incrementalROFiles, false, roJobConf, 400, commitTime1, updateTime,
           insertsTime);
     }
   }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableInsertUpdateDelete.java
Patch:
@@ -149,7 +149,7 @@ public void testInlineScheduleCompaction(boolean scheduleInlineCompaction) throw
 
     HoodieWriteConfig cfg = getConfigBuilder(false)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024 * 1024)
-            .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(2).withPreserveCommitMetadata(true).withScheduleInlineCompaction(scheduleInlineCompaction).build())
+            .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(2).withScheduleInlineCompaction(scheduleInlineCompaction).build())
         .build();
     try (SparkRDDWriteClient client = getHoodieWriteClient(cfg)) {
 
@@ -191,7 +191,7 @@ public void testRepeatedRollbackOfCompaction() throws Exception {
 
     HoodieWriteConfig cfg = getConfigBuilder(false)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024 * 1024)
-            .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(2).withPreserveCommitMetadata(true).withScheduleInlineCompaction(scheduleInlineCompaction).build())
+            .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(2).withScheduleInlineCompaction(scheduleInlineCompaction).build())
         .build();
     try (SparkRDDWriteClient client = getHoodieWriteClient(cfg)) {
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java
Patch:
@@ -181,6 +181,7 @@ public static HoodieClusteringPlan createClusteringPlan(String strategyClassName
         .setInputGroups(clusteringGroups)
         .setExtraMetadata(extraMetadata)
         .setStrategy(strategy)
+        .setPreserveHoodieMetadata(true)
         .build();
   }
 

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestBootstrap.java
Patch:
@@ -58,8 +58,8 @@
 import org.apache.hudi.keygen.NonpartitionedKeyGenerator;
 import org.apache.hudi.keygen.SimpleKeyGenerator;
 import org.apache.hudi.table.action.bootstrap.BootstrapUtils;
-import org.apache.hudi.testutils.HoodieSparkClientTestBase;
 import org.apache.hudi.testutils.HoodieMergeOnReadTestUtils;
+import org.apache.hudi.testutils.HoodieSparkClientTestBase;
 
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
@@ -305,7 +305,7 @@ private void testBootstrapCommon(boolean partitioned, boolean deltaCommit, Effec
       client.compact(compactionInstant.get());
       checkBootstrapResults(totalRecords, schema, compactionInstant.get(), checkNumRawFiles,
           numInstantsAfterBootstrap + 2, 2, updateTimestamp, updateTimestamp, !deltaCommit,
-          Arrays.asList(compactionInstant.get()), !config.isPreserveHoodieCommitMetadataForCompaction());
+          Arrays.asList(compactionInstant.get()), false);
     }
     client.close();
   }

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestOrcBootstrap.java
Patch:
@@ -297,7 +297,7 @@ private void testBootstrapCommon(boolean partitioned, boolean deltaCommit, Effec
       client.compact(compactionInstant.get());
       checkBootstrapResults(totalRecords, schema, compactionInstant.get(), checkNumRawFiles,
           numInstantsAfterBootstrap + 2, 2, updateTimestamp, updateTimestamp, !deltaCommit,
-          Arrays.asList(compactionInstant.get()), !config.isPreserveHoodieCommitMetadataForCompaction());
+          Arrays.asList(compactionInstant.get()), false);
     }
     client.close();
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/QueryInfo.java
Patch:
@@ -51,7 +51,7 @@ public QueryInfo(String queryType, String startInstant, String endInstant) {
     this.endInstant = endInstant;
   }
 
-  public Dataset<Row> initializeSourceForFilenames(String srcPath, SparkSession sparkSession) {
+  public Dataset<Row> initCloudObjectMetadata(String srcPath, SparkSession sparkSession) {
     if (isIncremental()) {
       return incrementalQuery(sparkSession).load(srcPath);
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java
Patch:
@@ -183,7 +183,7 @@ public class HoodieClusteringConfig extends HoodieConfig {
       .withDocumentation("Each group can produce 'N' (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups");
 
   public static final ConfigProperty<Boolean> PLAN_STRATEGY_SINGLE_GROUP_CLUSTERING_ENABLED = ConfigProperty
-      .key(CLUSTERING_STRATEGY_PARAM_PREFIX + ".single.group.clustering.enabled")
+      .key(CLUSTERING_STRATEGY_PARAM_PREFIX + "single.group.clustering.enabled")
       .defaultValue(true)
       .sinceVersion("0.14.0")
       .withDocumentation("Whether to generate clustering plan when there is only one file group involved, by default true");

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieMetadataBase.java
Patch:
@@ -47,7 +47,6 @@
 import org.apache.hudi.config.metrics.HoodieMetricsJmxConfig;
 import org.apache.hudi.exception.HoodieMetadataException;
 import org.apache.hudi.index.HoodieIndex;
-import org.apache.hudi.keygen.SimpleKeyGenerator;
 import org.apache.hudi.metadata.HoodieMetadataPayload;
 import org.apache.hudi.metadata.HoodieTableMetadata;
 import org.apache.hudi.metadata.HoodieTableMetadataKeyGenerator;
@@ -338,7 +337,6 @@ protected HoodieWriteConfig.Builder getWriteConfigBuilder(HoodieFailedWritesClea
                                                             boolean enableMetrics, boolean useRollbackUsingMarkers,
                                                             boolean validateMetadataPayloadConsistency) {
     Properties properties = new Properties();
-    properties.put(HoodieTableConfig.KEY_GENERATOR_CLASS_NAME.key(), SimpleKeyGenerator.class.getName());
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(TRIP_EXAMPLE_SCHEMA)
         .withParallelism(2, 2).withDeleteParallelism(2).withRollbackParallelism(2).withFinalizeWriteParallelism(2)
         .withAutoCommit(autoCommit)

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestHarness.java
Patch:
@@ -55,7 +55,6 @@
 import org.apache.hudi.data.HoodieJavaRDD;
 import org.apache.hudi.exception.HoodieMetadataException;
 import org.apache.hudi.index.HoodieIndex;
-import org.apache.hudi.keygen.SimpleKeyGenerator;
 import org.apache.hudi.metadata.FileSystemBackedTableMetadata;
 import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;
 import org.apache.hudi.metadata.HoodieTableMetadata;
@@ -346,7 +345,6 @@ protected Properties getPropertiesForKeyGen(boolean populateMetaFields) {
     properties.put("hoodie.datasource.write.partitionpath.field", "partition_path");
     properties.put(HoodieTableConfig.RECORDKEY_FIELDS.key(), "_row_key");
     properties.put(HoodieTableConfig.PARTITION_FIELDS.key(), "partition_path");
-    properties.put(HoodieTableConfig.KEY_GENERATOR_CLASS_NAME.key(), SimpleKeyGenerator.class.getName());
     return properties;
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/SparkClientFunctionalTestHarness.java
Patch:
@@ -47,7 +47,6 @@
 import org.apache.hudi.data.HoodieJavaRDD;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.index.HoodieIndex;
-import org.apache.hudi.keygen.SimpleKeyGenerator;
 import org.apache.hudi.table.HoodieSparkTable;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.testutils.providers.HoodieMetaClientProvider;
@@ -334,7 +333,6 @@ protected Properties getPropertiesForKeyGen(boolean populateMetaFields) {
     properties.put("hoodie.datasource.write.partitionpath.field", "partition_path");
     properties.put(HoodieTableConfig.RECORDKEY_FIELDS.key(), "_row_key");
     properties.put(HoodieTableConfig.PARTITION_FIELDS.key(), "partition_path");
-    properties.put(HoodieTableConfig.KEY_GENERATOR_CLASS_NAME.key(), SimpleKeyGenerator.class.getName());
     return properties;
   }
 

File: hudi-spark-datasource/hudi-spark-common/src/test/java/org/apache/hudi/internal/HoodieBulkInsertInternalWriterTestBase.java
Patch:
@@ -27,9 +27,9 @@
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
-import org.apache.hudi.keygen.SimpleKeyGenerator;
 import org.apache.hudi.testutils.HoodieClientTestHarness;
 import org.apache.hudi.testutils.SparkDatasetTestUtils;
+
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
 import org.junit.jupiter.api.AfterEach;
@@ -77,7 +77,6 @@ protected HoodieWriteConfig getWriteConfig(boolean populateMetaFields) {
   protected HoodieWriteConfig getWriteConfig(boolean populateMetaFields, String hiveStylePartitioningValue) {
     Properties properties = new Properties();
     if (!populateMetaFields) {
-      properties.setProperty(DataSourceWriteOptions.KEYGENERATOR_CLASS_NAME().key(), SimpleKeyGenerator.class.getName());
       properties.setProperty(DataSourceWriteOptions.RECORDKEY_FIELD().key(), SparkDatasetTestUtils.RECORD_KEY_FIELD_NAME);
       properties.setProperty(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), SparkDatasetTestUtils.PARTITION_PATH_FIELD_NAME);
       properties.setProperty(HoodieTableConfig.POPULATE_META_FIELDS.key(), "false");

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -71,7 +71,6 @@
 import org.apache.hudi.hive.HiveSyncTool;
 import org.apache.hudi.internal.schema.InternalSchema;
 import org.apache.hudi.keygen.KeyGenerator;
-import org.apache.hudi.keygen.SimpleKeyGenerator;
 import org.apache.hudi.keygen.SparkKeyGeneratorInterface;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 import org.apache.hudi.keygen.factory.HoodieSparkKeyGeneratorFactory;
@@ -376,7 +375,7 @@ private void initializeEmptyTable() throws IOException {
         .setPopulateMetaFields(props.getBoolean(HoodieTableConfig.POPULATE_META_FIELDS.key(),
             HoodieTableConfig.POPULATE_META_FIELDS.defaultValue()))
         .setKeyGeneratorClassProp(props.getProperty(DataSourceWriteOptions.KEYGENERATOR_CLASS_NAME().key(),
-            SimpleKeyGenerator.class.getName()))
+            keyGenerator.getClass().getName()))
         .setPreCombineField(cfg.sourceOrderingField)
         .setPartitionMetafileUseBaseFormat(props.getBoolean(HoodieTableConfig.PARTITION_METAFILE_USE_BASE_FORMAT.key(),
             HoodieTableConfig.PARTITION_METAFILE_USE_BASE_FORMAT.defaultValue()))
@@ -473,7 +472,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource(
           .setPopulateMetaFields(props.getBoolean(HoodieTableConfig.POPULATE_META_FIELDS.key(),
               HoodieTableConfig.POPULATE_META_FIELDS.defaultValue()))
           .setKeyGeneratorClassProp(props.getProperty(DataSourceWriteOptions.KEYGENERATOR_CLASS_NAME().key(),
-              SimpleKeyGenerator.class.getName()))
+              keyGenerator.getClass().getName()))
           .setPartitionMetafileUseBaseFormat(props.getBoolean(HoodieTableConfig.PARTITION_METAFILE_USE_BASE_FORMAT.key(),
               HoodieTableConfig.PARTITION_METAFILE_USE_BASE_FORMAT.defaultValue()))
           .setShouldDropPartitionColumns(isDropPartitionColumns())

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/GcsEventsHoodieIncrSource.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.utilities.sources.helpers.gcs.FileDataFetcher;
 import org.apache.hudi.utilities.sources.helpers.gcs.FilePathsFetcher;
 import org.apache.hudi.utilities.sources.helpers.gcs.QueryInfo;
+
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 import org.apache.spark.api.java.JavaSparkContext;
@@ -80,7 +81,6 @@
   --hoodie-conf hoodie.deltastreamer.source.cloud.data.ignore.relpath.substring="blah" \
   --hoodie-conf hoodie.datasource.write.recordkey.field=id \
   --hoodie-conf hoodie.datasource.write.partitionpath.field= \
-  --hoodie-conf hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.ComplexKeyGenerator \
   --filter-dupes \
   --hoodie-conf hoodie.datasource.write.insert.drop.duplicates=true \
   --hoodie-conf hoodie.combine.before.insert=true \

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -41,6 +41,7 @@
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 import org.apache.hudi.keygen.constant.KeyGeneratorType;
 import org.apache.hudi.table.action.cluster.ClusteringPlanPartitionFilterMode;
+import org.apache.hudi.util.ClientIds;
 
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;
@@ -572,7 +573,7 @@ private FlinkOptions() {
   public static final ConfigOption<String> WRITE_CLIENT_ID = ConfigOptions
       .key("write.client.id")
       .stringType()
-      .defaultValue("")
+      .defaultValue(ClientIds.INIT_CLIENT_ID)
       .withDescription("Unique identifier used to distinguish different writer pipelines for concurrent mode");
 
   // ------------------------------------------------------------------------

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/streamer/HoodieFlinkStreamer.java
Patch:
@@ -99,6 +99,7 @@ public static void main(String[] args) throws Exception {
     }
 
     OptionsInference.setupSinkTasks(conf, env.getParallelism());
+    OptionsInference.setupClientId(conf);
     DataStream<Object> pipeline;
     // Append mode
     if (OptionsResolver.isAppendMode(conf)) {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSink.java
Patch:
@@ -69,6 +69,8 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
       conf.setLong(FlinkOptions.WRITE_COMMIT_ACK_TIMEOUT, ckpTimeout);
       // set up default parallelism
       OptionsInference.setupSinkTasks(conf, dataStream.getExecutionConfig().getParallelism());
+      // set up client id
+      OptionsInference.setupClientId(conf);
 
       RowType rowType = (RowType) schema.toSinkRowDataType().notNull().getLogicalType();
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/ViewStorageProperties.java
Patch:
@@ -47,8 +47,6 @@ public class ViewStorageProperties {
 
   private static final String FILE_NAME = "view_storage_conf";
 
-  private static final String FILE_SUFFIX = ".properties";
-
   /**
    * Initialize the {@link #FILE_NAME} meta file.
    */

File: hudi-common/src/main/java/org/apache/hudi/common/util/SerializationUtils.java
Patch:
@@ -22,6 +22,8 @@
 import com.esotericsoftware.kryo.Serializer;
 import com.esotericsoftware.kryo.io.Input;
 import com.esotericsoftware.kryo.io.Output;
+import org.apache.avro.generic.GenericData;
+import org.apache.hudi.avro.GenericAvroSerializer;
 import org.apache.avro.util.Utf8;
 import org.objenesis.strategy.StdInstantiatorStrategy;
 
@@ -122,6 +124,7 @@ public Kryo newKryo() {
 
       // Register serializers
       kryo.register(Utf8.class, new AvroUtf8Serializer());
+      kryo.register(GenericData.Fixed.class, new GenericAvroSerializer<>());
 
       return kryo;
     }

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CommitsCommand.java
Patch:
@@ -210,7 +210,7 @@ public String showCommitPartitions(
       @ShellOption(value = {"--desc"}, help = "Ordering", defaultValue = "false") final boolean descending,
       @ShellOption(value = {"--headeronly"}, help = "Print Header Only",
           defaultValue = "false") final boolean headerOnly,
-      @ShellOption(value = {"includeArchivedTimeline"}, help = "Include archived commits as well",
+      @ShellOption(value = {"--includeArchivedTimeline"}, help = "Include archived commits as well",
               defaultValue = "false") final boolean includeArchivedTimeline)
       throws Exception {
 
@@ -278,7 +278,7 @@ public String showWriteStats(
       @ShellOption(value = {"--desc"}, help = "Ordering", defaultValue = "false") final boolean descending,
       @ShellOption(value = {"--headeronly"}, help = "Print Header Only",
           defaultValue = "false") final boolean headerOnly,
-      @ShellOption(value = {"includeArchivedTimeline"}, help = "Include archived commits as well",
+      @ShellOption(value = {"--includeArchivedTimeline"}, help = "Include archived commits as well",
               defaultValue = "false") final boolean includeArchivedTimeline)
       throws Exception {
 
@@ -324,7 +324,7 @@ public String showCommitFiles(
       @ShellOption(value = {"--desc"}, help = "Ordering", defaultValue = "false") final boolean descending,
       @ShellOption(value = {"--headeronly"}, help = "Print Header Only",
           defaultValue = "false") final boolean headerOnly,
-      @ShellOption(value = {"includeArchivedTimeline"}, help = "Include archived commits as well",
+      @ShellOption(value = {"--includeArchivedTimeline"}, help = "Include archived commits as well",
               defaultValue = "false") final boolean includeArchivedTimeline)
       throws Exception {
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -827,9 +827,9 @@ public void testUpsertsCOW_ContinuousModeDisabled(HoodieRecordType recordType) t
   }
 
   @ParameterizedTest
-  @EnumSource(value = HoodieRecordType.class, names = {"AVRO", "SPARK"})
-  public void testUpsertsCOWContinuousModeShutdownGracefully(HoodieRecordType recordType) throws Exception {
-    testUpsertsContinuousMode(HoodieTableType.COPY_ON_WRITE, "continuous_cow", true, recordType);
+  @EnumSource(value = HoodieRecordType.class, names = {"AVRO"})
+  public void testUpsertsMORContinuousModeShutdownGracefully(HoodieRecordType recordType) throws Exception {
+    testUpsertsContinuousMode(HoodieTableType.MERGE_ON_READ, "continuous_cow", true, recordType);
   }
 
   @Disabled("HUDI-5815 for investigation")

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/callback/kafka/HoodieWriteCommitKafkaCallback.java
Patch:
@@ -85,10 +85,10 @@ public KafkaProducer<String, String> createProducer(HoodieConfig hoodieConfig) {
     kafkaProducerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
     // default "all" to ensure no message loss
     kafkaProducerProps.setProperty(ProducerConfig.ACKS_CONFIG, hoodieConfig
-        .getString(ACKS));
+        .getStringOrDefault(ACKS));
     // retries 3 times by default
     kafkaProducerProps.setProperty(ProducerConfig.RETRIES_CONFIG, hoodieConfig
-        .getString(RETRIES));
+        .getStringOrDefault(RETRIES));
     kafkaProducerProps.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
         "org.apache.kafka.common.serialization.StringSerializer");
     kafkaProducerProps.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/AvroSchemaConverter.java
Patch:
@@ -301,7 +301,7 @@ public static Schema convertToSchema(LogicalType logicalType, String rowName) {
             LogicalTypes.decimal(decimalType.getPrecision(), decimalType.getScale())
                 .addToSchema(SchemaBuilder
                     .fixed(String.format("%s.fixed", rowName))
-                    .size(computeMinBytesForDecimlPrecision(decimalType.getPrecision())));
+                    .size(computeMinBytesForDecimalPrecision(decimalType.getPrecision())));
         return nullable ? nullableSchema(decimal) : decimal;
       case ROW:
         RowType rowType = (RowType) logicalType;
@@ -377,7 +377,7 @@ private static Schema nullableSchema(Schema schema) {
         : Schema.createUnion(SchemaBuilder.builder().nullType(), schema);
   }
 
-  private static int computeMinBytesForDecimlPrecision(int precision) {
+  private static int computeMinBytesForDecimalPrecision(int precision) {
     int numBytes = 1;
     while (Math.pow(2.0, 8 * numBytes - 1) < Math.pow(10.0, precision)) {
       numBytes += 1;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/OrcBootstrapMetadataHandler.java
Patch:
@@ -72,7 +72,7 @@ void executeBootstrap(HoodieBootstrapHandle<?, ?, ?, ?> bootstrapHandle, Path so
     }
     HoodieExecutor<Void> wrapper = null;
     Reader orcReader = OrcFile.createReader(sourceFilePath, OrcFile.readerOptions(table.getHadoopConf()));
-    TypeDescription orcSchema = orcReader.getSchema();
+    TypeDescription orcSchema = AvroOrcUtils.createOrcSchema(avroSchema);
     try (RecordReader reader = orcReader.rows(new Reader.Options(table.getHadoopConf()).schema(orcSchema))) {
       wrapper = ExecutorFactory.create(config, new OrcReaderIterator<GenericRecord>(reader, avroSchema, orcSchema),
           new BootstrapRecordConsumer(bootstrapHandle), inp -> {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/ListBasedIndexFileFilter.java
Patch:
@@ -58,7 +58,7 @@ public Set<Pair<String, String>> getMatchingFilesAndPartition(String partitionPa
   }
 
   /**
-   * if we dont have key ranges, then also we need to compare against the file. no other choice if we do, then only
+   * if we don't have key ranges, then also we need to compare against the file. no other choice if we do, then only
    * compare the file if the record key falls in range.
    */
   protected boolean shouldCompareWithFile(BloomIndexFileInfo indexInfo, String recordKey) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/NonpartitionedAvroKeyGenerator.java
Patch:
@@ -17,17 +17,18 @@
 
 package org.apache.hudi.keygen;
 
-import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 
+import org.apache.avro.generic.GenericRecord;
+
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
 import java.util.stream.Collectors;
 
 /**
- * Avro simple Key generator for unpartitioned Hive Tables.
+ * Avro simple Key generator for non-partitioned Hive Tables.
  */
 public class NonpartitionedAvroKeyGenerator extends BaseKeyGenerator {
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -162,7 +162,7 @@ protected <T extends SpecificRecordBase> HoodieBackedTableMetadataWriter(Configu
       this.metadataWriteConfig = createMetadataWriteConfig(writeConfig, failedWritesCleaningPolicy);
       enabled = true;
 
-      // Inline compaction and auto clean is required as we dont expose this table outside
+      // Inline compaction and auto clean is required as we do not expose this table outside
       ValidationUtils.checkArgument(!this.metadataWriteConfig.isAutoClean(),
           "Cleaning is controlled internally for Metadata table.");
       ValidationUtils.checkArgument(!this.metadataWriteConfig.inlineCompactionEnabled(),
@@ -1067,7 +1067,7 @@ protected void compactIfNecessary(BaseHoodieWriteClient writeClient, String inst
     // metadata table.
     final String compactionInstantTime = latestDeltaCommitTimeInMetadataTable + METADATA_COMPACTION_TIME_SUFFIX;
     // we need to avoid checking compaction w/ same instant again.
-    // lets say we trigger compaction after C5 in MDT and so compaction completes with C4001. but C5 crashed before completing in MDT.
+    // let's say we trigger compaction after C5 in MDT and so compaction completes with C4001. but C5 crashed before completing in MDT.
     // and again w/ C6, we will re-attempt compaction at which point latest delta commit is C4 in MDT.
     // and so we try compaction w/ instant C4001. So, we can avoid compaction if we already have compaction w/ same instant time.
     if (!metadataMetaClient.getActiveTimeline().filterCompletedInstants().containsInstant(compactionInstantTime)
@@ -1112,7 +1112,7 @@ private void initialCommit(String createInstantTime, List<MetadataPartitionType>
 
     Map<MetadataPartitionType, HoodieData<HoodieRecord>> partitionToRecordsMap = new HashMap<>();
 
-    // skip file system listing to populate metadata records if its a fresh table.
+    // skip file system listing to populate metadata records if it's a fresh table.
     // this is applicable only if the table already has N commits and metadata is enabled at a later point in time.
     if (createInstantTime.equals(SOLO_COMMIT_TIMESTAMP)) { // SOLO_COMMIT_TIMESTAMP will be the initial commit time in MDT for a fresh table.
       // If not, last completed commit in data table will be chosen as the initial commit time.

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/BulkInsertPartitioner.java
Patch:
@@ -53,7 +53,7 @@ public interface BulkInsertPartitioner<I> extends Serializable {
 
   /**
    * Return file group id prefix for the given data partition.
-   * By defauult, return a new file group id prefix, so that incoming records will route to a fresh new file group
+   * By default, return a new file group id prefix, so that incoming records will route to a fresh new file group
    *
    * @param partitionId data partition
    * @return

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java
Patch:
@@ -226,7 +226,7 @@ private List<String> getPartitionPathsForFullCleaning() {
   /**
    * Selects the older versions of files for cleaning, such that it bounds the number of versions of each file. This
    * policy is useful, if you are simply interested in querying the table, and you don't want too many versions for a
-   * single file (i.e run it with versionsRetained = 1)
+   * single file (i.e., run it with versionsRetained = 1)
    */
   private Pair<Boolean, List<CleanFileInfo>> getFilesToCleanKeepingLatestVersions(String partitionPath) {
     LOG.info("Cleaning " + partitionPath + ", retaining latest " + config.getCleanerFileVersionsRetained()
@@ -330,7 +330,7 @@ private Pair<Boolean, List<CleanFileInfo>> getFilesToCleanKeepingLatestCommits(S
             getLatestVersionBeforeCommit(fileSliceList, earliestCommitToRetain);
 
         // Ensure there are more than 1 version of the file (we only clean old files from updates)
-        // i.e always spare the last commit.
+        // i.e., always spare the last commit.
         for (FileSlice aSlice : fileSliceList) {
           Option<HoodieBaseFile> aFile = aSlice.getBaseFile();
           String fileCommitTime = aSlice.getBaseInstantTime();
@@ -340,7 +340,7 @@ private Pair<Boolean, List<CleanFileInfo>> getFilesToCleanKeepingLatestCommits(S
           }
 
           if (policy == HoodieCleaningPolicy.KEEP_LATEST_COMMITS) {
-            // Dont delete the latest commit and also the last commit before the earliest commit we
+            // Do not delete the latest commit and also the last commit before the earliest commit we
             // are retaining
             // The window of commit retain == max query run time. So a query could be running which
             // still

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/BaseRestoreActionExecutor.java
Patch:
@@ -103,7 +103,7 @@ private List<HoodieInstant> getInstantsToRollback(HoodieInstant restoreInstant)
     List<HoodieInstant> instantsToRollback = new ArrayList<>();
     HoodieRestorePlan restorePlan = RestoreUtils.getRestorePlan(table.getMetaClient(), restoreInstant);
     for (HoodieInstantInfo instantInfo : restorePlan.getInstantsToRollback()) {
-      // If restore crashed mid-way, there are chances that some commits are already rolled back,
+      // If restore crashed midway, there are chances that some commits are already rolled back,
       // but some are not. so, we can ignore those commits which are fully rolledback in previous attempt if any.
       Option<HoodieInstant> rollbackInstantOpt = table.getActiveTimeline().getWriteTimeline()
           .filter(instant -> instant.getTimestamp().equals(instantInfo.getCommitTime()) && instant.getAction().equals(instantInfo.getAction())).firstInstant();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackPlanActionExecutor.java
Patch:
@@ -94,7 +94,7 @@ private BaseRollbackPlanActionExecutor.RollbackStrategy getRollbackStrategy() {
   }
 
   /**
-   * Creates a Rollback plan if there are files to be rolledback and stores them in instant file.
+   * Creates a Rollback plan if there are files to be rolled back and stores them in instant file.
    * Rollback Plan contains absolute file paths.
    *
    * @param startRollbackTime Rollback Instant Time

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/SerializableHoodieRollbackRequest.java
Patch:
@@ -27,7 +27,7 @@
 import java.util.Map;
 
 /**
- * HoodieRollbackRequest in HoodieRollbackPlan (avro pojo) is not operable direclty within spark parallel engine.
+ * HoodieRollbackRequest in HoodieRollbackPlan (avro pojo) is not operable directly within spark parallel engine.
  * Hence converting the same to this {@link SerializableHoodieRollbackRequest} and then using it within spark.parallelize.
  */
 public class SerializableHoodieRollbackRequest implements Serializable {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/DowngradeHandler.java
Patch:
@@ -34,7 +34,7 @@ public interface DowngradeHandler {
    *
    * @param config                 instance of {@link HoodieWriteConfig} to be used.
    * @param context                instance of {@link HoodieEngineContext} to be used.
-   * @param instantTime            current instant time that should not touched.
+   * @param instantTime            current instant time that should not be touched.
    * @param upgradeDowngradeHelper instance of {@link SupportsUpgradeDowngrade} to be used.
    * @return Map of config properties and its values to be added to table properties.
    */

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/keygen/factory/TestHoodieAvroKeyGeneratorFactory.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.keygen.SimpleAvroKeyGenerator;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 import org.apache.hudi.keygen.constant.KeyGeneratorType;
+
 import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.Test;
 
@@ -51,7 +52,7 @@ public void testKeyGeneratorFactory() throws IOException {
     // set both class name and keyGenerator type
     props.put(HoodieWriteConfig.KEYGENERATOR_TYPE.key(), KeyGeneratorType.CUSTOM.name());
     KeyGenerator keyGenerator3 = HoodieAvroKeyGeneratorFactory.createKeyGenerator(props);
-    // KEYGENERATOR_TYPE_PROP was overitten by KEYGENERATOR_CLASS_PROP
+    // KEYGENERATOR_TYPE_PROP was overwritten by KEYGENERATOR_CLASS_PROP
     Assertions.assertEquals(SimpleAvroKeyGenerator.class.getName(), keyGenerator3.getClass().getName());
 
     // set wrong class name

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataCreateHandle.java
Patch:
@@ -47,7 +47,7 @@
 import java.util.concurrent.atomic.AtomicLong;
 
 /**
- * Create handle with RowData for datasource implemention of bulk insert.
+ * Create handle with RowData for datasource implementation of bulk insert.
  */
 public class HoodieRowDataCreateHandle implements Serializable {
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/HoodieSparkFileReader.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.spark.sql.catalyst.InternalRow;
 
 /**
- * Marker interface for every {@link HoodieFileReader} reading in Catalyst (Spark native tyeps, ie
+ * Marker interface for every {@link HoodieFileReader} reading in Catalyst (Spark native types, i.e.,
  * producing {@link InternalRow}s)
  */
 public interface HoodieSparkFileReader extends HoodieFileReader<InternalRow> {}

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestClientRollback.java
Patch:
@@ -204,7 +204,7 @@ public void testSavepointAndRollback(Boolean testFailedRestore, Boolean failedRe
           //restore again
           newClient.restoreToSavepoint(savepoint.getTimestamp());
 
-          //verify that we resuse the existing restore commit
+          //verify that we reuse the existing restore commit
           metaClient = HoodieTableMetaClient.reload(metaClient);
           table = HoodieSparkTable.create(getConfig(), context, metaClient);
           List<HoodieInstant> restoreInstants = table.getActiveTimeline().getRestoreTimeline().getInstants();
@@ -469,7 +469,7 @@ private static Stream<Arguments> testFailedRollbackCommitParams() {
   }
 
   /**
-   * Test Cases for effects of rollbacking completed/inflight commits.
+   * Test Cases for effects of rolling back completed/inflight commits.
    */
   @ParameterizedTest
   @MethodSource("testFailedRollbackCommitParams")

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientMultiWriter.java
Patch:
@@ -208,7 +208,7 @@ public void testHoodieClientBasicMultiWriterWithEarlyConflictDetection(String ta
     final JavaRDD<WriteStatus> writeStatusList2 = startCommitForUpdate(writeConfig, client2, nextCommitTime2, 100);
 
     // start to write commit 003
-    // this commit 003 will failed quickly because early conflict detection before create marker.
+    // this commit 003 will fail quickly because early conflict detection before create marker.
     final String nextCommitTime3 = "003";
     assertThrows(SparkException.class, () -> {
       final JavaRDD<WriteStatus> writeStatusList3 = startCommitForUpdate(writeConfig, client3, nextCommitTime3, 100);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieMetadataBootstrap.java
Patch:
@@ -106,7 +106,7 @@ public void testMetadataBootstrapWithExtraFiles() throws Exception {
 
     // validate
     validateMetadata(testTable);
-    // after bootstrap do two writes and validate its still functional.
+    // after bootstrap do two writes and validate it's still functional.
     doWriteInsertAndUpsert(testTable);
     validateMetadata(testTable);
   }
@@ -178,7 +178,7 @@ public void testMetadataBootstrapInflightCommit() throws Exception {
     // once the commit is complete, metadata should get fully synced.
     // in prod code path, SparkHoodieBackedTableMetadataWriter.create() will be called for every commit,
     // which may not be the case here if we directly call HoodieBackedTableMetadataWriter.update()
-    // hence lets first move the commit to complete and invoke sync directly
+    // hence let's first move the commit to complete and invoke sync directly
     ((HoodieMetadataTestTable) testTable).moveInflightCommitToComplete("00000007", inflightCommitMeta, true);
     syncTableMetadata(writeConfig);
     validateMetadata(testTable);
@@ -252,7 +252,7 @@ private void bootstrapAndVerify() throws Exception {
     initWriteConfigAndMetatableWriter(writeConfig, true);
     syncTableMetadata(writeConfig);
     validateMetadata(testTable);
-    // after bootstrap do two writes and validate its still functional.
+    // after bootstrap do two writes and validate it's still functional.
     doWriteInsertAndUpsert(testTable);
     validateMetadata(testTable);
   }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestSimpleExecutionInSpark.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.execution;
 
-import org.apache.avro.generic.IndexedRecord;
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
@@ -28,6 +27,8 @@
 import org.apache.hudi.common.util.queue.SimpleExecutor;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.testutils.HoodieClientTestHarness;
+
+import org.apache.avro.generic.IndexedRecord;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
@@ -166,7 +167,7 @@ public Integer finish() {
   }
 
   /**
-   * Test to ensure exception happend in iterator then we need to stop the simple ingestion.
+   * Test to ensure exception happen in iterator then we need to stop the simple ingestion.
    */
   @SuppressWarnings("unchecked")
   @Test

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java
Patch:
@@ -140,7 +140,7 @@ public void testSuccessfulCompactionBasedOnNumAfterCompactionRequest() throws Ex
         //should be here
       }
 
-      // step 3: compelete another 4 delta commit should be 2 compaction request after this
+      // step 3: complete another 4 delta commit should be 2 compaction request after this
       instants = IntStream.range(0, 4).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());
       records = dataGen.generateInsertsForPartition(instants.get(0), 100, "2022/03/15");
       for (String instant : instants) {
@@ -295,7 +295,7 @@ public void testCompactionRetryOnFailureBasedOnTime() throws Exception {
     }
 
     // When: commit happens after 1000s. assumption is that, there won't be any new compaction getting scheduled within 100s, but the previous failed one will be
-    // rolledback and retried to move it to completion.
+    // rolled back and retried to move it to completion.
     HoodieWriteConfig inlineCfg = getConfigForInlineCompaction(5, 1000, CompactionTriggerStrategy.TIME_ELAPSED);
     String instantTime2;
     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(inlineCfg)) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/strategy/TestHoodieCompactionStrategy.java
Patch:
@@ -262,7 +262,7 @@ public void testLogFileLengthBasedCompactionStrategy() {
         "LogFileLengthBasedCompactionStrategy should have resulted in fewer compactions");
     assertEquals(2, returned.size(), "LogFileLengthBasedCompactionStrategy should have resulted in 2 compaction");
 
-    // Delte log File length
+    // Delta log File length
     Integer allFileLength = returned.stream().map(s -> s.getDeltaFilePaths().size())
         .reduce(Integer::sum).orElse(0);
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableRollback.java
Patch:
@@ -21,6 +21,7 @@
 
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
@@ -46,7 +47,6 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieCleanConfig;
 import org.apache.hudi.config.HoodieCompactionConfig;
-import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.index.HoodieIndex;
@@ -556,7 +556,7 @@ void testRestoreWithCleanedUpCommits() throws Exception {
 
       upsertRecords(client, "010", records, dataGen);
 
-      // trigger clean. creating a new client with aggresive cleaner configs so that clean will kick in immediately.
+      // trigger clean. creating a new client with aggressive cleaner configs so that clean will kick in immediately.
       cfgBuilder = getConfigBuilder(false)
           .withCleanConfig(HoodieCleanConfig.newBuilder().retainCommits(1).build())
           // Timeline-server-based markers are not used for multi-rollback tests

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -87,7 +87,7 @@ public class HoodieFlinkWriteClient<T> extends
   public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig) {
     super(context, writeConfig, FlinkUpgradeDowngradeHelper.getInstance());
     this.bucketToHandles = new HashMap<>();
-    this.tableServiceClient = new HoodieFlinkTableServiceClient<>(context, writeConfig);
+    this.tableServiceClient = new HoodieFlinkTableServiceClient<>(context, writeConfig, getTimelineServer());
   }
 
   /**

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java
Patch:
@@ -53,15 +53,15 @@ public class HoodieJavaWriteClient<T> extends
 
   public HoodieJavaWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig) {
     super(context, writeConfig, JavaUpgradeDowngradeHelper.getInstance());
-    this.tableServiceClient = new HoodieJavaTableServiceClient(context, writeConfig);
+    this.tableServiceClient = new HoodieJavaTableServiceClient(context, writeConfig, getTimelineServer());
   }
 
   public HoodieJavaWriteClient(HoodieEngineContext context,
                                HoodieWriteConfig writeConfig,
                                boolean rollbackPending,
                                Option<EmbeddedTimelineService> timelineService) {
     super(context, writeConfig, timelineService, JavaUpgradeDowngradeHelper.getInstance());
-    this.tableServiceClient = new HoodieJavaTableServiceClient(context, writeConfig);
+    this.tableServiceClient = new HoodieJavaTableServiceClient(context, writeConfig, getTimelineServer());
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java
Patch:
@@ -83,7 +83,7 @@ public SparkRDDWriteClient(HoodieEngineContext context, HoodieWriteConfig writeC
   public SparkRDDWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig,
                              Option<EmbeddedTimelineService> timelineService) {
     super(context, writeConfig, timelineService, SparkUpgradeDowngradeHelper.getInstance());
-    this.tableServiceClient = new SparkRDDTableServiceClient<>(context, writeConfig);
+    this.tableServiceClient = new SparkRDDTableServiceClient<>(context, writeConfig, getTimelineServer());
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/ParquetBootstrapMetadataHandler.java
Patch:
@@ -18,17 +18,15 @@
 
 package org.apache.hudi.table.action.bootstrap;
 
-import org.apache.avro.generic.GenericData;
-import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.avro.model.HoodieFileStatus;
 import org.apache.hudi.client.bootstrap.BootstrapRecordPayload;
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordMerger;
 import org.apache.hudi.common.model.HoodieSparkRecord;
-import org.apache.hudi.common.util.ClosableIterator;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.collection.ClosableIterator;
 import org.apache.hudi.common.util.queue.HoodieExecutor;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
@@ -40,6 +38,8 @@
 import org.apache.hudi.util.ExecutorFactory;
 
 import org.apache.avro.Schema;
+import org.apache.avro.generic.GenericData;
+import org.apache.avro.generic.GenericRecord;
 import org.apache.hadoop.fs.Path;
 import org.apache.parquet.avro.AvroSchemaConverter;
 import org.apache.parquet.format.converter.ParquetMetadataConverter;

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDBDAO.java
Patch:
@@ -44,6 +44,7 @@
 import java.io.File;
 import java.io.IOException;
 import java.io.Serializable;
+import java.net.URI;
 import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.LinkedList;
@@ -69,7 +70,7 @@ public class RocksDBDAO {
 
   public RocksDBDAO(String basePath, String rocksDBBasePath) {
     this.rocksDBBasePath =
-        String.format("%s/%s/%s", rocksDBBasePath, basePath.replace("/", "_"), UUID.randomUUID().toString());
+        String.format("%s/%s/%s", rocksDBBasePath, URI.create(basePath).getPath().replace(":","").replace("/", "_"), UUID.randomUUID().toString());
     init();
     totalBytesWritten = 0L;
   }

File: hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.table.format.cow.vector.ParquetDecimalVector;
 
 import org.apache.flink.formats.parquet.vector.reader.ColumnReader;
+import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.columnar.ColumnarRowData;
 import org.apache.flink.table.data.columnar.vector.ColumnVector;
 import org.apache.flink.table.data.columnar.vector.VectorizedColumnBatch;
@@ -266,7 +267,7 @@ public boolean reachedEnd() throws IOException {
     return !ensureBatch();
   }
 
-  public ColumnarRowData nextRecord() {
+  public RowData nextRecord() {
     // return the next row
     row.setRowId(this.nextRow++);
     return row;

File: hudi-common/src/main/java/org/apache/hudi/BaseHoodieTableFileIndex.java
Patch:
@@ -51,7 +51,6 @@
 import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
@@ -343,7 +342,7 @@ private FileStatus[] listPartitionPathFiles(List<PartitionPath> partitions) {
             .collect(Collectors.toMap(Pair::getKey, p -> p.getRight().get()));
 
     Set<Path> missingPartitionPaths =
-        CollectionUtils.diffSet(new HashSet<>(partitionPaths), cachedPartitionPaths.keySet());
+        CollectionUtils.diffSet(partitionPaths, cachedPartitionPaths.keySet());
 
     // NOTE: We're constructing a mapping of absolute form of the partition-path into
     //       its relative one, such that we don't need to reconstruct these again later on

File: hudi-common/src/main/java/org/apache/hudi/common/util/CollectionUtils.java
Patch:
@@ -184,9 +184,10 @@ public static <K, V> Map<K, V> zipToMap(List<K> keys, List<V> values) {
   }
 
   /**
-   * Returns difference b/w {@code one} {@link Set} of elements and {@code another}
+   * Returns difference b/w {@code one} {@link Collection} of elements and {@code another}
+   * The elements in collection {@code one} are also duplicated and returned as a {@link Set}.
    */
-  public static <E> Set<E> diffSet(Set<E> one, Set<E> another) {
+  public static <E> Set<E> diffSet(Collection<E> one, Set<E> another) {
     Set<E> diff = new HashSet<>(one);
     diff.removeAll(another);
     return diff;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieClient.java
Patch:
@@ -170,7 +170,8 @@ public HoodieHeartbeatClient getHeartbeatClient() {
    *
    * @param table A hoodie table instance created after transaction starts so that the latest commits and files are captured.
    * @param metadata Current committing instant's metadata
-   * @param pendingInflightAndRequestedInstants
+   * @param pendingInflightAndRequestedInstants Pending instants on the timeline
+   *
    * @see {@link BaseHoodieWriteClient#preCommit}
    * @see {@link BaseHoodieTableServiceClient#preCommit}
    */

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java
Patch:
@@ -92,7 +92,7 @@ public BaseCommitActionExecutor(HoodieEngineContext context, HoodieWriteConfig c
     this.taskContextSupplier = context.getTaskContextSupplier();
     // TODO : Remove this once we refactor and move out autoCommit method from here, since the TxnManager is held in {@link BaseHoodieWriteClient}.
     this.txnManagerOption = config.shouldAutoCommit() ? Option.of(new TransactionManager(config, table.getMetaClient().getFs())) : Option.empty();
-    if (this.txnManagerOption.isPresent() && this.txnManagerOption.get().isOptimisticConcurrencyControlEnabled()) {
+    if (this.txnManagerOption.isPresent() && this.txnManagerOption.get().isNeedsLockGuard()) {
       // these txn metadata are only needed for auto commit when optimistic concurrent control is also enabled
       this.lastCompletedTxn = TransactionUtils.getLastCompletedTxnInstantAndMetadata(table.getMetaClient());
       this.pendingInflightAndRequestedInstants = TransactionUtils.getInflightAndRequestedInstants(table.getMetaClient());

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -280,7 +280,7 @@ public void preWrite(String instantTime, WriteOperationType writeOperationType,
    * should be called before the Driver starts a new transaction.
    */
   public void preTxn(HoodieTableMetaClient metaClient) {
-    if (txnManager.isOptimisticConcurrencyControlEnabled()) {
+    if (txnManager.isNeedsLockGuard()) {
       // refresh the meta client which is reused
       metaClient.reloadActiveTimeline();
       this.lastCompletedTxnAndMetadata = TransactionUtils.getLastCompletedTxnInstantAndMetadata(metaClient);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java
Patch:
@@ -287,7 +287,7 @@ private static DataType constructSparkSchemaFromType(Type type) {
 
   /**
    * Convert Int/long type to other Type.
-   * Now only support int/long -> long/float/double/string
+   * Now only support int/long -> long/float/double/string/Decimal
    * TODO: support more types
    */
   private static boolean convertIntLongType(WritableColumnVector oldV, WritableColumnVector newV, DataType newType, int len) {
@@ -321,7 +321,7 @@ private static boolean convertIntLongType(WritableColumnVector oldV, WritableCol
 
   /**
    * Convert float type to other Type.
-   * Now only support float -> double/String
+   * Now only support float -> double/String/Decimal
    * TODO: support more types
    */
   private static boolean convertFloatType(WritableColumnVector oldV, WritableColumnVector newV, DataType newType, int len) {

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/SchemaChangeUtils.java
Patch:
@@ -38,9 +38,9 @@ private SchemaChangeUtils() {
   /**
    * Whether to allow the column type to be updated.
    * now only support:
-   * int => long/float/double/string
-   * long => float/double/string
-   * float => double/String
+   * int => long/float/double/String/Decimal
+   * long => float/double/String/Decimal
+   * float => double/String/Decimal
    * double => String/Decimal
    * Decimal => Decimal/String
    * String => date/decimal

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -131,7 +131,7 @@ public abstract class BaseHoodieWriteClient<T, I, K, O> extends BaseHoodieClient
   protected transient Timer.Context writeTimer = null;
 
   protected Option<Pair<HoodieInstant, Map<String, String>>> lastCompletedTxnAndMetadata = Option.empty();
-  protected Set<String> pendingInflightAndRequestedInstants;
+  protected Set<String> pendingInflightAndRequestedInstants = Collections.emptySet();
 
   protected BaseHoodieTableServiceClient<O> tableServiceClient;
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -180,7 +180,7 @@ private FlinkOptions() {
   public static final ConfigOption<Boolean> METADATA_ENABLED = ConfigOptions
       .key("metadata.enabled")
       .booleanType()
-      .defaultValue(false)
+      .defaultValue(true)
       .withDescription("Enable the internal metadata table which serves table metadata like level file listings, default disabled");
 
   public static final ConfigOption<Integer> METADATA_COMPACTION_DELTA_COMMITS = ConfigOptions

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/HoodieFlinkClusteringJob.java
Patch:
@@ -191,6 +191,9 @@ public AsyncClusteringService(FlinkClusteringConfig cfg, Configuration conf) thr
       // set table schema
       CompactionUtil.setAvroSchema(conf, metaClient);
 
+      // infer metadata config
+      CompactionUtil.inferMetadataConf(conf, metaClient);
+
       this.writeClient = FlinkWriteClients.createWriteClientV2(conf);
       this.writeConfig = writeClient.getConfig();
       this.table = writeClient.getHoodieTable();

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/compact/HoodieFlinkCompactor.java
Patch:
@@ -179,6 +179,9 @@ public AsyncCompactionService(FlinkCompactionConfig cfg, Configuration conf) thr
       // infer changelog mode
       CompactionUtil.inferChangelogMode(conf, metaClient);
 
+      // infer metadata config
+      CompactionUtil.inferMetadataConf(conf, metaClient);
+
       this.writeClient = FlinkWriteClients.createWriteClientV2(conf);
       this.writeConfig = writeClient.getConfig();
       this.table = writeClient.getHoodieTable();

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java
Patch:
@@ -843,7 +843,7 @@ public void dropPartition(
     } catch (Exception e) {
       throw new CatalogException(
           String.format(
-              "Failed to drop partition %s of table %s", partitionSpec, tablePath));
+              "Failed to drop partition %s of table %s", partitionSpec, tablePath), e);
     }
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -298,7 +298,7 @@ public DeltaSync(HoodieDeltaStreamer.Config cfg, SparkSession sparkSession, Sche
     }
     this.formatAdapter = new SourceFormatAdapter(
         UtilHelpers.createSource(cfg.sourceClassName, props, jssc, sparkSession, schemaProvider, metrics),
-        this.errorTableWriter);
+        this.errorTableWriter, Option.of(props));
   }
 
   /**

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestJsonKafkaSource.java
Patch:
@@ -245,7 +245,7 @@ public void testErrorEventsForDataInRowForamt() throws IOException {
     props.put("hoodie.base.path","/tmp/json_kafka_row_events");
     Source jsonSource = new JsonKafkaSource(props, jsc(), spark(), schemaProvider, metrics);
     Option<BaseErrorTableWriter> errorTableWriter = Option.of(getAnonymousErrorTableWriter(props));
-    SourceFormatAdapter kafkaSource = new SourceFormatAdapter(jsonSource, errorTableWriter);
+    SourceFormatAdapter kafkaSource = new SourceFormatAdapter(jsonSource, errorTableWriter, Option.of(props));
     assertEquals(1000, kafkaSource.fetchNewDataInRowFormat(Option.empty(),Long.MAX_VALUE).getBatch().get().count());
     assertEquals(2,((JavaRDD)errorTableWriter.get().getErrorEvents(
         HoodieActiveTimeline.createNewInstantTime(), Option.empty()).get()).count());
@@ -276,7 +276,7 @@ public void testErrorEventsForDataInAvroFormat() throws IOException {
 
     Source jsonSource = new JsonKafkaSource(props, jsc(), spark(), schemaProvider, metrics);
     Option<BaseErrorTableWriter> errorTableWriter = Option.of(getAnonymousErrorTableWriter(props));
-    SourceFormatAdapter kafkaSource = new SourceFormatAdapter(jsonSource, errorTableWriter);
+    SourceFormatAdapter kafkaSource = new SourceFormatAdapter(jsonSource, errorTableWriter, Option.of(props));
     InputBatch<JavaRDD<GenericRecord>> fetch1 = kafkaSource.fetchNewDataInAvroFormat(Option.empty(),Long.MAX_VALUE);
     assertEquals(1000,fetch1.getBatch().get().count());
     assertEquals(2, ((JavaRDD)errorTableWriter.get().getErrorEvents(

File: hudi-common/src/main/java/org/apache/hudi/common/config/ConfigGroups.java
Patch:
@@ -58,7 +58,7 @@ public enum SubGroupNames {
             + " are auto managed internally."),
     COMMIT_CALLBACK(
         "Commit Callback Configs",
-        "Configurations controling callback behavior into HTTP endpoints, to push "
+        "Configurations controlling callback behavior into HTTP endpoints, to push "
             + "notifications on commits on hudi tables."),
     NONE(
         "None",

File: hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuard.java
Patch:
@@ -69,7 +69,7 @@ enum FileVisibility {
    * 
    * @param dirPath Directory Path
    * @param files Files
-   * @param targetVisibility Target Visibitlity
+   * @param targetVisibility Target Visibility
    * @throws IOException
    * @throws TimeoutException
    */

File: hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuardConfig.java
Patch:
@@ -137,8 +137,8 @@ public Builder withConsistencyCheckEnabled(boolean enabled) {
       return this;
     }
 
-    public Builder withInitialConsistencyCheckIntervalMs(int initialIntevalMs) {
-      consistencyGuardConfig.setValue(INITIAL_CHECK_INTERVAL_MS, String.valueOf(initialIntevalMs));
+    public Builder withInitialConsistencyCheckIntervalMs(int initialIntervalMs) {
+      consistencyGuardConfig.setValue(INITIAL_CHECK_INTERVAL_MS, String.valueOf(initialIntervalMs));
       return this;
     }
 

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java
Patch:
@@ -582,7 +582,7 @@ public static Short getDefaultReplication(FileSystem fs, Path path) {
   /**
    * When a file was opened and the task died without closing the stream, another task executor cannot open because the
    * existing lease will be active. We will try to recover the lease, from HDFS. If a data node went down, it takes
-   * about 10 minutes for the lease to be rocovered. But if the client dies, this should be instant.
+   * about 10 minutes for the lease to be recovered. But if the client dies, this should be instant.
    */
   public static boolean recoverDFSFileLease(final DistributedFileSystem dfs, final Path p)
       throws IOException, InterruptedException {

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FailSafeConsistencyGuard.java
Patch:
@@ -139,7 +139,7 @@ private void waitForFileVisibility(Path filePath, FileVisibility visibility) thr
   }
 
   /**
-   * Retries the predicate for condfigurable number of times till we the predicate returns success.
+   * Retries the predicate for configurable number of times till we the predicate returns success.
    *
    * @param dir directory of interest in which list of files are checked for visibility
    * @param files List of files to check for visibility

File: hudi-common/src/main/java/org/apache/hudi/common/fs/OptimisticConsistencyGuard.java
Patch:
@@ -41,7 +41,7 @@
  * Step1 and Step2 is handled by {@link FailSafeConsistencyGuard}.
  *
  * We are simplifying these steps with {@link OptimisticConsistencyGuard}.
- * Step1: Check if all files adhere to visibility event. If yes, proceed to Sptep 3.
+ * Step1: Check if all files adhere to visibility event. If yes, proceed to Step 3.
  * Step2: If not, Sleep for a configured threshold and then proceed to next step.
  * Step3: issue deletes.
  *

File: hudi-common/src/main/java/org/apache/hudi/common/fs/SizeAwareDataOutputStream.java
Patch:
@@ -25,11 +25,11 @@
 import java.util.concurrent.atomic.AtomicLong;
 
 /**
- * Wrapper for DataOutpuStream to keep track of number of bytes written.
+ * Wrapper for DataOutputStream to keep track of number of bytes written.
  */
 public class SizeAwareDataOutputStream {
 
-  // Actual outpuStream
+  // Actual outputStream
   private DataOutputStream outputStream;
   // Counter to keep track of number of bytes written
   private AtomicLong size;

File: hudi-common/src/main/java/org/apache/hudi/common/model/debezium/AbstractDebeziumAvroPayload.java
Patch:
@@ -38,7 +38,7 @@
  * - For inserts, op=i
  * - For deletes, op=d
  * - For updates, op=u
- * - For snapshort inserts, op=r
+ * - For snapshot inserts, op=r
  * <p>
  * This payload implementation will issue matching insert, delete, updates against the hudi table
  */

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/ProtoKafkaSource.java
Patch:
@@ -58,6 +58,9 @@ public ProtoKafkaSource(TypedProperties props, JavaSparkContext sparkContext,
     props.put(NATIVE_KAFKA_VALUE_DESERIALIZER_PROP, ByteArrayDeserializer.class);
     className = props.getString(ProtoClassBasedSchemaProvider.Config.PROTO_SCHEMA_CLASS_NAME.key());
     this.offsetGen = new KafkaOffsetGen(props);
+    if (this.shouldAddOffsets) {
+      throw new HoodieException("Appending kafka offsets to ProtoKafkaSource is not supported");
+    }
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/SchemaChangeUtils.java
Patch:
@@ -77,6 +77,9 @@ public static boolean isTypeUpdateAllow(Type src, Type dsr) {
           if (decimalDsr.isWiderThan(decimalSrc)) {
             return true;
           }
+          if (decimalDsr.precision() >= decimalSrc.precision() && decimalDsr.scale() == decimalSrc.scale()) {
+            return true;
+          }
         } else if (dsr.typeId() == Type.TypeID.STRING) {
           return true;
         }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/RowDataToAvroConverters.java
Patch:
@@ -37,7 +37,6 @@
 import java.math.BigDecimal;
 import java.nio.ByteBuffer;
 import java.time.Instant;
-import java.time.temporal.ChronoUnit;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/RowDataToAvroConverters.java
Patch:
@@ -176,7 +176,8 @@ public Object convert(Schema schema, Object object) {
 
                 @Override
                 public Object convert(Schema schema, Object object) {
-                  return ChronoUnit.MICROS.between(Instant.EPOCH, ((TimestampData) object).toInstant());
+                  Instant instant = ((TimestampData) object).toInstant();
+                  return  Math.addExact(Math.multiplyExact(instant.getEpochSecond(), 1000_000), instant.getNano() / 1000);
                 }
               };
         } else {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.utilities.UtilHelpers;
 import org.apache.hudi.utilities.schema.FilebasedSchemaProvider;
 import org.apache.hudi.utilities.schema.SchemaProvider;
+import org.apache.hudi.utilities.schema.SchemaRegistryProvider;
 import org.apache.hudi.utilities.sources.InputBatch;
 import org.apache.hudi.utilities.sources.Source;
 import org.apache.hudi.utilities.sources.helpers.AvroConvertor;
@@ -72,7 +73,7 @@ public InputBatch<JavaRDD<GenericRecord>> fetchNewDataInAvroFormat(Option<String
         return new InputBatch<>(Option.ofNullable(r.getBatch().map(
             rdd -> {
               SchemaProvider originalProvider = UtilHelpers.getOriginalSchemaProvider(r.getSchemaProvider());
-              return (originalProvider instanceof FilebasedSchemaProvider)
+              return ((originalProvider instanceof FilebasedSchemaProvider) || (originalProvider instanceof SchemaRegistryProvider))
                   // If the source schema is specified through Avro schema,
                   // pass in the schema for the Row-to-Avro conversion
                   // to avoid nullability mismatch between Avro schema and Row schema

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -150,7 +150,8 @@ protected void commit(String instantTime, Map<MetadataPartitionType, HoodieData<
     engineContext.setJobStatus(this.getClass().getName(), "Committing " + instantTime + " to metadata table " + metadataWriteConfig.getTableName());
     try (SparkRDDWriteClient writeClient = new SparkRDDWriteClient(engineContext, metadataWriteConfig)) {
       // rollback partially failed writes if any.
-      if (writeClient.rollbackFailedWrites()) {
+      if (dataWriteConfig.getFailedWritesCleanPolicy().isEager()
+          && writeClient.rollbackFailedWrites()) {
         metadataMetaClient = HoodieTableMetaClient.reload(metadataMetaClient);
       }
       if (canTriggerTableService) {

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -1155,7 +1155,6 @@ private HoodieIndexer.Config buildIndexerConfig(String basePath,
     return config;
   }
 
-  @Disabled("HUDI-5815 for investigation")
   @ParameterizedTest
   @EnumSource(value = HoodieRecordType.class, names = {"AVRO", "SPARK"})
   public void testHoodieIndexer(HoodieRecordType recordType) throws Exception {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestDisruptorExecutionInSpark.java
Patch:
@@ -31,7 +31,6 @@
 import org.apache.spark.TaskContext$;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
-import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.Timeout;
 
@@ -44,7 +43,6 @@
 import static org.junit.jupiter.api.Assertions.assertThrows;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
-@Disabled("HUDI-5792")
 public class TestDisruptorExecutionInSpark extends HoodieClientTestHarness {
 
   private final String instantTime = HoodieActiveTimeline.createNewInstantTime();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java
Patch:
@@ -84,7 +84,7 @@ class Heartbeat {
     private Boolean isHeartbeatStopped = false;
     private Long lastHeartbeatTime;
     private Integer numHeartbeats = 0;
-    private Timer timer = new Timer();
+    private Timer timer = new Timer(true);
 
     public String getInstantTime() {
       return instantTime;
@@ -226,6 +226,7 @@ public boolean isHeartbeatExpired(String instantTime) throws IOException {
       lastHeartbeatForWriter = new Heartbeat();
       lastHeartbeatForWriter.setLastHeartbeatTime(lastHeartbeatForWriterTime);
       lastHeartbeatForWriter.setInstantTime(instantTime);
+      lastHeartbeatForWriter.getTimer().cancel();
     }
     if (currentTime - lastHeartbeatForWriter.getLastHeartbeatTime() > this.maxAllowableHeartbeatIntervalInMs) {
       LOG.warn("Heartbeat expired, currentTime = " + currentTime + ", last heartbeat = " + lastHeartbeatForWriter

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/HMSDDLExecutor.java
Patch:
@@ -137,12 +137,12 @@ public void createTable(String tableName, MessageType storageSchema, String inpu
 
       if (!syncConfig.getBoolean(HIVE_CREATE_MANAGED_TABLE)) {
         newTb.putToParameters("EXTERNAL", "TRUE");
+        newTb.setTableType(TableType.EXTERNAL_TABLE.toString());
       }
 
       for (Map.Entry<String, String> entry : tableProperties.entrySet()) {
         newTb.putToParameters(entry.getKey(), entry.getValue());
       }
-      newTb.setTableType(TableType.EXTERNAL_TABLE.toString());
       client.createTable(newTb);
     } catch (Exception e) {
       LOG.error("failed to create table " + tableName, e);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -1218,7 +1218,7 @@ protected void setWriteSchemaForDeletes(HoodieTableMetaClient metaClient) {
           throw new HoodieIOException("Latest commit does not have any schema in commit metadata");
         }
       } else {
-        throw new HoodieIOException("Deletes issued without any prior commits");
+        LOG.warn("None rows are deleted because the table is empty");
       }
     } catch (IOException e) {
       throw new HoodieIOException("IOException thrown while reading last commit metadata", e);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -2219,9 +2219,7 @@ public void testDeletesWithoutInserts(boolean populateMetaFields) {
     List<HoodieRecord> dummyInserts = dataGen.generateInserts(commitTime1, 20);
     List<HoodieKey> hoodieKeysToDelete = randomSelectAsHoodieKeys(dummyInserts, 20);
     JavaRDD<HoodieKey> deleteKeys = jsc.parallelize(hoodieKeysToDelete, 1);
-    assertThrows(HoodieIOException.class, () -> {
-      client.delete(deleteKeys, commitTime1).collect();
-    }, "Should have thrown Exception");
+    client.delete(deleteKeys, commitTime1).collect();
   }
 
   /**

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -382,9 +382,9 @@ private FlinkOptions() {
       .key("write.ignore.failed")
       .booleanType()
       .defaultValue(false)
-      .withDescription("Flag to indicate whether to ignore any non exception error (e.g. writestatus error). within a checkpoint batch.\n"
-          + "By default false.  Turning this on, could hide the write status errors while the spark checkpoint moves ahead. \n"
-          + "  So, would recommend users to use this with caution.");
+      .withDescription("Flag to indicate whether to ignore any non exception error (e.g. writestatus error). within a checkpoint batch. \n"
+          + "By default false. Turning this on, could hide the write status errors while the flink checkpoint moves ahead. \n"
+          + "So, would recommend users to use this with caution.");
 
   public static final ConfigOption<String> RECORD_KEY_FIELD = ConfigOptions
       .key(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key())

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java
Patch:
@@ -138,8 +138,9 @@ public Option<HoodieIndexCommitMetadata> execute() {
       List<HoodieIndexPartitionInfo> finalIndexPartitionInfos = null;
       if (!firstTimeInitializingMetadataTable) {
         // start indexing for each partition
-        HoodieTableMetadataWriter metadataWriter = table.getMetadataWriter(instantTime)
-            .orElseThrow(() -> new HoodieIndexException(String.format("Could not get metadata writer to run index action for instant: %s", instantTime)));
+        HoodieTableMetadataWriter metadataWriter = table.getIndexingMetadataWriter(instantTime)
+            .orElseThrow(() -> new HoodieIndexException(String.format(
+                "Could not get metadata writer to run index action for instant: %s", instantTime)));
         // this will only build index upto base instant as generated by the plan, we will be doing catchup later
         String indexUptoInstant = indexPartitionInfos.get(0).getIndexUptoInstant();
         LOG.info("Starting Index Building with base instant: " + indexUptoInstant);

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/TestHoodieTableFactory.java
Patch:
@@ -189,7 +189,7 @@ void testTableTypeCheck() {
 
     // Table type unset. The default value will be ok
     final MockContext sourceContext1 = MockContext.getInstance(this.conf, schema, "f2");
-    assertThrows(HoodieValidationException.class, () -> new HoodieTableFactory().createDynamicTableSink(sourceContext1));
+    assertDoesNotThrow(() -> new HoodieTableFactory().createDynamicTableSink(sourceContext1));
 
     // Invalid table type will throw exception
     this.conf.set(FlinkOptions.TABLE_TYPE, "INVALID_TABLE_TYPE");

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/IncrementalInputSplits.java
Patch:
@@ -535,7 +535,8 @@ public List<HoodieInstant> filterInstantsWithRange(
    *
    * @return the filtered timeline
    */
-  private HoodieTimeline filterInstantsByCondition(HoodieTimeline timeline) {
+  @VisibleForTesting
+  public HoodieTimeline filterInstantsByCondition(HoodieTimeline timeline) {
     final HoodieTimeline oriTimeline = timeline;
     if (this.skipCompaction) {
       // the compaction commit uses 'commit' as action which is tricky
@@ -601,7 +602,7 @@ public static class Builder {
     // skip compaction
     private boolean skipCompaction = false;
     // skip clustering
-    private boolean skipClustering = true;
+    private boolean skipClustering = false;
 
     public Builder() {
     }

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -247,7 +247,9 @@ private void resetFileGroupsReplaced(HoodieTimeline timeline) {
       }
     });
 
-    Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups = resultStream.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
+    // Duplicate key error when insert_overwrite same partition in multi writer, keep the instant with greater timestamp when the file group id conflicts
+    Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups = resultStream.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue,
+        (instance1, instance2) -> HoodieTimeline.compareTimestamps(instance1.getTimestamp(), HoodieTimeline.LESSER_THAN, instance2.getTimestamp()) ? instance2 : instance1));
     resetReplacedFileGroups(replacedFileGroups);
     LOG.info("Took " + hoodieTimer.endTimer() + " ms to read  " + replacedTimeline.countInstants() + " instants, "
         + replacedFileGroups.size() + " replaced file groups");

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -165,11 +165,11 @@ public class HoodieWriteConfig extends HoodieConfig {
       .withValidValues(Arrays.stream(ExecutorType.values()).map(Enum::name).toArray(String[]::new))
       .sinceVersion("0.13.0")
       .withDocumentation("Set executor which orchestrates concurrent producers and consumers communicating through a message queue."
-          + "BOUNDED_IN_MEMORY(default): Use LinkedBlockingQueue as a bounded in-memory queue, this queue will use extra lock to balance producers and consumer"
+          + "BOUNDED_IN_MEMORY: Use LinkedBlockingQueue as a bounded in-memory queue, this queue will use extra lock to balance producers and consumer"
           + "DISRUPTOR: Use disruptor which a lock free message queue as inner message, this queue may gain better writing performance if lock was the bottleneck. "
-          + "SIMPLE: Executor with no inner message queue and no inner lock. Consuming and writing records from iterator directly. Compared with BIM and DISRUPTOR, "
+          + "SIMPLE(default): Executor with no inner message queue and no inner lock. Consuming and writing records from iterator directly. Compared with BIM and DISRUPTOR, "
           + "this queue has no need for additional memory and cpu resources due to lock or multithreading, but also lost some benefits such as speed limit. "
-          + "Although DISRUPTOR_EXECUTOR and SIMPLE are still in experimental.");
+          + "Although DISRUPTOR is still experimental.");
 
   public static final ConfigProperty<String> KEYGENERATOR_TYPE = ConfigProperty
       .key("hoodie.datasource.write.keygenerator.type")

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieIndexer.java
Patch:
@@ -126,8 +126,8 @@ public static class Config implements Serializable {
     public String indexTypes = null;
     @Parameter(names = {"--mode", "-m"}, description = "Set job mode: Set \"schedule\" to generate an indexing plan; "
         + "Set \"execute\" to execute the indexing plan at the given instant, which means --instant-time is required here; "
-        + "Set \"scheduleandExecute\" to generate an indexing plan first and execute that plan immediately;"
-        + "Set \"dropindex\" to drop the index types specified in --index-types;")
+        + "Set \"scheduleAndExecute\" to generate an indexing plan first and execute that plan immediately;"
+        + "Set \"dropIndex\" to drop the index types specified in --index-types;")
     public String runningMode = null;
     @Parameter(names = {"--help", "-h"}, help = true)
     public Boolean help = false;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieBootstrapConfig.java
Patch:
@@ -108,7 +108,7 @@ public class HoodieBootstrapConfig extends HoodieConfig {
       .key("hoodie.bootstrap.index.class")
       .defaultValue(HFileBootstrapIndex.class.getName())
       .sinceVersion("0.6.0")
-      .withDocumentation("Implementation to use, for mapping a skeleton base file to a boostrap base file.");
+      .withDocumentation("Implementation to use, for mapping a skeleton base file to a bootstrap base file.");
 
   /**
    * @deprecated Use {@link #BASE_PATH} and its methods instead

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieMergeOnReadTableInputFormat.java
Patch:
@@ -282,7 +282,7 @@ private FileSplit doMakeSplitForRealtimePath(HoodieRealtimePath path, long start
           inMemoryHosts == null
               ? super.makeSplit(path.getPathWithBootstrapFileStatus(), start, length, hosts)
               : super.makeSplit(path.getPathWithBootstrapFileStatus(), start, length, hosts, inMemoryHosts);
-      return createRealtimeBoostrapBaseFileSplit(
+      return createRealtimeBootstrapBaseFileSplit(
           (BootstrapBaseFileSplit) bf,
           path.getBasePath(),
           path.getDeltaLogFiles(),
@@ -312,7 +312,7 @@ private static HoodieRealtimeFileSplit createRealtimeFileSplit(HoodieRealtimePat
     }
   }
 
-  private static HoodieRealtimeBootstrapBaseFileSplit createRealtimeBoostrapBaseFileSplit(BootstrapBaseFileSplit split,
+  private static HoodieRealtimeBootstrapBaseFileSplit createRealtimeBootstrapBaseFileSplit(BootstrapBaseFileSplit split,
                                                                                           String basePath,
                                                                                           List<HoodieLogFile> logFiles,
                                                                                           String maxInstantTime,

File: hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java
Patch:
@@ -354,7 +354,7 @@ private static ColumnReader createColumnReader(
             return new BytesColumnReader(descriptor, pageReader);
           case FIXED_LEN_BYTE_ARRAY:
             return new FixedLenBytesColumnReader(
-                descriptor, pageReader, ((DecimalType) fieldType).getPrecision());
+                descriptor, pageReader);
           default:
             throw new AssertionError();
         }

File: hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/FixedLenBytesColumnReader.java
Patch:
@@ -39,7 +39,7 @@ public class FixedLenBytesColumnReader<V extends WritableColumnVector>
     extends AbstractColumnReader<V> {
 
   public FixedLenBytesColumnReader(
-      ColumnDescriptor descriptor, PageReader pageReader, int precision) throws IOException {
+      ColumnDescriptor descriptor, PageReader pageReader) throws IOException {
     super(descriptor, pageReader);
     checkTypeName(PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY);
   }

File: hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java
Patch:
@@ -354,7 +354,7 @@ private static ColumnReader createColumnReader(
             return new BytesColumnReader(descriptor, pageReader);
           case FIXED_LEN_BYTE_ARRAY:
             return new FixedLenBytesColumnReader(
-                descriptor, pageReader, ((DecimalType) fieldType).getPrecision());
+                descriptor, pageReader);
           default:
             throw new AssertionError();
         }

File: hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/FixedLenBytesColumnReader.java
Patch:
@@ -39,7 +39,7 @@ public class FixedLenBytesColumnReader<V extends WritableColumnVector>
     extends AbstractColumnReader<V> {
 
   public FixedLenBytesColumnReader(
-      ColumnDescriptor descriptor, PageReader pageReader, int precision) throws IOException {
+      ColumnDescriptor descriptor, PageReader pageReader) throws IOException {
     super(descriptor, pageReader);
     checkTypeName(PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY);
   }

File: hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java
Patch:
@@ -354,7 +354,7 @@ private static ColumnReader createColumnReader(
             return new BytesColumnReader(descriptor, pageReader);
           case FIXED_LEN_BYTE_ARRAY:
             return new FixedLenBytesColumnReader(
-                descriptor, pageReader, ((DecimalType) fieldType).getPrecision());
+                descriptor, pageReader);
           default:
             throw new AssertionError();
         }

File: hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/FixedLenBytesColumnReader.java
Patch:
@@ -39,7 +39,7 @@ public class FixedLenBytesColumnReader<V extends WritableColumnVector>
     extends AbstractColumnReader<V> {
 
   public FixedLenBytesColumnReader(
-      ColumnDescriptor descriptor, PageReader pageReader, int precision) throws IOException {
+      ColumnDescriptor descriptor, PageReader pageReader) throws IOException {
     super(descriptor, pageReader);
     checkTypeName(PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY);
   }

File: hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java
Patch:
@@ -354,7 +354,7 @@ private static ColumnReader createColumnReader(
             return new BytesColumnReader(descriptor, pageReader);
           case FIXED_LEN_BYTE_ARRAY:
             return new FixedLenBytesColumnReader(
-                descriptor, pageReader, ((DecimalType) fieldType).getPrecision());
+                descriptor, pageReader);
           default:
             throw new AssertionError();
         }

File: hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/FixedLenBytesColumnReader.java
Patch:
@@ -39,7 +39,7 @@ public class FixedLenBytesColumnReader<V extends WritableColumnVector>
     extends AbstractColumnReader<V> {
 
   public FixedLenBytesColumnReader(
-      ColumnDescriptor descriptor, PageReader pageReader, int precision) throws IOException {
+      ColumnDescriptor descriptor, PageReader pageReader) throws IOException {
     super(descriptor, pageReader);
     checkTypeName(PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY);
   }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java
Patch:
@@ -597,7 +597,7 @@ private Table instantiateHiveTable(ObjectPath tablePath, CatalogBaseTable table,
     serdeProperties.put(ConfigUtils.IS_QUERY_AS_RO_TABLE, String.valueOf(!useRealTimeInputFormat));
     serdeProperties.put("serialization.format", "1");
 
-    serdeProperties.putAll(TableOptionProperties.translateFlinkTableProperties2Spark(catalogTable, hiveConf, properties, partitionKeys));
+    serdeProperties.putAll(TableOptionProperties.translateFlinkTableProperties2Spark(catalogTable, hiveConf, properties, partitionKeys, withOperationField));
 
     sd.setSerdeInfo(new SerDeInfo(null, serDeClassName, serdeProperties));
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/HoodieLazyInsertIterable.java
Patch:
@@ -23,11 +23,11 @@
 import org.apache.hudi.client.utils.LazyIterableIterator;
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.util.queue.ExecutorType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.CreateHandleFactory;
 import org.apache.hudi.io.WriteHandleFactory;
 import org.apache.hudi.table.HoodieTable;
+import org.apache.hudi.util.ExecutorFactory;
 
 import java.util.Iterator;
 import java.util.List;
@@ -104,7 +104,7 @@ private static <T> Function<HoodieRecord<T>, HoodieInsertValueGenResult<HoodieRe
     //       it since these records will be subsequently buffered (w/in the in-memory queue);
     //       Only case when we don't need to make a copy is when using [[SimpleExecutor]] which
     //       is guaranteed to not hold on to references to any records
-    boolean shouldClone = writeConfig.getExecutorType() != ExecutorType.SIMPLE;
+    boolean shouldClone = ExecutorFactory.isBufferingRecords(writeConfig);
 
     return record -> {
       HoodieRecord<T> clonedRecord = shouldClone ? record.copy() : record;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java
Patch:
@@ -95,7 +95,7 @@ protected HoodieWriteHandle(HoodieWriteConfig config, String instantTime, String
         !hoodieTable.getIndex().isImplicitWithStorage(), config.getWriteStatusFailureFraction());
     this.taskContextSupplier = taskContextSupplier;
     this.writeToken = makeWriteToken();
-    schemaOnReadEnabled = !isNullOrEmpty(hoodieTable.getConfig().getInternalSchema());
+    this.schemaOnReadEnabled = !isNullOrEmpty(hoodieTable.getConfig().getInternalSchema());
     this.recordMerger = config.getRecordMerger();
   }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/HoodieSparkParquetReader.java
Patch:
@@ -122,7 +122,8 @@ public Schema getSchema() {
     // and therefore if we convert to Avro directly we'll lose logical type-info.
     MessageType messageType = ((ParquetUtils) parquetUtils).readSchema(conf, path);
     StructType structType = new ParquetToSparkSchemaConverter(conf).convert(messageType);
-    return SparkAdapterSupport$.MODULE$.sparkAdapter().getAvroSchemaConverters()
+    return SparkAdapterSupport$.MODULE$.sparkAdapter()
+        .getAvroSchemaConverters()
         .toAvroType(structType, true, messageType.getName(), StringUtils.EMPTY_STRING);
   }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java
Patch:
@@ -480,7 +480,7 @@ private Object[] getNestedFieldValues(InternalRow row, HoodieUnsafeRowUtils.Nest
     private HoodieUnsafeRowUtils.NestedFieldPath[] resolveNestedFieldPaths(List<String> fieldPaths, StructType schema, boolean returnNull) {
       try {
         return fieldPaths.stream()
-            .map(fieldPath -> HoodieUnsafeRowUtils$.MODULE$.composeNestedFieldPath(schema, fieldPath))
+            .map(fieldPath -> HoodieUnsafeRowUtils$.MODULE$.composeNestedFieldPath(schema, fieldPath).get())
             .toArray(HoodieUnsafeRowUtils.NestedFieldPath[]::new);
       } catch (Exception e) {
         if (returnNull) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/OrcBootstrapMetadataHandler.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.table.action.bootstrap;
 
-import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.avro.model.HoodieFileStatus;
 import org.apache.hudi.client.bootstrap.BootstrapRecordPayload;
 import org.apache.hudi.common.model.HoodieAvroRecord;
@@ -47,6 +46,8 @@
 
 import java.io.IOException;
 
+import static org.apache.hudi.io.HoodieBootstrapHandle.METADATA_BOOTSTRAP_RECORD_SCHEMA;
+
 class OrcBootstrapMetadataHandler extends BaseBootstrapMetadataHandler {
   private static final Logger LOG = LogManager.getLogger(OrcBootstrapMetadataHandler.class);
 
@@ -75,7 +76,7 @@ void executeBootstrap(HoodieBootstrapHandle<?, ?, ?, ?> bootstrapHandle, Path so
       wrapper = new BoundedInMemoryExecutor<GenericRecord, HoodieRecord, Void>(config.getWriteBufferLimitBytes(),
           new OrcReaderIterator(reader, avroSchema, orcSchema), new BootstrapRecordConsumer(bootstrapHandle), inp -> {
         String recKey = keyGenerator.getKey(inp).getRecordKey();
-        GenericRecord gr = new GenericData.Record(HoodieAvroUtils.RECORD_KEY_SCHEMA);
+        GenericRecord gr = new GenericData.Record(METADATA_BOOTSTRAP_RECORD_SCHEMA);
         gr.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, recKey);
         BootstrapRecordPayload payload = new BootstrapRecordPayload(gr);
         HoodieRecord rec = new HoodieAvroRecord(new HoodieKey(recKey, partitionPath), payload);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamerWithMultiWriter.java
Patch:
@@ -73,6 +73,7 @@ public class TestHoodieDeltaStreamerWithMultiWriter extends SparkClientFunctiona
   String propsFilePath;
   String tableBasePath;
 
+  @Disabled("HUDI-5653")
   @ParameterizedTest
   @EnumSource(HoodieTableType.class)
   void testUpsertsContinuousModeWithMultipleWritersForConflicts(HoodieTableType tableType) throws Exception {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bucket/BucketStreamWriteFunction.java
Patch:
@@ -156,7 +156,7 @@ private void bootstrapIndexIfNeed(String partition) {
 
     // Load existing fileID belongs to this task
     Map<Integer, String> bucketToFileIDMap = new HashMap<>();
-    this.writeClient.getHoodieTable().getFileSystemView().getAllFileGroups(partition).forEach(fileGroup -> {
+    this.writeClient.getHoodieTable().getHoodieView().getAllFileGroups(partition).forEach(fileGroup -> {
       String fileID = fileGroup.getFileGroupId().getFileId();
       int bucketNumber = BucketIdentifier.bucketIdFromFileId(fileID);
       if (isBucketToLoad(bucketNumber, partition)) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -140,7 +140,7 @@ public class HoodieTableConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> CDC_SUPPLEMENTAL_LOGGING_MODE = ConfigProperty
       .key("hoodie.table.cdc.supplemental.logging.mode")
-      .defaultValue(op_key_only.name())
+      .defaultValue(data_before_after.name())
       .withValidValues(
           op_key_only.name(),
           data_before.name(),

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/util/ExecutorFactory.java
Patch:
@@ -52,7 +52,7 @@ public static <I, O, E> HoodieExecutor<E> create(HoodieWriteConfig hoodieConfig,
         return new BoundedInMemoryExecutor<>(hoodieConfig.getWriteBufferLimitBytes(), inputItr, consumer,
             transformFunction, preExecuteRunnable);
       case DISRUPTOR:
-        return new DisruptorExecutor<>(hoodieConfig.getWriteExecutorDisruptorWriteBufferSize(), inputItr, consumer,
+        return new DisruptorExecutor<>(hoodieConfig.getWriteExecutorDisruptorWriteBufferLimitBytes(), inputItr, consumer,
             transformFunction, hoodieConfig.getWriteExecutorDisruptorWaitStrategy(), preExecuteRunnable);
       case SIMPLE:
         return new SimpleExecutor<>(inputItr, consumer, transformFunction);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestDisruptorMessageQueue.java
Patch:
@@ -68,7 +68,7 @@ public class TestDisruptorMessageQueue extends HoodieClientTestHarness {
 
   private final HoodieWriteConfig writeConfig = HoodieWriteConfig.newBuilder()
       .withExecutorType(ExecutorType.DISRUPTOR.name())
-      .withWriteExecutorDisruptorWriteBufferSize(16)
+      .withWriteExecutorDisruptorWriteBufferLimitBytes(16)
       .build(false);
 
   @BeforeEach
@@ -139,7 +139,7 @@ public Integer finish() {
     DisruptorExecutor<HoodieRecord, Tuple2<HoodieRecord, Option<IndexedRecord>>, Integer> exec = null;
 
     try {
-      exec = new DisruptorExecutor(writeConfig.getWriteExecutorDisruptorWriteBufferSize(), hoodieRecords.iterator(), consumer,
+      exec = new DisruptorExecutor(writeConfig.getWriteExecutorDisruptorWriteBufferLimitBytes(), hoodieRecords.iterator(), consumer,
           getTransformer(HoodieTestDataGenerator.AVRO_SCHEMA, writeConfig), WaitStrategyFactory.DEFAULT_STRATEGY, getPreExecuteRunnable());
       int result = exec.execute();
       // It should buffer and write 100 records

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java
Patch:
@@ -222,7 +222,7 @@ public String showLogFileRecords(
               .withDiskMapType(HoodieCommonConfig.SPILLABLE_DISK_MAP_TYPE.defaultValue())
               .withBitCaskDiskMapCompressionEnabled(HoodieCommonConfig.DISK_MAP_BITCASK_COMPRESSION_ENABLED.defaultValue())
               .withRecordMerger(HoodieRecordUtils.loadRecordMerger(HoodieAvroRecordMerger.class.getName()))
-              .withUseScanV2(Boolean.parseBoolean(HoodieCompactionConfig.USE_LOG_RECORD_READER_SCAN_V2.defaultValue()))
+              .withOptimizedLogBlocksScan(Boolean.parseBoolean(HoodieCompactionConfig.ENABLE_OPTIMIZED_LOG_BLOCKS_SCAN.defaultValue()))
               .build();
       for (HoodieRecord hoodieRecord : scanner) {
         Option<HoodieAvroIndexedRecord> record = hoodieRecord.toIndexedRecord(readerSchema, new Properties());

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestHoodieLogFileCommand.java
Patch:
@@ -233,7 +233,7 @@ public void testShowLogFileRecordsWithMerge() throws IOException, InterruptedExc
         .withDiskMapType(HoodieCommonConfig.SPILLABLE_DISK_MAP_TYPE.defaultValue())
         .withBitCaskDiskMapCompressionEnabled(HoodieCommonConfig.DISK_MAP_BITCASK_COMPRESSION_ENABLED.defaultValue())
         .withRecordMerger(HoodieRecordUtils.loadRecordMerger(HoodieAvroRecordMerger.class.getName()))
-        .withUseScanV2(Boolean.parseBoolean(HoodieCompactionConfig.USE_LOG_RECORD_READER_SCAN_V2.defaultValue()))
+        .withOptimizedLogBlocksScan(Boolean.parseBoolean(HoodieCompactionConfig.ENABLE_OPTIMIZED_LOG_BLOCKS_SCAN.defaultValue()))
         .build();
 
     Iterator<HoodieRecord> records = scanner.iterator();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -1306,8 +1306,8 @@ public int getLogCompactionBlocksThreshold() {
     return getInt(HoodieCompactionConfig.LOG_COMPACTION_BLOCKS_THRESHOLD);
   }
 
-  public boolean useScanV2ForLogRecordReader() {
-    return getBoolean(HoodieCompactionConfig.USE_LOG_RECORD_READER_SCAN_V2);
+  public boolean enableOptimizedLogBlocksScan() {
+    return getBoolean(HoodieCompactionConfig.ENABLE_OPTIMIZED_LOG_BLOCKS_SCAN);
   }
 
   public HoodieCleaningPolicy getCleanerPolicy() {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -283,7 +283,7 @@ private HoodieWriteConfig createMetadataWriteConfig(HoodieWriteConfig writeConfi
             // by default, the HFile does not keep the metadata fields, set up as false
             // to always use the metadata of the new record.
             .withPreserveCommitMetadata(false)
-            .withLogRecordReaderScanV2(String.valueOf(writeConfig.useScanV2ForLogRecordReader()))
+            .withEnableOptimizedLogBlocksScan(String.valueOf(writeConfig.enableOptimizedLogBlocksScan()))
             .build())
         .withParallelism(parallelism, parallelism)
         .withDeleteParallelism(parallelism)

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionExecutionHelper.java
Patch:
@@ -71,8 +71,8 @@ protected Iterator<List<WriteStatus>> writeFileAndGetWriteStats(HoodieCompaction
     return result;
   }
 
-  protected boolean useScanV2(HoodieWriteConfig writeConfig) {
-    return writeConfig.useScanV2ForLogRecordReader();
+  protected boolean enableOptimizedLogBlockScan(HoodieWriteConfig writeConfig) {
+    return writeConfig.enableOptimizedLogBlocksScan();
   }
 
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java
Patch:
@@ -199,7 +199,7 @@ public List<WriteStatus> compact(HoodieCompactionHandler compactionHandler,
         .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())
         .withOperationField(config.allowOperationMetadataField())
         .withPartition(operation.getPartitionPath())
-        .withUseScanV2(executionHelper.useScanV2(config))
+        .withOptimizedLogBlocksScan(executionHelper.enableOptimizedLogBlockScan(config))
         .withRecordMerger(config.getRecordMerger())
         .build();
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/LogCompactionExecutionHelper.java
Patch:
@@ -80,7 +80,7 @@ protected Iterator<List<WriteStatus>> writeFileAndGetWriteStats(HoodieCompaction
   }
 
   @Override
-  protected boolean useScanV2(HoodieWriteConfig writeConfig) {
+  protected boolean enableOptimizedLogBlockScan(HoodieWriteConfig writeConfig) {
     return true;
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/plan/generators/HoodieLogCompactionPlanGenerator.java
Patch:
@@ -90,7 +90,7 @@ private boolean isFileSliceEligibleForLogCompaction(FileSlice fileSlice, String
             .collect(Collectors.toList()))
         .withLatestInstantTime(maxInstantTime)
         .withBufferSize(writeConfig.getMaxDFSStreamBufferSize())
-        .withUseScanV2(true)
+        .withOptimizedLogBlocksScan(true)
         .withRecordMerger(writeConfig.getRecordMerger())
         .build();
     scanner.scan(true);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java
Patch:
@@ -293,7 +293,7 @@ private HoodieData<HoodieRecord<T>> readRecordsForGroupWithLogs(JavaSparkContext
               .withBufferSize(config.getMaxDFSStreamBufferSize())
               .withSpillableMapBasePath(config.getSpillableMapBasePath())
               .withPartition(clusteringOp.getPartitionPath())
-              .withUseScanV2(config.useScanV2ForLogRecordReader())
+              .withOptimizedLogBlocksScan(config.enableOptimizedLogBlocksScan())
               .withDiskMapType(config.getCommonConfig().getSpillableDiskMapType())
               .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())
               .withRecordMerger(config.getRecordMerger())

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnMergeOnReadStorage.java
Patch:
@@ -200,7 +200,7 @@ public void testLogCompactionOnMORTable() throws Exception {
   public void testLogCompactionOnMORTableWithoutBaseFile() throws Exception {
     HoodieCompactionConfig compactionConfig = HoodieCompactionConfig.newBuilder()
         .withLogCompactionBlocksThreshold("1")
-        .withLogRecordReaderScanV2("true")
+        .withEnableOptimizedLogBlocksScan("true")
         .build();
     HoodieWriteConfig config = getConfigBuilder(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA,
         HoodieIndex.IndexType.INMEMORY).withAutoCommit(true).withCompactionConfig(compactionConfig).build();
@@ -447,7 +447,7 @@ private void validateBlockInstantsBeforeAndAfterRollback(HoodieWriteConfig confi
                 .collect(Collectors.toList()))
             .withLatestInstantTime(instant)
             .withBufferSize(config.getMaxDFSStreamBufferSize())
-            .withUseScanV2(true)
+            .withOptimizedLogBlocksScan(true)
             .withRecordMerger(HoodieRecordUtils.loadRecordMerger(HoodieAvroRecordMerger.class.getName()))
             .build();
         scanner.scan(true);
@@ -461,7 +461,7 @@ private void validateBlockInstantsBeforeAndAfterRollback(HoodieWriteConfig confi
                 .collect(Collectors.toList()))
             .withLatestInstantTime(currentInstant)
             .withBufferSize(config.getMaxDFSStreamBufferSize())
-            .withUseScanV2(true)
+            .withOptimizedLogBlocksScan(true)
             .withRecordMerger(HoodieRecordUtils.loadRecordMerger(HoodieAvroRecordMerger.class.getName()))
             .build();
         scanner2.scan(true);

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -531,7 +531,7 @@ public Pair<HoodieMetadataLogRecordReader, Long> getLogRecordScanner(List<Hoodie
         .withLogBlockTimestamps(validInstantTimestamps)
         .enableFullScan(allowFullScan)
         .withPartition(partitionName)
-        .withUseScanV2(metadataConfig.getUseLogRecordReaderScanV2())
+        .withEnableOptimizedLogBlocksScan(metadataConfig.doEnableOptimizedLogBlocksScan())
         .build();
 
     Long logScannerOpenMs = timer.endTimer();

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataLogRecordReader.java
Patch:
@@ -209,8 +209,8 @@ public Builder enableFullScan(boolean enableFullScan) {
       return this;
     }
 
-    public Builder withUseScanV2(boolean useScanV2) {
-      scannerBuilder.withUseScanV2(useScanV2);
+    public Builder withEnableOptimizedLogBlocksScan(boolean enableOptimizedLogBlocksScan) {
+      scannerBuilder.withOptimizedLogBlocksScan(enableOptimizedLogBlocksScan);
       return this;
     }
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java
Patch:
@@ -100,7 +100,7 @@ private HoodieMergedLogRecordScanner getMergedLogRecordScanner() throws IOExcept
         .withDiskMapType(jobConf.getEnum(HoodieCommonConfig.SPILLABLE_DISK_MAP_TYPE.key(), HoodieCommonConfig.SPILLABLE_DISK_MAP_TYPE.defaultValue()))
         .withBitCaskDiskMapCompressionEnabled(jobConf.getBoolean(HoodieCommonConfig.DISK_MAP_BITCASK_COMPRESSION_ENABLED.key(),
             HoodieCommonConfig.DISK_MAP_BITCASK_COMPRESSION_ENABLED.defaultValue()))
-        .withUseScanV2(jobConf.getBoolean(HoodieRealtimeConfig.USE_LOG_RECORD_READER_SCAN_V2, false))
+        .withOptimizedLogBlocksScan(jobConf.getBoolean(HoodieRealtimeConfig.USE_LOG_RECORD_READER_SCAN_V2, false))
         .withInternalSchema(schemaEvolutionContext.internalSchemaOption.orElse(InternalSchema.getEmptyInternalSchema()))
         .withRecordMerger(HoodieRecordUtils.loadRecordMerger(HoodieAvroRecordMerger.class.getName()))
         .build();

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/reader/DFSHoodieDatasetInputReader.java
Patch:
@@ -290,7 +290,7 @@ private Iterator<IndexedRecord> readColumnarOrLogFiles(FileSlice fileSlice) thro
           .withSpillableMapBasePath(HoodieMemoryConfig.SPILLABLE_MAP_BASE_PATH.defaultValue())
           .withDiskMapType(HoodieCommonConfig.SPILLABLE_DISK_MAP_TYPE.defaultValue())
           .withBitCaskDiskMapCompressionEnabled(HoodieCommonConfig.DISK_MAP_BITCASK_COMPRESSION_ENABLED.defaultValue())
-          .withUseScanV2(Boolean.parseBoolean(HoodieCompactionConfig.USE_LOG_RECORD_READER_SCAN_V2.defaultValue()))
+          .withOptimizedLogBlocksScan(Boolean.parseBoolean(HoodieCompactionConfig.ENABLE_OPTIMIZED_LOG_BLOCKS_SCAN.defaultValue()))
           .withRecordMerger(HoodieRecordUtils.loadRecordMerger(HoodieAvroRecordMerger.class.getName()))
           .build();
       // readAvro log files

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/embedded/EmbeddedTimelineService.java
Patch:
@@ -95,7 +95,9 @@ public void startServer() throws IOException {
           .earlyConflictDetectionCheckCommitConflict(writeConfig.earlyConflictDetectionCheckCommitConflict())
           .asyncConflictDetectorInitialDelayMs(writeConfig.getAsyncConflictDetectorInitialDelayMs())
           .asyncConflictDetectorPeriodMs(writeConfig.getAsyncConflictDetectorPeriodMs())
-          .earlyConflictDetectionMaxAllowableHeartbeatIntervalInMs(writeConfig.getHoodieClientHeartbeatIntervalInMs());
+          .earlyConflictDetectionMaxAllowableHeartbeatIntervalInMs(
+              writeConfig.getHoodieClientHeartbeatIntervalInMs()
+                  * writeConfig.getHoodieClientHeartbeatTolerableMisses());
     }
 
     server = new TimelineService(context, hadoopConf.newCopy(), timelineServiceConfBuilder.build(),

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -582,7 +582,7 @@ public class HoodieWriteConfig extends HoodieConfig {
 
   public static final ConfigProperty<Long> ASYNC_CONFLICT_DETECTOR_INITIAL_DELAY_MS = ConfigProperty
       .key(CONCURRENCY_PREFIX + "async.conflict.detector.initial_delay_ms")
-      .defaultValue(30000L)
+      .defaultValue(0L)
       .sinceVersion("0.13.0")
       .withDocumentation("Used for timeline-server-based markers with "
           + "`AsyncTimelineServerBasedDetectionStrategy`. "

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/execution/FlinkLazyInsertIterable.java
Patch:
@@ -60,7 +60,7 @@ protected List<WriteStatus> computeNext() {
     try {
       final Schema schema = new Schema.Parser().parse(hoodieConfig.getSchema());
       bufferedIteratorExecutor = ExecutorFactory.create(hoodieConfig, inputItr, getExplicitInsertHandler(),
-          getCloningTransformer(schema, hoodieConfig));
+          getTransformer(schema, hoodieConfig));
       final List<WriteStatus> result = bufferedIteratorExecutor.execute();
       checkState(result != null && !result.isEmpty());
       return result;

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/execution/JavaLazyInsertIterable.java
Patch:
@@ -64,7 +64,7 @@ protected List<WriteStatus> computeNext() {
     try {
       final Schema schema = new Schema.Parser().parse(hoodieConfig.getSchema());
       bufferedIteratorExecutor =
-          ExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(), getCloningTransformer(schema));
+          ExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(), getTransformer(schema, hoodieConfig));
       final List<WriteStatus> result = bufferedIteratorExecutor.execute();
       checkState(result != null && !result.isEmpty());
       return result;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/SparkLazyInsertIterable.java
Patch:
@@ -87,7 +87,7 @@ protected List<WriteStatus> computeNext() {
       }
 
       bufferedIteratorExecutor = ExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(),
-          getCloningTransformer(schema, hoodieConfig), hoodieTable.getPreExecuteRunnable());
+          getTransformer(schema, hoodieConfig), hoodieTable.getPreExecuteRunnable());
 
       final List<WriteStatus> result = bufferedIteratorExecutor.execute();
       checkState(result != null && !result.isEmpty());

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/BoundedInMemoryExecutor.java
Patch:
@@ -61,7 +61,7 @@ protected void doConsume(HoodieMessageQueue<I, O> queue, HoodieConsumer<O, E> co
       }
       LOG.info("All records from the queue have been consumed");
     } catch (Exception e) {
-      LOG.error("Error consuming records", e);
+      LOG.error("Failed consuming records", e);
       queue.markAsFailed(e);
       throw new HoodieException(e);
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java
Patch:
@@ -81,7 +81,7 @@ public class HoodieLockConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS = ConfigProperty
       .key(LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP_KEY)
-      .defaultValue(String.valueOf(2000L))
+      .defaultValue(String.valueOf(5000L))
       .sinceVersion("0.8.0")
       .withDocumentation("Amount of time to wait between retries on the lock provider by the lock manager");
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java
Patch:
@@ -62,9 +62,8 @@ public class HoodieIndexUtils {
    * @param hoodieTable Instance of {@link HoodieTable} of interest
    * @return the list of {@link HoodieBaseFile}
    */
-  public static List<HoodieBaseFile> getLatestBaseFilesForPartition(
-      final String partition,
-      final HoodieTable hoodieTable) {
+  public static List<HoodieBaseFile> getLatestBaseFilesForPartition(String partition,
+                                                                    HoodieTable hoodieTable) {
     Option<HoodieInstant> latestCommitTime = hoodieTable.getMetaClient().getCommitsTimeline()
         .filterCompletedInstants().lastInstant();
     if (latestCommitTime.isPresent()) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/BaseHoodieBloomIndexHelper.java
Patch:
@@ -19,12 +19,11 @@
 
 package org.apache.hudi.index.bloom;
 
-import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.data.HoodiePairData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
+import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecordLocation;
-import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
@@ -51,7 +50,7 @@ public abstract class BaseHoodieBloomIndexHelper implements Serializable {
   public abstract HoodiePairData<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(
       HoodieWriteConfig config, HoodieEngineContext context, HoodieTable hoodieTable,
       HoodiePairData<String, String> partitionRecordKeyPairs,
-      HoodieData<Pair<String, HoodieKey>> fileComparisonPairs,
+      HoodiePairData<HoodieFileGroupId, String> fileComparisonPairs,
       Map<String, List<BloomIndexFileInfo>> partitionToFileInfo,
       Map<String, Long> recordsPerPartition);
 }

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroIndexedRecord.java
Patch:
@@ -113,7 +113,7 @@ public HoodieRecord joinWith(HoodieRecord other, Schema targetSchema) {
 
   @Override
   public HoodieRecord rewriteRecord(Schema recordSchema, Properties props, Schema targetSchema) throws IOException {
-    GenericRecord record = HoodieAvroUtils.rewriteRecord((GenericRecord) data, targetSchema);
+    GenericRecord record = HoodieAvroUtils.rewriteRecordWithNewSchema(data, targetSchema);
     return new HoodieAvroIndexedRecord(key, record, operation, metaData);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/MappingIterator.java
Patch:
@@ -21,7 +21,9 @@
 import java.util.Iterator;
 import java.util.function.Function;
 
-// TODO java-docs
+/**
+ * Iterator mapping elements of the provided source {@link Iterator} from {@code I} to {@code O}
+ */
 public class MappingIterator<I, O> implements Iterator<O> {
 
   protected final Iterator<I> source;

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataFileSystemView.java
Patch:
@@ -54,9 +54,8 @@ public HoodieMetadataFileSystemView(HoodieEngineContext engineContext,
                                       HoodieTableMetaClient metaClient,
                                       HoodieTimeline visibleActiveTimeline,
                                       HoodieMetadataConfig metadataConfig) {
-    super(metaClient, visibleActiveTimeline);
-    this.tableMetadata = HoodieTableMetadata.create(engineContext, metadataConfig, metaClient.getBasePath(),
-        FileSystemViewStorageConfig.SPILLABLE_DIR.defaultValue(), true);
+    this(metaClient, visibleActiveTimeline, HoodieTableMetadata.create(engineContext, metadataConfig,
+        metaClient.getBasePath(), FileSystemViewStorageConfig.SPILLABLE_DIR.defaultValue(), true));
   }
 
   /**

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestBase.java
Patch:
@@ -21,8 +21,8 @@
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
-import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.common.HoodieCleanStat;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieAvroRecord;

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/util/SparkDataSourceTableUtils.java
Patch:
@@ -98,8 +98,9 @@ public static Map<String, String> getSparkTableProperties(List<String> partition
 
   public static Map<String, String> getSparkSerdeProperties(boolean readAsOptimized, String basePath) {
     Map<String, String> sparkSerdeProperties = new HashMap<>();
-    sparkSerdeProperties.put("path", basePath);
+    sparkSerdeProperties.put(ConfigUtils.TABLE_SERDE_PATH, basePath);
     sparkSerdeProperties.put(ConfigUtils.IS_QUERY_AS_RO_TABLE, String.valueOf(readAsOptimized));
     return sparkSerdeProperties;
   }
+
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieWriteHelper.java
Patch:
@@ -35,7 +35,9 @@
 
 public class HoodieWriteHelper<T, R> extends BaseWriteHelper<T, HoodieData<HoodieRecord<T>>,
     HoodieData<HoodieKey>, HoodieData<WriteStatus>, R> {
+
   private HoodieWriteHelper() {
+    super(HoodieData::getNumPartitions);
   }
 
   private static class WriteHelperHolder {
@@ -79,5 +81,4 @@ protected HoodieData<HoodieRecord<T>> doDeduplicateRecords(
       return reducedRecord.newInstance(reducedKey);
     }, reduceParallelism).map(Pair::getRight);
   }
-
 }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeleteHelper.java
Patch:
@@ -50,6 +50,7 @@ public class FlinkDeleteHelper<R> extends
     BaseDeleteHelper<EmptyHoodieRecordPayload, List<HoodieRecord<EmptyHoodieRecordPayload>>, List<HoodieKey>, List<WriteStatus>, R> {
 
   private FlinkDeleteHelper() {
+    super(ignored -> -1);
   }
 
   private static class DeleteHelperHolder {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkWriteHelper.java
Patch:
@@ -55,6 +55,7 @@ public class FlinkWriteHelper<T, R> extends BaseWriteHelper<T, List<HoodieRecord
     List<HoodieKey>, List<WriteStatus>, R> {
 
   private FlinkWriteHelper() {
+    super(ignored -> -1);
   }
 
   private static class WriteHelperHolder {
@@ -67,7 +68,7 @@ public static FlinkWriteHelper newInstance() {
 
   @Override
   public HoodieWriteMetadata<List<WriteStatus>> write(String instantTime, List<HoodieRecord<T>> inputRecords, HoodieEngineContext context,
-                                                      HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table, boolean shouldCombine, int shuffleParallelism,
+                                                      HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table, boolean shouldCombine, int configuredShuffleParallelism,
                                                       BaseCommitActionExecutor<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>, R> executor, WriteOperationType operationType) {
     try {
       Instant lookupBegin = Instant.now();

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaDeleteHelper.java
Patch:
@@ -47,6 +47,7 @@ public class JavaDeleteHelper<R> extends
     BaseDeleteHelper<EmptyHoodieRecordPayload, List<HoodieRecord<EmptyHoodieRecordPayload>>, List<HoodieKey>, List<WriteStatus>, R> {
 
   private JavaDeleteHelper() {
+    super(ignored -> -1);
   }
 
   private static class DeleteHelperHolder {

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaWriteHelper.java
Patch:
@@ -41,6 +41,7 @@ public class JavaWriteHelper<T,R> extends BaseWriteHelper<T, List<HoodieRecord<T
     List<HoodieKey>, List<WriteStatus>, R> {
 
   private JavaWriteHelper() {
+    super(ignored -> -1);
   }
 
   private static class WriteHelperHolder {

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestComplexKeyGenerator.java
Patch:
@@ -77,8 +77,7 @@ public void testNullPartitionPathFields() {
 
   @Test
   public void testNullRecordKeyFields() {
-    ComplexKeyGenerator keyGenerator = new ComplexKeyGenerator(getPropertiesWithoutRecordKeyProp());
-    Assertions.assertThrows(IllegalArgumentException.class, () -> keyGenerator.getRecordKey(getRecord()));
+    Assertions.assertThrows(IllegalArgumentException.class, () -> new ComplexKeyGenerator(getPropertiesWithoutRecordKeyProp()));
   }
 
   @Test

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestSimpleKeyGenerator.java
Patch:
@@ -96,8 +96,7 @@ public void testNullPartitionPathFields() {
 
   @Test
   public void testNullRecordKeyFields() {
-    SimpleKeyGenerator keyGenerator = new SimpleKeyGenerator(getPropertiesWithoutRecordKeyProp());
-    assertThrows(IllegalArgumentException.class, () -> keyGenerator.getRecordKey(getRecord()));
+    assertThrows(IllegalArgumentException.class, () -> new SimpleKeyGenerator(getPropertiesWithoutRecordKeyProp()));
   }
 
   @Test

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedTableMetadata.java
Patch:
@@ -74,11 +74,10 @@
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.stream.Collectors;
 
-import static org.apache.hudi.common.model.WriteOperationType.INSERT;
-import static org.apache.hudi.common.model.WriteOperationType.UPSERT;
-
 import static java.util.Arrays.asList;
 import static java.util.Collections.emptyList;
+import static org.apache.hudi.common.model.WriteOperationType.INSERT;
+import static org.apache.hudi.common.model.WriteOperationType.UPSERT;
 import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;

File: hudi-common/src/main/java/org/apache/hudi/BaseHoodieTableFileIndex.java
Patch:
@@ -278,7 +278,7 @@ private Map<PartitionPath, List<FileSlice>> loadFileSlicesForPartitions(List<Par
   protected List<PartitionPath> listPartitionPaths(List<String> relativePartitionPaths) {
     List<String> matchedPartitionPaths;
     try {
-      matchedPartitionPaths = tableMetadata.getPartitionPathsWithPrefixes(relativePartitionPaths);
+      matchedPartitionPaths = tableMetadata.getPartitionPathWithPathPrefixes(relativePartitionPaths);
     } catch (IOException e) {
       throw new HoodieIOException("Error fetching partition paths", e);
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -264,12 +264,12 @@ private HoodieWriteConfig createMetadataWriteConfig(HoodieWriteConfig writeConfi
         .forTable(tableName)
         // we will trigger cleaning manually, to control the instant times
         .withCleanConfig(HoodieCleanConfig.newBuilder()
-            .withAsyncClean(writeConfig.isMetadataAsyncClean())
+            .withAsyncClean(HoodieMetadataConfig.ASYNC_CLEAN_ENABLE.defaultValue())
             .withAutoClean(false)
             .withCleanerParallelism(parallelism)
             .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS)
             .withFailedWritesCleaningPolicy(HoodieFailedWritesCleaningPolicy.EAGER)
-            .retainCommits(writeConfig.getMetadataCleanerCommitsRetained())
+            .retainCommits(HoodieMetadataConfig.CLEANER_COMMITS_RETAINED.defaultValue())
             .build())
         // we will trigger archive manually, to ensure only regular writer invokes it
         .withArchivalConfig(HoodieArchivalConfig.newBuilder()
@@ -291,7 +291,7 @@ private HoodieWriteConfig createMetadataWriteConfig(HoodieWriteConfig writeConfi
         .withFinalizeWriteParallelism(parallelism)
         .withAllowMultiWriteOnSameInstant(true)
         .withKeyGenerator(HoodieTableMetadataKeyGenerator.class.getCanonicalName())
-        .withPopulateMetaFields(dataWriteConfig.getMetadataConfig().populateMetaFields());
+        .withPopulateMetaFields(HoodieMetadataConfig.POPULATE_META_FIELDS.defaultValue());
 
     // RecordKey properties are needed for the metadata table records
     final Properties properties = new Properties();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/TestHoodieTimelineArchiver.java
Patch:
@@ -1296,7 +1296,7 @@ public void testArchivalAndCompactionInMetadataTable() throws Exception {
             .withRemoteServerPort(timelineServicePort).build())
         .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true)
             .withMaxNumDeltaCommitsBeforeCompaction(8)
-            .retainCommits(1).archiveCommitsWith(2, 4).build())
+            .retainCommits(3).archiveCommitsWith(4, 5).build())
         .forTable("test-trip-table").build();
     initWriteConfigAndMetatableWriter(writeConfig, true);
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -536,7 +536,7 @@ public Pair<HoodieMetadataLogRecordReader, Long> getLogRecordScanner(List<Hoodie
   private boolean isFullScanAllowedForPartition(String partitionName) {
     switch (partitionName) {
       case PARTITION_NAME_FILES:
-        return metadataConfig.allowFullScan();
+        return HoodieMetadataConfig.ENABLE_FULL_SCAN_LOG_FILES.defaultValue();
 
       case PARTITION_NAME_COLUMN_STATS:
       case PARTITION_NAME_BLOOM_FILTERS:

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestComplexKeyGenerator.java
Patch:
@@ -77,7 +77,8 @@ public void testNullPartitionPathFields() {
 
   @Test
   public void testNullRecordKeyFields() {
-    Assertions.assertThrows(IllegalArgumentException.class, () -> new ComplexKeyGenerator(getPropertiesWithoutRecordKeyProp()));
+    ComplexKeyGenerator keyGenerator = new ComplexKeyGenerator(getPropertiesWithoutRecordKeyProp());
+    Assertions.assertThrows(IllegalArgumentException.class, () -> keyGenerator.getRecordKey(getRecord()));
   }
 
   @Test

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestSimpleKeyGenerator.java
Patch:
@@ -96,7 +96,8 @@ public void testNullPartitionPathFields() {
 
   @Test
   public void testNullRecordKeyFields() {
-    assertThrows(IllegalArgumentException.class, () -> new SimpleKeyGenerator(getPropertiesWithoutRecordKeyProp()));
+    SimpleKeyGenerator keyGenerator = new SimpleKeyGenerator(getPropertiesWithoutRecordKeyProp());
+    assertThrows(IllegalArgumentException.class, () -> keyGenerator.getRecordKey(getRecord()));
   }
 
   @Test

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java
Patch:
@@ -318,7 +318,7 @@ public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context,
   }
 
   @Override
-  public HoodieCleanMetadata clean(HoodieEngineContext context, String cleanInstantTime, boolean skipLocking) {
+  public HoodieCleanMetadata clean(HoodieEngineContext context, String cleanInstantTime) {
     return new CleanActionExecutor(context, config, this, cleanInstantTime).execute();
   }
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java
Patch:
@@ -206,7 +206,8 @@ protected List<WriteStatus> postWrite(HoodieWriteMetadata<List<WriteStatus>> res
             result.getWriteStats().get().size());
       }
 
-      postCommit(hoodieTable, result.getCommitMetadata().get(), instantTime, Option.empty(), true);
+      postCommit(hoodieTable, result.getCommitMetadata().get(), instantTime, Option.empty());
+      mayBeCleanAndArchive(hoodieTable);
 
       emitCommitMetrics(instantTime, result.getCommitMetadata().get(), hoodieTable.getMetaClient().getCommitActionType());
     }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java
Patch:
@@ -216,7 +216,7 @@ public Option<HoodieCleanerPlan> scheduleCleaning(HoodieEngineContext context, S
 
   @Override
   public HoodieCleanMetadata clean(HoodieEngineContext context,
-                                   String cleanInstantTime, boolean skipLocking) {
+                                   String cleanInstantTime) {
     return new CleanActionExecutor(context, config, this, cleanInstantTime).execute();
   }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java
Patch:
@@ -267,7 +267,8 @@ protected JavaRDD<WriteStatus> postWrite(HoodieWriteMetadata<JavaRDD<WriteStatus
             result.getWriteStats().get().size());
       }
 
-      postCommit(hoodieTable, result.getCommitMetadata().get(), instantTime, Option.empty(), true);
+      postCommit(hoodieTable, result.getCommitMetadata().get(), instantTime, Option.empty());
+      mayBeCleanAndArchive(hoodieTable);
 
       emitCommitMetrics(instantTime, result.getCommitMetadata().get(), hoodieTable.getMetaClient().getCommitActionType());
     }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java
Patch:
@@ -259,8 +259,8 @@ public Iterator<List<WriteStatus>> handleInsert(
   }
 
   @Override
-  public HoodieCleanMetadata clean(HoodieEngineContext context, String cleanInstantTime, boolean skipLocking) {
-    return new CleanActionExecutor<>(context, config, this, cleanInstantTime, skipLocking).execute();
+  public HoodieCleanMetadata clean(HoodieEngineContext context, String cleanInstantTime) {
+    return new CleanActionExecutor<>(context, config, this, cleanInstantTime, false).execute();
   }
 
   @Override

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/deltastreamer/TestHoodieDeltaStreamer.java
Patch:
@@ -984,7 +984,7 @@ public void testDeltaSyncWithPendingClustering() throws Exception {
   }
 
   @ParameterizedTest
-  @CsvSource(value = {"false, AVRO", "false, SPARK"}) // TODO set asyncClean to true; disabled due to lock acquiring issue (HUDI-5593)
+  @CsvSource(value = {"true, AVRO", "true, SPARK", "false, AVRO", "false, SPARK"})
   public void testCleanerDeleteReplacedDataWithArchive(Boolean asyncClean, HoodieRecordType recordType) throws Exception {
     String tableBasePath = basePath + "/cleanerDeleteReplacedDataWithArchive" + asyncClean;
 
@@ -1052,7 +1052,7 @@ public void testCleanerDeleteReplacedDataWithArchive(Boolean asyncClean, HoodieR
     configs.add(String.format("%s=%s", HoodieArchivalConfig.MAX_COMMITS_TO_KEEP.key(), "3"));
     configs.add(String.format("%s=%s", HoodieCleanConfig.ASYNC_CLEAN.key(), asyncClean));
     configs.add(String.format("%s=%s", HoodieMetadataConfig.COMPACT_NUM_DELTA_COMMITS.key(), "1"));
-    cfg.configs.add(String.format("%s=%s", HoodieWriteConfig.MARKERS_TYPE.key(), "DIRECT"));
+    configs.add(String.format("%s=%s", HoodieWriteConfig.MARKERS_TYPE.key(), "DIRECT"));
     if (asyncClean) {
       configs.add(String.format("%s=%s", HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(),
           WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.name()));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieIOHandle.java
Patch:
@@ -39,5 +39,5 @@ public abstract class HoodieIOHandle<T, I, K, O> {
     this.fs = getFileSystem();
   }
 
-  protected abstract FileSystem getFileSystem();
+  public abstract FileSystem getFileSystem();
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieReadHandle.java
Patch:
@@ -44,7 +44,7 @@ public HoodieReadHandle(HoodieWriteConfig config, HoodieTable<T, I, K, O> hoodie
   }
 
   @Override
-  protected FileSystem getFileSystem() {
+  public FileSystem getFileSystem() {
     return hoodieTable.getMetaClient().getFs();
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/marker/MarkerOperation.java
Patch:
@@ -29,6 +29,7 @@ public class MarkerOperation implements Serializable {
 
   public static final String MARKER_DIR_PATH_PARAM = "markerdirpath";
   public static final String MARKER_NAME_PARAM = "markername";
+  public static final String MARKER_BASEPATH_PARAM = "basepath";
 
   // GET requests
   public static final String ALL_MARKERS_URL = String.format("%s/%s", BASE_URL, "all");

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/RequestHandler.java
Patch:
@@ -459,7 +459,8 @@ private void registerMarkerAPI() {
       ctx.future(markerHandler.createMarker(
           ctx,
           ctx.queryParamAsClass(MarkerOperation.MARKER_DIR_PATH_PARAM, String.class).getOrDefault(""),
-          ctx.queryParamAsClass(MarkerOperation.MARKER_NAME_PARAM, String.class).getOrDefault("")));
+          ctx.queryParamAsClass(MarkerOperation.MARKER_NAME_PARAM, String.class).getOrDefault(""),
+          ctx.queryParamAsClass(MarkerOperation.MARKER_BASEPATH_PARAM, String.class).getOrDefault("")));
     }, false));
 
     app.post(MarkerOperation.DELETE_MARKER_DIR_URL, new ViewHandler(ctx -> {

File: hudi-cli/src/main/java/org/apache/hudi/cli/Main.java
Patch:
@@ -26,7 +26,7 @@
 /**
  * Main class that delegates to Spring Shell's Bootstrap class in order to simplify debugging inside an IDE.
  */
-@SpringBootApplication
+@SpringBootApplication(exclude = {org.springframework.boot.autoconfigure.gson.GsonAutoConfiguration.class})
 public class Main {
 
   public static void main(String[] args) throws IOException {

File: hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkTempViewProvider.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.cli.utils;
 
+import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.Logger;
@@ -43,8 +44,8 @@ public class SparkTempViewProvider implements TempViewProvider {
 
   public SparkTempViewProvider(String appName) {
     try {
-      SparkConf sparkConf = new SparkConf().setAppName(appName)
-              .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer").setMaster("local[8]");
+      SparkConf sparkConf = SparkUtil.getDefaultConf(appName, Option.of("local[8]"));
+
       jsc = new JavaSparkContext(sparkConf);
       sqlContext = new SQLContext(jsc);
     } catch (Throwable ex) {

File: hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
 
-import org.apache.spark.HoodieSparkKryoProvider$;
 import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.launcher.SparkLauncher;
@@ -92,6 +91,7 @@ public static SparkConf getDefaultConf(final String appName, final Option<String
     sparkConf.set(HoodieCliSparkConfig.CLI_EVENT_LOG_OVERWRITE, "true");
     sparkConf.set(HoodieCliSparkConfig.CLI_EVENT_LOG_ENABLED, "false");
     sparkConf.set(HoodieCliSparkConfig.CLI_SERIALIZER, "org.apache.spark.serializer.KryoSerializer");
+    sparkConf.set("spark.kryo.registrator", "org.apache.spark.HoodieSparkKryoRegistrar");
 
     // Configure hadoop conf
     sparkConf.set(HoodieCliSparkConfig.CLI_MAPRED_OUTPUT_COMPRESS, "true");
@@ -116,7 +116,6 @@ public static JavaSparkContext initJavaSparkContext(String name, Option<String>
   }
 
   public static JavaSparkContext initJavaSparkContext(SparkConf sparkConf) {
-    HoodieSparkKryoProvider$.MODULE$.register(sparkConf);
     JavaSparkContext jsc = new JavaSparkContext(sparkConf);
     jsc.hadoopConfiguration().setBoolean(HoodieCliSparkConfig.CLI_PARQUET_ENABLE_SUMMARY_METADATA, false);
     FSUtils.prepareHadoopConf(jsc.hadoopConfiguration());

File: hudi-cli/src/test/java/org/apache/hudi/cli/functional/CLIFunctionalTestHarness.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hudi.timeline.service.TimelineService;
 
 import org.apache.hadoop.conf.Configuration;
-import org.apache.spark.HoodieSparkKryoProvider$;
+import org.apache.spark.HoodieSparkKryoRegistrar$;
 import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.SQLContext;
@@ -100,7 +100,7 @@ public synchronized void runBeforeEach() {
     initialized = spark != null;
     if (!initialized) {
       SparkConf sparkConf = conf();
-      HoodieSparkKryoProvider$.MODULE$.register(sparkConf);
+      HoodieSparkKryoRegistrar$.MODULE$.register(sparkConf);
       SparkRDDReadClient.addHoodieSupport(sparkConf);
       spark = SparkSession.builder().config(sparkConf).getOrCreate();
       sqlContext = spark.sqlContext();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/FunctionalTestHarness.java
Patch:
@@ -38,7 +38,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.spark.HoodieSparkKryoProvider$;
+import org.apache.spark.HoodieSparkKryoRegistrar$;
 import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.SQLContext;
@@ -139,7 +139,7 @@ public synchronized void runBeforeEach() throws Exception {
     initialized = spark != null && hdfsTestService != null;
     if (!initialized) {
       SparkConf sparkConf = conf();
-      HoodieSparkKryoProvider$.MODULE$.register(sparkConf);
+      HoodieSparkKryoRegistrar$.MODULE$.register(sparkConf);
       SparkRDDReadClient.addHoodieSupport(sparkConf);
       spark = SparkSession.builder().config(sparkConf).getOrCreate();
       sqlContext = spark.sqlContext();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/SparkClientFunctionalTestHarness.java
Patch:
@@ -60,7 +60,7 @@
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.spark.HoodieSparkKryoProvider$;
+import org.apache.spark.HoodieSparkKryoRegistrar$;
 import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
@@ -186,7 +186,7 @@ public synchronized void runBeforeEach() {
     initialized = spark != null;
     if (!initialized) {
       SparkConf sparkConf = conf();
-      HoodieSparkKryoProvider$.MODULE$.register(sparkConf);
+      HoodieSparkKryoRegistrar$.MODULE$.register(sparkConf);
       SparkRDDReadClient.addHoodieSupport(sparkConf);
       spark = SparkSession.builder().config(sparkConf).getOrCreate();
       sqlContext = spark.sqlContext();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/providers/SparkProvider.java
Patch:
@@ -47,6 +47,7 @@ default SparkConf conf(Map<String, String> overwritingConfigs) {
     sparkConf.set("spark.hadoop.mapred.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec");
     sparkConf.set("spark.hadoop.mapred.output.compression.type", "BLOCK");
     sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
+    sparkConf.set("spark.kryo.registrator", "org.apache.spark.HoodieSparkKryoRegistrar");
     overwritingConfigs.forEach(sparkConf::set);
     return sparkConf;
   }

File: hudi-common/src/main/java/org/apache/hudi/common/config/SerializableConfiguration.java
Patch:
@@ -31,6 +31,7 @@
 public class SerializableConfiguration implements Serializable {
 
   private static final long serialVersionUID = 1L;
+
   private transient Configuration configuration;
 
   public SerializableConfiguration(Configuration configuration) {

File: hudi-common/src/main/java/org/apache/hudi/common/util/SerializationUtils.java
Patch:
@@ -28,7 +28,6 @@
 import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.io.Serializable;
-import java.util.Arrays;
 
 /**
  * {@link SerializationUtils} class internally uses {@link Kryo} serializer for serializing / deserializing objects.
@@ -119,8 +118,7 @@ public Kryo newKryo() {
       kryo.setClassLoader(Thread.currentThread().getContextClassLoader());
 
       // Register Hudi's classes
-      Arrays.stream(new HoodieCommonKryoProvider().registerClasses())
-          .forEach(kryo::register);
+      new HoodieCommonKryoRegistrar().registerClasses(kryo);
 
       // Register serializers
       kryo.register(Utf8.class, new AvroUtf8Serializer());

File: hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/common/HoodieExampleSparkUtils.java
Patch:
@@ -32,8 +32,9 @@ public class HoodieExampleSparkUtils {
   private static Map<String, String> defaultConf() {
     Map<String, String> additionalConfigs = new HashMap<>();
     additionalConfigs.put("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
-    additionalConfigs.put("spark.kryoserializer.buffer.max", "512m");
+    additionalConfigs.put("spark.kryo.registrator", "org.apache.spark.HoodieSparkKryoRegistrar");
     additionalConfigs.put("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension");
+    additionalConfigs.put("spark.kryoserializer.buffer.max", "512m");
     return additionalConfigs;
   }
 

File: hudi-examples/hudi-examples-spark/src/test/java/org/apache/hudi/examples/quickstart/TestHoodieSparkQuickstart.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.testutils.providers.SparkProvider;
 
-import org.apache.spark.HoodieSparkKryoProvider$;
+import org.apache.spark.HoodieSparkKryoRegistrar$;
 import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.SQLContext;
@@ -84,7 +84,7 @@ public synchronized void runBeforeEach() {
     initialized = spark != null;
     if (!initialized) {
       SparkConf sparkConf = conf();
-      HoodieSparkKryoProvider$.MODULE$.register(sparkConf);
+      HoodieSparkKryoRegistrar$.MODULE$.register(sparkConf);
       SparkRDDReadClient.addHoodieSupport(sparkConf);
       spark = SparkSession.builder().config(sparkConf).getOrCreate();
       sqlContext = spark.sqlContext();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java
Patch:
@@ -52,6 +52,8 @@
 
 import scala.Tuple2;
 
+import static org.apache.hudi.utilities.UtilHelpers.buildSparkConf;
+
 /**
  * Hoodie snapshot copy job which copies latest files from all partitions to another place, for snapshot backup.
  *
@@ -183,8 +185,7 @@ public static void main(String[] args) throws IOException {
         cfg.outputPath));
 
     // Create a spark job to do the snapshot copy
-    SparkConf sparkConf = new SparkConf().setAppName("Hoodie-snapshot-copier");
-    sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
+    SparkConf sparkConf = buildSparkConf("Hoodie-snapshot-copier", "local[*]");
     JavaSparkContext jsc = new JavaSparkContext(sparkConf);
     LOG.info("Initializing spark job.");
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java
Patch:
@@ -66,6 +66,8 @@
 
 import scala.collection.JavaConversions;
 
+import static org.apache.hudi.utilities.UtilHelpers.buildSparkConf;
+
 /**
  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).
  */
@@ -282,8 +284,7 @@ public static void main(String[] args) throws IOException {
     final Config cfg = new Config();
     new JCommander(cfg, null, args);
 
-    SparkConf sparkConf = new SparkConf().setAppName("Hoodie-snapshot-exporter");
-    sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
+    SparkConf sparkConf = buildSparkConf("Hoodie-snapshot-exporter", "local[*]");
     JavaSparkContext jsc = new JavaSparkContext(sparkConf);
     LOG.info("Initializing spark job.");
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieRepairTool.java
Patch:
@@ -35,7 +35,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
-import org.apache.spark.HoodieSparkKryoProvider$;
+import org.apache.spark.HoodieSparkKryoRegistrar$;
 import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.SQLContext;
@@ -93,7 +93,7 @@ public void initWithCleanState() throws IOException {
     boolean initialized = spark != null;
     if (!initialized) {
       SparkConf sparkConf = conf();
-      HoodieSparkKryoProvider$.MODULE$.register(sparkConf);
+      HoodieSparkKryoRegistrar$.MODULE$.register(sparkConf);
       SparkRDDReadClient.addHoodieSupport(sparkConf);
       spark = SparkSession.builder().config(sparkConf).getOrCreate();
       sqlContext = spark.sqlContext();

File: hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroParquetReader.java
Patch:
@@ -32,9 +32,9 @@
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
-import org.apache.parquet.avro.AvroParquetReader;
 import org.apache.parquet.avro.AvroReadSupport;
 import org.apache.parquet.avro.AvroSchemaConverter;
+import org.apache.parquet.avro.HoodieAvroParquetReaderBuilder;
 import org.apache.parquet.hadoop.ParquetInputFormat;
 import org.apache.parquet.hadoop.ParquetReader;
 
@@ -165,7 +165,7 @@ private ClosableIterator<IndexedRecord> getIndexedRecordIteratorInternal(Schema
       AvroReadSupport.setAvroReadSchema(conf, requestedSchema.get());
       AvroReadSupport.setRequestedProjection(conf, requestedSchema.get());
     }
-    ParquetReader<IndexedRecord> reader = AvroParquetReader.<IndexedRecord>builder(path).withConf(conf).build();
+    ParquetReader<IndexedRecord> reader = new HoodieAvroParquetReaderBuilder<IndexedRecord>(path).withConf(conf).build();
     ParquetReaderIterator<IndexedRecord> parquetReaderIterator = new ParquetReaderIterator<>(reader);
     readerIterators.add(parquetReaderIterator);
     return parquetReaderIterator;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java
Patch:
@@ -667,6 +667,7 @@ protected Map<String, Option<HoodiePendingRollbackInfo>> getPendingRollbackInfos
 
   /**
    * Rollback all failed writes.
+   * @return true if rollback was triggered. false otherwise.
    */
   protected Boolean rollbackFailedWrites() {
     return rollbackFailedWrites(false);
@@ -675,14 +676,15 @@ protected Boolean rollbackFailedWrites() {
   /**
    * Rollback all failed writes.
    * @param skipLocking if this is triggered by another parent transaction, locking can be skipped.
+   * @return true if rollback was triggered. false otherwise.
    */
   protected Boolean rollbackFailedWrites(boolean skipLocking) {
     HoodieTable table = createTable(config, hadoopConf);
     List<String> instantsToRollback = getInstantsToRollback(table.getMetaClient(), config.getFailedWritesCleanPolicy(), Option.empty());
     Map<String, Option<HoodiePendingRollbackInfo>> pendingRollbacks = getPendingRollbackInfos(table.getMetaClient());
     instantsToRollback.forEach(entry -> pendingRollbacks.putIfAbsent(entry, Option.empty()));
     rollbackFailedWrites(pendingRollbacks, skipLocking);
-    return true;
+    return !pendingRollbacks.isEmpty();
   }
 
   protected void rollbackFailedWrites(Map<String, Option<HoodiePendingRollbackInfo>> instantsToRollback, boolean skipLocking) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java
Patch:
@@ -36,6 +36,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.exception.HoodieRollbackException;
+import org.apache.hudi.metadata.HoodieTableMetadata;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.BaseActionExecutor;
 import org.apache.hudi.table.marker.WriteMarkersFactory;
@@ -155,12 +156,13 @@ private void validateRollbackCommitSequence() {
     // since with LAZY rollback we support parallel writing which can allow a new inflight while rollback is ongoing
     // Remove this once we support LAZY rollback of failed writes by default as parallel writing becomes the default
     // writer mode.
-    if (config.getFailedWritesCleanPolicy().isEager()) {
+    if (config.getFailedWritesCleanPolicy().isEager()  && !HoodieTableMetadata.isMetadataTable(config.getBasePath())) {
       final String instantTimeToRollback = instantToRollback.getTimestamp();
       HoodieTimeline commitTimeline = table.getCompletedCommitsTimeline();
       HoodieTimeline inflightAndRequestedCommitTimeline = table.getPendingCommitTimeline();
       // Make sure only the last n commits are being rolled back
       // If there is a commit in-between or after that is not rolled back, then abort
+      // this condition may not hold good for metadata table. since the order of commits applied to MDT is data table commits and the ordering could be different.
       if ((instantTimeToRollback != null) && !commitTimeline.empty()
           && !commitTimeline.findInstantsAfter(instantTimeToRollback, Integer.MAX_VALUE).empty()) {
         // check if remnants are from a previous LAZY rollback config, if yes, let out of order rollback continue

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java
Patch:
@@ -80,7 +80,7 @@ public class FSUtils {
   // Log files are of this pattern - .b5068208-e1a4-11e6-bf01-fe55135034f3_20170101134598.log.1_1-0-1
   // Archive log files are of this pattern - .commits_.archive.1_1-0-1
   public static final Pattern LOG_FILE_PATTERN =
-      Pattern.compile("\\.(.+)_(.*)\\.(.+)\\.(\\d+)(_((\\d+)-(\\d+)-(\\d+))(.cdc)?)?");
+      Pattern.compile("^\\.(.+)_(.*)\\.(log|archive)\\.(\\d+)(_((\\d+)-(\\d+)-(\\d+))(.cdc)?)?");
   private static final int MAX_ATTEMPTS_RECOVER_LEASE = 10;
   private static final long MIN_CLEAN_TO_KEEP = 10;
   private static final long MIN_ROLLBACK_TO_KEEP = 10;

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -345,7 +345,7 @@ public void testAppendNotSupported(@TempDir java.nio.file.Path tempDir) throws I
 
     for (int i = 0; i < 2; i++) {
       Writer writer = HoodieLogFormat.newWriterBuilder().onParentPath(testPath)
-          .withFileExtension(HoodieArchivedLogFile.ARCHIVE_EXTENSION).withFileId("commits.archive").overBaseCommit("")
+          .withFileExtension(HoodieArchivedLogFile.ARCHIVE_EXTENSION).withFileId("commits").overBaseCommit("")
           .withFs(localFs).build();
       writer.appendBlock(dataBlock);
       writer.close();

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormatAppendFailure.java
Patch:
@@ -103,7 +103,7 @@ public void testFailedToGetAppendStreamFromHDFSNameNode()
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header, HoodieRecord.RECORD_KEY_METADATA_FIELD);
 
     Writer writer = HoodieLogFormat.newWriterBuilder().onParentPath(testPath)
-        .withFileExtension(HoodieArchivedLogFile.ARCHIVE_EXTENSION).withFileId("commits.archive")
+        .withFileExtension(HoodieArchivedLogFile.ARCHIVE_EXTENSION).withFileId("commits")
         .overBaseCommit("").withFs(fs).build();
 
     writer.appendBlock(dataBlock);
@@ -134,7 +134,7 @@ public void testFailedToGetAppendStreamFromHDFSNameNode()
     // Opening a new Writer right now will throw IOException. The code should handle this, rollover the logfile and
     // return a new writer with a bumped up logVersion
     writer = HoodieLogFormat.newWriterBuilder().onParentPath(testPath)
-        .withFileExtension(HoodieArchivedLogFile.ARCHIVE_EXTENSION).withFileId("commits.archive")
+        .withFileExtension(HoodieArchivedLogFile.ARCHIVE_EXTENSION).withFileId("commits")
         .overBaseCommit("").withFs(fs).build();
     header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestHoodieTableFileSystemView.java
Patch:
@@ -306,12 +306,14 @@ protected void testInvalidLogFiles() throws Exception {
     String fileName2 =
         FSUtils.makeLogFileName(fileId, HoodieLogFile.DELTA_EXTENSION, instantTime1, 1, TEST_WRITE_TOKEN);
     // create a dummy log file mimicing cloud stores marker files
-    String fileName3 = "_DUMMY_" + fileName1.substring(1, fileName1.length());
+    String fileName3 = "_GCS_SYNCABLE_TEMPFILE_" + fileName1;
+    String fileName4 = "_DUMMY_" + fileName1.substring(1, fileName1.length());
     // this file should not be deduced as a log file.
 
     Paths.get(basePath, partitionPath, fileName1).toFile().createNewFile();
     Paths.get(basePath, partitionPath, fileName2).toFile().createNewFile();
     Paths.get(basePath, partitionPath, fileName3).toFile().createNewFile();
+    Paths.get(basePath, partitionPath, fileName4).toFile().createNewFile();
     HoodieActiveTimeline commitTimeline = metaClient.getActiveTimeline();
 
     HoodieInstant instant1 = new HoodieInstant(true, HoodieTimeline.COMMIT_ACTION, instantTime1);

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestHoodieLogFileCommand.java
Patch:
@@ -194,7 +194,8 @@ public void testShowLogFileRecordsWithMerge() throws IOException, InterruptedExc
               .withFileExtension(HoodieLogFile.DELTA_EXTENSION)
               .withFileId("test-log-fileid1").overBaseCommit(INSTANT_TIME).withFs(fs).withSizeThreshold(500).build();
 
-      List<HoodieRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100).stream().map(HoodieAvroIndexedRecord::new).collect(Collectors.toList());
+      SchemaTestUtil testUtil = new SchemaTestUtil();
+      List<HoodieRecord> records1 = testUtil.generateHoodieTestRecords(0, 100).stream().map(HoodieAvroIndexedRecord::new).collect(Collectors.toList());
       Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
       header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, INSTANT_TIME);
       header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java
Patch:
@@ -86,8 +86,9 @@ public void init() throws Exception {
     Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());
 
     // generate 200 records
-    HoodieRecord[] hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema).toArray(new HoodieRecord[100]);
-    HoodieRecord[] hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema).toArray(new HoodieRecord[100]);
+    SchemaTestUtil testUtil = new SchemaTestUtil();
+    HoodieRecord[] hoodieRecords1 = testUtil.generateHoodieTestRecords(0, 100, schema).toArray(new HoodieRecord[100]);
+    HoodieRecord[] hoodieRecords2 = testUtil.generateHoodieTestRecords(100, 100, schema).toArray(new HoodieRecord[100]);
 
     // generate duplicates
     HoodieRecord[] dupRecords = Arrays.copyOf(hoodieRecords1, 10);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java
Patch:
@@ -43,7 +43,7 @@ public static HoodieFileSliceReader getFileSliceReader(
       Iterator<HoodieRecord> baseIterator = baseFileReader.get().getRecordIterator(schema);
       while (baseIterator.hasNext()) {
         scanner.processNextRecord(baseIterator.next().wrapIntoHoodieRecordPayloadWithParams(schema, props,
-            simpleKeyGenFieldsOpt, scanner.isWithOperationField(), scanner.getPartitionName(), false));
+            simpleKeyGenFieldsOpt, scanner.isWithOperationField(), scanner.getPartitionNameOverride(), false));
       }
     }
     return new HoodieFileSliceReader(scanner.iterator());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -410,10 +410,10 @@ public List<WriteStatus> close() {
 
       if (keyToNewRecords instanceof ExternalSpillableMap) {
         ((ExternalSpillableMap) keyToNewRecords).close();
-      } else {
-        keyToNewRecords.clear();
       }
-      writtenRecordKeys.clear();
+
+      keyToNewRecords = null;
+      writtenRecordKeys = null;
 
       if (fileWriter != null) {
         fileWriter.close();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java
Patch:
@@ -117,13 +117,14 @@ public List<WriteStatus> close() {
             writeRecord(hoodieRecord, Option.of(hoodieRecord), writeSchema, config.getProps());
           }
           insertRecordsWritten++;
+          writtenRecordKeys.add(hoodieRecord.getRecordKey());
         }
       } catch (IOException e) {
         throw new HoodieUpsertException("Failed to close UpdateHandle", e);
       }
     }
+
     newRecordKeysSorted.clear();
-    keyToNewRecords.clear();
 
     return super.close();
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/plan/generators/HoodieLogCompactionPlanGenerator.java
Patch:
@@ -29,7 +29,6 @@
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.log.HoodieUnMergedLogRecordScanner;
 import org.apache.hudi.common.util.CompactionUtils;
-import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.compact.LogCompactionExecutionHelper;
@@ -94,7 +93,7 @@ private boolean isFileSliceEligibleForLogCompaction(FileSlice fileSlice, String
         .withUseScanV2(true)
         .withRecordMerger(writeConfig.getRecordMerger())
         .build();
-    scanner.scanInternal(Option.empty(), true);
+    scanner.scan(true);
     int totalBlocks = scanner.getCurrentInstantLogBlocks().size();
     LOG.info("Total blocks seen are " + totalBlocks);
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnMergeOnReadStorage.java
Patch:
@@ -447,7 +447,7 @@ private void validateBlockInstantsBeforeAndAfterRollback(HoodieWriteConfig confi
             .withUseScanV2(true)
             .withRecordMerger(HoodieRecordUtils.loadRecordMerger(HoodieAvroRecordMerger.class.getName()))
             .build();
-        scanner.scanInternal(Option.empty(), true);
+        scanner.scan(true);
         List<String> prevInstants = scanner.getValidBlockInstants();
         HoodieUnMergedLogRecordScanner scanner2 = HoodieUnMergedLogRecordScanner.newBuilder()
             .withFileSystem(metaClient.getFs())
@@ -461,7 +461,7 @@ private void validateBlockInstantsBeforeAndAfterRollback(HoodieWriteConfig confi
             .withUseScanV2(true)
             .withRecordMerger(HoodieRecordUtils.loadRecordMerger(HoodieAvroRecordMerger.class.getName()))
             .build();
-        scanner2.scanInternal(Option.empty(), true);
+        scanner2.scan(true);
         List<String> currentInstants = scanner2.getValidBlockInstants();
         assertEquals(prevInstants, currentInstants);
       });

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestHarness.java
Patch:
@@ -228,11 +228,11 @@ protected void cleanupSparkContexts() {
       LOG.info("Clearing sql context cache of spark-session used in previous test-case");
       sqlContext.clearCache();
       sqlContext = null;
+      sparkSession = null;
     }
 
     if (jsc != null) {
       LOG.info("Closing spark context used in previous test-case");
-      jsc.close();
       jsc.stop();
       jsc = null;
     }

File: hudi-common/src/main/java/org/apache/hudi/BaseHoodieTableFileIndex.java
Patch:
@@ -51,6 +51,7 @@
 import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
@@ -342,7 +343,7 @@ private FileStatus[] listPartitionPathFiles(List<PartitionPath> partitions) {
             .collect(Collectors.toMap(Pair::getKey, p -> p.getRight().get()));
 
     Set<Path> missingPartitionPaths =
-        CollectionUtils.diff(partitionPaths, cachedPartitionPaths.keySet());
+        CollectionUtils.diffSet(new HashSet<>(partitionPaths), cachedPartitionPaths.keySet());
 
     // NOTE: We're constructing a mapping of absolute form of the partition-path into
     //       its relative one, such that we don't need to reconstruct these again later on

File: hudi-common/src/main/java/org/apache/hudi/common/util/CollectionUtils.java
Patch:
@@ -166,7 +166,7 @@ public static <K, V> Map<K, V> zipToMap(List<K> keys, List<V> values) {
   /**
    * Returns difference b/w {@code one} {@link Set} of elements and {@code another}
    */
-  public static <E> Set<E> diff(Collection<E> one, Collection<E> another) {
+  public static <E> Set<E> diffSet(Set<E> one, Set<E> another) {
     Set<E> diff = new HashSet<>(one);
     diff.removeAll(another);
     return diff;
@@ -178,7 +178,7 @@ public static <E> Set<E> diff(Collection<E> one, Collection<E> another) {
    * NOTE: This is less optimal counterpart to {@link #diff(Collection, Collection)}, accepting {@link List}
    *       as a holding collection to support duplicate elements use-cases
    */
-  public static <E> List<E> diff(List<E> one, List<E> another) {
+  public static <E> List<E> diff(Collection<E> one, Collection<E> another) {
     List<E> diff = new ArrayList<>(one);
     diff.removeAll(another);
     return diff;

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
+import javax.annotation.concurrent.NotThreadSafe;
 import java.io.IOException;
 import java.io.Serializable;
 import java.util.ArrayList;
@@ -51,6 +52,7 @@
  * map may occupy more memory than is available, resulting in OOM. However, if the spill threshold is too low, we spill
  * frequently and incur unnecessary disk writes.
  */
+@NotThreadSafe
 public class ExternalSpillableMap<T extends Serializable, R extends Serializable> implements Map<T, R>, Serializable {
 
   // Find the actual estimated payload size after inserting N records

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodieRecord.java
Patch:
@@ -42,7 +42,8 @@ public class TestHoodieRecord {
 
   @BeforeEach
   public void setUp() throws Exception {
-    final List<IndexedRecord> indexedRecords = SchemaTestUtil.generateHoodieTestRecords(0, 1);
+    SchemaTestUtil testUtil = new SchemaTestUtil();
+    final List<IndexedRecord> indexedRecords = testUtil.generateHoodieTestRecords(0, 1);
     final List<HoodieRecord> hoodieRecords =
         indexedRecords.stream().map(r -> new HoodieAvroRecord(new HoodieKey(UUID.randomUUID().toString(), "0000/00/00"),
             new AvroBinaryTestPayload(Option.of((GenericRecord) r)))).collect(Collectors.toList());

File: hudi-common/src/test/java/org/apache/hudi/common/util/collection/TestRocksDbBasedMap.java
Patch:
@@ -50,7 +50,8 @@ public void setUp() {
   @Test
   public void testSimple() throws IOException, URISyntaxException {
     RocksDbDiskMap records = new RocksDbDiskMap(basePath);
-    List<IndexedRecord> iRecords = SchemaTestUtil.generateHoodieTestRecords(0, 100);
+    SchemaTestUtil testUtil = new SchemaTestUtil();
+    List<IndexedRecord> iRecords = testUtil.generateHoodieTestRecords(0, 100);
     ((GenericRecord) iRecords.get(0)).get(HoodieRecord.COMMIT_TIME_METADATA_FIELD).toString();
     List<String> recordKeys = SpillableMapTestUtils.upsertRecords(iRecords, records);
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/TestDisruptorMessageQueue.java
Patch:
@@ -325,7 +325,7 @@ public Integer finish() {
     };
 
     DisruptorExecutor<HoodieRecord, Tuple2<HoodieRecord, Option<IndexedRecord>>, Integer> exec = new DisruptorExecutor(Option.of(1024),
-        producers, Option.of(consumer), getCloningTransformer(HoodieTestDataGenerator.AVRO_SCHEMA),
+        producers, consumer, getCloningTransformer(HoodieTestDataGenerator.AVRO_SCHEMA),
         Option.of(WaitStrategyFactory.DEFAULT_STRATEGY), getPreExecuteRunnable());
 
     final Throwable thrown = assertThrows(HoodieException.class, exec::execute,

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/BaseHoodieQueueBasedExecutor.java
Patch:
@@ -93,6 +93,8 @@ protected void doProduce(HoodieMessageQueue<I, O> queue, HoodieProducer<I> produ
 
   protected abstract void doConsume(HoodieMessageQueue<I, O> queue, HoodieConsumer<O, E> consumer);
 
+  protected void setUp() {}
+
   /**
    * Start producing
    */
@@ -165,6 +167,7 @@ public boolean isRunning() {
   public E execute() {
     try {
       checkState(this.consumer.isPresent());
+      setUp();
       // Start consuming/producing asynchronously
       CompletableFuture<Void> consuming = startConsumingAsync();
       CompletableFuture<Void> producing = startProducingAsync();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/bloom/TestKeyRangeLookupTree.java
Patch:
@@ -68,7 +68,7 @@ public void testFileGroupLookUpManyEntriesWithSameStartValue() {
     updateExpectedMatchesToTest(toInsert);
     keyRangeLookupTree.insert(toInsert);
     for (int i = 0; i < 10; i++) {
-      endKey += 1 + RANDOM.nextInt(100);
+      endKey += 1 + RANDOM.nextInt(50);
       toInsert = new KeyRangeNode(startKey, Long.toString(endKey), UUID.randomUUID().toString());
       updateExpectedMatchesToTest(toInsert);
       keyRangeLookupTree.insert(toInsert);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java
Patch:
@@ -81,7 +81,7 @@ public class HoodieLockConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS = ConfigProperty
       .key(LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP_KEY)
-      .defaultValue(String.valueOf(10000L))
+      .defaultValue(String.valueOf(2000L))
       .sinceVersion("0.8.0")
       .withDocumentation("Amount of time to wait between retries on the lock provider by the lock manager");
 
@@ -93,7 +93,7 @@ public class HoodieLockConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> LOCK_ACQUIRE_CLIENT_NUM_RETRIES = ConfigProperty
       .key(LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP_KEY)
-      .defaultValue(String.valueOf(10))
+      .defaultValue(String.valueOf(50))
       .sinceVersion("0.8.0")
       .withDocumentation("Maximum number of times to retry to acquire lock additionally from the lock manager.");
 

File: hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.table.format.cow.vector.ParquetDecimalVector;
 
 import org.apache.flink.formats.parquet.vector.reader.ColumnReader;
+import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.vector.ColumnVector;
 import org.apache.flink.table.data.vector.writable.WritableColumnVector;
 import org.apache.flink.table.types.logical.LogicalType;
@@ -266,7 +267,7 @@ public boolean reachedEnd() throws IOException {
     return !ensureBatch();
   }
 
-  public ColumnarRowData nextRecord() {
+  public RowData nextRecord() {
     // return the next row
     row.setRowId(this.nextRow++);
     return row;

File: hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java
Patch:
@@ -22,6 +22,7 @@
 
 import org.apache.flink.formats.parquet.vector.reader.ColumnReader;
 import org.apache.flink.table.data.ColumnarRowData;
+import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.vector.ColumnVector;
 import org.apache.flink.table.data.vector.VectorizedColumnBatch;
 import org.apache.flink.table.data.vector.writable.WritableColumnVector;
@@ -266,7 +267,7 @@ public boolean reachedEnd() throws IOException {
     return !ensureBatch();
   }
 
-  public ColumnarRowData nextRecord() {
+  public RowData nextRecord() {
     // return the next row
     row.setRowId(this.nextRow++);
     return row;

File: hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.table.format.cow.vector.ParquetDecimalVector;
 
 import org.apache.flink.formats.parquet.vector.reader.ColumnReader;
+import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.columnar.ColumnarRowData;
 import org.apache.flink.table.data.columnar.vector.ColumnVector;
 import org.apache.flink.table.data.columnar.vector.VectorizedColumnBatch;
@@ -266,7 +267,7 @@ public boolean reachedEnd() throws IOException {
     return !ensureBatch();
   }
 
-  public ColumnarRowData nextRecord() {
+  public RowData nextRecord() {
     // return the next row
     row.setRowId(this.nextRow++);
     return row;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RowCustomColumnsSortPartitioner.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.BulkInsertPartitioner;
+
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
 
@@ -44,8 +45,8 @@ public RowCustomColumnsSortPartitioner(String[] columnNames) {
   @Override
   public Dataset<Row> repartitionRecords(Dataset<Row> records, int outputSparkPartitions) {
     final String[] sortColumns = this.sortColumnNames;
-    return records.coalesce(outputSparkPartitions)
-        .sortWithinPartitions(HoodieRecord.PARTITION_PATH_METADATA_FIELD, sortColumns);
+    return records.sort(HoodieRecord.PARTITION_PATH_METADATA_FIELD, sortColumns)
+        .coalesce(outputSparkPartitions);
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/InternalSchemaUtils.java
Patch:
@@ -57,7 +57,7 @@ public static InternalSchema pruneInternalSchema(InternalSchema schema, List<Str
     List<Integer> prunedIds = names.stream().map(name -> {
       int id = schema.findIdByName(name);
       if (id == -1) {
-        throw new IllegalArgumentException(String.format("cannot prune col: %s which not exisit in hudi table", name));
+        throw new IllegalArgumentException(String.format("cannot prune col: %s which does not exist in hudi table", name));
       }
       return id;
     }).collect(Collectors.toList());
@@ -177,7 +177,7 @@ private static Type pruneType(Type type, List<Integer> fieldIds) {
   public static String reBuildFilterName(String name, InternalSchema fileSchema, InternalSchema querySchema) {
     int nameId = querySchema.findIdByName(name);
     if (nameId == -1) {
-      throw new IllegalArgumentException(String.format("cannot found filter col name：%s from querySchema: %s", name, querySchema));
+      throw new IllegalArgumentException(String.format("cannot find filter col name：%s from querySchema: %s", name, querySchema));
     }
     if (fileSchema.findField(nameId) == null) {
       // added operation found

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -137,7 +137,7 @@ public List<HoodieFileGroup> addFilesToView(FileStatus[] statuses) {
     List<HoodieFileGroup> fileGroups = buildFileGroups(statuses, visibleCommitsAndCompactionTimeline, true);
     long fgBuildTimeTakenMs = timer.endTimer();
     timer.startTimer();
-    // Group by partition for efficient updates for both InMemory and DiskBased stuctures.
+    // Group by partition for efficient updates for both InMemory and DiskBased structures.
     fileGroups.stream().collect(Collectors.groupingBy(HoodieFileGroup::getPartitionPath)).forEach((partition, value) -> {
       if (!isPartitionAvailableInStore(partition)) {
         if (bootstrapIndex.useIndex()) {

File: hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java
Patch:
@@ -196,7 +196,7 @@ private void alignDiskPositionToBufferStartIfNeeded() throws IOException {
   }
 
   /**
-   * If the new seek position is in the buffer, adjust the currentPostion.
+   * If the new seek position is in the buffer, adjust the currentPosition.
    * If the new seek position is outside of the buffer, flush the contents to
    * the file and reload the buffer corresponding to the position.
    *

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
Patch:
@@ -228,15 +228,16 @@ public static DFSPropertiesConfiguration getConfig(List<String> overriddenProps)
   public static TypedProperties buildProperties(List<String> props) {
     TypedProperties properties = DFSPropertiesConfiguration.getGlobalProps();
     props.forEach(x -> {
-      String[] kv = x.split("=");
+      // Some values may contain '=', such as the partition path
+      String[] kv = x.split("=", 2);
       ValidationUtils.checkArgument(kv.length == 2);
       properties.setProperty(kv[0], kv[1]);
     });
     return properties;
   }
 
   public static void validateAndAddProperties(String[] configs, SparkLauncher sparkLauncher) {
-    Arrays.stream(configs).filter(config -> config.contains("=") && config.split("=").length == 2).forEach(sparkLauncher::addAppArgs);
+    Arrays.stream(configs).filter(config -> config.contains("=") && config.split("=", 2).length == 2).forEach(sparkLauncher::addAppArgs);
   }
 
   /**

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/IncrementalInputSplits.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.source;
 
-import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.BaseFile;
 import org.apache.hudi.common.model.FileSlice;
@@ -43,6 +42,7 @@
 import org.apache.hudi.util.ClusteringUtil;
 import org.apache.hudi.util.StreamerUtil;
 
+import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.fs.Path;
 import org.apache.flink.table.types.logical.RowType;
@@ -462,7 +462,7 @@ private HoodieTimeline getReadTimeline(HoodieTableMetaClient metaClient) {
   }
 
   private HoodieTimeline getArchivedReadTimeline(HoodieTableMetaClient metaClient, String startInstant) {
-    HoodieArchivedTimeline archivedTimeline = metaClient.getArchivedTimeline(startInstant);
+    HoodieArchivedTimeline archivedTimeline = metaClient.getArchivedTimeline(startInstant, false);
     HoodieTimeline archivedCompleteTimeline = archivedTimeline.getCommitsTimeline().filterCompletedInstants();
     return filterInstantsByCondition(archivedCompleteTimeline);
   }

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/TestDFSHoodieTestSuiteWriterAdapter.java
Patch:
@@ -70,7 +70,7 @@ public static void initClass() throws Exception {
 
   @AfterAll
   public static void cleanupClass() {
-    UtilitiesTestBase.cleanupClass();
+    UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 
   @BeforeEach

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/TestFileDeltaInputWriter.java
Patch:
@@ -63,7 +63,7 @@ public static void initClass() throws Exception {
 
   @AfterAll
   public static void cleanupClass() {
-    UtilitiesTestBase.cleanupClass();
+    UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 
   @BeforeEach

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/job/TestHoodieTestSuiteJob.java
Patch:
@@ -137,7 +137,7 @@ public static void initClass() throws Exception {
 
   @AfterAll
   public static void cleanupClass() {
-    UtilitiesTestBase.cleanupClass();
+    UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 
   @BeforeEach

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/reader/TestDFSAvroDeltaInputReader.java
Patch:
@@ -48,7 +48,7 @@ public static void initClass() throws Exception {
 
   @AfterAll
   public static void cleanupClass() {
-    UtilitiesTestBase.cleanupClass();
+    UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 
   @BeforeEach

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/reader/TestDFSHoodieDatasetInputReader.java
Patch:
@@ -56,7 +56,7 @@ public static void initClass() throws Exception {
 
   @AfterAll
   public static void cleanupClass() {
-    UtilitiesTestBase.cleanupClass();
+    UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 
   @BeforeEach

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestSqlSource.java
Patch:
@@ -65,7 +65,7 @@ public static void initClass() throws Exception {
 
   @AfterAll
   public static void cleanupClass() {
-    UtilitiesTestBase.cleanupClass();
+    UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 
   @BeforeEach

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/debezium/TestAbstractDebeziumSource.java
Patch:
@@ -69,7 +69,7 @@ public static void initClass() throws Exception {
 
   @AfterAll
   public static void cleanupClass() {
-    UtilitiesTestBase.cleanupClass();
+    UtilitiesTestBase.cleanUpUtilitiesTestServices();
     testUtils.teardown();
   }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/testutils/UtilitiesTestBase.java
Patch:
@@ -163,7 +163,7 @@ public static void initTestServices(boolean needsHdfs, boolean needsHive, boolea
   }
 
   @AfterAll
-  public static void cleanupClass() {
+  public static void cleanUpUtilitiesTestServices() {
     if (hdfsTestService != null) {
       hdfsTestService.stop();
       hdfsTestService = null;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/testutils/sources/AbstractCloudObjectsSourceTestBase.java
Patch:
@@ -59,7 +59,7 @@ public static void initClass() throws Exception {
 
   @AfterAll
   public static void cleanupClass() {
-    UtilitiesTestBase.cleanupClass();
+    UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 
   @BeforeEach

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/transform/TestSqlFileBasedTransformer.java
Patch:
@@ -67,7 +67,7 @@ public static void initClass() throws Exception {
 
   @AfterAll
   public static void cleanupClass() {
-    UtilitiesTestBase.cleanupClass();
+    UtilitiesTestBase.cleanUpUtilitiesTestServices();
   }
 
   @Override

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkTable.java
Patch:
@@ -59,15 +59,15 @@ public static <T> HoodieFlinkTable<T> create(HoodieWriteConfig config, HoodieFli
             .setLoadActiveTimelineOnLoad(true).setConsistencyGuardConfig(config.getConsistencyGuardConfig())
             .setLayoutVersion(Option.of(new TimelineLayoutVersion(config.getTimelineLayoutVersion())))
             .setFileSystemRetryConfig(config.getFileSystemRetryConfig()).build();
-    if (config.getSchemaEvolutionEnable()) {
-      setLatestInternalSchema(config, metaClient);
-    }
     return HoodieFlinkTable.create(config, context, metaClient);
   }
 
   public static <T> HoodieFlinkTable<T> create(HoodieWriteConfig config,
                                                HoodieFlinkEngineContext context,
                                                HoodieTableMetaClient metaClient) {
+    if (config.getSchemaEvolutionEnable()) {
+      setLatestInternalSchema(config, metaClient);
+    }
     final HoodieFlinkTable<T> hoodieFlinkTable;
     switch (metaClient.getTableType()) {
       case COPY_ON_WRITE:

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaTable.java
Patch:
@@ -50,8 +50,8 @@ public static <T> HoodieJavaTable<T> create(HoodieWriteConfig config, HoodieEngi
   }
 
   public static <T> HoodieJavaTable<T> create(HoodieWriteConfig config,
-                                                                          HoodieJavaEngineContext context,
-                                                                          HoodieTableMetaClient metaClient) {
+                                              HoodieEngineContext context,
+                                              HoodieTableMetaClient metaClient) {
     switch (metaClient.getTableType()) {
       case COPY_ON_WRITE:
         return new HoodieJavaCopyOnWriteTable<>(config, context, metaClient);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkTable.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.table;
 
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieKey;
@@ -59,11 +58,11 @@ public static <T> HoodieSparkTable<T> create(HoodieWriteConfig config, HoodieEng
             .setLayoutVersion(Option.of(new TimelineLayoutVersion(config.getTimelineLayoutVersion())))
             .setFileSystemRetryConfig(config.getFileSystemRetryConfig())
             .setProperties(config.getProps()).build();
-    return HoodieSparkTable.create(config, (HoodieSparkEngineContext) context, metaClient);
+    return HoodieSparkTable.create(config, context, metaClient);
   }
 
   public static <T> HoodieSparkTable<T> create(HoodieWriteConfig config,
-                                               HoodieSparkEngineContext context,
+                                               HoodieEngineContext context,
                                                HoodieTableMetaClient metaClient) {
     HoodieSparkTable<T> hoodieSparkTable;
     switch (metaClient.getTableType()) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnMergeOnReadStorage.java
Patch:
@@ -230,7 +230,7 @@ public void testLogCompactionOnMORTableWithoutBaseFile() throws Exception {
     client.logCompact(timeStamp.get());
     // Verify all the records.
     assertDataInMORTable(config, lastCommitBeforeLogCompaction, timeStamp.get(),
-        hadoopConf, Arrays.asList(dataGen.getPartitionPaths()));
+        hadoopConf, Arrays.asList(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH));
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieMergeHelper.java
Patch:
@@ -42,7 +42,7 @@
 import org.apache.hudi.io.storage.HoodieFileReader;
 import org.apache.hudi.io.storage.HoodieFileReaderFactory;
 import org.apache.hudi.table.HoodieTable;
-import org.apache.hudi.util.QueueBasedExecutorFactory;
+import org.apache.hudi.util.ExecutorFactory;
 
 import org.apache.avro.Schema;
 import org.apache.avro.SchemaCompatibility;
@@ -108,7 +108,7 @@ public void runMerge(HoodieTable<?, ?, ?, ?> table,
         || !isPureProjection
         || baseFile.getBootstrapBaseFile().isPresent();
 
-    HoodieExecutor<HoodieRecord, HoodieRecord, Void> wrapper = null;
+    HoodieExecutor<Void> wrapper = null;
 
     try {
       Iterator<HoodieRecord> recordIterator;
@@ -135,7 +135,7 @@ public void runMerge(HoodieTable<?, ?, ?, ?> table,
         recordSchema = isPureProjection ? writerSchema : readerSchema;
       }
 
-      wrapper = QueueBasedExecutorFactory.create(writeConfig, recordIterator, new UpdateHandler(mergeHandle), record -> {
+      wrapper = ExecutorFactory.create(writeConfig, recordIterator, new UpdateHandler(mergeHandle), record -> {
         // NOTE: Record have to be cloned here to make sure if it holds low-level engine-specific
         //       payload pointing into a shared, mutable (underlying) buffer we get a clean copy of
         //       it since these records will be put into queue of QueueBasedExecutorFactory.

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/execution/JavaLazyInsertIterable.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.io.WriteHandleFactory;
 import org.apache.hudi.table.HoodieTable;
-import org.apache.hudi.util.QueueBasedExecutorFactory;
+import org.apache.hudi.util.ExecutorFactory;
 
 import java.util.Iterator;
 import java.util.List;
@@ -59,12 +59,12 @@ public JavaLazyInsertIterable(Iterator<HoodieRecord<T>> recordItr,
   @Override
   protected List<WriteStatus> computeNext() {
     // Executor service used for launching writer thread.
-    HoodieExecutor<HoodieRecord<T>, HoodieInsertValueGenResult<HoodieRecord>, List<WriteStatus>> bufferedIteratorExecutor =
+    HoodieExecutor<List<WriteStatus>> bufferedIteratorExecutor =
         null;
     try {
       final Schema schema = new Schema.Parser().parse(hoodieConfig.getSchema());
       bufferedIteratorExecutor =
-          QueueBasedExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(), getCloningTransformer(schema));
+          ExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(), getCloningTransformer(schema));
       final List<WriteStatus> result = bufferedIteratorExecutor.execute();
       checkState(result != null && !result.isEmpty());
       return result;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/SparkLazyInsertIterable.java
Patch:
@@ -29,7 +29,7 @@
 import org.apache.hudi.table.HoodieTable;
 
 import org.apache.avro.Schema;
-import org.apache.hudi.util.QueueBasedExecutorFactory;
+import org.apache.hudi.util.ExecutorFactory;
 
 import java.util.Iterator;
 import java.util.List;
@@ -79,14 +79,14 @@ public SparkLazyInsertIterable(Iterator<HoodieRecord<T>> recordItr,
   @Override
   protected List<WriteStatus> computeNext() {
     // Executor service used for launching writer thread.
-    HoodieExecutor<?, ?, List<WriteStatus>> bufferedIteratorExecutor = null;
+    HoodieExecutor<List<WriteStatus>> bufferedIteratorExecutor = null;
     try {
       Schema schema = new Schema.Parser().parse(hoodieConfig.getSchema());
       if (useWriterSchema) {
         schema = HoodieAvroUtils.addMetadataFields(schema);
       }
 
-      bufferedIteratorExecutor = QueueBasedExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(),
+      bufferedIteratorExecutor = ExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(),
           getCloningTransformer(schema, hoodieConfig), hoodieTable.getPreExecuteRunnable());
 
       final List<WriteStatus> result = bufferedIteratorExecutor.execute();

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/BaseHoodieQueueBasedExecutor.java
Patch:
@@ -49,7 +49,7 @@
  * is ingested from multiple sources (ie producers) into a singular sink (ie consumer), using
  * an internal queue to stage the records ingested from producers before these are consumed
  */
-public abstract class BaseHoodieQueueBasedExecutor<I, O, E> implements HoodieExecutor<I, O, E> {
+public abstract class BaseHoodieQueueBasedExecutor<I, O, E> implements HoodieExecutor<E> {
 
   private static final long TERMINATE_WAITING_TIME_SECS = 60L;
 
@@ -74,7 +74,7 @@ public BaseHoodieQueueBasedExecutor(List<HoodieProducer<I>> producers,
     this.producers = producers;
     this.consumer = consumer;
     // Ensure fixed thread for each producer thread
-    this.producerExecutorService = Executors.newFixedThreadPool(producers.size(), new CustomizedThreadFactory("executor-queue-producer", preExecuteRunnable));
+    this.producerExecutorService = Executors.newFixedThreadPool(Math.max(1, producers.size()), new CustomizedThreadFactory("executor-queue-producer", preExecuteRunnable));
     // Ensure single thread for consumer
     this.consumerExecutorService = Executors.newSingleThreadExecutor(new CustomizedThreadFactory("executor-queue-consumer", preExecuteRunnable));
   }

File: hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java
Patch:
@@ -21,11 +21,11 @@
 import org.apache.hudi.cli.HoodieCliSparkConfig;
 import org.apache.hudi.cli.commands.SparkEnvCommand;
 import org.apache.hudi.cli.commands.SparkMain;
-import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
 
+import org.apache.spark.HoodieSparkKryoProvider$;
 import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.launcher.SparkLauncher;
@@ -116,7 +116,7 @@ public static JavaSparkContext initJavaSparkContext(String name, Option<String>
   }
 
   public static JavaSparkContext initJavaSparkContext(SparkConf sparkConf) {
-    SparkRDDWriteClient.registerClasses(sparkConf);
+    HoodieSparkKryoProvider$.MODULE$.register(sparkConf);
     JavaSparkContext jsc = new JavaSparkContext(sparkConf);
     jsc.hadoopConfiguration().setBoolean(HoodieCliSparkConfig.CLI_PARQUET_ENABLE_SUMMARY_METADATA, false);
     FSUtils.prepareHadoopConf(jsc.hadoopConfiguration());

File: hudi-cli/src/test/java/org/apache/hudi/cli/functional/CLIFunctionalTestHarness.java
Patch:
@@ -20,14 +20,14 @@
 package org.apache.hudi.cli.functional;
 
 import org.apache.hudi.client.SparkRDDReadClient;
-import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.testutils.HoodieClientTestUtils;
 import org.apache.hudi.testutils.providers.SparkProvider;
 import org.apache.hudi.timeline.service.TimelineService;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.spark.HoodieSparkKryoProvider$;
 import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.SQLContext;
@@ -100,7 +100,7 @@ public synchronized void runBeforeEach() {
     initialized = spark != null;
     if (!initialized) {
       SparkConf sparkConf = conf();
-      SparkRDDWriteClient.registerClasses(sparkConf);
+      HoodieSparkKryoProvider$.MODULE$.register(sparkConf);
       SparkRDDReadClient.addHoodieSupport(sparkConf);
       spark = SparkSession.builder().config(sparkConf).getOrCreate();
       sqlContext = spark.sqlContext();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseClusterer.java
Patch:
@@ -19,7 +19,6 @@
 
 package org.apache.hudi.client;
 
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 
 import java.io.IOException;
@@ -28,7 +27,7 @@
 /**
  * Client will run one round of clustering.
  */
-public abstract class BaseClusterer<T extends HoodieRecordPayload, I, K, O> implements Serializable {
+public abstract class BaseClusterer<T, I, K, O> implements Serializable {
 
   private static final long serialVersionUID = 1L;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseCompactor.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.client;
 
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 
 import java.io.IOException;
@@ -27,7 +26,7 @@
 /**
  * Run one round of compaction.
  */
-public abstract class BaseCompactor<T extends HoodieRecordPayload, I, K, O> implements Serializable {
+public abstract class BaseCompactor<T, I, K, O> implements Serializable {
 
   private static final long serialVersionUID = 1L;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -44,7 +44,6 @@
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
 import org.apache.hudi.common.model.HoodieKey;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.TableServiceType;
 import org.apache.hudi.common.model.WriteOperationType;
@@ -123,12 +122,12 @@
  * Abstract Write Client providing functionality for performing commit, index updates and rollback
  * Reused for regular write operations like upsert/insert/bulk-insert.. as well as bootstrap
  *
- * @param <T> Sub type of HoodieRecordPayload
+ * @param <T> Type of data
  * @param <I> Type of inputs
  * @param <K> Type of keys
  * @param <O> Type of outputs
  */
-public abstract class BaseHoodieWriteClient<T extends HoodieRecordPayload, I, K, O> extends BaseHoodieClient
+public abstract class BaseHoodieWriteClient<T, I, K, O> extends BaseHoodieClient
     implements RunsTableService {
 
   protected static final String LOOKUP_STR = "lookup";

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/bootstrap/FullRecordBootstrapDataProvider.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.collection.Pair;
+import org.apache.hudi.config.HoodieWriteConfig;
 
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
@@ -49,8 +50,9 @@ public FullRecordBootstrapDataProvider(TypedProperties props, HoodieEngineContex
    * @param tableName Hudi Table Name
    * @param sourceBasePath Source Base Path
    * @param partitionPaths Partition Paths
+   * @param config config
    * @return input records
    */
   public abstract I generateInputRecords(String tableName,
-      String sourceBasePath, List<Pair<String, List<HoodieFileStatus>>> partitionPaths);
+      String sourceBasePath, List<Pair<String, List<HoodieFileStatus>>> partitionPaths, HoodieWriteConfig config);
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieRecord;
+import org.apache.hudi.common.model.HoodieRecord.HoodieRecordType;
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.HoodieTimer;
@@ -150,7 +151,8 @@ public static List<String> filterKeysFromFile(Path filePath, List<String> candid
                                                 Configuration configuration) throws HoodieIndexException {
     ValidationUtils.checkArgument(FSUtils.isBaseFile(filePath));
     List<String> foundRecordKeys = new ArrayList<>();
-    try (HoodieFileReader fileReader = HoodieFileReaderFactory.getFileReader(configuration, filePath)) {
+    try (HoodieFileReader fileReader = HoodieFileReaderFactory.getReaderFactory(HoodieRecordType.AVRO)
+        .getFileReader(configuration, filePath)) {
       // Load all rowKeys from the file, to double-confirm
       if (!candidateRecordKeys.isEmpty()) {
         HoodieTimer timer = HoodieTimer.start();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/AppendHandleFactory.java
Patch:
@@ -19,11 +19,10 @@
 package org.apache.hudi.io;
 
 import org.apache.hudi.common.engine.TaskContextSupplier;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
-public class AppendHandleFactory<T extends HoodieRecordPayload, I, K, O> extends WriteHandleFactory<T, I, K, O> {
+public class AppendHandleFactory<T, I, K, O> extends WriteHandleFactory<T, I, K, O> {
 
   @Override
   public HoodieAppendHandle<T, I, K, O> create(final HoodieWriteConfig hoodieConfig, final String commitTime,

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/CreateHandleFactory.java
Patch:
@@ -19,13 +19,12 @@
 package org.apache.hudi.io;
 
 import org.apache.hudi.common.engine.TaskContextSupplier;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
 import java.io.Serializable;
 
-public class CreateHandleFactory<T extends HoodieRecordPayload, I, K, O> extends WriteHandleFactory<T, I, K, O> implements Serializable {
+public class CreateHandleFactory<T, I, K, O> extends WriteHandleFactory<T, I, K, O> implements Serializable {
 
   private boolean preserveMetadata = false;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieBootstrapHandle.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
@@ -33,7 +32,7 @@
  *    writing more than 1 skeleton file for the same bootstrap file.
  * @param <T> HoodieRecordPayload
  */
-public class HoodieBootstrapHandle<T extends HoodieRecordPayload, I, K, O> extends HoodieCreateHandle<T, I, K, O> {
+public class HoodieBootstrapHandle<T, I, K, O> extends HoodieCreateHandle<T, I, K, O> {
 
   public HoodieBootstrapHandle(HoodieWriteConfig config, String commitTime, HoodieTable<T, I, K, O> hoodieTable,
       String partitionPath, String fileId, TaskContextSupplier taskContextSupplier) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCDCLogger.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.common.model.HoodieAvroIndexedRecord;
 import org.apache.hudi.common.model.HoodieAvroPayload;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.table.HoodieTableConfig;
@@ -179,10 +180,10 @@ public void put(HoodieRecord hoodieRecord,
   private void flushIfNeeded(Boolean force) {
     if (force || numOfCDCRecordsInMemory.get() * averageCDCRecordSize >= maxBlockSize) {
       try {
-        List<IndexedRecord> records = cdcData.values().stream()
+        List<HoodieRecord> records = cdcData.values().stream()
             .map(record -> {
               try {
-                return record.getInsertValue(cdcSchema).get();
+                return new HoodieAvroIndexedRecord(record.getInsertValue(cdcSchema).get());
               } catch (IOException e) {
                 throw new HoodieIOException("Failed to get cdc record", e);
               }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieIOHandle.java
Patch:
@@ -18,15 +18,14 @@
 
 package org.apache.hudi.io;
 
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
 import org.apache.hadoop.fs.FileSystem;
 
-public abstract class HoodieIOHandle<T extends HoodieRecordPayload, I, K, O> {
+public abstract class HoodieIOHandle<T, I, K, O> {
 
   protected final String instantTime;
   protected final HoodieWriteConfig config;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieKeyLocationFetchHandle.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecordLocation;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.BaseFileUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
@@ -40,7 +39,7 @@
  *
  * @param <T>
  */
-public class HoodieKeyLocationFetchHandle<T extends HoodieRecordPayload, I, K, O> extends HoodieReadHandle<T, I, K, O> {
+public class HoodieKeyLocationFetchHandle<T, I, K, O> extends HoodieReadHandle<T, I, K, O> {
 
   private final Pair<String, HoodieBaseFile> partitionPathBaseFilePair;
   private final Option<BaseKeyGenerator> keyGeneratorOpt;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandleFactory.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -38,7 +37,7 @@ public class HoodieMergeHandleFactory {
   /**
    * Creates a merge handle for normal write path.
    */
-  public static <T extends HoodieRecordPayload, I, K, O> HoodieMergeHandle<T, I, K, O> create(
+  public static <T, I, K, O> HoodieMergeHandle<T, I, K, O> create(
       WriteOperationType operationType,
       HoodieWriteConfig writeConfig,
       String instantTime,
@@ -70,7 +69,7 @@ public static <T extends HoodieRecordPayload, I, K, O> HoodieMergeHandle<T, I, K
   /**
    * Creates a merge handle for compaction path.
    */
-  public static <T extends HoodieRecordPayload, I, K, O> HoodieMergeHandle<T, I, K, O> create(
+  public static <T, I, K, O> HoodieMergeHandle<T, I, K, O> create(
       HoodieWriteConfig writeConfig,
       String instantTime,
       HoodieTable<T, I, K, O> table,

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieRangeInfoHandle.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.io;
 
 import org.apache.hudi.common.model.HoodieBaseFile;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.storage.HoodieFileReader;
@@ -30,7 +29,7 @@
 /**
  * Extract range information for a given file slice.
  */
-public class HoodieRangeInfoHandle<T extends HoodieRecordPayload, I, K, O> extends HoodieReadHandle<T, I, K, O> {
+public class HoodieRangeInfoHandle<T, I, K, O> extends HoodieReadHandle<T, I, K, O> {
 
   public HoodieRangeInfoHandle(HoodieWriteConfig config, HoodieTable<T, I, K, O> hoodieTable,
       Pair<String, String> partitionPathFilePair) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieUnboundedCreateHandle.java
Patch:
@@ -20,7 +20,6 @@
 
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
@@ -36,7 +35,7 @@
  * Please use this with caution. This can end up creating very large files if not used correctly.
  */
 @NotThreadSafe
-public class HoodieUnboundedCreateHandle<T extends HoodieRecordPayload, I, K, O> extends HoodieCreateHandle<T, I, K, O> {
+public class HoodieUnboundedCreateHandle<T, I, K, O> extends HoodieCreateHandle<T, I, K, O> {
 
   private static final Logger LOG = LogManager.getLogger(HoodieUnboundedCreateHandle.class);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/SingleFileHandleCreateFactory.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.io;
 
 import org.apache.hudi.common.engine.TaskContextSupplier;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.table.HoodieTable;
@@ -32,7 +31,7 @@
  * <p>
  * Please use this with caution. This can end up creating very large files if not used correctly.
  */
-public class SingleFileHandleCreateFactory<T extends HoodieRecordPayload, I, K, O> extends CreateHandleFactory<T, I, K, O> implements Serializable {
+public class SingleFileHandleCreateFactory<T, I, K, O> extends CreateHandleFactory<T, I, K, O> implements Serializable {
 
   private final AtomicBoolean isHandleCreated = new AtomicBoolean(false);
   private final String fileId;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/WriteHandleFactory.java
Patch:
@@ -20,13 +20,12 @@
 
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.fs.FSUtils;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
 import java.io.Serializable;
 
-public abstract class WriteHandleFactory<T extends HoodieRecordPayload, I, K, O> implements Serializable {
+public abstract class WriteHandleFactory<T, I, K, O> implements Serializable {
   private int numFilesWritten = 0;
 
   public abstract HoodieWriteHandle<T, I, K, O> create(HoodieWriteConfig config, String commitTime, HoodieTable<T, I, K, O> hoodieTable,

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -994,7 +994,9 @@ protected HoodieData<HoodieRecord> prepRecords(Map<MetadataPartitionType,
       HoodieData<HoodieRecord> rddSinglePartitionRecords = records.map(r -> {
         FileSlice slice = finalFileSlices.get(HoodieTableMetadataUtil.mapRecordKeyToFileGroupIndex(r.getRecordKey(),
             fileGroupCount));
+        r.unseal();
         r.setCurrentLocation(new HoodieRecordLocation(slice.getBaseInstantTime(), slice.getFileId()));
+        r.seal();
         return r;
       });
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/JmxReporterServer.java
Patch:
@@ -37,7 +37,6 @@
 import java.rmi.server.UnicastRemoteObject;
 import java.util.Objects;
 
-
 /**
  * A reporter which publishes metric values to a JMX server.
  */

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -43,7 +43,6 @@
 import org.apache.hudi.common.fs.OptimisticConsistencyGuard;
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieKey;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.HoodieTableConfig;
@@ -115,7 +114,7 @@
  * @param <K> Type of keys
  * @param <O> Type of outputs
  */
-public abstract class HoodieTable<T extends HoodieRecordPayload, I, K, O> implements Serializable {
+public abstract class HoodieTable<T, I, K, O> implements Serializable {
 
   private static final Logger LOG = LogManager.getLogger(HoodieTable.class);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/BaseActionExecutor.java
Patch:
@@ -26,12 +26,11 @@
 import org.apache.hudi.avro.model.HoodieRollbackMetadata;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
-public abstract class BaseActionExecutor<T extends HoodieRecordPayload, I, K, O, R> implements Serializable {
+public abstract class BaseActionExecutor<T, I, K, O, R> implements Serializable {
 
   protected final transient HoodieEngineContext context;
   protected final transient Configuration hadoopConf;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.HoodieCleanStat;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.CleanFileInfo;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
 import org.apache.hudi.common.util.CleanerUtils;
@@ -55,7 +54,7 @@
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
-public class CleanActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, HoodieCleanMetadata> {
+public class CleanActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I, K, O, HoodieCleanMetadata> {
 
   private static final long serialVersionUID = 1L;
   private static final Logger LOG = LogManager.getLogger(CleanActionExecutor.class);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanActionExecutor.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.CleanFileInfo;
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
@@ -46,7 +45,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
-public class CleanPlanActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieCleanerPlan>> {
+public class CleanPlanActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieCleanerPlan>> {
 
   private static final Logger LOG = LogManager.getLogger(CleanPlanner.class);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java
Patch:
@@ -30,7 +30,6 @@
 import org.apache.hudi.common.model.HoodieFileGroup;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieLogFile;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.cdc.HoodieCDCUtils;
@@ -74,7 +73,7 @@
  * <p>
  * 2) It bounds the growth of the files in the file system
  */
-public class CleanPlanner<T extends HoodieRecordPayload, I, K, O> implements Serializable {
+public class CleanPlanner<T, I, K, O> implements Serializable {
 
   private static final Logger LOG = LogManager.getLogger(CleanPlanner.class);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/ClusteringPlanActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.avro.model.HoodieClusteringPlan;
 import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -41,7 +40,7 @@
 import java.util.Collections;
 import java.util.Map;
 
-public class ClusteringPlanActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieClusteringPlan>> {
+public class ClusteringPlanActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieClusteringPlan>> {
 
   private static final Logger LOG = LogManager.getLogger(ClusteringPlanActionExecutor.class);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.model.BaseFile;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieFileGroupId;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.view.SyncableFileSystemView;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
@@ -50,7 +49,7 @@
 /**
  * Pluggable implementation for scheduling clustering and creating ClusteringPlan.
  */
-public abstract class ClusteringPlanStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {
+public abstract class ClusteringPlanStrategy<T,I,K,O> implements Serializable {
   private static final Logger LOG = LogManager.getLogger(ClusteringPlanStrategy.class);
 
   public static final int CLUSTERING_PLAN_VERSION_1 = 1;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareClusteringPlanStrategy.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.FileSlice;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
@@ -44,7 +43,7 @@
 /**
  * Scheduling strategy with restriction that clustering groups can only contain files from same partition.
  */
-public abstract class PartitionAwareClusteringPlanStrategy<T extends HoodieRecordPayload,I,K,O> extends ClusteringPlanStrategy<T,I,K,O> {
+public abstract class PartitionAwareClusteringPlanStrategy<T,I,K,O> extends ClusteringPlanStrategy<T,I,K,O> {
   private static final Logger LOG = LogManager.getLogger(PartitionAwareClusteringPlanStrategy.class);
 
   public PartitionAwareClusteringPlanStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java
Patch:
@@ -20,7 +20,6 @@
 
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieFileGroupId;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.table.HoodieTable;
 
@@ -30,7 +29,7 @@
 /**
  * When file groups in clustering, write records to these file group need to check.
  */
-public abstract class UpdateStrategy<T extends HoodieRecordPayload, I> implements Serializable {
+public abstract class UpdateStrategy<T, I> implements Serializable {
 
   protected final transient HoodieEngineContext engineContext;
   protected final HoodieTable table;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseBulkInsertHelper.java
Patch:
@@ -18,15 +18,14 @@
 
 package org.apache.hudi.table.action.commit;
 
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.WriteHandleFactory;
 import org.apache.hudi.table.BulkInsertPartitioner;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
-public abstract class BaseBulkInsertHelper<T extends HoodieRecordPayload, I, K, O, R> {
+public abstract class BaseBulkInsertHelper<T, I, K, O, R> {
 
   /**
    * Mark instant as inflight, write input records, update index and return result.

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java
Patch:
@@ -31,7 +31,6 @@
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
@@ -70,7 +69,7 @@
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
-public abstract class BaseCommitActionExecutor<T extends HoodieRecordPayload, I, K, O, R>
+public abstract class BaseCommitActionExecutor<T, I, K, O, R>
     extends BaseActionExecutor<T, I, K, O, R> {
 
   private static final Logger LOG = LogManager.getLogger(BaseCommitActionExecutor.class);
@@ -157,7 +156,6 @@ protected String getCommitActionType() {
     return  table.getMetaClient().getCommitActionType();
   }
 
-
   /**
    * Check if any validators are configured and run those validations. If any of the validations fail, throws HoodieValidationException.
    */

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseDeleteHelper.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.table.action.commit;
 
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
@@ -29,7 +28,7 @@
  *
  * @param <T>
  */
-public abstract class BaseDeleteHelper<T extends HoodieRecordPayload, I, K, O, R> {
+public abstract class BaseDeleteHelper<T, I, K, O, R> {
 
   /**
    * Deduplicate Hoodie records, using the given deduplication function.

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactHelpers.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -43,7 +42,7 @@
  * @param <K> Type of keys
  * @param <O> Type of outputs
  */
-public class CompactHelpers<T extends HoodieRecordPayload, I, K, O> {
+public class CompactHelpers<T, I, K, O> {
 
   private static final CompactHelpers SINGLETON_INSTANCE = new CompactHelpers();
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java
Patch:
@@ -27,7 +27,6 @@
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.CompactionOperation;
 import org.apache.hudi.common.model.HoodieBaseFile;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat.RuntimeStats;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -65,7 +64,7 @@
 /**
  * A HoodieCompactor runs compaction on a hoodie table.
  */
-public abstract class HoodieCompactor<T extends HoodieRecordPayload, I, K, O> implements Serializable {
+public abstract class HoodieCompactor<T, I, K, O> implements Serializable {
 
   private static final Logger LOG = LogManager.getLogger(HoodieCompactor.class);
 
@@ -201,6 +200,7 @@ public List<WriteStatus> compact(HoodieCompactionHandler compactionHandler,
         .withOperationField(config.allowOperationMetadataField())
         .withPartition(operation.getPartitionPath())
         .withUseScanV2(executionHelper.useScanV2(config))
+        .withRecordMerger(config.getRecordMerger())
         .build();
 
     Option<HoodieBaseFile> oldDataFileOpt =

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/RunCompactionActionExecutor.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -46,7 +45,7 @@
 import static org.apache.hudi.common.util.ValidationUtils.checkArgument;
 
 @SuppressWarnings("checkstyle:LineLength")
-public class RunCompactionActionExecutor<T extends HoodieRecordPayload> extends
+public class RunCompactionActionExecutor<T> extends
     BaseActionExecutor<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>, HoodieWriteMetadata<HoodieData<WriteStatus>>> {
 
   private final HoodieCompactor compactor;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.common.engine.EngineType;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
@@ -52,7 +51,7 @@
 
 import static org.apache.hudi.common.util.ValidationUtils.checkArgument;
 
-public class ScheduleCompactionActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieCompactionPlan>> {
+public class ScheduleCompactionActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieCompactionPlan>> {
 
   private static final Logger LOG = LogManager.getLogger(ScheduleCompactionActionExecutor.class);
   private WriteOperationType operationType;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/plan/generators/HoodieLogCompactionPlanGenerator.java
Patch:
@@ -92,6 +92,7 @@ private boolean isFileSliceEligibleForLogCompaction(FileSlice fileSlice, String
         .withLatestInstantTime(maxInstantTime)
         .withBufferSize(writeConfig.getMaxDFSStreamBufferSize())
         .withUseScanV2(true)
+        .withRecordMerger(writeConfig.getRecordMerger())
         .build();
     scanner.scanInternal(Option.empty(), true);
     int totalBlocks = scanner.getCurrentInstantLogBlocks().size();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java
Patch:
@@ -28,7 +28,6 @@
 import org.apache.hudi.client.transaction.TransactionManager;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
@@ -82,7 +81,7 @@
  * Reads the index plan and executes the plan.
  * It also reconciles updates on data timeline while indexing was in progress.
  */
-public class RunIndexActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieIndexCommitMetadata>> {
+public class RunIndexActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieIndexCommitMetadata>> {
 
   private static final Logger LOG = LogManager.getLogger(RunIndexActionExecutor.class);
   private static final Integer INDEX_COMMIT_METADATA_VERSION_1 = 1;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/ScheduleIndexActionExecutor.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.avro.model.HoodieIndexPlan;
 import org.apache.hudi.client.transaction.TransactionManager;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
@@ -60,7 +59,7 @@
  * 3. Initialize file groups for the enabled partition types within a transaction.
  * </li>
  */
-public class ScheduleIndexActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieIndexPlan>> {
+public class ScheduleIndexActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieIndexPlan>> {
 
   private static final Logger LOG = LogManager.getLogger(ScheduleIndexActionExecutor.class);
   private static final Integer INDEX_PLAN_VERSION_1 = 1;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/BaseRestoreActionExecutor.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.avro.model.HoodieRollbackMetadata;
 import org.apache.hudi.client.transaction.TransactionManager;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -49,7 +48,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
-public abstract class BaseRestoreActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, HoodieRestoreMetadata> {
+public abstract class BaseRestoreActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I, K, O, HoodieRestoreMetadata> {
 
   private static final Logger LOG = LogManager.getLogger(BaseRestoreActionExecutor.class);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/CopyOnWriteRestoreActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 
 import org.apache.hudi.avro.model.HoodieRollbackMetadata;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -30,7 +29,7 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.rollback.CopyOnWriteRollbackActionExecutor;
 
-public class CopyOnWriteRestoreActionExecutor<T extends HoodieRecordPayload, I, K, O>
+public class CopyOnWriteRestoreActionExecutor<T, I, K, O>
     extends BaseRestoreActionExecutor<T, I, K, O> {
   public CopyOnWriteRestoreActionExecutor(HoodieEngineContext context,
                                           HoodieWriteConfig config,

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/MergeOnReadRestoreActionExecutor.java
Patch:
@@ -21,15 +21,14 @@
 
 import org.apache.hudi.avro.model.HoodieRollbackMetadata;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.rollback.MergeOnReadRollbackActionExecutor;
 
-public class MergeOnReadRestoreActionExecutor<T extends HoodieRecordPayload, I, K, O>
+public class MergeOnReadRestoreActionExecutor<T, I, K, O>
     extends BaseRestoreActionExecutor<T, I, K, O> {
   public MergeOnReadRestoreActionExecutor(HoodieEngineContext context, HoodieWriteConfig config, HoodieTable<T, I, K, O> table,
                                           String instantTime, String restoreInstantTime) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.HoodieRollbackStat;
 import org.apache.hudi.common.bootstrap.index.BootstrapIndex;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -50,7 +49,7 @@
 import java.util.Objects;
 import java.util.stream.Collectors;
 
-public abstract class BaseRollbackActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, HoodieRollbackMetadata> {
+public abstract class BaseRollbackActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I, K, O, HoodieRollbackMetadata> {
 
   private static final Logger LOG = LogManager.getLogger(BaseRollbackActionExecutor.class);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackPlanActionExecutor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.avro.model.HoodieRollbackPlan;
 import org.apache.hudi.avro.model.HoodieRollbackRequest;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
@@ -44,7 +43,7 @@
  * Base rollback plan action executor to assist in scheduling rollback requests. This phase serialized {@link HoodieRollbackPlan}
  * to rollback.requested instant.
  */
-public class BaseRollbackPlanActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieRollbackPlan>> {
+public class BaseRollbackPlanActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieRollbackPlan>> {
 
   private static final Logger LOG = LogManager.getLogger(BaseRollbackPlanActionExecutor.class);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/CopyOnWriteRollbackActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.avro.model.HoodieRollbackPlan;
 import org.apache.hudi.common.HoodieRollbackStat;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.HoodieTimer;
@@ -34,7 +33,7 @@
 import java.util.ArrayList;
 import java.util.List;
 
-public class CopyOnWriteRollbackActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseRollbackActionExecutor<T, I, K, O> {
+public class CopyOnWriteRollbackActionExecutor<T, I, K, O> extends BaseRollbackActionExecutor<T, I, K, O> {
 
   private static final Logger LOG = LogManager.getLogger(CopyOnWriteRollbackActionExecutor.class);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/MarkerBasedRollbackStrategy.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieLogFile;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.IOType;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
@@ -48,7 +47,7 @@
 /**
  * Performs rollback using marker files generated during the write..
  */
-public class MarkerBasedRollbackStrategy<T extends HoodieRecordPayload, I, K, O> implements BaseRollbackPlanActionExecutor.RollbackStrategy {
+public class MarkerBasedRollbackStrategy<T, I, K, O> implements BaseRollbackPlanActionExecutor.RollbackStrategy {
 
   private static final Logger LOG = LogManager.getLogger(MarkerBasedRollbackStrategy.class);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/MergeOnReadRollbackActionExecutor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.avro.model.HoodieRollbackPlan;
 import org.apache.hudi.common.HoodieRollbackStat;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.HoodieTimer;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -34,7 +33,7 @@
 import java.util.ArrayList;
 import java.util.List;
 
-public class MergeOnReadRollbackActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseRollbackActionExecutor<T, I, K, O> {
+public class MergeOnReadRollbackActionExecutor<T, I, K, O> extends BaseRollbackActionExecutor<T, I, K, O> {
 
   private static final Logger LOG = LogManager.getLogger(MergeOnReadRollbackActionExecutor.class);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/RestorePlanActionExecutor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.avro.model.HoodieInstantInfo;
 import org.apache.hudi.avro.model.HoodieRestorePlan;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -45,7 +44,7 @@
 /**
  * Plans the restore action and add a restore.requested meta file to timeline.
  */
-public class RestorePlanActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieRestorePlan>> {
+public class RestorePlanActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieRestorePlan>> {
 
 
   private static final Logger LOG = LogManager.getLogger(RestorePlanActionExecutor.class);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/savepoint/SavepointActionExecutor.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieBaseFile;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
@@ -44,7 +43,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
-public class SavepointActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, HoodieSavepointMetadata> {
+public class SavepointActionExecutor<T, I, K, O> extends BaseActionExecutor<T, I, K, O, HoodieSavepointMetadata> {
 
   private static final Logger LOG = LogManager.getLogger(SavepointActionExecutor.class);
 

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/avro/TestHoodieAvroParquetWriter.java
Patch:
@@ -71,8 +71,8 @@ public void testProperWriting() throws IOException {
 
     Path filePath = new Path(tmpDir.resolve("test.parquet").toAbsolutePath().toString());
 
-    try (HoodieAvroParquetWriter<GenericRecord> writer =
-        new HoodieAvroParquetWriter<>(filePath, parquetConfig, "001", new DummyTaskContextSupplier(), true)) {
+    try (HoodieAvroParquetWriter writer =
+        new HoodieAvroParquetWriter(filePath, parquetConfig, "001", new DummyTaskContextSupplier(), true)) {
       for (GenericRecord record : records) {
         writer.writeAvro((String) record.get("_row_key"), record);
       }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -27,7 +27,6 @@
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.TableServiceType;
@@ -86,7 +85,7 @@
  * @param <T> type of the payload
  */
 @SuppressWarnings("checkstyle:LineLength")
-public class HoodieFlinkWriteClient<T extends HoodieRecordPayload> extends
+public class HoodieFlinkWriteClient<T> extends
     BaseHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {
 
   private static final Logger LOG = LoggerFactory.getLogger(HoodieFlinkWriteClient.class);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/FlinkSizeBasedClusteringPlanStrategy.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -48,7 +47,7 @@
  * 1) Creates clustering groups based on max size allowed per group.
  * 2) Excludes files that are greater than 'small.file.limit' from clustering plan.
  */
-public class FlinkSizeBasedClusteringPlanStrategy<T extends HoodieRecordPayload<T>>
+public class FlinkSizeBasedClusteringPlanStrategy<T>
     extends PartitionAwareClusteringPlanStrategy<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {
   private static final Logger LOG = LogManager.getLogger(FlinkSizeBasedClusteringPlanStrategy.class);
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java
Patch:
@@ -26,7 +26,6 @@
 import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieIndexException;
 import org.apache.hudi.table.HoodieTable;
@@ -37,7 +36,7 @@
 /**
  * Base flink implementation of {@link HoodieIndex}.
  */
-public abstract class FlinkHoodieIndex<T extends HoodieRecordPayload> extends HoodieIndex<List<HoodieRecord<T>>, List<WriteStatus>> {
+public abstract class FlinkHoodieIndex<T> extends HoodieIndex<List<HoodieRecord<T>>, List<WriteStatus>> {
   protected FlinkHoodieIndex(HoodieWriteConfig config) {
     super(config);
   }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/ExplicitWriteHandleFactory.java
Patch:
@@ -19,14 +19,13 @@
 package org.apache.hudi.io;
 
 import org.apache.hudi.common.engine.TaskContextSupplier;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
 /**
  * Create handle factory for Flink writer, use the specified write handle directly.
  */
-public class ExplicitWriteHandleFactory<T extends HoodieRecordPayload, I, K, O>
+public class ExplicitWriteHandleFactory<T, I, K, O>
     extends WriteHandleFactory<T, I, K, O> {
   private final HoodieWriteHandle<T, I, K, O> writeHandle;
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkAppendHandle.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.marker.WriteMarkers;
@@ -44,7 +43,7 @@
  * <p>The back-up writer may rollover on condition(for e.g, the filesystem does not support append
  * or the file size hits the configured threshold).
  */
-public class FlinkAppendHandle<T extends HoodieRecordPayload, I, K, O>
+public class FlinkAppendHandle<T, I, K, O>
     extends HoodieAppendHandle<T, I, K, O> implements MiniBatchHandle {
 
   private static final Logger LOG = LoggerFactory.getLogger(FlinkAppendHandle.class);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkCreateHandle.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
@@ -48,7 +47,7 @@
  *
  * @see FlinkMergeAndReplaceHandle
  */
-public class FlinkCreateHandle<T extends HoodieRecordPayload, I, K, O>
+public class FlinkCreateHandle<T, I, K, O>
     extends HoodieCreateHandle<T, I, K, O> implements MiniBatchHandle {
 
   private static final Logger LOG = LogManager.getLogger(FlinkCreateHandle.class);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkMergeAndReplaceHandle.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
@@ -50,7 +49,7 @@
  * then closes the file and rename to the old file name,
  * behaves like the new data buffer are appended to the old file.
  */
-public class FlinkMergeAndReplaceHandle<T extends HoodieRecordPayload, I, K, O>
+public class FlinkMergeAndReplaceHandle<T, I, K, O>
     extends HoodieMergeHandle<T, I, K, O>
     implements MiniBatchHandle {
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkMergeHandle.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
@@ -51,7 +50,7 @@
  *
  * @see FlinkMergeAndReplaceHandle
  */
-public class FlinkMergeHandle<T extends HoodieRecordPayload, I, K, O>
+public class FlinkMergeHandle<T, I, K, O>
     extends HoodieMergeHandle<T, I, K, O>
     implements MiniBatchHandle {
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/ExplicitWriteHandleTable.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.io.HoodieWriteHandle;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
@@ -32,7 +31,7 @@
  * HoodieTable that need to pass in the
  * {@link org.apache.hudi.io.HoodieWriteHandle} explicitly.
  */
-public interface ExplicitWriteHandleTable<T extends HoodieRecordPayload> {
+public interface ExplicitWriteHandleTable<T> {
   /**
    * Upsert a batch of new records into Hoodie table at the supplied instantTime.
    *

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java
Patch:
@@ -35,7 +35,6 @@
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
@@ -83,7 +82,7 @@
  * <p>
  * UPDATES - Produce a new version of the file, just replacing the updated records with new values
  */
-public class HoodieFlinkCopyOnWriteTable<T extends HoodieRecordPayload>
+public class HoodieFlinkCopyOnWriteTable<T>
     extends HoodieFlinkTable<T> implements HoodieCompactionHandler<T> {
 
   private static final Logger LOG = LoggerFactory.getLogger(HoodieFlinkCopyOnWriteTable.class);
@@ -402,7 +401,7 @@ protected HoodieMergeHandle getUpdateHandle(String instantTime, String partition
   @Override
   public Iterator<List<WriteStatus>> handleInsert(
       String instantTime, String partitionPath, String fileId,
-      Map<String, HoodieRecord<? extends HoodieRecordPayload>> recordMap) {
+      Map<String, HoodieRecord<?>> recordMap) {
     HoodieCreateHandle<?, ?, ?, ?> createHandle =
         new HoodieCreateHandle(config, instantTime, this, partitionPath, fileId, recordMap, taskContextSupplier);
     createHandle.write();

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
@@ -48,7 +47,7 @@
 /**
  * Flink MERGE_ON_READ table.
  */
-public class HoodieFlinkMergeOnReadTable<T extends HoodieRecordPayload>
+public class HoodieFlinkMergeOnReadTable<T>
     extends HoodieFlinkCopyOnWriteTable<T> {
 
   HoodieFlinkMergeOnReadTable(

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
@@ -67,7 +66,7 @@
  * <p>Computing the records batch locations all at a time is a pressure to the engine,
  * we should avoid that in streaming system.
  */
-public abstract class BaseFlinkCommitActionExecutor<T extends HoodieRecordPayload> extends
+public abstract class BaseFlinkCommitActionExecutor<T> extends
     BaseCommitActionExecutor<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>, HoodieWriteMetadata> {
 
   private static final Logger LOG = LogManager.getLogger(BaseFlinkCommitActionExecutor.class);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeleteCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieKey;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.HoodieWriteHandle;
@@ -33,7 +32,7 @@
 /**
  * Flink delete commit action executor.
  */
-public class FlinkDeleteCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseFlinkCommitActionExecutor<T> {
+public class FlinkDeleteCommitActionExecutor<T> extends BaseFlinkCommitActionExecutor<T> {
   private final List<HoodieKey> keys;
 
   public FlinkDeleteCommitActionExecutor(HoodieEngineContext context,

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.HoodieWriteHandle;
@@ -33,7 +32,7 @@
 /**
  * Flink insert commit action executor.
  */
-public class FlinkInsertCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseFlinkCommitActionExecutor<T> {
+public class FlinkInsertCommitActionExecutor<T> extends BaseFlinkCommitActionExecutor<T> {
 
   private List<HoodieRecord<T>> inputRecords;
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertOverwriteCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -34,7 +33,7 @@
 /**
  * Flink INSERT OVERWRITE commit action executor.
  */
-public class FlinkInsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class FlinkInsertOverwriteCommitActionExecutor<T>
     extends BaseFlinkCommitActionExecutor<T> {
 
   protected List<HoodieRecord<T>> inputRecords;

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertOverwriteTableCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.HoodieWriteHandle;
@@ -33,7 +32,7 @@
 /**
  * Flink INSERT OVERWRITE TABLE commit action executor.
  */
-public class FlinkInsertOverwriteTableCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class FlinkInsertOverwriteTableCommitActionExecutor<T>
     extends FlinkInsertOverwriteCommitActionExecutor<T> {
 
   public FlinkInsertOverwriteTableCommitActionExecutor(HoodieEngineContext context,

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertPreppedCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.HoodieWriteHandle;
@@ -33,7 +32,7 @@
 /**
  * Flink insert prepped commit action executor.
  */
-public class FlinkInsertPreppedCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseFlinkCommitActionExecutor<T> {
+public class FlinkInsertPreppedCommitActionExecutor<T> extends BaseFlinkCommitActionExecutor<T> {
 
   private final List<HoodieRecord<T>> preppedRecords;
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkUpsertCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.HoodieWriteHandle;
@@ -33,7 +32,7 @@
 /**
  * Flink upsert commit action executor.
  */
-public class FlinkUpsertCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseFlinkCommitActionExecutor<T> {
+public class FlinkUpsertCommitActionExecutor<T> extends BaseFlinkCommitActionExecutor<T> {
 
   private List<HoodieRecord<T>> inputRecords;
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkUpsertPreppedCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.HoodieWriteHandle;
@@ -33,7 +32,7 @@
 /**
  * Flink upsert prepped commit action executor.
  */
-public class FlinkUpsertPreppedCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseFlinkCommitActionExecutor<T> {
+public class FlinkUpsertPreppedCommitActionExecutor<T> extends BaseFlinkCommitActionExecutor<T> {
 
   private final List<HoodieRecord<T>> preppedRecords;
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/delta/BaseFlinkDeltaCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.execution.FlinkLazyInsertIterable;
@@ -37,7 +36,7 @@
 /**
  * Base flink delta commit action executor.
  */
-public abstract class BaseFlinkDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public abstract class BaseFlinkDeltaCommitActionExecutor<T>
     extends BaseFlinkCommitActionExecutor<T> {
 
   public BaseFlinkDeltaCommitActionExecutor(HoodieEngineContext context,

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/delta/FlinkUpsertDeltaCommitActionExecutor.java
Patch:
@@ -20,7 +20,6 @@
 
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.FlinkAppendHandle;
@@ -33,7 +32,7 @@
 /**
  * Flink upsert delta commit action executor.
  */
-public class FlinkUpsertDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class FlinkUpsertDeltaCommitActionExecutor<T>
     extends BaseFlinkDeltaCommitActionExecutor<T> {
   private final List<HoodieRecord<T>> inputRecords;
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/delta/FlinkUpsertPreppedDeltaCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.FlinkAppendHandle;
@@ -33,7 +32,7 @@
 /**
  * Flink upsert prepped delta commit action executor.
  */
-public class FlinkUpsertPreppedDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class FlinkUpsertPreppedDeltaCommitActionExecutor<T>
     extends BaseFlinkDeltaCommitActionExecutor<T> {
 
   private final List<HoodieRecord<T>> preppedRecords;

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/compact/HoodieFlinkMergeOnReadTableCompactor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -39,7 +38,7 @@
  * <p>Note: the compaction logic is invoked through the flink pipeline.
  */
 @SuppressWarnings("checkstyle:LineLength")
-public class HoodieFlinkMergeOnReadTableCompactor<T extends HoodieRecordPayload>
+public class HoodieFlinkMergeOnReadTableCompactor<T>
     extends HoodieCompactor<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {
 
   @Override

File: hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkWriteableTestTable.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.common.bloom.BloomFilter;
 import org.apache.hudi.common.bloom.BloomFilterFactory;
 import org.apache.hudi.common.bloom.BloomFilterTypeCode;
+import org.apache.hudi.common.model.HoodieAvroIndexedRecord;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
@@ -144,7 +145,7 @@ private Pair<String, HoodieLogFile> appendRecordsToLogFile(List<HoodieRecord> gr
           LOG.warn("Failed to convert record " + r.toString(), e);
           return null;
         }
-      }).collect(Collectors.toList()), header, HoodieRecord.RECORD_KEY_METADATA_FIELD));
+      }).map(HoodieAvroIndexedRecord::new).collect(Collectors.toList()), header, HoodieRecord.RECORD_KEY_METADATA_FIELD));
       return Pair.of(partitionPath, logWriter.getLogFile());
     }
   }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -49,7 +48,7 @@
 import java.util.function.BiConsumer;
 import java.util.stream.Collectors;
 
-public class HoodieJavaWriteClient<T extends HoodieRecordPayload> extends
+public class HoodieJavaWriteClient<T> extends
     BaseHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {
 
   public HoodieJavaWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/JavaSizeBasedClusteringPlanStrategy.java
Patch:
@@ -26,7 +26,6 @@
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -48,7 +47,7 @@
  * 1) Creates clustering groups based on max size allowed per group.
  * 2) Excludes files that are greater than 'small.file.limit' from clustering plan.
  */
-public class JavaSizeBasedClusteringPlanStrategy<T extends HoodieRecordPayload<T>>
+public class JavaSizeBasedClusteringPlanStrategy<T>
     extends PartitionAwareClusteringPlanStrategy<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {
   private static final Logger LOG = LogManager.getLogger(JavaSizeBasedClusteringPlanStrategy.class);
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaSortAndSizeExecutionStrategy.java
Patch:
@@ -24,8 +24,7 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.CreateHandleFactory;
 import org.apache.hudi.table.HoodieTable;
@@ -43,7 +42,7 @@
  * 1) Java execution engine.
  * 2) Uses bulk_insert to write data into new files.
  */
-public class JavaSortAndSizeExecutionStrategy<T extends HoodieRecordPayload<T>>
+public class JavaSortAndSizeExecutionStrategy<T>
     extends JavaExecutionStrategy<T> {
   private static final Logger LOG = LogManager.getLogger(JavaSortAndSizeExecutionStrategy.class);
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/execution/JavaLazyInsertIterable.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.queue.HoodieExecutor;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
@@ -35,7 +34,7 @@
 
 import static org.apache.hudi.common.util.ValidationUtils.checkState;
 
-public class JavaLazyInsertIterable<T extends HoodieRecordPayload> extends HoodieLazyInsertIterable<T> {
+public class JavaLazyInsertIterable<T> extends HoodieLazyInsertIterable<T> {
   public JavaLazyInsertIterable(Iterator<HoodieRecord<T>> recordItr,
                                 boolean areRecordsSorted,
                                 HoodieWriteConfig config,
@@ -65,7 +64,7 @@ protected List<WriteStatus> computeNext() {
     try {
       final Schema schema = new Schema.Parser().parse(hoodieConfig.getSchema());
       bufferedIteratorExecutor =
-          QueueBasedExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(), getTransformFunction(schema));
+          QueueBasedExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(), getCloningTransformer(schema));
       final List<WriteStatus> result = bufferedIteratorExecutor.execute();
       checkState(result != null && !result.isEmpty());
       return result;

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/execution/bulkinsert/JavaGlobalSortPartitioner.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.execution.bulkinsert;
 
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.table.BulkInsertPartitioner;
 
 import java.util.Comparator;
@@ -32,7 +31,7 @@
  *
  * @param <T> HoodieRecordPayload type
  */
-public class JavaGlobalSortPartitioner<T extends HoodieRecordPayload>
+public class JavaGlobalSortPartitioner<T>
     implements BulkInsertPartitioner<List<HoodieRecord<T>>> {
 
   @Override

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/execution/bulkinsert/JavaNonSortPartitioner.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.execution.bulkinsert;
 
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.table.BulkInsertPartitioner;
 
 import java.util.List;
@@ -30,7 +29,7 @@
  *
  * @param <T> HoodieRecordPayload type
  */
-public class JavaNonSortPartitioner<T extends HoodieRecordPayload>
+public class JavaNonSortPartitioner<T>
     implements BulkInsertPartitioner<List<HoodieRecord<T>>> {
 
   @Override

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieIndex.java
Patch:
@@ -26,15 +26,14 @@
 import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieIndexException;
 import org.apache.hudi.table.HoodieTable;
 
 import java.util.List;
 import java.util.stream.Collectors;
 
-public abstract class JavaHoodieIndex<T extends HoodieRecordPayload> extends HoodieIndex<List<HoodieRecord<T>>, List<WriteStatus>> {
+public abstract class JavaHoodieIndex<T> extends HoodieIndex<List<HoodieRecord<T>>, List<WriteStatus>> {
   protected JavaHoodieIndex(HoodieWriteConfig config) {
     super(config);
   }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java
Patch:
@@ -35,7 +35,6 @@
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
@@ -78,7 +77,7 @@
 import java.util.List;
 import java.util.Map;
 
-public class HoodieJavaCopyOnWriteTable<T extends HoodieRecordPayload>
+public class HoodieJavaCopyOnWriteTable<T>
     extends HoodieJavaTable<T> implements HoodieCompactionHandler<T> {
 
   private static final Logger LOG = LoggerFactory.getLogger(HoodieJavaCopyOnWriteTable.class);
@@ -300,7 +299,7 @@ protected HoodieMergeHandle getUpdateHandle(String instantTime, String partition
   @Override
   public Iterator<List<WriteStatus>> handleInsert(
       String instantTime, String partitionPath, String fileId,
-      Map<String, HoodieRecord<? extends HoodieRecordPayload>> recordMap) {
+      Map<String, HoodieRecord<?>> recordMap) {
     HoodieCreateHandle<?, ?, ?, ?> createHandle =
         new HoodieCreateHandle(config, instantTime, this, partitionPath, fileId, recordMap, taskContextSupplier);
     createHandle.write();

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaMergeOnReadTable.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.client.common.HoodieJavaEngineContext;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.Option;
@@ -38,7 +37,7 @@
 import java.util.List;
 import java.util.Map;
 
-public class HoodieJavaMergeOnReadTable<T extends HoodieRecordPayload> extends HoodieJavaCopyOnWriteTable<T> {
+public class HoodieJavaMergeOnReadTable<T> extends HoodieJavaCopyOnWriteTable<T> {
   protected HoodieJavaMergeOnReadTable(HoodieWriteConfig config, HoodieEngineContext context, HoodieTableMetaClient metaClient) {
     super(config, context, metaClient);
   }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/cluster/JavaExecuteClusteringCommitActionExecutor.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.ClusteringUtils;
@@ -36,7 +35,7 @@
 
 import java.util.List;
 
-public class JavaExecuteClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class JavaExecuteClusteringCommitActionExecutor<T>
     extends BaseJavaCommitActionExecutor<T> {
 
   private final HoodieClusteringPlan clusteringPlan;

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -63,7 +62,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
-public abstract class BaseJavaCommitActionExecutor<T extends HoodieRecordPayload> extends
+public abstract class BaseJavaCommitActionExecutor<T> extends
     BaseCommitActionExecutor<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>, HoodieWriteMetadata> {
 
   private static final Logger LOG = LogManager.getLogger(BaseJavaCommitActionExecutor.class);

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.HoodieJavaEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -33,7 +32,7 @@
 import java.util.List;
 import java.util.Map;
 
-public class JavaBulkInsertCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseJavaCommitActionExecutor<T> {
+public class JavaBulkInsertCommitActionExecutor<T> extends BaseJavaCommitActionExecutor<T> {
 
   private final List<HoodieRecord<T>> inputRecords;
   private final Option<BulkInsertPartitioner> bulkInsertPartitioner;

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ReflectionUtils;
@@ -45,7 +44,7 @@
  * @param <T>
  */
 @SuppressWarnings("checkstyle:LineLength")
-public class JavaBulkInsertHelper<T extends HoodieRecordPayload, R> extends BaseBulkInsertHelper<T, List<HoodieRecord<T>>,
+public class JavaBulkInsertHelper<T, R> extends BaseBulkInsertHelper<T, List<HoodieRecord<T>>,
     List<HoodieKey>, List<WriteStatus>, R> {
 
   private JavaBulkInsertHelper() {

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertPreppedCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.HoodieJavaEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -32,7 +31,7 @@
 
 import java.util.List;
 
-public class JavaBulkInsertPreppedCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class JavaBulkInsertPreppedCommitActionExecutor<T>
     extends BaseJavaCommitActionExecutor<T> {
 
   private final List<HoodieRecord<T>> preppedInputRecord;

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaDeleteCommitActionExecutor.java
Patch:
@@ -21,15 +21,14 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieKey;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
 import java.util.List;
 
-public class JavaDeleteCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseJavaCommitActionExecutor<T> {
+public class JavaDeleteCommitActionExecutor<T> extends BaseJavaCommitActionExecutor<T> {
   private final List<HoodieKey> keys;
 
   public JavaDeleteCommitActionExecutor(HoodieEngineContext context,

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertCommitActionExecutor.java
Patch:
@@ -21,15 +21,14 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
 import java.util.List;
 
-public class JavaInsertCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseJavaCommitActionExecutor<T> {
+public class JavaInsertCommitActionExecutor<T> extends BaseJavaCommitActionExecutor<T> {
 
   private List<HoodieRecord<T>> inputRecords;
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.collection.Pair;
@@ -33,7 +32,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
-public class JavaInsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class JavaInsertOverwriteCommitActionExecutor<T>
     extends BaseJavaCommitActionExecutor<T> {
 
   private final List<HoodieRecord<T>> inputRecords;

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteTableCommitActionExecutor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -34,7 +33,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
-public class JavaInsertOverwriteTableCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class JavaInsertOverwriteTableCommitActionExecutor<T>
     extends JavaInsertOverwriteCommitActionExecutor<T> {
 
   public JavaInsertOverwriteTableCommitActionExecutor(HoodieEngineContext context,

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertPreppedCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.HoodieJavaEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
@@ -30,7 +29,7 @@
 
 import java.util.List;
 
-public class JavaInsertPreppedCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class JavaInsertPreppedCommitActionExecutor<T>
     extends BaseJavaCommitActionExecutor<T> {
 
   private final List<HoodieRecord<T>> preppedRecords;

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertCommitActionExecutor.java
Patch:
@@ -21,15 +21,14 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
 import java.util.List;
 
-public class JavaUpsertCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseJavaCommitActionExecutor<T> {
+public class JavaUpsertCommitActionExecutor<T> extends BaseJavaCommitActionExecutor<T> {
 
   private List<HoodieRecord<T>> inputRecords;
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecordLocation;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -52,7 +51,7 @@
 /**
  * Packs incoming records to be upserted, into buckets.
  */
-public class JavaUpsertPartitioner<T extends HoodieRecordPayload<T>> implements Partitioner  {
+public class JavaUpsertPartitioner<T> implements Partitioner  {
 
   private static final Logger LOG = LogManager.getLogger(JavaUpsertPartitioner.class);
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPreppedCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.HoodieJavaEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
@@ -30,7 +29,7 @@
 
 import java.util.List;
 
-public class JavaUpsertPreppedCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class JavaUpsertPreppedCommitActionExecutor<T>
     extends BaseJavaCommitActionExecutor<T> {
 
   private final List<HoodieRecord<T>> preppedRecords;

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/compact/HoodieJavaMergeOnReadTableCompactor.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -37,7 +36,7 @@
  * compactions, passes it through a CompactionFilter and executes all the compactions and
  * writes a new version of base files and make a normal commit.
  */
-public class HoodieJavaMergeOnReadTableCompactor<T extends HoodieRecordPayload>
+public class HoodieJavaMergeOnReadTableCompactor<T>
     extends HoodieCompactor<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {
 
   @Override

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/deltacommit/BaseJavaDeltaCommitActionExecutor.java
Patch:
@@ -20,13 +20,12 @@
 package org.apache.hudi.table.action.deltacommit;
 
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.commit.BaseJavaCommitActionExecutor;
 
-public abstract class BaseJavaDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseJavaCommitActionExecutor<T> {
+public abstract class BaseJavaDeltaCommitActionExecutor<T> extends BaseJavaCommitActionExecutor<T> {
 
   public BaseJavaDeltaCommitActionExecutor(HoodieEngineContext context, HoodieWriteConfig config, HoodieTable table,
                                            String instantTime, WriteOperationType operationType) {

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/deltacommit/JavaUpsertPreppedDeltaCommitActionExecutor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.HoodieJavaEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
@@ -41,7 +40,7 @@
 import java.util.LinkedList;
 import java.util.List;
 
-public class JavaUpsertPreppedDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseJavaDeltaCommitActionExecutor<T> {
+public class JavaUpsertPreppedDeltaCommitActionExecutor<T> extends BaseJavaDeltaCommitActionExecutor<T> {
 
   private static final Logger LOG = LogManager.getLogger(JavaUpsertPreppedDeltaCommitActionExecutor.class);
 

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/execution/bulkinsert/TestJavaBulkInsertInternalPartitioner.java
Patch:
@@ -20,6 +20,7 @@
 package org.apache.hudi.execution.bulkinsert;
 
 import org.apache.hudi.avro.HoodieAvroUtils;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
@@ -70,7 +71,7 @@ public void testCustomColumnSortPartitioner(String sortColumnString) throws Exce
 
   private Comparator<HoodieRecord> getCustomColumnComparator(Schema schema, String[] sortColumns) {
     return Comparator.comparing(
-        record -> HoodieAvroUtils.getRecordColumnValues(record, sortColumns, schema, false).toString());
+        record -> HoodieAvroUtils.getRecordColumnValues((HoodieAvroRecord)record, sortColumns, schema, false).toString());
   }
 
   private void verifyRecordAscendingOrder(List<HoodieRecord> records,

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java
Patch:
@@ -35,7 +35,7 @@
 import org.apache.hudi.common.util.BaseFileUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.hadoop.HoodieParquetInputFormat;
 import org.apache.hudi.hadoop.utils.HoodieHiveUtils;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieReadClient.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.client;
 
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 
@@ -31,7 +30,7 @@
  * @deprecated This. Use {@link SparkRDDReadClient instead.}
  */
 @Deprecated
-public class HoodieReadClient<T extends HoodieRecordPayload<T>> extends SparkRDDReadClient<T> {
+public class HoodieReadClient<T> extends SparkRDDReadClient<T> {
 
   public HoodieReadClient(HoodieSparkEngineContext context, String basePath) {
     super(context, basePath);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkClusteringClient.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 
@@ -37,7 +36,7 @@
 /**
  * Async clustering client for Spark datasource.
  */
-public class HoodieSparkClusteringClient<T extends HoodieRecordPayload> extends
+public class HoodieSparkClusteringClient<T> extends
     BaseClusterer<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {
 
   private static final Logger LOG = LogManager.getLogger(HoodieSparkClusteringClient.class);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkCompactor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
@@ -34,7 +33,7 @@
 
 import java.util.List;
 
-public class HoodieSparkCompactor<T extends HoodieRecordPayload> extends BaseCompactor<T,
+public class HoodieSparkCompactor<T> extends BaseCompactor<T,
     JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {
   private static final Logger LOG = LogManager.getLogger(HoodieSparkCompactor.class);
   private transient HoodieEngineContext context;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDReadClient.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.CompactionUtils;
 import org.apache.hudi.common.util.Option;
@@ -59,7 +58,7 @@
 /**
  * Provides an RDD based API for accessing/filtering Hoodie tables, based on keys.
  */
-public class SparkRDDReadClient<T extends HoodieRecordPayload<T>> implements Serializable {
+public class SparkRDDReadClient<T> implements Serializable {
 
   private static final long serialVersionUID = 1L;
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkSingleFileSortPlanStrategy.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.avro.model.HoodieClusteringGroup;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.FileSlice;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
@@ -36,7 +35,7 @@
  * In this strategy, clustering group for each partition is built in the same way as {@link SparkSizeBasedClusteringPlanStrategy}.
  * The difference is that the output groups is 1 and file group id remains the same.
  */
-public class SparkSingleFileSortPlanStrategy<T extends HoodieRecordPayload<T>>
+public class SparkSingleFileSortPlanStrategy<T>
     extends SparkSizeBasedClusteringPlanStrategy<T> {
 
   public SparkSingleFileSortPlanStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkSizeBasedClusteringPlanStrategy.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -49,7 +48,7 @@
  * 1) Creates clustering groups based on max size allowed per group.
  * 2) Excludes files that are greater than 'small.file.limit' from clustering plan.
  */
-public class SparkSizeBasedClusteringPlanStrategy<T extends HoodieRecordPayload<T>>
+public class SparkSizeBasedClusteringPlanStrategy<T>
     extends PartitionAwareClusteringPlanStrategy<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {
   private static final Logger LOG = LogManager.getLogger(SparkSizeBasedClusteringPlanStrategy.class);
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkSingleFileSortExecutionStrategy.java
Patch:
@@ -25,8 +25,7 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieClusteringException;
 import org.apache.hudi.io.SingleFileHandleCreateFactory;
@@ -46,7 +45,7 @@
  * This strategy is similar to {@link SparkSortAndSizeExecutionStrategy} with the difference being that
  * there should be only one large file group per clustering group.
  */
-public class SparkSingleFileSortExecutionStrategy<T extends HoodieRecordPayload<T>>
+public class SparkSingleFileSortExecutionStrategy<T>
     extends MultipleSparkJobExecutionStrategy<T> {
 
   private static final Logger LOG = LogManager.getLogger(SparkSingleFileSortExecutionStrategy.class);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkSortAndSizeExecutionStrategy.java
Patch:
@@ -24,8 +24,7 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.CreateHandleFactory;
 import org.apache.hudi.table.HoodieTable;
@@ -45,7 +44,7 @@
  * 1) Spark execution engine.
  * 2) Uses bulk_insert to write data into new files.
  */
-public class SparkSortAndSizeExecutionStrategy<T extends HoodieRecordPayload<T>>
+public class SparkSortAndSizeExecutionStrategy<T>
     extends MultipleSparkJobExecutionStrategy<T> {
   private static final Logger LOG = LogManager.getLogger(SparkSortAndSizeExecutionStrategy.class);
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/BaseSparkUpdateStrategy.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.cluster.strategy.UpdateStrategy;
 
@@ -33,7 +32,7 @@
  * Spark base update strategy, write records to the file groups which are in clustering
  * need to check. Spark relate implementations should extend this base class.
  */
-public abstract class BaseSparkUpdateStrategy<T extends HoodieRecordPayload<T>> extends UpdateStrategy<T, HoodieData<HoodieRecord<T>>> {
+public abstract class BaseSparkUpdateStrategy<T> extends UpdateStrategy<T, HoodieData<HoodieRecord<T>>> {
 
   public BaseSparkUpdateStrategy(HoodieEngineContext engineContext, HoodieTable table,
                                  Set<HoodieFileGroupId> fileGroupsInPendingClustering) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkAllowUpdateStrategy.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.table.HoodieTable;
 
@@ -33,7 +32,7 @@
 /**
  * Allow ingestion commits during clustering job.
  */
-public class SparkAllowUpdateStrategy<T extends HoodieRecordPayload<T>> extends BaseSparkUpdateStrategy<T> {
+public class SparkAllowUpdateStrategy<T> extends BaseSparkUpdateStrategy<T> {
 
   public SparkAllowUpdateStrategy(
       HoodieEngineContext engineContext, HoodieTable table, Set<HoodieFileGroupId> fileGroupsInPendingClustering) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieClusteringUpdateException;
 import org.apache.hudi.table.HoodieTable;
@@ -38,7 +37,7 @@
  * Update strategy based on following.
  * if some file groups have update record, throw exception
  */
-public class SparkRejectUpdateStrategy<T extends HoodieRecordPayload<T>> extends BaseSparkUpdateStrategy<T> {
+public class SparkRejectUpdateStrategy<T> extends BaseSparkUpdateStrategy<T> {
   private static final Logger LOG = LogManager.getLogger(SparkRejectUpdateStrategy.class);
 
   public SparkRejectUpdateStrategy(HoodieEngineContext engineContext, HoodieTable table, Set<HoodieFileGroupId> fileGroupsInPendingClustering) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SparkPreCommitValidator.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.util.HoodieTimer;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -42,7 +41,7 @@
 /**
  * Validator can be configured pre-commit. 
  */
-public abstract class SparkPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends HoodieData<WriteStatus>> {
+public abstract class SparkPreCommitValidator<T, I, K, O extends HoodieData<WriteStatus>> {
   private static final Logger LOG = LogManager.getLogger(SparkPreCommitValidator.class);
 
   private HoodieSparkTable<T> table;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQueryEqualityPreCommitValidator.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodiePreCommitValidatorConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieValidationException;
@@ -40,7 +39,7 @@
  * 
  * Expects both queries to return same result.
  */
-public class SqlQueryEqualityPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends HoodieData<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
+public class SqlQueryEqualityPreCommitValidator<T, I, K, O extends HoodieData<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
 
   private static final Logger LOG = LogManager.getLogger(SqlQueryEqualityPreCommitValidator.class);
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQueryInequalityPreCommitValidator.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodiePreCommitValidatorConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieValidationException;
@@ -40,7 +39,7 @@
  * <p>
  * Expects query results do not match.
  */
-public class SqlQueryInequalityPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends HoodieData<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
+public class SqlQueryInequalityPreCommitValidator<T, I, K, O extends HoodieData<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
   private static final Logger LOG = LogManager.getLogger(SqlQueryInequalityPreCommitValidator.class);
 
   public SqlQueryInequalityPreCommitValidator(HoodieSparkTable<T> table, HoodieEngineContext engineContext, HoodieWriteConfig config) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQueryPreCommitValidator.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieValidationException;
@@ -42,7 +41,7 @@
 /**
  * Validator framework to run sql queries and compare table state at different locations.
  */
-public abstract class SqlQueryPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends HoodieData<WriteStatus>> extends SparkPreCommitValidator<T, I, K, O> {
+public abstract class SqlQueryPreCommitValidator<T, I, K, O extends HoodieData<WriteStatus>> extends SparkPreCommitValidator<T, I, K, O> {
   private static final Logger LOG = LogManager.getLogger(SqlQueryPreCommitValidator.class);
   private static final AtomicInteger TABLE_COUNTER = new AtomicInteger(0);
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQuerySingleResultPreCommitValidator.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodiePreCommitValidatorConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieValidationException;
@@ -40,7 +39,7 @@
  * <p>
  * Example configuration: "query1#expectedResult1;query2#expectedResult2;"
  */
-public class SqlQuerySingleResultPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends HoodieData<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
+public class SqlQuerySingleResultPreCommitValidator<T, I, K, O extends HoodieData<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
   private static final Logger LOG = LogManager.getLogger(SqlQueryInequalityPreCommitValidator.class);
 
   public SqlQuerySingleResultPreCommitValidator(HoodieSparkTable<T> table, HoodieEngineContext engineContext, HoodieWriteConfig config) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/SparkLazyInsertIterable.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.queue.HoodieExecutor;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
@@ -37,7 +36,7 @@
 
 import static org.apache.hudi.common.util.ValidationUtils.checkState;
 
-public class SparkLazyInsertIterable<T extends HoodieRecordPayload> extends HoodieLazyInsertIterable<T> {
+public class SparkLazyInsertIterable<T> extends HoodieLazyInsertIterable<T> {
 
   private boolean useWriterSchema;
 
@@ -88,7 +87,7 @@ protected List<WriteStatus> computeNext() {
       }
 
       bufferedIteratorExecutor = QueueBasedExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(),
-          getTransformFunction(schema, hoodieConfig), hoodieTable.getPreExecuteRunnable());
+          getCloningTransformer(schema, hoodieConfig), hoodieTable.getPreExecuteRunnable());
 
       final List<WriteStatus> result = bufferedIteratorExecutor.execute();
       checkState(result != null && !result.isEmpty());

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java
Patch:
@@ -20,7 +20,6 @@
 
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.execution.SparkLazyInsertIterable;
 import org.apache.hudi.io.WriteHandleFactory;
@@ -35,7 +34,7 @@
 /**
  * Map function that handles a stream of HoodieRecords.
  */
-public class BulkInsertMapFunction<T extends HoodieRecordPayload>
+public class BulkInsertMapFunction<T>
     implements Function2<Integer, Iterator<HoodieRecord<T>>, Iterator<List<WriteStatus>>> {
 
   private String instantTime;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/GlobalSortPartitioner.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.execution.bulkinsert;
 
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.table.BulkInsertPartitioner;
@@ -35,7 +34,7 @@
  *
  * @param <T> HoodieRecordPayload type
  */
-public class GlobalSortPartitioner<T extends HoodieRecordPayload>
+public class GlobalSortPartitioner<T>
     implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {
 
   private final boolean shouldPopulateMetaFields;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/NonSortPartitioner.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.execution.bulkinsert;
 
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.table.BulkInsertPartitioner;
 
 import org.apache.spark.api.java.JavaRDD;
@@ -36,7 +35,7 @@
  *
  * @param <T> HoodieRecordPayload type
  */
-public class NonSortPartitioner<T extends HoodieRecordPayload>
+public class NonSortPartitioner<T>
     implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {
 
   private final boolean enforceNumOutputPartitions;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDBucketIndexPartitioner.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.execution.bulkinsert;
 
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.table.BulkInsertPartitioner;
 
 import org.apache.spark.api.java.JavaRDD;
@@ -28,6 +27,6 @@
  * Abstract of bucket index bulk_insert partitioner
  * TODO implement partitioner for SIMPLE BUCKET INDEX
  */
-public abstract class RDDBucketIndexPartitioner<T extends HoodieRecordPayload>
+public abstract class RDDBucketIndexPartitioner<T>
     implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDPartitionSortPartitioner.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.execution.bulkinsert;
 
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.table.BulkInsertPartitioner;
@@ -41,7 +40,7 @@
  *
  * @param <T> HoodieRecordPayload type
  */
-public class RDDPartitionSortPartitioner<T extends HoodieRecordPayload>
+public class RDDPartitionSortPartitioner<T>
     implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {
 
   private final boolean shouldPopulateMetaFields;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/SparkHoodieIndex.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.data.HoodieJavaRDD;
 import org.apache.hudi.exception.HoodieIndexException;
@@ -34,7 +33,7 @@
 import org.apache.spark.api.java.JavaRDD;
 
 @SuppressWarnings("checkstyle:LineLength")
-public abstract class SparkHoodieIndex<T extends HoodieRecordPayload<T>>
+public abstract class SparkHoodieIndex<T>
     extends HoodieIndex<JavaRDD<HoodieRecord<T>>, JavaRDD<WriteStatus>> {
   protected SparkHoodieIndex(HoodieWriteConfig config) {
     super(config);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieInternalRowFileWriterFactory.java
Patch:
@@ -67,7 +67,7 @@ private static HoodieInternalRowFileWriter newParquetInternalRowFileWriter(Path
   )
       throws IOException {
     HoodieRowParquetWriteSupport writeSupport =
-            new HoodieRowParquetWriteSupport(table.getHadoopConf(), structType, bloomFilterOpt, writeConfig);
+            new HoodieRowParquetWriteSupport(table.getHadoopConf(), structType, bloomFilterOpt, writeConfig.getStorageConfig());
 
     return new HoodieInternalRowParquetWriter(
         path,

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/NonpartitionedKeyGenerator.java
Patch:
@@ -61,13 +61,13 @@ public String getRecordKey(GenericRecord record) {
   @Override
   public String getRecordKey(Row row) {
     tryInitRowAccessor(row.schema());
-    return combineRecordKey(rowAccessor.getRecordKeyParts(row));
+    return combineRecordKey(getRecordKeyFieldNames(), Arrays.asList(rowAccessor.getRecordKeyParts(row)));
   }
 
   @Override
   public UTF8String getRecordKey(InternalRow internalRow, StructType schema) {
     tryInitRowAccessor(schema);
-    return combineRecordKeyUnsafe(rowAccessor.getRecordKeyParts(internalRow));
+    return combineRecordKeyUnsafe(getRecordKeyFieldNames(), Arrays.asList(rowAccessor.getRecordKeyParts(internalRow)));
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.spark.unsafe.types.UTF8String;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Objects;
 
 import static org.apache.hudi.keygen.KeyGenUtils.HUDI_DEFAULT_PARTITION_PATH;
@@ -61,13 +62,13 @@ public String getPartitionPath(GenericRecord record) {
   @Override
   public String getRecordKey(Row row) {
     tryInitRowAccessor(row.schema());
-    return combineRecordKey(rowAccessor.getRecordKeyParts(row));
+    return combineRecordKey(getRecordKeyFieldNames(), Arrays.asList(rowAccessor.getRecordKeyParts(row)));
   }
 
   @Override
   public UTF8String getRecordKey(InternalRow internalRow, StructType schema) {
     tryInitRowAccessor(schema);
-    return combineRecordKeyUnsafe(rowAccessor.getRecordKeyParts(internalRow));
+    return combineRecordKeyUnsafe(getRecordKeyFieldNames(), Arrays.asList(rowAccessor.getRecordKeyParts(internalRow)));
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java
Patch:
@@ -37,7 +37,6 @@
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -94,7 +93,7 @@
  * <p>
  * UPDATES - Produce a new version of the file, just replacing the updated records with new values
  */
-public class HoodieSparkCopyOnWriteTable<T extends HoodieRecordPayload>
+public class HoodieSparkCopyOnWriteTable<T>
     extends HoodieSparkTable<T> implements HoodieCompactionHandler<T> {
 
   private static final Logger LOG = LogManager.getLogger(HoodieSparkCopyOnWriteTable.class);
@@ -252,7 +251,7 @@ protected HoodieMergeHandle getUpdateHandle(String instantTime, String partition
   @Override
   public Iterator<List<WriteStatus>> handleInsert(
       String instantTime, String partitionPath, String fileId,
-      Map<String, HoodieRecord<? extends HoodieRecordPayload>> recordMap) {
+      Map<String, HoodieRecord<?>> recordMap) {
     HoodieCreateHandle<?, ?, ?, ?> createHandle =
         new HoodieCreateHandle(config, instantTime, this, partitionPath, fileId, recordMap, taskContextSupplier);
     createHandle.write();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java
Patch:
@@ -28,7 +28,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -78,7 +77,7 @@
  * action
  * </p>
  */
-public class HoodieSparkMergeOnReadTable<T extends HoodieRecordPayload> extends HoodieSparkCopyOnWriteTable<T> implements HoodieCompactionHandler<T> {
+public class HoodieSparkMergeOnReadTable<T> extends HoodieSparkCopyOnWriteTable<T> implements HoodieCompactionHandler<T> {
 
   HoodieSparkMergeOnReadTable(HoodieWriteConfig config, HoodieEngineContext context, HoodieTableMetaClient metaClient) {
     super(config, context, metaClient);
@@ -177,7 +176,7 @@ public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context,
 
   @Override
   public Iterator<List<WriteStatus>> handleInsertsForLogCompaction(String instantTime, String partitionPath, String fileId,
-                                                          Map<String, HoodieRecord<? extends HoodieRecordPayload>> recordMap,
+                                                          Map<String, HoodieRecord<?>> recordMap,
                                                           Map<HoodieLogBlock.HeaderMetadataType, String> header) {
     HoodieAppendHandle appendHandle = new HoodieAppendHandle(config, instantTime, this,
         partitionPath, fileId, recordMap.values().iterator(), taskContextSupplier, header);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java
Patch:
@@ -38,7 +38,6 @@
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -86,7 +85,7 @@
 import static org.apache.hudi.config.HoodieWriteConfig.WRITE_STATUS_STORAGE_LEVEL_VALUE;
 import static org.apache.hudi.table.action.bootstrap.MetadataBootstrapHandlerFactory.getMetadataHandler;
 
-public class SparkBootstrapCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkBootstrapCommitActionExecutor<T>
     extends BaseCommitActionExecutor<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>, HoodieBootstrapWriteMetadata<HoodieData<WriteStatus>>> {
 
   private static final Logger LOG = LogManager.getLogger(SparkBootstrapCommitActionExecutor.class);
@@ -280,7 +279,7 @@ protected Option<HoodieWriteMetadata<HoodieData<WriteStatus>>> fullBootstrap(Lis
             properties, context);
     JavaRDD<HoodieRecord> inputRecordsRDD =
         (JavaRDD<HoodieRecord>) inputProvider.generateInputRecords("bootstrap_source", config.getBootstrapSourceBasePath(),
-            partitionFilesList);
+            partitionFilesList, config);
     // Start Full Bootstrap
     String bootstrapInstantTime = HoodieTimeline.FULL_BOOTSTRAP_INSTANT_TS;
     final HoodieInstant requested = new HoodieInstant(

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapDeltaCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -31,7 +30,7 @@
 
 import java.util.Map;
 
-public class SparkBootstrapDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkBootstrapDeltaCommitActionExecutor<T>
     extends SparkBootstrapCommitActionExecutor<T> {
 
   public SparkBootstrapDeltaCommitActionExecutor(HoodieSparkEngineContext context,

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkExecuteClusteringCommitActionExecutor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.ClusteringUtils;
@@ -33,7 +32,7 @@
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;
 
-public class SparkExecuteClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkExecuteClusteringCommitActionExecutor<T>
     extends BaseSparkCommitActionExecutor<T> {
 
   private final HoodieClusteringPlan clusteringPlan;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java
Patch:
@@ -27,7 +27,6 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
@@ -81,7 +80,7 @@
 import static org.apache.hudi.common.util.ClusteringUtils.getAllFileGroupsInPendingClusteringPlans;
 import static org.apache.hudi.config.HoodieWriteConfig.WRITE_STATUS_STORAGE_LEVEL_VALUE;
 
-public abstract class BaseSparkCommitActionExecutor<T extends HoodieRecordPayload> extends
+public abstract class BaseSparkCommitActionExecutor<T> extends
     BaseCommitActionExecutor<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>, HoodieWriteMetadata<HoodieData<WriteStatus>>> {
 
   private static final Logger LOG = LogManager.getLogger(BaseSparkCommitActionExecutor.class);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBucketIndexPartitioner.java
Patch:
@@ -33,7 +33,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecordLocation;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -46,7 +45,7 @@
 /**
  * Packs incoming records to be inserted into buckets (1 bucket = 1 RDD partition).
  */
-public class SparkBucketIndexPartitioner<T extends HoodieRecordPayload<T>> extends
+public class SparkBucketIndexPartitioner<T> extends
     SparkHoodiePartitioner<T> {
 
   private final int numBuckets;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertCommitActionExecutor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -33,7 +32,7 @@
 
 import java.util.Map;
 
-public class SparkBulkInsertCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseSparkCommitActionExecutor<T> {
+public class SparkBulkInsertCommitActionExecutor<T> extends BaseSparkCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> inputRecordsRDD;
   private final Option<BulkInsertPartitioner> bulkInsertPartitioner;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -45,7 +44,7 @@
  * @param <T>
  */
 @SuppressWarnings("checkstyle:LineLength")
-public class SparkBulkInsertHelper<T extends HoodieRecordPayload, R> extends BaseBulkInsertHelper<T, HoodieData<HoodieRecord<T>>,
+public class SparkBulkInsertHelper<T, R> extends BaseBulkInsertHelper<T, HoodieData<HoodieRecord<T>>,
     HoodieData<HoodieKey>, HoodieData<WriteStatus>, R> {
 
   private SparkBulkInsertHelper() {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertPreppedCommitActionExecutor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -31,7 +30,7 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
-public class SparkBulkInsertPreppedCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkBulkInsertPreppedCommitActionExecutor<T>
     extends BaseSparkCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> preppedInputRecordRdd;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeleteCommitActionExecutor.java
Patch:
@@ -22,13 +22,12 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieKey;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
-public class SparkDeleteCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkDeleteCommitActionExecutor<T>
     extends BaseSparkCommitActionExecutor<T> {
 
   private final HoodieData<HoodieKey> keys;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
@@ -47,7 +46,7 @@
 import static org.apache.hudi.common.table.timeline.HoodieInstant.State.REQUESTED;
 import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;
 
-public class SparkDeletePartitionCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkDeletePartitionCommitActionExecutor<T>
     extends SparkInsertOverwriteCommitActionExecutor<T> {
 
   private List<String> partitions;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkHoodiePartitioner.java
Patch:
@@ -18,15 +18,14 @@
 
 package org.apache.hudi.table.action.commit;
 
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.WorkloadProfile;
 import org.apache.spark.Partitioner;
 
 /**
  * Packs incoming records to be inserted into buckets (1 bucket = 1 RDD partition).
  */
-public abstract class SparkHoodiePartitioner<T extends HoodieRecordPayload<T>> extends Partitioner
+public abstract class SparkHoodiePartitioner<T> extends Partitioner
     implements org.apache.hudi.table.action.commit.Partitioner {
 
   /**

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertCommitActionExecutor.java
Patch:
@@ -22,13 +22,12 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
-public class SparkInsertCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkInsertCommitActionExecutor<T>
     extends BaseSparkCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> inputRecordsRDD;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteCommitActionExecutor.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.collection.Pair;
@@ -39,7 +38,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
-public class SparkInsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkInsertOverwriteCommitActionExecutor<T>
     extends BaseSparkCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> inputRecordsRDD;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteTableCommitActionExecutor.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -35,7 +34,7 @@
 import java.util.List;
 import java.util.Map;
 
-public class SparkInsertOverwriteTableCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkInsertOverwriteTableCommitActionExecutor<T>
     extends SparkInsertOverwriteCommitActionExecutor<T> {
 
   public SparkInsertOverwriteTableCommitActionExecutor(HoodieEngineContext context,

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertPreppedCommitActionExecutor.java
Patch:
@@ -22,13 +22,12 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
-public class SparkInsertPreppedCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkInsertPreppedCommitActionExecutor<T>
     extends BaseSparkCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> preppedRecords;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkUpsertCommitActionExecutor.java
Patch:
@@ -22,13 +22,12 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
-public class SparkUpsertCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkUpsertCommitActionExecutor<T>
     extends BaseSparkCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> inputRecordsRDD;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkUpsertPreppedCommitActionExecutor.java
Patch:
@@ -22,13 +22,12 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
-public class SparkUpsertPreppedCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkUpsertPreppedCommitActionExecutor<T>
     extends BaseSparkCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> preppedRecords;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecordLocation;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -61,7 +60,7 @@
 /**
  * Packs incoming records to be upserted, into buckets (1 bucket = 1 RDD partition).
  */
-public class UpsertPartitioner<T extends HoodieRecordPayload<T>> extends SparkHoodiePartitioner<T> {
+public class UpsertPartitioner<T> extends SparkHoodiePartitioner<T> {
 
   private static final Logger LOG = LogManager.getLogger(UpsertPartitioner.class);
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/HoodieSparkMergeOnReadTableCompactor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -37,7 +36,7 @@
  * a normal commit
  */
 @SuppressWarnings("checkstyle:LineLength")
-public class HoodieSparkMergeOnReadTableCompactor<T extends HoodieRecordPayload>
+public class HoodieSparkMergeOnReadTableCompactor<T>
     extends HoodieCompactor<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>> {
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/BaseSparkDeltaCommitActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -43,7 +42,7 @@
 import java.util.List;
 import java.util.Map;
 
-public abstract class BaseSparkDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public abstract class BaseSparkDeltaCommitActionExecutor<T>
     extends BaseSparkCommitActionExecutor<T> {
   private static final Logger LOG = LogManager.getLogger(BaseSparkDeltaCommitActionExecutor.class);
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkBulkInsertDeltaCommitActionExecutor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -34,7 +33,7 @@
 
 import java.util.Map;
 
-public class SparkBulkInsertDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkBulkInsertDeltaCommitActionExecutor<T>
     extends BaseSparkDeltaCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> inputRecordsRDD;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkBulkInsertPreppedDeltaCommitActionExecutor.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -32,7 +31,7 @@
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.hudi.table.action.commit.SparkBulkInsertHelper;
 
-public class SparkBulkInsertPreppedDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkBulkInsertPreppedDeltaCommitActionExecutor<T>
     extends BaseSparkDeltaCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> preppedInputRecordRdd;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkDeleteDeltaCommitActionExecutor.java
Patch:
@@ -22,14 +22,13 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieKey;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.hudi.table.action.commit.HoodieDeleteHelper;
 
-public class SparkDeleteDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkDeleteDeltaCommitActionExecutor<T>
     extends BaseSparkDeltaCommitActionExecutor<T> {
 
   private final HoodieData<HoodieKey> keys;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkInsertDeltaCommitActionExecutor.java
Patch:
@@ -22,14 +22,13 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.hudi.table.action.commit.HoodieWriteHelper;
 
-public class SparkInsertDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkInsertDeltaCommitActionExecutor<T>
     extends BaseSparkDeltaCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> inputRecordsRDD;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkInsertPreppedDeltaCommitActionExecutor.java
Patch:
@@ -22,13 +22,12 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
-public class SparkInsertPreppedDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkInsertPreppedDeltaCommitActionExecutor<T>
     extends BaseSparkDeltaCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> preppedRecords;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkUpsertDeltaCommitActionExecutor.java
Patch:
@@ -22,14 +22,13 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.hudi.table.action.commit.HoodieWriteHelper;
 
-public class SparkUpsertDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkUpsertDeltaCommitActionExecutor<T>
     extends BaseSparkDeltaCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> inputRecordsRDD;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkUpsertDeltaCommitPartitioner.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodieRecordLocation;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -43,7 +42,7 @@
  * UpsertPartitioner for MergeOnRead table type, this allows auto correction of small parquet files to larger ones
  * without the need for an index in the logFile.
  */
-public class SparkUpsertDeltaCommitPartitioner<T extends HoodieRecordPayload<T>> extends UpsertPartitioner<T> {
+public class SparkUpsertDeltaCommitPartitioner<T> extends UpsertPartitioner<T> {
 
   public SparkUpsertDeltaCommitPartitioner(WorkloadProfile profile, HoodieSparkEngineContext context, HoodieTable table,
                                            HoodieWriteConfig config) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkUpsertPreppedDeltaCommitActionExecutor.java
Patch:
@@ -22,13 +22,12 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
-public class SparkUpsertPreppedDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
+public class SparkUpsertPreppedDeltaCommitActionExecutor<T>
     extends BaseSparkDeltaCommitActionExecutor<T> {
 
   private final HoodieData<HoodieRecord<T>> preppedRecords;

File: hudi-client/hudi-spark-client/src/main/scala/org/apache/hudi/unsafe/UTF8StringBuilder.java
Patch:
@@ -52,8 +52,8 @@ public UTF8StringBuilder(int initialSize) {
   private void grow(int neededSize) {
     if (neededSize > ARRAY_MAX - totalSize()) {
       throw new UnsupportedOperationException(
-          "Cannot grow internal buffer by size " + neededSize + " because the size after growing " +
-              "exceeds size limitation " + ARRAY_MAX);
+          "Cannot grow internal buffer by size " + neededSize + " because the size after growing "
+              + "exceeds size limitation " + ARRAY_MAX);
     }
     final int length = totalSize() + neededSize;
     if (buffer.length < length) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieAvroRecord;
+import org.apache.hudi.common.model.HoodieAvroIndexedRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
@@ -134,7 +135,8 @@ private void assertSchemaEvolutionOnUpdateResult(WriteStatus insertResult, Hoodi
                 new Path(updateTable.getConfig().getBasePath() + "/" + insertResult.getStat().getPath()),
                 mergeHandle.getWriterSchemaWithMetaFields());
         for (GenericRecord rec : oldRecords) {
-          mergeHandle.write(rec);
+          // TODO create hoodie record with rec can getRecordKey
+          mergeHandle.write(new HoodieAvroIndexedRecord(rec));
         }
         mergeHandle.close();
       };

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/clustering/plan/strategy/TestSparkConsistentBucketClusteringPlanStrategy.java
Patch:
@@ -29,7 +29,7 @@
 import org.apache.hudi.common.testutils.HoodieTestUtils;
 import org.apache.hudi.common.util.collection.Triple;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.index.bucket.ConsistentBucketIdentifier;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestConsistentBucketIndex.java
Patch:
@@ -33,7 +33,7 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.hadoop.HoodieParquetInputFormat;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestDataValidationCheckForLogCompactionActions.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
@@ -40,7 +41,6 @@
 import org.apache.hudi.config.HoodieCleanConfig;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.testutils.GenericRecordValidationTestUtils;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieIndex.java
Patch:
@@ -38,7 +38,7 @@
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieLayoutConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.index.HoodieIndex.IndexType;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieMetadataBase.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.avro.model.HoodieMetadataRecord;
 import org.apache.hudi.client.HoodieTimelineArchiver;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
@@ -39,7 +40,6 @@
 import org.apache.hudi.config.HoodieCleanConfig;
 import org.apache.hudi.config.HoodieArchivalConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.config.metrics.HoodieMetricsConfig;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestSparkConsistentBucketClustering.java
Patch:
@@ -36,7 +36,7 @@
 import org.apache.hudi.config.HoodieClusteringConfig;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.execution.bulkinsert.BulkInsertSortMode;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/hbase/TestHBaseQPSResourceAllocator.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieHBaseIndexConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/hbase/TestSparkHoodieHBaseIndex.java
Patch:
@@ -37,7 +37,7 @@
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieHBaseIndexConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.table.HoodieSparkTable;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/TestHoodieKeyLocationFetchHandle.java
Patch:
@@ -33,7 +33,7 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndexUtils;
 import org.apache.hudi.keygen.BaseKeyGenerator;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/TestHoodieMergeHandle.java
Patch:
@@ -33,7 +33,7 @@
 import org.apache.hudi.common.util.collection.ExternalSpillableMap;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.testutils.HoodieClientTestHarness;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/storage/row/TestHoodieInternalRowParquetWriter.java
Patch:
@@ -24,10 +24,10 @@
 import org.apache.hudi.common.bloom.BloomFilter;
 import org.apache.hudi.common.bloom.BloomFilterFactory;
 import org.apache.hudi.common.bloom.BloomFilterTypeCode;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ParquetUtils;
-import org.apache.hudi.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.storage.HoodieParquetConfig;
 import org.apache.hudi.testutils.HoodieClientTestHarness;
@@ -131,6 +131,6 @@ private HoodieRowParquetWriteSupport getWriteSupport(HoodieWriteConfig.Builder w
         writeConfig.getBloomFilterFPP(),
         writeConfig.getDynamicBloomFilterMaxNumEntries(),
         writeConfig.getBloomFilterType());
-    return new HoodieRowParquetWriteSupport(hadoopConf, SparkDatasetTestUtils.STRUCT_TYPE, Option.of(filter), writeConfig);
+    return new HoodieRowParquetWriteSupport(hadoopConf, SparkDatasetTestUtils.STRUCT_TYPE, Option.of(filter), writeConfig.getStorageConfig());
   }
 }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java
Patch:
@@ -40,7 +40,7 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieLayoutConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.data.HoodieJavaRDD;
 import org.apache.hudi.hadoop.HoodieParquetInputFormat;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java
Patch:
@@ -38,7 +38,7 @@
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieHBaseIndexConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/CompactionTestBase.java
Patch:
@@ -42,7 +42,7 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.table.HoodieTable;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java
Patch:
@@ -37,7 +37,7 @@
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieMemoryConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieNotSupportedException;
 import org.apache.hudi.index.HoodieIndex;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/rollback/TestMergeOnReadRollbackActionExecutor.java
Patch:
@@ -39,7 +39,7 @@
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.table.HoodieTable;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableCompaction.java
Patch:
@@ -35,7 +35,7 @@
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieLayoutConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.table.action.commit.SparkBucketIndexPartitioner;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableRollback.java
Patch:
@@ -46,7 +46,7 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieCleanConfig;
 import org.apache.hudi.config.HoodieCompactionConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.index.HoodieIndex;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/FunctionalTestHarness.java
Patch:
@@ -38,6 +38,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.spark.HoodieSparkKryoProvider$;
 import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.SQLContext;
@@ -138,7 +139,7 @@ public synchronized void runBeforeEach() throws Exception {
     initialized = spark != null && hdfsTestService != null;
     if (!initialized) {
       SparkConf sparkConf = conf();
-      SparkRDDWriteClient.registerClasses(sparkConf);
+      HoodieSparkKryoProvider$.MODULE$.register(sparkConf);
       SparkRDDReadClient.addHoodieSupport(sparkConf);
       spark = SparkSession.builder().config(sparkConf).getOrCreate();
       sqlContext = spark.sqlContext();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestBase.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.common.HoodieCleanStat;
 import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;
@@ -42,7 +43,6 @@
 import org.apache.hudi.config.HoodieCleanConfig;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.index.HoodieIndex.IndexType;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java
Patch:
@@ -67,7 +67,7 @@
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
-import static org.apache.hudi.io.storage.HoodieHFileReader.SCHEMA_KEY;
+import static org.apache.hudi.io.storage.HoodieAvroHFileReader.SCHEMA_KEY;
 
 /**
  * Utility methods to aid testing inside the HoodieClient module.

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/SparkClientFunctionalTestHarness.java
Patch:
@@ -41,7 +41,7 @@
 import org.apache.hudi.config.HoodieClusteringConfig;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.data.HoodieJavaRDD;
 import org.apache.hudi.exception.HoodieIOException;
@@ -60,6 +60,7 @@
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.spark.HoodieSparkKryoProvider$;
 import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
@@ -185,7 +186,7 @@ public synchronized void runBeforeEach() {
     initialized = spark != null;
     if (!initialized) {
       SparkConf sparkConf = conf();
-      SparkRDDWriteClient.registerClasses(sparkConf);
+      HoodieSparkKryoProvider$.MODULE$.register(sparkConf);
       SparkRDDReadClient.addHoodieSupport(sparkConf);
       spark = SparkSession.builder().config(sparkConf).getOrCreate();
       sqlContext = spark.sqlContext();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/SparkDatasetTestUtils.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.common.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
 

File: hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaCompatibility.java
Patch:
@@ -220,7 +220,6 @@ public ReaderWriterCompatibilityChecker(boolean checkNaming) {
       this.checkNaming = checkNaming;
     }
 
-
     /**
      * Reports the compatibility of a reader/writer schema pair.
      *

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java
Patch:
@@ -18,14 +18,13 @@
 
 package org.apache.hudi.common.model;
 
+import org.apache.avro.Schema;
+import org.apache.avro.generic.IndexedRecord;
 import org.apache.hudi.ApiMaturityLevel;
 import org.apache.hudi.PublicAPIClass;
 import org.apache.hudi.PublicAPIMethod;
 import org.apache.hudi.common.util.Option;
 
-import org.apache.avro.Schema;
-import org.apache.avro.generic.IndexedRecord;
-
 import java.io.IOException;
 import java.io.Serializable;
 import java.util.Map;

File: hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java
Patch:
@@ -143,7 +143,6 @@ private Option<IndexedRecord> mergeOldRecord(IndexedRecord oldRecord,
       Schema schema,
       boolean isOldRecordNewer) throws IOException {
     Option<IndexedRecord> recordOption = getInsertValue(schema);
-
     if (!recordOption.isPresent()) {
       // use natural order for delete record
       return Option.empty();

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java
Patch:
@@ -227,7 +227,7 @@ private HoodieLogBlock readBlock() throws IOException {
             String.format("Parquet block could not be of version (%d)", HoodieLogFormatVersion.DEFAULT_VERSION));
 
         return new HoodieParquetDataBlock(inputStream, content, readBlockLazily, logBlockContentLoc,
-             Option.ofNullable(readerSchema), header, footer, keyField);
+            getTargetReaderSchemaForBlock(), header, footer, keyField);
 
       case DELETE_BLOCK:
         return new HoodieDeleteBlock(content, inputStream, readBlockLazily, Option.of(logBlockContentLoc), header, footer);
@@ -284,7 +284,7 @@ private HoodieLogBlock createCorruptBlock(long blockStartPos) throws IOException
   private boolean isBlockCorrupted(int blocksize) throws IOException {
     long currentPos = inputStream.getPos();
     long blockSizeFromFooter;
-    
+
     try {
       // check if the blocksize mentioned in the footer is the same as the header;
       // by seeking and checking the length of a long.  We do not seek `currentPos + blocksize`

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieCDCDataBlock.java
Patch:
@@ -18,10 +18,10 @@
 
 package org.apache.hudi.common.table.log.block;
 
+import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.Option;
 
 import org.apache.avro.Schema;
-import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.fs.FSDataInputStream;
 
 import java.util.HashMap;
@@ -45,7 +45,7 @@ public HoodieCDCDataBlock(
         Option.of(readerSchema), header, new HashMap<>(), keyField);
   }
 
-  public HoodieCDCDataBlock(List<IndexedRecord> records,
+  public HoodieCDCDataBlock(List<HoodieRecord> records,
                             Map<HeaderMetadataType, String> header,
                             String keyField) {
     super(records, header, keyField);

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/table/action/cluster/strategy/TestPartitionAwareClusteringPlanStrategy.java
Patch:
@@ -71,7 +71,7 @@ public void testFilterPartitionPaths() {
     fakeTimeBasedPartitionsPath.add("20210719");
     fakeTimeBasedPartitionsPath.add("20210721");
 
-    List list = strategyTestRegexPattern.getMatchedPartitions(hoodieWriteConfig, fakeTimeBasedPartitionsPath);
+    List list = strategyTestRegexPattern.getRegexPatternMatchedPartitions(hoodieWriteConfig, fakeTimeBasedPartitionsPath);
     assertEquals(2, list.size());
     assertTrue(list.contains("20210721"));
     assertTrue(list.contains("20210723"));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -63,6 +63,7 @@
 import org.apache.hudi.common.util.ClusteringUtils;
 import org.apache.hudi.common.util.CommitUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieArchivalConfig;
@@ -1605,7 +1606,8 @@ protected void setWriteSchemaForDeletes(HoodieTableMetaClient metaClient) {
       if (lastInstant.isPresent()) {
         HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(
             activeTimeline.getInstantDetails(lastInstant.get()).get(), HoodieCommitMetadata.class);
-        if (commitMetadata.getExtraMetadata().containsKey(SCHEMA_KEY)) {
+        String extraSchema = commitMetadata.getExtraMetadata().get(SCHEMA_KEY);
+        if (!StringUtils.isNullOrEmpty(extraSchema)) {
           config.setSchema(commitMetadata.getExtraMetadata().get(SCHEMA_KEY));
         } else {
           throw new HoodieIOException("Latest commit does not have any schema in commit metadata");

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -349,7 +349,7 @@ protected void preCommit(HoodieInstant inflightInstant, HoodieCommitMetadata met
   protected void writeTableMetadata(HoodieTable table, String instantTime, String actionType, HoodieCommitMetadata metadata) {
     context.setJobStatus(this.getClass().getSimpleName(), "Committing to metadata table: " + config.getTableName());
     table.getMetadataWriter(instantTime).ifPresent(w -> ((HoodieTableMetadataWriter) w).update(metadata, instantTime,
-        table.isTableServiceAction(actionType)));
+        table.isTableServiceAction(actionType, instantTime)));
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/BaseActionExecutor.java
Patch:
@@ -58,7 +58,7 @@ public BaseActionExecutor(HoodieEngineContext context, HoodieWriteConfig config,
    */
   protected final void writeTableMetadata(HoodieCommitMetadata metadata, String actionType) {
     table.getMetadataWriter(instantTime).ifPresent(w -> w.update(
-        metadata, instantTime, table.isTableServiceAction(actionType)));
+        metadata, instantTime, table.isTableServiceAction(actionType, instantTime)));
   }
 
   /**

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -290,7 +290,7 @@ protected void writeTableMetadata(HoodieTable table, String instantTime, String
     // the schema expects to be immutable for SQL jobs but may be not for non-SQL
     // jobs.
     this.metadataWriter.initTableMetadata();
-    this.metadataWriter.update(metadata, instantTime, getHoodieTable().isTableServiceAction(actionType));
+    this.metadataWriter.update(metadata, instantTime, getHoodieTable().isTableServiceAction(actionType, instantTime));
   }
 
   /**

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java
Patch:
@@ -483,7 +483,7 @@ private void validateClusteringCommit(HoodieWriteMetadata<JavaRDD<WriteStatus>>
 
   private void updateTableMetadata(HoodieTable table, HoodieCommitMetadata commitMetadata,
                                    HoodieInstant hoodieInstant) {
-    boolean isTableServiceAction = table.isTableServiceAction(hoodieInstant.getAction());
+    boolean isTableServiceAction = table.isTableServiceAction(hoodieInstant.getAction(), hoodieInstant.getTimestamp());
     // Do not do any conflict resolution here as we do with regular writes. We take the lock here to ensure all writes to metadata table happens within a
     // single lock (single writer). Because more than one write to metadata table will result in conflicts since all of them updates the same partition.
     table.getMetadataWriter(hoodieInstant.getTimestamp())

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstant.java
Patch:
@@ -67,7 +67,7 @@ public enum State {
     // Committed instant
     COMPLETED,
     // Invalid instant
-    INVALID
+    NIL
   }
 
   private State state = State.COMPLETED;

File: hudi-common/src/test/java/org/apache/hudi/common/table/timeline/TestHoodieActiveTimeline.java
Patch:
@@ -663,7 +663,7 @@ private List<HoodieInstant> getAllInstants() {
     List<HoodieInstant> allInstants = new ArrayList<>();
     long instantTime = 1;
     for (State state : State.values()) {
-      if (state == State.INVALID) {
+      if (state == State.NIL) {
         continue;
       }
       for (String action : HoodieTimeline.VALID_ACTIONS_IN_TIMELINE) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/DayBasedCompactionStrategy.java
Patch:
@@ -78,7 +78,8 @@ public List<HoodieCompactionOperation> orderAndFilter(HoodieWriteConfig writeCon
   public List<String> filterPartitionPaths(HoodieWriteConfig writeConfig, List<String> allPartitionPaths) {
     return allPartitionPaths.stream().map(partition -> partition.replace("/", "-"))
         .sorted(Comparator.reverseOrder()).map(partitionPath -> partitionPath.replace("-", "/"))
-        .collect(Collectors.toList()).subList(0, writeConfig.getTargetPartitionsPerDayBasedCompaction());
+        .collect(Collectors.toList()).subList(0, Math.min(allPartitionPaths.size(),
+            writeConfig.getTargetPartitionsPerDayBasedCompaction()));
   }
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java
Patch:
@@ -79,9 +79,8 @@ public class FSUtils {
   private static final Logger LOG = LogManager.getLogger(FSUtils.class);
   // Log files are of this pattern - .b5068208-e1a4-11e6-bf01-fe55135034f3_20170101134598.log.1_1-0-1
   // Archive log files are of this pattern - .commits_.archive.1_1-0-1
-  private static final Pattern LOG_FILE_PATTERN =
+  public static final Pattern LOG_FILE_PATTERN =
       Pattern.compile("\\.(.+)_(.*)\\.(.+)\\.(\\d+)(_((\\d+)-(\\d+)-(\\d+))(.cdc)?)?");
-  private static final String LOG_FILE_PREFIX = ".";
   private static final int MAX_ATTEMPTS_RECOVER_LEASE = 10;
   private static final long MIN_CLEAN_TO_KEEP = 10;
   private static final long MIN_ROLLBACK_TO_KEEP = 10;
@@ -467,7 +466,7 @@ public static String makeLogFileName(String fileId, String logFileExtension, Str
     String suffix = (writeToken == null)
         ? String.format("%s_%s%s.%d", fileId, baseCommitTime, logFileExtension, version)
         : String.format("%s_%s%s.%d_%s", fileId, baseCommitTime, logFileExtension, version, writeToken);
-    return LOG_FILE_PREFIX + suffix;
+    return HoodieLogFile.LOG_FILE_PREFIX + suffix;
   }
 
   public static boolean isBaseFile(Path path) {

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieLogFile.java
Patch:
@@ -39,6 +39,7 @@ public class HoodieLogFile implements Serializable {
 
   private static final long serialVersionUID = 1L;
   public static final String DELTA_EXTENSION = ".log";
+  public static final String LOG_FILE_PREFIX = ".";
   public static final Integer LOGFILE_BASE_VERSION = 1;
 
   private static final Comparator<HoodieLogFile> LOG_FILE_COMPARATOR = new LogFileComparator();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java
Patch:
@@ -207,9 +207,9 @@ private <I> BulkInsertPartitioner<I> getPartitioner(Map<String, String> strategy
       }
     }).orElse(isRowPartitioner
         ? BulkInsertInternalPartitionerWithRowsFactory.get(
-        getWriteConfig().getBulkInsertSortMode(), getHoodieTable().isPartitioned())
+        getWriteConfig().getBulkInsertSortMode(), getHoodieTable().isPartitioned(), true)
         : BulkInsertInternalPartitionerFactory.get(
-        getWriteConfig().getBulkInsertSortMode(), getHoodieTable().isPartitioned()));
+        getWriteConfig().getBulkInsertSortMode(), getHoodieTable().isPartitioned(), true));
   }
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java
Patch:
@@ -151,7 +151,7 @@ public static HashMap<String, String> getFileIdWithoutSuffixAndRelativePaths(Map
    * @return An optional commit metadata with latest checkpoint.
    */
   public static Option<HoodieCommitMetadata> getLatestCommitMetadataWithValidCheckpointInfo(HoodieTimeline timeline, String checkpointKey) {
-    return (Option<HoodieCommitMetadata>) timeline.getReverseOrderedInstants().map(instant -> {
+    return (Option<HoodieCommitMetadata>) timeline.filterCompletedInstants().getReverseOrderedInstants().map(instant -> {
       try {
         HoodieCommitMetadata commitMetadata = HoodieCommitMetadata
             .fromBytes(timeline.getInstantDetails(instant).get(), HoodieCommitMetadata.class);

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java
Patch:
@@ -68,6 +68,7 @@
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
 import java.util.Random;
 import java.util.Set;
@@ -363,7 +364,7 @@ private void generateExtraSchemaValues(GenericRecord rec) {
     rec.put("current_date", (int) LocalDateTime.ofInstant(instant, ZoneOffset.UTC).toLocalDate().toEpochDay());
     rec.put("current_ts", randomMillis);
 
-    BigDecimal bigDecimal = new BigDecimal(String.format("%5f", rand.nextFloat()));
+    BigDecimal bigDecimal = new BigDecimal(String.format(Locale.ENGLISH, "%5f", rand.nextFloat()));
     Schema decimalSchema = AVRO_SCHEMA.getField("height").schema();
     Conversions.DecimalConversion decimalConversions = new Conversions.DecimalConversion();
     GenericFixed genericFixed = decimalConversions.toFixed(bigDecimal, decimalSchema, LogicalTypes.decimal(10, 6));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -305,7 +305,7 @@ protected boolean writeRecord(HoodieRecord<T> hoodieRecord, Option<IndexedRecord
     return writeRecord(hoodieRecord, indexedRecord, false);
   }
 
-  protected boolean writeRecord(HoodieRecord<T> hoodieRecord, Option<IndexedRecord> indexedRecord, boolean isDelete) {
+  private boolean writeRecord(HoodieRecord<T> hoodieRecord, Option<IndexedRecord> indexedRecord, boolean isDelete) {
     Option recordMetadata = hoodieRecord.getData().getMetadata();
     if (!partitionPath.equals(hoodieRecord.getPartitionPath())) {
       HoodieUpsertException failureEx = new HoodieUpsertException("mismatched partition path, record partition: "

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/SparkLazyInsertIterable.java
Patch:
@@ -35,6 +35,8 @@
 import java.util.Iterator;
 import java.util.List;
 
+import static org.apache.hudi.common.util.ValidationUtils.checkState;
+
 public class SparkLazyInsertIterable<T extends HoodieRecordPayload> extends HoodieLazyInsertIterable<T> {
 
   private boolean useWriterSchema;
@@ -89,7 +91,7 @@ protected List<WriteStatus> computeNext() {
           getTransformFunction(schema, hoodieConfig), hoodieTable.getPreExecuteRunnable());
 
       final List<WriteStatus> result = bufferedIteratorExecutor.execute();
-      assert result != null && !result.isEmpty() && !bufferedIteratorExecutor.isRemaining();
+      checkState(result != null && !result.isEmpty());
       return result;
     } catch (Exception e) {
       throw new HoodieException(e);

File: hudi-common/src/main/java/org/apache/hudi/common/util/ParquetReaderIterator.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.util;
 
-import org.apache.hudi.common.util.queue.BoundedInMemoryQueueIterable;
+import org.apache.hudi.common.util.queue.BoundedInMemoryQueue;
 import org.apache.hudi.exception.HoodieException;
 
 import org.apache.parquet.hadoop.ParquetReader;
@@ -27,7 +27,7 @@
 
 /**
  * This class wraps a parquet reader and provides an iterator based api to read from a parquet file. This is used in
- * {@link BoundedInMemoryQueueIterable}
+ * {@link BoundedInMemoryQueue}
  */
 public class ParquetReaderIterator<T> implements ClosableIterator<T> {
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -294,7 +294,7 @@ private void saveInternalSchema(HoodieTable table, String instantTime, HoodieCom
       InternalSchema internalSchema;
       Schema avroSchema = HoodieAvroUtils.createHoodieWriteSchema(config.getSchema(), config.allowOperationMetadataField());
       if (historySchemaStr.isEmpty()) {
-        internalSchema = AvroInternalSchemaConverter.convert(avroSchema);
+        internalSchema = SerDeHelper.fromJson(config.getInternalSchema()).orElse(AvroInternalSchemaConverter.convert(avroSchema));
         internalSchema.setSchemaId(Long.parseLong(instantTime));
       } else {
         internalSchema = InternalSchemaUtils.searchSchema(Long.parseLong(instantTime),

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java
Patch:
@@ -192,7 +192,7 @@ protected ClosableIterator<IndexedRecord> lookupRecords(List<String> keys, boole
     //       is appropriately carried over
     Configuration inlineConf = new Configuration(blockContentLoc.getHadoopConf());
     inlineConf.set("fs." + InLineFileSystem.SCHEME + ".impl", InLineFileSystem.class.getName());
-    inlineConf.setClassLoader(Thread.currentThread().getContextClassLoader());
+    inlineConf.setClassLoader(InLineFileSystem.class.getClassLoader());
 
     Path inlinePath = InLineFSUtils.getInlineFilePath(
         blockContentLoc.getLogFile().getPath(),

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnMergeOnReadStorage.java
Patch:
@@ -168,7 +168,6 @@ public void testLogCompactionOnMORTable() throws Exception {
     client.compact(compactionTimeStamp.get());
 
     prevCommitTime = compactionTimeStamp.get();
-    //TODO: Below commits are creating duplicates when all the tests are run together. but individually they are passing.
     for (int i = 0; i < 2; i++) {
       // Upsert
       newCommitTime = HoodieActiveTimeline.createNewInstantTime();

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieRealtimePath.java
Patch:
@@ -89,7 +89,7 @@ public boolean getBelongsToIncrementalQuery() {
   }
 
   public boolean isSplitable() {
-    return !toString().contains(".log") && !includeBootstrapFilePath();
+    return !toString().contains(".log") && deltaLogFiles.isEmpty() && !includeBootstrapFilePath();
   }
 
   public PathWithBootstrapFileStatus getPathWithBootstrapFileStatus() {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/IncrementalInputSplits.java
Patch:
@@ -492,7 +492,7 @@ public List<HoodieInstant> filterInstantsWithRange(
 
     if (OptionsResolver.hasNoSpecificReadCommits(this.conf)) {
       // by default read from the latest commit
-      List<HoodieInstant> instants = completedTimeline.getInstants().collect(Collectors.toList());
+      List<HoodieInstant> instants = completedTimeline.getInstants();
       if (instants.size() > 1) {
         return Collections.singletonList(instants.get(instants.size() - 1));
       }

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java
Patch:
@@ -317,7 +317,7 @@ private static String printAllCompactions(HoodieDefaultTimeline timeline,
         .collect(Collectors.toList());
 
     Set<String> committedInstants = timeline.getCommitTimeline().filterCompletedInstants()
-        .getInstants().map(HoodieInstant::getTimestamp).collect(Collectors.toSet());
+        .getInstantsAsStream().map(HoodieInstant::getTimestamp).collect(Collectors.toSet());
 
     List<Comparable[]> rows = new ArrayList<>();
     for (Pair<HoodieInstant, HoodieCompactionPlan> compactionPlan : compactionPlans) {

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/DiffCommand.java
Patch:
@@ -137,7 +137,7 @@ private String printDiffWithMetadata(HoodieDefaultTimeline timeline, Integer lim
                                        BiFunction<HoodieWriteStat, String, Boolean> diffEntityChecker) throws IOException {
     List<Comparable[]> rows = new ArrayList<>();
     List<HoodieInstant> commits = timeline.getCommitsTimeline().filterCompletedInstants()
-        .getInstants().sorted(HoodieInstant.COMPARATOR.reversed()).collect(Collectors.toList());
+        .getInstantsAsStream().sorted(HoodieInstant.COMPARATOR.reversed()).collect(Collectors.toList());
 
     for (final HoodieInstant commit : commits) {
       Option<byte[]> instantDetails = timeline.getInstantDetails(commit);

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/ExportCommand.java
Patch:
@@ -87,7 +87,7 @@ public String exportInstants(
     // The non archived instants can be listed from the Timeline.
     HoodieTimeline timeline = HoodieCLI.getTableMetaClient().getActiveTimeline().filterCompletedInstants()
         .filter(i -> actionSet.contains(i.getAction()));
-    List<HoodieInstant> nonArchivedInstants = timeline.getInstants().collect(Collectors.toList());
+    List<HoodieInstant> nonArchivedInstants = timeline.getInstants();
 
     // Archived instants are in the commit archive files
     FileStatus[] statuses = FSUtils.getFs(basePath, HoodieCLI.conf).globStatus(archivePath);

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/FileSystemViewCommand.java
Patch:
@@ -256,7 +256,7 @@ private HoodieTableFileSystemView buildFileSystemView(String globRegex, String m
       timeline = timeline.filterCompletedInstants();
     }
 
-    instantsStream = timeline.getInstants();
+    instantsStream = timeline.getInstantsAsStream();
 
     if (!maxInstant.isEmpty()) {
       final BiPredicate<String, String> predicate;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieSyncValidateCommand.java
Patch:
@@ -90,7 +90,7 @@ public String validateSync(
   private String getString(HoodieTableMetaClient target, HoodieTimeline targetTimeline, HoodieTableMetaClient source, long sourceCount, long targetCount, String sourceLatestCommit)
       throws IOException {
     List<HoodieInstant> commitsToCatchup = targetTimeline.findInstantsAfter(sourceLatestCommit, Integer.MAX_VALUE)
-        .getInstants().collect(Collectors.toList());
+        .getInstants();
     if (commitsToCatchup.isEmpty()) {
       return "Count difference now is (count(" + target.getTableConfig().getTableName() + ") - count("
           + source.getTableConfig().getTableName() + ") == " + (targetCount - sourceCount);

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java
Patch:
@@ -211,7 +211,7 @@ public void removeCorruptedPendingCleanAction() {
   public void showFailedCommits() {
     HoodieTableMetaClient metaClient = HoodieCLI.getTableMetaClient();
     HoodieActiveTimeline activeTimeline =  metaClient.getActiveTimeline();
-    activeTimeline.filterCompletedInstants().getInstants().filter(activeTimeline::isEmpty).forEach(hoodieInstant -> LOG.warn("Empty Commit: " + hoodieInstant.toString()));
+    activeTimeline.filterCompletedInstants().getInstantsAsStream().filter(activeTimeline::isEmpty).forEach(hoodieInstant -> LOG.warn("Empty Commit: " + hoodieInstant.toString()));
   }
 
   @ShellMethod(key = "repair migrate-partition-meta", value = "Migrate all partition meta file currently stored in text format "

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SavepointsCommand.java
Patch:
@@ -106,7 +106,7 @@ public String rollbackToSavepoint(
     }
     HoodieActiveTimeline activeTimeline = metaClient.getActiveTimeline();
     HoodieTimeline timeline = activeTimeline.getCommitsTimeline().filterCompletedInstants();
-    List<HoodieInstant> instants = timeline.getInstants().filter(instant -> instant.getTimestamp().equals(instantTime)).collect(Collectors.toList());
+    List<HoodieInstant> instants = timeline.getInstantsAsStream().filter(instant -> instant.getTimestamp().equals(instantTime)).collect(Collectors.toList());
 
     if (instants.isEmpty()) {
       return "Commit " + instantTime + " not found in Commits " + timeline;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/StatsCommand.java
Patch:
@@ -45,7 +45,6 @@
 import java.util.List;
 import java.util.Map;
 import java.util.function.Function;
-import java.util.stream.Collectors;
 
 /**
  * CLI command to displays stats options.
@@ -73,7 +72,7 @@ public String writeAmplificationStats(
 
     List<Comparable[]> rows = new ArrayList<>();
     DecimalFormat df = new DecimalFormat("#.00");
-    for (HoodieInstant instantTime : timeline.getInstants().collect(Collectors.toList())) {
+    for (HoodieInstant instantTime : timeline.getInstants()) {
       String waf = "0";
       HoodieCommitMetadata commit = HoodieCommitMetadata.fromBytes(activeTimeline.getInstantDetails(instantTime).get(),
           HoodieCommitMetadata.class);

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/TimelineCommand.java
Patch:
@@ -208,7 +208,7 @@ private String printTimelineInfo(
       Integer limit, String sortByField, boolean descending, boolean headerOnly, boolean withRowNo,
       boolean showTimeSeconds, boolean showRollbackInfo) {
     Map<String, List<String>> rollbackInfo = getRolledBackInstantInfo(timeline);
-    final List<Comparable[]> rows = timeline.getInstants().map(instant -> {
+    final List<Comparable[]> rows = timeline.getInstantsAsStream().map(instant -> {
       int numColumns = showRollbackInfo ? 7 : 6;
       Comparable[] row = new Comparable[numColumns];
       String instantTimestamp = instant.getTimestamp();
@@ -343,8 +343,7 @@ private Map<String, List<String>> getRolledBackInstantInfo(HoodieTimeline timeli
     // Instant rolled back or to roll back -> rollback instants
     Map<String, List<String>> rollbackInfoMap = new HashMap<>();
     List<HoodieInstant> rollbackInstants = timeline.filter(instant ->
-            HoodieTimeline.ROLLBACK_ACTION.equalsIgnoreCase(instant.getAction()))
-        .getInstants().collect(Collectors.toList());
+            HoodieTimeline.ROLLBACK_ACTION.equalsIgnoreCase(instant.getAction())).getInstants();
     rollbackInstants.forEach(rollbackInstant -> {
       try {
         if (rollbackInstant.isInflight()) {

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestArchiveCommand.java
Patch:
@@ -66,13 +66,13 @@ public void testArchiving() throws Exception {
 
     //get instants in the active timeline only returns the latest state of the commit
     //therefore we expect 2 instants because minCommits is 2
-    assertEquals(2, metaClient.getActiveTimeline().getInstants().count());
+    assertEquals(2, metaClient.getActiveTimeline().countInstants());
 
     //get instants in the archived timeline returns all instants in the commit
     //therefore we expect 12 instants because 6 commits - 2 commits in active timeline = 4 in archived
     //since each commit is completed, there are 3 instances per commit (requested, inflight, completed)
     //and 3 instances per commit * 4 commits = 12 instances
-    assertEquals(12, metaClient.getArchivedTimeline().getInstants().count());
+    assertEquals(12, metaClient.getArchivedTimeline().countInstants());
   }
 
 }

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCommitsCommand.java
Patch:
@@ -521,7 +521,7 @@ public void testCompareCommits(HoodieTableType tableType) throws Exception {
 
     // the latest instant of test_table2 is 101
     List<String> commitsToCatchup = metaClient.getActiveTimeline().findInstantsAfter("101", Integer.MAX_VALUE)
-        .getInstants().map(HoodieInstant::getTimestamp).collect(Collectors.toList());
+        .getInstantsAsStream().map(HoodieInstant::getTimestamp).collect(Collectors.toList());
     String expected = String.format("Source %s is ahead by %d commits. Commits to catch up - %s",
         tableName1, commitsToCatchup.size(), commitsToCatchup);
     assertEquals(expected, result.toString());

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java
Patch:
@@ -255,14 +255,14 @@ public void testRemoveCorruptedPendingCleanAction() throws IOException {
     // reload meta client
     metaClient = HoodieTableMetaClient.reload(metaClient);
     // first, there are four instants
-    assertEquals(4, metaClient.getActiveTimeline().filterInflightsAndRequested().getInstants().count());
+    assertEquals(4, metaClient.getActiveTimeline().filterInflightsAndRequested().countInstants());
 
     Object result = shell.evaluate(() -> "repair corrupted clean files");
     assertTrue(ShellEvaluationResultUtil.isSuccess(result));
 
     // reload meta client
     metaClient = HoodieTableMetaClient.reload(metaClient);
-    assertEquals(0, metaClient.getActiveTimeline().filterInflightsAndRequested().getInstants().count());
+    assertEquals(0, metaClient.getActiveTimeline().filterInflightsAndRequested().countInstants());
   }
 
   /**
@@ -283,7 +283,7 @@ public void testShowFailedCommits() {
       HoodieTestCommitMetadataGenerator.createCommitFile(tablePath, timestamp, conf);
     }
 
-    metaClient.getActiveTimeline().getInstants().filter(hoodieInstant -> Integer.parseInt(hoodieInstant.getTimestamp()) % 4 == 0).forEach(hoodieInstant -> {
+    metaClient.getActiveTimeline().getInstantsAsStream().filter(hoodieInstant -> Integer.parseInt(hoodieInstant.getTimestamp()) % 4 == 0).forEach(hoodieInstant -> {
       metaClient.getActiveTimeline().deleteInstantFileIfExists(hoodieInstant);
       metaClient.getActiveTimeline().createNewInstant(hoodieInstant);
     });

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRollbacksCommand.java
Patch:
@@ -128,7 +128,7 @@ public void testShowRollbacks() {
 
     // get rollback instants
     HoodieActiveTimeline activeTimeline = new RollbacksCommand.RollbackTimeline(HoodieCLI.getTableMetaClient());
-    Stream<HoodieInstant> rollback = activeTimeline.getRollbackTimeline().filterCompletedInstants().getInstants();
+    Stream<HoodieInstant> rollback = activeTimeline.getRollbackTimeline().filterCompletedInstants().getInstantsAsStream();
 
     List<Comparable[]> rows = new ArrayList<>();
     rollback.sorted().forEach(instant -> {
@@ -169,7 +169,7 @@ public void testShowRollbacks() {
   public void testShowRollback() throws IOException {
     // get instant
     HoodieActiveTimeline activeTimeline = new RollbacksCommand.RollbackTimeline(HoodieCLI.getTableMetaClient());
-    Stream<HoodieInstant> rollback = activeTimeline.getRollbackTimeline().filterCompletedInstants().getInstants();
+    Stream<HoodieInstant> rollback = activeTimeline.getRollbackTimeline().filterCompletedInstants().getInstantsAsStream();
     HoodieInstant instant = rollback.findFirst().orElse(null);
     assertNotNull(instant, "The instant can not be null.");
 

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestClusteringCommand.java
Patch:
@@ -128,12 +128,12 @@ public void testClustering() throws IOException {
 
     // assert clustering complete
     assertTrue(HoodieCLI.getTableMetaClient().getActiveTimeline().reload()
-        .filterCompletedInstants().getInstants()
+        .filterCompletedInstants().getInstantsAsStream()
         .map(HoodieInstant::getTimestamp).collect(Collectors.toList()).contains(instance),
         "Pending clustering must be completed");
 
     assertTrue(HoodieCLI.getTableMetaClient().getActiveTimeline().reload()
-            .getCompletedReplaceTimeline().getInstants()
+            .getCompletedReplaceTimeline().getInstantsAsStream()
             .map(HoodieInstant::getTimestamp).collect(Collectors.toList()).contains(instance),
         "Pending clustering must be completed");
   }
@@ -156,7 +156,7 @@ public void testClusteringScheduleAndExecute() throws IOException {
 
     // assert clustering complete
     assertTrue(HoodieCLI.getTableMetaClient().getActiveTimeline().reload()
-            .getCompletedReplaceTimeline().getInstants()
+            .getCompletedReplaceTimeline().getInstantsAsStream()
             .map(HoodieInstant::getTimestamp).count() > 0,
         "Completed clustering couldn't be 0");
   }

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestCompactionCommand.java
Patch:
@@ -136,7 +136,7 @@ public void testCompact() throws IOException {
 
     // assert compaction complete
     assertTrue(HoodieCLI.getTableMetaClient().getActiveTimeline().reload()
-        .filterCompletedInstants().getInstants()
+        .filterCompletedInstants().getInstantsAsStream()
         .map(HoodieInstant::getTimestamp).collect(Collectors.toList()).contains(instance),
         "Pending compaction must be completed");
   }
@@ -164,7 +164,7 @@ public void testCompactScheduleAndExecute() throws IOException {
 
     // assert compaction complete
     assertTrue(HoodieCLI.getTableMetaClient().getActiveTimeline().reload()
-            .filterCompletedInstants().getInstants()
+            .filterCompletedInstants().getInstantsAsStream()
             .map(HoodieInstant::getTimestamp).count() > 0,
         "Completed compaction couldn't be 0");
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/HoodieTimelineArchiver.java
Patch:
@@ -385,7 +385,7 @@ private Map<String, Boolean> deleteFilesParallelize(HoodieTableMetaClient metaCl
   private Stream<HoodieInstant> getCleanInstantsToArchive() {
     HoodieTimeline cleanAndRollbackTimeline = table.getActiveTimeline()
         .getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.CLEAN_ACTION, HoodieTimeline.ROLLBACK_ACTION)).filterCompletedInstants();
-    return cleanAndRollbackTimeline.getInstants()
+    return cleanAndRollbackTimeline.getInstantsAsStream()
         .collect(Collectors.groupingBy(HoodieInstant::getAction)).values().stream()
         .map(hoodieInstants -> {
           if (hoodieInstants.size() > this.maxInstantsToKeep) {
@@ -430,7 +430,7 @@ private Stream<HoodieInstant> getCommitInstantsToArchive() {
               : Option.empty();
 
       // Actually do the commits
-      Stream<HoodieInstant> instantToArchiveStream = commitTimeline.getInstants()
+      Stream<HoodieInstant> instantToArchiveStream = commitTimeline.getInstantsAsStream()
           .filter(s -> {
             if (config.shouldArchiveBeyondSavepoint()) {
               // skip savepoint commits and proceed further
@@ -473,7 +473,7 @@ private Stream<HoodieInstant> getInstantsToArchive() {
 
     // For archiving and cleaning instants, we need to include intermediate state files if they exist
     HoodieActiveTimeline rawActiveTimeline = new HoodieActiveTimeline(metaClient, false);
-    Map<Pair<String, String>, List<HoodieInstant>> groupByTsAction = rawActiveTimeline.getInstants()
+    Map<Pair<String, String>, List<HoodieInstant>> groupByTsAction = rawActiveTimeline.getInstantsAsStream()
         .collect(Collectors.groupingBy(i -> Pair.of(i.getTimestamp(),
             HoodieInstant.getComparableAction(i.getAction()))));
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/SimpleConcurrentFileWritesConflictResolutionStrategy.java
Patch:
@@ -59,13 +59,13 @@ public Stream<HoodieInstant> getCandidateInstants(HoodieActiveTimeline activeTim
         .getCommitsTimeline()
         .filterCompletedInstants()
         .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : HoodieTimeline.INIT_INSTANT_TS)
-        .getInstants();
+        .getInstantsAsStream();
 
     Stream<HoodieInstant> compactionAndClusteringPendingTimeline = activeTimeline
         .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))
         .findInstantsAfter(currentInstant.getTimestamp())
         .filterInflightsAndRequested()
-        .getInstants();
+        .getInstantsAsStream();
     return Stream.concat(completedCommitsInstantStream, compactionAndClusteringPendingTimeline);
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java
Patch:
@@ -131,7 +131,7 @@ public static Set<String> getInflightAndRequestedInstants(HoodieTableMetaClient
         .getActiveTimeline()
         .getTimelineOfActions(timelineActions)
         .filterInflightsAndRequested()
-        .getInstants()
+        .getInstantsAsStream()
         .map(HoodieInstant::getTimestamp)
         .collect(Collectors.toSet());
   }
@@ -144,7 +144,7 @@ public static Stream<HoodieInstant> getCompletedInstantsDuringCurrentWriteOperat
         .reloadActiveTimeline()
         .getCommitsTimeline()
         .filterCompletedInstants()
-        .getInstants()
+        .getInstantsAsStream()
         .filter(f -> pendingInstants.contains(f.getTimestamp()));
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -576,7 +576,7 @@ private boolean anyPendingDataInstant(HoodieTableMetaClient dataMetaClient, Opti
 
     // We can only initialize if there are no pending operations on the dataset
     List<HoodieInstant> pendingDataInstant = dataMetaClient.getActiveTimeline()
-        .getInstants().filter(i -> !i.isCompleted())
+        .getInstantsAsStream().filter(i -> !i.isCompleted())
         .filter(i -> !inflightInstantTimestamp.isPresent() || !i.getTimestamp().equals(inflightInstantTimestamp.get()))
         // regular writers should not be blocked due to pending indexing action
         .filter(i -> !HoodieTimeline.INDEXING_ACTION.equals(i.getAction()))
@@ -760,7 +760,7 @@ public void dropMetadataPartitions(List<MetadataPartitionType> metadataPartition
    * if the partition path in the plan matches with the given partition path.
    */
   private static void deletePendingIndexingInstant(HoodieTableMetaClient metaClient, String partitionPath) {
-    metaClient.reloadActiveTimeline().filterPendingIndexTimeline().getInstants().filter(instant -> REQUESTED.equals(instant.getState()))
+    metaClient.reloadActiveTimeline().filterPendingIndexTimeline().getInstantsAsStream().filter(instant -> REQUESTED.equals(instant.getState()))
         .forEach(instant -> {
           try {
             HoodieIndexPlan indexPlan = deserializeIndexPlan(metaClient.getActiveTimeline().readIndexPlanAsBytes(instant).get());
@@ -1015,7 +1015,7 @@ protected void compactIfNecessary(BaseHoodieWriteClient writeClient, String inst
     String latestDeltaCommitTime = metadataMetaClient.reloadActiveTimeline().getDeltaCommitTimeline().filterCompletedInstants().lastInstant()
         .get().getTimestamp();
     List<HoodieInstant> pendingInstants = dataMetaClient.reloadActiveTimeline().filterInflightsAndRequested()
-        .findInstantsBefore(instantTime).getInstants().collect(Collectors.toList());
+        .findInstantsBefore(instantTime).getInstants();
 
     if (!pendingInstants.isEmpty()) {
       LOG.info(String.format("Cannot compact metadata table as there are %d inflight instants before latest deltacommit %s: %s",

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -372,7 +372,7 @@ public HoodieTimeline getCompletedSavepointTimeline() {
    * Get the list of savepoint timestamps in this table.
    */
   public Set<String> getSavepointTimestamps() {
-    return getCompletedSavepointTimeline().getInstants().map(HoodieInstant::getTimestamp).collect(Collectors.toSet());
+    return getCompletedSavepointTimeline().getInstantsAsStream().map(HoodieInstant::getTimestamp).collect(Collectors.toSet());
   }
 
   public HoodieActiveTimeline getActiveTimeline() {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java
Patch:
@@ -238,7 +238,7 @@ public HoodieCleanMetadata execute() {
     List<HoodieCleanMetadata> cleanMetadataList = new ArrayList<>();
     // If there are inflight(failed) or previously requested clean operation, first perform them
     List<HoodieInstant> pendingCleanInstants = table.getCleanTimeline()
-        .filterInflightsAndRequested().getInstants().collect(Collectors.toList());
+        .filterInflightsAndRequested().getInstants();
     if (pendingCleanInstants.size() > 0) {
       // try to clean old history schema.
       try {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java
Patch:
@@ -187,7 +187,7 @@ private List<String> getPartitionPathsForIncrementalCleaning(HoodieCleanMetadata
     LOG.info("Incremental Cleaning mode is enabled. Looking up partition-paths that have since changed "
         + "since last cleaned at " + cleanMetadata.getEarliestCommitToRetain()
         + ". New Instant to retain : " + newInstantToRetain);
-    return hoodieTable.getCompletedCommitsTimeline().getInstants().filter(
+    return hoodieTable.getCompletedCommitsTimeline().getInstantsAsStream().filter(
         instant -> HoodieTimeline.compareTimestamps(instant.getTimestamp(), HoodieTimeline.GREATER_THAN_OR_EQUALS,
             cleanMetadata.getEarliestCommitToRetain()) && HoodieTimeline.compareTimestamps(instant.getTimestamp(),
             HoodieTimeline.LESSER_THAN, newInstantToRetain.get().getTimestamp())).flatMap(instant -> {
@@ -493,7 +493,7 @@ public Option<HoodieInstant> getEarliestCommitToRetain() {
       Instant instant = Instant.now();
       ZonedDateTime currentDateTime = ZonedDateTime.ofInstant(instant, ZoneId.systemDefault());
       String earliestTimeToRetain = HoodieActiveTimeline.formatDate(Date.from(currentDateTime.minusHours(hoursRetained).toInstant()));
-      earliestCommitToRetain = Option.fromJavaOptional(commitTimeline.getInstants().filter(i -> HoodieTimeline.compareTimestamps(i.getTimestamp(),
+      earliestCommitToRetain = Option.fromJavaOptional(commitTimeline.getInstantsAsStream().filter(i -> HoodieTimeline.compareTimestamps(i.getTimestamp(),
               HoodieTimeline.GREATER_THAN_OR_EQUALS, earliestTimeToRetain)).findFirst());
     }
     return earliestCommitToRetain;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java
Patch:
@@ -99,7 +99,7 @@ public Option<HoodieCompactionPlan> execute() {
       }
       // Committed and pending compaction instants should have strictly lower timestamps
       List<HoodieInstant> conflictingInstants = table.getActiveTimeline()
-          .getWriteTimeline().filterCompletedAndCompactionInstants().getInstants()
+          .getWriteTimeline().filterCompletedAndCompactionInstants().getInstantsAsStream()
           .filter(instant -> HoodieTimeline.compareTimestamps(
               instant.getTimestamp(), HoodieTimeline.GREATER_THAN_OR_EQUALS, instantTime))
           .collect(Collectors.toList());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java
Patch:
@@ -140,7 +140,7 @@ public HoodieRollbackMetadata execute() {
 
   private void validateSavepointRollbacks() {
     // Check if any of the commits is a savepoint - do not allow rollback on those commits
-    List<String> savepoints = table.getCompletedSavepointTimeline().getInstants()
+    List<String> savepoints = table.getCompletedSavepointTimeline().getInstantsAsStream()
         .map(HoodieInstant::getTimestamp)
         .collect(Collectors.toList());
     savepoints.forEach(s -> {
@@ -176,7 +176,7 @@ private void validateRollbackCommitSequence() {
         }
       }
 
-      List<String> inflights = inflightAndRequestedCommitTimeline.getInstants().filter(instant -> {
+      List<String> inflights = inflightAndRequestedCommitTimeline.getInstantsAsStream().filter(instant -> {
         if (!instant.getAction().equals(HoodieTimeline.REPLACE_COMMIT_ACTION)) {
           return true;
         }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/repair/RepairUtils.java
Patch:
@@ -121,8 +121,8 @@ public static List<String> findInstantFilesToRemove(
       String instantToRepair, List<String> baseAndLogFilesFromFs,
       HoodieActiveTimeline activeTimeline, HoodieArchivedTimeline archivedTimeline) {
     // Skips the instant if it is requested or inflight in active timeline
-    if (activeTimeline.filter(instant -> instant.getTimestamp().equals(instantToRepair)
-        && !instant.isCompleted()).getInstants().findAny().isPresent()) {
+    if (!activeTimeline.filter(instant -> instant.getTimestamp().equals(instantToRepair)
+        && !instant.isCompleted()).empty()) {
       return Collections.emptyList();
     }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/ZeroToOneUpgradeHandler.java
Patch:
@@ -86,7 +86,7 @@ protected void recreateMarkers(final String commitInstantTime,
                                  int parallelism) throws HoodieRollbackException {
     try {
       // fetch hoodie instant
-      Option<HoodieInstant> commitInstantOpt = Option.fromJavaOptional(table.getActiveTimeline().getCommitsTimeline().getInstants()
+      Option<HoodieInstant> commitInstantOpt = Option.fromJavaOptional(table.getActiveTimeline().getCommitsTimeline().getInstantsAsStream()
           .filter(instant -> HoodieActiveTimeline.EQUALS.test(instant.getTimestamp(), commitInstantTime))
           .findFirst());
       if (commitInstantOpt.isPresent()) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkConsistentBucketClusteringPlanStrategy.java
Patch:
@@ -86,7 +86,7 @@ public boolean checkPrecondition() {
     HoodieTimeline timeline = getHoodieTable().getActiveTimeline().getDeltaCommitTimeline().filterInflightsAndRequested();
     if (!timeline.empty()) {
       LOG.warn("When using consistent bucket, clustering cannot be scheduled async if there are concurrent writers. "
-          + "Writer instant: " + timeline.getInstants().collect(Collectors.toList()));
+          + "Writer instant: " + timeline.getInstants());
       return false;
     }
     return true;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkConsistentBucketDuplicateUpdateStrategy.java
Patch:
@@ -88,7 +88,7 @@ public Pair<HoodieData<HoodieRecord<T>>, Set<HoodieFileGroupId>> handleUpdate(Ho
 
     // Read all pending/ongoing clustering plans
     List<Pair<HoodieInstant, HoodieClusteringPlan>> instantPlanPairs =
-        table.getMetaClient().getActiveTimeline().filterInflightsAndRequested().filter(instant -> instant.getAction().equals(HoodieTimeline.REPLACE_COMMIT_ACTION)).getInstants()
+        table.getMetaClient().getActiveTimeline().filterInflightsAndRequested().filter(instant -> instant.getAction().equals(HoodieTimeline.REPLACE_COMMIT_ACTION)).getInstantsAsStream()
             .map(instant -> ClusteringUtils.getClusteringPlan(table.getMetaClient(), instant))
             .flatMap(o -> o.isPresent() ? Stream.of(o.get()) : Stream.empty())
             .collect(Collectors.toList());

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientMultiWriter.java
Patch:
@@ -414,9 +414,9 @@ public void testMultiWriterWithAsyncTableServicesWithConflict(HoodieTableType ta
 
     validInstants.addAll(
         metaClient.reloadActiveTimeline().getCompletedReplaceTimeline()
-            .filterCompletedInstants().getInstants().map(HoodieInstant::getTimestamp).collect(Collectors.toSet()));
+            .filterCompletedInstants().getInstantsAsStream().map(HoodieInstant::getTimestamp).collect(Collectors.toSet()));
     Set<String> completedInstants = metaClient.reloadActiveTimeline().getCommitsTimeline()
-        .filterCompletedInstants().getInstants().map(HoodieInstant::getTimestamp)
+        .filterCompletedInstants().getInstantsAsStream().map(HoodieInstant::getTimestamp)
         .collect(Collectors.toSet());
     assertTrue(validInstants.containsAll(completedInstants));
   }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestDataValidationCheckForLogCompactionActions.java
Patch:
@@ -157,9 +157,9 @@ public void stressTestCompactionAndLogCompactionOperations(int seed) throws Exce
         scheduleLogCompactionOnExperimentTable(experimentTable);
 
         // Verify that no compaction plans are left on the timeline.
-        assertEquals(0, mainTable.metaClient.reloadActiveTimeline().filterPendingCompactionTimeline().getInstants().count());
-        assertEquals(0, experimentTable.metaClient.reloadActiveTimeline().filterPendingCompactionTimeline().getInstants().count());
-        assertEquals(0, experimentTable.metaClient.reloadActiveTimeline().filterPendingLogCompactionTimeline().getInstants().count());
+        assertEquals(0, mainTable.metaClient.reloadActiveTimeline().filterPendingCompactionTimeline().countInstants());
+        assertEquals(0, experimentTable.metaClient.reloadActiveTimeline().filterPendingCompactionTimeline().countInstants());
+        assertEquals(0, experimentTable.metaClient.reloadActiveTimeline().filterPendingLogCompactionTimeline().countInstants());
 
         // Verify the records in both the tables.
         verifyRecords(mainTable, experimentTable);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -619,7 +619,7 @@ private void testUpsertsInternal(HoodieWriteConfig config,
         0, 150);
 
     HoodieActiveTimeline activeTimeline = new HoodieActiveTimeline(metaClient, false);
-    List<HoodieInstant> instants = activeTimeline.getCommitTimeline().getInstants().collect(Collectors.toList());
+    List<HoodieInstant> instants = activeTimeline.getCommitTimeline().getInstants();
     assertEquals(5, instants.size());
     assertEquals(new HoodieInstant(COMPLETED, COMMIT_ACTION, "001"),
         instants.get(0));

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnMergeOnReadStorage.java
Patch:
@@ -516,7 +516,7 @@ public void testArchivalOnLogCompaction() throws Exception {
       }
     }
     boolean logCompactionInstantArchived = false;
-    Map<String, List<HoodieInstant>> instantsMap = metaClient.getArchivedTimeline().getInstants()
+    Map<String, List<HoodieInstant>> instantsMap = metaClient.getArchivedTimeline().getInstantsAsStream()
         .collect(Collectors.groupingBy(HoodieInstant::getTimestamp));
     for (String logCompactionTimeStamp: logCompactionInstantTimes) {
       List<HoodieInstant> instants = instantsMap.get(logCompactionTimeStamp);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/clean/TestCleanerInsertAndCleanByCommits.java
Patch:
@@ -158,11 +158,11 @@ private void testInsertAndCleanByCommits(
         // NOTE: See CleanPlanner#getFilesToCleanKeepingLatestCommits. We explicitly keep one commit before earliest
         // commit
         Option<HoodieInstant> earliestRetainedCommit = activeTimeline.nthFromLastInstant(maxCommits);
-        Set<HoodieInstant> acceptableCommits = activeTimeline.getInstants().collect(Collectors.toSet());
+        Set<HoodieInstant> acceptableCommits = activeTimeline.getInstantsAsStream().collect(Collectors.toSet());
         if (earliestRetainedCommit.isPresent()) {
           acceptableCommits
               .removeAll(activeTimeline.findInstantsInRange("000", earliestRetainedCommit.get().getTimestamp())
-                  .getInstants().collect(Collectors.toSet()));
+                  .getInstantsAsStream().collect(Collectors.toSet()));
           acceptableCommits.add(earliestRetainedCommit.get());
         }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/clean/TestCleanerInsertAndCleanByVersions.java
Patch:
@@ -190,7 +190,7 @@ private void testInsertAndCleanByVersions(
         for (String partitionPath : dataGen.getPartitionPaths()) {
           // compute all the versions of all files, from time 0
           HashMap<String, TreeSet<String>> fileIdToVersions = new HashMap<>();
-          for (HoodieInstant entry : timeline.getInstants().collect(Collectors.toList())) {
+          for (HoodieInstant entry : timeline.getInstants()) {
             HoodieCommitMetadata commitMetadata =
                 HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(entry).get(), HoodieCommitMetadata.class);
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/CompactionTestBase.java
Patch:
@@ -154,7 +154,7 @@ protected void moveCompactionFromRequestedToInflight(String compactionInstantTim
     HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();
     HoodieInstant compactionInstant = HoodieTimeline.getCompactionRequestedInstant(compactionInstantTime);
     metaClient.getActiveTimeline().transitionCompactionRequestedToInflight(compactionInstant);
-    HoodieInstant instant = metaClient.getActiveTimeline().reload().filterPendingCompactionTimeline().getInstants()
+    HoodieInstant instant = metaClient.getActiveTimeline().reload().filterPendingCompactionTimeline().getInstantsAsStream()
         .filter(in -> in.getTimestamp().equals(compactionInstantTime)).findAny().get();
     assertTrue(instant.isInflight(), "Instant must be marked inflight");
   }
@@ -219,7 +219,7 @@ protected void executeCompactionWithReplacedFiles(String compactionInstantTime,
     table = getHoodieTable(HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).setLoadActiveTimelineOnLoad(true).build(), cfg);
     HoodieTimeline timeline = table.getMetaClient().getCommitTimeline().filterCompletedInstants();
     // verify compaction commit is visible in timeline
-    assertTrue(timeline.filterCompletedInstants().getInstants()
+    assertTrue(timeline.filterCompletedInstants().getInstantsAsStream()
         .filter(instant -> compactionInstantTime.equals(instant.getTimestamp())).findFirst().isPresent());
     for (String partition: partitions) {
       table.getSliceView().getLatestFileSlicesBeforeOrOn(partition, compactionInstantTime, true).forEach(fs -> {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestAsyncCompaction.java
Patch:
@@ -92,7 +92,7 @@ public void testRollbackForInflightCompaction() throws Exception {
           new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, compactionInstantTime));
       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();
       pendingCompactionInstant = metaClient.getCommitsAndCompactionTimeline().filterPendingCompactionTimeline()
-          .getInstants().findFirst().get();
+          .getInstantsAsStream().findFirst().get();
       assertEquals("compaction", pendingCompactionInstant.getAction());
       assertEquals(State.REQUESTED, pendingCompactionInstant.getState());
       assertEquals(compactionInstantTime, pendingCompactionInstant.getTimestamp());
@@ -148,7 +148,7 @@ public void testRollbackInflightIngestionWithPendingCompaction() throws Exceptio
       inflightInstant = metaClient.getActiveTimeline().filterPendingExcludingCompaction().firstInstant().get();
       assertEquals(inflightInstant.getTimestamp(), nextInflightInstantTime, "inflight instant has expected instant time");
       assertEquals(1, metaClient.getActiveTimeline()
-              .filterPendingExcludingCompaction().getInstants().count(),
+              .filterPendingExcludingCompaction().countInstants(),
           "Expect only one inflight instant");
       // Expect pending Compaction to be present
       pendingCompactionInstant = metaClient.getActiveTimeline().filterPendingCompactionTimeline().firstInstant().get();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java
Patch:
@@ -126,7 +126,7 @@ public void testSuccessfulCompactionBasedOnNumAfterCompactionRequest() throws Ex
       scheduleCompaction(requestInstant, writeClient, cfg);
 
       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();
-      assertEquals(metaClient.getActiveTimeline().getInstants()
+      assertEquals(metaClient.getActiveTimeline().getInstantsAsStream()
             .filter(hoodieInstant -> hoodieInstant.getAction().equals(HoodieTimeline.COMPACTION_ACTION)
                   && hoodieInstant.getState() == HoodieInstant.State.REQUESTED).count(), 1);
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java
Patch:
@@ -157,8 +157,8 @@ public static Dataset<Row> readCommit(String basePath, SQLContext sqlContext, Ho
   public static long countRecordsOptionallySince(JavaSparkContext jsc, String basePath, SQLContext sqlContext,
                                                  HoodieTimeline commitTimeline, Option<String> lastCommitTimeOpt) {
     List<HoodieInstant> commitsToReturn =
-        lastCommitTimeOpt.isPresent() ? commitTimeline.findInstantsAfter(lastCommitTimeOpt.get(), Integer.MAX_VALUE).getInstants().collect(Collectors.toList()) :
-            commitTimeline.getInstants().collect(Collectors.toList());
+        lastCommitTimeOpt.isPresent() ? commitTimeline.findInstantsAfter(lastCommitTimeOpt.get(), Integer.MAX_VALUE).getInstants() :
+            commitTimeline.getInstants();
     try {
       // Go over the commit metadata, and obtain the new files that need to be read.
       HashMap<String, String> fileIdToFullPath = getLatestFileIDsToFullPath(basePath, commitTimeline, commitsToReturn);

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -520,7 +520,7 @@ public static FileStatus[] scanFiles(FileSystem fs, Path metaPath, PathFilter na
    * @return {@code true} if any commits are found, else {@code false}.
    */
   public boolean isTimelineNonEmpty() {
-    return getCommitsTimeline().filterCompletedInstants().getInstants().collect(Collectors.toList()).size() > 0;
+    return !getCommitsTimeline().filterCompletedInstants().empty();
   }
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/table/cdc/HoodieCDCExtractor.java
Patch:
@@ -213,7 +213,7 @@ private void initInstantAndCommitMetadatas() {
     try {
       Set<String> requiredActions = new HashSet<>(Arrays.asList(COMMIT_ACTION, DELTA_COMMIT_ACTION, REPLACE_COMMIT_ACTION));
       HoodieActiveTimeline activeTimeLine = metaClient.getActiveTimeline();
-      this.commits = activeTimeLine.getInstants()
+      this.commits = activeTimeLine.getInstantsAsStream()
           .filter(instant ->
               instant.isCompleted()
                   && instantRange.isInRange(instant.getTimestamp())

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -337,7 +337,7 @@ public Option<Pair<HoodieInstant, HoodieCommitMetadata>> getLastCommitMetadataWi
   private Stream<Pair<HoodieInstant, HoodieCommitMetadata>> getCommitMetadataStream() {
     // NOTE: Streams are lazy
     return getCommitsTimeline().filterCompletedInstants()
-        .getInstants()
+        .getInstantsAsStream()
         .sorted(Comparator.comparing(HoodieInstant::getTimestamp).reversed())
         .map(instant -> {
           try {

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieArchivedTimeline.java
Patch:
@@ -365,7 +365,7 @@ private int getArchivedFileSuffix(FileStatus f) {
   public HoodieDefaultTimeline getWriteTimeline() {
     // filter in-memory instants
     Set<String> validActions = CollectionUtils.createSet(COMMIT_ACTION, DELTA_COMMIT_ACTION, COMPACTION_ACTION, LOG_COMPACTION_ACTION, REPLACE_COMMIT_ACTION);
-    return new HoodieDefaultTimeline(getInstants().filter(i ->
+    return new HoodieDefaultTimeline(getInstantsAsStream().filter(i ->
             readCommits.containsKey(i.getTimestamp()))
         .filter(s -> validActions.contains(s.getAction())), details);
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimelineUtils.java
Patch:
@@ -65,7 +65,7 @@ public static List<String> getWrittenPartitions(HoodieTimeline timeline) {
   public static List<String> getDroppedPartitions(HoodieTimeline timeline) {
     HoodieTimeline replaceCommitTimeline = timeline.getWriteTimeline().filterCompletedInstants().getCompletedReplaceTimeline();
 
-    return replaceCommitTimeline.getInstants().flatMap(instant -> {
+    return replaceCommitTimeline.getInstantsAsStream().flatMap(instant -> {
       try {
         HoodieReplaceCommitMetadata commitMetadata = HoodieReplaceCommitMetadata.fromBytes(
             replaceCommitTimeline.getInstantDetails(instant).get(), HoodieReplaceCommitMetadata.class);
@@ -85,7 +85,7 @@ public static List<String> getDroppedPartitions(HoodieTimeline timeline) {
    * Returns partitions that have been modified including internal operations such as clean in the passed timeline.
    */
   public static List<String> getAffectedPartitions(HoodieTimeline timeline) {
-    return timeline.filterCompletedInstants().getInstants().flatMap(s -> {
+    return timeline.filterCompletedInstants().getInstantsAsStream().flatMap(s -> {
       switch (s.getAction()) {
         case HoodieTimeline.COMMIT_ACTION:
         case HoodieTimeline.DELTA_COMMIT_ACTION:

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/TimelineDTO.java
Patch:
@@ -39,7 +39,7 @@ public class TimelineDTO {
 
   public static TimelineDTO fromTimeline(HoodieTimeline timeline) {
     TimelineDTO dto = new TimelineDTO();
-    dto.instants = timeline.getInstants().map(InstantDTO::fromInstant).collect(Collectors.toList());
+    dto.instants = timeline.getInstantsAsStream().map(InstantDTO::fromInstant).collect(Collectors.toList());
     return dto;
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -219,7 +219,7 @@ private void resetFileGroupsReplaced(HoodieTimeline timeline) {
     HoodieTimer hoodieTimer = HoodieTimer.start();
     // for each REPLACE instant, get map of (partitionPath -> deleteFileGroup)
     HoodieTimeline replacedTimeline = timeline.getCompletedReplaceTimeline();
-    Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> resultStream = replacedTimeline.getInstants().flatMap(instant -> {
+    Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> resultStream = replacedTimeline.getInstantsAsStream().flatMap(instant -> {
       try {
         HoodieReplaceCommitMetadata replaceMetadata = HoodieReplaceCommitMetadata.fromBytes(metaClient.getActiveTimeline().getInstantDetails(instant).get(),
             HoodieReplaceCommitMetadata.class);
@@ -394,7 +394,7 @@ protected boolean isBaseFileDueToPendingCompaction(HoodieBaseFile baseFile) {
    */
   protected boolean isBaseFileDueToPendingClustering(HoodieBaseFile baseFile) {
     List<String> pendingReplaceInstants =
-        metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().map(HoodieInstant::getTimestamp).collect(Collectors.toList());
+        metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstantsAsStream().map(HoodieInstant::getTimestamp).collect(Collectors.toList());
 
     return !pendingReplaceInstants.isEmpty() && pendingReplaceInstants.contains(baseFile.getCommitTime());
   }

File: hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java
Patch:
@@ -66,7 +66,7 @@ public class ClusteringUtils {
   public static Stream<Pair<HoodieInstant, HoodieClusteringPlan>> getAllPendingClusteringPlans(
       HoodieTableMetaClient metaClient) {
     List<HoodieInstant> pendingReplaceInstants =
-        metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().collect(Collectors.toList());
+        metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants();
     return pendingReplaceInstants.stream().map(instant -> getClusteringPlan(metaClient, instant))
         .filter(Option::isPresent).map(Option::get);
   }
@@ -216,7 +216,7 @@ private static Map<String, Double> buildMetrics(List<FileSlice> fileSlices) {
   }
 
   public static List<HoodieInstant> getPendingClusteringInstantTimes(HoodieTableMetaClient metaClient) {
-    return metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants()
+    return metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstantsAsStream()
             .filter(instant -> isPendingClusteringInstant(metaClient, instant))
             .collect(Collectors.toList());
   }

File: hudi-common/src/main/java/org/apache/hudi/common/util/CompactionUtils.java
Patch:
@@ -158,7 +158,7 @@ public static List<Pair<HoodieInstant, HoodieCompactionPlan>> getAllPendingLogCo
   private static List<Pair<HoodieInstant, HoodieCompactionPlan>> getCompactionPlansByTimeline(
       HoodieTableMetaClient metaClient, Function<HoodieTableMetaClient, HoodieTimeline> filteredTimelineSupplier,
       Function<String, HoodieInstant> requestedInstantWrapper) {
-    List<HoodieInstant> filteredInstants = filteredTimelineSupplier.apply(metaClient).getInstants().collect(Collectors.toList());
+    List<HoodieInstant> filteredInstants = filteredTimelineSupplier.apply(metaClient).getInstants();
     return filteredInstants.stream()
         .map(instant -> Pair.of(instant, getCompactionPlan(metaClient, requestedInstantWrapper.apply(instant.getTimestamp()))))
         .collect(Collectors.toList());
@@ -265,7 +265,7 @@ public static Stream<Pair<HoodieFileGroupId, Pair<String, HoodieCompactionOperat
    * @return
    */
   public static List<HoodieInstant> getPendingCompactionInstantTimes(HoodieTableMetaClient metaClient) {
-    return metaClient.getActiveTimeline().filterPendingCompactionTimeline().getInstants().collect(Collectors.toList());
+    return metaClient.getActiveTimeline().filterPendingCompactionTimeline().getInstants();
   }
 
   /**
@@ -356,7 +356,7 @@ public static Option<HoodieInstant> getOldestInstantToRetainForCompaction(
         return Option.of(deltaCommitsInfo.getRight());
       } else {
         // delta commits with the last one to keep
-        List<HoodieInstant> instants = deltaCommitTimeline.getInstants()
+        List<HoodieInstant> instants = deltaCommitTimeline.getInstantsAsStream()
             .limit(numDeltaCommits - maxDeltaCommits + 1).collect(Collectors.toList());
         return Option.of(instants.get(instants.size() - 1));
       }

File: hudi-common/src/main/java/org/apache/hudi/common/util/InternalSchemaCache.java
Patch:
@@ -117,7 +117,7 @@ private static TreeMap<Long, InternalSchema> getHistoricalSchemas(HoodieTableMet
   private static Option<InternalSchema> getSchemaByReadingCommitFile(long versionID, HoodieTableMetaClient metaClient) {
     try {
       HoodieTimeline timeline = metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();
-      List<HoodieInstant> instants = timeline.getInstants().filter(f -> f.getTimestamp().equals(String.valueOf(versionID))).collect(Collectors.toList());
+      List<HoodieInstant> instants = timeline.getInstantsAsStream().filter(f -> f.getTimestamp().equals(String.valueOf(versionID))).collect(Collectors.toList());
       if (instants.isEmpty()) {
         return Option.empty();
       }

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/io/FileBasedInternalSchemaStorageManager.java
Patch:
@@ -131,7 +131,7 @@ public void cleanOldFiles(List<String> validateCommits) {
 
   private List<String> getValidInstants() {
     return getMetaClient().getCommitsTimeline()
-        .filterCompletedInstants().getInstants().map(f -> f.getTimestamp()).collect(Collectors.toList());
+        .filterCompletedInstants().getInstantsAsStream().map(f -> f.getTimestamp()).collect(Collectors.toList());
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -455,14 +455,14 @@ private Set<String> getValidInstantTimestamps() {
     // Only those log files which have a corresponding completed instant on the dataset should be read
     // This is because the metadata table is updated before the dataset instants are committed.
     HoodieActiveTimeline datasetTimeline = dataMetaClient.getActiveTimeline();
-    Set<String> validInstantTimestamps = datasetTimeline.filterCompletedInstants().getInstants()
+    Set<String> validInstantTimestamps = datasetTimeline.filterCompletedInstants().getInstantsAsStream()
         .map(HoodieInstant::getTimestamp).collect(Collectors.toSet());
 
     // For any rollbacks and restores, we cannot neglect the instants that they are rolling back.
     // The rollback instant should be more recent than the start of the timeline for it to have rolled back any
     // instant which we have a log block for.
     final String earliestInstantTime = validInstantTimestamps.isEmpty() ? SOLO_COMMIT_TIMESTAMP : Collections.min(validInstantTimestamps);
-    datasetTimeline.getRollbackAndRestoreTimeline().filterCompletedInstants().getInstants()
+    datasetTimeline.getRollbackAndRestoreTimeline().filterCompletedInstants().getInstantsAsStream()
         .filter(instant -> HoodieTimeline.compareTimestamps(instant.getTimestamp(), HoodieTimeline.GREATER_THAN, earliestInstantTime))
         .forEach(instant -> {
           validInstantTimestamps.addAll(getRollbackedCommits(instant, datasetTimeline));

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestHoodieTableMetaClient.java
Patch:
@@ -76,7 +76,7 @@ public void checkSerDe() {
     commitTimeline.saveAsComplete(instant, Option.of("test-detail".getBytes()));
     commitTimeline = commitTimeline.reload();
     HoodieInstant completedInstant = HoodieTimeline.getCompletedInstant(instant);
-    assertEquals(completedInstant, commitTimeline.getInstants().findFirst().get(),
+    assertEquals(completedInstant, commitTimeline.getInstantsAsStream().findFirst().get(),
         "Commit should be 1 and completed");
     assertArrayEquals("test-detail".getBytes(), commitTimeline.getInstantDetails(completedInstant).get(),
         "Commit value should be \"test-detail\"");
@@ -101,7 +101,7 @@ public void checkCommitTimeline() {
     activeTimeline = activeTimeline.reload();
     activeCommitTimeline = activeTimeline.getCommitTimeline();
     assertFalse(activeCommitTimeline.empty(), "Should be the 1 commit we made");
-    assertEquals(completedInstant, activeCommitTimeline.getInstants().findFirst().get(),
+    assertEquals(completedInstant, activeCommitTimeline.getInstantsAsStream().findFirst().get(),
         "Commit should be 1");
     assertArrayEquals("test-detail".getBytes(), activeCommitTimeline.getInstantDetails(completedInstant).get(),
         "Commit value should be \"test-detail\"");

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java
Patch:
@@ -875,7 +875,7 @@ private Map<String, List<String>> testMultipleWriteSteps(SyncableFileSystemView
       assertEquals(State.COMPLETED, view.getLastInstant().get().getState());
       assertEquals(lastInstant.getAction(), view.getLastInstant().get().getAction(),
           "Expected Last=" + lastInstant + ", Found Instants="
-              + view.getTimeline().getInstants().collect(Collectors.toList()));
+              + view.getTimeline().getInstants());
       partitions.forEach(p -> assertEquals(fileIdsPerPartition.size(), view.getLatestFileSlices(p).count()));
       final long expTotalFileSlicesPerPartition = fileIdsPerPartition.size() * multiple;
       partitions.forEach(p -> assertEquals(expTotalFileSlicesPerPartition, view.getAllFileSlices(p).count()));

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestCompactionUtils.java
Patch:
@@ -245,14 +245,14 @@ public void testGetDeltaCommitsSinceLatestCompaction(boolean hasCompletedCompact
     Pair<HoodieTimeline, HoodieInstant> actual =
         CompactionUtils.getDeltaCommitsSinceLatestCompaction(timeline).get();
     if (hasCompletedCompaction) {
-      Stream<HoodieInstant> instants = actual.getLeft().getInstants();
+      Stream<HoodieInstant> instants = actual.getLeft().getInstantsAsStream();
       assertEquals(
           Stream.of(
               new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, "07"),
               new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, "08"),
               new HoodieInstant(true, HoodieTimeline.DELTA_COMMIT_ACTION, "09"))
               .collect(Collectors.toList()),
-          actual.getLeft().getInstants().collect(Collectors.toList()));
+          actual.getLeft().getInstants());
       assertEquals(
           new HoodieInstant(false, HoodieTimeline.COMMIT_ACTION, "06"),
           actual.getRight());
@@ -268,7 +268,7 @@ public void testGetDeltaCommitsSinceLatestCompaction(boolean hasCompletedCompact
               new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, "08"),
               new HoodieInstant(true, HoodieTimeline.DELTA_COMMIT_ACTION, "09"))
               .collect(Collectors.toList()),
-          actual.getLeft().getInstants().collect(Collectors.toList()));
+          actual.getLeft().getInstants());
       assertEquals(
           new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, "01"),
           actual.getRight());

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/partitioner/profile/WriteProfile.java
Patch:
@@ -262,7 +262,7 @@ public synchronized void reload(long checkpointId) {
     oldFsView.close();
 
     recordProfile();
-    cleanMetadataCache(this.metaClient.getCommitsTimeline().filterCompletedInstants().getInstants());
+    cleanMetadataCache(this.metaClient.getCommitsTimeline().filterCompletedInstants().getInstantsAsStream());
     this.smallFilesMap.clear();
     this.reloadedCheckpointId = checkpointId;
   }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/IncrementalInputSplits.java
Patch:
@@ -449,7 +449,7 @@ private List<HoodieCommitMetadata> getArchivedMetadata(
       // read the archived metadata if the start instant is archived.
       HoodieTimeline archivedTimeline = getArchivedReadTimeline(metaClient, instantRange.getStartInstant());
       if (!archivedTimeline.empty()) {
-        return archivedTimeline.getInstants()
+        return archivedTimeline.getInstantsAsStream()
             .map(instant -> WriteProfiles.getCommitMetadata(tableName, path, instant, archivedTimeline)).collect(Collectors.toList());
       }
     }
@@ -481,12 +481,12 @@ private List<HoodieInstant> filterInstantsWithRange(
     if (issuedInstant != null) {
       // returns early for streaming mode
       return commitTimeline
-          .getInstants()
+          .getInstantsAsStream()
           .filter(s -> HoodieTimeline.compareTimestamps(s.getTimestamp(), GREATER_THAN, issuedInstant))
           .collect(Collectors.toList());
     }
 
-    Stream<HoodieInstant> instantStream = completedTimeline.getInstants();
+    Stream<HoodieInstant> instantStream = completedTimeline.getInstantsAsStream();
 
     if (OptionsResolver.isSpecificStartCommit(this.conf)) {
       final String startCommit = this.conf.get(FlinkOptions.READ_START_COMMIT);

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/ITTestHoodieDataSource.java
Patch:
@@ -378,7 +378,7 @@ void testStreamWriteWithCleaning() {
     Configuration conf = Configuration.fromMap(options1);
     HoodieTimeline timeline = StreamerUtil.createMetaClient(conf).getActiveTimeline();
     assertTrue(timeline.filterCompletedInstants()
-            .getInstants().anyMatch(instant -> instant.getAction().equals("clean")),
+            .getInstantsAsStream().anyMatch(instant -> instant.getAction().equals("clean")),
         "some commits should be cleaned");
   }
 

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/format/TestInputFormat.java
Patch:
@@ -631,7 +631,7 @@ void testReadIncrementally(HoodieTableType tableType) throws Exception {
     }
 
     HoodieTableMetaClient metaClient = StreamerUtil.createMetaClient(tempFile.getAbsolutePath(), HadoopConfigurations.getHadoopConf(conf));
-    List<String> commits = metaClient.getCommitsTimeline().filterCompletedInstants().getInstants()
+    List<String> commits = metaClient.getCommitsTimeline().filterCompletedInstants().getInstantsAsStream()
         .map(HoodieInstant::getTimestamp).collect(Collectors.toList());
 
     assertThat(commits.size(), is(3));
@@ -748,13 +748,13 @@ void testReadArchivedCommitsIncrementally() throws Exception {
     writeClient.clean();
 
     HoodieTableMetaClient metaClient = StreamerUtil.createMetaClient(tempFile.getAbsolutePath(), HadoopConfigurations.getHadoopConf(conf));
-    List<String> commits = metaClient.getCommitsTimeline().filterCompletedInstants().getInstants()
+    List<String> commits = metaClient.getCommitsTimeline().filterCompletedInstants().getInstantsAsStream()
         .map(HoodieInstant::getTimestamp).collect(Collectors.toList());
 
     assertThat(commits.size(), is(4));
 
     List<String> archivedCommits = metaClient.getArchivedTimeline().getCommitsTimeline().filterCompletedInstants()
-        .getInstants().map(HoodieInstant::getTimestamp).collect(Collectors.toList());
+        .getInstantsAsStream().map(HoodieInstant::getTimestamp).collect(Collectors.toList());
 
     assertThat(archivedCommits.size(), is(6));
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieMergeOnReadTableInputFormat.java
Patch:
@@ -148,7 +148,7 @@ protected List<FileStatus> listStatusForIncrementalMode(JobConf job,
       return result;
     }
     HoodieTimeline commitsTimelineToReturn = HoodieInputFormatUtils.getHoodieTimelineForIncrementalQuery(jobContext, incrementalTableName, timeline.get());
-    Option<List<HoodieInstant>> commitsToCheck = Option.of(commitsTimelineToReturn.getInstants().collect(Collectors.toList()));
+    Option<List<HoodieInstant>> commitsToCheck = Option.of(commitsTimelineToReturn.getInstants());
     if (!commitsToCheck.isPresent()) {
       return result;
     }

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java
Patch:
@@ -283,8 +283,7 @@ public static Option<HoodieTimeline> getFilteredCommitsTimeline(JobContext job,
    * @return
    */
   public static Option<List<HoodieInstant>> getCommitsForIncrementalQuery(Job job, String tableName, HoodieTimeline timeline) {
-    return Option.of(getHoodieTimelineForIncrementalQuery(job, tableName, timeline)
-        .getInstants().collect(Collectors.toList()));
+    return Option.of(getHoodieTimelineForIncrementalQuery(job, tableName, timeline).getInstants());
   }
 
   /**

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateAsyncOperations.java
Patch:
@@ -69,7 +69,7 @@ public void execute(ExecutionContext executionContext, int curItrCount) throws E
           String earliestCommitToRetain = cleanMetadata.getEarliestCommitToRetain();
           log.warn("Earliest commit to retain : " + earliestCommitToRetain);
           long unCleanedInstants = metaClient.getActiveTimeline().filterCompletedInstants().filter(instant ->
-              HoodieTimeline.compareTimestamps(instant.getTimestamp(), HoodieTimeline.GREATER_THAN_OR_EQUALS, earliestCommitToRetain)).getInstants().count();
+              HoodieTimeline.compareTimestamps(instant.getTimestamp(), HoodieTimeline.GREATER_THAN_OR_EQUALS, earliestCommitToRetain)).countInstants();
           ValidationUtils.checkArgument(unCleanedInstants >= (maxCommitsRetained + 1), "Total uncleaned instants " + unCleanedInstants
               + " mismatched with max commits retained " + (maxCommitsRetained + 1));
         }

File: hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/HoodieDataSourceHelpers.java
Patch:
@@ -55,7 +55,7 @@ public static boolean hasNewCommits(FileSystem fs, String basePath, String commi
   @PublicAPIMethod(maturity = ApiMaturityLevel.STABLE)
   public static List<String> listCommitsSince(FileSystem fs, String basePath, String instantTimestamp) {
     HoodieTimeline timeline = allCompletedCommitsCompactions(fs, basePath);
-    return timeline.findInstantsAfter(instantTimestamp, Integer.MAX_VALUE).getInstants()
+    return timeline.findInstantsAfter(instantTimestamp, Integer.MAX_VALUE).getInstantsAsStream()
         .map(HoodieInstant::getTimestamp).collect(Collectors.toList());
   }
 

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveSyncClient.java
Patch:
@@ -249,7 +249,7 @@ public Option<String> getLastReplicatedTime(String tableName) {
   }
 
   public void updateLastReplicatedTimeStamp(String tableName, String timeStamp) {
-    if (getActiveTimeline().getInstants().noneMatch(i -> i.getTimestamp().equals(timeStamp))) {
+    if (getActiveTimeline().getInstantsAsStream().noneMatch(i -> i.getTimestamp().equals(timeStamp))) {
       throw new HoodieHiveSyncException(
           "Not a valid completed timestamp " + timeStamp + " for table " + tableName);
     }

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
Patch:
@@ -1044,7 +1044,7 @@ public void testNotPickingOlderParquetFileWhenLatestCommitReadFails(String syncM
     HiveSyncTool tool = new HiveSyncTool(hiveSyncProps, getHiveConf());
     // now delete the evolved commit instant
     Path fullPath = new Path(HiveTestUtil.basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/"
-        + hiveClient.getActiveTimeline().getInstants()
+        + hiveClient.getActiveTimeline().getInstantsAsStream()
         .filter(inst -> inst.getTimestamp().equals(commitTime2))
         .findFirst().get().getFileName());
     assertTrue(HiveTestUtil.fileSystem.delete(fullPath, false));
@@ -1088,7 +1088,7 @@ public void testNotPickingOlderParquetFileWhenLatestCommitReadFailsForExistingTa
     reinitHiveSyncClient();
     // now delete the evolved commit instant
     Path fullPath = new Path(HiveTestUtil.basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/"
-        + hiveClient.getActiveTimeline().getInstants()
+        + hiveClient.getActiveTimeline().getInstantsAsStream()
         .filter(inst -> inst.getTimestamp().equals(commitTime2))
         .findFirst().get().getFileName());
     assertTrue(HiveTestUtil.fileSystem.delete(fullPath, false));

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HiveIncrementalPuller.java
Patch:
@@ -315,13 +315,13 @@ private boolean ensureTempPathExists(FileSystem fs, String lastCommitTime) throw
   private String getLastCommitTimePulled(FileSystem fs, String sourceTableLocation) {
     HoodieTableMetaClient metadata = HoodieTableMetaClient.builder().setConf(fs.getConf()).setBasePath(sourceTableLocation).build();
     List<String> commitsToSync = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()
-        .findInstantsAfter(config.fromCommitTime, config.maxCommits).getInstants().map(HoodieInstant::getTimestamp)
+        .findInstantsAfter(config.fromCommitTime, config.maxCommits).getInstantsAsStream().map(HoodieInstant::getTimestamp)
         .collect(Collectors.toList());
     if (commitsToSync.isEmpty()) {
       LOG.warn(
           "Nothing to sync. All commits in "
               + config.sourceTable + " are " + metadata.getActiveTimeline().getCommitsTimeline()
-                  .filterCompletedInstants().getInstants().collect(Collectors.toList())
+                  .filterCompletedInstants().getInstants()
               + " and from commit time is " + config.fromCommitTime);
       return null;
     }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java
Patch:
@@ -321,7 +321,7 @@ public void doDataTableValidation() {
         Map<String, List<String>> instantToFilesMap = RepairUtils.tagInstantsOfBaseAndLogFiles(
             metaClient.getBasePath(), allDataFilePaths);
         HoodieActiveTimeline activeTimeline = metaClient.getActiveTimeline();
-        List<HoodieInstant> hoodieInstants = activeTimeline.filterCompletedInstants().getInstants().collect(Collectors.toList());
+        List<HoodieInstant> hoodieInstants = activeTimeline.filterCompletedInstants().getInstants();
 
         List<String> danglingFiles = engineContext.flatMap(hoodieInstants, instant -> {
           Option<Set<String>> filesFromTimeline = RepairUtils.getBaseAndLogFilePathsFromTimeline(

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java
Patch:
@@ -407,7 +407,7 @@ public void doMetadataTableValidation() {
     if (cfg.skipDataFilesForCleaning) {
       HoodieTimeline inflightCleaningTimeline = metaClient.getActiveTimeline().getCleanerTimeline().filterInflights();
 
-      baseFilesForCleaning = inflightCleaningTimeline.getInstants().flatMap(instant -> {
+      baseFilesForCleaning = inflightCleaningTimeline.getInstantsAsStream().flatMap(instant -> {
         try {
           // convert inflight instant to requested and get clean plan
           instant = new HoodieInstant(HoodieInstant.State.REQUESTED, instant.getAction(), instant.getTimestamp());

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -287,7 +287,7 @@ public void refreshTimeline() throws IOException {
             initializeEmptyTable();
             // reload the timeline from metaClient and validate that its empty table. If there are any instants found, then we should fail the pipeline, bcoz hoodie.properties got deleted by mistake.
             HoodieTableMetaClient metaClientToValidate = HoodieTableMetaClient.builder().setConf(new Configuration(fs.getConf())).setBasePath(cfg.targetBasePath).build();
-            if (metaClientToValidate.reloadActiveTimeline().getInstants().count() > 0) {
+            if (metaClientToValidate.reloadActiveTimeline().countInstants() > 0) {
               // Deleting the recreated hoodie.properties and throwing exception.
               fs.delete(new Path(String.format("%s%s/%s", basePathWithForwardSlash, HoodieTableMetaClient.METAFOLDER_NAME, HoodieTableConfig.HOODIE_PROPERTIES_FILE)));
               throw new HoodieIOException("hoodie.properties is missing. Likely due to some external entity. Please populate the hoodie.properties and restart the pipeline. ",
@@ -558,7 +558,7 @@ private Option<String> getCheckpointToResume(Option<HoodieTimeline> commitTimeli
           throw new HoodieDeltaStreamerException(
               "Unable to find previous checkpoint. Please double check if this table "
                   + "was indeed built via delta streamer. Last Commit :" + lastCommit + ", Instants :"
-                  + commitTimelineOpt.get().getInstants().collect(Collectors.toList()) + ", CommitMetadata="
+                  + commitTimelineOpt.get().getInstants() + ", CommitMetadata="
                   + commitMetadata.toJsonString());
         }
         // KAFKA_CHECKPOINT_TYPE will be honored only for first batch.

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java
Patch:
@@ -112,7 +112,7 @@ public static Pair<String, Pair<String, String>> calculateBeginAndEndInstants(Ja
 
     if (missingCheckpointStrategy == MissingCheckpointStrategy.READ_LATEST || !activeCommitTimeline.isBeforeTimelineStarts(beginInstantTime)) {
       Option<HoodieInstant> nthInstant = Option.fromJavaOptional(activeCommitTimeline
-          .findInstantsAfter(beginInstantTime, numInstantsPerFetch).getInstants().reduce((x, y) -> y));
+          .findInstantsAfter(beginInstantTime, numInstantsPerFetch).getInstantsAsStream().reduce((x, y) -> y));
       return Pair.of(DataSourceReadOptions.QUERY_TYPE_INCREMENTAL_OPT_VAL(), Pair.of(beginInstantTime, nthInstant.map(HoodieInstant::getTimestamp).orElse(beginInstantTime)));
     } else {
       // when MissingCheckpointStrategy is set to read everything until latest, trigger snapshot query.

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamerWithMultiWriter.java
Patch:
@@ -428,8 +428,7 @@ class GetCommitsAfterInstant {
 
     long getCommitsAfterInstant() {
       HoodieTimeline timeline1 = meta.reloadActiveTimeline().getAllCommitsTimeline().findInstantsAfter(lastSuccessfulCommit);
-      // LOG.info("Timeline Instants=" + meta1.getActiveTimeline().getInstants().collect(Collectors.toList()));
-      return timeline1.getInstants().count();
+      return timeline1.countInstants();
     }
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -279,9 +279,9 @@ private Option<IndexedRecord> getIndexedRecord(HoodieRecord<T> hoodieRecord) {
 
   private Option<IndexedRecord> getInsertValue(HoodieRecord<T> hoodieRecord) throws IOException {
     if (useWriterSchema) {
-      return hoodieRecord.getData().getInsertValue(tableSchemaWithMetaFields, recordProperties);
+      return hoodieRecord.getData().getInsertValue(writeSchemaWithMetaFields, recordProperties);
     } else {
-      return hoodieRecord.getData().getInsertValue(tableSchema, recordProperties);
+      return hoodieRecord.getData().getInsertValue(writeSchema, recordProperties);
     }
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -185,9 +185,9 @@ public void write() {
         final String key = keyIterator.next();
         HoodieRecord<T> record = recordMap.get(key);
         if (useWriterSchema) {
-          write(record, record.getData().getInsertValue(tableSchemaWithMetaFields, config.getProps()));
+          write(record, record.getData().getInsertValue(writeSchemaWithMetaFields, config.getProps()));
         } else {
-          write(record, record.getData().getInsertValue(tableSchema, config.getProps()));
+          write(record, record.getData().getInsertValue(writeSchema, config.getProps()));
         }
       }
     } catch (IOException io) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandleWithChangeLog.java
Patch:
@@ -54,7 +54,7 @@ public HoodieMergeHandleWithChangeLog(HoodieWriteConfig config, String instantTi
         hoodieTable.getMetaClient().getTableConfig(),
         partitionPath,
         fs,
-        tableSchema,
+        getWriterSchema(),
         createLogWriter(instantTime, HoodieCDCUtils.CDC_LOGFILE_SUFFIX),
         IOUtils.getMaxMemoryPerPartitionMerge(taskContextSupplier, config));
   }
@@ -72,7 +72,7 @@ public HoodieMergeHandleWithChangeLog(HoodieWriteConfig config, String instantTi
         hoodieTable.getMetaClient().getTableConfig(),
         partitionPath,
         fs,
-        tableSchema,
+        getWriterSchema(),
         createLogWriter(instantTime, HoodieCDCUtils.CDC_LOGFILE_SUFFIX),
         IOUtils.getMaxMemoryPerPartitionMerge(taskContextSupplier, config));
   }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkMergeAndReplaceHandleWithChangeLog.java
Patch:
@@ -56,7 +56,7 @@ public FlinkMergeAndReplaceHandleWithChangeLog(HoodieWriteConfig config, String
         hoodieTable.getMetaClient().getTableConfig(),
         partitionPath,
         getFileSystem(),
-        tableSchema,
+        getWriterSchema(),
         createLogWriter(instantTime, HoodieCDCUtils.CDC_LOGFILE_SUFFIX),
         IOUtils.getMaxMemoryPerPartitionMerge(taskContextSupplier, config));
   }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkMergeHandleWithChangeLog.java
Patch:
@@ -59,7 +59,7 @@ public FlinkMergeHandleWithChangeLog(HoodieWriteConfig config, String instantTim
         hoodieTable.getMetaClient().getTableConfig(),
         partitionPath,
         getFileSystem(),
-        tableSchema,
+        getWriterSchema(),
         createLogWriter(instantTime, HoodieCDCUtils.CDC_LOGFILE_SUFFIX),
         IOUtils.getMaxMemoryPerPartitionMerge(taskContextSupplier, config));
   }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java
Patch:
@@ -62,12 +62,11 @@
 import org.apache.hudi.table.action.commit.FlinkInsertOverwriteCommitActionExecutor;
 import org.apache.hudi.table.action.commit.FlinkInsertOverwriteTableCommitActionExecutor;
 import org.apache.hudi.table.action.commit.FlinkInsertPreppedCommitActionExecutor;
-import org.apache.hudi.table.action.commit.FlinkMergeHelper;
 import org.apache.hudi.table.action.commit.FlinkUpsertCommitActionExecutor;
 import org.apache.hudi.table.action.commit.FlinkUpsertPreppedCommitActionExecutor;
+import org.apache.hudi.table.action.commit.HoodieMergeHelper;
 import org.apache.hudi.table.action.rollback.BaseRollbackPlanActionExecutor;
 import org.apache.hudi.table.action.rollback.CopyOnWriteRollbackActionExecutor;
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -379,7 +378,7 @@ protected Iterator<List<WriteStatus>> handleUpdateInternal(HoodieMergeHandle<?,
       throw new HoodieUpsertException(
           "Error in finding the old file path at commit " + instantTime + " for fileId: " + fileId);
     } else {
-      FlinkMergeHelper.newInstance().runMerge(this, upsertHandle);
+      HoodieMergeHelper.newInstance().runMerge(this, upsertHandle);
     }
 
     // TODO(vc): This needs to be revisited

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java
Patch:
@@ -225,7 +225,7 @@ protected Iterator<List<WriteStatus>> handleUpdateInternal(HoodieMergeHandle<?,
       throw new HoodieUpsertException(
           "Error in finding the old file path at commit " + instantTime + " for fileId: " + fileId);
     } else {
-      FlinkMergeHelper.newInstance().runMerge(table, upsertHandle);
+      HoodieMergeHelper.newInstance().runMerge(table, upsertHandle);
     }
 
     // TODO(vc): This needs to be revisited

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java
Patch:
@@ -53,14 +53,14 @@
 import org.apache.hudi.table.action.clean.CleanPlanActionExecutor;
 import org.apache.hudi.table.action.cluster.ClusteringPlanActionExecutor;
 import org.apache.hudi.table.action.cluster.JavaExecuteClusteringCommitActionExecutor;
+import org.apache.hudi.table.action.commit.HoodieMergeHelper;
 import org.apache.hudi.table.action.commit.JavaBulkInsertCommitActionExecutor;
 import org.apache.hudi.table.action.commit.JavaBulkInsertPreppedCommitActionExecutor;
 import org.apache.hudi.table.action.commit.JavaDeleteCommitActionExecutor;
 import org.apache.hudi.table.action.commit.JavaInsertCommitActionExecutor;
 import org.apache.hudi.table.action.commit.JavaInsertOverwriteCommitActionExecutor;
 import org.apache.hudi.table.action.commit.JavaInsertOverwriteTableCommitActionExecutor;
 import org.apache.hudi.table.action.commit.JavaInsertPreppedCommitActionExecutor;
-import org.apache.hudi.table.action.commit.JavaMergeHelper;
 import org.apache.hudi.table.action.commit.JavaUpsertCommitActionExecutor;
 import org.apache.hudi.table.action.commit.JavaUpsertPreppedCommitActionExecutor;
 import org.apache.hudi.table.action.index.RunIndexActionExecutor;
@@ -285,7 +285,7 @@ protected Iterator<List<WriteStatus>> handleUpdateInternal(HoodieMergeHandle<?,
       throw new HoodieUpsertException(
           "Error in finding the old file path at commit " + instantTime + " for fileId: " + fileId);
     } else {
-      JavaMergeHelper.newInstance().runMerge(this, upsertHandle);
+      HoodieMergeHelper.newInstance().runMerge(this, upsertHandle);
     }
 
     // TODO(yihua): This needs to be revisited

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java
Patch:
@@ -278,7 +278,7 @@ protected Iterator<List<WriteStatus>> handleUpdateInternal(HoodieMergeHandle<?,?
       throw new HoodieUpsertException(
           "Error in finding the old file path at commit " + instantTime + " for fileId: " + fileId);
     } else {
-      JavaMergeHelper.newInstance().runMerge(table, upsertHandle);
+      HoodieMergeHelper.newInstance().runMerge(table, upsertHandle);
     }
 
     List<WriteStatus> statuses = upsertHandle.writeStatuses();

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestSchemaEvolutionClient.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.table.TableSchemaResolver;
 import org.apache.hudi.common.testutils.RawTripTestPayload;
+import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.internal.schema.Types;
 import org.apache.hudi.testutils.HoodieJavaClientTestHarness;
@@ -71,6 +72,7 @@ private HoodieJavaWriteClient<RawTripTestPayload> getWriteClient() {
         .withEngineType(EngineType.JAVA)
         .withPath(basePath)
         .withSchema(SCHEMA.toString())
+        .withProps(CollectionUtils.createImmutableMap(HoodieWriteConfig.TBL_NAME.key(), "hoodie_test_table"))
         .build();
     return new HoodieJavaWriteClient<>(context, config);
   }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java
Patch:
@@ -80,6 +80,7 @@
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.Path;
 import org.apache.spark.api.java.JavaRDD;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.ValueSource;
@@ -116,6 +117,8 @@
 /**
  * Test Cleaning related logic.
  */
+// TODO uncomment
+@Disabled
 public class TestCleaner extends HoodieClientTestBase {
 
   private static final int BIG_BATCH_INSERT_SIZE = 500;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestHoodieMergeOnReadTable.java
Patch:
@@ -403,7 +403,9 @@ public void testLogBlocksCountsAfterLogCompaction(boolean populateMetaFields) th
   @ValueSource(booleans = {true, false})
   public void testMetadataStatsOnCommit(Boolean rollbackUsingMarkers) throws Exception {
     HoodieWriteConfig cfg = getConfigBuilder(false, rollbackUsingMarkers, IndexType.INMEMORY)
-        .withAutoCommit(false).build();
+        .withAvroSchemaValidate(false)
+        .withAutoCommit(false)
+        .build();
 
     setUp(cfg.getProps());
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieCDCDataBlock.java
Patch:
@@ -42,7 +42,7 @@ public HoodieCDCDataBlock(
       Map<HeaderMetadataType, String> header,
       String keyField) {
     super(inputStream, content, readBlockLazily, logBlockContentLocation,
-        Option.of(readerSchema), header, new HashMap<>(), keyField, null);
+        Option.of(readerSchema), header, new HashMap<>(), keyField);
   }
 
   public HoodieCDCDataBlock(List<IndexedRecord> records,

File: hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java
Patch:
@@ -224,9 +224,9 @@ public Map<String, String> readFooter(Configuration configuration, boolean requi
   }
 
   @Override
-  public Schema readAvroSchema(Configuration configuration, Path parquetFilePath) {
-    MessageType parquetSchema = readSchema(configuration, parquetFilePath);
-    return new AvroSchemaConverter(configuration).convert(parquetSchema);
+  public Schema readAvroSchema(Configuration conf, Path parquetFilePath) {
+    MessageType parquetSchema = readSchema(conf, parquetFilePath);
+    return new AvroSchemaConverter(conf).convert(parquetSchema);
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/action/InternalSchemaMerger.java
Patch:
@@ -78,7 +78,7 @@ public InternalSchemaMerger(InternalSchema fileSchema, InternalSchema querySchem
    */
   public InternalSchema mergeSchema() {
     Types.RecordType record = (Types.RecordType) mergeType(querySchema.getRecord(), 0);
-    return new InternalSchema(record.fields());
+    return new InternalSchema(record);
   }
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/InternalSchemaUtils.java
Patch:
@@ -94,7 +94,7 @@ public static InternalSchema pruneInternalSchemaByID(InternalSchema schema, List
         }
       }
     }
-    return new InternalSchema(newFields.isEmpty() ? recordType.fields() : newFields);
+    return new InternalSchema(newFields.isEmpty() ? recordType : Types.RecordType.get(newFields));
   }
 
   /**

File: hudi-common/src/test/java/org/apache/hudi/internal/schema/io/TestFileBasedInternalSchemaStorageManager.java
Patch:
@@ -111,7 +111,7 @@ private InternalSchema getSimpleSchema() {
         Types.Field.get(0, "bool", Types.BooleanType.get()),
         Types.Field.get(1, "int", Types.IntType.get()),
     }));
-    return new InternalSchema(record.fields());
+    return new InternalSchema(record);
   }
 }
 

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -43,12 +43,12 @@
 import org.apache.hudi.exception.HoodieNotSupportedException;
 import org.apache.hudi.exception.TableNotFoundException;
 import org.apache.hudi.table.BulkInsertPartitioner;
-import org.apache.hudi.util.DataTypeUtils;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.Dataset;
+import org.apache.spark.sql.HoodieDataTypeUtils;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.types.StructType;
 
@@ -281,7 +281,7 @@ public static JavaRDD<HoodieRecord> dropDuplicates(JavaSparkContext jssc, JavaRD
    * @param schema schema of the dataset being written
    */
   public static void tryOverrideParquetWriteLegacyFormatProperty(Map<String, String> properties, StructType schema) {
-    if (DataTypeUtils.hasSmallPrecisionDecimalType(schema)
+    if (HoodieDataTypeUtils.hasSmallPrecisionDecimalType(schema)
         && properties.get(HoodieStorageConfig.PARQUET_WRITE_LEGACY_FORMAT_ENABLED.key()) == null) {
       // ParquetWriteSupport writes DecimalType to parquet as INT32/INT64 when the scale of decimalType
       // is less than {@code Decimal.MAX_LONG_DIGITS}, but {@code AvroParquetReader} which is used by

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestBootstrap.java
Patch:
@@ -53,6 +53,7 @@
 import org.apache.hudi.hadoop.HoodieParquetInputFormat;
 import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;
 import org.apache.hudi.index.HoodieIndex.IndexType;
+import org.apache.hudi.io.storage.HoodieParquetReader;
 import org.apache.hudi.keygen.NonpartitionedKeyGenerator;
 import org.apache.hudi.keygen.SimpleKeyGenerator;
 import org.apache.hudi.table.action.bootstrap.BootstrapUtils;
@@ -172,9 +173,8 @@ public Schema generateNewDataSetAndReturnSchema(long timestamp, int numRecords,
     String filePath = FileStatusUtils.toPath(BootstrapUtils.getAllLeafFoldersWithFiles(metaClient, metaClient.getFs(),
             srcPath, context).stream().findAny().map(p -> p.getValue().stream().findAny())
             .orElse(null).get().getPath()).toString();
-    ParquetFileReader reader = ParquetFileReader.open(metaClient.getHadoopConf(), new Path(filePath));
-    MessageType schema = reader.getFooter().getFileMetaData().getSchema();
-    return new AvroSchemaConverter().convert(schema);
+    HoodieParquetReader<?> parquetReader = new HoodieParquetReader<>(metaClient.getHadoopConf(), new Path(filePath));
+    return parquetReader.getSchema();
   }
 
   @Test

File: hudi-spark-datasource/hudi-spark3-common/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.internal.BaseDefaultSource;
 import org.apache.hudi.internal.DataSourceInternalWriterHelper;
-
+import org.apache.spark.sql.HoodieDataTypeUtils;
 import org.apache.spark.sql.connector.catalog.Table;
 import org.apache.spark.sql.connector.catalog.TableProvider;
 import org.apache.spark.sql.connector.expressions.Transform;
@@ -44,7 +44,8 @@ public class DefaultSource extends BaseDefaultSource implements TableProvider {
 
   @Override
   public StructType inferSchema(CaseInsensitiveStringMap options) {
-    return StructType.fromDDL(options.get(HoodieInternalConfig.BULKINSERT_INPUT_DATA_SCHEMA_DDL.key()));
+    String jsonSchema = options.get(HoodieInternalConfig.BULKINSERT_INPUT_DATA_SCHEMA_DDL.key());
+    return HoodieDataTypeUtils.parseStructTypeFromJson(jsonSchema);
   }
 
   @Override

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/HoodieDeltaStreamerTestBase.java
Patch:
@@ -125,6 +125,7 @@ protected static void prepareInitialConfigs(FileSystem dfs, String dfsBasePath,
         dfsBasePath + "/sql-transformer.properties");
     UtilitiesTestBase.Helpers.copyToDFS("delta-streamer-config/source.avsc", dfs, dfsBasePath + "/source.avsc");
     UtilitiesTestBase.Helpers.copyToDFS("delta-streamer-config/source_evolved.avsc", dfs, dfsBasePath + "/source_evolved.avsc");
+    UtilitiesTestBase.Helpers.copyToDFS("delta-streamer-config/source_evolved_post_processed.avsc", dfs, dfsBasePath + "/source_evolved_post_processed.avsc");
     UtilitiesTestBase.Helpers.copyToDFS("delta-streamer-config/source-flattened.avsc", dfs, dfsBasePath + "/source-flattened.avsc");
     UtilitiesTestBase.Helpers.copyToDFS("delta-streamer-config/target.avsc", dfs, dfsBasePath + "/target.avsc");
     UtilitiesTestBase.Helpers.copyToDFS("delta-streamer-config/target-flattened.avsc", dfs, dfsBasePath + "/target-flattened.avsc");

File: hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaUtils.java
Patch:
@@ -27,6 +27,9 @@
 
 import static org.apache.hudi.common.util.ValidationUtils.checkState;
 
+/**
+ * Utils for Avro Schema.
+ */
 public class AvroSchemaUtils {
 
   private AvroSchemaUtils() {}

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java
Patch:
@@ -53,7 +53,6 @@
 import org.apache.avro.io.JsonDecoder;
 import org.apache.avro.io.JsonEncoder;
 import org.apache.avro.specific.SpecificRecordBase;
-
 import org.apache.hadoop.util.VersionUtil;
 
 import java.io.ByteArrayInputStream;
@@ -71,12 +70,12 @@
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
+import java.util.Deque;
 import java.util.HashMap;
 import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
-import java.util.Deque;
-import java.util.LinkedList;
 import java.util.Set;
 import java.util.TimeZone;
 import java.util.stream.Collectors;

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroWriteSupport.java
Patch:
@@ -18,10 +18,11 @@
 
 package org.apache.hudi.avro;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.common.bloom.BloomFilter;
 import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.Option;
+
+import org.apache.avro.Schema;
 import org.apache.parquet.avro.AvroWriteSupport;
 import org.apache.parquet.hadoop.api.WriteSupport;
 import org.apache.parquet.schema.MessageType;

File: hudi-common/src/main/java/org/apache/hudi/common/config/ConfigGroups.java
Patch:
@@ -25,6 +25,9 @@
  * This class maintains the human readable name and description of each config group.
  */
 public class ConfigGroups {
+  /**
+   * Config group names.
+   */
   public enum Names {
     SPARK_DATASOURCE("Spark Datasource Configs"),
     FLINK_SQL("Flink Sql Configs"),

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -329,6 +329,9 @@ public boolean getUseLogRecordReaderScanV2() {
     return getBoolean(USE_LOG_RECORD_READER_SCAN_V2);
   }
 
+  /**
+   * Builder for {@link HoodieMetadataConfig}.
+   */
   public static class Builder {
 
     private EngineType engineType = EngineType.SPARK;

File: hudi-common/src/main/java/org/apache/hudi/common/engine/HoodieLocalEngineContext.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.hudi.common.engine;
 
-import org.apache.hadoop.conf.Configuration;
-
 import org.apache.hudi.common.config.SerializableConfiguration;
 import org.apache.hudi.common.data.HoodieAccumulator;
 import org.apache.hudi.common.data.HoodieAtomicLongAccumulator;
@@ -31,10 +29,11 @@
 import org.apache.hudi.common.function.SerializablePairFlatMapFunction;
 import org.apache.hudi.common.function.SerializablePairFunction;
 import org.apache.hudi.common.util.Option;
-
 import org.apache.hudi.common.util.collection.ImmutablePair;
 import org.apache.hudi.common.util.collection.Pair;
 
+import org.apache.hadoop.conf.Configuration;
+
 import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;

File: hudi-common/src/main/java/org/apache/hudi/common/engine/LocalTaskContextSupplier.java
Patch:
@@ -22,6 +22,9 @@
 
 import java.util.function.Supplier;
 
+/**
+ * Supplier of task context using local Java engine.
+ */
 public final class LocalTaskContextSupplier extends TaskContextSupplier {
   @Override
   public Supplier<Integer> getPartitionIdSupplier() {

File: hudi-common/src/main/java/org/apache/hudi/common/fs/BoundedFsDataInputStream.java
Patch:
@@ -23,6 +23,9 @@
 import java.io.IOException;
 import java.io.InputStream;
 
+/**
+ * Implementation of {@link FSDataInputStream} with bound check based on file size.
+ */
 public class BoundedFsDataInputStream extends FSDataInputStream {
   private FileSystem fs;
   private Path file;

File: hudi-common/src/main/java/org/apache/hudi/common/metrics/Registry.java
Patch:
@@ -18,14 +18,13 @@
 
 package org.apache.hudi.common.metrics;
 
+import org.apache.hudi.common.util.ReflectionUtils;
+
 import java.io.Serializable;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 
-import org.apache.hudi.common.util.ReflectionUtils;
-
-
 /**
  * Interface which defines a lightweight Metrics Registry to track Hudi events.
  */

File: hudi-common/src/main/java/org/apache/hudi/common/model/AWSDmsAvroPayload.java
Patch:
@@ -18,10 +18,11 @@
 
 package org.apache.hudi.common.model;
 
+import org.apache.hudi.common.util.Option;
+
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
-import org.apache.hudi.common.util.Option;
 
 import java.io.IOException;
 import java.util.Properties;

File: hudi-common/src/main/java/org/apache/hudi/common/model/BaseFile.java
Patch:
@@ -18,9 +18,10 @@
 
 package org.apache.hudi.common.model;
 
+import org.apache.hudi.hadoop.CachingPath;
+
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
-import org.apache.hudi.hadoop.CachingPath;
 
 import java.io.Serializable;
 import java.util.Objects;

File: hudi-common/src/main/java/org/apache/hudi/common/model/BootstrapBaseFileMapping.java
Patch:
@@ -18,10 +18,11 @@
 
 package org.apache.hudi.common.model;
 
-import java.io.Serializable;
 import org.apache.hudi.avro.model.HoodieFileStatus;
 import org.apache.hudi.common.bootstrap.FileStatusUtils;
 
+import java.io.Serializable;
+
 /**
  * POJO storing (partitionPath, hoodieFileId) -> external base file path.
  */

File: hudi-common/src/main/java/org/apache/hudi/common/model/BootstrapFileMapping.java
Patch:
@@ -18,9 +18,10 @@
 
 package org.apache.hudi.common.model;
 
+import org.apache.hudi.avro.model.HoodieFileStatus;
+
 import java.io.Serializable;
 import java.util.Objects;
-import org.apache.hudi.avro.model.HoodieFileStatus;
 
 /**
  * Value stored in the bootstrap index.

File: hudi-common/src/main/java/org/apache/hudi/common/model/CleanFileInfo.java
Patch:
@@ -22,6 +22,9 @@
 
 import java.io.Serializable;
 
+/**
+ * File info for clean action.
+ */
 public class CleanFileInfo implements Serializable {
 
   private final String filePath;

File: hudi-common/src/main/java/org/apache/hudi/common/model/ConsistentHashingNode.java
Patch:
@@ -86,6 +86,9 @@ public String toString() {
     return sb.toString();
   }
 
+  /**
+   * Node tag.
+   */
   public enum NodeTag {
     /**
      * Standard node.

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieColumnRangeMetadata.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.common.model;
 
 import javax.annotation.Nullable;
+
 import java.io.Serializable;
 import java.util.Objects;
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java
Patch:
@@ -22,15 +22,14 @@
 import org.apache.hudi.common.util.JsonUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
+import org.apache.hudi.exception.HoodieException;
 
 import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.node.ArrayNode;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
-import org.apache.hudi.exception.HoodieException;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieDeltaWriteStat.java
Patch:
@@ -18,9 +18,10 @@
 
 package org.apache.hudi.common.model;
 
-import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
 import org.apache.hudi.common.util.Option;
 
+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
+
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieLogFile.java
Patch:
@@ -18,10 +18,11 @@
 
 package org.apache.hudi.common.model;
 
+import org.apache.hudi.common.fs.FSUtils;
+
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hudi.common.fs.FSUtils;
 
 import java.io.IOException;
 import java.io.Serializable;

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieMetadataWrapper.java
Patch:
@@ -20,6 +20,9 @@
 
 import org.apache.hudi.avro.model.HoodieArchivedMetaEntry;
 
+/**
+ * Wrapper for Hudi metadata.
+ */
 public class HoodieMetadataWrapper {
 
   private HoodieArchivedMetaEntry avroMetadataFromTimeline;

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java
Patch:
@@ -18,9 +18,10 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.hadoop.fs.Path;
 import org.apache.hudi.common.util.JsonUtils;
 
+import org.apache.hadoop.fs.Path;
+
 import javax.annotation.Nullable;
 
 import java.io.Serializable;

File: hudi-common/src/main/java/org/apache/hudi/common/model/OverwriteNonDefaultsWithLatestAvroPayload.java
Patch:
@@ -18,13 +18,13 @@
 
 package org.apache.hudi.common.model;
 
+import org.apache.hudi.common.util.Option;
+
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.GenericRecordBuilder;
 import org.apache.avro.generic.IndexedRecord;
 
-import org.apache.hudi.common.util.Option;
-
 import java.io.IOException;
 import java.util.List;
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/OverwriteWithLatestAvroPayload.java
Patch:
@@ -18,10 +18,10 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.avro.JsonProperties;
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.Option;
 
+import org.apache.avro.JsonProperties;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;

File: hudi-common/src/main/java/org/apache/hudi/common/model/RewriteAvroPayload.java
Patch:
@@ -18,10 +18,11 @@
 
 package org.apache.hudi.common.model;
 
+import org.apache.hudi.common.util.Option;
+
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
-import org.apache.hudi.common.util.Option;
 
 import java.io.IOException;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -54,11 +54,11 @@
 import java.io.IOException;
 import java.time.Instant;
 import java.util.Arrays;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
 import java.util.Set;
-import java.util.HashSet;
 import java.util.function.BiConsumer;
 import java.util.stream.Collectors;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieCDCLogRecordIterator.java
Patch:
@@ -25,13 +25,15 @@
 
 import org.apache.avro.Schema;
 import org.apache.avro.generic.IndexedRecord;
-
 import org.apache.hadoop.fs.FileSystem;
 
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.Iterator;
 
+/**
+ * Record iterator for Hudi logs in CDC format.
+ */
 public class HoodieCDCLogRecordIterator implements ClosableIterator<IndexedRecord> {
 
   private final FileSystem fs;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatReader.java
Patch:
@@ -21,10 +21,10 @@
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
 import org.apache.hudi.exception.HoodieIOException;
+import org.apache.hudi.internal.schema.InternalSchema;
 
 import org.apache.avro.Schema;
 import org.apache.hadoop.fs.FileSystem;
-import org.apache.hudi.internal.schema.InternalSchema;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieUnMergedLogRecordScanner.java
Patch:
@@ -23,10 +23,10 @@
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.cdc.HoodieCDCUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.internal.schema.InternalSchema;
 
 import org.apache.avro.Schema;
 import org.apache.hadoop.fs.FileSystem;
-import org.apache.hudi.internal.schema.InternalSchema;
 
 import java.util.List;
 import java.util.stream.Collectors;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieCDCDataBlock.java
Patch:
@@ -18,13 +18,12 @@
 
 package org.apache.hudi.common.table.log.block;
 
+import org.apache.hudi.common.util.Option;
+
 import org.apache.avro.Schema;
 import org.apache.avro.generic.IndexedRecord;
-
 import org.apache.hadoop.fs.FSDataInputStream;
 
-import org.apache.hudi.common.util.Option;
-
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDataBlock.java
Patch:
@@ -21,11 +21,11 @@
 import org.apache.hudi.common.util.ClosableIterator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieIOException;
+import org.apache.hudi.internal.schema.InternalSchema;
 
 import org.apache.avro.Schema;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hudi.internal.schema.InternalSchema;
 
 import java.io.IOException;
 import java.util.HashSet;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieLogBlock.java
Patch:
@@ -18,14 +18,14 @@
 
 package org.apache.hudi.common.table.log.block;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.TypeUtils;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
 
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataInputStream;
 
 import javax.annotation.Nonnull;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstantTimeGenerator.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.common.table.timeline;
 
 import org.apache.hudi.common.model.HoodieTimelineTimeZone;
+
 import java.text.ParseException;
 import java.time.LocalDateTime;
 import java.time.ZoneId;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/ClusteringOpDTO.java
Patch:
@@ -18,12 +18,13 @@
 
 package org.apache.hudi.common.table.timeline.dto;
 
-import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
-import com.fasterxml.jackson.annotation.JsonProperty;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.collection.Pair;
 
+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
+import com.fasterxml.jackson.annotation.JsonProperty;
+
 /**
  * The data transfer object of clustering.
  */

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanMetadataMigrator.java
Patch:
@@ -24,6 +24,9 @@
 
 import java.util.Arrays;
 
+/**
+ * Migrator for clean metadata.
+ */
 public class CleanMetadataMigrator extends MetadataMigrator<HoodieCleanMetadata> {
 
   public CleanMetadataMigrator(HoodieTableMetaClient metaClient) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanMetadataV1MigrationHandler.java
Patch:
@@ -31,6 +31,9 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
+/**
+ * Migration handler for clean metadata in version 1.
+ */
 public class CleanMetadataV1MigrationHandler extends AbstractMigratorBase<HoodieCleanMetadata> {
 
   public static final Integer VERSION = 1;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanMetadataV2MigrationHandler.java
Patch:
@@ -31,6 +31,9 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
+/**
+ * Migration handler for clean metadata in version 2.
+ */
 public class CleanMetadataV2MigrationHandler extends AbstractMigratorBase<HoodieCleanMetadata> {
 
   public static final Integer VERSION = 2;

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTablePreCommitFileSystemView.java
Patch:
@@ -18,11 +18,12 @@
 
 package org.apache.hudi.common.table.view;
 
-import org.apache.hadoop.fs.Path;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 
+import org.apache.hadoop.fs.Path;
+
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.hudi.common.table.view;
 
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.Path;
 import org.apache.hudi.avro.model.HoodieCleanMetadata;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.avro.model.HoodieRestoreMetadata;
@@ -44,6 +42,9 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
+
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.hudi.common.table.view;
 
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.Path;
 import org.apache.hudi.common.model.BootstrapBaseFileMapping;
 import org.apache.hudi.common.model.CompactionOperation;
 import org.apache.hudi.common.model.FileSlice;
@@ -35,6 +33,9 @@
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.common.util.collection.RocksDBDAO;
+
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.hudi.common.table.view;
 
-import org.apache.hadoop.fs.FileStatus;
-
 import org.apache.hudi.common.config.HoodieCommonConfig;
 import org.apache.hudi.common.model.BootstrapBaseFileMapping;
 import org.apache.hudi.common.model.CompactionOperation;
@@ -32,6 +30,8 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.ExternalSpillableMap;
 import org.apache.hudi.common.util.collection.Pair;
+
+import org.apache.hadoop.fs.FileStatus;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/Base64CodecUtil.java
Patch:
@@ -21,6 +21,9 @@
 import java.nio.charset.StandardCharsets;
 import java.util.Base64;
 
+/**
+ * Utils for Base64 encoding and decoding.
+ */
 public final class Base64CodecUtil {
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/util/BaseFileUtils.java
Patch:
@@ -39,6 +39,9 @@
 import java.util.Map;
 import java.util.Set;
 
+/**
+ * Utils for Hudi base file.
+ */
 public abstract class BaseFileUtils {
 
   public static BaseFileUtils getInstance(String path) {

File: hudi-common/src/main/java/org/apache/hudi/common/util/BinaryUtil.java
Patch:
@@ -22,6 +22,9 @@
 import java.nio.charset.Charset;
 import java.util.zip.CRC32;
 
+/**
+ * Utils for Java byte array.
+ */
 public class BinaryUtil {
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java
Patch:
@@ -36,6 +36,7 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
+
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/CollectionUtils.java
Patch:
@@ -42,6 +42,9 @@
 
 import static org.apache.hudi.common.util.ValidationUtils.checkArgument;
 
+/**
+ * Utils for Java Collection.
+ */
 public class CollectionUtils {
 
   private static final Properties EMPTY_PROPERTIES = new Properties();

File: hudi-common/src/main/java/org/apache/hudi/common/util/CustomizedThreadFactory.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.common.util;
 
 import javax.annotation.Nonnull;
+
 import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.atomic.AtomicLong;
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/DateTimeUtils.java
Patch:
@@ -35,6 +35,9 @@
 import java.util.Objects;
 import java.util.stream.Collectors;
 
+/**
+ * Utils for Hudi instant time.
+ */
 public class DateTimeUtils {
   private static final Map<String, ChronoUnit> LABEL_TO_UNIT_MAP =
       Collections.unmodifiableMap(initMap());

File: hudi-common/src/main/java/org/apache/hudi/common/util/FutureUtils.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.common.util;
 
 import javax.annotation.Nonnull;
+
 import java.util.List;
 import java.util.concurrent.CompletableFuture;
 import java.util.stream.Collectors;

File: hudi-common/src/main/java/org/apache/hudi/common/util/MapUtils.java
Patch:
@@ -22,6 +22,9 @@
 import java.util.Map;
 import java.util.Objects;
 
+/**
+ * Utils for Java Map.
+ */
 public class MapUtils {
 
   public static boolean isNullOrEmpty(Map<?, ?> m) {

File: hudi-common/src/main/java/org/apache/hudi/common/util/OrcReaderIterator.java
Patch:
@@ -18,17 +18,17 @@
 
 package org.apache.hudi.common.util;
 
-import java.util.List;
+import org.apache.hudi.exception.HoodieIOException;
+
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericData;
 import org.apache.avro.generic.GenericData.Record;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hudi.exception.HoodieIOException;
-
 import org.apache.orc.RecordReader;
 import org.apache.orc.TypeDescription;
 
 import java.io.IOException;
+import java.util.List;
 
 /**
  * This class wraps a ORC reader and provides an iterator based api to read from an ORC file.

File: hudi-common/src/main/java/org/apache/hudi/common/util/OrcUtils.java
Patch:
@@ -32,14 +32,14 @@
 import org.apache.avro.generic.GenericRecord;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.orc.OrcFile;
 import org.apache.orc.OrcProto.UserMetadataItem;
 import org.apache.orc.Reader;
 import org.apache.orc.Reader.Options;
 import org.apache.orc.RecordReader;
 import org.apache.orc.TypeDescription;
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 
 import java.io.IOException;
 import java.nio.ByteBuffer;

File: hudi-common/src/main/java/org/apache/hudi/common/util/ThreadUtils.java
Patch:
@@ -21,6 +21,9 @@
 import java.util.Arrays;
 import java.util.List;
 
+/**
+ * Utils for Java threading.
+ */
 public class ThreadUtils {
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/util/ValidationUtils.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.util;
 
-/*
+/**
  * Simple utility to test validation conditions (to replace Guava's PreConditions)
  */
 public class ValidationUtils {

File: hudi-common/src/main/java/org/apache/hudi/common/util/hash/HashID.java
Patch:
@@ -19,11 +19,12 @@
 
 package org.apache.hudi.common.util.hash;
 
+import org.apache.hudi.exception.HoodieIOException;
+
 import net.jpountz.xxhash.XXHash32;
 import net.jpountz.xxhash.XXHash64;
 import net.jpountz.xxhash.XXHashFactory;
 import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hudi.exception.HoodieIOException;
 
 import java.io.Serializable;
 import java.nio.charset.StandardCharsets;

File: hudi-common/src/main/java/org/apache/hudi/common/util/io/ByteBufferBackedInputStream.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.common.util.io;
 
 import javax.annotation.Nonnull;
+
 import java.io.InputStream;
 import java.nio.ByteBuffer;
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/BoundedInMemoryExecutor.java
Patch:
@@ -23,8 +23,8 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.SizeEstimator;
 import org.apache.hudi.exception.HoodieException;
-
 import org.apache.hudi.exception.HoodieIOException;
+
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/DisruptorExecutor.java
Patch:
@@ -20,8 +20,8 @@
 
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieException;
-
 import org.apache.hudi.exception.HoodieIOException;
+
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/DisruptorWaitStrategyType.java
Patch:
@@ -24,6 +24,9 @@
 import java.util.Arrays;
 import java.util.List;
 
+/**
+ * Enum for the type of waiting strategy in Disruptor Queue.
+ */
 public enum DisruptorWaitStrategyType {
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/HoodieExecutorBase.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.exception.HoodieException;
+
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/HoodieMessageQueue.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.common.util.queue;
 
 import org.apache.hudi.common.util.Option;
+
 import java.io.Closeable;
 
 /**

File: hudi-common/src/main/java/org/apache/hudi/exception/HoodieCompactException.java
Patch:
@@ -18,6 +18,9 @@
 
 package org.apache.hudi.exception;
 
+/**
+ * Exception for Hudi compaction.
+ */
 public class HoodieCompactException extends HoodieException {
 
   public HoodieCompactException(String msg) {

File: hudi-common/src/main/java/org/apache/hudi/exception/HoodieDebeziumAvroPayloadException.java
Patch:
@@ -21,6 +21,9 @@
 
 import java.io.IOException;
 
+/**
+ * Exception for HoodieDebeziumAvroPayload.
+ */
 public class HoodieDebeziumAvroPayloadException extends IOException {
 
   public HoodieDebeziumAvroPayloadException(String msg) {

File: hudi-common/src/main/java/org/apache/hudi/exception/HoodieSecondaryIndexException.java
Patch:
@@ -19,6 +19,9 @@
 
 package org.apache.hudi.exception;
 
+/**
+ * Exception for Hudi secondary index.
+ */
 public class HoodieSecondaryIndexException extends HoodieException {
   public HoodieSecondaryIndexException(String message) {
     super(message);

File: hudi-common/src/main/java/org/apache/hudi/hadoop/CachingPath.java
Patch:
@@ -18,10 +18,12 @@
 
 package org.apache.hudi.hadoop;
 
-import org.apache.hadoop.fs.Path;
 import org.apache.hudi.exception.HoodieException;
 
+import org.apache.hadoop.fs.Path;
+
 import javax.annotation.concurrent.ThreadSafe;
+
 import java.net.URI;
 import java.net.URISyntaxException;
 

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChanges.java
Patch:
@@ -35,6 +35,9 @@
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 
+/**
+ * Operations of schema changes supported in schema evolution
+ */
 public class TableChanges {
 
   /** Deal with update columns changes for table. */

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/AvroSchemaEvolutionUtils.java
Patch:
@@ -18,12 +18,13 @@
 
 package org.apache.hudi.internal.schema.utils;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.internal.schema.InternalSchema;
 import org.apache.hudi.internal.schema.Types;
 import org.apache.hudi.internal.schema.action.TableChanges;
 import org.apache.hudi.internal.schema.convert.AvroInternalSchemaConverter;
 
+import org.apache.avro.Schema;
+
 import java.util.ArrayList;
 import java.util.List;
 import java.util.TreeMap;

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/InternalSchemaUtils.java
Patch:
@@ -30,9 +30,9 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.TreeMap;
-import java.util.SortedMap;
 import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
 import java.util.stream.Collectors;
 
 /**

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/visitor/NameToIDVisitor.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.hudi.internal.schema.visitor;
 
-import static org.apache.hudi.internal.schema.utils.InternalSchemaUtils.createFullName;
-
 import org.apache.hudi.internal.schema.InternalSchema;
 import org.apache.hudi.internal.schema.Type;
 import org.apache.hudi.internal.schema.Types;
@@ -30,6 +28,8 @@
 import java.util.List;
 import java.util.Map;
 
+import static org.apache.hudi.internal.schema.utils.InternalSchemaUtils.createFullName;
+
 /**
  * Schema visitor to produce name -> id map for internalSchema.
  */

File: hudi-common/src/main/java/org/apache/hudi/keygen/KeyGeneratorInterface.java
Patch:
@@ -18,9 +18,10 @@
 
 package org.apache.hudi.keygen;
 
-import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.common.model.HoodieKey;
 
+import org.apache.avro.generic.GenericRecord;
+
 import java.io.Serializable;
 import java.util.List;
 

File: hudi-common/src/main/java/org/apache/hudi/keygen/constant/KeyGeneratorOptions.java
Patch:
@@ -23,6 +23,9 @@
 import org.apache.hudi.common.config.ConfigProperty;
 import org.apache.hudi.common.config.HoodieConfig;
 
+/**
+ * Key generator configs.
+ */
 @ConfigClassProperty(name = "Key Generator Options",
     groupName = ConfigGroups.Names.WRITE_CLIENT,
     description = "Hudi maintains keys (record key + partition path) "

File: hudi-common/src/main/java/org/apache/hudi/metadata/BaseTableMetadata.java
Patch:
@@ -66,6 +66,9 @@
 import java.util.function.Function;
 import java.util.stream.Collectors;
 
+/**
+ * Abstract class for implementing common table metadata operations.
+ */
 public abstract class BaseTableMetadata implements HoodieTableMetadata {
 
   private static final Logger LOG = LogManager.getLogger(BaseTableMetadata.class);

File: hudi-common/src/main/java/org/apache/hudi/metadata/FileSystemBackedTableMetadata.java
Patch:
@@ -47,6 +47,9 @@
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.stream.Collectors;
 
+/**
+ * Implementation of {@link HoodieTableMetadata} based file-system-backed table metadata.
+ */
 public class FileSystemBackedTableMetadata implements HoodieTableMetadata {
 
   private static final int DEFAULT_LISTING_PARALLELISM = 1500;

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMetrics.java
Patch:
@@ -36,6 +36,9 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
+/**
+ * Metrics for metadata.
+ */
 public class HoodieMetadataMetrics implements Serializable {
 
   // Metric names

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadata.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.hudi.metadata;
 
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.Path;
 import org.apache.hudi.avro.model.HoodieMetadataColumnStats;
 import org.apache.hudi.common.bloom.BloomFilter;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
@@ -32,6 +30,9 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieMetadataException;
 
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+
 import java.io.IOException;
 import java.io.Serializable;
 import java.util.Collection;

File: hudi-common/src/main/java/org/apache/hudi/metadata/MetadataPartitionType.java
Patch:
@@ -21,6 +21,9 @@
 import java.util.Arrays;
 import java.util.List;
 
+/**
+ * Partition types for metadata table.
+ */
 public enum MetadataPartitionType {
   FILES(HoodieTableMetadataUtil.PARTITION_NAME_FILES, "files-"),
   COLUMN_STATS(HoodieTableMetadataUtil.PARTITION_NAME_COLUMN_STATS, "col-stats-"),

File: hudi-common/src/main/java/org/apache/hudi/parquet/io/ByteBufferBackedInputFile.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.parquet.io;
 
 import org.apache.hudi.common.util.io.ByteBufferBackedInputStream;
+
 import org.apache.parquet.io.DelegatingSeekableInputStream;
 import org.apache.parquet.io.InputFile;
 import org.apache.parquet.io.SeekableInputStream;

File: hudi-common/src/main/java/org/apache/hudi/parquet/io/OutputStreamBackedOutputFile.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.parquet.io.PositionOutputStream;
 
 import javax.annotation.Nonnull;
+
 import java.io.IOException;
 
 /**

File: hudi-common/src/main/java/org/apache/hudi/secondary/index/SecondaryIndexManager.java
Patch:
@@ -41,6 +41,9 @@
 
 import static org.apache.hudi.secondary.index.SecondaryIndexUtils.getSecondaryIndexes;
 
+/**
+ * Manages secondary index.
+ */
 public class SecondaryIndexManager {
   private static final Logger LOG = LoggerFactory.getLogger(SecondaryIndexManager.class);
 

File: hudi-common/src/main/java/org/apache/hudi/secondary/index/SecondaryIndexType.java
Patch:
@@ -23,6 +23,9 @@
 
 import java.util.Arrays;
 
+/**
+ * Type of secondary index.
+ */
 public enum SecondaryIndexType {
   LUCENE((byte) 1);
 

File: hudi-common/src/main/java/org/apache/hudi/secondary/index/SecondaryIndexUtils.java
Patch:
@@ -33,6 +33,9 @@
 
 import java.util.List;
 
+/**
+ * Utils for secondary index.
+ */
 public class SecondaryIndexUtils {
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/util/Lazy.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.util;
 
 import javax.annotation.concurrent.ThreadSafe;
+
 import java.util.function.Supplier;
 
 /**

File: hudi-common/src/test/java/org/apache/hudi/avro/TestHoodieAvroUtils.java
Patch:
@@ -28,9 +28,8 @@
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericData;
 import org.apache.avro.generic.GenericRecord;
-
-import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.Assertions;
+import org.junit.jupiter.api.Test;
 
 import java.io.IOException;
 import java.math.BigDecimal;

File: hudi-common/src/test/java/org/apache/hudi/common/fs/TestFSUtilsWithRetryWrapperEnable.java
Patch:
@@ -28,7 +28,6 @@
 import org.apache.hadoop.fs.RemoteIterator;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.util.Progressable;
-
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
 

File: hudi-common/src/test/java/org/apache/hudi/common/fs/inline/InLineFSUtilsTest.java
Patch:
@@ -18,8 +18,9 @@
 
 package org.apache.hudi.common.fs.inline;
 
-import org.apache.hadoop.fs.Path;
 import org.apache.hudi.common.testutils.FileSystemTestUtils;
+
+import org.apache.hadoop.fs.Path;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.Arguments;
 import org.junit.jupiter.params.provider.MethodSource;

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodieConsistentHashingMetadata.java
Patch:
@@ -21,6 +21,9 @@
 import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.Test;
 
+/**
+ * Tests {@link HoodieConsistentHashingMetadata}.
+ */
 public class TestHoodieConsistentHashingMetadata {
 
   @Test

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodieFileGroup.java
Patch:
@@ -32,6 +32,9 @@
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
+/**
+ * Tests {@link HoodieFileGroup}.
+ */
 public class TestHoodieFileGroup {
 
   @Test

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodiePartitionMetadata.java
Patch:
@@ -39,6 +39,9 @@
 import static org.junit.jupiter.api.Assertions.assertThrows;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
+/**
+ * Tests {@link HoodiePartitionMetadata}.
+ */
 public class TestHoodiePartitionMetadata extends HoodieCommonTestHarness {
 
   FileSystem fs;

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodieReplaceCommitMetadata.java
Patch:
@@ -29,6 +29,9 @@
 
 import static org.apache.hudi.common.model.TestHoodieCommitMetadata.verifyMetadataFieldNames;
 
+/**
+ * Tests {@link HoodieReplaceCommitMetadata}.
+ */
 public class TestHoodieReplaceCommitMetadata {
 
   private static final List<String> EXPECTED_FIELD_NAMES = Arrays.asList(

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodieWriteStat.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
+
 import org.apache.hadoop.fs.Path;
 import org.junit.jupiter.api.Test;
 

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestOverwriteNonDefaultsWithLatestAvroPayload.java
Patch:
@@ -18,12 +18,13 @@
 
 package org.apache.hudi.common.model;
 
+import org.apache.hudi.avro.HoodieAvroUtils;
+
 import org.apache.avro.JsonProperties;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericData;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
-import org.apache.hudi.avro.HoodieAvroUtils;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
 

File: hudi-common/src/test/java/org/apache/hudi/common/model/debezium/TestMySqlDebeziumAvroPayload.java
Patch:
@@ -38,6 +38,9 @@
 import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertThrows;
 
+/**
+ * Tests {@link MySqlDebeziumAvroPayload}.
+ */
 public class TestMySqlDebeziumAvroPayload {
 
   private static final String KEY_FIELD_NAME = "Key";

File: hudi-common/src/test/java/org/apache/hudi/common/model/debezium/TestPostgresDebeziumAvroPayload.java
Patch:
@@ -43,6 +43,9 @@
 import static org.junit.jupiter.api.Assertions.assertNull;
 import static org.junit.jupiter.api.Assertions.assertThrows;
 
+/**
+ * Tests {@link PostgresDebeziumAvroPayload}.
+ */
 public class TestPostgresDebeziumAvroPayload {
 
   private static final String KEY_FIELD_NAME = "Key";

File: hudi-common/src/test/java/org/apache/hudi/common/properties/TestOrderedProperties.java
Patch:
@@ -27,6 +27,9 @@
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
 
+/**
+ * Tests {@link OrderedProperties}.
+ */
 public class TestOrderedProperties {
 
   @Test

File: hudi-common/src/test/java/org/apache/hudi/common/properties/TestTypedProperties.java
Patch:
@@ -28,6 +28,9 @@
 import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
+/**
+ * Tests {@link TypedProperties}.
+ */
 public class TestTypedProperties {
   @Test
   public void testGetString() {

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestHoodieTableConfig.java
Patch:
@@ -42,6 +42,9 @@
 import static org.junit.jupiter.api.Assertions.assertThrows;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
+/**
+ * Tests {@link HoodieTableConfig}.
+ */
 public class TestHoodieTableConfig extends HoodieCommonTestHarness {
 
   private FileSystem fs;

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestTableSchemaResolver.java
Patch:
@@ -30,6 +30,9 @@
 import static org.junit.jupiter.api.Assertions.assertNotEquals;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
+/**
+ * Tests {@link TableSchemaResolver}.
+ */
 public class TestTableSchemaResolver {
 
   @Test

File: hudi-common/src/test/java/org/apache/hudi/common/table/timeline/TestTimelineLayout.java
Patch:
@@ -30,6 +30,9 @@
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
+/**
+ * Tests {@link TimelineLayout}.
+ */
 public class TestTimelineLayout  {
 
   @Test

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestHoodieTableFSViewWithClustering.java
Patch:
@@ -51,6 +51,9 @@
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;
 
+/**
+ * Tests {@link TestHoodieTableFSViewWithClustering}.
+ */
 public class TestHoodieTableFSViewWithClustering extends HoodieCommonTestHarness {
 
   private static final String TEST_WRITE_TOKEN = "1-0-1";

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java
Patch:
@@ -50,9 +50,9 @@
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
+import org.apache.hudi.exception.HoodieIOException;
 
 import org.apache.hadoop.fs.Path;
-import org.apache.hudi.exception.HoodieIOException;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 import org.junit.jupiter.api.AfterEach;

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/FileCreateUtils.java
Patch:
@@ -68,6 +68,9 @@
 import static org.apache.hudi.common.table.timeline.TimelineMetadataUtils.serializeRollbackMetadata;
 import static org.apache.hudi.common.table.timeline.TimelineMetadataUtils.serializeRollbackPlan;
 
+/**
+ * Utils for creating dummy Hudi files in testing.
+ */
 public class FileCreateUtils {
 
   private static final Logger LOG = LogManager.getLogger(FileCreateUtils.class);

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java
Patch:
@@ -882,6 +882,9 @@ public int getNumExistingKeys(String schemaStr) {
     return numKeysBySchema.getOrDefault(schemaStr, 0);
   }
 
+  /**
+   * Object containing the key and partition path for testing.
+   */
   public static class KeyPartition implements Serializable {
 
     public HoodieKey key;

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/NetworkTestUtils.java
Patch:
@@ -23,6 +23,9 @@
 import java.io.IOException;
 import java.net.ServerSocket;
 
+/**
+ * Utils for networking setup in testing.
+ */
 public class NetworkTestUtils {
 
   public static int nextFreePort() {

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestBase64CodecUtil.java
Patch:
@@ -25,6 +25,9 @@
 
 import static org.junit.jupiter.api.Assertions.assertArrayEquals;
 
+/**
+ * Tests {@link Base64CodecUtil}.
+ */
 public class TestBase64CodecUtil {
 
   @Test

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestClusteringUtils.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
 import org.apache.hudi.common.testutils.HoodieCommonTestHarness;
+
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
 

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestDFSPropertiesConfiguration.java
Patch:
@@ -18,16 +18,16 @@
 
 package org.apache.hudi.common.util;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hudi.common.config.DFSPropertiesConfiguration;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.testutils.minicluster.HdfsTestService;
+import org.apache.hudi.exception.HoodieIOException;
 
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hudi.exception.HoodieIOException;
 import org.junit.Rule;
 import org.junit.contrib.java.lang.system.EnvironmentVariables;
 import org.junit.jupiter.api.AfterAll;

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestDateTimeUtils.java
Patch:
@@ -28,6 +28,9 @@
 import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;
 import static org.junit.jupiter.api.Assertions.assertThrows;
 
+/**
+ * Tests {@link DateTimeUtils}.
+ */
 public class TestDateTimeUtils {
 
   @ParameterizedTest

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestParquetReaderIterator.java
Patch:
@@ -34,6 +34,9 @@
 import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.when;
 
+/**
+ * Tests {@link ParquetReaderIterator}.
+ */
 public class TestParquetReaderIterator {
 
   @Test

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestSerializableSchema.java
Patch:
@@ -18,10 +18,11 @@
 
 package org.apache.hudi.common.util;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.config.SerializableSchema;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
+
+import org.apache.avro.Schema;
 import org.junit.jupiter.api.Test;
 
 import java.io.ByteArrayInputStream;

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestStringUtils.java
Patch:
@@ -30,6 +30,9 @@
 import static org.junit.jupiter.api.Assertions.assertNull;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
+/**
+ * Tests {@link StringUtils}.
+ */
 public class TestStringUtils {
 
   private static final String[] STRINGS = {"This", "is", "a", "test"};

File: hudi-common/src/test/java/org/apache/hudi/common/util/collection/TestRocksDBDAO.java
Patch:
@@ -238,6 +238,9 @@ public void testWithSerializableKey() {
     assertFalse(new File(rocksDBBasePath).exists());
   }
 
+  /**
+   * Payload key object.
+   */
   public static class PayloadKey implements Serializable {
     private String key;
 

File: hudi-common/src/test/java/org/apache/hudi/common/util/io/TestByteBufferBackedInputStream.java
Patch:
@@ -26,6 +26,9 @@
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertThrows;
 
+/**
+ * Tests {@link ByteBufferBackedInputStream}.
+ */
 public class TestByteBufferBackedInputStream {
 
   @Test

File: hudi-common/src/test/java/org/apache/hudi/internal/schema/io/TestFileBasedInternalSchemaStorageManager.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.internal.schema.InternalSchema;
 import org.apache.hudi.internal.schema.Types;
 import org.apache.hudi.internal.schema.utils.SerDeHelper;
+
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -353,8 +353,8 @@ private Map<String, HoodieRecord<HoodieMetadataPayload>> fetchBaseFileRecordsByK
 
     return toStream(records)
         .map(record -> Pair.of(
-            (String) record.get(HoodieMetadataPayload.KEY_FIELD_NAME),
-            composeRecord(record, partitionName)))
+          (String) record.get(HoodieMetadataPayload.KEY_FIELD_NAME),
+          composeRecord(record, partitionName)))
         .collect(Collectors.toMap(Pair::getKey, Pair::getValue));
   }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/ComplexKeyGenerator.java
Patch:
@@ -41,11 +41,11 @@ public class ComplexKeyGenerator extends BuiltinKeyGenerator {
 
   public ComplexKeyGenerator(TypedProperties props) {
     super(props);
-    this.recordKeyFields = Arrays.stream(props.getString(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key()).split(","))
+    this.recordKeyFields = Arrays.stream(props.getString(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key()).split(FIELDS_SEP))
         .map(String::trim)
         .filter(s -> !s.isEmpty())
         .collect(Collectors.toList());
-    this.partitionPathFields = Arrays.stream(props.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME.key()).split(","))
+    this.partitionPathFields = Arrays.stream(props.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME.key()).split(FIELDS_SEP))
         .map(String::trim)
         .filter(s -> !s.isEmpty())
         .collect(Collectors.toList());

File: hudi-common/src/main/java/org/apache/hudi/common/util/CollectionUtils.java
Patch:
@@ -155,7 +155,7 @@ public static <K, V> HashMap<K, V> combine(Map<K, V> one, Map<K, V> another, BiF
   /**
    * Returns difference b/w {@code one} {@link Set} of elements and {@code another}
    */
-  public static <E> Set<E> diff(Set<E> one, Set<E> another) {
+  public static <E> Set<E> diff(Collection<E> one, Collection<E> another) {
     Set<E> diff = new HashSet<>(one);
     diff.removeAll(another);
     return diff;
@@ -164,7 +164,7 @@ public static <E> Set<E> diff(Set<E> one, Set<E> another) {
   /**
    * Returns difference b/w {@code one} {@link List} of elements and {@code another}
    *
-   * NOTE: This is less optimal counterpart to {@link #diff(Set, Set)}, accepting {@link List}
+   * NOTE: This is less optimal counterpart to {@link #diff(Collection, Collection)}, accepting {@link List}
    *       as a holding collection to support duplicate elements use-cases
    */
   public static <E> List<E> diff(List<E> one, List<E> another) {

File: hudi-common/src/main/java/org/apache/hudi/metadata/BaseTableMetadata.java
Patch:
@@ -55,6 +55,7 @@
 import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
@@ -149,7 +150,7 @@ public FileStatus[] getAllFilesInPartition(Path partitionPath)
   }
 
   @Override
-  public Map<String, FileStatus[]> getAllFilesInPartitions(List<String> partitions)
+  public Map<String, FileStatus[]> getAllFilesInPartitions(Collection<String> partitions)
       throws IOException {
     if (partitions.isEmpty()) {
       return Collections.emptyMap();

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java
Patch:
@@ -81,7 +81,7 @@
 import static org.apache.hudi.common.util.TypeUtils.unsafeCast;
 import static org.apache.hudi.common.util.ValidationUtils.checkArgument;
 import static org.apache.hudi.common.util.ValidationUtils.checkState;
-import static org.apache.hudi.hadoop.CachingPath.createPathUnsafe;
+import static org.apache.hudi.hadoop.CachingPath.createRelativePathUnsafe;
 import static org.apache.hudi.metadata.HoodieTableMetadata.RECORDKEY_PARTITION_LIST;
 import static org.apache.hudi.metadata.HoodieTableMetadataUtil.getPartitionIdentifier;
 import static org.apache.hudi.metadata.HoodieTableMetadataUtil.tryUpcastDecimal;
@@ -487,7 +487,7 @@ public FileStatus[] getFileStatuses(FileSystem fs, Path partitionPath) {
         .map(e -> {
           // NOTE: Since we know that the Metadata Table's Payload is simply a file-name we're
           //       creating Hadoop's Path using more performant unsafe variant
-          CachingPath filePath = new CachingPath(partitionPath, createPathUnsafe(e.getKey()));
+          CachingPath filePath = new CachingPath(partitionPath, createRelativePathUnsafe(e.getKey()));
           return new FileStatus(e.getValue().getSize(), false, 0, blockSize, 0, 0,
               null, null, null, filePath);
         })

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieMergeOnReadTableInputFormat.java
Patch:
@@ -92,7 +92,7 @@ protected FileStatus createFileStatusUnchecked(FileSlice fileSlice, HiveHoodieTa
     Stream<HoodieLogFile> logFiles = fileSlice.getLogFiles();
 
     Option<HoodieInstant> latestCompletedInstantOpt = fileIndex.getLatestCompletedInstant();
-    String tableBasePath = fileIndex.getBasePath();
+    String tableBasePath = fileIndex.getBasePath().toString();
 
     // Check if we're reading a MOR table
     if (baseFileOpt.isPresent()) {

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestSimpleKeyGenerator.java
Patch:
@@ -120,8 +120,9 @@ public void testWrongPartitionPathField() {
 
   @Test
   public void testComplexRecordKeyField() {
-    SimpleKeyGenerator keyGenerator = new SimpleKeyGenerator(getComplexRecordKeyProp());
-    assertThrows(HoodieKeyException.class, () -> keyGenerator.getRecordKey(getRecord()));
+    assertThrows(IllegalArgumentException.class, () -> {
+      new SimpleKeyGenerator(getComplexRecordKeyProp());
+    });
   }
 
   @Test

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -93,6 +93,7 @@
 import java.util.stream.Collectors;
 
 import static org.apache.hudi.common.util.queue.ExecutorType.BOUNDED_IN_MEMORY;
+import static org.apache.hudi.common.util.queue.ExecutorType.DISRUPTOR;
 import static org.apache.hudi.config.HoodieCleanConfig.CLEANER_POLICY;
 
 /**
@@ -138,6 +139,7 @@ public class HoodieWriteConfig extends HoodieConfig {
   public static final ConfigProperty<String> EXECUTOR_TYPE = ConfigProperty
       .key("hoodie.write.executor.type")
       .defaultValue(BOUNDED_IN_MEMORY.name())
+      .withValidValues(BOUNDED_IN_MEMORY.name(), DISRUPTOR.name())
       .withDocumentation("Set executor which orchestrates concurrent producers and consumers communicating through a message queue."
           + "default value is BOUNDED_IN_MEMORY which use a bounded in-memory queue using LinkedBlockingQueue."
           + "Also users could use DISRUPTOR, which use disruptor as a lock free message queue "
@@ -1000,7 +1002,7 @@ public String getKeyGeneratorClass() {
   }
 
   public ExecutorType getExecutorType() {
-    return ExecutorType.valueOf(getString(EXECUTOR_TYPE).toUpperCase(Locale.ROOT));
+    return ExecutorType.valueOf(getStringOrDefault(EXECUTOR_TYPE).toUpperCase(Locale.ROOT));
   }
 
   public boolean isCDCEnabled() {

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/avro/TestHoodieAvroParquetWriter.java
Patch:
@@ -67,7 +67,7 @@ public void testProperWriting() throws IOException {
 
     HoodieParquetConfig<HoodieAvroWriteSupport> parquetConfig =
         new HoodieParquetConfig(writeSupport, CompressionCodecName.GZIP, ParquetWriter.DEFAULT_BLOCK_SIZE,
-            ParquetWriter.DEFAULT_PAGE_SIZE, 1024 * 1024 * 1024, hadoopConf, 0.1);
+            ParquetWriter.DEFAULT_PAGE_SIZE, 1024 * 1024 * 1024, hadoopConf, 0.1, true);
 
     Path filePath = new Path(tmpDir.resolve("test.parquet").toAbsolutePath().toString());
 

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java
Patch:
@@ -110,7 +110,7 @@ public Path withInserts(String partition, String fileId, List<HoodieRecord> reco
           new AvroSchemaConverter().convert(schema), schema, Option.of(filter));
       HoodieParquetConfig<HoodieAvroWriteSupport> config = new HoodieParquetConfig<>(writeSupport, CompressionCodecName.GZIP,
           ParquetWriter.DEFAULT_BLOCK_SIZE, ParquetWriter.DEFAULT_PAGE_SIZE, 120 * 1024 * 1024,
-          new Configuration(), Double.parseDouble(HoodieStorageConfig.PARQUET_COMPRESSION_RATIO_FRACTION.defaultValue()));
+          new Configuration(), Double.parseDouble(HoodieStorageConfig.PARQUET_COMPRESSION_RATIO_FRACTION.defaultValue()), true);
       try (HoodieAvroParquetWriter writer = new HoodieAvroParquetWriter<>(
           new Path(Paths.get(basePath, partition, fileName).toString()), config, currentInstantTime,
           contextSupplier, populateMetaFields)) {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataFileWriterFactory.java
Patch:
@@ -75,6 +75,7 @@ private static HoodieRowDataFileWriter newParquetInternalRowFileWriter(
         writeConfig.getParquetPageSize(),
         writeConfig.getParquetMaxFileSize(),
         writeSupport.getHadoopConf(),
-        writeConfig.getParquetCompressionRatio()));
+        writeConfig.getParquetCompressionRatio(),
+        writeConfig.parquetDictionaryEnabled()));
   }
 }

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -2347,7 +2347,7 @@ public void testDataBlockFormatAppendAndReadWithProjectedSchema(
           new HashMap<HoodieLogBlockType, Integer>() {{
             put(HoodieLogBlockType.AVRO_DATA_BLOCK, 0); // not supported
             put(HoodieLogBlockType.HFILE_DATA_BLOCK, 0); // not supported
-            put(HoodieLogBlockType.PARQUET_DATA_BLOCK, HoodieAvroUtils.gteqAvro1_9() ? 2593 : 2605);
+            put(HoodieLogBlockType.PARQUET_DATA_BLOCK, HoodieAvroUtils.gteqAvro1_9() ? 1802 : 1809);
           }};
 
       List<IndexedRecord> recordsRead = getRecords(dataBlockRead);
@@ -2378,7 +2378,7 @@ private HoodieDataBlock getDataBlock(HoodieLogBlockType dataBlockType, List<Inde
       case HFILE_DATA_BLOCK:
         return new HoodieHFileDataBlock(records, header, Compression.Algorithm.GZ, pathForReader);
       case PARQUET_DATA_BLOCK:
-        return new HoodieParquetDataBlock(records, header, HoodieRecord.RECORD_KEY_METADATA_FIELD, CompressionCodecName.GZIP);
+        return new HoodieParquetDataBlock(records, header, HoodieRecord.RECORD_KEY_METADATA_FIELD, CompressionCodecName.GZIP, 0.1, true);
       default:
         throw new RuntimeException("Unknown data block type " + dataBlockType);
     }

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/testutils/InputFormatTestUtil.java
Patch:
@@ -399,7 +399,7 @@ public static HoodieLogFormat.Writer writeDataBlockToLogFile(File partitionDir,
       dataBlock = new HoodieHFileDataBlock(
           records, header, Compression.Algorithm.GZ, writer.getLogFile().getPath());
     } else if (logBlockType == HoodieLogBlock.HoodieLogBlockType.PARQUET_DATA_BLOCK) {
-      dataBlock = new HoodieParquetDataBlock(records, header, HoodieRecord.RECORD_KEY_METADATA_FIELD, CompressionCodecName.GZIP);
+      dataBlock = new HoodieParquetDataBlock(records, header, HoodieRecord.RECORD_KEY_METADATA_FIELD, CompressionCodecName.GZIP, 0.1, true);
     } else {
       dataBlock = new HoodieAvroDataBlock(records, header, HoodieRecord.RECORD_KEY_METADATA_FIELD);
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieOrcWriter.java
Patch:
@@ -35,8 +35,8 @@
 import org.apache.orc.OrcFile;
 import org.apache.orc.TypeDescription;
 import org.apache.orc.Writer;
-import org.apache.orc.storage.ql.exec.vector.ColumnVector;
-import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 
 import java.io.Closeable;
 import java.io.IOException;

File: hudi-common/src/main/java/org/apache/hudi/common/table/cdc/HoodieCDCFileSplit.java
Patch:
@@ -22,8 +22,6 @@
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.util.Option;
 
-import org.jetbrains.annotations.NotNull;
-
 import java.io.Serializable;
 import java.util.Collection;
 import java.util.Collections;
@@ -125,7 +123,7 @@ public Option<FileSlice> getAfterFileSlice() {
   }
 
   @Override
-  public int compareTo(@NotNull HoodieCDCFileSplit o) {
+  public int compareTo(HoodieCDCFileSplit o) {
     return this.instant.compareTo(o.instant);
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/common/util/CustomizedThreadFactory.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.util;
 
-import org.jetbrains.annotations.NotNull;
+import javax.annotation.Nonnull;
 import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.atomic.AtomicLong;
 
@@ -59,7 +59,7 @@ public CustomizedThreadFactory(String threadNamePrefix, boolean daemon) {
   }
 
   @Override
-  public Thread newThread(@NotNull Runnable r) {
+  public Thread newThread(@Nonnull Runnable r) {
     Thread runThread = preExecuteRunnable == null ? new Thread(r) : new Thread(new Runnable() {
 
       @Override
@@ -68,7 +68,6 @@ public void run() {
         r.run();
       }
     });
-
     runThread.setDaemon(daemon);
     runThread.setName(threadName + threadNum.getAndIncrement());
     return runThread;

File: hudi-common/src/main/java/org/apache/hudi/common/util/OrcReaderIterator.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericData;
 import org.apache.avro.generic.GenericData.Record;
-import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hudi.exception.HoodieIOException;
 
 import org.apache.orc.RecordReader;

File: hudi-common/src/main/java/org/apache/hudi/common/util/OrcUtils.java
Patch:
@@ -38,8 +38,8 @@
 import org.apache.orc.Reader.Options;
 import org.apache.orc.RecordReader;
 import org.apache.orc.TypeDescription;
-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;
-import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 
 import java.io.IOException;
 import java.nio.ByteBuffer;

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestOrcReaderIterator.java
Patch:
@@ -28,9 +28,9 @@
 import org.apache.orc.RecordReader;
 import org.apache.orc.TypeDescription;
 import org.apache.orc.Writer;
-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;
-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;
-import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/testutils/UtilitiesTestBase.java
Patch:
@@ -62,8 +62,8 @@
 import org.apache.orc.OrcFile;
 import org.apache.orc.TypeDescription;
 import org.apache.orc.Writer;
-import org.apache.orc.storage.ql.exec.vector.ColumnVector;
-import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.parquet.avro.AvroParquetWriter;
 import org.apache.parquet.hadoop.ParquetFileWriter.Mode;
 import org.apache.parquet.hadoop.ParquetWriter;

File: hudi-common/src/main/java/org/apache/hudi/common/model/debezium/AbstractDebeziumAvroPayload.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.common.model.debezium;
 
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.util.Option;
 
@@ -76,7 +77,7 @@ private Option<IndexedRecord> handleDeleteOperation(IndexedRecord insertRecord)
     boolean delete = false;
     if (insertRecord instanceof GenericRecord) {
       GenericRecord record = (GenericRecord) insertRecord;
-      Object value = record.get(DebeziumConstants.FLATTENED_OP_COL_NAME);
+      Object value = HoodieAvroUtils.getFieldVal(record, DebeziumConstants.FLATTENED_OP_COL_NAME);
       delete = value != null && value.toString().equalsIgnoreCase(DebeziumConstants.DELETE_OP);
     }
 

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -2347,7 +2347,7 @@ public void testDataBlockFormatAppendAndReadWithProjectedSchema(
           new HashMap<HoodieLogBlockType, Integer>() {{
             put(HoodieLogBlockType.AVRO_DATA_BLOCK, 0); // not supported
             put(HoodieLogBlockType.HFILE_DATA_BLOCK, 0); // not supported
-            put(HoodieLogBlockType.PARQUET_DATA_BLOCK, 2605);
+            put(HoodieLogBlockType.PARQUET_DATA_BLOCK, HoodieAvroUtils.gteqAvro1_9() ? 2593 : 2605);
           }};
 
       List<IndexedRecord> recordsRead = getRecords(dataBlockRead);

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestPartialUpdateAvroPayload.java
Patch:
@@ -55,7 +55,7 @@ public class TestPartialUpdateAvroPayload {
       + "    {\"name\": \"id\", \"type\": [\"null\", \"string\"]},\n"
       + "    {\"name\": \"partition\", \"type\": [\"null\", \"string\"]},\n"
       + "    {\"name\": \"ts\", \"type\": [\"null\", \"long\"]},\n"
-      + "    {\"name\": \"_hoodie_is_deleted\", \"type\": [\"null\", \"boolean\"], \"default\":false},\n"
+      + "    {\"name\": \"_hoodie_is_deleted\", \"type\": \"boolean\", \"default\": false},\n"
       + "    {\"name\": \"city\", \"type\": [\"null\", \"string\"]},\n"
       + "    {\"name\": \"child\", \"type\": [\"null\", {\"type\": \"array\", \"items\": \"string\"}]}\n"
       + "  ]\n"

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java
Patch:
@@ -220,13 +220,13 @@ public String compact(
       @ShellOption(value = {"--parallelism"}, defaultValue = "3",
           help = "Parallelism for hoodie compaction") final String parallelism,
       @ShellOption(value = "--schemaFilePath",
-          help = "Path for Avro schema file", defaultValue = ShellOption.NULL) final String schemaFilePath,
+          help = "Path for Avro schema file", defaultValue = "") final String schemaFilePath,
       @ShellOption(value = "--sparkMaster", defaultValue = "local",
           help = "Spark Master") String master,
       @ShellOption(value = "--sparkMemory", defaultValue = "4G",
           help = "Spark executor memory") final String sparkMemory,
       @ShellOption(value = "--retry", defaultValue = "1", help = "Number of retries") final String retry,
-      @ShellOption(value = "--compactionInstant", help = "Base path for the target hoodie table",
+      @ShellOption(value = "--compactionInstant", help = "Instant of compaction.request",
           defaultValue = ShellOption.NULL) String compactionInstantTime,
       @ShellOption(value = "--propsFilePath", help = "path to properties file on localfs or dfs with configurations for hoodie client for compacting",
           defaultValue = "") final String propsFilePath,

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -151,7 +151,7 @@ public static void main(String[] args) throws Exception {
           }
           configs = new ArrayList<>();
           if (args.length > 10) {
-            configs.addAll(Arrays.asList(args).subList(9, args.length));
+            configs.addAll(Arrays.asList(args).subList(10, args.length));
           }
           returnCode = compact(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]), args[7],
               Integer.parseInt(args[8]), HoodieCompactor.EXECUTE, propsFilePath, configs);

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestCompactionCommand.java
Patch:
@@ -126,7 +126,7 @@ public void testCompact() throws IOException {
     writeSchemaToTmpFile(schemaPath);
 
     Object result2 = shell.evaluate(() ->
-            String.format("compaction run --parallelism %s --schemaFilePath %s --sparkMaster %s",
+            String.format("compaction run --parallelism %s --schemaFilePath %s --sparkMaster %s --hoodieConfigs hoodie.embed.timeline.server=false",
             2, schemaPath, "local"));
 
     assertAll("Command run failed",

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimelineUtils.java
Patch:
@@ -185,7 +185,7 @@ private static Option<String> getMetadataValue(HoodieTableMetaClient metaClient,
     }
   }
   
-  private static boolean isClusteringCommit(HoodieTableMetaClient metaClient, HoodieInstant instant) {
+  public static boolean isClusteringCommit(HoodieTableMetaClient metaClient, HoodieInstant instant) {
     try {
       if (HoodieTimeline.REPLACE_COMMIT_ACTION.equals(instant.getAction())) {
         // replacecommit is used for multiple operations: insert_overwrite/cluster etc. 
@@ -194,7 +194,7 @@ private static boolean isClusteringCommit(HoodieTableMetaClient metaClient, Hood
             metaClient.getActiveTimeline().getInstantDetails(instant).get(), HoodieReplaceCommitMetadata.class);
         return WriteOperationType.CLUSTER.equals(replaceMetadata.getOperationType());
       }
-      
+
       return false;
     } catch (IOException e) {
       throw new HoodieIOException("Unable to read instant information: " + instant + " for " + metaClient.getBasePath(), e);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java
Patch:
@@ -148,7 +148,9 @@ public Pair<Option<Dataset<Row>>, String> fetchNextBatch(Option<String> lastCkpt
           .load(srcPath)
           // add filtering so that only interested records are returned.
           .filter(String.format("%s > '%s'", HoodieRecord.COMMIT_TIME_METADATA_FIELD,
-              queryTypeAndInstantEndpts.getRight().getLeft()));
+              queryTypeAndInstantEndpts.getRight().getLeft()))
+          .filter(String.format("%s <= '%s'", HoodieRecord.COMMIT_TIME_METADATA_FIELD,
+              queryTypeAndInstantEndpts.getRight().getRight()));
     }
 
     /*

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java
Patch:
@@ -157,7 +157,9 @@ public Pair<Option<Dataset<Row>>, String> fetchNextBatch(Option<String> lastCkpt
           .option(DataSourceReadOptions.QUERY_TYPE().key(), DataSourceReadOptions.QUERY_TYPE_SNAPSHOT_OPT_VAL()).load(srcPath)
           // add filtering so that only interested records are returned.
           .filter(String.format("%s > '%s'", HoodieRecord.COMMIT_TIME_METADATA_FIELD,
-              queryTypeAndInstantEndpts.getRight().getLeft()));
+              queryTypeAndInstantEndpts.getRight().getLeft()))
+          .filter(String.format("%s <= '%s'", HoodieRecord.COMMIT_TIME_METADATA_FIELD,
+              queryTypeAndInstantEndpts.getRight().getRight()));
     }
 
     if (source.isEmpty()) {

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java
Patch:
@@ -77,9 +77,10 @@
 public class FSUtils {
 
   private static final Logger LOG = LogManager.getLogger(FSUtils.class);
-  // Log files are of this pattern - .b5068208-e1a4-11e6-bf01-fe55135034f3_20170101134598.log.1
+  // Log files are of this pattern - .b5068208-e1a4-11e6-bf01-fe55135034f3_20170101134598.log.1_1-0-1
+  // Archive log files are of this pattern - .commits_.archive.1_1-0-1
   private static final Pattern LOG_FILE_PATTERN =
-      Pattern.compile("\\.(.*)_(.*)\\.(.*)\\.([0-9]*)(_(([0-9]*)-([0-9]*)-([0-9]*)(-cdc)?))?");
+      Pattern.compile("\\.(.+)_(.*)\\.(.+)\\.(\\d+)(_((\\d+)-(\\d+)-(\\d+))(-cdc)?)?");
   private static final String LOG_FILE_PREFIX = ".";
   private static final int MAX_ATTEMPTS_RECOVER_LEASE = 10;
   private static final long MIN_CLEAN_TO_KEEP = 10;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java
Patch:
@@ -553,7 +553,8 @@ private Table instantiateHiveTable(ObjectPath tablePath, CatalogBaseTable table,
     // because since Hive 3.x, there is validation when altering table,
     // when the metadata fields are synced through the hive sync tool,
     // a compatability issue would be reported.
-    List<FieldSchema> allColumns = HiveSchemaUtils.toHiveFieldSchema(table.getSchema());
+    boolean withOperationField = Boolean.parseBoolean(table.getOptions().getOrDefault(FlinkOptions.CHANGELOG_ENABLED.key(), "false"));
+    List<FieldSchema> allColumns = HiveSchemaUtils.toHiveFieldSchema(table.getSchema(), withOperationField);
 
     // Table columns and partition keys
     CatalogTable catalogTable = (CatalogTable) table;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java
Patch:
@@ -677,10 +677,10 @@ private void validateBucketIndexConfig() {
         // check the bucket index hash field
         if (StringUtils.isNullOrEmpty(hoodieIndexConfig.getString(BUCKET_INDEX_HASH_FIELD))) {
           hoodieIndexConfig.setValue(BUCKET_INDEX_HASH_FIELD,
-              hoodieIndexConfig.getStringOrDefault(KeyGeneratorOptions.RECORDKEY_FIELD_NAME));
+              hoodieIndexConfig.getString(KeyGeneratorOptions.RECORDKEY_FIELD_NAME));
         } else {
           boolean valid = Arrays
-              .stream(hoodieIndexConfig.getStringOrDefault(KeyGeneratorOptions.RECORDKEY_FIELD_NAME).split(","))
+              .stream(hoodieIndexConfig.getString(KeyGeneratorOptions.RECORDKEY_FIELD_NAME).split(","))
               .collect(Collectors.toSet())
               .containsAll(Arrays.asList(hoodieIndexConfig.getString(BUCKET_INDEX_HASH_FIELD).split(",")));
           if (!valid) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/bucket/TestHoodieSimpleBucketIndex.java
Patch:
@@ -78,6 +78,7 @@ public void tearDown() throws Exception {
   public void testBucketIndexValidityCheck() {
     Properties props = new Properties();
     props.setProperty(HoodieIndexConfig.BUCKET_INDEX_HASH_FIELD.key(), "_row_key");
+    props.setProperty(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key(), "uuid");
     assertThrows(HoodieIndexException.class, () -> {
       HoodieIndexConfig.newBuilder().fromProperties(props)
           .withIndexType(HoodieIndex.IndexType.BUCKET)

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java
Patch:
@@ -145,18 +145,17 @@ private Properties makeIndexConfig(HoodieIndex.IndexType indexType) {
     Properties props = new Properties();
     HoodieIndexConfig.Builder indexConfig = HoodieIndexConfig.newBuilder()
         .withIndexType(indexType);
-    props.putAll(indexConfig.build().getProps());
     if (indexType.equals(HoodieIndex.IndexType.BUCKET)) {
       props.setProperty(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key(), "_row_key");
       indexConfig.fromProperties(props)
           .withIndexKeyField("_row_key")
           .withBucketNum("1")
           .withBucketIndexEngineType(HoodieIndex.BucketIndexEngineType.SIMPLE);
-      props.putAll(indexConfig.build().getProps());
       props.putAll(HoodieLayoutConfig.newBuilder().fromProperties(props)
           .withLayoutType(HoodieStorageLayout.LayoutType.BUCKET.name())
           .withLayoutPartitioner(SparkBucketIndexPartitioner.class.getName()).build().getProps());
     }
+    props.putAll(indexConfig.build().getProps());
     return props;
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/config/ConfigProperty.java
Patch:
@@ -27,9 +27,9 @@
 import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
+import java.util.Objects;
 import java.util.Set;
 import java.util.function.Function;
-import java.util.Objects;
 
 /**
  * ConfigProperty describes a configuration property. It contains the configuration
@@ -76,7 +76,7 @@ public String key() {
 
   public T defaultValue() {
     if (defaultValue == null) {
-      throw new HoodieException("There's no default value for this config");
+      throw new HoodieException(String.format("There's no default value for this config: %s", key));
     }
     return defaultValue;
   }

File: hudi-common/src/main/java/org/apache/hudi/keygen/constant/KeyGeneratorOptions.java
Patch:
@@ -45,7 +45,7 @@ public class KeyGeneratorOptions extends HoodieConfig {
 
   public static final ConfigProperty<String> RECORDKEY_FIELD_NAME = ConfigProperty
       .key("hoodie.datasource.write.recordkey.field")
-      .defaultValue("uuid")
+      .noDefaultValue()
       .withDocumentation("Record key field. Value to be used as the `recordKey` component of `HoodieKey`.\n"
           + "Actual value will be obtained by invoking .toString() on the field value. Nested fields can be specified using\n"
           + "the dot notation eg: `a.b.c`");

File: hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/quickstart/HoodieSparkQuickstart.java
Patch:
@@ -210,7 +210,7 @@ public static Dataset<Row> delete(SparkSession spark, String tablePath, String t
     df.write().format("org.apache.hudi")
         .options(QuickstartUtils.getQuickstartWriteConfigs())
         .option(HoodieWriteConfig.PRECOMBINE_FIELD_NAME.key(), "ts")
-        .option(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME.key(), "uuid")
+        .option(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key(), "uuid")
         .option(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME.key(), "partitionpath")
         .option(TBL_NAME.key(), tableName)
         .option("hoodie.datasource.write.operation", WriteOperationType.DELETE.value())

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/ITTestHoodieDataSource.java
Patch:
@@ -436,7 +436,8 @@ void testStreamReadMorTableWithCompactionPlan() throws Exception {
         .end();
     streamTableEnv.executeSql(hoodieTableDDL);
 
-    streamTableEnv.executeSql("insert into t1 select * from source");
+    String insertInto = "insert into t1 select * from source";
+    execInsertSql(streamTableEnv, insertInto);
 
     List<Row> result = execSelectSql(streamTableEnv, "select * from t1", 10);
     final String expected = "["

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieHFileRealtimeInputFormat.java
Patch:
@@ -72,7 +72,7 @@ public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSpli
           // For e:g _hoodie_record_key would be missing and merge step would throw exceptions.
           // TO fix this, hoodie columns are appended late at the time record-reader gets built instead of construction
           // time.
-          HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf, Option.empty());
+          HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf, Option.empty(), Option.empty());
 
           this.conf = jobConf;
           this.conf.set(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP, "true");

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/CopyOnWriteInsertHandler.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer;
+import org.apache.hudi.common.util.queue.IteratorBasedQueueConsumer;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.execution.HoodieLazyInsertIterable.HoodieInsertValueGenResult;
 import org.apache.hudi.io.HoodieWriteHandle;
@@ -38,7 +38,7 @@
  * Consumes stream of hoodie records from in-memory queue and writes to one or more create-handles.
  */
 public class CopyOnWriteInsertHandler<T extends HoodieRecordPayload>
-    extends BoundedInMemoryQueueConsumer<HoodieInsertValueGenResult<HoodieRecord>, List<WriteStatus>> {
+    extends IteratorBasedQueueConsumer<HoodieInsertValueGenResult<HoodieRecord>, List<WriteStatus>> {
 
   private HoodieWriteConfig config;
   private String instantTime;

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/execution/ExplicitWriteHandler.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer;
+import org.apache.hudi.common.util.queue.IteratorBasedQueueConsumer;
 import org.apache.hudi.io.HoodieWriteHandle;
 
 import java.util.ArrayList;
@@ -31,7 +31,7 @@
  * Consumes stream of hoodie records from in-memory queue and writes to one explicit create handle.
  */
 public class ExplicitWriteHandler<T extends HoodieRecordPayload>
-    extends BoundedInMemoryQueueConsumer<HoodieLazyInsertIterable.HoodieInsertValueGenResult<HoodieRecord>, List<WriteStatus>> {
+    extends IteratorBasedQueueConsumer<HoodieLazyInsertIterable.HoodieInsertValueGenResult<HoodieRecord>, List<WriteStatus>> {
 
   private final List<WriteStatus> statuses = new ArrayList<>();
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/ParquetReaderIterator.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.util;
 
-import org.apache.hudi.common.util.queue.BoundedInMemoryQueue;
+import org.apache.hudi.common.util.queue.BoundedInMemoryQueueIterable;
 import org.apache.hudi.exception.HoodieException;
 
 import org.apache.parquet.hadoop.ParquetReader;
@@ -27,7 +27,7 @@
 
 /**
  * This class wraps a parquet reader and provides an iterator based api to read from a parquet file. This is used in
- * {@link BoundedInMemoryQueue}
+ * {@link BoundedInMemoryQueueIterable}
  */
 public class ParquetReaderIterator<T> implements ClosableIterator<T> {
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/IteratorBasedQueueProducer.java
Patch:
@@ -28,7 +28,7 @@
  *
  * @param <I> Item type produced for the buffer.
  */
-public class IteratorBasedQueueProducer<I> implements BoundedInMemoryQueueProducer<I> {
+public class IteratorBasedQueueProducer<I> implements HoodieProducer<I> {
 
   private static final Logger LOG = LogManager.getLogger(IteratorBasedQueueProducer.class);
 
@@ -40,7 +40,7 @@ public IteratorBasedQueueProducer(Iterator<I> inputIterator) {
   }
 
   @Override
-  public void produce(BoundedInMemoryQueue<I, ?> queue) throws Exception {
+  public void produce(HoodieMessageQueue<I, ?> queue) throws Exception {
     LOG.info("starting to buffer records");
     while (inputIterator.hasNext()) {
       queue.insertRecord(inputIterator.next());

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/FormatUtils.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.ExternalSpillableMap;
 import org.apache.hudi.common.util.queue.BoundedInMemoryExecutor;
-import org.apache.hudi.common.util.queue.BoundedInMemoryQueueProducer;
+import org.apache.hudi.common.util.queue.HoodieProducer;
 import org.apache.hudi.common.util.queue.FunctionBasedQueueProducer;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.configuration.FlinkOptions;
@@ -229,8 +229,8 @@ public Iterator<HoodieRecord<?>> getRecordsIterator() {
     /**
      * Setup log and parquet reading in parallel. Both write to central buffer.
      */
-    private List<BoundedInMemoryQueueProducer<HoodieRecord<?>>> getParallelProducers() {
-      List<BoundedInMemoryQueueProducer<HoodieRecord<?>>> producers = new ArrayList<>();
+    private List<HoodieProducer<HoodieRecord<?>>> getParallelProducers() {
+      List<HoodieProducer<HoodieRecord<?>>> producers = new ArrayList<>();
       producers.add(new FunctionBasedQueueProducer<>(buffer -> {
         scanner.scan();
         return null;

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java
Patch:
@@ -24,8 +24,8 @@
 import org.apache.hudi.common.util.Functions;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.queue.BoundedInMemoryExecutor;
-import org.apache.hudi.common.util.queue.BoundedInMemoryQueueProducer;
 import org.apache.hudi.common.util.queue.FunctionBasedQueueProducer;
+import org.apache.hudi.common.util.queue.HoodieProducer;
 import org.apache.hudi.common.util.queue.IteratorBasedQueueProducer;
 import org.apache.hudi.hadoop.RecordReaderValueIterator;
 import org.apache.hudi.hadoop.SafeParquetRecordReaderWrapper;
@@ -104,8 +104,8 @@ public RealtimeUnmergedRecordReader(RealtimeSplit split, JobConf job,
   /**
    * Setup log and parquet reading in parallel. Both write to central buffer.
    */
-  private List<BoundedInMemoryQueueProducer<ArrayWritable>> getParallelProducers() {
-    List<BoundedInMemoryQueueProducer<ArrayWritable>> producers = new ArrayList<>();
+  private List<HoodieProducer<ArrayWritable>> getParallelProducers() {
+    List<HoodieProducer<ArrayWritable>> producers = new ArrayList<>();
     producers.add(new FunctionBasedQueueProducer<>(buffer -> {
       logRecordScanner.scan();
       return null;

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteJob.java
Patch:
@@ -340,5 +340,8 @@ public static class HoodieTestSuiteConfig extends HoodieDeltaStreamer.Config {
 
     @Parameter(names = {"--trino-jdbc-password"}, description = "Password corresponding to the username to use for authentication")
     public String trinoPassword;
+
+    @Parameter(names = {"--index-type"}, description = "Index type to use for writes")
+    public String indexType = "SIMPLE";
   }
 }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteWriter.java
Patch:
@@ -91,6 +91,7 @@ public HoodieWriteConfig getWriteConfig() {
   }
 
   private HoodieWriteConfig getHoodieClientConfig(HoodieTestSuiteJob.HoodieTestSuiteConfig cfg, Properties props, String schema) {
+
     HoodieWriteConfig.Builder builder =
         HoodieWriteConfig.newBuilder().combineInput(true, true).withPath(cfg.targetBasePath)
             .withAutoCommit(false)
@@ -99,7 +100,7 @@ private HoodieWriteConfig getHoodieClientConfig(HoodieTestSuiteJob.HoodieTestSui
                 .withPayloadClass(cfg.payloadClassName)
                 .build())
             .forTable(cfg.targetTableName)
-            .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())
+            .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.valueOf(cfg.indexType)).build())
             .withProps(props);
     builder = builder.withSchema(schema);
     return builder.build();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieInternalRowFileWriter.java
Patch:
@@ -29,7 +29,7 @@
 public interface HoodieInternalRowFileWriter {
 
   /**
-   * @returns {@code true} if this RowFileWriter can take in more writes. else {@code false}.
+   * @return {@code true} if this RowFileWriter can take in more writes. else {@code false}.
    */
   boolean canWrite();
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.model.HoodieAvroPayload;
 import org.apache.hudi.common.model.HoodiePayloadProps;
+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.common.table.TableSchemaResolver;
@@ -85,7 +86,7 @@ public AbstractRealtimeRecordReader(RealtimeSplit split, JobConf job) {
 
   private boolean usesCustomPayload(HoodieTableMetaClient metaClient) {
     return !(metaClient.getTableConfig().getPayloadClass().contains(HoodieAvroPayload.class.getName())
-        || metaClient.getTableConfig().getPayloadClass().contains("org.apache.hudi.OverwriteWithLatestAvroPayload"));
+        || metaClient.getTableConfig().getPayloadClass().contains(OverwriteWithLatestAvroPayload.class.getName()));
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/DirectWriteMarkers.java
Patch:
@@ -106,7 +106,7 @@ public Set<String> createdAndMergedDataPaths(HoodieEngineContext context, int pa
       context.setJobStatus(this.getClass().getSimpleName(), "Obtaining marker files for all created, merged paths");
       dataFiles.addAll(context.flatMap(subDirectories, directory -> {
         Path path = new Path(directory);
-        FileSystem fileSystem = path.getFileSystem(serializedConf.get());
+        FileSystem fileSystem = FSUtils.getFs(path, serializedConf.get());
         RemoteIterator<LocatedFileStatus> itr = fileSystem.listFiles(path, true);
         List<String> result = new ArrayList<>();
         while (itr.hasNext()) {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java
Patch:
@@ -440,7 +440,7 @@ public static HoodieFlinkWriteClient createWriteClient(Configuration conf, Runti
   public static HoodieFlinkWriteClient createWriteClient(Configuration conf) throws IOException {
     HoodieWriteConfig writeConfig = getHoodieClientConfig(conf, true, false);
     // build the write client to start the embedded timeline server
-    final HoodieFlinkWriteClient writeClient = new HoodieFlinkWriteClient<>(HoodieFlinkEngineContext.DEFAULT, writeConfig);
+    final HoodieFlinkWriteClient writeClient = new HoodieFlinkWriteClient<>(new HoodieFlinkEngineContext(HadoopConfigurations.getHadoopConf(conf)), writeConfig);
     writeClient.setOperationType(WriteOperationType.fromValue(conf.getString(FlinkOptions.OPERATION)));
     // create the filesystem view storage properties for client
     final FileSystemViewStorageConfig viewStorageConfig = writeConfig.getViewStorageConfig();

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/meta/CkpMetadata.java
Patch:
@@ -90,7 +90,7 @@ public void close() {
   // -------------------------------------------------------------------------
 
   /**
-   * Initialize the message bus, would clean all the messages and publish the last pending instant.
+   * Initialize the message bus, would clean all the messages
    *
    * <p>This expects to be called by the driver.
    */

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java
Patch:
@@ -767,14 +767,14 @@ private static Object rewriteRecordWithNewSchema(Object oldRecord, Schema oldAvr
           Schema.Field field = fields.get(i);
           String fieldName = field.name();
           fieldNames.push(fieldName);
-          if (oldSchema.getField(field.name()) != null) {
+          if (oldSchema.getField(field.name()) != null && !renameCols.containsKey(field.name())) {
             Schema.Field oldField = oldSchema.getField(field.name());
             newRecord.put(i, rewriteRecordWithNewSchema(indexedRecord.get(oldField.pos()), oldField.schema(), fields.get(i).schema(), renameCols, fieldNames));
           } else {
             String fieldFullName = createFullName(fieldNames);
             String fieldNameFromOldSchema = renameCols.getOrDefault(fieldFullName, "");
             // deal with rename
-            if (oldSchema.getField(field.name()) == null && oldSchema.getField(fieldNameFromOldSchema) != null) {
+            if (oldSchema.getField(fieldNameFromOldSchema) != null) {
               // find rename
               Schema.Field oldField = oldSchema.getField(fieldNameFromOldSchema);
               newRecord.put(i, rewriteRecordWithNewSchema(indexedRecord.get(oldField.pos()), oldField.schema(), fields.get(i).schema(), renameCols, fieldNames));

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java
Patch:
@@ -322,10 +322,9 @@ public static List<String> getAllPartitionPaths(HoodieEngineContext engineContex
   public static Map<String, FileStatus[]> getFilesInPartitions(HoodieEngineContext engineContext,
                                                                HoodieMetadataConfig metadataConfig,
                                                                String basePathStr,
-                                                               String[] partitionPaths,
-                                                               String spillableMapPath) {
+                                                               String[] partitionPaths) {
     try (HoodieTableMetadata tableMetadata = HoodieTableMetadata.create(engineContext, metadataConfig, basePathStr,
-        spillableMapPath, true)) {
+        FileSystemViewStorageConfig.SPILLABLE_DIR.defaultValue(), true)) {
       return tableMetadata.getAllFilesInPartitions(Arrays.asList(partitionPaths));
     } catch (Exception ex) {
       throw new HoodieException("Error get files in partitions: " + String.join(",", partitionPaths), ex);

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/FileIndex.java
Patch:
@@ -138,7 +138,7 @@ public FileStatus[] getFilesInPartitions() {
     }
     String[] partitions = getOrBuildPartitionPaths().stream().map(p -> fullPartitionPath(path, p)).toArray(String[]::new);
     FileStatus[] allFileStatus = FSUtils.getFilesInPartitions(HoodieFlinkEngineContext.DEFAULT, metadataConfig, path.toString(),
-            partitions, "/tmp/")
+            partitions)
         .values().stream().flatMap(Arrays::stream).toArray(FileStatus[]::new);
     Set<String> candidateFiles = candidateFilesInMetadataTable(allFileStatus);
     if (candidateFiles == null) {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java
Patch:
@@ -52,7 +52,7 @@ public static class Config {
      * {@value #NUM_INSTANTS_PER_FETCH} allows the max number of instants whose changes can be incrementally fetched.
      */
     static final String NUM_INSTANTS_PER_FETCH = "hoodie.deltastreamer.source.hoodieincr.num_instants";
-    static final Integer DEFAULT_NUM_INSTANTS_PER_FETCH = 1;
+    static final Integer DEFAULT_NUM_INSTANTS_PER_FETCH = 5;
 
     /**
      * {@value #HOODIE_SRC_PARTITION_FIELDS} specifies partition fields that needs to be added to source table after

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java
Patch:
@@ -2101,6 +2101,7 @@ public void testHoodieIncrFallback() throws Exception {
     HoodieDeltaStreamer.Config downstreamCfg =
         TestHelpers.makeConfigForHudiIncrSrc(tableBasePath, downstreamTableBasePath,
             WriteOperationType.BULK_INSERT, true, null);
+    downstreamCfg.configs.add("hoodie.deltastreamer.source.hoodieincr.num_instants=1");
     new HoodieDeltaStreamer(downstreamCfg, jsc).sync();
 
     insertInTable(tableBasePath, 9, WriteOperationType.UPSERT);
@@ -2112,6 +2113,8 @@ public void testHoodieIncrFallback() throws Exception {
       downstreamCfg.configs = new ArrayList<>();
     }
 
+    // Remove source.hoodieincr.num_instants config
+    downstreamCfg.configs.remove(downstreamCfg.configs.size() - 1);
     downstreamCfg.configs.add(DataSourceReadOptions.INCREMENTAL_FALLBACK_TO_FULL_TABLE_SCAN_FOR_NON_EXISTING_FILES().key() + "=true");
     //Adding this conf to make testing easier :)
     downstreamCfg.configs.add("hoodie.deltastreamer.source.hoodieincr.num_instants=10");

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java
Patch:
@@ -171,7 +171,7 @@ public static HoodieWriteConfig getHoodieClientConfig(
             .withMergeAllowDuplicateOnInserts(OptionsResolver.insertClustering(conf))
             .withClusteringConfig(
                 HoodieClusteringConfig.newBuilder()
-                    .withAsyncClustering(conf.getBoolean(FlinkOptions.CLUSTERING_ASYNC_ENABLED))
+                    .withAsyncClustering(conf.getBoolean(FlinkOptions.CLUSTERING_SCHEDULE_ENABLED))
                     .withClusteringPlanStrategyClass(conf.getString(FlinkOptions.CLUSTERING_PLAN_STRATEGY_CLASS))
                     .withClusteringPlanPartitionFilterMode(
                         ClusteringPlanPartitionFilterMode.valueOf(conf.getString(FlinkOptions.CLUSTERING_PLAN_PARTITION_FILTER_MODE_NAME)))

File: hudi-common/src/main/java/org/apache/hudi/common/fs/inline/InLineFsDataInputStream.java
Patch:
@@ -33,11 +33,11 @@
  */
 public class InLineFsDataInputStream extends FSDataInputStream {
 
-  private final int startOffset;
+  private final long startOffset;
   private final FSDataInputStream outerStream;
-  private final int length;
+  private final long length;
 
-  public InLineFsDataInputStream(int startOffset, FSDataInputStream outerStream, int length) throws IOException {
+  public InLineFsDataInputStream(long startOffset, FSDataInputStream outerStream, long length) throws IOException {
     super(outerStream.getWrappedStream());
     this.startOffset = startOffset;
     this.outerStream = outerStream;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java
Patch:
@@ -137,7 +137,7 @@ public HoodieWriteMetadata<HoodieData<WriteStatus>> performClustering(final Hood
 
   /**
    * Execute clustering to write inputRecords into new files based on strategyParams.
-   * Different from {@link performClusteringWithRecordsRDD}, this method take {@link Dataset<Row>}
+   * Different from {@link MultipleSparkJobExecutionStrategy#performClusteringWithRecordsRDD}, this method take {@link Dataset<Row>}
    * as inputs.
    */
   public abstract HoodieData<WriteStatus> performClusteringWithRecordsAsRow(final Dataset<Row> inputRecords,

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieCatalogFactory.java
Patch:
@@ -56,9 +56,7 @@ public Catalog createCatalog(Context context) {
       case "hms":
         return new HoodieHiveCatalog(
             context.getName(),
-            helper.getOptions().get(CatalogOptions.CATALOG_PATH),
-            helper.getOptions().get(CatalogOptions.DEFAULT_DATABASE),
-            helper.getOptions().get(CatalogOptions.HIVE_CONF_DIR));
+            (Configuration) helper.getOptions());
       case "dfs":
         return new HoodieCatalog(
             context.getName(),

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/catalog/TestHoodieCatalogFactory.java
Patch:
@@ -78,6 +78,7 @@ void testCreateHMSCatalog() {
     options.put(CommonCatalogOptions.CATALOG_TYPE.key(), HoodieCatalogFactory.IDENTIFIER);
     options.put(CatalogOptions.HIVE_CONF_DIR.key(), CONF_DIR.getPath());
     options.put(CatalogOptions.MODE.key(), "hms");
+    options.put(CatalogOptions.TABLE_EXTERNAL.key(), "false");
 
     final Catalog actualCatalog =
         FactoryUtil.createCatalog(

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -144,6 +144,7 @@ public void setUp() throws IOException, InterruptedException {
   @AfterEach
   public void tearDown() throws IOException {
     fs.delete(partitionPath, true);
+    fs.delete(new Path(basePath), true);
   }
 
   @Test

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/minicluster/HdfsTestService.java
Patch:
@@ -145,6 +145,7 @@ private static Configuration configureDFSCluster(Configuration config, String lo
     config.set("hadoop.proxyuser." + user + ".groups", "*");
     config.set("hadoop.proxyuser." + user + ".hosts", "*");
     config.setBoolean("dfs.permissions", false);
+    config.set("dfs.blocksize","16777216");
     return config;
   }
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -144,7 +144,7 @@ class ColumnStats {
 
         GenericRecord genericRecord = (GenericRecord) record;
 
-        final Object fieldVal = convertValueForSpecificDataTypes(field.schema(), genericRecord.get(field.name()), true);
+        final Object fieldVal = convertValueForSpecificDataTypes(field.schema(), genericRecord.get(field.name()), false);
         final Schema fieldSchema = getNestedFieldSchemaFromWriteSchema(genericRecord.getSchema(), field.name());
 
         if (fieldVal != null && canCompare(fieldSchema)) {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/stats/ExpressionEvaluator.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.flink.table.types.logical.DecimalType;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.table.types.logical.TimestampType;
 
 import javax.validation.constraints.NotNull;
 
@@ -523,6 +524,8 @@ private static Object getValAsJavaObj(RowData indexRow, int pos, LogicalType col
       //       manually encoding corresponding values as int and long w/in the Column Stats Index and
       //       here we have to decode those back into corresponding logical representation.
       case TIMESTAMP_WITHOUT_TIME_ZONE:
+        TimestampType tsType = (TimestampType) colType;
+        return indexRow.getTimestamp(pos, tsType.getPrecision()).getMillisecond();
       case TIME_WITHOUT_TIME_ZONE:
       case DATE:
         return indexRow.getLong(pos);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestProtoKafkaSource.java
Patch:
@@ -98,7 +98,7 @@ public void testProtoKafkaSourceWithFlattenWrappedPrimitives() {
     final String topic = TEST_TOPIC_PREFIX + "testProtoKafkaSourceFlatten";
     testUtils.createTopic(topic, 2);
     TypedProperties props = createPropsForKafkaSource(topic, null, "earliest");
-    props.setProperty(ProtoClassBasedSchemaProvider.Config.PROTO_SCHEMA_FLATTEN_WRAPPED_PRIMITIVES.key(), "true");
+    props.setProperty(ProtoClassBasedSchemaProvider.Config.PROTO_SCHEMA_WRAPPED_PRIMITIVES_AS_RECORDS.key(), "true");
     SchemaProvider schemaProvider = new ProtoClassBasedSchemaProvider(props, jsc());
     Source protoKafkaSource = new ProtoKafkaSource(props, jsc(), spark(), schemaProvider, metrics);
     SourceFormatAdapter kafkaSource = new SourceFormatAdapter(protoKafkaSource);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -1165,7 +1165,7 @@ public void testUpdateRejectForClustering() throws IOException {
 
     // 3. insert one record with no updating reject exception, and not merge the small file, just generate a new file group
     String commitTime3 = "003";
-    insertBatchRecords(client, commitTime3, 1, 1).getKey();
+    insertBatchRecords(client, commitTime3, 1, 1);
     List<String> fileGroupIds2 = table.getFileSystemView().getAllFileGroups(testPartitionPath)
         .map(fileGroup -> fileGroup.getFileGroupId().getFileId()).collect(Collectors.toList());
     assertEquals(3, fileGroupIds2.size());
@@ -1178,7 +1178,7 @@ public void testUpdateRejectForClustering() throws IOException {
     String assertMsg = String.format("Not allowed to update the clustering files in partition: %s "
         + "For pending clustering operations, we are not going to support update for now.", testPartitionPath);
     assertThrows(HoodieUpsertException.class, () -> {
-      writeClient.upsert(jsc.parallelize(insertsAndUpdates3, 1), commitTime3).collect(); }, assertMsg);
+      writeClient.upsert(jsc.parallelize(insertsAndUpdates3, 1), commitTime4).collect(); }, assertMsg);
 
     // 5. insert one record with no updating reject exception, will merge the small file
     String commitTime5 = "005";

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -1694,7 +1694,7 @@ private void commitTableChange(InternalSchema newSchema, HoodieTableMetaClient m
       throw new HoodieCommitException("Failed to commit " + instantTime + " unable to save inflight metadata ", io);
     }
     Map<String, String> extraMeta = new HashMap<>();
-    extraMeta.put(SerDeHelper.LATEST_SCHEMA, SerDeHelper.toJson(newSchema.setSchemaId(Long.getLong(instantTime))));
+    extraMeta.put(SerDeHelper.LATEST_SCHEMA, SerDeHelper.toJson(newSchema.setSchemaId(Long.parseLong(instantTime))));
     // try to save history schemas
     FileBasedInternalSchemaStorageManager schemasManager = new FileBasedInternalSchemaStorageManager(metaClient);
     schemasManager.persistHistorySchemaStr(instantTime, SerDeHelper.inheritSchemas(newSchema, historySchemaStr));

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java
Patch:
@@ -90,6 +90,7 @@
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
+import static org.apache.hudi.common.config.HoodieCommonConfig.TIMESTAMP_AS_OF;
 import static org.apache.hudi.common.table.log.HoodieFileSliceReader.getFileSliceReader;
 import static org.apache.hudi.config.HoodieClusteringConfig.PLAN_STRATEGY_SORT_COLUMNS;
 
@@ -375,7 +376,7 @@ private Dataset<Row> readRecordsForGroupAsRow(JavaSparkContext jsc,
 
     HashMap<String, String> params = new HashMap<>();
     params.put("hoodie.datasource.query.type", "snapshot");
-    params.put("as.of.instant", instantTime);
+    params.put(TIMESTAMP_AS_OF.key(), instantTime);
 
     Path[] paths;
     if (hasLogFiles) {

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateAsyncOperations.java
Patch:
@@ -62,7 +62,7 @@ public void execute(ExecutionContext executionContext, int curItrCount) throws E
         
         HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setBasePath(executionContext.getHoodieTestSuiteWriter().getCfg().targetBasePath)
             .setConf(executionContext.getJsc().hadoopConfiguration()).build();
-        Option<HoodieInstant> latestCleanInstant = metaClient.getActiveTimeline().filter(instant -> instant.getAction().equals(HoodieTimeline.CLEAN_ACTION)).lastInstant();
+        Option<HoodieInstant> latestCleanInstant = metaClient.getActiveTimeline().getCleanerTimeline().filterCompletedInstants().lastInstant();
         if (latestCleanInstant.isPresent()) {
           log.warn("Latest clean commit " + latestCleanInstant.get());
           HoodieCleanMetadata cleanMetadata = CleanerUtils.getCleanerMetadata(metaClient, latestCleanInstant.get());

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/ProtoConversionUtil.java
Patch:
@@ -386,6 +386,9 @@ private static Object convertObject(Schema schema, Object value) {
           if (unsignedLongValue instanceof UInt64Value) {
             // Unwrap UInt64Value
             unsignedLongValue = getWrappedValue(unsignedLongValue);
+          } else if (unsignedLongValue instanceof Message) {
+            // Unexpected message type
+            throw new HoodieException("Unexpected Message type when converting as an unsigned long: " + unsignedLongValue.getClass().getName());
           }
           // convert the long to its unsigned value
           return DECIMAL_CONVERSION.toFixed(new BigDecimal(toUnsignedBigInteger((Long) unsignedLongValue)), schema, schema.getLogicalType());

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/ProtoConversionUtil.java
Patch:
@@ -278,7 +278,7 @@ private static Schema finalizeSchema(Schema schema, Descriptors.FieldDescriptor
         updatedSchema = Schema.createArray(updatedSchema);
       }
       // all fields in the oneof will be treated as nullable
-      if (fieldDescriptor.getContainingOneof() != null && !schema.isNullable()) {
+      if (fieldDescriptor.getContainingOneof() != null && !(schema.getType() == Schema.Type.UNION && schema.getTypes().get(0).getType() == Schema.Type.NULL)) {
         updatedSchema = makeSchemaNullable(updatedSchema);
       }
       return updatedSchema;
@@ -383,7 +383,7 @@ private static Object convertObject(Schema schema, Object value) {
             return GenericData.get().createFixed(null, (byte[]) value, schema);
           }
           Object unsignedLongValue = value;
-          if (unsignedLongValue instanceof Message) {
+          if (unsignedLongValue instanceof UInt64Value) {
             // Unwrap UInt64Value
             unsignedLongValue = getWrappedValue(unsignedLongValue);
           }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteOperatorCoordinator.java
Patch:
@@ -341,7 +341,7 @@ private static void initMetadataTable(HoodieFlinkWriteClient<?> writeClient) {
 
   private static CkpMetadata initCkpMetadata(HoodieTableMetaClient metaClient) throws IOException {
     CkpMetadata ckpMetadata = CkpMetadata.getInstance(metaClient.getFs(), metaClient.getBasePath());
-    ckpMetadata.bootstrap(metaClient);
+    ckpMetadata.bootstrap();
     return ckpMetadata;
   }
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/meta/CkpMetadata.java
Patch:
@@ -94,7 +94,7 @@ public void close() {
    *
    * <p>This expects to be called by the driver.
    */
-  public void bootstrap(HoodieTableMetaClient metaClient) throws IOException {
+  public void bootstrap() throws IOException {
     fs.delete(path, true);
     fs.mkdirs(path);
   }
@@ -173,8 +173,8 @@ private void load() {
   @Nullable
   public String lastPendingInstant() {
     load();
-    for (int i = this.messages.size() - 1; i >= 0; i--) {
-      CkpMessage ckpMsg = this.messages.get(i);
+    if (this.messages.size() > 0) {
+      CkpMessage ckpMsg = this.messages.get(this.messages.size() - 1);
       // consider 'aborted' as pending too to reuse the instant
       if (!ckpMsg.isComplete()) {
         return ckpMsg.getInstant();

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/meta/TestCkpMetadata.java
Patch:
@@ -32,6 +32,7 @@
 import java.io.File;
 import java.util.stream.IntStream;
 
+import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.MatcherAssert.assertThat;
 
@@ -63,7 +64,7 @@ void testWriteAndReadMessage() {
 
     assertThat(metadata.lastPendingInstant(), is("2"));
     metadata.commitInstant("2");
-    assertThat(metadata.lastPendingInstant(), is("1"));
+    assertThat(metadata.lastPendingInstant(), equalTo(null));
 
     // test cleaning
     IntStream.range(3, 6).forEach(i -> metadata.startInstant(i + ""));

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestProtoKafkaSource.java
Patch:
@@ -98,7 +98,7 @@ public void testProtoKafkaSourceWithFlattenWrappedPrimitives() {
     final String topic = TEST_TOPIC_PREFIX + "testProtoKafkaSourceFlatten";
     testUtils.createTopic(topic, 2);
     TypedProperties props = createPropsForKafkaSource(topic, null, "earliest");
-    props.setProperty(ProtoClassBasedSchemaProvider.Config.PROTO_SCHEMA_FLATTEN_WRAPPED_PRIMITIVES.key(), "true");
+    props.setProperty(ProtoClassBasedSchemaProvider.Config.PROTO_SCHEMA_WRAPPED_PRIMITIVES_AS_RECORDS.key(), "true");
     SchemaProvider schemaProvider = new ProtoClassBasedSchemaProvider(props, jsc());
     Source protoKafkaSource = new ProtoKafkaSource(props, jsc(), spark(), schemaProvider, metrics);
     SourceFormatAdapter kafkaSource = new SourceFormatAdapter(protoKafkaSource);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/helpers/TestProtoConversionUtil.java
Patch:
@@ -105,7 +105,7 @@ public void noFieldsSet_wellKnownTypesAreFlattened() throws IOException {
   @Test
   public void recursiveSchema_noOverflow() throws IOException {
     Schema.Parser parser = new Schema.Parser();
-    Schema convertedSchema = parser.parse(getClass().getClassLoader().getResourceAsStream("schema-provider/proto/parent_schema_recursive.avsc"));
+    Schema convertedSchema = parser.parse(getClass().getClassLoader().getResourceAsStream("schema-provider/proto/parent_schema_recursive_depth_2.avsc"));
     Pair<Parent, GenericRecord> inputAndOutput = createInputOutputForRecursiveSchemaNoOverflow(convertedSchema);
     GenericRecord actual = serializeAndDeserializeAvro(ProtoConversionUtil.convertToAvro(convertedSchema, inputAndOutput.getLeft()), convertedSchema);
     Assertions.assertEquals(inputAndOutput.getRight(), actual);
@@ -114,7 +114,7 @@ public void recursiveSchema_noOverflow() throws IOException {
   @Test
   public void recursiveSchema_withOverflow() throws Exception {
     Schema.Parser parser = new Schema.Parser();
-    Schema convertedSchema = parser.parse(getClass().getClassLoader().getResourceAsStream("schema-provider/proto/parent_schema_recursive.avsc"));
+    Schema convertedSchema = parser.parse(getClass().getClassLoader().getResourceAsStream("schema-provider/proto/parent_schema_recursive_depth_2.avsc"));
     Pair<Parent, GenericRecord> inputAndOutput = createInputOutputForRecursiveSchemaWithOverflow(convertedSchema);
     Parent input = inputAndOutput.getLeft();
     GenericRecord actual = serializeAndDeserializeAvro(ProtoConversionUtil.convertToAvro(convertedSchema, inputAndOutput.getLeft()), convertedSchema);

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkEnvCommand.java
Patch:
@@ -60,7 +60,7 @@ public String showEnvByKey(@ShellOption(value = {"--key"}, help = "Which env con
     if (key == null || key.isEmpty()) {
       return showAllEnv();
     } else {
-      return HoodiePrintHelper.print(new String[] {"key", "value"}, new String[][] {new String[] {key, env.get(key)}});
+      return HoodiePrintHelper.print(new String[] {"key", "value"}, new String[][] {new String[] {key, env.getOrDefault(key, "")}});
     }
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java
Patch:
@@ -166,7 +166,7 @@ protected void runPrecommitValidators(HoodieWriteMetadata<O> writeMetadata) {
     }
     throw new HoodieIOException("Precommit validation not implemented for all engines yet");
   }
-  
+
   protected void commitOnAutoCommit(HoodieWriteMetadata result) {
     // validate commit action before committing result
     runPrecommitValidators(result);
@@ -249,7 +249,6 @@ protected HoodieWriteMetadata<HoodieData<WriteStatus>> executeClustering(HoodieC
     HoodieData<WriteStatus> statuses = updateIndex(writeStatusList, writeMetadata);
     writeMetadata.setWriteStats(statuses.map(WriteStatus::getStat).collectAsList());
     writeMetadata.setPartitionToReplaceFileIds(getPartitionToReplacedFileIds(clusteringPlan, writeMetadata));
-    validateWriteResult(clusteringPlan, writeMetadata);
     commitOnAutoCommit(writeMetadata);
     if (!writeMetadata.getCommitMetadata().isPresent()) {
       HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeMetadata.getWriteStats().get(), writeMetadata.getPartitionToReplaceFileIds(),

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/LockManager.java
Patch:
@@ -72,7 +72,7 @@ public void lock() {
           Thread.sleep(maxWaitTimeInMs);
         } catch (HoodieLockException | InterruptedException e) {
           if (retryCount >= maxRetries) {
-            throw new HoodieLockException("Unable to acquire lock, lock object ", e);
+            throw new HoodieLockException("Unable to acquire lock, lock object " + lockProvider.getLock(), e);
           }
           try {
             Thread.sleep(maxWaitTimeInMs);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/ConcurrentOperation.java
Patch:
@@ -124,14 +124,15 @@ private void init(HoodieInstant instant) {
             HoodieRequestedReplaceMetadata requestedReplaceMetadata = this.metadataWrapper.getMetadataFromTimeline().getHoodieRequestedReplaceMetadata();
             org.apache.hudi.avro.model.HoodieCommitMetadata inflightCommitMetadata = this.metadataWrapper.getMetadataFromTimeline().getHoodieInflightReplaceMetadata();
             if (instant.isRequested()) {
-              if (requestedReplaceMetadata != null) {
+              // for insert_overwrite/insert_overwrite_table clusteringPlan will be empty
+              if (requestedReplaceMetadata != null && requestedReplaceMetadata.getClusteringPlan() != null) {
                 this.mutatedFileIds = getFileIdsFromRequestedReplaceMetadata(requestedReplaceMetadata);
                 this.operationType = WriteOperationType.CLUSTER;
               }
             } else {
               if (inflightCommitMetadata != null) {
                 this.mutatedFileIds = getFileIdWithoutSuffixAndRelativePathsFromSpecificRecord(inflightCommitMetadata.getPartitionToWriteStats()).keySet();
-                this.operationType = WriteOperationType.fromValue(this.metadataWrapper.getMetadataFromTimeline().getHoodieCommitMetadata().getOperationType());
+                this.operationType = WriteOperationType.fromValue(this.metadataWrapper.getMetadataFromTimeline().getHoodieInflightReplaceMetadata().getOperationType());
               } else if (requestedReplaceMetadata != null) {
                 // inflight replacecommit metadata is empty due to clustering, read fileIds from requested replacecommit
                 this.mutatedFileIds = getFileIdsFromRequestedReplaceMetadata(requestedReplaceMetadata);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndex.java
Patch:
@@ -111,13 +111,13 @@ public HoodieData<WriteStatus> updateLocation(
    * implementation is able to obtain the same mapping, for two hoodie keys with same `recordKey` but different
    * `partitionPath`
    *
-   * @return whether or not, the index implementation is global in nature
+   * @return whether the index implementation is global in nature
    */
   @PublicAPIMethod(maturity = ApiMaturityLevel.STABLE)
   public abstract boolean isGlobal();
 
   /**
-   * This is used by storage to determine, if its safe to send inserts, straight to the log, i.e having a
+   * This is used by storage to determine, if it is safe to send inserts, straight to the log, i.e. having a
    * {@link FileSlice}, with no data file.
    *
    * @return Returns true/false depending on whether the impl has this capability
@@ -133,7 +133,7 @@ public HoodieData<WriteStatus> updateLocation(
   public abstract boolean isImplicitWithStorage();
 
   /**
-   * To indicate if a operation type requires location tagging before writing
+   * To indicate if an operation type requires location tagging before writing
    */
   @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)
   public boolean requiresTagging(WriteOperationType operationType) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndex.java
Patch:
@@ -147,7 +147,7 @@ public boolean requiresTagging(WriteOperationType operationType) {
   }
 
   /**
-   * Each index type should implement it's own logic to release any resources acquired during the process.
+   * Each index type should implement its own logic to release any resources acquired during the process.
    */
   public void close() {
   }

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -711,7 +711,7 @@ public void testAppendAndReadOnCorruptedLog() throws IOException, URISyntaxExcep
     writer.appendBlock(dataBlock);
     writer.close();
 
-    // Append some arbit byte[] to thee end of the log (mimics a partially written commit)
+    // Append some arbit byte[] to the end of the log (mimics a partially written commit)
     fs = FSUtils.getFs(fs.getUri().toString(), fs.getConf());
     FSDataOutputStream outputStream = fs.append(writer.getLogFile().getPath());
     // create a block with
@@ -1013,7 +1013,7 @@ public void testAvroLogRecordReaderWithFailedPartialBlock(ExternalSpillableMap.D
 
     // Write 2
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "101");
-    // Append some arbit byte[] to thee end of the log (mimics a partially written commit)
+    // Append some arbit byte[] to the end of the log (mimics a partially written commit)
     fs = FSUtils.getFs(fs.getUri().toString(), fs.getConf());
     FSDataOutputStream outputStream = fs.append(writer.getLogFile().getPath());
     // create a block with
@@ -1898,7 +1898,7 @@ public void testAppendAndReadOnCorruptedLogInReverse(boolean readBlocksLazily)
 
     FileCreateUtils.createDeltaCommit(basePath, "100", fs);
 
-    // Append some arbit byte[] to thee end of the log (mimics a partially written commit)
+    // Append some arbit byte[] to the end of the log (mimics a partially written commit)
     fs = FSUtils.getFs(fs.getUri().toString(), fs.getConf());
     FSDataOutputStream outputStream = fs.append(writer.getLogFile().getPath());
     // create a block with

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/ProtoKafkaSource.java
Patch:
@@ -53,10 +53,10 @@ public ProtoKafkaSource(TypedProperties props, JavaSparkContext sparkContext,
                           SparkSession sparkSession, SchemaProvider schemaProvider, HoodieDeltaStreamerMetrics metrics) {
     super(props, sparkContext, sparkSession, schemaProvider, SourceType.PROTO, metrics);
     DataSourceUtils.checkRequiredProperties(props, Collections.singletonList(
-        ProtoClassBasedSchemaProvider.Config.PROTO_SCHEMA_CLASS_NAME));
+        ProtoClassBasedSchemaProvider.Config.PROTO_SCHEMA_CLASS_NAME.key()));
     props.put(NATIVE_KAFKA_KEY_DESERIALIZER_PROP, StringDeserializer.class);
     props.put(NATIVE_KAFKA_VALUE_DESERIALIZER_PROP, ByteArrayDeserializer.class);
-    className = props.getString(ProtoClassBasedSchemaProvider.Config.PROTO_SCHEMA_CLASS_NAME);
+    className = props.getString(ProtoClassBasedSchemaProvider.Config.PROTO_SCHEMA_CLASS_NAME.key());
     this.offsetGen = new KafkaOffsetGen(props);
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java
Patch:
@@ -73,7 +73,8 @@ public class HoodieIndexConfig extends HoodieConfig {
       .noDefaultValue()
       .withValidValues(HBASE.name(), INMEMORY.name(), BLOOM.name(), GLOBAL_BLOOM.name(),
           SIMPLE.name(), GLOBAL_SIMPLE.name(), BUCKET.name())
-      .withDocumentation("Type of index to use. Default is Bloom filter. "
+      .withDocumentation("Type of index to use. Default is SIMPLE on Spark engine, "
+          + "and INMEMORY on Flink and Java engines. "
           + "Possible options are [BLOOM | GLOBAL_BLOOM |SIMPLE | GLOBAL_SIMPLE | INMEMORY | HBASE | BUCKET]. "
           + "Bloom filters removes the dependency on a external system "
           + "and is stored in the footer of the Parquet Data Files");

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkCopyOnWriteTableArchiveWithReplace.java
Patch:
@@ -57,7 +57,7 @@ public void testDeletePartitionAndArchive(boolean metadataEnabled) throws IOExce
     HoodieWriteConfig writeConfig = getConfigBuilder(true)
         .withCleanConfig(HoodieCleanConfig.newBuilder().retainCommits(1).build())
         .withArchivalConfig(HoodieArchivalConfig.newBuilder().archiveCommitsWith(2, 3).build())
-            .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(metadataEnabled).build())
+            .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(metadataEnabled).withMaxNumDeltaCommitsBeforeCompaction(2).build())
         .build();
     try (SparkRDDWriteClient client = getHoodieWriteClient(writeConfig);
          HoodieTestDataGenerator dataGen = new HoodieTestDataGenerator(DEFAULT_PARTITION_PATHS)) {
@@ -81,7 +81,7 @@ public void testDeletePartitionAndArchive(boolean metadataEnabled) throws IOExce
       client.startCommitWithTime(instantTime4, HoodieActiveTimeline.REPLACE_COMMIT_ACTION);
       client.deletePartitions(Arrays.asList(DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH), instantTime4);
 
-      // 2nd write batch; 4 commits for the 4th partition; the 4th commit to trigger archiving the replace commit
+      // 2nd write batch; 4 commits for the 3rd partition; the 4th commit to trigger archiving the replace commit
       for (int i = 5; i < 9; i++) {
         String instantTime = HoodieActiveTimeline.createNewInstantTime(i * 1000);
         client.startCommitWithTime(instantTime);

File: hudi-common/src/main/java/org/apache/hudi/BaseHoodieTableFileIndex.java
Patch:
@@ -68,7 +68,6 @@
  * </ul>
  */
 public abstract class BaseHoodieTableFileIndex implements AutoCloseable {
-
   private static final Logger LOG = LogManager.getLogger(BaseHoodieTableFileIndex.class);
 
   private final String[] partitionColumns;

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java
Patch:
@@ -290,9 +290,7 @@ private boolean syncSchema(String tableName, boolean tableExists, boolean useRea
         // Sync the table properties if the schema has changed
         if (config.getString(HIVE_TABLE_PROPERTIES) != null || config.getBoolean(HIVE_SYNC_AS_DATA_SOURCE_TABLE)) {
           syncClient.updateTableProperties(tableName, tableProperties);
-          HoodieFileFormat baseFileFormat = HoodieFileFormat.valueOf(config.getStringOrDefault(META_SYNC_BASE_FILE_FORMAT).toUpperCase());
-          String serDeFormatClassName = HoodieInputFormatUtils.getSerDeClassName(baseFileFormat);
-          syncClient.updateTableSerDeInfo(tableName, serDeFormatClassName, serdeProperties);
+          syncClient.updateSerdeProperties(tableName, serdeProperties);
           LOG.info("Sync table properties for " + tableName + ", table properties is: " + tableProperties);
         }
         schemaChanged = true;

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
Patch:
@@ -39,6 +39,7 @@
 import org.apache.avro.Schema.Field;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.junit.jupiter.api.AfterAll;
 import org.junit.jupiter.api.AfterEach;
@@ -159,9 +160,6 @@ public void testBasicSync(boolean useSchemaFromCommitMetadata, String syncMode)
 
     assertTrue(hiveClient.tableExists(HiveTestUtil.TABLE_NAME),
         "Table " + HiveTestUtil.TABLE_NAME + " should exist after sync completes");
-    assertEquals("org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe",
-        hiveClient.getTable(HiveTestUtil.TABLE_NAME).getSd().getSerdeInfo().getSerializationLib(),
-        "SerDe info not updated or does not match");
     assertEquals(hiveClient.getMetastoreSchema(HiveTestUtil.TABLE_NAME).size(),
         hiveClient.getStorageSchema().getColumns().size() + 1,
         "Hive Schema should match the table schema + partition field");
@@ -303,6 +301,7 @@ public void testSyncCOWTableWithProperties(boolean useSchemaFromCommitMetadata,
     hiveDriver.run("SHOW CREATE TABLE " + dbTableName);
     hiveDriver.getResults(results);
     String ddl = String.join("\n", results);
+    assertTrue(ddl.contains(String.format("ROW FORMAT SERDE \n  '%s'", ParquetHiveSerDe.class.getName())));
     assertTrue(ddl.contains("'path'='" + HiveTestUtil.basePath + "'"));
     if (syncAsDataSourceTable) {
       assertTrue(ddl.contains("'" + ConfigUtils.IS_QUERY_AS_RO_TABLE + "'='false'"));
@@ -405,6 +404,7 @@ public void testSyncMORTableWithProperties(boolean useSchemaFromCommitMetadata,
       hiveDriver.run("SHOW CREATE TABLE " + dbTableName);
       hiveDriver.getResults(results);
       String ddl = String.join("\n", results);
+      assertTrue(ddl.contains(String.format("ROW FORMAT SERDE \n  '%s'", ParquetHiveSerDe.class.getName())));
       assertTrue(ddl.contains("'path'='" + HiveTestUtil.basePath + "'"));
       assertTrue(ddl.toLowerCase().contains("create external table"));
       if (syncAsDataSourceTable) {

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/HoodieMetaSyncOperations.java
Patch:
@@ -174,9 +174,10 @@ default void updateTableProperties(String tableName, Map<String, String> tablePr
   }
 
   /**
-   * Update the table SerDeInfo in metastore.
+   * Update the SerDe properties in metastore.
    */
-  default void updateTableSerDeInfo(String tableName, String serdeClass, Map<String, String> serdeProperties) {
+  default void updateSerdeProperties(String tableName, Map<String, String> serdeProperties) {
+
   }
 
   /**

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
Patch:
@@ -160,8 +160,8 @@ public void testBasicSync(boolean useSchemaFromCommitMetadata, String syncMode)
     assertTrue(hiveClient.tableExists(HiveTestUtil.TABLE_NAME),
         "Table " + HiveTestUtil.TABLE_NAME + " should exist after sync completes");
     assertEquals("org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe",
-      hiveClient.getTable(HiveTestUtil.TABLE_NAME).getSd().getSerdeInfo().getSerializationLib(),
-      "SerDe info not updated or does not match");
+        hiveClient.getTable(HiveTestUtil.TABLE_NAME).getSd().getSerdeInfo().getSerializationLib(),
+        "SerDe info not updated or does not match");
     assertEquals(hiveClient.getMetastoreSchema(HiveTestUtil.TABLE_NAME).size(),
         hiveClient.getStorageSchema().getColumns().size() + 1,
         "Hive Schema should match the table schema + partition field");

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java
Patch:
@@ -290,6 +290,9 @@ private boolean syncSchema(String tableName, boolean tableExists, boolean useRea
         // Sync the table properties if the schema has changed
         if (config.getString(HIVE_TABLE_PROPERTIES) != null || config.getBoolean(HIVE_SYNC_AS_DATA_SOURCE_TABLE)) {
           syncClient.updateTableProperties(tableName, tableProperties);
+          HoodieFileFormat baseFileFormat = HoodieFileFormat.valueOf(config.getStringOrDefault(META_SYNC_BASE_FILE_FORMAT).toUpperCase());
+          String serDeFormatClassName = HoodieInputFormatUtils.getSerDeClassName(baseFileFormat);
+          syncClient.updateTableSerDeInfo(tableName, serDeFormatClassName, serdeProperties);
           LOG.info("Sync table properties for " + tableName + ", table properties is: " + tableProperties);
         }
         schemaChanged = true;

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
Patch:
@@ -159,6 +159,9 @@ public void testBasicSync(boolean useSchemaFromCommitMetadata, String syncMode)
 
     assertTrue(hiveClient.tableExists(HiveTestUtil.TABLE_NAME),
         "Table " + HiveTestUtil.TABLE_NAME + " should exist after sync completes");
+    assertEquals("org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe",
+      hiveClient.getTable(HiveTestUtil.TABLE_NAME).getSd().getSerdeInfo().getSerializationLib(),
+      "SerDe info not updated or does not match");
     assertEquals(hiveClient.getMetastoreSchema(HiveTestUtil.TABLE_NAME).size(),
         hiveClient.getStorageSchema().getColumns().size() + 1,
         "Hive Schema should match the table schema + partition field");

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/bucket/TestHoodieSimpleBucketIndex.java
Patch:
@@ -139,7 +139,9 @@ public void testTagLocation(boolean isInsert) throws Exception {
         .filter(r -> BucketIdentifier.bucketIdFromFileId(r.getCurrentLocation().getFileId())
             != getRecordBucketId(r)).findAny().isPresent());
     assertTrue(taggedRecordRDD.collectAsList().stream().filter(r -> r.getPartitionPath().equals("2015/01/31")
-        && !r.isCurrentLocationKnown()).count() == 1L);
+            && !r.isCurrentLocationKnown()).count() == 1L);
+    assertTrue(taggedRecordRDD.collectAsList().stream().filter(r -> r.getPartitionPath().equals("2016/01/31")
+            && r.isCurrentLocationKnown()).count() == 3L);
   }
 
   private HoodieWriteConfig makeConfig() {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/callback/client/http/HoodieWriteCommitHttpCallbackClient.java
Patch:
@@ -28,8 +28,8 @@
 import org.apache.http.impl.client.HttpClientBuilder;
 import org.apache.hudi.config.HoodieWriteCommitCallbackConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
-import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.Logger;
+import org.apache.log4j.LogManager;
+import org.apache.log4j.Logger;
 
 import java.io.Closeable;
 import java.io.IOException;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/datadog/DatadogHttpClient.java
Patch:
@@ -32,8 +32,8 @@
 import org.apache.http.entity.StringEntity;
 import org.apache.http.impl.client.CloseableHttpClient;
 import org.apache.http.impl.client.HttpClientBuilder;
-import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.Logger;
+import org.apache.log4j.LogManager;
+import org.apache.log4j.Logger;
 
 import java.io.Closeable;
 import java.io.IOException;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/datadog/DatadogReporter.java
Patch:
@@ -34,8 +34,8 @@
 import com.fasterxml.jackson.databind.node.ArrayNode;
 import com.fasterxml.jackson.databind.node.ObjectNode;
 import com.fasterxml.jackson.databind.node.TextNode;
-import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.Logger;
+import org.apache.log4j.LogManager;
+import org.apache.log4j.Logger;
 
 import java.io.IOException;
 import java.util.List;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkConsistentBucketClusteringPlanStrategy.java
Patch:
@@ -43,8 +43,8 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.cluster.strategy.PartitionAwareClusteringPlanStrategy;
 
-import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.Logger;
+import org.apache.log4j.LogManager;
+import org.apache.log4j.Logger;
 import org.apache.spark.api.java.JavaRDD;
 
 import java.io.IOException;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkConsistentBucketClusteringExecutionStrategy.java
Patch:
@@ -33,8 +33,8 @@
 import org.apache.hudi.table.action.commit.SparkBulkInsertHelper;
 
 import org.apache.avro.Schema;
-import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.Logger;
+import org.apache.log4j.LogManager;
+import org.apache.log4j.Logger;
 
 import java.util.List;
 import java.util.Map;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkConsistentBucketDuplicateUpdateStrategy.java
Patch:
@@ -42,8 +42,8 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.cluster.strategy.UpdateStrategy;
 
-import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.Logger;
+import org.apache.log4j.LogManager;
+import org.apache.log4j.Logger;
 
 import java.util.Arrays;
 import java.util.Collections;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDConsistentBucketPartitioner.java
Patch:
@@ -37,8 +37,8 @@
 import org.apache.hudi.table.HoodieTable;
 
 import org.apache.avro.Schema;
-import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.Logger;
+import org.apache.log4j.LogManager;
+import org.apache.log4j.Logger;
 import org.apache.spark.Partitioner;
 import org.apache.spark.api.java.JavaRDD;
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieMergeOnReadTestUtils.java
Patch:
@@ -40,8 +40,8 @@
 import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.RecordReader;
-import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.Logger;
+import org.apache.log4j.LogManager;
+import org.apache.log4j.Logger;
 
 import java.io.IOException;
 import java.util.ArrayList;

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestTimelineUtils.java
Patch:
@@ -130,11 +130,11 @@ public void testGetPartitions() throws IOException {
     assertEquals(partitions, Arrays.asList(new String[] {"0", "2", "3", "4"}));
 
     // verify only commit actions
-    partitions = TimelineUtils.getPartitionsWritten(metaClient.getActiveTimeline().findInstantsAfter("1", 10));
+    partitions = TimelineUtils.getWrittenPartitions(metaClient.getActiveTimeline().findInstantsAfter("1", 10));
     assertEquals(4, partitions.size());
     assertEquals(partitions, Arrays.asList(new String[] {"2", "3", "4", "5"}));
 
-    partitions = TimelineUtils.getPartitionsWritten(metaClient.getActiveTimeline().findInstantsInRange("1", "4"));
+    partitions = TimelineUtils.getWrittenPartitions(metaClient.getActiveTimeline().findInstantsInRange("1", "4"));
     assertEquals(3, partitions.size());
     assertEquals(partitions, Arrays.asList(new String[] {"2", "3", "4"}));
   }

File: hudi-sync/hudi-adb-sync/src/main/java/org/apache/hudi/sync/adb/AdbSyncTool.java
Patch:
@@ -194,7 +194,7 @@ private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat, b
     if (config.getSplitStrings(META_SYNC_PARTITION_FIELDS).isEmpty()) {
       writtenPartitionsSince = new ArrayList<>();
     } else {
-      writtenPartitionsSince = syncClient.getPartitionsWrittenToSince(lastCommitTimeSynced);
+      writtenPartitionsSince = syncClient.getWrittenPartitionsSince(lastCommitTimeSynced);
     }
     LOG.info("Scan partitions complete, partitionNum:{}", writtenPartitionsSince.size());
 

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java
Patch:
@@ -493,7 +493,7 @@ public static void createCommitFile(HoodieCommitMetadata commitMetadata, String
     fsout.close();
   }
 
-  public static void createReplaceCommitFile(HoodieCommitMetadata commitMetadata, String instantTime) throws IOException {
+  public static void createReplaceCommitFile(HoodieReplaceCommitMetadata commitMetadata, String instantTime) throws IOException {
     byte[] bytes = commitMetadata.toJsonString().getBytes(StandardCharsets.UTF_8);
     Path fullPath = new Path(basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/"
         + HoodieTimeline.makeReplaceFileName(instantTime));

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -60,7 +60,8 @@
 import org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer;
 
 import org.apache.hadoop.fs.Path;
-import org.apache.log4j.Logger;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
@@ -86,7 +87,7 @@
  */
 public class SparkMain {
 
-  private static final Logger LOG = Logger.getLogger(SparkMain.class);
+  private static final Logger LOG = LogManager.getLogger(SparkMain.class);
 
   /**
    * Commands.

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java
Patch:
@@ -508,7 +508,7 @@ private void testIncrementalSparkSQLQuery() throws Exception {
   }
 
   private void scheduleAndRunCompaction() throws Exception {
-    executeCommandStringInDocker(ADHOC_1_CONTAINER, HUDI_CLI_TOOL + " --cmdfile " + COMPACTION_COMMANDS, true);
-    executeCommandStringInDocker(ADHOC_1_CONTAINER, HUDI_CLI_TOOL + " --cmdfile " + COMPACTION_BOOTSTRAP_COMMANDS, true);
+    executeCommandStringInDocker(ADHOC_1_CONTAINER, HUDI_CLI_TOOL + " script --file " + COMPACTION_COMMANDS, true);
+    executeCommandStringInDocker(ADHOC_1_CONTAINER, HUDI_CLI_TOOL + " script --file " + COMPACTION_BOOTSTRAP_COMMANDS, true);
   }
 }

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/command/ITTestHoodieSyncCommand.java
Patch:
@@ -49,11 +49,11 @@ public void testValidateSync() throws Exception {
         hiveTableName, HoodieTableType.COPY_ON_WRITE.name(), PartitionType.SINGLE_KEY_PARTITIONED, "append", hiveTableName);
 
     TestExecStartResultCallback result =
-        executeCommandStringInDocker(ADHOC_1_CONTAINER, HUDI_CLI_TOOL + " --cmdfile " + SYNC_VALIDATE_COMMANDS, true);
+        executeCommandStringInDocker(ADHOC_1_CONTAINER, HUDI_CLI_TOOL + " script --file " + SYNC_VALIDATE_COMMANDS, true);
 
     String expected = String.format("Count difference now is (count(%s) - count(%s) == %d. Catch up count is %d",
         hiveTableName, hiveTableName2, 100, 200);
-    assertTrue(result.getStderr().toString().contains(expected));
+    assertTrue(result.getStdout().toString().contains(expected));
 
     dropHiveTables(hiveTableName, HoodieTableType.COPY_ON_WRITE.name());
     dropHiveTables(hiveTableName2, HoodieTableType.COPY_ON_WRITE.name());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGenUtils.java
Patch:
@@ -74,7 +74,7 @@ public static String getPartitionPathFromGenericRecord(GenericRecord genericReco
   public static String[] extractRecordKeys(String recordKey) {
     String[] fieldKV = recordKey.split(",");
     return Arrays.stream(fieldKV).map(kv -> {
-      final String[] kvArray = kv.split(":");
+      final String[] kvArray = kv.split(":", 2);
       if (kvArray.length == 1) {
         return kvArray[0];
       } else if (kvArray[1].equals(NULL_RECORDKEY_PLACEHOLDER)) {

File: hudi-common/src/main/java/org/apache/hudi/common/model/OverwriteNonDefaultsWithLatestAvroPayload.java
Patch:
@@ -70,7 +70,7 @@ public Option<IndexedRecord> combineAndGetUpdateValue(IndexedRecord currentValue
         if (!overwriteField(value, defaultValue)) {
           builder.set(field, value);
         } else {
-          builder.set(field, currentRecord.get(field.pos()));
+          builder.set(field, currentRecord.get(field.name()));
         }
       });
       return Option.of(builder.build());

File: hudi-common/src/main/java/org/apache/hudi/internal/schema/action/InternalSchemaChangeApplier.java
Patch:
@@ -51,7 +51,8 @@ public InternalSchema applyAddChange(
       TableChange.ColumnPositionChange.ColumnPositionType positionType) {
     TableChanges.ColumnAddChange add = TableChanges.ColumnAddChange.get(latestSchema);
     String parentName = TableChangesHelper.getParentName(colName);
-    add.addColumns(parentName, colName, colType, doc);
+    String leafName = TableChangesHelper.getLeafName(colName);
+    add.addColumns(parentName, leafName, colType, doc);
     if (positionType != null) {
       switch (positionType) {
         case NO_OPERATION:

File: hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/bootstrap/SparkFullBootstrapDataProviderBase.java
Patch:
@@ -60,7 +60,9 @@ public JavaRDD<HoodieRecord> generateInputRecords(String tableName, String sourc
         .flatMap(f -> f.stream().map(fs -> FileStatusUtils.toPath(fs.getPath()).toString()))
         .toArray(String[]::new);
 
-    Dataset inputDataset = sparkSession.read().format(getFormat()).load(filePaths);
+    // NOTE: "basePath" option is required for spark to discover the partition column
+    // More details at https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery
+    Dataset inputDataset = sparkSession.read().format(getFormat()).option("basePath", sourceBasePath).load(filePaths);
     try {
       KeyGenerator keyGenerator = HoodieSparkKeyGeneratorFactory.createKeyGenerator(props);
       String structName = tableName + "_record";

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java
Patch:
@@ -345,7 +345,7 @@ private HoodieData<BootstrapWriteStatus> runMetadataBootstrap(List<Pair<String,
         })
         .collect(Collectors.toList());
 
-    context.setJobStatus(this.getClass().getSimpleName(), "Bootstrap metadata table: " + config.getTableName());
+    context.setJobStatus(this.getClass().getSimpleName(), "Run metadata-only bootstrap operation: " + config.getTableName());
     return context.parallelize(bootstrapPaths, config.getBootstrapParallelism())
         .map(partitionFsPair -> getMetadataHandler(config, table, partitionFsPair.getRight().getRight()).runMetadataBootstrap(partitionFsPair.getLeft(),
                 partitionFsPair.getRight().getLeft(), keyGenerator));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java
Patch:
@@ -167,6 +167,7 @@ List<HoodieCleanStat> clean(HoodieEngineContext context, HoodieCleanerPlan clean
                   ? new HoodieInstant(HoodieInstant.State.valueOf(actionInstant.getState()),
                   actionInstant.getAction(), actionInstant.getTimestamp())
                   : null))
+          .withLastCompletedCommitTimestamp(cleanerPlan.getLastCompletedCommitTimestamp())
           .withDeletePathPattern(partitionCleanStat.deletePathPatterns())
           .withSuccessfulDeletes(partitionCleanStat.successDeleteFiles())
           .withFailedDeletes(partitionCleanStat.failedDeleteFiles())

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanMetadataV1MigrationHandler.java
Patch:
@@ -83,6 +83,7 @@ public HoodieCleanMetadata downgradeFrom(HoodieCleanMetadata input) {
 
     return HoodieCleanMetadata.newBuilder()
         .setEarliestCommitToRetain(input.getEarliestCommitToRetain())
+        .setLastCompletedCommitTimestamp(input.getLastCompletedCommitTimestamp())
         .setStartCleanTime(input.getStartCleanTime())
         .setTimeTakenInMillis(input.getTimeTakenInMillis())
         .setTotalFilesDeleted(input.getTotalFilesDeleted())

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanPlanV2MigrationHandler.java
Patch:
@@ -53,8 +53,8 @@ public HoodieCleanerPlan upgradeFrom(HoodieCleanerPlan plan) {
           .map(v -> new HoodieCleanFileInfo(
             new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), e.getKey()), v).toString(), false))
           .collect(Collectors.toList()))).collect(Collectors.toMap(Pair::getKey, Pair::getValue));
-    return new HoodieCleanerPlan(plan.getEarliestInstantToRetain(), plan.getPolicy(), new HashMap<>(), VERSION,
-        filePathsPerPartition, new ArrayList<>());
+    return new HoodieCleanerPlan(plan.getEarliestInstantToRetain(), plan.getLastCompletedCommitTimestamp(),
+        plan.getPolicy(), new HashMap<>(), VERSION, filePathsPerPartition, new ArrayList<>());
   }
 
   @Override

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestTimelineUtils.java
Patch:
@@ -140,7 +140,7 @@ public void testGetPartitions() throws IOException {
   }
 
   @Test
-  public void testGetPartitionsUnpartitioned() throws IOException {
+  public void testGetPartitionsUnPartitioned() throws IOException {
     HoodieActiveTimeline activeTimeline = metaClient.getActiveTimeline();
     HoodieTimeline activeCommitTimeline = activeTimeline.getCommitTimeline();
     assertTrue(activeCommitTimeline.empty());
@@ -217,7 +217,7 @@ public void testGetExtraMetadata() throws Exception {
     verifyExtraMetadataLatestValue(extraMetadataKey, extraMetadataValue1, false);
     assertFalse(TimelineUtils.getExtraMetadataFromLatest(metaClient, "unknownKey").isPresent());
     
-    // verify adding clustering commit doesnt change behavior of getExtraMetadataFromLatest
+    // verify adding clustering commit doesn't change behavior of getExtraMetadataFromLatest
     String ts2 = "2";
     HoodieInstant instant2 = new HoodieInstant(true, HoodieTimeline.REPLACE_COMMIT_ACTION, ts2);
     activeTimeline.createNewInstant(instant2);
@@ -338,6 +338,7 @@ private Option<byte[]> getCleanMetadata(String partition, String time) throws IO
         .setTotalFilesDeleted(1)
         .setStartCleanTime(time)
         .setEarliestCommitToRetain(time)
+        .setLastCompletedCommitTimestamp("")
         .setPartitionMetadata(partitionToFilesCleaned).build();
 
     return TimelineMetadataUtils.serializeCleanMetadata(cleanMetadata);

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java
Patch:
@@ -536,7 +536,7 @@ private void performClean(String instant, List<String> files, String cleanInstan
     Map<String, List<String>> partititonToFiles = deleteFiles(files);
     List<HoodieCleanStat> cleanStats = partititonToFiles.entrySet().stream().map(e ->
         new HoodieCleanStat(HoodieCleaningPolicy.KEEP_LATEST_COMMITS, e.getKey(), e.getValue(), e.getValue(),
-        new ArrayList<>(), Integer.toString(Integer.parseInt(instant) + 1))).collect(Collectors.toList());
+        new ArrayList<>(), Integer.toString(Integer.parseInt(instant) + 1), "")).collect(Collectors.toList());
 
     HoodieInstant cleanInflightInstant = new HoodieInstant(true, HoodieTimeline.CLEAN_ACTION, cleanInstant);
     metaClient.getActiveTimeline().createNewInstant(cleanInflightInstant);
@@ -860,7 +860,7 @@ private List<String> addReplaceInstant(HoodieTableMetaClient metaClient, String
                                  List<Pair<String, HoodieWriteStat>> writeStats,
                                  Map<String, List<String>> partitionToReplaceFileIds) throws IOException {
     // created requested
-    HoodieInstant newRequestedInstant = new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.REPLACE_COMMIT_ACTION, instant);
+    HoodieInstant newRequestedInstant = new HoodieInstant(State.REQUESTED, HoodieTimeline.REPLACE_COMMIT_ACTION, instant);
     HoodieRequestedReplaceMetadata requestedReplaceMetadata = HoodieRequestedReplaceMetadata.newBuilder()
         .setOperationType(WriteOperationType.UNKNOWN.name()).build();
     metaClient.getActiveTimeline().saveToPendingReplaceCommit(newRequestedInstant,

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -1006,21 +1006,21 @@ protected void compactIfNecessary(BaseHoodieWriteClient writeClient, String inst
     // finish off any pending compactions if any from previous attempt.
     writeClient.runAnyPendingCompactions();
 
-    String latestDeltacommitTime = metadataMetaClient.reloadActiveTimeline().getDeltaCommitTimeline().filterCompletedInstants().lastInstant()
+    String latestDeltaCommitTime = metadataMetaClient.reloadActiveTimeline().getDeltaCommitTimeline().filterCompletedInstants().lastInstant()
         .get().getTimestamp();
     List<HoodieInstant> pendingInstants = dataMetaClient.reloadActiveTimeline().filterInflightsAndRequested()
         .findInstantsBefore(instantTime).getInstants().collect(Collectors.toList());
 
     if (!pendingInstants.isEmpty()) {
       LOG.info(String.format("Cannot compact metadata table as there are %d inflight instants before latest deltacommit %s: %s",
-          pendingInstants.size(), latestDeltacommitTime, Arrays.toString(pendingInstants.toArray())));
+          pendingInstants.size(), latestDeltaCommitTime, Arrays.toString(pendingInstants.toArray())));
       return;
     }
 
     // Trigger compaction with suffixes based on the same instant time. This ensures that any future
     // delta commits synced over will not have an instant time lesser than the last completed instant on the
     // metadata table.
-    final String compactionInstantTime = latestDeltacommitTime + "001";
+    final String compactionInstantTime = latestDeltaCommitTime + "001";
     if (writeClient.scheduleCompactionAtInstant(compactionInstantTime, Option.empty())) {
       writeClient.compact(compactionInstantTime);
     }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java
Patch:
@@ -77,7 +77,6 @@ public abstract class BuiltinKeyGenerator extends BaseKeyGenerator implements Sp
   protected static final UTF8String NULL_RECORD_KEY_PLACEHOLDER_UTF8 = UTF8String.fromString(NULL_RECORDKEY_PLACEHOLDER);
   protected static final UTF8String EMPTY_RECORD_KEY_PLACEHOLDER_UTF8 = UTF8String.fromString(EMPTY_RECORDKEY_PLACEHOLDER);
 
-
   protected transient volatile SparkRowConverter rowConverter;
   protected transient volatile SparkRowAccessor rowAccessor;
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestConsistentBucketIndex.java
Patch:
@@ -97,7 +97,7 @@ private void setUp(boolean populateMetaFields, boolean partitioned) throws Excep
       initTestDataGenerator(new String[] {""});
     }
     initFileSystem();
-    Properties props = populateMetaFields ? new Properties() : getPropertiesForKeyGen();
+    Properties props = getPropertiesForKeyGen(populateMetaFields);
     props.setProperty(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key(), "_row_key");
     metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ, props);
     config = getConfigBuilder()

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableIncrementalRead.java
Patch:
@@ -82,7 +82,7 @@ void setUp() {
   public void testIncrementalReadsWithCompaction() throws Exception {
     final String partitionPath = "2020/02/20"; // use only one partition for this test
     final HoodieTestDataGenerator dataGen = new HoodieTestDataGenerator(new String[] { partitionPath });
-    Properties props = new Properties();
+    Properties props = getPropertiesForKeyGen(true);
     props.setProperty(HoodieTableConfig.BASE_FILE_FORMAT.key(), HoodieFileFormat.PARQUET.toString());
     HoodieTableMetaClient metaClient = getHoodieMetaClient(HoodieTableType.MERGE_ON_READ, props);
     HoodieWriteConfig cfg = getConfigBuilder(true).build();

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -399,7 +399,7 @@ public HoodieArchivedTimeline getArchivedTimeline(String startTs) {
   public void validateTableProperties(Properties properties) {
     // Once meta fields are disabled, it cant be re-enabled for a given table.
     if (!getTableConfig().populateMetaFields()
-        && Boolean.parseBoolean((String) properties.getOrDefault(HoodieTableConfig.POPULATE_META_FIELDS.key(), HoodieTableConfig.POPULATE_META_FIELDS.defaultValue()))) {
+        && Boolean.parseBoolean((String) properties.getOrDefault(HoodieTableConfig.POPULATE_META_FIELDS.key(), HoodieTableConfig.POPULATE_META_FIELDS.defaultValue().toString()))) {
       throw new HoodieException(HoodieTableConfig.POPULATE_META_FIELDS.key() + " already disabled for the table. Can't be re-enabled back");
     }
 

File: hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileUtils.java
Patch:
@@ -67,7 +67,9 @@ public static HFile.Reader createHFileReader(
    */
   public static HFile.Reader createHFileReader(
       FileSystem fs, Path dummyPath, byte[] content) throws IOException {
-    Configuration conf = new Configuration();
+    // Avoid loading default configs, from the FS, since this configuration is mostly
+    // used as a stub to initialize HFile reader
+    Configuration conf = new Configuration(false);
     HoodieHFileReader.SeekableByteArrayInputStream bis = new HoodieHFileReader.SeekableByteArrayInputStream(content);
     FSDataInputStream fsdis = new FSDataInputStream(bis);
     FSDataInputStreamWrapper stream = new FSDataInputStreamWrapper(fsdis);

File: hudi-spark-datasource/hudi-spark3-common/src/main/java/org/apache/hudi/spark3/internal/ReflectUtil.java
Patch:
@@ -53,12 +53,12 @@ public static DateFormatter getDateFormatter(ZoneId zoneId) {
     try {
       ClassLoader loader = Thread.currentThread().getContextClassLoader();
       if (HoodieSparkUtils.gteqSpark3_2()) {
-        Class clazz = loader.loadClass(DateFormatter.class.getName());
+        Class<?> clazz = loader.loadClass(DateFormatter.class.getName());
         Method applyMethod = clazz.getDeclaredMethod("apply");
         applyMethod.setAccessible(true);
         return (DateFormatter)applyMethod.invoke(null);
       } else {
-        Class clazz = loader.loadClass(DateFormatter.class.getName());
+        Class<?> clazz = loader.loadClass(DateFormatter.class.getName());
         Method applyMethod = clazz.getDeclaredMethod("apply", ZoneId.class);
         applyMethod.setAccessible(true);
         return (DateFormatter)applyMethod.invoke(null, zoneId);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestHoodieIncrSource.java
Patch:
@@ -143,9 +143,9 @@ private void readAndAssert(IncrSourceHelper.MissingCheckpointStrategy missingChe
     if (expectedCount == 0) {
       assertFalse(batchCheckPoint.getKey().isPresent());
     } else {
-      assertEquals(batchCheckPoint.getKey().get().count(), expectedCount);
+      assertEquals(expectedCount, batchCheckPoint.getKey().get().count());
     }
-    Assertions.assertEquals(batchCheckPoint.getRight(), expectedCheckpoint);
+    Assertions.assertEquals(expectedCheckpoint, batchCheckPoint.getRight());
   }
 
   private Pair<String, List<HoodieRecord>> writeRecords(SparkRDDWriteClient writeClient, boolean insert, List<HoodieRecord> insertRecords, String commit) throws IOException {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/FlinkTaskContextSupplier.java
Patch:
@@ -61,5 +61,5 @@ public Option<String> getProperty(EngineProperty prop) {
     // no operation for now
     return Option.empty();
   }
-  
+
 }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/MiniBatchHandle.java
Patch:
@@ -30,7 +30,8 @@ public interface MiniBatchHandle {
    * come from one checkpoint interval. The file handle may roll over to new name
    * if the name conflicts, give a chance to clean the intermediate file.
    */
-  default void finalizeWrite() {}
+  default void finalizeWrite() {
+  }
 
   /**
    * Close the file handle gracefully, if any error happens during the file handle close,

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataParquetWriter.java
Patch:
@@ -20,10 +20,10 @@
 
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.fs.HoodieWrapperFileSystem;
+import org.apache.hudi.io.storage.HoodieParquetConfig;
 
 import org.apache.flink.table.data.RowData;
 import org.apache.hadoop.fs.Path;
-import org.apache.hudi.io.storage.HoodieParquetConfig;
 import org.apache.parquet.hadoop.ParquetFileWriter;
 import org.apache.parquet.hadoop.ParquetWriter;
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java
Patch:
@@ -31,7 +31,6 @@
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.MapType;
 import org.apache.flink.table.types.logical.RowType;
-
 import org.apache.flink.table.types.logical.TimestampType;
 import org.apache.parquet.schema.GroupType;
 import org.apache.parquet.schema.LogicalTypeAnnotation;
@@ -76,7 +75,7 @@ public static TypeInformation<?> fromParquetType(MessageType type) {
    * Converts Flink Internal Type to Parquet schema.
    *
    * @param typeInformation Flink type information
-   * @param legacyMode is standard LIST and MAP schema or back-compatible schema
+   * @param legacyMode      is standard LIST and MAP schema or back-compatible schema
    * @return Parquet schema
    */
   public static MessageType toParquetType(
@@ -569,7 +568,7 @@ private static Type convertToParquetType(
         int scale = ((DecimalType) type).getScale();
         int numBytes = computeMinBytesForDecimalPrecision(precision);
         return Types.primitive(
-            PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, repetition)
+                PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, repetition)
             .as(LogicalTypeAnnotation.decimalType(scale, precision))
             .length(numBytes)
             .named(name);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -42,6 +42,9 @@
 import java.util.List;
 import java.util.Map;
 
+/**
+ * Flink hoodie backed table metadata writer.
+ */
 public class FlinkHoodieBackedTableMetadataWriter extends HoodieBackedTableMetadataWriter {
 
   private static final Logger LOG = LogManager.getLogger(FlinkHoodieBackedTableMetadataWriter.class);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java
Patch:
@@ -45,6 +45,9 @@
 import java.util.List;
 import java.util.Map;
 
+/**
+ * Flink MERGE_ON_READ table.
+ */
 public class HoodieFlinkMergeOnReadTable<T extends HoodieRecordPayload>
     extends HoodieFlinkCopyOnWriteTable<T> {
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkTable.java
Patch:
@@ -40,6 +40,9 @@
 
 import java.util.List;
 
+/**
+ * Impl of a flink hoodie table.
+ */
 public abstract class HoodieFlinkTable<T extends HoodieRecordPayload>
     extends HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>>
     implements ExplicitWriteHandleTable<T> {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeleteCommitActionExecutor.java
Patch:
@@ -30,6 +30,9 @@
 
 import java.util.List;
 
+/**
+ * Flink delete commit action executor.
+ */
 public class FlinkDeleteCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseFlinkCommitActionExecutor<T> {
   private final List<HoodieKey> keys;
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeleteHelper.java
Patch:
@@ -42,6 +42,9 @@
 import java.util.List;
 import java.util.stream.Collectors;
 
+/**
+ * Flink delete helper.
+ */
 @SuppressWarnings("checkstyle:LineLength")
 public class FlinkDeleteHelper<R> extends
     BaseDeleteHelper<EmptyHoodieRecordPayload, List<HoodieRecord<EmptyHoodieRecordPayload>>, List<HoodieKey>, List<WriteStatus>, R> {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertCommitActionExecutor.java
Patch:
@@ -30,6 +30,9 @@
 
 import java.util.List;
 
+/**
+ * Flink insert commit action executor.
+ */
 public class FlinkInsertCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseFlinkCommitActionExecutor<T> {
 
   private List<HoodieRecord<T>> inputRecords;

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertOverwriteCommitActionExecutor.java
Patch:
@@ -31,6 +31,9 @@
 
 import java.util.List;
 
+/**
+ * Flink INSERT OVERWRITE commit action executor.
+ */
 public class FlinkInsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>
     extends BaseFlinkCommitActionExecutor<T> {
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertOverwriteTableCommitActionExecutor.java
Patch:
@@ -30,6 +30,9 @@
 
 import java.util.List;
 
+/**
+ * Flink INSERT OVERWRITE TABLE commit action executor.
+ */
 public class FlinkInsertOverwriteTableCommitActionExecutor<T extends HoodieRecordPayload<T>>
     extends FlinkInsertOverwriteCommitActionExecutor<T> {
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertPreppedCommitActionExecutor.java
Patch:
@@ -30,6 +30,9 @@
 
 import java.util.List;
 
+/**
+ * Flink insert prepped commit action executor.
+ */
 public class FlinkInsertPreppedCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseFlinkCommitActionExecutor<T> {
 
   private final List<HoodieRecord<T>> preppedRecords;

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkMergeHelper.java
Patch:
@@ -44,6 +44,9 @@
 import java.util.Iterator;
 import java.util.List;
 
+/**
+ * Flink merge helper.
+ */
 public class FlinkMergeHelper<T extends HoodieRecordPayload> extends BaseMergeHelper<T, List<HoodieRecord<T>>,
     List<HoodieKey>, List<WriteStatus>> {
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkUpsertCommitActionExecutor.java
Patch:
@@ -30,6 +30,9 @@
 
 import java.util.List;
 
+/**
+ * Flink upsert commit action executor.
+ */
 public class FlinkUpsertCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseFlinkCommitActionExecutor<T> {
 
   private List<HoodieRecord<T>> inputRecords;

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkUpsertPreppedCommitActionExecutor.java
Patch:
@@ -30,6 +30,9 @@
 
 import java.util.List;
 
+/**
+ * Flink upsert prepped commit action executor.
+ */
 public class FlinkUpsertPreppedCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseFlinkCommitActionExecutor<T> {
 
   private final List<HoodieRecord<T>> preppedRecords;

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/delta/BaseFlinkDeltaCommitActionExecutor.java
Patch:
@@ -34,6 +34,9 @@
 import java.util.Iterator;
 import java.util.List;
 
+/**
+ * Base flink delta commit action executor.
+ */
 public abstract class BaseFlinkDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
     extends BaseFlinkCommitActionExecutor<T> {
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/delta/FlinkUpsertDeltaCommitActionExecutor.java
Patch:
@@ -30,6 +30,9 @@
 
 import java.util.List;
 
+/**
+ * Flink upsert delta commit action executor.
+ */
 public class FlinkUpsertDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
     extends BaseFlinkDeltaCommitActionExecutor<T> {
   private final List<HoodieRecord<T>> inputRecords;

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/delta/FlinkUpsertPreppedDeltaCommitActionExecutor.java
Patch:
@@ -30,6 +30,9 @@
 
 import java.util.List;
 
+/**
+ * Flink upsert prepped delta commit action executor.
+ */
 public class FlinkUpsertPreppedDeltaCommitActionExecutor<T extends HoodieRecordPayload<T>>
     extends BaseFlinkDeltaCommitActionExecutor<T> {
 

File: hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/client/common/TestHoodieFlinkEngineContext.java
Patch:
@@ -19,8 +19,8 @@
 package org.apache.hudi.client.common;
 
 import org.apache.hudi.client.FlinkTaskContextSupplier;
-
 import org.apache.hudi.common.util.collection.ImmutablePair;
+
 import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -580,7 +580,7 @@ private FlinkOptions() {
       .stringType()
       .defaultValue(HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name())
       .withDescription("Clean policy to manage the Hudi table. Available option: KEEP_LATEST_COMMITS, KEEP_LATEST_FILE_VERSIONS, KEEP_LATEST_BY_HOURS."
-          +  "Default is KEEP_LATEST_COMMITS.");
+          + "Default is KEEP_LATEST_COMMITS.");
 
   public static final ConfigOption<Integer> CLEAN_RETAIN_COMMITS = ConfigOptions
       .key("clean.retain_commits")

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/HadoopConfigurations.java
Patch:
@@ -30,7 +30,7 @@
  */
 public class HadoopConfigurations {
   private static final String HADOOP_PREFIX = "hadoop.";
-  private static final  String PARQUET_PREFIX = "parquet.";
+  private static final String PARQUET_PREFIX = "parquet.";
 
   /**
    * Creates a merged hadoop configuration with given flink configuration and hadoop configuration.

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/HoodieFlinkClusteringJob.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.sink.clustering;
 
-import org.apache.flink.client.deployment.application.ApplicationExecutionException;
 import org.apache.hudi.async.HoodieAsyncTableService;
 import org.apache.hudi.avro.model.HoodieClusteringPlan;
 import org.apache.hudi.client.HoodieFlinkWriteClient;
@@ -42,6 +41,7 @@
 import org.apache.avro.Schema;
 import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.client.deployment.application.ApplicationExecutionException;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/compact/CompactionPlanOperator.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.sink.compact;
 
-import org.apache.flink.streaming.api.operators.BoundedOneInput;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.common.model.CompactionOperation;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
@@ -33,6 +32,7 @@
 import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.streaming.api.operators.AbstractStreamOperator;
+import org.apache.flink.streaming.api.operators.BoundedOneInput;
 import org.apache.flink.streaming.api.operators.OneInputStreamOperator;
 import org.apache.flink.streaming.api.operators.Output;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/compact/HoodieFlinkCompactor.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.sink.compact;
 
-import org.apache.flink.client.deployment.application.ApplicationExecutionException;
 import org.apache.hudi.async.HoodieAsyncTableService;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.client.HoodieFlinkWriteClient;
@@ -38,6 +37,7 @@
 import com.beust.jcommander.JCommander;
 import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.client.deployment.application.ApplicationExecutionException;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.streaming.api.operators.ProcessOperator;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/stats/ExpressionEvaluator.java
Patch:
@@ -379,6 +379,7 @@ public void bindVals(Object... vals) {
   }
 
   // component predicate
+
   /**
    * To evaluate NOT expr.
    */

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/streamer/FlinkStreamerConfig.java
Patch:
@@ -59,7 +59,7 @@ public class FlinkStreamerConfig extends Configuration {
   public String flinkCheckPointPath;
 
   @Parameter(names = {"--flink-state-backend-type"}, description = "Flink state backend type, support only hashmap and rocksdb by now,"
-          + " default hashmap.", converter = FlinkStateBackendConverter.class)
+      + " default hashmap.", converter = FlinkStateBackendConverter.class)
   public StateBackend stateBackend = new HashMapStateBackend();
 
   @Parameter(names = {"--instant-retry-times"}, description = "Times to retry when latest instant has not completed.")
@@ -264,7 +264,7 @@ public class FlinkStreamerConfig extends Configuration {
 
   @Parameter(names = {"--clean-policy"},
       description = "Clean policy to manage the Hudi table. Available option: KEEP_LATEST_COMMITS, KEEP_LATEST_FILE_VERSIONS, KEEP_LATEST_BY_HOURS."
-          +  "Default is KEEP_LATEST_COMMITS.")
+          + "Default is KEEP_LATEST_COMMITS.")
   public String cleanPolicy = HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name();
 
   @Parameter(names = {"--clean-retain-commits"},

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java
Patch:
@@ -404,7 +404,7 @@ private List<MergeOnReadInputSplit> buildFileIndex() {
   /**
    * Returns whether the hoodie table data exists .
    */
-  private  boolean tableDataExists() {
+  private boolean tableDataExists() {
     HoodieActiveTimeline activeTimeline = metaClient.getActiveTimeline();
     Option<Pair<HoodieInstant, HoodieCommitMetadata>> instantAndCommitMetadata = activeTimeline.getLastCommitMetadataWithValidData();
     return instantAndCommitMetadata.isPresent();

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java
Patch:
@@ -443,7 +443,7 @@ public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ig
       //create hive table
       client.createTable(hiveTable);
       //init hoodie metaClient
-      initTableIfNotExists(tablePath, (CatalogTable)table);
+      initTableIfNotExists(tablePath, (CatalogTable) table);
     } catch (AlreadyExistsException e) {
       if (!ignoreIfExists) {
         throw new TableAlreadyExistException(getName(), tablePath, e);

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/AvroSchemaConverter.java
Patch:
@@ -109,7 +109,7 @@ public static DataType convertToDataType(Schema schema) {
 
           if (recordTypesOfSameNumFields(nonNullTypes)) {
             DataType converted = DataTypes.ROW(
-                DataTypes.FIELD("wrapper", rawDataType))
+                    DataTypes.FIELD("wrapper", rawDataType))
                 .notNull();
             return nullable ? converted.nullable() : converted;
           }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/ClusteringUtil.java
Patch:
@@ -62,7 +62,7 @@ public static void scheduleClustering(Configuration conf, HoodieFlinkWriteClient
   /**
    * Force rolls back all the inflight clustering instants, especially for job failover restart.
    *
-   * @param table The hoodie table
+   * @param table       The hoodie table
    * @param writeClient The write client
    */
   public static void rollbackClustering(HoodieFlinkTable<?> table, HoodieFlinkWriteClient writeClient) {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/CompactionUtil.java
Patch:
@@ -122,7 +122,7 @@ public static void setAvroSchema(HoodieWriteConfig writeConfig, HoodieTableMetaC
   /**
    * Sets up the preCombine field into the given configuration {@code conf}
    * through reading from the hoodie table metadata.
-   *
+   * <p>
    * This value is non-null as compaction can only be performed on MOR tables.
    * Of which, MOR tables will have non-null precombine fields.
    *

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/DataTypeUtils.java
Patch:
@@ -73,7 +73,7 @@ public static boolean isDatetimeType(DataType type) {
    * Projects the row fields with given names.
    */
   public static RowType.RowField[] projectRowFields(RowType rowType, String[] names) {
-    int [] fieldIndices = Arrays.stream(names).mapToInt(rowType::getFieldIndex).toArray();
+    int[] fieldIndices = Arrays.stream(names).mapToInt(rowType::getFieldIndex).toArray();
     return Arrays.stream(fieldIndices).mapToObj(i -> rowType.getFields().get(i)).toArray(RowType.RowField[]::new);
   }
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java
Patch:
@@ -45,10 +45,10 @@
 import org.apache.hudi.config.HoodieClusteringConfig;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieLockConfig;
-import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.config.HoodieMemoryConfig;
-import org.apache.hudi.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodiePayloadConfig;
+import org.apache.hudi.config.HoodieStorageConfig;
+import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.configuration.FlinkOptions;
 import org.apache.hudi.configuration.HadoopConfigurations;
 import org.apache.hudi.configuration.OptionsResolver;

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/ITTestDataStreamWrite.java
Patch:
@@ -104,7 +104,7 @@ public void testWriteCopyOnWrite(String indexType) throws Exception {
     conf.setString(FlinkOptions.INDEX_TYPE, indexType);
     conf.setInteger(FlinkOptions.BUCKET_INDEX_NUM_BUCKETS, 1);
     conf.setString(FlinkOptions.INDEX_KEY_FIELD, "id");
-    conf.setBoolean(FlinkOptions.PRE_COMBINE,true);
+    conf.setBoolean(FlinkOptions.PRE_COMBINE, true);
 
     testWriteToHoodie(conf, "cow_write", 2, EXPECTED);
   }
@@ -372,7 +372,7 @@ public void testHoodiePipelineBuilderSink() throws Exception {
     // Read from file source
     RowType rowType =
         (RowType) AvroSchemaConverter.convertToDataType(StreamerUtil.getSourceSchema(conf))
-        .getLogicalType();
+            .getLogicalType();
 
     JsonRowDataDeserializationSchema deserializationSchema = new JsonRowDataDeserializationSchema(
         rowType,

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/compact/ITTestHoodieFlinkCompactor.java
Patch:
@@ -294,7 +294,7 @@ public void testCompactionInBatchExecutionMode() throws Exception {
     EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();
     TableEnvironment tableEnv = TableEnvironmentImpl.create(settings);
     tableEnv.getConfig().getConfiguration()
-            .setInteger(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 4);
+        .setInteger(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 4);
     Map<String, String> options = new HashMap<>();
     options.put(FlinkOptions.COMPACTION_DELTA_COMMITS.key(), "2");
     options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/source/stats/TestExpressionEvaluator.java
Patch:
@@ -54,7 +54,7 @@ public class TestExpressionEvaluator {
       DataTypes.FIELD("f_time", DataTypes.TIME(3)),
       DataTypes.FIELD("f_date", DataTypes.DATE()),
       DataTypes.FIELD("f_timestamp", DataTypes.TIMESTAMP(3))
-      ).notNull();
+  ).notNull();
   private static final DataType INDEX_ROW_DATA_TYPE = DataTypes.ROW(
       DataTypes.FIELD("file_name", DataTypes.STRING()),
       DataTypes.FIELD("value_cnt", DataTypes.BIGINT()),

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/ITTestHoodieDataSource.java
Patch:
@@ -23,8 +23,8 @@
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.configuration.FlinkOptions;
-import org.apache.hudi.table.catalog.HoodieHiveCatalog;
 import org.apache.hudi.table.catalog.HoodieCatalogTestUtils;
+import org.apache.hudi.table.catalog.HoodieHiveCatalog;
 import org.apache.hudi.util.StreamerUtil;
 import org.apache.hudi.utils.TestConfigurations;
 import org.apache.hudi.utils.TestData;
@@ -1348,7 +1348,7 @@ void testWriteAndReadWithDataSkipping(HoodieTableType tableType) {
         .option(FlinkOptions.METADATA_ENABLED, true)
         .option("hoodie.metadata.index.column.stats.enable", true)
         .option(FlinkOptions.READ_DATA_SKIPPING_ENABLED, true)
-        .option(FlinkOptions.TABLE_TYPE,tableType)
+        .option(FlinkOptions.TABLE_TYPE, tableType)
         .end();
     tableEnv.executeSql(hoodieTableDDL);
 

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/catalog/TestHoodieCatalogFactory.java
Patch:
@@ -87,7 +87,7 @@ public void testCreateDFSCatalog() {
         FactoryUtil.createCatalog(
             catalogName, options, null, Thread.currentThread().getContextClassLoader());
 
-    checkEquals(expectedCatalog, (AbstractCatalog)actualCatalog);
+    checkEquals(expectedCatalog, (AbstractCatalog) actualCatalog);
   }
 
   private static void checkEquals(AbstractCatalog c1, AbstractCatalog c2) {

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/catalog/TestHoodieHiveCatalog.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.configuration.FlinkOptions;
 import org.apache.hudi.exception.HoodieCatalogException;
+import org.apache.hudi.util.StreamerUtil;
 
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.api.Schema;
@@ -37,7 +38,6 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hudi.util.StreamerUtil;
 import org.junit.jupiter.api.AfterAll;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeAll;
@@ -138,7 +138,7 @@ public void testCreateAndGetHoodieTable(HoodieTableType tableType) throws Except
     String expectedTableSchema = "`uuid` INT NOT NULL,`name` STRING,`age` INT,`par1` STRING,`ts` BIGINT";
     assertEquals(expectedTableSchema, tableSchema);
     assertEquals(Collections.singletonList("uuid"), table1.getUnresolvedSchema().getPrimaryKey().get().getColumnNames());
-    assertEquals(Collections.singletonList("par1"), ((CatalogTable)table1).getPartitionKeys());
+    assertEquals(Collections.singletonList("par1"), ((CatalogTable) table1).getPartitionKeys());
 
     // validate explicit primary key
     options.put(FlinkOptions.RECORD_KEY_FIELD.key(), "id");

File: hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/HeapMapColumnVector.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.table.format.cow.vector;
 
-import org.apache.flink.table.data.columnar.ColumnarMapData;
 import org.apache.flink.table.data.MapData;
+import org.apache.flink.table.data.columnar.ColumnarMapData;
 import org.apache.flink.table.data.columnar.vector.ColumnVector;
 import org.apache.flink.table.data.columnar.vector.MapColumnVector;
 import org.apache.flink.table.data.columnar.vector.heap.AbstractHeapVector;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java
Patch:
@@ -176,8 +176,8 @@ public static HoodieWriteConfig getHoodieClientConfig(
                         ClusteringPlanPartitionFilterMode.valueOf(conf.getString(FlinkOptions.CLUSTERING_PLAN_PARTITION_FILTER_MODE_NAME)))
                     .withClusteringTargetPartitions(conf.getInteger(FlinkOptions.CLUSTERING_TARGET_PARTITIONS))
                     .withClusteringMaxNumGroups(conf.getInteger(FlinkOptions.CLUSTERING_MAX_NUM_GROUPS))
-                    .withClusteringTargetFileMaxBytes(conf.getInteger(FlinkOptions.CLUSTERING_PLAN_STRATEGY_TARGET_FILE_MAX_BYTES))
-                    .withClusteringPlanSmallFileLimit(conf.getInteger(FlinkOptions.CLUSTERING_PLAN_STRATEGY_SMALL_FILE_LIMIT) * 1024 * 1024L)
+                    .withClusteringTargetFileMaxBytes(conf.getLong(FlinkOptions.CLUSTERING_PLAN_STRATEGY_TARGET_FILE_MAX_BYTES))
+                    .withClusteringPlanSmallFileLimit(conf.getLong(FlinkOptions.CLUSTERING_PLAN_STRATEGY_SMALL_FILE_LIMIT) * 1024 * 1024L)
                     .withClusteringSkipPartitionsFromLatest(conf.getInteger(FlinkOptions.CLUSTERING_PLAN_STRATEGY_SKIP_PARTITIONS_FROM_LATEST))
                     .withAsyncClusteringMaxCommits(conf.getInteger(FlinkOptions.CLUSTERING_DELTA_COMMITS))
                     .build())

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/AvroToRowDataConverters.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.flink.table.data.TimestampData;
 import org.apache.flink.table.types.logical.ArrayType;
 import org.apache.flink.table.types.logical.DecimalType;
+import org.apache.flink.table.types.logical.LocalZonedTimestampType;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.table.types.logical.TimestampType;
@@ -127,6 +128,8 @@ public static AvroToRowDataConverter createConverter(LogicalType type) {
         return AvroToRowDataConverters::convertToDate;
       case TIME_WITHOUT_TIME_ZONE:
         return AvroToRowDataConverters::convertToTime;
+      case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
+        return createTimestampConverter(((LocalZonedTimestampType) type).getPrecision());
       case TIMESTAMP_WITHOUT_TIME_ZONE:
         return createTimestampConverter(((TimestampType) type).getPrecision());
       case CHAR:

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/RowDataToAvroConverters.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.flink.table.types.logical.ArrayType;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
-import org.apache.flink.table.types.logical.TimestampType;
 
 import java.io.Serializable;
 import java.math.BigDecimal;
@@ -157,8 +156,9 @@ public Object convert(Schema schema, Object object) {
               }
             };
         break;
+      case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
       case TIMESTAMP_WITHOUT_TIME_ZONE:
-        final int precision = ((TimestampType) type).getPrecision();
+        final int precision = DataTypeUtils.precision(type);
         if (precision <= 3) {
           converter =
               new RowDataToAvroConverter() {
@@ -231,7 +231,7 @@ public Object convert(Schema schema, Object object) {
             actualSchema = types.get(1);
           } else {
             throw new IllegalArgumentException(
-                "The Avro schema is not a nullable type: " + schema.toString());
+                "The Avro schema is not a nullable type: " + schema);
           }
         } else {
           actualSchema = schema;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/utils/Pipelines.java
Patch:
@@ -144,7 +144,7 @@ public static DataStreamSink<Object> bulkInsert(Configuration conf, RowType rowT
         // sort by partition keys
         dataStream = dataStream
             .transform("partition_key_sorter",
-                TypeInformation.of(RowData.class),
+                InternalTypeInfo.of(rowType),
                 sortOperatorGen.createSortOperator())
             .setParallelism(conf.getInteger(FlinkOptions.WRITE_TASKS));
         ExecNodeUtil.setManagedMemoryWeight(dataStream.getTransformation(),

File: hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java
Patch:
@@ -303,7 +303,9 @@ public int addInputAndValidateIngestion(SparkSession spark, FileSystem fs, Strin
     }
 
     if (tableType.equals(HoodieTableType.MERGE_ON_READ.name())) {
-      numExpCommits += 1;
+      if (inputDF2 != null) {
+        numExpCommits += 1;
+      }
       // Wait for compaction to also finish and track latest timestamp as commit timestamp
       waitTillNCommits(fs, numExpCommits, 180, 3);
       commitInstantTime2 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGenUtils.java
Patch:
@@ -75,7 +75,9 @@ public static String[] extractRecordKeys(String recordKey) {
     String[] fieldKV = recordKey.split(",");
     return Arrays.stream(fieldKV).map(kv -> {
       final String[] kvArray = kv.split(":");
-      if (kvArray[1].equals(NULL_RECORDKEY_PLACEHOLDER)) {
+      if (kvArray.length == 1) {
+        return kvArray[0];
+      } else if (kvArray[1].equals(NULL_RECORDKEY_PLACEHOLDER)) {
         return null;
       } else if (kvArray[1].equals(EMPTY_RECORDKEY_PLACEHOLDER)) {
         return "";

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java
Patch:
@@ -73,7 +73,7 @@ public static Pair<String, Pair<String, String>> calculateBeginAndEndInstants(Ja
     HoodieTableMetaClient srcMetaClient = HoodieTableMetaClient.builder().setConf(jssc.hadoopConfiguration()).setBasePath(srcBasePath).setLoadActiveTimelineOnLoad(true).build();
 
     final HoodieTimeline activeCommitTimeline =
-        srcMetaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants();
+        srcMetaClient.getCommitsAndCompactionTimeline().filterCompletedInstants();
 
     String beginInstantTime = beginInstant.orElseGet(() -> {
       if (missingCheckpointStrategy != null) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/InProcessLockProvider.java
Patch:
@@ -95,13 +95,13 @@ public void unlock() {
     try {
       if (LOCK.isWriteLockedByCurrentThread()) {
         LOCK.writeLock().unlock();
+        LOG.info(getLogMessage(LockState.RELEASED));
       } else {
         LOG.warn("Cannot unlock because the current thread does not hold the lock.");
       }
     } catch (Exception e) {
       throw new HoodieLockException(getLogMessage(LockState.FAILED_TO_RELEASE), e);
     }
-    LOG.info(getLogMessage(LockState.RELEASED));
   }
 
   @Override

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java
Patch:
@@ -195,8 +195,8 @@ private HoodieCleanMetadata runClean(HoodieTable<T, I, K, O> table, HoodieInstan
     ValidationUtils.checkArgument(cleanInstant.getState().equals(HoodieInstant.State.REQUESTED)
         || cleanInstant.getState().equals(HoodieInstant.State.INFLIGHT));
 
+    HoodieInstant inflightInstant = null;
     try {
-      final HoodieInstant inflightInstant;
       final HoodieTimer timer = new HoodieTimer();
       timer.startTimer();
       if (cleanInstant.isRequested()) {
@@ -218,7 +218,7 @@ private HoodieCleanMetadata runClean(HoodieTable<T, I, K, O> table, HoodieInstan
           cleanStats
       );
       if (!skipLocking) {
-        this.txnManager.beginTransaction(Option.empty(), Option.empty());
+        this.txnManager.beginTransaction(Option.of(inflightInstant), Option.empty());
       }
       writeTableMetadata(metadata, inflightInstant.getTimestamp());
       table.getActiveTimeline().transitionCleanInflightToComplete(inflightInstant,
@@ -229,7 +229,7 @@ private HoodieCleanMetadata runClean(HoodieTable<T, I, K, O> table, HoodieInstan
       throw new HoodieIOException("Failed to clean up after commit", e);
     } finally {
       if (!skipLocking) {
-        this.txnManager.endTransaction(Option.empty());
+        this.txnManager.endTransaction(Option.of(inflightInstant));
       }
     }
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java
Patch:
@@ -238,7 +238,7 @@ protected void finishRollback(HoodieInstant inflightInstant, HoodieRollbackMetad
     boolean enableLocking = (!skipLocking && !skipTimelinePublish);
     try {
       if (enableLocking) {
-        this.txnManager.beginTransaction(Option.empty(), Option.empty());
+        this.txnManager.beginTransaction(Option.of(inflightInstant), Option.empty());
       }
 
       // If publish the rollback to the timeline, we first write the rollback metadata
@@ -261,7 +261,7 @@ protected void finishRollback(HoodieInstant inflightInstant, HoodieRollbackMetad
       throw new HoodieIOException("Error executing rollback at instant " + instantTime, e);
     } finally {
       if (enableLocking) {
-        this.txnManager.endTransaction(Option.empty());
+        this.txnManager.endTransaction(Option.of(inflightInstant));
       }
     }
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/Source.java
Patch:
@@ -38,7 +38,7 @@
 public abstract class Source<T> implements SourceCommitCallback, Serializable {
 
   public enum SourceType {
-    JSON, AVRO, ROW
+    JSON, AVRO, ROW, PROTO
   }
 
   protected transient TypedProperties props;

File: hudi-aws/src/main/java/org/apache/hudi/aws/sync/AWSGlueCatalogSyncClient.java
Patch:
@@ -162,7 +162,7 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit
         StorageDescriptor partitionSd = sd.clone();
         String fullPartitionPath = FSUtils.getPartitionPath(getBasePath(), partition).toString();
         List<String> partitionValues = partitionValueExtractor.extractPartitionValuesInPath(partition);
-        sd.setLocation(fullPartitionPath);
+        partitionSd.setLocation(fullPartitionPath);
         PartitionInput partitionInput = new PartitionInput().withValues(partitionValues).withStorageDescriptor(partitionSd);
         return new BatchUpdatePartitionRequestEntry().withPartitionInput(partitionInput).withPartitionValueList(partitionValues);
       }).collect(Collectors.toList());

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java
Patch:
@@ -532,7 +532,7 @@ public void testPartitionMetafileFormat(boolean partitionMetafileUseBaseFormat)
     writeClient.startCommitWithTime(instantTime);
 
     // Insert new records
-    final JavaRDD<HoodieRecord> inputRecords = generateTestRecordsForBulkInsert(jsc, 10);
+    final JavaRDD<HoodieRecord> inputRecords = generateTestRecordsForBulkInsert(jsc, 50);
     writeClient.bulkInsert(inputRecords, instantTime);
 
     // Partition metafile should be created

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java
Patch:
@@ -902,6 +902,7 @@ public void testCleanerDeleteReplacedDataWithArchive(Boolean asyncClean) throws
     cfg.configs.addAll(getAsyncServicesConfigs(totalRecords, "false", "true", "2", "", ""));
     cfg.configs.add(String.format("%s=%s", HoodieCompactionConfig.PARQUET_SMALL_FILE_LIMIT.key(), "0"));
     cfg.configs.add(String.format("%s=%s", HoodieMetadataConfig.COMPACT_NUM_DELTA_COMMITS.key(), "1"));
+    cfg.configs.add(String.format("%s=%s", HoodieWriteConfig.MARKERS_TYPE.key(), "DIRECT"));
     HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);
     deltaStreamerTestRunner(ds, cfg, (r) -> {
       TestHelpers.assertAtLeastNReplaceCommits(2, tableBasePath, dfs);
@@ -947,13 +948,14 @@ public void testCleanerDeleteReplacedDataWithArchive(Boolean asyncClean) throws
     assertFalse(replacedFilePaths.isEmpty());
 
     // Step 4 : Insert 1 record and trigger sync/async cleaner and archive.
-    List<String> configs = getAsyncServicesConfigs(1, "true", "true", "2", "", "");
+    List<String> configs = getAsyncServicesConfigs(1, "true", "true", "6", "", "");
     configs.add(String.format("%s=%s", HoodieCleanConfig.CLEANER_POLICY.key(), "KEEP_LATEST_COMMITS"));
     configs.add(String.format("%s=%s", HoodieCleanConfig.CLEANER_COMMITS_RETAINED.key(), "1"));
     configs.add(String.format("%s=%s", HoodieArchivalConfig.MIN_COMMITS_TO_KEEP.key(), "2"));
     configs.add(String.format("%s=%s", HoodieArchivalConfig.MAX_COMMITS_TO_KEEP.key(), "3"));
     configs.add(String.format("%s=%s", HoodieCleanConfig.ASYNC_CLEAN.key(), asyncClean));
     configs.add(String.format("%s=%s", HoodieMetadataConfig.COMPACT_NUM_DELTA_COMMITS.key(), "1"));
+    cfg.configs.add(String.format("%s=%s", HoodieWriteConfig.MARKERS_TYPE.key(), "DIRECT"));
     if (asyncClean) {
       configs.add(String.format("%s=%s", HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(),
           WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.name()));

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieContinousTestSuiteWriter.java
Patch:
@@ -22,8 +22,6 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
-import org.apache.hudi.config.HoodieWriteConfig;
-import org.apache.hudi.integ.testsuite.HoodieTestSuiteWriter;
 import org.apache.hudi.integ.testsuite.writer.DeltaWriteStats;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieMultiWriterTestSuiteJob.java
Patch:
@@ -34,7 +34,6 @@
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
-import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 
@@ -134,7 +133,7 @@ public static void main(String[] args) throws Exception {
     AtomicBoolean jobFailed = new AtomicBoolean(false);
     AtomicInteger counter = new AtomicInteger(0);
     List<Long> waitTimes = new ArrayList<>();
-    for (int i = 0;i < jobIndex ;i++) {
+    for (int i = 0; i < jobIndex; i++) {
       if (i == 0) {
         waitTimes.add(0L);
       } else {

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteWriter.java
Patch:
@@ -116,7 +116,7 @@ private boolean allowWriteClientAccess(DagNode dagNode) {
 
   public abstract RDD<GenericRecord> getNextBatch() throws Exception;
 
-  public abstract Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> fetchSource() throws Exception ;
+  public abstract Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> fetchSource() throws Exception;
 
   public abstract Option<String> startCommit();
 
@@ -132,7 +132,7 @@ private boolean allowWriteClientAccess(DagNode dagNode) {
 
   public abstract JavaRDD<WriteStatus> compact(Option<String> instantTime) throws Exception;
 
-  public abstract void inlineClustering() throws Exception ;
+  public abstract void inlineClustering() throws Exception;
 
   public abstract Option<String> scheduleCompaction(Option<Map<String, String>> previousCommitExtraMetadata) throws Exception;
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/SparkDataSourceContinuousIngestTool.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.fs.FSUtils;
-import org.apache.hudi.integ.testsuite.SparkDataSourceContinuousIngest;
 import org.apache.hudi.utilities.HoodieRepairTool;
 import org.apache.hudi.utilities.IdentitySplitter;
 import org.apache.hudi.utilities.UtilHelpers;

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/configuration/DFSDeltaConfig.java
Patch:
@@ -46,7 +46,7 @@ public DFSDeltaConfig(DeltaOutputMode deltaOutputMode, DeltaInputType deltaInput
                         SerializableConfiguration configuration,
                         String deltaBasePath, String targetBasePath, String schemaStr, Long maxFileSize,
                         int inputParallelism, boolean deleteOldInputData, boolean useHudiToGenerateUpdates) {
-     super(deltaOutputMode, deltaInputType, configuration);
+    super(deltaOutputMode, deltaInputType, configuration);
     this.deltaBasePath = deltaBasePath;
     this.schemaStr = schemaStr;
     this.maxFileSize = maxFileSize;

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/BaseQueryNode.java
Patch:
@@ -40,8 +40,7 @@ public void executeAndValidateQueries(List<Pair<String, Integer>> queriesWithRes
       if (!res.next()) {
         log.info("res.next() was False - typically this means the query returned no rows.");
         assert 0 == queryAndResult.getRight();
-      }
-      else {
+      } else {
         Integer result = res.getInt(1);
         if (!queryAndResult.getRight().equals(result)) {
           throw new AssertionError(

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/HiveQueryNode.java
Patch:
@@ -67,8 +67,7 @@ public void execute(ExecutionContext executionContext, int curItrCount) throws E
       executeAndValidateQueries(this.config.getHiveQueries(), stmt);
       stmt.close();
       this.hiveServiceProvider.stopLocalHiveServiceIfNeeded();
-    }
-    catch (Exception e) {
+    } catch (Exception e) {
       throw new HoodieValidationException("Hive query validation failed due to " + e.getMessage(), e);
     }
   }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/HiveSyncNode.java
Patch:
@@ -31,7 +31,6 @@
  */
 public class HiveSyncNode extends DagNode<Boolean> {
 
-
   public HiveSyncNode(Config config) {
     this.config = config;
   }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/PrestoQueryNode.java
Patch:
@@ -52,8 +52,7 @@ public void execute(ExecutionContext context, int curItrCount) throws Exception
       setSessionProperties(this.config.getPrestoProperties(), stmt);
       executeAndValidateQueries(this.config.getPrestoQueries(), stmt);
       stmt.close();
-    }
-    catch (Exception e) {
+    } catch (Exception e) {
       throw new HoodieValidationException("Presto query validation failed due to " + e.getMessage(), e);
     }
   }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/TrinoQueryNode.java
Patch:
@@ -27,7 +27,7 @@
 import java.sql.DriverManager;
 import java.sql.Statement;
 
-public class TrinoQueryNode extends BaseQueryNode{
+public class TrinoQueryNode extends BaseQueryNode {
 
   public TrinoQueryNode(DeltaConfig.Config config) {
     this.config = config;
@@ -52,8 +52,7 @@ public void execute(ExecutionContext context, int curItrCount) throws Exception
       setSessionProperties(this.config.getTrinoProperties(), stmt);
       executeAndValidateQueries(this.config.getTrinoQueries(), stmt);
       stmt.close();
-    }
-    catch (Exception e) {
+    }  catch (Exception e) {
       throw new HoodieValidationException("Trino query validation failed due to " + e.getMessage(), e);
     }
   }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/scheduler/DagScheduler.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.integ.testsuite.dag.WriterContext;
 import org.apache.hudi.integ.testsuite.dag.nodes.DagNode;
 import org.apache.hudi.integ.testsuite.dag.nodes.DelayNode;
-import org.apache.hudi.metrics.Metrics;
 
 import org.apache.spark.api.java.JavaSparkContext;
 import org.slf4j.Logger;

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/GenericRecordFullPayloadGenerator.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.integ.testsuite.generator;
 
-import com.google.common.annotations.VisibleForTesting;
 import org.apache.hudi.common.util.collection.Pair;
 
 import org.apache.avro.Schema;
@@ -331,12 +330,13 @@ public boolean validate(GenericRecord record) {
     return genericData.validate(baseSchema, record);
   }
 
-  /*
+  /**
    * Generates a sequential timestamp (daily increment), and updates the timestamp field of the record.
    * Note: When generating records, number of records to be generated must be more than numDatePartitions * parallelism,
    * to guarantee that at least numDatePartitions are created.
+   *
+   * @VisibleForTesting
    */
-  @VisibleForTesting
   public GenericRecord updateTimestamp(GenericRecord record, String fieldName) {
     long delta = TimeUnit.SECONDS.convert((partitionIndex++ % numDatePartitions) + startPartition, TimeUnit.DAYS);
     record.put(fieldName, delta);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -340,6 +340,7 @@ public Pair<Option<String>, JavaRDD<WriteStatus>> syncOnce() throws IOException
 
     metrics.updateDeltaStreamerSyncMetrics(System.currentTimeMillis());
 
+    // TODO revisit (too early to unpersist)
     // Clear persistent RDDs
     jssc.getPersistentRDDs().values().forEach(JavaRDD::unpersist);
     return result;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -832,7 +832,7 @@ protected Boolean onInitializingWriteClient(SparkRDDWriteClient writeClient) {
      * Close all resources.
      */
     public void close() {
-      if (null != deltaSync) {
+      if (deltaSync != null) {
         deltaSync.close();
       }
     }

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/ITTestHoodieDataSource.java
Patch:
@@ -1015,8 +1015,8 @@ void testBulkInsertNonPartitionedTable() {
   void testAppendWrite(boolean clustering) {
     TableEnvironment tableEnv = streamTableEnv;
     // csv source
-    String csvSourceDDL = TestConfigurations.getCsvSourceDDL("csv_source", "test_source_5.data");
-    tableEnv.executeSql(csvSourceDDL);
+    String sourceDDL = TestConfigurations.getFileSourceDDL("source");
+    tableEnv.executeSql(sourceDDL);
 
     String hoodieTableDDL = sql("hoodie_sink")
         .option(FlinkOptions.PATH, tempFile.getAbsolutePath())
@@ -1025,7 +1025,7 @@ void testAppendWrite(boolean clustering) {
         .end();
     tableEnv.executeSql(hoodieTableDDL);
 
-    String insertInto = "insert into hoodie_sink select * from csv_source";
+    String insertInto = "insert into hoodie_sink select * from source";
     execInsertSql(tableEnv, insertInto);
 
     List<Row> result1 = CollectionUtil.iterableToList(

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -332,7 +332,8 @@ private FlinkOptions() {
       .booleanType()
       .defaultValue(false)
       .withDescription("Flag to indicate whether to ignore any non exception error (e.g. writestatus error). within a checkpoint batch.\n"
-          + "By default false");
+          + "By default false.  Turning this on, could hide the write status errors while the spark checkpoint moves ahead. \n"
+          + "  So, would recommend users to use this with caution.");
 
   public static final ConfigOption<String> RECORD_KEY_FIELD = ConfigOptions
       .key(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key())

File: hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java
Patch:
@@ -372,6 +372,7 @@ public void stream(Dataset<Row> streamingInput, String operationType, String che
         .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE().key(), "true")
         .option(DataSourceWriteOptions.ASYNC_CLUSTERING_ENABLE().key(), "true")
         .option(HoodieCompactionConfig.PRESERVE_COMMIT_METADATA.key(), "false")
+        .option(DataSourceWriteOptions.STREAMING_IGNORE_FAILED_BATCH().key(),"true")
         .option(HoodieWriteConfig.TBL_NAME.key(), tableName).option("checkpointLocation", checkpointLocation)
         .outputMode(OutputMode.Append());
 

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/metrics/datadog/TestDatadogReporter.java
Patch:
@@ -39,6 +39,7 @@
 
 import java.io.IOException;
 import java.util.Arrays;
+import java.util.UUID;
 import java.util.concurrent.TimeUnit;
 
 import static org.junit.jupiter.api.Assertions.assertEquals;
@@ -78,7 +79,7 @@ public void stopShouldCloseEnclosedClient() throws IOException {
 
   @Test
   public void stopShouldLogWhenEnclosedClientFailToClose() throws IOException {
-    when(appender.getName()).thenReturn("MockAppender");
+    when(appender.getName()).thenReturn("MockAppender-" + UUID.randomUUID());
     when(appender.isStarted()).thenReturn(true);
     when(appender.isStopped()).thenReturn(false);
     ((Logger) LogManager.getLogger(DatadogReporter.class)).addAppender(appender);

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -99,7 +99,7 @@ private FlinkOptions() {
 
   public static final String NO_PRE_COMBINE = "no_precombine";
   public static final ConfigOption<String> PRECOMBINE_FIELD = ConfigOptions
-      .key("payload.ordering.field")
+      .key("precombine.field")
       .stringType()
       .defaultValue("ts")
       .withFallbackKeys("write.precombine.field")
@@ -330,9 +330,9 @@ private FlinkOptions() {
   public static final ConfigOption<Boolean> IGNORE_FAILED = ConfigOptions
       .key("write.ignore.failed")
       .booleanType()
-      .defaultValue(true)
+      .defaultValue(false)
       .withDescription("Flag to indicate whether to ignore any non exception error (e.g. writestatus error). within a checkpoint batch.\n"
-          + "By default true (in favor of streaming progressing over data integrity)");
+          + "By default false");
 
   public static final ConfigOption<String> RECORD_KEY_FIELD = ConfigOptions
       .key(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key())

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/ITTestDataStreamWrite.java
Patch:
@@ -246,7 +246,6 @@ private void testWriteToHoodie(
     execEnv.addOperator(pipeline.getTransformation());
 
     if (isMor) {
-      Pipelines.clean(conf, pipeline);
       Pipelines.compact(conf, pipeline);
     }
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.configuration.FlinkOptions;
 import org.apache.hudi.configuration.HadoopConfigurations;
+import org.apache.hudi.configuration.OptionsInference;
 import org.apache.hudi.configuration.OptionsResolver;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.source.FileIndex;
@@ -179,6 +180,7 @@ public DataStream<RowData> produceDataStream(StreamExecutionEnvironment execEnv)
         @SuppressWarnings("unchecked")
         TypeInformation<RowData> typeInfo =
             (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(getProducedDataType());
+        OptionsInference.setupSourceTasks(conf, execEnv.getParallelism());
         if (conf.getBoolean(FlinkOptions.READ_AS_STREAMING)) {
           StreamReadMonitoringFunction monitoringFunction = new StreamReadMonitoringFunction(
               conf, FilePathUtils.toFlinkPath(path), tableRowType, maxCompactionMemoryInBytes, getRequiredPartitionPaths());

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/cluster/ITTestHoodieFlinkClustering.java
Patch:
@@ -88,7 +88,7 @@ public void testHoodieFlinkClustering() throws Exception {
     EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();
     TableEnvironment tableEnv = TableEnvironmentImpl.create(settings);
     tableEnv.getConfig().getConfiguration()
-        .setInteger(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 1);
+        .setInteger(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 4);
     Map<String, String> options = new HashMap<>();
     options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());
 
@@ -187,7 +187,7 @@ public void testHoodieFlinkClusteringService() throws Exception {
     EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();
     TableEnvironment tableEnv = TableEnvironmentImpl.create(settings);
     tableEnv.getConfig().getConfiguration()
-        .setInteger(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 1);
+        .setInteger(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 4);
     Map<String, String> options = new HashMap<>();
     options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());
 

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/ITTestHoodieDataSource.java
Patch:
@@ -84,7 +84,7 @@ void beforeEach() {
     EnvironmentSettings settings = EnvironmentSettings.newInstance().build();
     streamTableEnv = TableEnvironmentImpl.create(settings);
     streamTableEnv.getConfig().getConfiguration()
-        .setInteger(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 1);
+        .setInteger(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 4);
     Configuration execConf = streamTableEnv.getConfig().getConfiguration();
     execConf.setString("execution.checkpointing.interval", "2s");
     // configure not to retry after failure
@@ -93,7 +93,7 @@ void beforeEach() {
 
     batchTableEnv = TestTableEnvs.getBatchTableEnv();
     batchTableEnv.getConfig().getConfiguration()
-        .setInteger(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 1);
+        .setInteger(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 4);
   }
 
   @TempDir

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/streamer/FlinkStreamerConfig.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.configuration.FlinkOptions;
-import org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor;
+import org.apache.hudi.hive.MultiPartKeysValueExtractor;
 import org.apache.hudi.keygen.constant.KeyGeneratorType;
 import org.apache.hudi.util.FlinkStateBackendConverter;
 import org.apache.hudi.util.StreamerUtil;
@@ -321,8 +321,8 @@ public class FlinkStreamerConfig extends Configuration {
   public String hiveSyncPartitionFields = "";
 
   @Parameter(names = {"--hive-sync-partition-extractor-class"}, description = "Tool to extract the partition value from HDFS path, "
-      + "default 'SlashEncodedDayPartitionValueExtractor'")
-  public String hiveSyncPartitionExtractorClass = SlashEncodedDayPartitionValueExtractor.class.getCanonicalName();
+      + "default 'MultiPartKeysValueExtractor'")
+  public String hiveSyncPartitionExtractorClass = MultiPartKeysValueExtractor.class.getCanonicalName();
 
   @Parameter(names = {"--hive-sync-assume-date-partitioning"}, description = "Assume partitioning is yyyy/mm/dd, default false")
   public Boolean hiveSyncAssumeDatePartition = false;

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/MarkerHandler.java
Patch:
@@ -34,7 +34,7 @@
 import org.apache.log4j.Logger;
 
 import java.io.IOException;
-import java.util.HashMap;
+import java.util.concurrent.ConcurrentHashMap;
 import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.CompletableFuture;
@@ -74,7 +74,8 @@ public class MarkerHandler extends Handler {
   // Parallelism for reading and deleting marker files
   private final int parallelism;
   // Marker directory states, {markerDirPath -> MarkerDirState instance}
-  private final Map<String, MarkerDirState> markerDirStateMap = new HashMap<>();
+  // Use ConcurrentHashMap to ensure thread safety in dispatchingExecutorService
+  private final Map<String, MarkerDirState> markerDirStateMap = new ConcurrentHashMap<>();
   // A thread to dispatch marker creation requests to batch processing threads
   private final MarkerCreationDispatchingRunnable markerCreationDispatchingRunnable;
   private final Object firstCreationRequestSeenLock = new Object();

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -515,8 +515,10 @@ protected static int upgradeOrDowngradeTable(JavaSparkContext jsc, String basePa
             .setLoadActiveTimelineOnLoad(false).setConsistencyGuardConfig(config.getConsistencyGuardConfig())
             .setLayoutVersion(Option.of(new TimelineLayoutVersion(config.getTimelineLayoutVersion())))
             .setFileSystemRetryConfig(config.getFileSystemRetryConfig()).build();
+    HoodieWriteConfig updatedConfig = HoodieWriteConfig.newBuilder().withProps(config.getProps())
+        .forTable(metaClient.getTableConfig().getTableName()).build();
     try {
-      new UpgradeDowngrade(metaClient, config, new HoodieSparkEngineContext(jsc), SparkUpgradeDowngradeHelper.getInstance())
+      new UpgradeDowngrade(metaClient, updatedConfig, new HoodieSparkEngineContext(jsc), SparkUpgradeDowngradeHelper.getInstance())
           .run(HoodieTableVersion.valueOf(toVersion), null);
       LOG.info(String.format("Table at \"%s\" upgraded / downgraded to version \"%s\".", basePath, toVersion));
       return 0;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java
Patch:
@@ -480,7 +480,7 @@ private void processQueuedBlocksForInstant(Deque<HoodieLogBlock> logBlocks, int
       }
     }
     // At this step the lastBlocks are consumed. We track approximate progress by number of log-files seen
-    progress = numLogFilesSeen - 1 / logFilePaths.size();
+    progress = (numLogFilesSeen - 1) / logFilePaths.size();
   }
 
   private ClosableIterator<IndexedRecord> getRecordsIterator(HoodieDataBlock dataBlock, Option<KeySpec> keySpecOpt) throws IOException {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/streamer/HoodieFlinkStreamer.java
Patch:
@@ -69,12 +69,12 @@ public static void main(String[] args) throws Exception {
     TypedProperties kafkaProps = DFSPropertiesConfiguration.getGlobalProps();
     kafkaProps.putAll(StreamerUtil.appendKafkaProps(cfg));
 
+    Configuration conf = FlinkStreamerConfig.toFlinkConfig(cfg);
     // Read from kafka source
     RowType rowType =
-        (RowType) AvroSchemaConverter.convertToDataType(StreamerUtil.getSourceSchema(cfg))
+        (RowType) AvroSchemaConverter.convertToDataType(StreamerUtil.getSourceSchema(conf))
             .getLogicalType();
 
-    Configuration conf = FlinkStreamerConfig.toFlinkConfig(cfg);
     long ckpTimeout = env.getCheckpointConfig().getCheckpointTimeout();
     int parallelism = env.getParallelism();
     conf.setLong(FlinkOptions.WRITE_COMMIT_ACK_TIMEOUT, ckpTimeout);

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestFileSystemViewCommand.java
Patch:
@@ -161,7 +161,7 @@ private void createPartitionedTable() throws IOException {
   @Test
   public void testShowCommits() {
     // Test default show fsview all
-    CommandResult cr = shell().executeCommand("show fsview all");
+    CommandResult cr = shell().executeCommand("show fsview all --pathRegex */*/*");
     assertTrue(cr.isSuccess());
 
     // Get all file groups
@@ -209,7 +209,7 @@ public void testShowCommits() {
   @Test
   public void testShowCommitsWithSpecifiedValues() {
     // Test command with options, baseFileOnly and maxInstant is 2
-    CommandResult cr = shell().executeCommand("show fsview all --baseFileOnly true --maxInstant 2");
+    CommandResult cr = shell().executeCommand("show fsview all --pathRegex */*/* --baseFileOnly true --maxInstant 2");
     assertTrue(cr.isSuccess());
 
     List<Comparable[]> rows = new ArrayList<>();

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/QueryBasedDDLExecutor.java
Patch:
@@ -100,7 +100,7 @@ public void updateTableDefinition(String tableName, MessageType newSchema) {
     try {
       String newSchemaStr = HiveSchemaUtil.generateSchemaString(newSchema, config.getSplitStrings(META_SYNC_PARTITION_FIELDS), config.getBoolean(HIVE_SUPPORT_TIMESTAMP_TYPE));
       // Cascade clause should not be present for non-partitioned tables
-      String cascadeClause = config.getSplitStrings(HIVE_SUPPORT_TIMESTAMP_TYPE).size() > 0 ? " cascade" : "";
+      String cascadeClause = config.getSplitStrings(META_SYNC_PARTITION_FIELDS).size() > 0 ? " cascade" : "";
       StringBuilder sqlBuilder = new StringBuilder("ALTER TABLE ").append(HIVE_ESCAPE_CHARACTER)
           .append(databaseName).append(HIVE_ESCAPE_CHARACTER).append(".")
           .append(HIVE_ESCAPE_CHARACTER).append(tableName)

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/replication/TestHiveSyncGlobalCommitTool.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.hudi.hive.replication;
 
+import org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor;
 import org.apache.hudi.hive.testutils.HiveTestCluster;
 
 import org.apache.hadoop.fs.Path;
@@ -41,6 +42,7 @@
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_ASSUME_DATE_PARTITION;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_BASE_PATH;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_DATABASE_NAME;
+import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_PARTITION_EXTRACTOR_CLASS;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_PARTITION_FIELDS;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_TABLE_NAME;
 import static org.junit.jupiter.api.Assertions.assertEquals;
@@ -75,6 +77,7 @@ private HiveSyncGlobalCommitParams getGlobalCommitConfig(String commitTime) thro
     params.loadedProps.setProperty(META_SYNC_ASSUME_DATE_PARTITION.key(), "true");
     params.loadedProps.setProperty(HIVE_USE_PRE_APACHE_INPUT_FORMAT.key(), "false");
     params.loadedProps.setProperty(META_SYNC_PARTITION_FIELDS.key(), "datestr");
+    params.loadedProps.setProperty(META_SYNC_PARTITION_EXTRACTOR_CLASS.key(), SlashEncodedDayPartitionValueExtractor.class.getName());
     return params;
   }
 

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/HMSDDLExecutor.java
Patch:
@@ -225,8 +225,9 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit
         String fullPartitionPath = StorageSchemes.HDFS.getScheme().equals(partitionScheme)
             ? FSUtils.getDFSFullPartitionPath(syncConfig.getHadoopFileSystem(), partitionPath) : partitionPath.toString();
         List<String> partitionValues = partitionValueExtractor.extractPartitionValuesInPath(partition);
-        sd.setLocation(fullPartitionPath);
-        return new Partition(partitionValues, databaseName, tableName, 0, 0, sd, null);
+        StorageDescriptor partitionSd = sd.deepCopy();
+        partitionSd.setLocation(fullPartitionPath);
+        return new Partition(partitionValues, databaseName, tableName, 0, 0, partitionSd, null);
       }).collect(Collectors.toList());
       client.alter_partitions(databaseName, tableName, partitionList, null);
     } catch (TException e) {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/compact/HoodieFlinkCompactor.java
Patch:
@@ -173,6 +173,8 @@ public AsyncCompactionService(FlinkCompactionConfig cfg, Configuration conf, Str
       // set table schema
       CompactionUtil.setAvroSchema(conf, metaClient);
 
+      CompactionUtil.setPreCombineField(conf, metaClient);
+
       // infer changelog mode
       CompactionUtil.inferChangelogMode(conf, metaClient);
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/MarkerUtils.java
Patch:
@@ -64,7 +64,8 @@ public class MarkerUtils {
    * @return marker file name
    */
   public static String stripMarkerFolderPrefix(String fullMarkerPath, String basePath, String instantTime) {
-    ValidationUtils.checkArgument(fullMarkerPath.contains(HoodieTableMetaClient.MARKER_EXTN));
+    ValidationUtils.checkArgument(fullMarkerPath.contains(HoodieTableMetaClient.MARKER_EXTN),
+        String.format("Using DIRECT markers but marker path does not contain extension: %s", HoodieTableMetaClient.MARKER_EXTN));
     String markerRootPath = Path.getPathWithoutSchemeAndAuthority(
         new Path(String.format("%s/%s/%s", basePath, HoodieTableMetaClient.TEMPFOLDER_NAME, instantTime))).toString();
     return stripMarkerFolderPrefix(fullMarkerPath, markerRootPath);

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/marker/MarkerDirState.java
Patch:
@@ -273,7 +273,7 @@ private void syncMarkersFromFileSystem() {
   private void writeMarkerTypeToFile() {
     Path dirPath = new Path(markerDirPath);
     try {
-      if (!fileSystem.exists(dirPath)) {
+      if (!fileSystem.exists(dirPath) || !MarkerUtils.doesMarkerTypeFileExist(fileSystem, markerDirPath)) {
         // There is no existing marker directory, create a new directory and write marker type
         fileSystem.mkdirs(dirPath);
         MarkerUtils.writeMarkerTypeToFile(MarkerType.TIMELINE_SERVER_BASED, fileSystem, markerDirPath);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -2082,6 +2082,7 @@ public void testRollbackDuringUpgradeForDoubleLocking() throws IOException, Inte
         .withWriteConcurrencyMode(WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL)
         .withLockConfig(HoodieLockConfig.newBuilder().withLockProvider(InProcessLockProvider.class).build())
         .withProperties(properties)
+        .withEmbeddedTimelineServerEnabled(false)
         .build();
 
     // With next commit the table should be re-bootstrapped and partial commit should be rolled back.

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -2015,7 +2015,7 @@ public void testUpgradeDowngrade() throws IOException {
     assertTrue(currentStatus.getModificationTime() > prevStatus.getModificationTime());
 
     initMetaClient();
-    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.FOUR.versionCode());
+    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.FIVE.versionCode());
     assertTrue(fs.exists(new Path(metadataTableBasePath)), "Metadata table should exist");
     FileStatus newStatus = fs.getFileStatus(new Path(metadataTableBasePath));
     assertTrue(oldStatus.getModificationTime() < newStatus.getModificationTime());
@@ -2095,7 +2095,7 @@ public void testRollbackDuringUpgradeForDoubleLocking() throws IOException, Inte
     }
 
     initMetaClient();
-    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.FOUR.versionCode());
+    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.FIVE.versionCode());
     assertTrue(fs.exists(new Path(metadataTableBasePath)), "Metadata table should exist");
     FileStatus newStatus = fs.getFileStatus(new Path(metadataTableBasePath));
     assertTrue(oldStatus.getModificationTime() < newStatus.getModificationTime());

File: hudi-common/src/main/java/org/apache/hudi/common/util/PartitionPathEncodeUtils.java
Patch:
@@ -25,6 +25,7 @@
  */
 public class PartitionPathEncodeUtils {
 
+  public static final String DEPRECATED_DEFAULT_PARTITION_PATH = "default";
   public static final String DEFAULT_PARTITION_PATH = "__HIVE_DEFAULT_PARTITION__";
 
   static BitSet charToEscape = new BitSet(128);

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java
Patch:
@@ -92,7 +92,8 @@ public class ITTestHoodieDemo extends ITTestBase {
   private HoodieFileFormat baseFileFormat;
 
   private static String HIVE_SYNC_CMD_FMT =
-      " --enable-hive-sync --hoodie-conf hoodie.datasource.hive_sync.jdbcurl=jdbc:hive2://hiveserver:10000 "
+      " --enable-hive-sync --hoodie-conf hoodie.datasource.hive_sync.jdbcurl=jdbc:hive2://hiveserver:10000/ "
+          + " --hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor "
           + " --hoodie-conf hoodie.datasource.hive_sync.username=hive "
           + " --hoodie-conf hoodie.datasource.hive_sync.password=hive "
           + " --hoodie-conf hoodie.datasource.hive_sync.partition_fields=%s "
@@ -215,6 +216,7 @@ private void ingestFirstBatchAndHiveSync() throws Exception {
             + " --user hive"
             + " --pass hive"
             + " --jdbc-url jdbc:hive2://hiveserver:10000"
+            + " --partition-value-extractor org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor"
             + " --partitioned-by dt",
         ("spark-submit"
             + " --conf \'spark.executor.extraJavaOptions=-Dlog4jspark.root.logger=WARN,console\'"

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java
Patch:
@@ -91,7 +91,7 @@ public HiveSyncTool(Properties props, Configuration hadoopConf) {
     HiveSyncConfig config = new HiveSyncConfig(props, hadoopConf);
     this.config = config;
     this.databaseName = config.getStringOrDefault(META_SYNC_DATABASE_NAME);
-    this.tableName = config.getString(META_SYNC_TABLE_NAME);
+    this.tableName = config.getStringOrDefault(META_SYNC_TABLE_NAME);
     initSyncClient(config);
     initTableNameVars(config);
   }
@@ -109,6 +109,7 @@ protected void initSyncClient(HiveSyncConfig config) {
   }
 
   private void initTableNameVars(HiveSyncConfig config) {
+    final String tableName = config.getStringOrDefault(META_SYNC_TABLE_NAME);
     if (syncClient != null) {
       switch (syncClient.getTableType()) {
         case COPY_ON_WRITE:

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java
Patch:
@@ -47,6 +47,7 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.hive.HiveSyncConfig;
 import org.apache.hudi.hive.HiveSyncTool;
+import org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor;
 import org.apache.hudi.hive.ddl.HiveQueryDDLExecutor;
 import org.apache.hudi.hive.ddl.QueryBasedDDLExecutor;
 
@@ -92,6 +93,7 @@
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_ASSUME_DATE_PARTITION;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_BASE_PATH;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_DATABASE_NAME;
+import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_PARTITION_EXTRACTOR_CLASS;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_PARTITION_FIELDS;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_TABLE_NAME;
 import static org.junit.jupiter.api.Assertions.fail;
@@ -138,6 +140,7 @@ public static void setUp() throws IOException, InterruptedException, HiveExcepti
     hiveSyncProps.setProperty(META_SYNC_ASSUME_DATE_PARTITION.key(), "true");
     hiveSyncProps.setProperty(HIVE_USE_PRE_APACHE_INPUT_FORMAT.key(), "false");
     hiveSyncProps.setProperty(META_SYNC_PARTITION_FIELDS.key(), "datestr");
+    hiveSyncProps.setProperty(META_SYNC_PARTITION_EXTRACTOR_CLASS.key(), SlashEncodedDayPartitionValueExtractor.class.getName());
     hiveSyncProps.setProperty(HIVE_BATCH_SYNC_PARTITION_NUM.key(), "3");
 
     hiveSyncConfig = new HiveSyncConfig(hiveSyncProps, hiveTestService.getHiveConf());

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -705,6 +705,7 @@ public void runMetaSync() {
 
       TypedProperties metaProps = new TypedProperties();
       metaProps.putAll(props);
+      metaProps.putAll(writeClient.getConfig().getProps());
       if (props.getBoolean(HIVE_SYNC_BUCKET_SYNC.key(), HIVE_SYNC_BUCKET_SYNC.defaultValue())) {
         metaProps.put(HIVE_SYNC_BUCKET_SYNC_SPEC.key(), HiveSyncConfig.getBucketSpec(props.getString(HoodieIndexConfig.BUCKET_INDEX_HASH_FIELD.key()),
             props.getInteger(HoodieIndexConfig.BUCKET_INDEX_NUM_BUCKETS.key())));

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bulk/sort/SortOperatorGen.java
Patch:
@@ -26,7 +26,6 @@
 import org.apache.flink.table.types.logical.RowType;
 
 import java.util.Arrays;
-import java.util.stream.IntStream;
 
 /**
  * Tools to generate the sort operator.
@@ -50,7 +49,9 @@ public OneInputStreamOperator<RowData, RowData> createSortOperator() {
 
   public SortCodeGenerator createSortCodeGenerator() {
     SortSpec.SortSpecBuilder builder = SortSpec.builder();
-    IntStream.range(0, sortIndices.length).forEach(i -> builder.addField(i, true, true));
+    for (int sortIndex : sortIndices) {
+      builder.addField(sortIndex, true, true);
+    }
     return new SortCodeGenerator(tableConfig, rowType, builder.build());
   }
 }

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/HoodieSyncConfig.java
Patch:
@@ -86,7 +86,7 @@ public class HoodieSyncConfig extends HoodieConfig {
       .key("hoodie.datasource.hive_sync.partition_extractor_class")
       .defaultValue("org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor")
       .withInferFunction(cfg -> {
-        if (cfg.contains(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME)) {
+        if (StringUtils.nonEmpty(cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME))) {
           int numOfPartFields = cfg.getString(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME).split(",").length;
           if (numOfPartFields == 1
               && cfg.contains(KeyGeneratorOptions.HIVE_STYLE_PARTITIONING_ENABLE)

File: hudi-common/src/main/java/org/apache/hudi/common/util/DefaultSizeEstimator.java
Patch:
@@ -18,12 +18,14 @@
 
 package org.apache.hudi.common.util;
 
+import java.io.Serializable;
+
 /**
  * Default implementation of size-estimator that uses Twitter's ObjectSizeCalculator.
  * 
  * @param <T>
  */
-public class DefaultSizeEstimator<T> implements SizeEstimator<T> {
+public class DefaultSizeEstimator<T> implements SizeEstimator<T>, Serializable {
 
   @Override
   public long sizeEstimate(T t) {

File: hudi-common/src/main/java/org/apache/hudi/common/util/HoodieRecordSizeEstimator.java
Patch:
@@ -26,12 +26,14 @@
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
+import java.io.Serializable;
+
 /**
  * Size Estimator for Hoodie record payload.
  * 
  * @param <T>
  */
-public class HoodieRecordSizeEstimator<T extends HoodieRecordPayload> implements SizeEstimator<HoodieRecord<T>> {
+public class HoodieRecordSizeEstimator<T extends HoodieRecordPayload> implements SizeEstimator<HoodieRecord<T>>, Serializable {
 
   private static final Logger LOG = LogManager.getLogger(HoodieRecordSizeEstimator.class);
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java
Patch:
@@ -52,7 +52,7 @@
  * map may occupy more memory than is available, resulting in OOM. However, if the spill threshold is too low, we spill
  * frequently and incur unnecessary disk writes.
  */
-public class ExternalSpillableMap<T extends Serializable, R extends Serializable> implements Map<T, R> {
+public class ExternalSpillableMap<T extends Serializable, R extends Serializable> implements Map<T, R>, Serializable {
 
   // Find the actual estimated payload size after inserting N records
   private static final int NUMBER_OF_RECORDS_TO_ESTIMATE_PAYLOAD_SIZE = 100;

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDBBasedMap.java
Patch:
@@ -29,12 +29,12 @@
 /**
  * A map's implementation based on RocksDB.
  */
-public final class RocksDBBasedMap<K extends Serializable, R extends Serializable> implements Map<K, R> {
+public final class RocksDBBasedMap<K extends Serializable, R extends Serializable> implements Map<K, R>, Serializable {
 
   private static final String COL_FAMILY_NAME = "map_handle";
 
   private final String rocksDbStoragePath;
-  private RocksDBDAO rocksDBDAO;
+  private transient RocksDBDAO rocksDBDAO;
   private final String columnFamilyName;
 
   public RocksDBBasedMap(String rocksDbStoragePath) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackStrategy.java
Patch:
@@ -242,11 +242,11 @@ private FileStatus[] fetchFilesFromCommitMetadata(HoodieInstant instantToRollbac
     return fs.listStatus(Arrays.stream(filePaths).filter(entry -> {
       try {
         return fs.exists(entry);
-      } catch (IOException e) {
+      } catch (Exception e) {
         LOG.error("Exists check failed for " + entry.toString(), e);
       }
-      // if IOException is thrown, do not ignore. lets try to add the file of interest to be deleted. we can't miss any files to be rolled back.
-      return false;
+      // if any Exception is thrown, do not ignore. let's try to add the file of interest to be deleted. we can't miss any files to be rolled back.
+      return true;
     }).toArray(Path[]::new), pathFilter);
   }
 

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/transaction/lock/HiveMetastoreBasedLockProvider.java
Patch:
@@ -149,6 +149,7 @@ public void close() {
     try {
       if (lock != null) {
         hiveClient.unlock(lock.getLockid());
+        lock = null;
       }
       Hive.closeCurrent();
     } catch (Exception e) {
@@ -197,6 +198,7 @@ private void acquireLockInternal(long time, TimeUnit unit, LockComponent lockCom
       // it is better to release WAITING lock, otherwise hive lock will hang forever
       if (this.lock != null && this.lock.getState() != LockState.ACQUIRED) {
         hiveClient.unlock(this.lock.getLockid());
+        lock = null;
       }
     }
   }

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -104,8 +104,8 @@ private FlinkOptions() {
   public static final ConfigOption<Boolean> METADATA_ENABLED = ConfigOptions
       .key("metadata.enabled")
       .booleanType()
-      .defaultValue(true)
-      .withDescription("Enable the internal metadata table which serves table metadata like level file listings, default enabled");
+      .defaultValue(false)
+      .withDescription("Enable the internal metadata table which serves table metadata like level file listings, default disabled");
 
   public static final ConfigOption<Integer> METADATA_COMPACTION_DELTA_COMMITS = ConfigOptions
       .key("metadata.compaction.delta_commits")

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java
Patch:
@@ -225,6 +225,7 @@ public static HoodieWriteConfig getHoodieClientConfig(
                 .withLockProvider(FileSystemBasedLockProvider.class)
                 .withLockWaitTimeInMillis(2000L) // 2s
                 .withFileSystemLockExpire(1) // 1 minute
+                .withClientNumRetries(30)
                 .withFileSystemLockPath(StreamerUtil.getAuxiliaryPath(conf))
                 .build())
             .withPayloadConfig(HoodiePayloadConfig.newBuilder()

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SingleSparkJobExecutionStrategy.java
Patch:
@@ -136,7 +136,7 @@ private Stream<WriteStatus> runClusteringForGroup(ClusteringGroupInfo clustering
   /**
    * Execute clustering to write inputRecords into new files as defined by rules in strategy parameters.
    * The number of new file groups created is bounded by numOutputGroups.
-   * Note that commit is not done as part of strategy. commit is callers responsibility.
+   * Note that commit is not done as part of strategy. Commit is callers responsibility.
    */
   public abstract Iterator<List<WriteStatus>> performClusteringWithRecordsIterator(final Iterator<HoodieRecord<T>> records, final int numOutputGroups,
                                                                                    final String instantTime,

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java
Patch:
@@ -81,7 +81,7 @@ private SparkInternalSchemaConverter() {
   public static final String HOODIE_VALID_COMMITS_LIST = "hoodie.valid.commits.list";
 
   /**
-   * Converts a spark schema to an hudi internal schema. Fields without IDs are kept and assigned fallback IDs.
+   * Convert a spark schema to an hudi internal schema. Fields without IDs are kept and assigned fallback IDs.
    *
    * @param sparkSchema a spark schema
    * @return a matching internal schema for the provided spark schema
@@ -157,7 +157,7 @@ public static Type buildTypeFromStructType(DataType sparkType, Boolean firstVisi
   }
 
   /**
-   * Converts Spark schema to Hudi internal schema, and prune fields.
+   * Convert Spark schema to Hudi internal schema, and prune fields.
    * Fields without IDs are kept and assigned fallback IDs.
    *
    * @param sparkSchema a pruned spark schema

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkValidatorUtils.java
Patch:
@@ -50,7 +50,7 @@
 import scala.collection.JavaConverters;
 
 /**
- * Spark validator utils to verify and run any precommit validators configured.
+ * Spark validator utils to verify and run any pre-commit validators configured.
  */
 public class SparkValidatorUtils {
   private static final Logger LOG = LogManager.getLogger(BaseSparkCommitActionExecutor.class);

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java
Patch:
@@ -308,7 +308,7 @@ public byte[] getBytes(Schema schema) throws IOException {
     ByteArrayOutputStream baos = new ByteArrayOutputStream();
     DataOutputStream output = new DataOutputStream(baos);
 
-    // 2. Compress and Write schema out
+    // 1. Compress and Write schema out
     byte[] schemaContent = compress(schema.toString());
     output.writeInt(schemaContent.length);
     output.write(schemaContent);
@@ -318,10 +318,10 @@ public byte[] getBytes(Schema schema) throws IOException {
       recordItr.forEachRemaining(records::add);
     }
 
-    // 3. Write total number of records
+    // 2. Write total number of records
     output.writeInt(records.size());
 
-    // 4. Write the records
+    // 3. Write the records
     Iterator<IndexedRecord> itr = records.iterator();
     while (itr.hasNext()) {
       IndexedRecord s = itr.next();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -311,7 +311,7 @@ public class HoodieWriteConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> BULK_INSERT_SORT_MODE = ConfigProperty
       .key("hoodie.bulkinsert.sort.mode")
-      .defaultValue(BulkInsertSortMode.GLOBAL_SORT.toString())
+      .defaultValue(BulkInsertSortMode.NONE.toString())
       .withDocumentation("Sorting modes to use for sorting records for bulk insert. This is use when user "
           + BULKINSERT_USER_DEFINED_PARTITIONER_CLASS_NAME.key() + "is not configured. Available values are - "
           + "GLOBAL_SORT: this ensures best file sizes, with lowest memory overhead at cost of sorting. "

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
Patch:
@@ -107,7 +107,7 @@ private static Iterable<Object[]> syncModeAndSchemaFromCommitMetadata() {
   private HoodieHiveSyncClient hiveClient;
 
   @AfterAll
-  public static void cleanUpClass() {
+  public static void cleanUpClass() throws IOException {
     HiveTestUtil.shutdown();
   }
 

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/replication/TestHiveSyncGlobalCommitTool.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hadoop.fs.Path;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
-import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.extension.RegisterExtension;
 
@@ -49,7 +48,6 @@
 import static org.junit.jupiter.api.Assertions.assertNotEquals;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
-@Disabled
 public class TestHiveSyncGlobalCommitTool {
 
   @RegisterExtension

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java
Patch:
@@ -141,7 +141,7 @@ public static void setUp() throws IOException, InterruptedException, HiveExcepti
     hiveSyncProps.setProperty(META_SYNC_PARTITION_FIELDS.key(), "datestr");
     hiveSyncProps.setProperty(HIVE_BATCH_SYNC_PARTITION_NUM.key(), "3");
 
-    hiveSyncConfig = new HiveSyncConfig(hiveSyncProps, configuration);
+    hiveSyncConfig = new HiveSyncConfig(hiveSyncProps, getHiveConf());
 
     dtfOut = DateTimeFormatter.ofPattern("yyyy/MM/dd");
     ddlExecutor = new HiveQueryDDLExecutor(hiveSyncConfig);
@@ -176,7 +176,7 @@ public static HiveConf getHiveConf() {
     return hiveServer.getHiveConf();
   }
 
-  public static void shutdown() {
+  public static void shutdown() throws IOException {
     if (hiveServer != null) {
       hiveServer.stop();
     }
@@ -186,6 +186,7 @@ public static void shutdown() {
     if (zkServer != null) {
       zkServer.shutdown();
     }
+    FileSystem.closeAll();
   }
 
   public static void createCOWTable(String instantTime, int numberOfPartitions, boolean useSchemaFromCommitMetadata,

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/TestCluster.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.hive.testutils;
 
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hudi.avro.HoodieAvroWriteSupport;
 import org.apache.hudi.common.bloom.BloomFilter;
 import org.apache.hudi.common.bloom.BloomFilterFactory;
@@ -264,10 +265,11 @@ public void startHiveServer2() {
     }
   }
 
-  public void shutDown() {
+  public void shutDown() throws IOException {
     stopHiveServer2();
     Hive.closeCurrent();
     hiveTestService.getHiveMetaStore().stop();
     hdfsTestService.stop();
+    FileSystem.closeAll();
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/common/util/PartitionPathEncodeUtils.java
Patch:
@@ -25,7 +25,7 @@
  */
 public class PartitionPathEncodeUtils {
 
-  public static final String DEFAULT_PARTITION_PATH = "default";
+  public static final String DEFAULT_PARTITION_PATH = "__HIVE_DEFAULT_PARTITION__";
 
   static BitSet charToEscape = new BitSet(128);
   static {

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -45,6 +45,7 @@
 import java.util.Map;
 import java.util.Set;
 
+import static org.apache.hudi.common.util.PartitionPathEncodeUtils.DEFAULT_PARTITION_PATH;
 import static org.apache.hudi.config.HoodieClusteringConfig.DAYBASED_LOOKBACK_PARTITIONS;
 import static org.apache.hudi.config.HoodieClusteringConfig.PARTITION_FILTER_BEGIN_PARTITION;
 import static org.apache.hudi.config.HoodieClusteringConfig.PARTITION_FILTER_END_PARTITION;
@@ -81,7 +82,7 @@ private FlinkOptions() {
   public static final ConfigOption<String> PARTITION_DEFAULT_NAME = ConfigOptions
       .key("partition.default_name")
       .stringType()
-      .defaultValue("default") // keep sync with hoodie style
+      .defaultValue(DEFAULT_PARTITION_PATH) // keep sync with hoodie style
       .withDescription("The default partition name in case the dynamic partition"
           + " column value is null/empty string");
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/streamer/FlinkStreamerConfig.java
Patch:
@@ -39,6 +39,7 @@
 import java.util.List;
 import java.util.Map;
 
+import static org.apache.hudi.common.util.PartitionPathEncodeUtils.DEFAULT_PARTITION_PATH;
 import static org.apache.hudi.configuration.FlinkOptions.PARTITION_FORMAT_DAY;
 
 /**
@@ -178,7 +179,7 @@ public class FlinkStreamerConfig extends Configuration {
 
   @Parameter(names = {"--partition-default-name"},
       description = "The default partition name in case the dynamic partition column value is null/empty string")
-  public String partitionDefaultName = "default";
+  public String partitionDefaultName = DEFAULT_PARTITION_PATH;
 
   @Parameter(names = {"--index-bootstrap-enabled"},
       description = "Whether to bootstrap the index state from existing hoodie table, default false")

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/job/TestHoodieTestSuiteJob.java
Patch:
@@ -55,7 +55,6 @@
 import java.util.stream.Stream;
 
 import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_URL;
-import static org.apache.hudi.hive.testutils.HiveTestService.HS2_JDBC_URL;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_DATABASE_NAME;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_PARTITION_FIELDS;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_TABLE_NAME;
@@ -181,7 +180,7 @@ private static TypedProperties getProperties() {
     // Make path selection test suite specific
     props.setProperty("hoodie.deltastreamer.source.input.selector", DFSTestSuitePathSelector.class.getName());
     // Hive Configs
-    props.setProperty(HIVE_URL.key(), HS2_JDBC_URL);
+    props.setProperty(HIVE_URL.key(), "jdbc:hive2://127.0.0.1:9999/");
     props.setProperty(META_SYNC_DATABASE_NAME.key(), "testdb1");
     props.setProperty(META_SYNC_TABLE_NAME.key(), "table1");
     props.setProperty(META_SYNC_PARTITION_FIELDS.key(), "datestr");

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java
Patch:
@@ -63,8 +63,7 @@ public HiveSyncConfig(Properties props) {
 
   public HiveSyncConfig(Properties props, Configuration hadoopConf) {
     super(props, hadoopConf);
-    HiveConf hiveConf = hadoopConf instanceof HiveConf
-        ? (HiveConf) hadoopConf : new HiveConf(hadoopConf, HiveConf.class);
+    HiveConf hiveConf = new HiveConf(hadoopConf, HiveConf.class);
     // HiveConf needs to load fs conf to allow instantiation via AWSGlueClientFactory
     hiveConf.addResource(getHadoopFileSystem().getConf());
     setHadoopConf(hiveConf);

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/replication/TestHiveSyncGlobalCommitTool.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.apache.hudi.hive.replication;
 
-import org.apache.hudi.hive.testutils.HiveTestCluster;
+import org.apache.hudi.hive.testutils.TestCluster;
 
 import org.apache.hadoop.fs.Path;
 import org.junit.jupiter.api.AfterEach;
@@ -53,9 +53,9 @@
 public class TestHiveSyncGlobalCommitTool {
 
   @RegisterExtension
-  public static HiveTestCluster localCluster = new HiveTestCluster();
+  public static TestCluster localCluster = new TestCluster();
   @RegisterExtension
-  public static HiveTestCluster remoteCluster = new HiveTestCluster();
+  public static TestCluster remoteCluster = new TestCluster();
 
   private static final String DB_NAME = "foo";
   private static final String TBL_NAME = "bar";

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java
Patch:
@@ -125,6 +125,7 @@ public static void setUp() throws IOException, InterruptedException, HiveExcepti
       hiveTestService = new HiveTestService(configuration);
       hiveServer = hiveTestService.start();
     }
+    fileSystem = FileSystem.get(configuration);
 
     basePath = Files.createTempDirectory("hivesynctest" + Instant.now().toEpochMilli()).toUri().toString();
 
@@ -140,8 +141,7 @@ public static void setUp() throws IOException, InterruptedException, HiveExcepti
     hiveSyncProps.setProperty(META_SYNC_PARTITION_FIELDS.key(), "datestr");
     hiveSyncProps.setProperty(HIVE_BATCH_SYNC_PARTITION_NUM.key(), "3");
 
-    hiveSyncConfig = new HiveSyncConfig(hiveSyncProps, hiveTestService.getHiveConf());
-    fileSystem = hiveSyncConfig.getHadoopFileSystem();
+    hiveSyncConfig = new HiveSyncConfig(hiveSyncProps, configuration);
 
     dtfOut = DateTimeFormatter.ofPattern("yyyy/MM/dd");
     ddlExecutor = new HiveQueryDDLExecutor(hiveSyncConfig);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/HoodieDeltaStreamerTestBase.java
Patch:
@@ -48,7 +48,6 @@
 import java.util.Random;
 
 import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_URL;
-import static org.apache.hudi.hive.testutils.HiveTestService.HS2_JDBC_URL;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_ASSUME_DATE_PARTITION;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_DATABASE_NAME;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_PARTITION_EXTRACTOR_CLASS;
@@ -187,7 +186,7 @@ protected static void writeCommonPropsToFile(FileSystem dfs, String dfsBasePath)
     props.setProperty("hoodie.deltastreamer.schemaprovider.target.schema.file", dfsBasePath + "/target.avsc");
 
     // Hive Configs
-    props.setProperty(HIVE_URL.key(), HS2_JDBC_URL);
+    props.setProperty(HIVE_URL.key(), "jdbc:hive2://127.0.0.1:9999/");
     props.setProperty(META_SYNC_DATABASE_NAME.key(), "testdb1");
     props.setProperty(META_SYNC_TABLE_NAME.key(), "hive_trips");
     props.setProperty(META_SYNC_PARTITION_FIELDS.key(), "datestr");
@@ -247,7 +246,7 @@ protected static void populateCommonKafkaProps(TypedProperties props, String bro
 
   protected static void populateCommonHiveProps(TypedProperties props) {
     // Hive Configs
-    props.setProperty(HIVE_URL.key(), HS2_JDBC_URL);
+    props.setProperty(HIVE_URL.key(), "jdbc:hive2://127.0.0.1:9999/");
     props.setProperty(META_SYNC_DATABASE_NAME.key(), "testdb2");
     props.setProperty(META_SYNC_ASSUME_DATE_PARTITION.key(), "false");
     props.setProperty(META_SYNC_PARTITION_FIELDS.key(), "datestr");

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java
Patch:
@@ -1359,7 +1359,7 @@ public void testBulkInsertsAndUpsertsWithSQLBasedTransformerFor2StepPipeline() t
     // Test Hive integration
     HiveSyncConfig hiveSyncConfig = getHiveSyncConfig(tableBasePath, "hive_trips");
     hiveSyncConfig.setValue(META_SYNC_PARTITION_FIELDS, "year,month,day");
-    hiveSyncConfig.setHadoopConf(hiveTestService.getHiveConf());
+    hiveSyncConfig.setHadoopConf(hiveServer.getHiveConf());
     HoodieHiveSyncClient hiveClient = new HoodieHiveSyncClient(hiveSyncConfig);
     final String tableName = hiveSyncConfig.getString(META_SYNC_TABLE_NAME);
     assertTrue(hiveClient.tableExists(tableName), "Table " + tableName + " should exist");

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/job/TestHoodieTestSuiteJob.java
Patch:
@@ -55,6 +55,7 @@
 import java.util.stream.Stream;
 
 import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_URL;
+import static org.apache.hudi.hive.testutils.HiveTestService.HS2_JDBC_URL;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_DATABASE_NAME;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_PARTITION_FIELDS;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_TABLE_NAME;
@@ -180,7 +181,7 @@ private static TypedProperties getProperties() {
     // Make path selection test suite specific
     props.setProperty("hoodie.deltastreamer.source.input.selector", DFSTestSuitePathSelector.class.getName());
     // Hive Configs
-    props.setProperty(HIVE_URL.key(), "jdbc:hive2://127.0.0.1:9999/");
+    props.setProperty(HIVE_URL.key(), HS2_JDBC_URL);
     props.setProperty(META_SYNC_DATABASE_NAME.key(), "testdb1");
     props.setProperty(META_SYNC_TABLE_NAME.key(), "table1");
     props.setProperty(META_SYNC_PARTITION_FIELDS.key(), "datestr");

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java
Patch:
@@ -63,7 +63,8 @@ public HiveSyncConfig(Properties props) {
 
   public HiveSyncConfig(Properties props, Configuration hadoopConf) {
     super(props, hadoopConf);
-    HiveConf hiveConf = new HiveConf(hadoopConf, HiveConf.class);
+    HiveConf hiveConf = hadoopConf instanceof HiveConf
+        ? (HiveConf) hadoopConf : new HiveConf(hadoopConf, HiveConf.class);
     // HiveConf needs to load fs conf to allow instantiation via AWSGlueClientFactory
     hiveConf.addResource(getHadoopFileSystem().getConf());
     setHadoopConf(hiveConf);

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/replication/TestHiveSyncGlobalCommitTool.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.apache.hudi.hive.replication;
 
-import org.apache.hudi.hive.testutils.TestCluster;
+import org.apache.hudi.hive.testutils.HiveTestCluster;
 
 import org.apache.hadoop.fs.Path;
 import org.junit.jupiter.api.AfterEach;
@@ -53,9 +53,9 @@
 public class TestHiveSyncGlobalCommitTool {
 
   @RegisterExtension
-  public static TestCluster localCluster = new TestCluster();
+  public static HiveTestCluster localCluster = new HiveTestCluster();
   @RegisterExtension
-  public static TestCluster remoteCluster = new TestCluster();
+  public static HiveTestCluster remoteCluster = new HiveTestCluster();
 
   private static final String DB_NAME = "foo";
   private static final String TBL_NAME = "bar";

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java
Patch:
@@ -125,7 +125,6 @@ public static void setUp() throws IOException, InterruptedException, HiveExcepti
       hiveTestService = new HiveTestService(configuration);
       hiveServer = hiveTestService.start();
     }
-    fileSystem = FileSystem.get(configuration);
 
     basePath = Files.createTempDirectory("hivesynctest" + Instant.now().toEpochMilli()).toUri().toString();
 
@@ -141,7 +140,8 @@ public static void setUp() throws IOException, InterruptedException, HiveExcepti
     hiveSyncProps.setProperty(META_SYNC_PARTITION_FIELDS.key(), "datestr");
     hiveSyncProps.setProperty(HIVE_BATCH_SYNC_PARTITION_NUM.key(), "3");
 
-    hiveSyncConfig = new HiveSyncConfig(hiveSyncProps, configuration);
+    hiveSyncConfig = new HiveSyncConfig(hiveSyncProps, hiveTestService.getHiveConf());
+    fileSystem = hiveSyncConfig.getHadoopFileSystem();
 
     dtfOut = DateTimeFormatter.ofPattern("yyyy/MM/dd");
     ddlExecutor = new HiveQueryDDLExecutor(hiveSyncConfig);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/HoodieDeltaStreamerTestBase.java
Patch:
@@ -48,6 +48,7 @@
 import java.util.Random;
 
 import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_URL;
+import static org.apache.hudi.hive.testutils.HiveTestService.HS2_JDBC_URL;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_ASSUME_DATE_PARTITION;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_DATABASE_NAME;
 import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_PARTITION_EXTRACTOR_CLASS;
@@ -186,7 +187,7 @@ protected static void writeCommonPropsToFile(FileSystem dfs, String dfsBasePath)
     props.setProperty("hoodie.deltastreamer.schemaprovider.target.schema.file", dfsBasePath + "/target.avsc");
 
     // Hive Configs
-    props.setProperty(HIVE_URL.key(), "jdbc:hive2://127.0.0.1:9999/");
+    props.setProperty(HIVE_URL.key(), HS2_JDBC_URL);
     props.setProperty(META_SYNC_DATABASE_NAME.key(), "testdb1");
     props.setProperty(META_SYNC_TABLE_NAME.key(), "hive_trips");
     props.setProperty(META_SYNC_PARTITION_FIELDS.key(), "datestr");
@@ -246,7 +247,7 @@ protected static void populateCommonKafkaProps(TypedProperties props, String bro
 
   protected static void populateCommonHiveProps(TypedProperties props) {
     // Hive Configs
-    props.setProperty(HIVE_URL.key(), "jdbc:hive2://127.0.0.1:9999/");
+    props.setProperty(HIVE_URL.key(), HS2_JDBC_URL);
     props.setProperty(META_SYNC_DATABASE_NAME.key(), "testdb2");
     props.setProperty(META_SYNC_ASSUME_DATE_PARTITION.key(), "false");
     props.setProperty(META_SYNC_PARTITION_FIELDS.key(), "datestr");

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java
Patch:
@@ -1359,7 +1359,7 @@ public void testBulkInsertsAndUpsertsWithSQLBasedTransformerFor2StepPipeline() t
     // Test Hive integration
     HiveSyncConfig hiveSyncConfig = getHiveSyncConfig(tableBasePath, "hive_trips");
     hiveSyncConfig.setValue(META_SYNC_PARTITION_FIELDS, "year,month,day");
-    hiveSyncConfig.setHadoopConf(hiveServer.getHiveConf());
+    hiveSyncConfig.setHadoopConf(hiveTestService.getHiveConf());
     HoodieHiveSyncClient hiveClient = new HoodieHiveSyncClient(hiveSyncConfig);
     final String tableName = hiveSyncConfig.getString(META_SYNC_TABLE_NAME);
     assertTrue(hiveClient.tableExists(tableName), "Table " + tableName + " should exist");

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -594,11 +594,11 @@ public String getKeyGeneratorClassName() {
   }
 
   public String getHiveStylePartitioningEnable() {
-    return getString(HIVE_STYLE_PARTITIONING_ENABLE);
+    return getStringOrDefault(HIVE_STYLE_PARTITIONING_ENABLE);
   }
 
   public String getUrlEncodePartitioning() {
-    return getString(URL_ENCODE_PARTITIONING);
+    return getStringOrDefault(URL_ENCODE_PARTITIONING);
   }
 
   public Boolean shouldDropPartitionColumns() {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -491,7 +491,7 @@ public abstract O bulkInsertPreppedRecords(I preppedRecords, final String instan
    * @param writeOperationType
    * @param metaClient
    */
-  protected void preWrite(String instantTime, WriteOperationType writeOperationType,
+  public void preWrite(String instantTime, WriteOperationType writeOperationType,
       HoodieTableMetaClient metaClient) {
     setOperationType(writeOperationType);
     this.lastCompletedTxnAndMetadata = TransactionUtils.getLastCompletedTxnInstantAndMetadata(metaClient);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -254,7 +254,7 @@ public List<WriteStatus> delete(List<HoodieKey> keys, String instantTime) {
   }
 
   @Override
-  protected void preWrite(String instantTime, WriteOperationType writeOperationType, HoodieTableMetaClient metaClient) {
+  public void preWrite(String instantTime, WriteOperationType writeOperationType, HoodieTableMetaClient metaClient) {
     setOperationType(writeOperationType);
     // Note: the code to read the commit metadata is not thread safe for JSON deserialization,
     // remove the table metadata sync

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/DataSourceInternalWriterHelper.java
Patch:
@@ -68,6 +68,7 @@ public DataSourceInternalWriterHelper(String instantTime, HoodieWriteConfig writ
     this.metaClient = HoodieTableMetaClient.builder().setConf(configuration).setBasePath(writeConfig.getBasePath()).build();
     this.metaClient.validateTableProperties(writeConfig.getProps());
     this.hoodieTable = HoodieSparkTable.create(writeConfig, new HoodieSparkEngineContext(new JavaSparkContext(sparkSession.sparkContext())), metaClient);
+    writeClient.preWrite(instantTime, WriteOperationType.BULK_INSERT, metaClient);
   }
 
   public boolean useCommitCoordinator() {

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestTable.java
Patch:
@@ -52,6 +52,7 @@
 import org.apache.hudi.common.table.timeline.versioning.clean.CleanPlanV2MigrationHandler;
 import org.apache.hudi.common.util.CompactionUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieIOException;
@@ -1060,7 +1061,7 @@ public static List<HoodieWriteStat> generateHoodieWriteStatForPartition(Map<Stri
             FileCreateUtils.baseFileName(commitTime, fileIdInfo.getKey());
         writeStat.setFileId(fileName);
         writeStat.setPartitionPath(partition);
-        writeStat.setPath(partition + "/" + fileName);
+        writeStat.setPath(StringUtils.isNullOrEmpty(partition) ? fileName : partition + "/" + fileName);
         writeStat.setTotalWriteBytes(fileIdInfo.getValue());
         writeStat.setFileSizeInBytes(fileIdInfo.getValue());
         writeStats.add(writeStat);
@@ -1086,7 +1087,7 @@ private static List<HoodieWriteStat> generateHoodieWriteStatForPartitionLogFiles
             FileCreateUtils.logFileName(commitTime, fileIdInfo.getKey(), fileIdInfo.getValue()[0]);
         writeStat.setFileId(fileName);
         writeStat.setPartitionPath(partition);
-        writeStat.setPath(partition + "/" + fileName);
+        writeStat.setPath(StringUtils.isNullOrEmpty(partition) ? fileName : partition + "/" + fileName);
         writeStat.setTotalWriteBytes(fileIdInfo.getValue()[1]);
         writeStat.setFileSizeInBytes(fileIdInfo.getValue()[1]);
         writeStats.add(writeStat);

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -441,9 +441,9 @@ public HoodieMetadataConfig build() {
 
     private boolean getDefaultMetadataEnable(EngineType engineType) {
       switch (engineType) {
+        case FLINK:
         case SPARK:
           return ENABLE.defaultValue();
-        case FLINK:
         case JAVA:
           return false;
         default:

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -103,8 +103,8 @@ private FlinkOptions() {
   public static final ConfigOption<Boolean> METADATA_ENABLED = ConfigOptions
       .key("metadata.enabled")
       .booleanType()
-      .defaultValue(false)
-      .withDescription("Enable the internal metadata table which serves table metadata like level file listings, default false");
+      .defaultValue(true)
+      .withDescription("Enable the internal metadata table which serves table metadata like level file listings, default enabled");
 
   public static final ConfigOption<Integer> METADATA_COMPACTION_DELTA_COMMITS = ConfigOptions
       .key("metadata.compaction.delta_commits")

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/stats/ExpressionEvaluator.java
Patch:
@@ -87,8 +87,8 @@ public abstract static class Evaluator {
      * 2. bind the field reference;
      * 3. bind the column stats.
      *
-     * <p>Normalize the expression to simplify the following decision logic:
-     * always put the literal expression in the right.
+     * <p>Normalize the expression to simplify the subsequent decision logic:
+     * always put the literal expression in the RHS.
      */
     public static Evaluator bindCall(CallExpression call, RowData indexRow, RowType.RowField[] queryFields) {
       FunctionDefinition funDef = call.getFunctionDefinition();

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/format/TestInputFormat.java
Patch:
@@ -480,6 +480,8 @@ void testReadArchivedCommitsIncrementally() throws Exception {
     options.put(FlinkOptions.ARCHIVE_MIN_COMMITS.key(), "3");
     options.put(FlinkOptions.ARCHIVE_MAX_COMMITS.key(), "4");
     options.put(FlinkOptions.CLEAN_RETAIN_COMMITS.key(), "2");
+    // disable the metadata table to make the archiving behavior deterministic
+    options.put(FlinkOptions.METADATA_ENABLED.key(), "false");
     options.put("hoodie.commits.archival.batch", "1");
     beforeEach(HoodieTableType.COPY_ON_WRITE, options);
 

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieCatalogFactory.java
Patch:
@@ -34,7 +34,6 @@
 import java.util.Set;
 
 import static org.apache.flink.table.factories.FactoryUtil.PROPERTY_VERSION;
-import static org.apache.hudi.table.catalog.CatalogOptions.CATALOG_PATH;
 
 /**
  * A catalog factory impl that creates {@link HoodieCatalog}.
@@ -59,6 +58,7 @@ public Catalog createCatalog(Context context) {
       case "hms":
         return new HoodieHiveCatalog(
             context.getName(),
+            helper.getOptions().get(CatalogOptions.CATALOG_PATH),
             helper.getOptions().get(CatalogOptions.DEFAULT_DATABASE),
             helper.getOptions().get(CatalogOptions.HIVE_CONF_DIR));
       case "dfs":
@@ -82,7 +82,7 @@ public Set<ConfigOption<?>> optionalOptions() {
     options.add(PROPERTY_VERSION);
     options.add(CatalogOptions.HIVE_CONF_DIR);
     options.add(CatalogOptions.MODE);
-    options.add(CATALOG_PATH);
+    options.add(CatalogOptions.CATALOG_PATH);
     return options;
   }
 }

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/ITTestHoodieDataSource.java
Patch:
@@ -1342,7 +1342,6 @@ void testBuiltinFunctionWithHMSCatalog() {
         .field("f_par string")
         .pkField("f_int")
         .partitionField("f_par")
-        .option(FlinkOptions.PATH, tempFile.getAbsolutePath() + "/" + dbName + "/" + "t1")
         .option(FlinkOptions.RECORD_KEY_FIELD, "f_int")
         .option(FlinkOptions.PRECOMBINE_FIELD, "f_date")
         .end();

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/catalog/HoodieCatalogTestUtils.java
Patch:
@@ -41,6 +41,7 @@ public static HoodieHiveCatalog createHiveCatalog(String name) {
     return new HoodieHiveCatalog(
         name,
         null,
+        null,
         createHiveConf(),
         true);
   }

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/catalog/TestHoodieHiveCatalog.java
Patch:
@@ -54,6 +54,7 @@
 import static org.apache.flink.table.factories.FactoryUtil.CONNECTOR;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;
+import static org.junit.jupiter.api.Assertions.assertNull;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
 /**
@@ -106,7 +107,7 @@ public void testCreateAndGetHoodieTable(HoodieTableType tableType) throws Except
     assertEquals(table1.getOptions().get(CONNECTOR.key()), "hudi");
     assertEquals(table1.getOptions().get(FlinkOptions.TABLE_TYPE.key()), tableType.toString());
     assertEquals(table1.getOptions().get(FlinkOptions.RECORD_KEY_FIELD.key()), "uuid");
-    assertEquals(table1.getOptions().get(FlinkOptions.PRECOMBINE_FIELD.key()), "ts");
+    assertNull(table1.getOptions().get(FlinkOptions.PRECOMBINE_FIELD.key()), "preCombine key is not declared");
     assertEquals(table1.getUnresolvedSchema().getPrimaryKey().get().getColumnNames(), Collections.singletonList("uuid"));
     assertEquals(((CatalogTable)table1).getPartitionKeys(), Collections.singletonList("par1"));
   }

File: hudi-common/src/main/java/org/apache/hudi/metadata/MetadataPartitionType.java
Patch:
@@ -48,7 +48,7 @@ public String getFileIdPrefix() {
     return fileIdPrefix;
   }
 
-  void setFileGroupCount(final int fileGroupCount) {
+  public void setFileGroupCount(final int fileGroupCount) {
     this.fileGroupCount = fileGroupCount;
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java
Patch:
@@ -40,7 +40,6 @@
 import org.apache.hudi.index.HoodieIndexUtils;
 import org.apache.hudi.io.HoodieRangeInfoHandle;
 import org.apache.hudi.table.HoodieTable;
-
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/ListBasedHoodieBloomIndexHelper.java
Patch:
@@ -20,7 +20,6 @@
 package org.apache.hudi.index.bloom;
 
 import org.apache.hudi.common.data.HoodieData;
-import org.apache.hudi.common.data.HoodieList;
 import org.apache.hudi.common.data.HoodiePairData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieKey;
@@ -60,7 +59,7 @@ public HoodiePairData<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecor
       HoodieData<Pair<String, HoodieKey>> fileComparisonPairs,
       Map<String, List<BloomIndexFileInfo>> partitionToFileInfo, Map<String, Long> recordsPerPartition) {
     List<Pair<String, HoodieKey>> fileComparisonPairList =
-        HoodieList.getList(fileComparisonPairs).stream()
+        fileComparisonPairs.collectAsList().stream()
             .sorted(Comparator.comparing(Pair::getLeft)).collect(toList());
 
     List<HoodieKeyLookupResult> keyLookupResults = new ArrayList<>();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bucket/HoodieBucketIndex.java
Patch:
@@ -83,7 +83,8 @@ protected HoodieRecord<R> computeNext() {
             Option<HoodieRecordLocation> loc = mapper.getRecordLocation(record.getKey(), record.getPartitionPath());
             return HoodieIndexUtils.getTaggedRecord(record, loc);
           }
-        }
+        },
+        false
     );
   }
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.async.AsyncCleanerService;
 import org.apache.hudi.client.common.HoodieFlinkEngineContext;
-import org.apache.hudi.common.data.HoodieList;
+import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.FileSlice;
@@ -127,8 +127,7 @@ public List<HoodieRecord<T>> filterExists(List<HoodieRecord<T>> hoodieRecords) {
     // Create a Hoodie table which encapsulated the commits and files visible
     HoodieFlinkTable<T> table = getHoodieTable();
     Timer.Context indexTimer = metrics.getIndexCtx();
-    List<HoodieRecord<T>> recordsWithLocation = HoodieList.getList(
-        getIndex().tagLocation(HoodieList.of(hoodieRecords), context, table));
+    List<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(HoodieListData.eager(hoodieRecords), context, table).collectAsList();
     metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));
     return recordsWithLocation.stream().filter(v1 -> !v1.isCurrentLocationKnown()).collect(Collectors.toList());
   }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/common/HoodieFlinkEngineContext.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.common.data.HoodieAccumulator;
 import org.apache.hudi.common.data.HoodieAtomicLongAccumulator;
 import org.apache.hudi.common.data.HoodieData;
-import org.apache.hudi.common.data.HoodieList;
+import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.EngineProperty;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.engine.TaskContextSupplier;
@@ -84,12 +84,12 @@ public HoodieAccumulator newAccumulator() {
 
   @Override
   public <T> HoodieData<T> emptyHoodieData() {
-    return HoodieList.of(Collections.emptyList());
+    return HoodieListData.eager(Collections.emptyList());
   }
 
   @Override
   public <T> HoodieData<T> parallelize(List<T> data, int parallelism) {
-    return HoodieList.of(data);
+    return HoodieListData.eager(data);
   }
 
   public RuntimeContext getRuntimeContext() {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.HoodieFlinkWriteClient;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.data.HoodieData;
-import org.apache.hudi.common.data.HoodieList;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.metrics.Registry;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -106,7 +105,7 @@ protected void commit(String instantTime, Map<MetadataPartitionType, HoodieData<
     ValidationUtils.checkState(enabled, "Metadata table cannot be committed to as it is not enabled");
     ValidationUtils.checkState(metadataMetaClient != null, "Metadata table is not fully initialized yet.");
     HoodieData<HoodieRecord> preppedRecords = prepRecords(partitionRecordsMap);
-    List<HoodieRecord> preppedRecordList = HoodieList.getList(preppedRecords);
+    List<HoodieRecord> preppedRecordList = preppedRecords.collectAsList();
 
     try (HoodieFlinkWriteClient writeClient = new HoodieFlinkWriteClient(engineContext, metadataWriteConfig)) {
       if (canTriggerTableService) {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkTable.java
Patch:
@@ -40,8 +40,6 @@
 
 import java.util.List;
 
-import static org.apache.hudi.common.data.HoodieList.getList;
-
 public abstract class HoodieFlinkTable<T extends HoodieRecordPayload>
     extends HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>>
     implements ExplicitWriteHandleTable<T> {
@@ -78,7 +76,7 @@ public static <T extends HoodieRecordPayload> HoodieFlinkTable<T> create(HoodieW
 
   public static HoodieWriteMetadata<List<WriteStatus>> convertMetadata(
       HoodieWriteMetadata<HoodieData<WriteStatus>> metadata) {
-    return metadata.clone(getList(metadata.getWriteStatuses()));
+    return metadata.clone(metadata.getWriteStatuses().collectAsList());
   }
 
   @Override

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeleteHelper.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.table.action.commit;
 
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.common.data.HoodieList;
+import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieAvroRecord;
@@ -97,8 +97,7 @@ public HoodieWriteMetadata<List<WriteStatus>> execute(String instantTime,
           dedupedKeys.stream().map(key -> new HoodieAvroRecord<>(key, new EmptyHoodieRecordPayload())).collect(Collectors.toList());
       Instant beginTag = Instant.now();
       // perform index look up to get existing location of records
-      List<HoodieRecord<EmptyHoodieRecordPayload>> taggedRecords = HoodieList.getList(
-          table.getIndex().tagLocation(HoodieList.of(dedupedRecords), context, table));
+      List<HoodieRecord<EmptyHoodieRecordPayload>> taggedRecords = table.getIndex().tagLocation(HoodieListData.eager(dedupedRecords), context, table).collectAsList();
       Duration tagLocationDuration = Duration.between(beginTag, Instant.now());
 
       // filter out non existent keys/records

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkWriteHelper.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.table.action.commit;
 
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.common.data.HoodieList;
+import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
@@ -83,8 +83,7 @@ public HoodieWriteMetadata<List<WriteStatus>> write(String instantTime, List<Hoo
 
   @Override
   protected List<HoodieRecord<T>> tag(List<HoodieRecord<T>> dedupedRecords, HoodieEngineContext context, HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table) {
-    return HoodieList.getList(
-        table.getIndex().tagLocation(HoodieList.of(dedupedRecords), context, table));
+    return table.getIndex().tagLocation(HoodieListData.eager(dedupedRecords), context, table).collectAsList();
   }
 
   @Override

File: hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java
Patch:
@@ -21,7 +21,8 @@
 import org.apache.hudi.client.FlinkTaskContextSupplier;
 import org.apache.hudi.client.HoodieFlinkWriteClient;
 import org.apache.hudi.client.common.HoodieFlinkEngineContext;
-import org.apache.hudi.common.data.HoodieList;
+import org.apache.hudi.common.data.HoodieData;
+import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
@@ -133,7 +134,7 @@ protected void initMetaClient(HoodieTableType tableType) throws IOException {
 
   protected List<HoodieRecord> tagLocation(
       HoodieIndex index, List<HoodieRecord> records, HoodieTable table) {
-    return HoodieList.getList(index.tagLocation(HoodieList.of(records), context, table));
+    return ((HoodieData<HoodieRecord>) index.tagLocation(HoodieListData.eager(records), context, table)).collectAsList();
   }
 
   /**

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.client.common.HoodieJavaEngineContext;
 import org.apache.hudi.client.embedded.EmbeddedTimelineService;
-import org.apache.hudi.common.data.HoodieList;
+import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieKey;
@@ -67,8 +67,7 @@ public List<HoodieRecord<T>> filterExists(List<HoodieRecord<T>> hoodieRecords) {
     // Create a Hoodie table which encapsulated the commits and files visible
     HoodieJavaTable<T> table = HoodieJavaTable.create(config, (HoodieJavaEngineContext) context);
     Timer.Context indexTimer = metrics.getIndexCtx();
-    List<HoodieRecord<T>> recordsWithLocation = HoodieList.getList(
-        getIndex().tagLocation(HoodieList.of(hoodieRecords), context, table));
+    List<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(HoodieListData.eager(hoodieRecords), context, table).collectAsList();
     metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));
     return recordsWithLocation.stream().filter(v1 -> !v1.isCurrentLocationKnown()).collect(Collectors.toList());
   }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaExecutionStrategy.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.JavaTaskContextSupplier;
 import org.apache.hudi.common.data.HoodieData;
-import org.apache.hudi.common.data.HoodieList;
+import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.ClusteringOperation;
 import org.apache.hudi.common.model.HoodieAvroRecord;
@@ -94,7 +94,7 @@ public HoodieWriteMetadata<HoodieData<WriteStatus>> performClustering(
             Option.ofNullable(clusteringPlan.getPreserveHoodieMetadata()).orElse(false),
             instantTime)));
     HoodieWriteMetadata<HoodieData<WriteStatus>> writeMetadata = new HoodieWriteMetadata<>();
-    writeMetadata.setWriteStatuses(HoodieList.of(writeStatusList));
+    writeMetadata.setWriteStatuses(HoodieListData.eager(writeStatusList));
     return writeMetadata;
   }
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/common/HoodieJavaEngineContext.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.common.data.HoodieAccumulator;
 import org.apache.hudi.common.data.HoodieAtomicLongAccumulator;
 import org.apache.hudi.common.data.HoodieData;
-import org.apache.hudi.common.data.HoodieList;
+import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.EngineProperty;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.engine.TaskContextSupplier;
@@ -74,12 +74,12 @@ public HoodieAccumulator newAccumulator() {
 
   @Override
   public <T> HoodieData<T> emptyHoodieData() {
-    return HoodieList.of(Collections.emptyList());
+    return HoodieListData.eager(Collections.emptyList());
   }
 
   @Override
   public <T> HoodieData<T> parallelize(List<T> data, int parallelism) {
-    return HoodieList.of(data);
+    return HoodieListData.eager(data);
   }
 
   @Override

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaTable.java
Patch:
@@ -36,8 +36,6 @@
 
 import java.util.List;
 
-import static org.apache.hudi.common.data.HoodieList.getList;
-
 public abstract class HoodieJavaTable<T extends HoodieRecordPayload>
     extends HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {
   protected HoodieJavaTable(HoodieWriteConfig config, HoodieEngineContext context, HoodieTableMetaClient metaClient) {
@@ -67,7 +65,7 @@ public static <T extends HoodieRecordPayload> HoodieJavaTable<T> create(HoodieWr
 
   public static HoodieWriteMetadata<List<WriteStatus>> convertMetadata(
       HoodieWriteMetadata<HoodieData<WriteStatus>> metadata) {
-    return metadata.clone(getList(metadata.getWriteStatuses()));
+    return metadata.clone(metadata.getWriteStatuses().collectAsList());
   }
 
   @Override

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaDeleteHelper.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.table.action.commit;
 
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.common.data.HoodieList;
+import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieAvroRecord;
@@ -99,8 +99,7 @@ public HoodieWriteMetadata<List<WriteStatus>> execute(String instantTime,
           dedupedKeys.stream().map(key -> new HoodieAvroRecord<>(key, new EmptyHoodieRecordPayload())).collect(Collectors.toList());
       Instant beginTag = Instant.now();
       // perform index look up to get existing location of records
-      List<HoodieRecord<EmptyHoodieRecordPayload>> taggedRecords = HoodieList.getList(
-          table.getIndex().tagLocation(HoodieList.of(dedupedRecords), context, table));
+      List<HoodieRecord<EmptyHoodieRecordPayload>> taggedRecords = table.getIndex().tagLocation(HoodieListData.eager(dedupedRecords), context, table).collectAsList();
       Duration tagLocationDuration = Duration.between(beginTag, Instant.now());
 
       // filter out non existent keys/records

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaWriteHelper.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.table.action.commit;
 
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.common.data.HoodieList;
+import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
@@ -50,8 +50,7 @@ public static JavaWriteHelper newInstance() {
 
   @Override
   protected List<HoodieRecord<T>> tag(List<HoodieRecord<T>> dedupedRecords, HoodieEngineContext context, HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table) {
-    return HoodieList.getList(
-        table.getIndex().tagLocation(HoodieList.of(dedupedRecords), context, table));
+    return table.getIndex().tagLocation(HoodieListData.eager(dedupedRecords), context, table).collectAsList();
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/common/engine/HoodieLocalEngineContext.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.common.data.HoodieAccumulator;
 import org.apache.hudi.common.data.HoodieAtomicLongAccumulator;
 import org.apache.hudi.common.data.HoodieData;
-import org.apache.hudi.common.data.HoodieList;
+import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.function.SerializableBiFunction;
 import org.apache.hudi.common.function.SerializableConsumer;
 import org.apache.hudi.common.function.SerializableFunction;
@@ -71,12 +71,12 @@ public HoodieAccumulator newAccumulator() {
 
   @Override
   public <T> HoodieData<T> emptyHoodieData() {
-    return HoodieList.of(Collections.emptyList());
+    return HoodieListData.eager(Collections.emptyList());
   }
 
   @Override
   public <T> HoodieData<T> parallelize(List<T> data, int parallelism) {
-    return HoodieList.of(data);
+    return HoodieListData.eager(data);
   }
 
   @Override

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/compact/CompactionCommitSink.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.common.data.HoodieList;
+import org.apache.hudi.common.data.HoodieListData;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.util.CompactionUtils;
 import org.apache.hudi.common.util.Option;
@@ -158,7 +158,7 @@ private void doCommit(String instant, Collection<CompactionCommitEvent> events)
         .collect(Collectors.toList());
 
     HoodieCommitMetadata metadata = CompactHelpers.getInstance().createCompactionMetadata(
-        table, instant, HoodieList.of(statuses), writeClient.getConfig().getSchema());
+        table, instant, HoodieListData.eager(statuses), writeClient.getConfig().getSchema());
 
     // commit the compaction
     this.writeClient.commitCompaction(instant, metadata, Option.empty());

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/ITTestDataStreamWrite.java
Patch:
@@ -263,7 +263,7 @@ private void testWriteToHoodie(
       client.getJobExecutionResult().get();
     }
 
-    TestData.checkWrittenFullData(tempFile, expected);
+    TestData.checkWrittenDataCOW(tempFile, expected);
   }
 
   private void testWriteToHoodieWithCluster(
@@ -327,7 +327,7 @@ private void testWriteToHoodieWithCluster(
     // wait for the streaming job to finish
     client.getJobExecutionResult().get();
 
-    TestData.checkWrittenFullData(tempFile, expected);
+    TestData.checkWrittenDataCOW(tempFile, expected);
   }
 
   public void execute(StreamExecutionEnvironment execEnv, boolean isMor, String jobName) throws Exception {
@@ -449,7 +449,7 @@ public void testHoodiePipelineBuilderSink() throws Exception {
     builder.sink(dataStream, false);
 
     execute(execEnv, true, "Api_Sink_Test");
-    TestData.checkWrittenFullData(tempFile, EXPECTED);
+    TestData.checkWrittenDataCOW(tempFile, EXPECTED);
   }
 
 }

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/TestWriteCopyOnWrite.java
Patch:
@@ -250,7 +250,7 @@ public void testInsertAppendMode() throws Exception {
         .checkpoint(2)
         .assertNextEvent()
         .checkpointComplete(2)
-        .checkWrittenFullData(EXPECTED5)
+        .checkWrittenDataCOW(EXPECTED5)
         .end();
   }
 
@@ -282,7 +282,7 @@ public void testInsertClustering() throws Exception {
         .checkpoint(2)
         .handleEvents(2)
         .checkpointComplete(2)
-        .checkWrittenFullData(EXPECTED5)
+        .checkWrittenDataCOW(EXPECTED5)
         .end();
   }
 
@@ -305,7 +305,7 @@ public void testInsertAsyncClustering() throws Exception {
         .checkpoint(2)
         .handleEvents(1)
         .checkpointComplete(2)
-        .checkWrittenFullData(EXPECTED5)
+        .checkWrittenDataCOW(EXPECTED5)
         .end();
   }
 

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/compact/ITTestHoodieFlinkCompactor.java
Patch:
@@ -161,7 +161,7 @@ public void testHoodieFlinkCompactor(boolean enableChangelog) throws Exception {
 
     env.execute("flink_hudi_compaction");
     writeClient.close();
-    TestData.checkWrittenFullData(tempFile, EXPECTED1);
+    TestData.checkWrittenDataCOW(tempFile, EXPECTED1);
   }
 
   @ParameterizedTest
@@ -202,7 +202,7 @@ public void testHoodieFlinkCompactorService(boolean enableChangelog) throws Exce
 
     asyncCompactionService.shutDown();
 
-    TestData.checkWrittenFullData(tempFile, EXPECTED2);
+    TestData.checkWrittenDataCOW(tempFile, EXPECTED2);
   }
 
   @ParameterizedTest
@@ -281,7 +281,7 @@ public void testHoodieFlinkCompactorWithPlanSelectStrategy(boolean enableChangel
 
     env.execute("flink_hudi_compaction");
     writeClient.close();
-    TestData.checkWrittenFullData(tempFile, EXPECTED3);
+    TestData.checkWrittenDataCOW(tempFile, EXPECTED3);
   }
 
   private String scheduleCompactionPlan(HoodieTableMetaClient metaClient, HoodieFlinkWriteClient<?> writeClient) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieMergeHelper.java
Patch:
@@ -100,7 +100,7 @@ public void runMerge(HoodieTable<T, HoodieData<HoodieRecord<T>>, HoodieData<Hood
     // TODO support bootstrap
     if (querySchemaOpt.isPresent() && !baseFile.getBootstrapBaseFile().isPresent()) {
       // check implicitly add columns, and position reorder(spark sql may change cols order)
-      InternalSchema querySchema = AvroSchemaEvolutionUtils.evolveSchemaFromNewAvroSchema(readSchema, querySchemaOpt.get(), true);
+      InternalSchema querySchema = AvroSchemaEvolutionUtils.reconcileSchema(readSchema, querySchemaOpt.get());
       long commitInstantTime = Long.valueOf(FSUtils.getCommitTime(mergeHandle.getOldFilePath().getName()));
       InternalSchema writeInternalSchema = InternalSchemaCache.searchSchemaAndCache(commitInstantTime, table.getMetaClient(), table.getConfig().getInternalSchemaCacheEnable());
       if (writeInternalSchema.isEmptySchema()) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java
Patch:
@@ -57,8 +57,8 @@
 import java.io.IOException;
 import java.util.ArrayDeque;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.Deque;
-import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
@@ -380,7 +380,7 @@ private void processDataBlock(HoodieDataBlock dataBlock, Option<KeySpec> keySpec
       Option<Schema> schemaOption = getMergedSchema(dataBlock);
       while (recordIterator.hasNext()) {
         IndexedRecord currentRecord = recordIterator.next();
-        IndexedRecord record = schemaOption.isPresent() ? HoodieAvroUtils.rewriteRecordWithNewSchema(currentRecord, schemaOption.get(), new HashMap<>()) : currentRecord;
+        IndexedRecord record = schemaOption.isPresent() ? HoodieAvroUtils.rewriteRecordWithNewSchema(currentRecord, schemaOption.get(), Collections.emptyMap()) : currentRecord;
         processNextRecord(createHoodieRecord(record, this.hoodieTableMetaClient.getTableConfig(), this.payloadClassFQN,
             this.preCombineField, this.withOperationField, this.simpleKeyGenFields, this.partitionName));
         totalLogRecords.incrementAndGet();

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/minicluster/HdfsTestService.java
Patch:
@@ -103,7 +103,9 @@ public MiniDFSCluster start(boolean format) throws IOException {
 
   public void stop() {
     LOG.info("HDFS Minicluster service being shut down.");
-    miniDfsCluster.shutdown();
+    if (miniDfsCluster != null) {
+      miniDfsCluster.shutdown();
+    }
     miniDfsCluster = null;
     hadoopConf = null;
   }

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CommitsCommand.java
Patch:
@@ -233,7 +233,7 @@ public String rollbackCommit(@CliOption(key = {"commit"}, help = "Commit to roll
       @CliOption(key = "sparkMaster", unspecifiedDefaultValue = "", help = "Spark Master") String master,
       @CliOption(key = "sparkMemory", unspecifiedDefaultValue = "4G",
          help = "Spark executor memory") final String sparkMemory,
-      @CliOption(key = "rollbackUsingMarkers", unspecifiedDefaultValue = "true",
+      @CliOption(key = "rollbackUsingMarkers", unspecifiedDefaultValue = "false",
          help = "Enabling marker based rollback") final String rollbackUsingMarkers)
       throws Exception {
     HoodieActiveTimeline activeTimeline = HoodieCLI.getTableMetaClient().getActiveTimeline();

File: hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/DefaultSource.java
Patch:
@@ -36,7 +36,7 @@
 import java.util.Map;
 import java.util.Optional;
 
-import static org.apache.hudi.DataSourceUtils.mayBeOverwriteParquetWriteLegacyFormatProp;
+import static org.apache.hudi.DataSourceUtils.tryOverrideParquetWriteLegacyFormatProperty;
 
 /**
  * DataSource V2 implementation for managing internal write logic. Only called internally.
@@ -69,7 +69,7 @@ public Optional<DataSourceWriter> createWriter(String writeUUID, StructType sche
         HoodieTableConfig.POPULATE_META_FIELDS.defaultValue());
     Map<String, String> properties = options.asMap();
     // Auto set the value of "hoodie.parquet.writelegacyformat.enabled"
-    mayBeOverwriteParquetWriteLegacyFormatProp(properties, schema);
+    tryOverrideParquetWriteLegacyFormatProperty(properties, schema);
     // 1st arg to createHoodieConfig is not really required to be set. but passing it anyways.
     HoodieWriteConfig config = DataSourceUtils.createHoodieConfig(options.get(HoodieWriteConfig.AVRO_SCHEMA_STRING.key()).get(), path, tblName, properties);
     boolean arePartitionRecordsSorted = HoodieInternalConfig.getBulkInsertIsPartitionRecordsSorted(

File: hudi-spark-datasource/hudi-spark3-common/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java
Patch:
@@ -34,7 +34,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
-import static org.apache.hudi.DataSourceUtils.mayBeOverwriteParquetWriteLegacyFormatProp;
+import static org.apache.hudi.DataSourceUtils.tryOverrideParquetWriteLegacyFormatProperty;
 
 /**
  * DataSource V2 implementation for managing internal write logic. Only called internally.
@@ -59,7 +59,7 @@ public Table getTable(StructType schema, Transform[] partitioning, Map<String, S
     // Create a new map as the properties is an unmodifiableMap on Spark 3.2.0
     Map<String, String> newProps = new HashMap<>(properties);
     // Auto set the value of "hoodie.parquet.writelegacyformat.enabled"
-    mayBeOverwriteParquetWriteLegacyFormatProp(newProps, schema);
+    tryOverrideParquetWriteLegacyFormatProperty(newProps, schema);
     // 1st arg to createHoodieConfig is not really required to be set. but passing it anyways.
     HoodieWriteConfig config = DataSourceUtils.createHoodieConfig(newProps.get(HoodieWriteConfig.AVRO_SCHEMA_STRING.key()), path, tblName, newProps);
     return new HoodieDataSourceInternalTable(instantTime, config, schema, getSparkSession(),

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteOperatorCoordinator.java
Patch:
@@ -271,7 +271,7 @@ public void notifyCheckpointComplete(long checkpointId) {
 
   @Override
   public void notifyCheckpointAborted(long checkpointId) {
-    if (checkpointId == this.checkpointId) {
+    if (checkpointId == this.checkpointId && !WriteMetadataEvent.BOOTSTRAP_INSTANT.equals(this.instant)) {
       executor.execute(() -> {
         this.ckpMetadata.abortInstant(this.instant);
       }, "abort instant %s", this.instant);

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/meta/CkpMetadata.java
Patch:
@@ -97,8 +97,6 @@ public void close() {
   public void bootstrap(HoodieTableMetaClient metaClient) throws IOException {
     fs.delete(path, true);
     fs.mkdirs(path);
-    metaClient.getActiveTimeline().getCommitsTimeline().filterPendingExcludingCompaction()
-        .lastInstant().ifPresent(instant -> startInstant(instant.getTimestamp()));
   }
 
   public void startInstant(String instant) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroParquetWriter.java
Patch:
@@ -47,11 +47,11 @@ public class HoodieAvroParquetWriter<R extends IndexedRecord>
 
   @SuppressWarnings({"unchecked", "rawtypes"})
   public HoodieAvroParquetWriter(Path file,
-                                 HoodieAvroParquetConfig parquetConfig,
+                                 HoodieParquetConfig<HoodieAvroWriteSupport> parquetConfig,
                                  String instantTime,
                                  TaskContextSupplier taskContextSupplier,
                                  boolean populateMetaFields) throws IOException {
-    super(file, (HoodieBaseParquetConfig) parquetConfig);
+    super(file, (HoodieParquetConfig) parquetConfig);
     this.fileName = file.getName();
     this.writeSupport = parquetConfig.getWriteSupport();
     this.instantTime = instantTime;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieBaseParquetWriter.java
Patch:
@@ -43,7 +43,7 @@ public abstract class HoodieBaseParquetWriter<R> extends ParquetWriter<R> {
   private long lastCachedDataSize = -1;
 
   public HoodieBaseParquetWriter(Path file,
-                                 HoodieBaseParquetConfig<? extends WriteSupport<R>> parquetConfig) throws IOException {
+                                 HoodieParquetConfig<? extends WriteSupport<R>> parquetConfig) throws IOException {
     super(HoodieWrapperFileSystem.convertToHoodiePath(file, parquetConfig.getHadoopConf()),
         ParquetFileWriter.Mode.CREATE,
         parquetConfig.getWriteSupport(),

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieFileWriterFactory.java
Patch:
@@ -77,7 +77,7 @@ private static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFi
     Option<BloomFilter> filter = enableBloomFilter ? Option.of(createBloomFilter(config)) : Option.empty();
     HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(new AvroSchemaConverter(conf).convert(schema), schema, filter);
 
-    HoodieAvroParquetConfig parquetConfig = new HoodieAvroParquetConfig(writeSupport, config.getParquetCompressionCodec(),
+    HoodieParquetConfig<HoodieAvroWriteSupport> parquetConfig = new HoodieParquetConfig<>(writeSupport, config.getParquetCompressionCodec(),
         config.getParquetBlockSize(), config.getParquetPageSize(), config.getParquetMaxFileSize(),
         conf, config.getParquetCompressionRatio(), config.parquetDictionaryEnabled());
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataFileWriterFactory.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.common.bloom.BloomFilterFactory;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
+import org.apache.hudi.io.storage.HoodieParquetConfig;
 import org.apache.hudi.table.HoodieTable;
 
 import org.apache.flink.table.types.logical.RowType;
@@ -67,7 +68,7 @@ private static HoodieRowDataFileWriter newParquetInternalRowFileWriter(
     HoodieRowDataParquetWriteSupport writeSupport =
         new HoodieRowDataParquetWriteSupport(table.getHadoopConf(), rowType, filter);
     return new HoodieRowDataParquetWriter(
-        path, new HoodieRowDataParquetConfig(
+        path, new HoodieParquetConfig<>(
         writeSupport,
         writeConfig.getParquetCompressionCodec(),
         writeConfig.getParquetBlockSize(),

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataParquetWriter.java
Patch:
@@ -23,6 +23,7 @@
 
 import org.apache.flink.table.data.RowData;
 import org.apache.hadoop.fs.Path;
+import org.apache.hudi.io.storage.HoodieParquetConfig;
 import org.apache.parquet.hadoop.ParquetFileWriter;
 import org.apache.parquet.hadoop.ParquetWriter;
 
@@ -39,7 +40,7 @@ public class HoodieRowDataParquetWriter extends ParquetWriter<RowData>
   private final long maxFileSize;
   private final HoodieRowDataParquetWriteSupport writeSupport;
 
-  public HoodieRowDataParquetWriter(Path file, HoodieRowDataParquetConfig parquetConfig)
+  public HoodieRowDataParquetWriter(Path file, HoodieParquetConfig<HoodieRowDataParquetWriteSupport> parquetConfig)
       throws IOException {
     super(HoodieWrapperFileSystem.convertToHoodiePath(file, parquetConfig.getHadoopConf()),
         ParquetFileWriter.Mode.CREATE, parquetConfig.getWriteSupport(), parquetConfig.getCompressionCodecName(),

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieInternalRowParquetWriter.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.io.storage.row;
 
 import org.apache.hadoop.fs.Path;
+import org.apache.hudi.io.storage.HoodieParquetConfig;
 import org.apache.hudi.io.storage.HoodieBaseParquetWriter;
 import org.apache.spark.sql.catalyst.InternalRow;
 
@@ -32,7 +33,7 @@ public class HoodieInternalRowParquetWriter extends HoodieBaseParquetWriter<Inte
 
   private final HoodieRowParquetWriteSupport writeSupport;
 
-  public HoodieInternalRowParquetWriter(Path file, HoodieRowParquetConfig parquetConfig)
+  public HoodieInternalRowParquetWriter(Path file, HoodieParquetConfig<HoodieRowParquetWriteSupport> parquetConfig)
       throws IOException {
     super(file, parquetConfig);
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/storage/row/TestHoodieInternalRowParquetWriter.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.config.HoodieStorageConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
+import org.apache.hudi.io.storage.HoodieParquetConfig;
 import org.apache.hudi.testutils.HoodieClientTestHarness;
 import org.apache.hudi.testutils.SparkDatasetTestUtils;
 
@@ -73,9 +74,9 @@ public void endToEndTest(boolean parquetWriteLegacyFormatEnabled) throws Excepti
       // init write support and parquet config
       HoodieRowParquetWriteSupport writeSupport = getWriteSupport(writeConfigBuilder, hadoopConf, parquetWriteLegacyFormatEnabled);
       HoodieWriteConfig cfg = writeConfigBuilder.build();
-      HoodieRowParquetConfig parquetConfig = new HoodieRowParquetConfig(writeSupport,
+      HoodieParquetConfig<HoodieRowParquetWriteSupport> parquetConfig = new HoodieParquetConfig<>(writeSupport,
           CompressionCodecName.SNAPPY, cfg.getParquetBlockSize(), cfg.getParquetPageSize(), cfg.getParquetMaxFileSize(),
-          writeSupport.getHadoopConf(), cfg.getParquetCompressionRatio());
+          writeSupport.getHadoopConf(), cfg.getParquetCompressionRatio(), cfg.parquetDictionaryEnabled());
 
       // prepare path
       String fileId = UUID.randomUUID().toString();

File: hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieParquetStreamWriter.java
Patch:
@@ -38,7 +38,7 @@ public class HoodieParquetStreamWriter<R extends IndexedRecord> implements AutoC
   private final HoodieAvroWriteSupport writeSupport;
 
   public HoodieParquetStreamWriter(FSDataOutputStream outputStream,
-                                   HoodieAvroParquetConfig parquetConfig) throws IOException {
+                                   HoodieParquetConfig<HoodieAvroWriteSupport> parquetConfig) throws IOException {
     this.writeSupport = parquetConfig.getWriteSupport();
     this.writer = new Builder<R>(new OutputStreamBackedOutputFile(outputStream), writeSupport)
         .withWriteMode(ParquetFileWriter.Mode.CREATE)

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -363,7 +363,7 @@ private FlinkOptions() {
   public static final ConfigOption<String> KEYGEN_CLASS_NAME = ConfigOptions
       .key(HoodieWriteConfig.KEYGENERATOR_CLASS_NAME.key())
       .stringType()
-      .defaultValue("")
+      .noDefaultValue()
       .withDescription("Key generator class, that implements will extract the key out of incoming record");
 
   public static final ConfigOption<String> KEYGEN_TYPE = ConfigOptions

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bulk/RowDataKeyGen.java
Patch:
@@ -116,7 +116,7 @@ private RowDataKeyGen(
 
   public static RowDataKeyGen instance(Configuration conf, RowType rowType) {
     Option<TimestampBasedAvroKeyGenerator> keyGeneratorOpt = Option.empty();
-    if (conf.getString(FlinkOptions.KEYGEN_CLASS_NAME).equals(TimestampBasedAvroKeyGenerator.class.getName())) {
+    if (TimestampBasedAvroKeyGenerator.class.getName().equals(conf.getString(FlinkOptions.KEYGEN_CLASS_NAME))) {
       try {
         keyGeneratorOpt = Option.of(new TimestampBasedAvroKeyGenerator(StreamerUtil.flinkConf2TypedProperties(conf)));
       } catch (IOException e) {

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestHoodieTableFileSystemView.java
Patch:
@@ -18,9 +18,6 @@
 
 package org.apache.hudi.common.table.view;
 
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsAction;
 import org.apache.hudi.avro.model.HoodieClusteringPlan;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.avro.model.HoodieFSPermission;
@@ -61,6 +58,9 @@
 import org.apache.hudi.common.util.collection.ImmutablePair;
 import org.apache.hudi.common.util.collection.Pair;
 
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsAction;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 import org.junit.jupiter.api.BeforeEach;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -451,7 +451,7 @@ private static int doBootstrap(JavaSparkContext jsc, String tableName, String ta
   }
 
   private static int rollback(JavaSparkContext jsc, String instantTime, String basePath, Boolean rollbackUsingMarkers) throws Exception {
-    SparkRDDWriteClient client = createHoodieClient(jsc, basePath, rollbackUsingMarkers);
+    SparkRDDWriteClient client = createHoodieClient(jsc, basePath, rollbackUsingMarkers, false);
     if (client.rollback(instantTime)) {
       LOG.info(String.format("The commit \"%s\" rolled back.", instantTime));
       return 0;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CommitsCommand.java
Patch:
@@ -442,8 +442,8 @@ public String compareCommits(@CliOption(key = {"path"}, help = "Path of the tabl
     }
   }
 
-  @CliCommand(value = "commits sync", help = "Compare commits with another Hoodie table")
-  public String syncCommits(@CliOption(key = {"path"}, help = "Path of the table to compare to") final String path) {
+  @CliCommand(value = "commits sync", help = "Sync commits with another Hoodie table")
+  public String syncCommits(@CliOption(key = {"path"}, help = "Path of the table to sync to") final String path) {
     HoodieCLI.syncTableMetadata = HoodieTableMetaClient.builder().setConf(HoodieCLI.conf).setBasePath(path).build();
     HoodieCLI.state = HoodieCLI.CLIState.SYNC;
     return "Load sync state between " + HoodieCLI.getTableMetaClient().getTableConfig().getTableName() + " and "

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteOperatorCoordinator.java
Patch:
@@ -285,7 +285,8 @@ public void handleEventFromOperator(int i, OperatorEvent operatorEvent) {
 
     if (event.isEndInput()) {
       // handle end input event synchronously
-      handleEndInputEvent(event);
+      // wrap handleEndInputEvent in executeSync to preserve the order of events
+      executor.executeSync(() -> handleEndInputEvent(event), "handle end input event for instant %s", this.instant);
     } else {
       executor.execute(
           () -> {

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java
Patch:
@@ -76,7 +76,7 @@ public class HoodieLogFileReader implements HoodieLogFormat.Reader {
   private final HoodieLogFile logFile;
   private final byte[] magicBuffer = new byte[6];
   private final Schema readerSchema;
-  private InternalSchema internalSchema = InternalSchema.getEmptyInternalSchema();
+  private InternalSchema internalSchema;
   private final String keyField;
   private boolean readBlockLazily;
   private long reverseLogFilePosition;

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/AvroSchemaConverter.java
Patch:
@@ -71,6 +71,8 @@ public static DataType convertToDataType(Schema schema) {
         }
         return DataTypes.ROW(fields).notNull();
       case ENUM:
+      case STRING:
+        // convert Avro's Utf8/CharSequence to String
         return DataTypes.STRING().notNull();
       case ARRAY:
         return DataTypes.ARRAY(convertToDataType(schema.getElementType())).notNull();
@@ -110,9 +112,6 @@ public static DataType convertToDataType(Schema schema) {
         }
         // convert fixed size binary data to primitive byte arrays
         return DataTypes.VARBINARY(schema.getFixedSize()).notNull();
-      case STRING:
-        // convert Avro's Utf8/CharSequence to String
-        return DataTypes.STRING().notNull();
       case BYTES:
         // logical decimal type
         if (schema.getLogicalType() instanceof LogicalTypes.Decimal) {

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -148,7 +148,7 @@ public static void main(String[] args) throws Exception {
           }
           configs = new ArrayList<>();
           if (args.length > 9) {
-            configs.addAll(Arrays.asList(args).subList(8, args.length));
+            configs.addAll(Arrays.asList(args).subList(9, args.length));
           }
 
           returnCode = compact(jsc, args[3], args[4], null, Integer.parseInt(args[5]), args[6],

File: hudi-cli/src/test/java/org/apache/hudi/cli/TestSparkUtil.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.cli.testutils;
+package org.apache.hudi.cli;
 
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.cli.utils.SparkUtil;
@@ -30,7 +30,7 @@
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertNotNull;
 
-public class SparkUtilTest {
+public class TestSparkUtil {
 
   @Test
   public void testInitSparkLauncher() throws URISyntaxException {

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestBootstrapCommand.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.cli.HoodieCLI;
 import org.apache.hudi.cli.HoodiePrintHelper;
 import org.apache.hudi.cli.commands.TableCommand;
-import org.apache.hudi.cli.testutils.AbstractShellIntegrationTest;
+import org.apache.hudi.cli.testutils.HoodieCLIIntegrationTestBase;
 import org.apache.hudi.functional.TestBootstrap;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
@@ -44,7 +44,7 @@
 /**
  * Test class of {@link org.apache.hudi.cli.commands.BootstrapCommand}.
  */
-public class ITTestBootstrapCommand extends AbstractShellIntegrationTest {
+public class ITTestBootstrapCommand extends HoodieCLIIntegrationTestBase {
 
   private static final int NUM_OF_RECORDS = 100;
   private static final String PARTITION_FIELD = "datestr";

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestHDFSParquetImportCommand.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.cli.HoodieCLI;
 import org.apache.hudi.cli.commands.TableCommand;
-import org.apache.hudi.cli.testutils.AbstractShellIntegrationTest;
+import org.apache.hudi.cli.testutils.HoodieCLIIntegrationTestBase;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
@@ -55,7 +55,7 @@
  * Test class for {@link org.apache.hudi.cli.commands.HDFSParquetImportCommand}.
  */
 @Disabled("Disable due to flakiness and feature deprecation.")
-public class ITTestHDFSParquetImportCommand extends AbstractShellIntegrationTest {
+public class ITTestHDFSParquetImportCommand extends HoodieCLIIntegrationTestBase {
 
   private Path sourcePath;
   private Path targetPath;

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestMarkersCommand.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hudi.cli.commands.TableCommand;
-import org.apache.hudi.cli.testutils.AbstractShellIntegrationTest;
+import org.apache.hudi.cli.testutils.HoodieCLIIntegrationTestBase;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.IOType;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
@@ -40,7 +40,7 @@
  * A command use SparkLauncher need load jars under lib which generate during mvn package.
  * Use integration test instead of unit test.
  */
-public class ITTestMarkersCommand extends AbstractShellIntegrationTest {
+public class ITTestMarkersCommand extends HoodieCLIIntegrationTestBase {
 
   private String tablePath;
 

File: hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.cli.HoodieCLI;
 import org.apache.hudi.cli.commands.RepairsCommand;
 import org.apache.hudi.cli.commands.TableCommand;
-import org.apache.hudi.cli.testutils.AbstractShellIntegrationTest;
+import org.apache.hudi.cli.testutils.HoodieCLIIntegrationTestBase;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -57,7 +57,7 @@
  * A command use SparkLauncher need load jars under lib which generate during mvn package.
  * Use integration test instead of unit test.
  */
-public class ITTestRepairsCommand extends AbstractShellIntegrationTest {
+public class ITTestRepairsCommand extends HoodieCLIIntegrationTestBase {
 
   private String duplicatedPartitionPath;
   private String duplicatedPartitionPathWithUpdates;

File: hudi-cli/src/test/java/org/apache/hudi/cli/testutils/HoodieCLIIntegrationTestBase.java
Patch:
@@ -25,7 +25,7 @@
 /**
  * Class to initial resources for shell.
  */
-public abstract class AbstractShellIntegrationTest extends AbstractShellBaseIntegrationTest {
+public class HoodieCLIIntegrationTestBase extends HoodieCLIIntegrationTestHarness {
 
   @Override
   @BeforeEach

File: hudi-cli/src/test/java/org/apache/hudi/cli/testutils/HoodieCLIIntegrationTestHarness.java
Patch:
@@ -30,7 +30,7 @@
 /**
  * Class to start Bootstrap and JLineShellComponent.
  */
-public class AbstractShellBaseIntegrationTest extends HoodieClientTestHarness {
+public class HoodieCLIIntegrationTestHarness extends HoodieClientTestHarness {
 
   private static JLineShellComponent shell;
 

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/TestDFSHoodieTestSuiteWriterAdapter.java
Patch:
@@ -65,7 +65,7 @@ public class TestDFSHoodieTestSuiteWriterAdapter extends UtilitiesTestBase {
 
   @BeforeAll
   public static void initClass() throws Exception {
-    UtilitiesTestBase.initClass();
+    UtilitiesTestBase.initTestServices(false, false);
   }
 
   @AfterAll

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/TestFileDeltaInputWriter.java
Patch:
@@ -58,7 +58,7 @@ public class TestFileDeltaInputWriter extends UtilitiesTestBase {
 
   @BeforeAll
   public static void initClass() throws Exception {
-    UtilitiesTestBase.initClass();
+    UtilitiesTestBase.initTestServices(false, false);
   }
 
   @AfterAll

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/job/TestHoodieTestSuiteJob.java
Patch:
@@ -90,7 +90,7 @@ public static Stream<Arguments> configParams() {
 
   @BeforeAll
   public static void initClass() throws Exception {
-    UtilitiesTestBase.initClass();
+    UtilitiesTestBase.initTestServices(true, true);
     // prepare the configs.
     UtilitiesTestBase.Helpers.copyToDFSFromAbsolutePath(System.getProperty("user.dir") + "/.."
         + BASE_PROPERTIES_DOCKER_DEMO_RELATIVE_PATH, dfs, dfsBasePath + "/base.properties");

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/reader/TestDFSAvroDeltaInputReader.java
Patch:
@@ -43,7 +43,7 @@ public class TestDFSAvroDeltaInputReader extends UtilitiesTestBase {
 
   @BeforeAll
   public static void initClass() throws Exception {
-    UtilitiesTestBase.initClass();
+    UtilitiesTestBase.initTestServices(false, false);
   }
 
   @AfterAll

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/reader/TestDFSHoodieDatasetInputReader.java
Patch:
@@ -51,7 +51,7 @@ public class TestDFSHoodieDatasetInputReader extends UtilitiesTestBase {
 
   @BeforeAll
   public static void initClass() throws Exception {
-    UtilitiesTestBase.initClass();
+    UtilitiesTestBase.initTestServices(false, false);
   }
 
   @AfterAll

File: hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.testutils.HoodieClientTestUtils;
-
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.catalyst.InternalRow;

File: hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java
Patch:
@@ -102,7 +102,7 @@ public void testDataInternalWriter(boolean sorted, boolean populateMetaFields) t
       Option<List<String>> fileNames = Option.of(new ArrayList<>());
 
       // verify write statuses
-      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, sorted, fileAbsPaths, fileNames);
+      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, sorted, fileAbsPaths, fileNames, false);
 
       // verify rows
       Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/HoodieDeltaStreamerTestBase.java
Patch:
@@ -97,7 +97,7 @@ public class HoodieDeltaStreamerTestBase extends UtilitiesTestBase {
 
   @BeforeAll
   public static void initClass() throws Exception {
-    UtilitiesTestBase.initClass(true);
+    UtilitiesTestBase.initTestServices(true, true);
     PARQUET_SOURCE_ROOT = dfsBasePath + "/parquetFiles";
     ORC_SOURCE_ROOT = dfsBasePath + "/orcFiles";
     JSON_KAFKA_SOURCE_ROOT = dfsBasePath + "/jsonKafkaFiles";

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestSqlSource.java
Patch:
@@ -59,7 +59,7 @@ public class TestSqlSource extends UtilitiesTestBase {
 
   @BeforeAll
   public static void initClass() throws Exception {
-    UtilitiesTestBase.initClass();
+    UtilitiesTestBase.initTestServices(false, false);
   }
 
   @AfterAll

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/debezium/TestAbstractDebeziumSource.java
Patch:
@@ -64,7 +64,7 @@ public abstract class TestAbstractDebeziumSource extends UtilitiesTestBase {
 
   @BeforeAll
   public static void initClass() throws Exception {
-    UtilitiesTestBase.initClass(false);
+    UtilitiesTestBase.initTestServices(false, false);
   }
 
   @AfterAll

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/testutils/sources/AbstractCloudObjectsSourceTestBase.java
Patch:
@@ -54,7 +54,7 @@ public abstract class AbstractCloudObjectsSourceTestBase extends UtilitiesTestBa
 
   @BeforeAll
   public static void initClass() throws Exception {
-    UtilitiesTestBase.initClass();
+    UtilitiesTestBase.initTestServices(false, false);
   }
 
   @AfterAll

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/testutils/sources/AbstractDFSSourceTestBase.java
Patch:
@@ -62,7 +62,7 @@ public abstract class AbstractDFSSourceTestBase extends UtilitiesTestBase {
 
   @BeforeAll
   public static void initClass() throws Exception {
-    UtilitiesTestBase.initClass();
+    UtilitiesTestBase.initTestServices(false, false);
   }
 
   @AfterAll

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/transform/TestSqlFileBasedTransformer.java
Patch:
@@ -49,7 +49,7 @@ public class TestSqlFileBasedTransformer extends UtilitiesTestBase {
 
   @BeforeAll
   public static void initClass() throws Exception {
-    UtilitiesTestBase.initClass();
+    UtilitiesTestBase.initTestServices(false, false);
     UtilitiesTestBase.Helpers.copyToDFS(
         "delta-streamer-config/sql-file-transformer.sql",
         UtilitiesTestBase.dfs,

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java
Patch:
@@ -78,6 +78,8 @@ public HiveSyncTool(HiveSyncConfig hiveSyncConfig, HiveConf hiveConf, FileSystem
     if (StringUtils.isNullOrEmpty(hiveConf.get(HiveConf.ConfVars.METASTOREURIS.varname))) {
       hiveConf.set(HiveConf.ConfVars.METASTOREURIS.varname, hiveSyncConfig.metastoreUris);
     }
+    // HiveConf needs to load fs conf to allow instantiation via AWSGlueClientFactory
+    hiveConf.addResource(fs.getConf());
     initClient(hiveSyncConfig, hiveConf);
     initConfig(hiveSyncConfig);
   }

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
Patch:
@@ -75,11 +75,10 @@ public class TestHiveSyncTool {
 
   private static final List<Object> SYNC_MODES = Arrays.asList(
       "hms",
-      "hiveql",
       "jdbc");
 
   private static Iterable<Object> syncMode() {
-    return SYNC_MODES;
+    return SYNC_MODES; // TODO include hiveql; skipped due to CI issue
   }
 
   // useSchemaFromCommitMetadata, syncMode

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -2094,13 +2094,13 @@ public void testCommitWritesRelativePaths(boolean populateMetaFields) throws Exc
       HoodieCommitMetadata commitMetadata = HoodieCommitMetadata
           .fromBytes(commitTimeline.getInstantDetails(commitInstant).get(), HoodieCommitMetadata.class);
       String basePath = table.getMetaClient().getBasePath();
-      Collection<String> commitPathNames = commitMetadata.getFileIdAndFullPaths(basePath).values();
+      Collection<String> commitPathNames = commitMetadata.getFileIdAndFullPaths(new Path(basePath)).values();
 
       // Read from commit file
       try (FSDataInputStream inputStream = fs.open(testTable.getCommitFilePath(instantTime))) {
         String everything = FileIOUtils.readAsUTFString(inputStream);
         HoodieCommitMetadata metadata = HoodieCommitMetadata.fromJsonString(everything, HoodieCommitMetadata.class);
-        HashMap<String, String> paths = metadata.getFileIdAndFullPaths(basePath);
+        HashMap<String, String> paths = metadata.getFileIdAndFullPaths(new Path(basePath));
         // Compare values in both to make sure they are equal.
         for (String pathName : paths.values()) {
           assertTrue(commitPathNames.contains(pathName));

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java
Patch:
@@ -109,7 +109,7 @@ private static HashMap<String, String> getLatestFileIDsToFullPath(String basePat
     for (HoodieInstant commit : commitsToReturn) {
       HoodieCommitMetadata metadata =
           HoodieCommitMetadata.fromBytes(commitTimeline.getInstantDetails(commit).get(), HoodieCommitMetadata.class);
-      fileIdToFullPath.putAll(metadata.getFileIdAndFullPaths(basePath));
+      fileIdToFullPath.putAll(metadata.getFileIdAndFullPaths(new Path(basePath)));
     }
     return fileIdToFullPath;
   }

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java
Patch:
@@ -125,7 +125,7 @@ public WriteOperationType getOperationType() {
     return this.operationType;
   }
 
-  public HashMap<String, String> getFileIdAndFullPaths(String basePath) {
+  public HashMap<String, String> getFileIdAndFullPaths(Path basePath) {
     HashMap<String, String> fullPaths = new HashMap<>();
     for (Map.Entry<String, String> entry : getFileIdAndRelativePaths().entrySet()) {
       String fullPath = entry.getValue() != null

File: hudi-common/src/main/java/org/apache/hudi/util/Lazy.java
Patch:
@@ -18,13 +18,15 @@
 
 package org.apache.hudi.util;
 
+import javax.annotation.concurrent.ThreadSafe;
 import java.util.function.Supplier;
 
 /**
  * Utility implementing lazy semantics in Java
  *
  * @param <T> type of the object being held by {@link Lazy}
  */
+@ThreadSafe
 public class Lazy<T> {
 
   private volatile boolean initialized;

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/sink/ITTestDataStreamWrite.java
Patch:
@@ -102,7 +102,7 @@ public void testWriteCopyOnWrite(String indexType) throws Exception {
     conf.setString(FlinkOptions.INDEX_KEY_FIELD, "id");
     conf.setBoolean(FlinkOptions.PRE_COMBINE,true);
 
-    testWriteToHoodie(conf, "cow_write", 1, EXPECTED);
+    testWriteToHoodie(conf, "cow_write", 2, EXPECTED);
   }
 
   @Test

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -296,7 +296,7 @@ protected static int deleteMarker(JavaSparkContext jsc, String instantTime, Stri
       SparkRDDWriteClient client = createHoodieClient(jsc, basePath, false);
       HoodieWriteConfig config = client.getConfig();
       HoodieEngineContext context = client.getEngineContext();
-      HoodieSparkTable table = HoodieSparkTable.create(config, context, true);
+      HoodieSparkTable table = HoodieSparkTable.create(config, context);
       WriteMarkersFactory.get(config.getMarkersType(), table, instantTime)
           .quietDeleteMarkerDir(context, config.getMarkersDeleteParallelism());
       return 0;

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -117,8 +117,7 @@ public boolean commit(String instantTime, List<WriteStatus> writeStatuses, Optio
   }
 
   @Override
-  protected HoodieTable createTable(HoodieWriteConfig config, Configuration hadoopConf,
-                                    boolean refreshTimeline) {
+  protected HoodieTable createTable(HoodieWriteConfig config, Configuration hadoopConf) {
     return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);
   }
 

File: hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/index/bloom/TestFlinkHoodieBloomIndex.java
Patch:
@@ -104,7 +104,7 @@ private HoodieWriteConfig makeConfig(boolean rangePruning, boolean treeFiltering
   public void testLoadInvolvedFiles(boolean rangePruning, boolean treeFiltering, boolean bucketizedChecking) throws Exception {
     HoodieWriteConfig config = makeConfig(rangePruning, treeFiltering, bucketizedChecking);
     HoodieBloomIndex index = new HoodieBloomIndex(config, ListBasedHoodieBloomIndexHelper.getInstance());
-    HoodieTable hoodieTable = HoodieFlinkTable.create(config, context, metaClient, false);
+    HoodieTable hoodieTable = HoodieFlinkTable.create(config, context, metaClient);
     HoodieFlinkWriteableTestTable testTable = HoodieFlinkWriteableTestTable.of(hoodieTable, SCHEMA);
 
     // Create some partitions, and put some files

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java
Patch:
@@ -89,9 +89,7 @@ public boolean commit(String instantTime,
   }
 
   @Override
-  protected HoodieTable createTable(HoodieWriteConfig config,
-                                    Configuration hadoopConf,
-                                    boolean refreshTimeline) {
+  protected HoodieTable createTable(HoodieWriteConfig config, Configuration hadoopConf) {
     return HoodieJavaTable.create(config, context);
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedTableMetadata.java
Patch:
@@ -111,7 +111,7 @@ private void verifyBaseMetadataTable() throws IOException {
     assertEquals(fsPartitions, metadataPartitions, "Partitions should match");
 
     // Files within each partition should match
-    HoodieTable table = HoodieSparkTable.create(writeConfig, context, true);
+    HoodieTable table = HoodieSparkTable.create(writeConfig, context);
     TableFileSystemView tableView = table.getHoodieView();
     List<String> fullPartitionPaths = fsPartitions.stream().map(partition -> basePath + "/" + partition).collect(Collectors.toList());
     Map<String, FileStatus[]> partitionToFilesMap = tableMetadata.getAllFilesInPartitions(fullPartitionPaths);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestHoodieMergeOnReadTable.java
Patch:
@@ -247,7 +247,7 @@ public void testLogFileCountsAfterCompaction(boolean preserveCommitMeta) throws
       assertEquals(allPartitions.size(), testTable.listAllBaseFiles().length);
 
       // Verify that all data file has one log file
-      HoodieTable table = HoodieSparkTable.create(config, context(), metaClient, true);
+      HoodieTable table = HoodieSparkTable.create(config, context(), metaClient);
       for (String partitionPath : dataGen.getPartitionPaths()) {
         List<FileSlice> groupedLogFiles =
             table.getSliceView().getLatestFileSlices(partitionPath).collect(Collectors.toList());

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestHarness.java
Patch:
@@ -559,7 +559,7 @@ public void validateMetadata(HoodieTestTable testTable, List<String> inflightCom
 
     // Files within each partition should match
     metaClient = HoodieTableMetaClient.reload(metaClient);
-    HoodieTable table = HoodieSparkTable.create(writeConfig, engineContext, true);
+    HoodieTable table = HoodieSparkTable.create(writeConfig, engineContext);
     TableFileSystemView tableView = table.getHoodieView();
     List<String> fullPartitionPaths = fsPartitions.stream().map(partition -> basePath + "/" + partition).collect(Collectors.toList());
     Map<String, FileStatus[]> partitionToFilesMap = tableMetadata.getAllFilesInPartitions(fullPartitionPaths);

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java
Patch:
@@ -199,6 +199,7 @@ protected void resetViewState() {
     LOG.info("Deleting all rocksdb data associated with table filesystem view");
     rocksDB.close();
     rocksDB = new RocksDBDAO(metaClient.getBasePath(), config.getRocksdbBasePath());
+    schemaHelper.getAllColumnFamilies().forEach(rocksDB::addColumnFamily);
   }
 
   @Override

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/TimelineService.java
Patch:
@@ -150,7 +150,7 @@ public static class Builder {
       private int markerBatchNumThreads = 20;
       private long markerBatchIntervalMs = 50L;
       private int markerParallelism = 100;
-      private boolean refreshTimelineBasedOnLatestCommit = false;
+      private boolean refreshTimelineBasedOnLatestCommit = true;
 
       public Builder() {
       }
@@ -240,6 +240,7 @@ public Config build() {
         config.markerBatchNumThreads = this.markerBatchNumThreads;
         config.markerBatchIntervalMs = this.markerBatchIntervalMs;
         config.markerParallelism = this.markerParallelism;
+        config.refreshTimelineBasedOnLatestCommit = this.refreshTimelineBasedOnLatestCommit;
         return config;
       }
     }

File: hudi-common/src/test/java/org/apache/hudi/common/fs/TestStorageSchemes.java
Patch:
@@ -49,7 +49,7 @@ public void testStorageSchemes() {
     assertFalse(StorageSchemes.isAppendSupported("bos"));
     assertFalse(StorageSchemes.isAppendSupported("ks3"));
     assertTrue(StorageSchemes.isAppendSupported("ofs"));
-    assertTrue(StorageSchemes.isAppendSupported("oci"));
+    assertFalse(StorageSchemes.isAppendSupported("oci"));
     assertThrows(IllegalArgumentException.class, () -> {
       StorageSchemes.isAppendSupported("s2");
     }, "Should throw exception for unsupported schemes");

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java
Patch:
@@ -364,7 +364,7 @@ private HoodieWriteConfig getWriteConfig() {
 
   private void initJavaSparkContext(Option<String> userDefinedMaster) {
     if (jsc == null) {
-      jsc = SparkUtil.initJavaSparkConf(SparkUtil.getDefaultConf("HoodieCLI", userDefinedMaster));
+      jsc = SparkUtil.initJavaSparkContext(SparkUtil.getDefaultConf("HoodieCLI", userDefinedMaster));
     }
   }
 }
\ No newline at end of file

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -95,7 +95,7 @@ public static void main(String[] args) throws Exception {
     LOG.info("Invoking SparkMain: " + commandString);
     final SparkCommand cmd = SparkCommand.valueOf(commandString);
 
-    JavaSparkContext jsc = SparkUtil.initJavaSparkConf("hoodie-cli-" + commandString,
+    JavaSparkContext jsc = SparkUtil.initJavaSparkContext("hoodie-cli-" + commandString,
         Option.of(args[1]), Option.of(args[2]));
 
     int returnCode = 0;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/UpgradeOrDowngradeCommand.java
Patch:
@@ -56,7 +56,7 @@ public String upgradeHoodieTable(
     if (exitCode != 0) {
       return String.format("Failed: Could not Upgrade/Downgrade Hoodie table to \"%s\".", toVersion);
     }
-    return String.format("Hoodie table upgraded/downgraded to ", toVersion);
+    return String.format("Hoodie table upgraded/downgraded to %s", toVersion);
   }
 
   @CliCommand(value = "downgrade table", help = "Downgrades a table")
@@ -78,6 +78,6 @@ public String downgradeHoodieTable(
     if (exitCode != 0) {
       return String.format("Failed: Could not Upgrade/Downgrade Hoodie table to \"%s\".", toVersion);
     }
-    return String.format("Hoodie table upgraded/downgraded to ", toVersion);
+    return String.format("Hoodie table upgraded/downgraded to %s", toVersion);
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/RunCompactionActionExecutor.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
+import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.CompactionUtils;
 import org.apache.hudi.common.util.Option;
@@ -99,6 +100,7 @@ public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
         metadata.addMetadata(SerDeHelper.LATEST_SCHEMA, schemaPair.getLeft().get());
         metadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, schemaPair.getRight().get());
       }
+      metadata.setOperationType(WriteOperationType.COMPACT);
       compactionMetadata.setWriteStatuses(statuses);
       compactionMetadata.setCommitted(false);
       compactionMetadata.setCommitMetadata(Option.of(metadata));

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -53,7 +53,6 @@
 import org.apache.hudi.io.MiniBatchHandle;
 import org.apache.hudi.metadata.FlinkHoodieBackedTableMetadataWriter;
 import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;
-import org.apache.hudi.metadata.HoodieTableMetadataWriter;
 import org.apache.hudi.table.BulkInsertPartitioner;
 import org.apache.hudi.table.HoodieFlinkTable;
 import org.apache.hudi.table.HoodieTable;
@@ -365,8 +364,7 @@ public void completeCompaction(
       // commit to data table after committing to metadata table.
       // Do not do any conflict resolution here as we do with regular writes. We take the lock here to ensure all writes to metadata table happens within a
       // single lock (single writer). Because more than one write to metadata table will result in conflicts since all of them updates the same partition.
-      table.getMetadataWriter(compactionInstant.getTimestamp()).ifPresent(
-          w -> ((HoodieTableMetadataWriter) w).update(metadata, compactionInstant.getTimestamp(), table.isTableServiceAction(compactionInstant.getAction())));
+      writeTableMetadata(table, compactionCommitTime, compactionInstant.getAction(), metadata);
       LOG.info("Committing Compaction {} finished with result {}.", compactionCommitTime, metadata);
       CompactHelpers.getInstance().completeInflightCompaction(table, compactionCommitTime, metadata);
     } finally {

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestFileSystemViewCommand.java
Patch:
@@ -88,11 +88,11 @@ public void init() throws IOException {
     // Write date files and log file
     String testWriteToken = "1-0-1";
     Files.createFile(Paths.get(fullPartitionPath, FSUtils
-        .makeDataFileName(commitTime1, testWriteToken, fileId1)));
+        .makeBaseFileName(commitTime1, testWriteToken, fileId1)));
     Files.createFile(Paths.get(fullPartitionPath, FSUtils
         .makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime1, 0, testWriteToken)));
     Files.createFile(Paths.get(fullPartitionPath, FSUtils
-        .makeDataFileName(commitTime2, testWriteToken, fileId1)));
+        .makeBaseFileName(commitTime2, testWriteToken, fileId1)));
     Files.createFile(Paths.get(fullPartitionPath, FSUtils
         .makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime2, 0, testWriteToken)));
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -180,7 +180,7 @@ private void init(HoodieRecord record) {
         // base file to denote some log appends happened on a slice. writeToken will still fence concurrent
         // writers.
         // https://issues.apache.org/jira/browse/HUDI-1517
-        createMarkerFile(partitionPath, FSUtils.makeDataFileName(baseInstantTime, writeToken, fileId, hoodieTable.getBaseFileExtension()));
+        createMarkerFile(partitionPath, FSUtils.makeBaseFileName(baseInstantTime, writeToken, fileId, hoodieTable.getBaseFileExtension()));
 
         this.writer = createLogWriter(fileSlice, baseInstantTime);
       } catch (Exception e) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -100,7 +100,7 @@ public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTa
           new Path(config.getBasePath()), FSUtils.getPartitionPath(config.getBasePath(), partitionPath),
           hoodieTable.getPartitionMetafileFormat());
       partitionMetadata.trySave(getPartitionId());
-      createMarkerFile(partitionPath, FSUtils.makeDataFileName(this.instantTime, this.writeToken, this.fileId, hoodieTable.getBaseFileExtension()));
+      createMarkerFile(partitionPath, FSUtils.makeBaseFileName(this.instantTime, this.writeToken, this.fileId, hoodieTable.getBaseFileExtension()));
       this.fileWriter = HoodieFileWriterFactory.getFileWriter(instantTime, path, hoodieTable, config,
         writeSchemaWithMetaFields, this.taskContextSupplier);
     } catch (IOException e) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -174,7 +174,7 @@ private void init(String fileId, String partitionPath, HoodieBaseFile baseFileTo
           hoodieTable.getPartitionMetafileFormat());
       partitionMetadata.trySave(getPartitionId());
 
-      String newFileName = FSUtils.makeDataFileName(instantTime, writeToken, fileId, hoodieTable.getBaseFileExtension());
+      String newFileName = FSUtils.makeBaseFileName(instantTime, writeToken, fileId, hoodieTable.getBaseFileExtension());
       makeOldAndNewFilePaths(partitionPath, latestValidFilePath, newFileName);
 
       LOG.info(String.format("Merging new data into oldPath %s, as newPath %s", oldFilePath.toString(),

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java
Patch:
@@ -164,7 +164,7 @@ public Path makeNewPath(String partitionPath) {
       throw new HoodieIOException("Failed to make dir " + path, e);
     }
 
-    return new Path(path.toString(), FSUtils.makeDataFileName(instantTime, writeToken, fileId,
+    return new Path(path.toString(), FSUtils.makeBaseFileName(instantTime, writeToken, fileId,
         hoodieTable.getMetaClient().getTableConfig().getBaseFileFormat().getFileExtension()));
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/ZeroToOneUpgradeHandler.java
Patch:
@@ -137,6 +137,6 @@ private static String getFileNameForMarkerFromLogFile(String logFilePath, Hoodie
     String baseInstant = FSUtils.getBaseCommitTimeFromLogPath(logPath);
     String writeToken = FSUtils.getWriteTokenFromLogPath(logPath);
 
-    return FSUtils.makeDataFileName(baseInstant, writeToken, fileId, table.getBaseFileFormat().getFileExtension());
+    return FSUtils.makeBaseFileName(baseInstant, writeToken, fileId, table.getBaseFileFormat().getFileExtension());
   }
 }

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/HoodieTestCommitGenerator.java
Patch:
@@ -103,7 +103,7 @@ public static void setupTimelineInFS(
   }
 
   public static String getBaseFilename(String instantTime, String fileId) {
-    return FSUtils.makeDataFileName(instantTime, BASE_FILE_WRITE_TOKEN, fileId);
+    return FSUtils.makeBaseFileName(instantTime, BASE_FILE_WRITE_TOKEN, fileId);
   }
 
   public static String getLogFilename(String instantTime, String fileId) {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkCreateHandle.java
Patch:
@@ -88,7 +88,7 @@ public FlinkCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTab
    */
   private void deleteInvalidDataFile(long lastAttemptId) {
     final String lastWriteToken = FSUtils.makeWriteToken(getPartitionId(), getStageId(), lastAttemptId);
-    final String lastDataFileName = FSUtils.makeDataFileName(instantTime,
+    final String lastDataFileName = FSUtils.makeBaseFileName(instantTime,
         lastWriteToken, this.fileId, hoodieTable.getBaseFileExtension());
     final Path path = makeNewFilePath(partitionPath, lastDataFileName);
     try {
@@ -136,7 +136,7 @@ public boolean canWrite(HoodieRecord record) {
    * Use the writeToken + "-" + rollNumber as the new writeToken of a mini-batch write.
    */
   private Path newFilePathWithRollover(int rollNumber) {
-    final String dataFileName = FSUtils.makeDataFileName(instantTime, writeToken + "-" + rollNumber, fileId,
+    final String dataFileName = FSUtils.makeBaseFileName(instantTime, writeToken + "-" + rollNumber, fileId,
         hoodieTable.getBaseFileExtension());
     return makeNewFilePath(partitionPath, dataFileName);
   }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkMergeAndReplaceHandle.java
Patch:
@@ -90,7 +90,7 @@ public FlinkMergeAndReplaceHandle(HoodieWriteConfig config, String instantTime,
    */
   private void deleteInvalidDataFile(long lastAttemptId) {
     final String lastWriteToken = FSUtils.makeWriteToken(getPartitionId(), getStageId(), lastAttemptId);
-    final String lastDataFileName = FSUtils.makeDataFileName(instantTime,
+    final String lastDataFileName = FSUtils.makeBaseFileName(instantTime,
         lastWriteToken, this.fileId, hoodieTable.getBaseFileExtension());
     final Path path = makeNewFilePath(partitionPath, lastDataFileName);
     try {
@@ -139,7 +139,7 @@ protected void makeOldAndNewFilePaths(String partitionPath, String oldFileName,
   protected String newFileNameWithRollover(int rollNumber) {
     // make the intermediate file as hidden
     final String fileID = "." + this.fileId;
-    return FSUtils.makeDataFileName(instantTime, writeToken + "-" + rollNumber,
+    return FSUtils.makeBaseFileName(instantTime, writeToken + "-" + rollNumber,
         fileID, hoodieTable.getBaseFileExtension());
   }
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkMergeHandle.java
Patch:
@@ -94,7 +94,7 @@ public FlinkMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTabl
    */
   private void deleteInvalidDataFile(long lastAttemptId) {
     final String lastWriteToken = FSUtils.makeWriteToken(getPartitionId(), getStageId(), lastAttemptId);
-    final String lastDataFileName = FSUtils.makeDataFileName(instantTime,
+    final String lastDataFileName = FSUtils.makeBaseFileName(instantTime,
         lastWriteToken, this.fileId, hoodieTable.getBaseFileExtension());
     final Path path = makeNewFilePath(partitionPath, lastDataFileName);
     if (path.equals(oldFilePath)) {
@@ -159,7 +159,7 @@ protected void makeOldAndNewFilePaths(String partitionPath, String oldFileName,
    */
   protected String newFileNameWithRollover(int rollNumber) {
     // make the intermediate file as hidden
-    return FSUtils.makeDataFileName(instantTime, writeToken + "-" + rollNumber,
+    return FSUtils.makeBaseFileName(instantTime, writeToken + "-" + rollNumber,
         this.fileId, hoodieTable.getBaseFileExtension());
   }
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataCreateHandle.java
Patch:
@@ -97,7 +97,7 @@ public HoodieRowDataCreateHandle(HoodieTable table, HoodieWriteConfig writeConfi
               FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath),
               table.getPartitionMetafileFormat());
       partitionMetadata.trySave(taskPartitionId);
-      createMarkerFile(partitionPath, FSUtils.makeDataFileName(this.instantTime, getWriteToken(), this.fileId, table.getBaseFileExtension()));
+      createMarkerFile(partitionPath, FSUtils.makeBaseFileName(this.instantTime, getWriteToken(), this.fileId, table.getBaseFileExtension()));
       this.fileWriter = createNewFileWriter(path, table, writeConfig, rowType);
     } catch (IOException e) {
       throw new HoodieInsertException("Failed to initialize file writer for path " + path, e);
@@ -180,7 +180,7 @@ private Path makeNewPath(String partitionPath) {
       throw new HoodieIOException("Failed to make dir " + path, e);
     }
     HoodieTableConfig tableConfig = table.getMetaClient().getTableConfig();
-    return new Path(path.toString(), FSUtils.makeDataFileName(instantTime, getWriteToken(), fileId,
+    return new Path(path.toString(), FSUtils.makeBaseFileName(instantTime, getWriteToken(), fileId,
         tableConfig.getBaseFileFormat().getFileExtension()));
   }
 

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java
Patch:
@@ -103,7 +103,7 @@ public void testMakeNewPath() {
     }).collect(Collectors.toList()).get(0);
 
     assertEquals(newPathWithWriteToken.getKey().toString(), Paths.get(this.basePath, partitionPath,
-        FSUtils.makeDataFileName(instantTime, newPathWithWriteToken.getRight(), fileName)).toString());
+        FSUtils.makeBaseFileName(instantTime, newPathWithWriteToken.getRight(), fileName)).toString());
   }
 
   private HoodieWriteConfig makeHoodieClientConfig() {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowCreateHandle.java
Patch:
@@ -96,7 +96,7 @@ public HoodieRowCreateHandle(HoodieTable table, HoodieWriteConfig writeConfig, S
               FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath),
               table.getPartitionMetafileFormat());
       partitionMetadata.trySave(taskPartitionId);
-      createMarkerFile(partitionPath, FSUtils.makeDataFileName(this.instantTime, getWriteToken(), this.fileId, table.getBaseFileExtension()));
+      createMarkerFile(partitionPath, FSUtils.makeBaseFileName(this.instantTime, getWriteToken(), this.fileId, table.getBaseFileExtension()));
       this.fileWriter = createNewFileWriter(path, table, writeConfig, structType);
     } catch (IOException e) {
       throw new HoodieInsertException("Failed to initialize file writer for path " + path, e);
@@ -178,7 +178,7 @@ private Path makeNewPath(String partitionPath) {
       throw new HoodieIOException("Failed to make dir " + path, e);
     }
     HoodieTableConfig tableConfig = table.getMetaClient().getTableConfig();
-    return new Path(path.toString(), FSUtils.makeDataFileName(instantTime, getWriteToken(), fileId,
+    return new Path(path.toString(), FSUtils.makeBaseFileName(instantTime, getWriteToken(), fileId,
         tableConfig.getBaseFileFormat().getFileExtension()));
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -2496,7 +2496,7 @@ private Pair<Path, JavaRDD<WriteStatus>> testConsistencyCheck(HoodieTableMetaCli
     Option<Path> markerFilePath = WriteMarkersFactory.get(
             cfg.getMarkersType(), getHoodieTable(metaClient, cfg), instantTime)
         .create(partitionPath,
-            FSUtils.makeDataFileName(instantTime, "1-0-1", UUID.randomUUID().toString()),
+            FSUtils.makeBaseFileName(instantTime, "1-0-1", UUID.randomUUID().toString()),
             IOType.MERGE);
     LOG.info("Created a dummy marker path=" + markerFilePath.get());
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java
Patch:
@@ -127,7 +127,7 @@ public void testMakeNewPath() {
     }).collect().get(0);
 
     assertEquals(newPathWithWriteToken.getKey().toString(), Paths.get(this.basePath, partitionPath,
-        FSUtils.makeDataFileName(instantTime, newPathWithWriteToken.getRight(), fileName)).toString());
+        FSUtils.makeBaseFileName(instantTime, newPathWithWriteToken.getRight(), fileName)).toString());
   }
 
   private HoodieWriteConfig makeHoodieClientConfig() {

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java
Patch:
@@ -159,12 +159,12 @@ public static String makeWriteToken(int taskPartitionId, int stageId, long taskA
   }
 
   // TODO: this should be removed
-  public static String makeDataFileName(String instantTime, String writeToken, String fileId) {
+  public static String makeBaseFileName(String instantTime, String writeToken, String fileId) {
     return String.format("%s_%s_%s%s", fileId, writeToken, instantTime,
         HoodieTableConfig.BASE_FILE_FORMAT.defaultValue().getFileExtension());
   }
 
-  public static String makeDataFileName(String instantTime, String writeToken, String fileId, String fileExtension) {
+  public static String makeBaseFileName(String instantTime, String writeToken, String fileId, String fileExtension) {
     return String.format("%s_%s_%s%s", fileId, writeToken, instantTime, fileExtension);
   }
 

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodieWriteStat.java
Patch:
@@ -45,7 +45,7 @@ public void testSetPaths() {
     Path basePath = new Path(basePathString);
     Path partitionPath = new Path(basePath, partitionPathString);
 
-    Path finalizeFilePath = new Path(partitionPath, FSUtils.makeDataFileName(instantTime, writeToken, fileName));
+    Path finalizeFilePath = new Path(partitionPath, FSUtils.makeBaseFileName(instantTime, writeToken, fileName));
     HoodieWriteStat writeStat = new HoodieWriteStat();
     writeStat.setPath(basePath, finalizeFilePath);
     assertEquals(finalizeFilePath, new Path(basePath, writeStat.getPath()));

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java
Patch:
@@ -826,7 +826,7 @@ private List<Pair<String, HoodieWriteStat>> generateDataForInstant(String baseIn
         File file = new File(basePath + "/" + p + "/"
             + (deltaCommit
             ? FSUtils.makeLogFileName(f, ".log", baseInstant, Integer.parseInt(instant), TEST_WRITE_TOKEN)
-            : FSUtils.makeDataFileName(instant, TEST_WRITE_TOKEN, f)));
+            : FSUtils.makeBaseFileName(instant, TEST_WRITE_TOKEN, f)));
         file.createNewFile();
         HoodieWriteStat w = new HoodieWriteStat();
         w.setFileId(f);

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/FileCreateUtils.java
Patch:
@@ -80,7 +80,7 @@ public static String baseFileName(String instantTime, String fileId) {
   }
 
   public static String baseFileName(String instantTime, String fileId, String fileExtension) {
-    return FSUtils.makeDataFileName(instantTime, WRITE_TOKEN, fileId, fileExtension);
+    return FSUtils.makeBaseFileName(instantTime, WRITE_TOKEN, fileId, fileExtension);
   }
 
   public static String logFileName(String instantTime, String fileId, int version) {

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestClusteringUtils.java
Patch:
@@ -147,7 +147,7 @@ private HoodieInstant createRequestedReplaceInstant(String partitionPath1, Strin
 
   private FileSlice generateFileSlice(String partitionPath, String fileId, String baseInstant) {
     FileSlice fs = new FileSlice(new HoodieFileGroupId(partitionPath, fileId), baseInstant);
-    fs.setBaseFile(new HoodieBaseFile(FSUtils.makeDataFileName(baseInstant, "1-0-1", fileId)));
+    fs.setBaseFile(new HoodieBaseFile(FSUtils.makeBaseFileName(baseInstant, "1-0-1", fileId)));
     return fs;
   }
 

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java
Patch:
@@ -234,7 +234,7 @@ public static void createCOWTableWithSchema(String instantTime, String schemaFil
     fileSystem.mkdirs(partPath);
     List<HoodieWriteStat> writeStats = new ArrayList<>();
     String fileId = UUID.randomUUID().toString();
-    Path filePath = new Path(partPath.toString() + "/" + FSUtils.makeDataFileName(instantTime, "1-0-1", fileId));
+    Path filePath = new Path(partPath.toString() + "/" + FSUtils.makeBaseFileName(instantTime, "1-0-1", fileId));
     Schema schema = SchemaTestUtil.getSchemaFromResource(HiveTestUtil.class, schemaFileName);
     generateParquetDataWithSchema(filePath, schema);
     HoodieWriteStat writeStat = new HoodieWriteStat();
@@ -371,7 +371,7 @@ private static List<HoodieWriteStat> createTestData(Path partPath, boolean isPar
     for (int i = 0; i < 5; i++) {
       // Create 5 files
       String fileId = UUID.randomUUID().toString();
-      Path filePath = new Path(partPath.toString() + "/" + FSUtils.makeDataFileName(instantTime, "1-0-1", fileId));
+      Path filePath = new Path(partPath.toString() + "/" + FSUtils.makeBaseFileName(instantTime, "1-0-1", fileId));
       generateParquetData(filePath, isParquetSchemaSimple);
       HoodieWriteStat writeStat = new HoodieWriteStat();
       writeStat.setFileId(fileId);

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/TestCluster.java
Patch:
@@ -211,7 +211,7 @@ private List<HoodieWriteStat> createTestData(Path partPath, boolean isParquetSch
       // Create 5 files
       String fileId = UUID.randomUUID().toString();
       Path filePath = new Path(partPath.toString() + "/" + FSUtils
-          .makeDataFileName(commitTime, "1-0-1", fileId));
+          .makeBaseFileName(commitTime, "1-0-1", fileId));
       generateParquetData(filePath, isParquetSchemaSimple);
       HoodieWriteStat writeStat = new HoodieWriteStat();
       writeStat.setFileId(fileId);

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/scheduler/DagScheduler.java
Patch:
@@ -117,9 +117,6 @@ private void execute(ExecutorService service, WorkflowDag workflowDag) throws Ex
       if (curRound < workflowDag.getRounds()) {
         new DelayNode(workflowDag.getIntermittentDelayMins()).execute(executionContext, curRound);
       }
-
-      // After each level, report and flush the metrics
-      Metrics.flush();
     } while (curRound++ < workflowDag.getRounds());
     log.info("Finished workloads");
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -846,7 +846,7 @@ private Schema getSchemaForWriteConfig(Schema targetSchema) {
       }
       return newWriteSchema;
     } catch (Exception e) {
-      throw new HoodieException("Failed to fetch schema from table.");
+      throw new HoodieException("Failed to fetch schema from table ", e);
     }
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/config/TypedProperties.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hudi.common.config;
 
+import org.apache.hudi.common.util.StringUtils;
+
 import java.io.Serializable;
 import java.util.Arrays;
 import java.util.Enumeration;
@@ -73,7 +75,7 @@ public List<String> getStringList(String property, String delimiter, List<String
     if (!containsKey(property)) {
       return defaultVal;
     }
-    return Arrays.stream(getProperty(property).split(delimiter)).map(String::trim).collect(Collectors.toList());
+    return Arrays.stream(getProperty(property).split(delimiter)).map(String::trim).filter(s -> !StringUtils.isNullOrEmpty(s)).collect(Collectors.toList());
   }
 
   public int getInteger(String property) {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/InputBatch.java
Patch:
@@ -28,6 +28,7 @@
 
 public class InputBatch<T> {
 
+  public static final Schema NULL_SCHEMA = Schema.create(Schema.Type.NULL);
   private final Option<T> batch;
   private final String checkpointForNextBatch;
   private final SchemaProvider schemaProvider;
@@ -69,7 +70,7 @@ public NullSchemaProvider(TypedProperties props, JavaSparkContext jssc) {
 
     @Override
     public Schema getSourceSchema() {
-      return Schema.create(Schema.Type.NULL);
+      return NULL_SCHEMA;
     }
   }
 }

File: hudi-aws/src/main/java/org/apache/hudi/aws/sync/AWSGlueCatalogSyncClient.java
Patch:
@@ -394,7 +394,7 @@ public void createDatabase(String databaseName) {
   public Option<String> getLastCommitTimeSynced(String tableName) {
     try {
       Table table = getTable(awsGlue, databaseName, tableName);
-      return Option.of(table.getParameters().getOrDefault(HOODIE_LAST_COMMIT_TIME_SYNC, null));
+      return Option.ofNullable(table.getParameters().get(HOODIE_LAST_COMMIT_TIME_SYNC));
     } catch (Exception e) {
       throw new HoodieGlueSyncException("Fail to get last sync commit time for " + tableId(databaseName, tableName), e);
     }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/DeltaGenerator.java
Patch:
@@ -123,7 +123,7 @@ public JavaRDD<GenericRecord> generateInserts(Config operation) {
     int startPartition = operation.getStartPartition();
 
     // Each spark partition below will generate records for a single partition given by the integer index.
-    List<Integer> partitionIndexes = IntStream.rangeClosed(0 + startPartition, numPartitions + startPartition)
+    List<Integer> partitionIndexes = IntStream.rangeClosed(0 + startPartition, numPartitions + startPartition - 1)
         .boxed().collect(Collectors.toList());
 
     JavaRDD<GenericRecord> inputBatch = jsc.parallelize(partitionIndexes, numPartitions)

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/GenericRecordFullPayloadGenerator.java
Patch:
@@ -338,7 +338,7 @@ public boolean validate(GenericRecord record) {
    */
   @VisibleForTesting
   public GenericRecord updateTimestamp(GenericRecord record, String fieldName) {
-    long delta = TimeUnit.SECONDS.convert((++partitionIndex % numDatePartitions) + startPartition, TimeUnit.DAYS);
+    long delta = TimeUnit.SECONDS.convert((partitionIndex++ % numDatePartitions) + startPartition, TimeUnit.DAYS);
     record.put(fieldName, delta);
     return record;
   }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/HoodieDeltaStreamerTestBase.java
Patch:
@@ -30,7 +30,7 @@
 import org.apache.hudi.hive.HiveSyncConfig;
 import org.apache.hudi.hive.MultiPartKeysValueExtractor;
 import org.apache.hudi.utilities.schema.FilebasedSchemaProvider;
-import org.apache.hudi.utilities.sources.TestParquetDFSSourceEmptyBatch;
+import org.apache.hudi.utilities.sources.TestDataSource;
 import org.apache.hudi.utilities.testutils.UtilitiesTestBase;
 
 import org.apache.avro.Schema;
@@ -192,7 +192,7 @@ protected static void writeCommonPropsToFile(FileSystem dfs, String dfsBasePath)
   @BeforeEach
   public void setup() throws Exception {
     super.setup();
-    TestParquetDFSSourceEmptyBatch.returnEmptyBatch = false;
+    TestDataSource.returnEmptyBatch = false;
   }
 
   @AfterAll

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/GlobalDeleteKeyGenerator.java
Patch:
@@ -60,8 +60,8 @@ public List<String> getPartitionPathFields() {
 
   @Override
   public String getRecordKey(Row row) {
-    buildFieldPositionMapIfNeeded(row.schema());
-    return RowKeyGeneratorHelper.getRecordKeyFromRow(row, getRecordKeyFields(), recordKeyPositions, true);
+    buildFieldSchemaInfoIfNeeded(row.schema());
+    return RowKeyGeneratorHelper.getRecordKeyFromRow(row, getRecordKeyFields(), recordKeySchemaInfo, true);
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java
Patch:
@@ -400,7 +400,7 @@ public static GenericRecord rewriteRecordWithMetadata(GenericRecord genericRecor
       copyOldValueOrSetDefault(genericRecord, newRecord, f);
     }
     // do not preserve FILENAME_METADATA_FIELD
-    newRecord.put(HoodieRecord.FILENAME_METADATA_FIELD_POS, fileName);
+    newRecord.put(HoodieRecord.FILENAME_META_FIELD_POS, fileName);
     if (!GenericData.get().validate(newSchema, newRecord)) {
       throw new SchemaCompatibilityException(
           "Unable to validate the rewritten record " + genericRecord + " against schema " + newSchema);
@@ -412,7 +412,7 @@ public static GenericRecord rewriteRecordWithMetadata(GenericRecord genericRecor
   public static GenericRecord rewriteEvolutionRecordWithMetadata(GenericRecord genericRecord, Schema newSchema, String fileName) {
     GenericRecord newRecord = HoodieAvroUtils.rewriteRecordWithNewSchema(genericRecord, newSchema, new HashMap<>());
     // do not preserve FILENAME_METADATA_FIELD
-    newRecord.put(HoodieRecord.FILENAME_METADATA_FIELD_POS, fileName);
+    newRecord.put(HoodieRecord.FILENAME_META_FIELD_POS, fileName);
     return newRecord;
   }
 

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BulkInsertDataInternalWriterHelper.java
Patch:
@@ -25,8 +25,8 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieIOException;
-import org.apache.hudi.io.storage.row.HoodieRowCreateHandleWithoutMetaFields;
 import org.apache.hudi.io.storage.row.HoodieRowCreateHandle;
+import org.apache.hudi.io.storage.row.HoodieRowCreateHandleWithoutMetaFields;
 import org.apache.hudi.keygen.BuiltinKeyGenerator;
 import org.apache.hudi.keygen.NonpartitionedKeyGenerator;
 import org.apache.hudi.keygen.SimpleKeyGenerator;
@@ -123,8 +123,7 @@ public void write(InternalRow record) throws IOException {
     try {
       String partitionPath = null;
       if (populateMetaFields) { // usual path where meta fields are pre populated in prep step.
-        partitionPath = record.getUTF8String(
-            HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.get(HoodieRecord.PARTITION_PATH_METADATA_FIELD)).toString();
+        partitionPath = String.valueOf(record.getUTF8String(HoodieRecord.PARTITION_PATH_META_FIELD_POS));
       } else { // if meta columns are disabled.
         if (!keyGeneratorOpt.isPresent()) { // NoPartitionerKeyGen
           partitionPath = "";

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestComplexKeyGenerator.java
Patch:
@@ -83,7 +83,7 @@ public void testNullRecordKeyFields() {
   public void testWrongRecordKeyField() {
     ComplexKeyGenerator keyGenerator = new ComplexKeyGenerator(getWrongRecordKeyFieldProps());
     Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.getRecordKey(getRecord()));
-    Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.buildFieldPositionMapIfNeeded(KeyGeneratorTestUtilities.structType));
+    Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.buildFieldSchemaInfoIfNeeded(KeyGeneratorTestUtilities.structType));
   }
 
   @Test

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestGlobalDeleteRecordGenerator.java
Patch:
@@ -68,7 +68,7 @@ public void testNullRecordKeyFields() {
   public void testWrongRecordKeyField() {
     GlobalDeleteKeyGenerator keyGenerator = new GlobalDeleteKeyGenerator(getWrongRecordKeyFieldProps());
     Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.getRecordKey(getRecord()));
-    Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.buildFieldPositionMapIfNeeded(KeyGeneratorTestUtilities.structType));
+    Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.buildFieldSchemaInfoIfNeeded(KeyGeneratorTestUtilities.structType));
   }
 
   @Test
@@ -78,7 +78,7 @@ public void testHappyFlow() {
     HoodieKey key = keyGenerator.getKey(record);
     Assertions.assertEquals(key.getRecordKey(), "_row_key:key1,pii_col:pi");
     Assertions.assertEquals(key.getPartitionPath(), "");
-    keyGenerator.buildFieldPositionMapIfNeeded(KeyGeneratorTestUtilities.structType);
+    keyGenerator.buildFieldSchemaInfoIfNeeded(KeyGeneratorTestUtilities.structType);
     Row row = KeyGeneratorTestUtilities.getRow(record);
     Assertions.assertEquals(keyGenerator.getRecordKey(row), "_row_key:key1,pii_col:pi");
     Assertions.assertEquals(keyGenerator.getPartitionPath(row), "");

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestNonpartitionedKeyGenerator.java
Patch:
@@ -94,7 +94,7 @@ public void testNullPartitionPathFields() {
   public void testWrongRecordKeyField() {
     NonpartitionedKeyGenerator keyGenerator = new NonpartitionedKeyGenerator(getWrongRecordKeyFieldProps());
     Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.getRecordKey(getRecord()));
-    Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.buildFieldPositionMapIfNeeded(KeyGeneratorTestUtilities.structType));
+    Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.buildFieldSchemaInfoIfNeeded(KeyGeneratorTestUtilities.structType));
   }
 
   @Test

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/keygen/TestSimpleKeyGenerator.java
Patch:
@@ -100,7 +100,7 @@ public void testNullRecordKeyFields() {
   public void testWrongRecordKeyField() {
     SimpleKeyGenerator keyGenerator = new SimpleKeyGenerator(getWrongRecordKeyFieldProps());
     Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.getRecordKey(getRecord()));
-    Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.buildFieldPositionMapIfNeeded(KeyGeneratorTestUtilities.structType));
+    Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.buildFieldSchemaInfoIfNeeded(KeyGeneratorTestUtilities.structType));
   }
 
   @Test
@@ -116,7 +116,7 @@ public void testWrongPartitionPathField() {
   public void testComplexRecordKeyField() {
     SimpleKeyGenerator keyGenerator = new SimpleKeyGenerator(getComplexRecordKeyProp());
     Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.getRecordKey(getRecord()));
-    Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.buildFieldPositionMapIfNeeded(KeyGeneratorTestUtilities.structType));
+    Assertions.assertThrows(HoodieKeyException.class, () -> keyGenerator.buildFieldSchemaInfoIfNeeded(KeyGeneratorTestUtilities.structType));
   }
 
   @Test

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/HoodieDeltaStreamerTestBase.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hudi.hive.HiveSyncConfig;
 import org.apache.hudi.hive.MultiPartKeysValueExtractor;
 import org.apache.hudi.utilities.schema.FilebasedSchemaProvider;
+import org.apache.hudi.utilities.sources.TestParquetDFSSourceEmptyBatch;
 import org.apache.hudi.utilities.testutils.UtilitiesTestBase;
 
 import org.apache.avro.Schema;
@@ -191,6 +192,7 @@ protected static void writeCommonPropsToFile(FileSystem dfs, String dfsBasePath)
   @BeforeEach
   public void setup() throws Exception {
     super.setup();
+    TestParquetDFSSourceEmptyBatch.returnEmptyBatch = false;
   }
 
   @AfterAll

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -2468,6 +2468,8 @@ protected void setDefaults() {
       writeConfig.setDefaultValue(MARKERS_TYPE, getDefaultMarkersType(engineType));
       // Check for mandatory properties
       writeConfig.setDefaults(HoodieWriteConfig.class.getName());
+      // Set default values of HoodieHBaseIndexConfig
+      writeConfig.setDefaults(HoodieHBaseIndexConfig.class.getName());
       // Make sure the props is propagated
       writeConfig.setDefaultOnCondition(
           !isIndexConfigSet, HoodieIndexConfig.newBuilder().withEngineType(engineType).fromProperties(

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
Patch:
@@ -279,6 +279,7 @@ private static SparkConf buildSparkConf(String appName, String defaultMaster, Ma
     sparkConf.set("spark.hadoop.mapred.output.compression.codec", "true");
     sparkConf.set("spark.hadoop.mapred.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec");
     sparkConf.set("spark.hadoop.mapred.output.compression.type", "BLOCK");
+    sparkConf.set("spark.driver.allowMultipleContexts", "true");
 
     additionalConfigs.forEach(sparkConf::set);
     return SparkRDDWriteClient.registerClasses(sparkConf);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java
Patch:
@@ -87,7 +87,7 @@ public class HoodieClusteringConfig extends HoodieConfig {
       .key(CLUSTERING_STRATEGY_PARAM_PREFIX + "small.file.limit")
       .defaultValue(String.valueOf(300 * 1024 * 1024L))
       .sinceVersion("0.7.0")
-      .withDocumentation("Files smaller than the size specified here are candidates for clustering");
+      .withDocumentation("Files smaller than the size in bytes specified here are candidates for clustering");
 
   public static final ConfigProperty<String> PARTITION_REGEX_PATTERN = ConfigProperty
       .key(CLUSTERING_STRATEGY_PARAM_PREFIX + "partition.regex.pattern")

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieMemoryConfig.java
Patch:
@@ -65,17 +65,17 @@ public class HoodieMemoryConfig extends HoodieConfig {
   public static final ConfigProperty<Long> MAX_MEMORY_FOR_MERGE = ConfigProperty
       .key("hoodie.memory.merge.max.size")
       .defaultValue(DEFAULT_MAX_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES)
-      .withDocumentation("Maximum amount of memory used for merge operations, before spilling to local storage.");
+      .withDocumentation("Maximum amount of memory used  in bytes for merge operations, before spilling to local storage.");
 
   public static final ConfigProperty<String> MAX_MEMORY_FOR_COMPACTION = ConfigProperty
       .key("hoodie.memory.compaction.max.size")
       .noDefaultValue()
-      .withDocumentation("Maximum amount of memory used for compaction operations, before spilling to local storage.");
+      .withDocumentation("Maximum amount of memory used  in bytes for compaction operations in bytes , before spilling to local storage.");
 
   public static final ConfigProperty<Integer> MAX_DFS_STREAM_BUFFER_SIZE = ConfigProperty
       .key("hoodie.memory.dfs.buffer.max.size")
       .defaultValue(16 * 1024 * 1024)
-      .withDocumentation("Property to control the max memory for dfs input stream buffer size");
+      .withDocumentation("Property to control the max memory in bytes for dfs input stream buffer size");
 
   public static final ConfigProperty<String> SPILLABLE_MAP_BASE_PATH = ConfigProperty
       .key("hoodie.memory.spillable.map.path")

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -150,7 +150,7 @@ public class HoodieWriteConfig extends HoodieConfig {
       .key("hoodie.table.base.file.format")
       .defaultValue(HoodieFileFormat.PARQUET)
       .withAlternatives("hoodie.table.ro.file.format")
-      .withDocumentation("");
+      .withDocumentation("Base file format to store all the base file data.");
 
   public static final ConfigProperty<String> BASE_PATH = ConfigProperty
       .key("hoodie.base.path")

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewStorageConfig.java
Patch:
@@ -78,7 +78,7 @@ public class FileSystemViewStorageConfig extends HoodieConfig {
   public static final ConfigProperty<Long> SPILLABLE_MEMORY = ConfigProperty
       .key("hoodie.filesystem.view.spillable.mem")
       .defaultValue(100 * 1024 * 1024L) // 100 MB
-      .withDocumentation("Amount of memory to be used for holding file system view, before spilling to disk.");
+      .withDocumentation("Amount of memory to be used in bytes for holding file system view, before spilling to disk.");
 
   public static final ConfigProperty<Double> SPILLABLE_COMPACTION_MEM_FRACTION = ConfigProperty
       .key("hoodie.filesystem.view.spillable.compaction.mem.fraction")

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -137,7 +137,7 @@ private FlinkOptions() {
       .key("index.partition.regex")
       .stringType()
       .defaultValue(".*")
-      .withDescription("Whether to load partitions in state if partition path matching， default *");
+      .withDescription("Whether to load partitions in state if partition path matching， default `*`");
 
   // ------------------------------------------------------------------------
   //  Read Options
@@ -542,7 +542,7 @@ private FlinkOptions() {
       .key("compaction.target_io")
       .longType()
       .defaultValue(500 * 1024L) // default 500 GB
-      .withDescription("Target IO per compaction (both read and write), default 500 GB");
+      .withDescription("Target IO in MB for per compaction (both read and write), default 500 GB");
 
   public static final ConfigOption<Boolean> CLEAN_ASYNC_ENABLED = ConfigOptions
       .key("clean.async.enabled")

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -103,7 +103,7 @@ private FlinkOptions() {
       .key("metadata.compaction.delta_commits")
       .intType()
       .defaultValue(10)
-      .withDescription("Max delta commits for metadata table to trigger compaction, default 24");
+      .withDescription("Max delta commits for metadata table to trigger compaction, default 10");
 
   // ------------------------------------------------------------------------
   //  Index Options

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -367,13 +367,14 @@ private FlinkOptions() {
 
   public static final String PARTITION_FORMAT_HOUR = "yyyyMMddHH";
   public static final String PARTITION_FORMAT_DAY = "yyyyMMdd";
+  public static final String PARTITION_FORMAT_DASHED_DAY = "yyyy-MM-dd";
   public static final ConfigOption<String> PARTITION_FORMAT = ConfigOptions
       .key("write.partition.format")
       .stringType()
       .noDefaultValue()
       .withDescription("Partition path format, only valid when 'write.datetime.partitioning' is true, default is:\n"
           + "1) 'yyyyMMddHH' for timestamp(3) WITHOUT TIME ZONE, LONG, FLOAT, DOUBLE, DECIMAL;\n"
-          + "2) 'yyyyMMdd' for DAY and INT.");
+          + "2) 'yyyyMMdd' for DATE and INT.");
 
   public static final ConfigOption<Integer> INDEX_BOOTSTRAP_TASKS = ConfigOptions
       .key("write.index_bootstrap.tasks")

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/TestHoodieTableFactory.java
Patch:
@@ -419,7 +419,6 @@ void testSetupCleaningOptionsForSink() {
   @Test
   void testSetupTimestampBasedKeyGenForSink() {
     this.conf.setString(FlinkOptions.RECORD_KEY_FIELD, "dummyField");
-    this.conf.setString(FlinkOptions.KEYGEN_CLASS_NAME, "dummyKeyGenClass");
     // definition with simple primary key and partition path
     ResolvedSchema schema1 = SchemaBuilder.instance()
         .field("f0", DataTypes.INT().notNull())

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java
Patch:
@@ -58,6 +58,7 @@
 import java.util.ArrayDeque;
 import java.util.Arrays;
 import java.util.Deque;
+import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
@@ -379,7 +380,7 @@ private void processDataBlock(HoodieDataBlock dataBlock, Option<KeySpec> keySpec
       Option<Schema> schemaOption = getMergedSchema(dataBlock);
       while (recordIterator.hasNext()) {
         IndexedRecord currentRecord = recordIterator.next();
-        IndexedRecord record = schemaOption.isPresent() ? HoodieAvroUtils.rewriteRecordWithNewSchema(currentRecord, schemaOption.get()) : currentRecord;
+        IndexedRecord record = schemaOption.isPresent() ? HoodieAvroUtils.rewriteRecordWithNewSchema(currentRecord, schemaOption.get(), new HashMap<>()) : currentRecord;
         processNextRecord(createHoodieRecord(record, this.hoodieTableMetaClient.getTableConfig(), this.payloadClassFQN,
             this.preCombineField, this.withOperationField, this.simpleKeyGenFields, this.partitionName));
         totalLogRecords.incrementAndGet();

File: hudi-common/src/test/java/org/apache/hudi/internal/schema/utils/TestAvroSchemaEvolutionUtils.java
Patch:
@@ -284,7 +284,7 @@ public void testReWriteRecordWithTypeChanged() {
         .updateColumnType("col6", Types.StringType.get());
     InternalSchema newSchema = SchemaChangeUtils.applyTableChanges2Schema(internalSchema, updateChange);
     Schema newAvroSchema = AvroInternalSchemaConverter.convert(newSchema, avroSchema.getName());
-    GenericRecord newRecord = HoodieAvroUtils.rewriteRecordWithNewSchema(avroRecord, newAvroSchema);
+    GenericRecord newRecord = HoodieAvroUtils.rewriteRecordWithNewSchema(avroRecord, newAvroSchema, new HashMap<>());
 
     Assertions.assertEquals(GenericData.get().validate(newAvroSchema, newRecord), true);
   }
@@ -349,7 +349,7 @@ public void testReWriteNestRecord() {
     );
 
     Schema newAvroSchema = AvroInternalSchemaConverter.convert(newRecord, schema.getName());
-    GenericRecord newAvroRecord = HoodieAvroUtils.rewriteRecordWithNewSchema(avroRecord, newAvroSchema);
+    GenericRecord newAvroRecord = HoodieAvroUtils.rewriteRecordWithNewSchema(avroRecord, newAvroSchema, new HashMap<>());
     // test the correctly of rewrite
     Assertions.assertEquals(GenericData.get().validate(newAvroSchema, newAvroRecord), true);
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/LockManager.java
Patch:
@@ -70,11 +70,12 @@ public void lock() {
           }
           LOG.info("Retrying to acquire lock...");
           Thread.sleep(maxWaitTimeInMs);
-          retryCount++;
         } catch (HoodieLockException | InterruptedException e) {
           if (retryCount >= maxRetries) {
             throw new HoodieLockException("Unable to acquire lock, lock object ", e);
           }
+        } finally {
+          retryCount++;
         }
       }
       if (!acquired) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java
Patch:
@@ -92,7 +92,7 @@ public class HoodieLockConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> LOCK_ACQUIRE_CLIENT_NUM_RETRIES = ConfigProperty
       .key(LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP_KEY)
-      .defaultValue(String.valueOf(0))
+      .defaultValue(String.valueOf(10))
       .sinceVersion("0.8.0")
       .withDocumentation("Maximum number of times to retry to acquire lock additionally from the lock manager.");
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodiePartitionMetadata.java
Patch:
@@ -213,7 +213,7 @@ private boolean readTextFormatMetaFile() {
       format = Option.empty();
       return true;
     } catch (Throwable t) {
-      LOG.warn("Unable to read partition meta properties file for partition " + partitionPath, t);
+      LOG.debug("Unable to read partition meta properties file for partition " + partitionPath);
       return false;
     }
   }
@@ -229,8 +229,7 @@ private boolean readBaseFormatMetaFile() {
         format = Option.of(reader.getFormat());
         return true;
       } catch (Throwable t) {
-        // any error, log, check the next base format
-        LOG.warn("Unable to read partition metadata " + metafilePath.getName() + " for partition " + partitionPath, t);
+        LOG.debug("Unable to read partition metadata " + metafilePath.getName() + " for partition " + partitionPath);
       }
     }
     return false;

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -607,7 +607,7 @@ public String getUrlEncodePartitioning() {
     return getString(URL_ENCODE_PARTITIONING);
   }
 
-  public Boolean isDropPartitionColumns() {
+  public Boolean shouldDropPartitionColumns() {
     return getBooleanOrDefault(DROP_PARTITION_COLUMNS);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java
Patch:
@@ -89,10 +89,10 @@
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
+import static org.apache.hudi.avro.AvroSchemaUtils.resolveNullableSchema;
 import static org.apache.hudi.avro.HoodieAvroUtils.addMetadataFields;
 import static org.apache.hudi.avro.HoodieAvroUtils.convertValueForSpecificDataTypes;
 import static org.apache.hudi.avro.HoodieAvroUtils.getNestedFieldSchemaFromWriteSchema;
-import static org.apache.hudi.avro.HoodieAvroUtils.resolveNullableSchema;
 import static org.apache.hudi.common.util.StringUtils.isNullOrEmpty;
 import static org.apache.hudi.common.util.ValidationUtils.checkState;
 import static org.apache.hudi.metadata.HoodieMetadataPayload.unwrapStatisticValueWrapper;

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestTableSchemaResolver.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.avro.Schema;
 
+import org.apache.hudi.avro.AvroSchemaUtils;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
 
@@ -57,7 +58,7 @@ public void testRecreateSchemaWhenDropPartitionColumns() {
     assertNotEquals(originSchema, s4);
     assertTrue(s4.getFields().stream().anyMatch(f -> f.name().equals("user_partition")));
     Schema.Field f = s4.getField("user_partition");
-    assertEquals(f.schema().getType().getName(), "string");
+    assertEquals(f.schema(), AvroSchemaUtils.createNullableSchema(Schema.Type.STRING));
 
     // case5: user_partition is in originSchema, but partition_path is in originSchema
     String[] pts4 = {"user_partition", "partition_path"};

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieConcatHandle.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.HoodieBaseFile;
+import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieRecordPayload;
@@ -93,7 +94,8 @@ public HoodieConcatHandle(HoodieWriteConfig config, String instantTime, HoodieTa
   public void write(GenericRecord oldRecord) {
     String key = KeyGenUtils.getRecordKeyFromGenericRecord(oldRecord, keyGeneratorOpt);
     try {
-      fileWriter.writeAvro(key, oldRecord);
+      // NOTE: We're enforcing preservation of the record metadata to keep existing semantic
+      writeToFile(new HoodieKey(key, partitionPath), oldRecord, true);
     } catch (IOException | RuntimeException e) {
       String errMsg = String.format("Failed to write old record into new file for key %s from old file %s to new file %s with writerSchema %s",
           key, getOldFilePath(), newFilePath, writeSchemaWithMetaFields.toString(true));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -142,7 +142,7 @@ public void write(HoodieRecord record, Option<IndexedRecord> avroRecord) {
           fileWriter.writeAvro(record.getRecordKey(),
               rewriteRecordWithMetadata((GenericRecord) avroRecord.get(), path.getName()));
         } else {
-          fileWriter.writeAvroWithMetadata(rewriteRecord((GenericRecord) avroRecord.get()), record);
+          fileWriter.writeAvroWithMetadata(record.getKey(), rewriteRecord((GenericRecord) avroRecord.get()));
         }
         // update the new location of record, so we know where to find it next
         record.unseal();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java
Patch:
@@ -47,7 +47,7 @@
  */
 public class HoodieSortedMergeHandle<T extends HoodieRecordPayload, I, K, O> extends HoodieMergeHandle<T, I, K, O> {
 
-  private Queue<String> newRecordKeysSorted = new PriorityQueue<>();
+  private final Queue<String> newRecordKeysSorted = new PriorityQueue<>();
 
   public HoodieSortedMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T, I, K, O> hoodieTable,
                                  Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId, TaskContextSupplier taskContextSupplier,

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/io/storage/TestHoodieHFileReaderWriter.java
Patch:
@@ -154,8 +154,9 @@ public void testWriteReadHFileWithMetaFields(boolean populateMetaFields, boolean
       record.put("time", Integer.toString(RANDOM.nextInt()));
       record.put("number", i);
       if (testAvroWithMeta) {
-        writer.writeAvroWithMetadata(record, new HoodieAvroRecord(new HoodieKey((String) record.get("_row_key"),
-            Integer.toString((Integer) record.get("number"))), new EmptyHoodieRecordPayload())); // payload does not matter. GenericRecord passed in is what matters
+        // payload does not matter. GenericRecord passed in is what matters
+        writer.writeAvroWithMetadata(new HoodieAvroRecord(new HoodieKey((String) record.get("_row_key"),
+                Integer.toString((Integer) record.get("number"))), new EmptyHoodieRecordPayload()).getKey(), record);
         // only HoodieKey will be looked up from the 2nd arg(HoodieRecord).
       } else {
         writer.writeAvro(key, record);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java
Patch:
@@ -744,7 +744,7 @@ public void testCleanEmptyInstants() throws Exception {
 
     for (int i = 0; i < cleanCount; i++, startInstant++) {
       String commitTime = makeNewCommitTime(startInstant, "%09d");
-      createCleanMetadata(commitTime + "", false, true);
+      createEmptyCleanMetadata(commitTime + "", false);
     }
 
     int instantClean = startInstant;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/HoodieTimelineArchiver.java
Patch:
@@ -157,7 +157,8 @@ public boolean archiveIfRequired(HoodieEngineContext context) throws IOException
   public boolean archiveIfRequired(HoodieEngineContext context, boolean acquireLock) throws IOException {
     try {
       if (acquireLock) {
-        txnManager.beginTransaction();
+        // there is no owner or instant time per se for archival.
+        txnManager.beginTransaction(Option.empty(), Option.empty());
       }
       List<HoodieInstant> instantsToArchive = getInstantsToArchive().collect(Collectors.toList());
       verifyLastMergeArchiveFilesIfNecessary(context);
@@ -179,7 +180,7 @@ public boolean archiveIfRequired(HoodieEngineContext context, boolean acquireLoc
     } finally {
       close();
       if (acquireLock) {
-        txnManager.endTransaction();
+        txnManager.endTransaction(Option.empty());
       }
     }
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java
Patch:
@@ -232,14 +232,14 @@ private void updateTableConfigAndTimeline(HoodieInstant indexInstant,
                                             HoodieIndexCommitMetadata indexCommitMetadata) throws IOException {
     try {
       // update the table config and timeline in a lock as there could be another indexer running
-      txnManager.beginTransaction();
+      txnManager.beginTransaction(Option.of(indexInstant), Option.empty());
       updateMetadataPartitionsTableConfig(table.getMetaClient(),
           finalIndexPartitionInfos.stream().map(HoodieIndexPartitionInfo::getMetadataPartitionPath).collect(Collectors.toSet()));
       table.getActiveTimeline().saveAsComplete(
           new HoodieInstant(true, INDEXING_ACTION, indexInstant.getTimestamp()),
           TimelineMetadataUtils.serializeIndexCommitMetadata(indexCommitMetadata));
     } finally {
-      txnManager.endTransaction();
+      txnManager.endTransaction(Option.of(indexInstant));
     }
   }
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/BaseValidateDatasetNode.java
Patch:
@@ -81,7 +81,7 @@ public void execute(ExecutionContext context, int curItrCount) throws Exception
       SparkSession session = SparkSession.builder().sparkContext(context.getJsc().sc()).getOrCreate();
       // todo: Fix partitioning schemes. For now, assumes data based partitioning.
       String inputPath = context.getHoodieTestSuiteWriter().getCfg().inputBasePath + "/*/*";
-      log.warn("Validation using data from input path " + inputPath);
+      log.info("Validation using data from input path " + inputPath);
       // listing batches to be validated
       String inputPathStr = context.getHoodieTestSuiteWriter().getCfg().inputBasePath;
       if (log.isDebugEnabled()) {
@@ -166,7 +166,7 @@ private Dataset<Row> getInputDf(ExecutionContext context, SparkSession session,
     ExpressionEncoder encoder = getEncoder(inputDf.schema());
     return inputDf.groupByKey(
             (MapFunction<Row, String>) value ->
-                value.getAs(partitionPathField) + "+" + value.getAs(recordKeyField), Encoders.STRING())
+                (partitionPathField.isEmpty() ? value.getAs(recordKeyField) : (value.getAs(partitionPathField) + "+" + value.getAs(recordKeyField))), Encoders.STRING())
         .reduceGroups((ReduceFunction<Row>) (v1, v2) -> {
           int ts1 = v1.getAs(SchemaUtils.SOURCE_ORDERING_FIELD);
           int ts2 = v2.getAs(SchemaUtils.SOURCE_ORDERING_FIELD);

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieRecord;
+import org.apache.hudi.DataSourceWriteOptions;
 import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;
 import org.apache.hudi.integ.testsuite.dag.ExecutionContext;
 import org.apache.spark.sql.Dataset;
@@ -48,8 +49,8 @@ public Logger getLogger() {
   @Override
   public Dataset<Row> getDatasetToValidate(SparkSession session, ExecutionContext context,
                                            StructType inputSchema) {
-    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + "/*/*/*";
-    log.info("Validate data in target hudi path " + hudiPath);
+    String partitionPathField = context.getWriterContext().getProps().getString(DataSourceWriteOptions.PARTITIONPATH_FIELD().key());
+    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + (partitionPathField.isEmpty() ? "/" : "/*/*/*");
     Dataset<Row> hudiDf = session.read().option(HoodieMetadataConfig.ENABLE.key(), String.valueOf(config.isEnableMetadataValidate()))
         .format("hudi").load(hudiPath);
     return hudiDf.drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)

File: hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java
Patch:
@@ -48,7 +48,7 @@
 import org.apache.hadoop.security.Credentials;
 import org.apache.hadoop.security.token.Token;
 import org.apache.hadoop.util.Progressable;
-import org.apache.hudi.hadoop.FileNameCachingPath;
+import org.apache.hudi.hadoop.CachingPath;
 
 import java.io.IOException;
 import java.net.URI;
@@ -142,7 +142,7 @@ public static Path convertPathWithScheme(Path oldPath, String newScheme) {
     try {
       newURI = new URI(newScheme, oldURI.getUserInfo(), oldURI.getHost(), oldURI.getPort(), oldURI.getPath(),
           oldURI.getQuery(), oldURI.getFragment());
-      return new FileNameCachingPath(newURI);
+      return new CachingPath(newURI);
     } catch (URISyntaxException e) {
       // TODO - Better Exception handling
       throw new RuntimeException(e);

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -95,7 +95,7 @@ public abstract class AbstractTableFileSystemView implements SyncableFileSystemV
   private BootstrapIndex bootstrapIndex;
 
   private String getPartitionPathFromFilePath(String fullPath) {
-    return FSUtils.getRelativePartitionPath(new Path(metaClient.getBasePath()), new Path(fullPath).getParent());
+    return FSUtils.getRelativePartitionPath(metaClient.getBasePathV2(), new Path(fullPath).getParent());
   }
 
   /**
@@ -172,7 +172,7 @@ protected List<HoodieFileGroup> buildFileGroups(Stream<HoodieBaseFile> baseFileS
 
     Map<Pair<String, String>, List<HoodieLogFile>> logFiles = logFileStream.collect(Collectors.groupingBy((logFile) -> {
       String partitionPathStr =
-          FSUtils.getRelativePartitionPath(new Path(metaClient.getBasePath()), logFile.getPath().getParent());
+          FSUtils.getRelativePartitionPath(metaClient.getBasePathV2(), logFile.getPath().getParent());
       return Pair.of(partitionPathStr, logFile.getFileId());
     }));
 
@@ -299,7 +299,7 @@ private void ensurePartitionLoadedCorrectly(String partition) {
         try {
           LOG.info("Building file system view for partition (" + partitionPathStr + ")");
 
-          Path partitionPath = FSUtils.getPartitionPath(metaClient.getBasePath(), partitionPathStr);
+          Path partitionPath = FSUtils.getPartitionPath(metaClient.getBasePathV2(), partitionPathStr);
           long beginLsTs = System.currentTimeMillis();
           FileStatus[] statuses = listPartition(partitionPath);
           long endLsTs = System.currentTimeMillis();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -338,8 +338,7 @@ public void write(GenericRecord oldRecord) {
         } else if (writeUpdateRecord(hoodieRecord, oldRecord, combinedAvroRecord)) {
           /*
            * ONLY WHEN 1) we have an update for this key AND 2) We are able to successfully
-           * write the the combined new
-           * value
+           * write the combined new value
            *
            * We no longer need to copy the old record over.
            */

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -1557,11 +1557,11 @@ public boolean isMetadataColumnStatsIndexEnabled() {
     return isMetadataTableEnabled() && getMetadataConfig().isColumnStatsIndexEnabled();
   }
 
-  public String getColumnsEnabledForColumnStatsIndex() {
+  public List<String> getColumnsEnabledForColumnStatsIndex() {
     return getMetadataConfig().getColumnsEnabledForColumnStatsIndex();
   }
 
-  public String getColumnsEnabledForBloomFilterIndex() {
+  public List<String> getColumnsEnabledForBloomFilterIndex() {
     return getMetadataConfig().getColumnsEnabledForBloomFilterIndex();
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieConfig.java
Patch:
@@ -38,6 +38,8 @@ public class HoodieConfig implements Serializable {
 
   private static final Logger LOG = LogManager.getLogger(HoodieConfig.class);
 
+  protected static final String CONFIG_VALUES_DELIMITER = ",";
+
   public static HoodieConfig create(FSDataInputStream inputStream) throws IOException {
     HoodieConfig config = new HoodieConfig();
     config.props.load(inputStream);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java
Patch:
@@ -926,7 +926,6 @@ public HoodieMetadataValidationContext(
           .enable(enableMetadataTable)
           .withMetadataIndexBloomFilter(enableMetadataTable)
           .withMetadataIndexColumnStats(enableMetadataTable)
-          .withMetadataIndexForAllColumns(enableMetadataTable)
           .withAssumeDatePartitioning(cfg.assumeDatePartitioning)
           .build();
       this.fileSystemView = FileSystemViewManager.createInMemoryFileSystemView(engineContext,

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -1454,7 +1454,7 @@ public void testColStatsPrefixLookup() throws IOException {
               .forEach(partitionWriteStat -> {
                 String partitionStatName = partitionWriteStat.getKey();
                 List<HoodieWriteStat> writeStats = partitionWriteStat.getValue();
-                String partition = HoodieTableMetadataUtil.getPartition(partitionStatName);
+                String partition = HoodieTableMetadataUtil.getPartitionIdentifier(partitionStatName);
                 if (!commitToPartitionsToFiles.get(commitTime).containsKey(partition)) {
                   commitToPartitionsToFiles.get(commitTime).put(partition, new ArrayList<>());
                 }

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java
Patch:
@@ -80,7 +80,7 @@
 import static org.apache.hudi.common.util.ValidationUtils.checkArgument;
 import static org.apache.hudi.common.util.ValidationUtils.checkState;
 import static org.apache.hudi.metadata.HoodieTableMetadata.RECORDKEY_PARTITION_LIST;
-import static org.apache.hudi.metadata.HoodieTableMetadataUtil.getPartition;
+import static org.apache.hudi.metadata.HoodieTableMetadataUtil.getPartitionIdentifier;
 import static org.apache.hudi.metadata.HoodieTableMetadataUtil.tryUpcastDecimal;
 
 /**
@@ -256,7 +256,7 @@ public static HoodieRecord<HoodieMetadataPayload> createPartitionListRecord(List
    */
   public static HoodieRecord<HoodieMetadataPayload> createPartitionListRecord(List<String> partitions, boolean isDeleted) {
     Map<String, HoodieMetadataFileInfo> fileInfo = new HashMap<>();
-    partitions.forEach(partition -> fileInfo.put(getPartition(partition), new HoodieMetadataFileInfo(0L, isDeleted)));
+    partitions.forEach(partition -> fileInfo.put(getPartitionIdentifier(partition), new HoodieMetadataFileInfo(0L, isDeleted)));
 
     HoodieKey key = new HoodieKey(RECORDKEY_PARTITION_LIST, MetadataPartitionType.FILES.getPartitionPath());
     HoodieMetadataPayload payload = new HoodieMetadataPayload(key.getRecordKey(), METADATA_TYPE_PARTITION_LIST,

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestHoodieLogFileCommand.java
Patch:
@@ -65,6 +65,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
+import static org.apache.hudi.common.fs.FSUtils.getRelativePartitionPath;
 import static org.apache.hudi.common.testutils.SchemaTestUtil.getSimpleSchema;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertNotNull;
@@ -203,6 +204,7 @@ public void testShowLogFileRecordsWithMerge() throws IOException, InterruptedExc
     // get expected result of 10 records.
     List<String> logFilePaths = Arrays.stream(fs.globStatus(new Path(partitionPath + "/*")))
         .map(status -> status.getPath().toString()).collect(Collectors.toList());
+    assertTrue(logFilePaths.size() > 0);
     HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()
         .withFileSystem(fs)
         .withBasePath(tablePath)
@@ -221,6 +223,7 @@ public void testShowLogFileRecordsWithMerge() throws IOException, InterruptedExc
         .withSpillableMapBasePath(HoodieMemoryConfig.SPILLABLE_MAP_BASE_PATH.defaultValue())
         .withDiskMapType(HoodieCommonConfig.SPILLABLE_DISK_MAP_TYPE.defaultValue())
         .withBitCaskDiskMapCompressionEnabled(HoodieCommonConfig.DISK_MAP_BITCASK_COMPRESSION_ENABLED.defaultValue())
+        .withPartition(getRelativePartitionPath(new Path(tablePath), new Path(logFilePaths.get(0)).getParent()))
         .build();
 
     Iterator<HoodieRecord<? extends HoodieRecordPayload>> records = scanner.iterator();

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java
Patch:
@@ -230,7 +230,7 @@ public static String getRelativePartitionPath(Path basePath, Path fullPartitionP
 
   /**
    * Obtain all the partition paths, that are present in this table, denoted by presence of
-   * {@link HoodiePartitionMetadata#HOODIE_PARTITION_METAFILE}.
+   * {@link HoodiePartitionMetadata#HOODIE_PARTITION_METAFILE_PREFIX}.
    *
    * If the basePathStr is a subdirectory of .hoodie folder then we assume that the partitions of an internal
    * table (a hoodie table within the .hoodie directory) are to be obtained.

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMergedLogRecordReader.java
Patch:
@@ -60,7 +60,7 @@ private HoodieMetadataMergedLogRecordReader(FileSystem fs, String basePath, Stri
                                               ExternalSpillableMap.DiskMapType diskMapType,
                                               boolean isBitCaskDiskMapCompressionEnabled,
                                               Option<InstantRange> instantRange, boolean allowFullScan) {
-    super(fs, basePath, logFilePaths, readerSchema, latestInstantTime, maxMemorySizeInBytes, false, false, bufferSize,
+    super(fs, basePath, logFilePaths, readerSchema, latestInstantTime, maxMemorySizeInBytes, true, false, bufferSize,
         spillableMapBasePath, instantRange, diskMapType, isBitCaskDiskMapCompressionEnabled, false, allowFullScan, Option.of(partitionName), InternalSchema.getEmptyInternalSchema());
   }
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieHiveUtils.java
Patch:
@@ -71,7 +71,6 @@ public class HoodieHiveUtils {
   public static final String DEFAULT_SCAN_MODE = SNAPSHOT_SCAN_MODE;
   public static final int DEFAULT_MAX_COMMITS = 1;
   public static final int MAX_COMMIT_ALL = -1;
-  public static final int DEFAULT_LEVELS_TO_BASEPATH = 3;
   public static final Pattern HOODIE_CONSUME_MODE_PATTERN_STRING = Pattern.compile("hoodie\\.(.*)\\.consume\\.mode");
   public static final String GLOBALLY_CONSISTENT_READ_TIMESTAMP = "last_replication_timestamp";
 

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/ColumnStatsIndexHelper.java
Patch:
@@ -130,7 +130,7 @@ public class ColumnStatsIndexHelper {
    *
    * <pre>
    * +---------------------------+------------+------------+-------------+
-   * |          file             | A_minValue | A_maxValue | A_num_nulls |
+   * |          file             | A_minValue | A_maxValue | A_nullCount |
    * +---------------------------+------------+------------+-------------+
    * | one_base_file.parquet     |          1 |         10 |           0 |
    * | another_base_file.parquet |        -10 |          0 |           5 |

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/ArchivedCommitsCommand.java
Patch:
@@ -81,7 +81,7 @@ public String showArchivedCommits(
       // read the avro blocks
       while (reader.hasNext()) {
         HoodieAvroDataBlock blk = (HoodieAvroDataBlock) reader.next();
-        blk.getRecordItr().forEachRemaining(readRecords::add);
+        blk.getRecordIterator().forEachRemaining(readRecords::add);
       }
       List<Comparable[]> readCommits = readRecords.stream().map(r -> (GenericRecord) r)
           .filter(r -> r.get("actionType").toString().equals(HoodieTimeline.COMMIT_ACTION)
@@ -155,7 +155,7 @@ public String showCommits(
       // read the avro blocks
       while (reader.hasNext()) {
         HoodieAvroDataBlock blk = (HoodieAvroDataBlock) reader.next();
-        try (ClosableIterator<IndexedRecord> recordItr = blk.getRecordItr()) {
+        try (ClosableIterator<IndexedRecord> recordItr = blk.getRecordIterator()) {
           recordItr.forEachRemaining(readRecords::add);
         }
       }

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/ExportCommand.java
Patch:
@@ -124,7 +124,7 @@ private int copyArchivedInstants(List<FileStatus> statuses, Set<String> actionSe
       // read the avro blocks
       while (reader.hasNext() && copyCount < limit) {
         HoodieAvroDataBlock blk = (HoodieAvroDataBlock) reader.next();
-        try (ClosableIterator<IndexedRecord> recordItr = blk.getRecordItr()) {
+        try (ClosableIterator<IndexedRecord> recordItr = blk.getRecordIterator()) {
           while (recordItr.hasNext()) {
             IndexedRecord ir = recordItr.next();
             // Archived instants are saved as arvo encoded HoodieArchivedMetaEntry records. We need to get the

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java
Patch:
@@ -122,7 +122,7 @@ public String showLogFileCommits(
             instantTime = "dummy_instant_time_" + dummyInstantTimeCount;
           }
           if (n instanceof HoodieDataBlock) {
-            try (ClosableIterator<IndexedRecord> recordItr = ((HoodieDataBlock) n).getRecordItr()) {
+            try (ClosableIterator<IndexedRecord> recordItr = ((HoodieDataBlock) n).getRecordIterator()) {
               recordItr.forEachRemaining(r -> recordCount.incrementAndGet());
             }
           }
@@ -236,7 +236,7 @@ public String showLogFileRecords(
           HoodieLogBlock n = reader.next();
           if (n instanceof HoodieDataBlock) {
             HoodieDataBlock blk = (HoodieDataBlock) n;
-            try (ClosableIterator<IndexedRecord> recordItr = blk.getRecordItr()) {
+            try (ClosableIterator<IndexedRecord> recordItr = blk.getRecordIterator()) {
               recordItr.forEachRemaining(record -> {
                 if (allRecords.size() < limit) {
                   allRecords.add(record);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/HoodieTimelineArchiver.java
Patch:
@@ -339,7 +339,7 @@ public void mergeArchiveFiles(List<FileStatus> compactCandidate) throws IOExcept
           // Read the avro blocks
           while (reader.hasNext()) {
             HoodieAvroDataBlock blk = (HoodieAvroDataBlock) reader.next();
-            blk.getRecordItr().forEachRemaining(records::add);
+            blk.getRecordIterator().forEachRemaining(records::add);
             if (records.size() >= this.config.getCommitArchivalBatchSize()) {
               writeToFile(wrapperSchema, records);
             }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileWriter.java
Patch:
@@ -107,7 +107,7 @@ public HoodieHFileWriter(String instantTime, Path file, HoodieHFileConfig hfileC
         .withFileContext(context)
         .create();
 
-    writer.appendFileInfo(HoodieHFileReader.KEY_SCHEMA.getBytes(), schema.toString().getBytes());
+    writer.appendFileInfo(HoodieHFileReader.SCHEMA_KEY.getBytes(), schema.toString().getBytes());
   }
 
   @Override

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/io/storage/TestHoodieReaderWriterBase.java
Patch:
@@ -38,6 +38,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.TreeSet;
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
@@ -217,7 +218,7 @@ protected void verifyComplexRecords(Iterator<GenericRecord> iterator) {
 
   private void verifyFilterRowKeys(HoodieFileReader<GenericRecord> hoodieReader) {
     Set<String> candidateRowKeys = IntStream.range(40, NUM_RECORDS * 2)
-        .mapToObj(i -> "key" + String.format("%02d", i)).collect(Collectors.toSet());
+        .mapToObj(i -> "key" + String.format("%02d", i)).collect(Collectors.toCollection(TreeSet::new));
     List<String> expectedKeys = IntStream.range(40, NUM_RECORDS)
         .mapToObj(i -> "key" + String.format("%02d", i)).sorted().collect(Collectors.toList());
     assertEquals(expectedKeys, hoodieReader.filterRowKeys(candidateRowKeys)

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java
Patch:
@@ -67,7 +67,7 @@
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
-import static org.apache.hudi.io.storage.HoodieHFileReader.KEY_SCHEMA;
+import static org.apache.hudi.io.storage.HoodieHFileReader.SCHEMA_KEY;
 
 /**
  * Utility methods to aid testing inside the HoodieClient module.
@@ -247,7 +247,7 @@ public static Stream<GenericRecord> readHFile(JavaSparkContext jsc, String[] pat
         HFile.Reader reader =
             HoodieHFileUtils.createHFileReader(fs, new Path(path), cacheConfig, fs.getConf());
         if (schema == null) {
-          schema = new Schema.Parser().parse(new String(reader.getHFileInfo().get(KEY_SCHEMA.getBytes())));
+          schema = new Schema.Parser().parse(new String(reader.getHFileInfo().get(SCHEMA_KEY.getBytes())));
         }
         HFileScanner scanner = reader.getScanner(false, false);
         if (!scanner.seekTo()) {

File: hudi-common/src/main/java/org/apache/hudi/BaseHoodieTableFileIndex.java
Patch:
@@ -69,7 +69,7 @@ public abstract class BaseHoodieTableFileIndex {
   private final String[] partitionColumns;
 
   private final FileSystemViewStorageConfig fileSystemStorageConfig;
-  private final HoodieMetadataConfig metadataConfig;
+  protected final HoodieMetadataConfig metadataConfig;
 
   private final HoodieTableQueryType queryType;
   private final Option<String> specifiedQueryInstant;

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -289,8 +289,8 @@ public String getDirectoryFilterRegex() {
     return getString(DIR_FILTER_REGEX);
   }
 
-  public boolean enableFullScan() {
-    return getBoolean(ENABLE_FULL_SCAN_LOG_FILES);
+  public boolean allowFullScan() {
+    return getBooleanOrDefault(ENABLE_FULL_SCAN_LOG_FILES);
   }
 
   public boolean populateMetaFields() {

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java
Patch:
@@ -314,7 +314,7 @@ public byte[] getBytes(Schema schema) throws IOException {
     output.write(schemaContent);
 
     List<IndexedRecord> records = new ArrayList<>();
-    try (ClosableIterator<IndexedRecord> recordItr = getRecordItr()) {
+    try (ClosableIterator<IndexedRecord> recordItr = getRecordIterator()) {
       recordItr.forEachRemaining(records::add);
     }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieArchivedTimeline.java
Patch:
@@ -257,7 +257,7 @@ private List<HoodieInstant> loadInstants(TimeRangeFilter filter, boolean loadIns
               HoodieAvroDataBlock avroBlock = (HoodieAvroDataBlock) block;
               // TODO If we can store additional metadata in datablock, we can skip parsing records
               // (such as startTime, endTime of records in the block)
-              try (ClosableIterator<IndexedRecord> itr = avroBlock.getRecordItr()) {
+              try (ClosableIterator<IndexedRecord> itr = avroBlock.getRecordIterator()) {
                 StreamSupport.stream(Spliterators.spliteratorUnknownSize(itr, Spliterator.IMMUTABLE), true)
                     // Filter blocks in desired time window
                     .filter(r -> commitsFilter.apply((GenericRecord) r))

File: hudi-common/src/main/java/org/apache/hudi/metadata/BaseTableMetadata.java
Patch:
@@ -378,7 +378,7 @@ private void handleSpuriousDeletes(Option<HoodieRecord<HoodieMetadataPayload>> h
 
   protected abstract Option<HoodieRecord<HoodieMetadataPayload>> getRecordByKey(String key, String partitionName);
 
-  protected abstract List<Pair<String, Option<HoodieRecord<HoodieMetadataPayload>>>> getRecordsByKeys(List<String> key, String partitionName);
+  public abstract List<Pair<String, Option<HoodieRecord<HoodieMetadataPayload>>>> getRecordsByKeys(List<String> key, String partitionName);
 
   protected HoodieEngineContext getEngineContext() {
     return engineContext != null ? engineContext : new HoodieLocalEngineContext(hadoopConf.get());

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java
Patch:
@@ -75,8 +75,8 @@
 import java.util.stream.Stream;
 
 import static org.apache.hudi.TypeUtils.unsafeCast;
-import static org.apache.hudi.common.util.DateTimeUtils.microsToInstant;
 import static org.apache.hudi.common.util.DateTimeUtils.instantToMicros;
+import static org.apache.hudi.common.util.DateTimeUtils.microsToInstant;
 import static org.apache.hudi.common.util.ValidationUtils.checkArgument;
 import static org.apache.hudi.common.util.ValidationUtils.checkState;
 import static org.apache.hudi.metadata.HoodieTableMetadata.RECORDKEY_PARTITION_LIST;
@@ -391,7 +391,7 @@ public Option<IndexedRecord> combineAndGetUpdateValue(IndexedRecord oldRecord, S
   }
 
   @Override
-  public Option<IndexedRecord> getInsertValue(Schema schema, Properties properties) throws IOException {
+  public Option<IndexedRecord> getInsertValue(Schema schemaIgnored, Properties propertiesIgnored) throws IOException {
     if (key == null) {
       return Option.empty();
     }

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -2038,7 +2038,7 @@ private static Stream<Arguments> testArguments() {
    * Utility to convert the given iterator to a List.
    */
   private static List<IndexedRecord> getRecords(HoodieDataBlock dataBlock) {
-    ClosableIterator<IndexedRecord> itr = dataBlock.getRecordItr();
+    ClosableIterator<IndexedRecord> itr = dataBlock.getRecordIterator();
 
     List<IndexedRecord> elements = new ArrayList<>();
     itr.forEachRemaining(elements::add);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java
Patch:
@@ -90,7 +90,7 @@ public void write(GenericRecord oldRecord) {
         throw new HoodieUpsertException("Insert/Update not in sorted order");
       }
       try {
-        if (useWriterSchema) {
+        if (useWriterSchemaForCompaction) {
           writeRecord(hoodieRecord, hoodieRecord.getData().getInsertValue(tableSchemaWithMetaFields, config.getProps()));
         } else {
           writeRecord(hoodieRecord, hoodieRecord.getData().getInsertValue(tableSchema, config.getProps()));
@@ -113,7 +113,7 @@ public List<WriteStatus> close() {
         String key = newRecordKeysSorted.poll();
         HoodieRecord<T> hoodieRecord = keyToNewRecords.get(key);
         if (!writtenRecordKeys.contains(hoodieRecord.getRecordKey())) {
-          if (useWriterSchema) {
+          if (useWriterSchemaForCompaction) {
             writeRecord(hoodieRecord, hoodieRecord.getData().getInsertValue(tableSchemaWithMetaFields, config.getProps()));
           } else {
             writeRecord(hoodieRecord, hoodieRecord.getData().getInsertValue(tableSchema, config.getProps()));

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkTable.java
Patch:
@@ -105,6 +105,9 @@ protected HoodieIndex getIndex(HoodieWriteConfig config, HoodieEngineContext con
   public <T extends SpecificRecordBase> Option<HoodieTableMetadataWriter> getMetadataWriter(String triggeringInstantTimestamp,
                                                                                             Option<T> actionMetadata) {
     if (config.isMetadataTableEnabled()) {
+      // even with metadata enabled, some index could have been disabled
+      // delete metadata partitions corresponding to such indexes
+      deleteMetadataIndexIfNecessary();
       return Option.of(FlinkHoodieBackedTableMetadataWriter.create(context.getHadoopConf().get(), config,
           context, actionMetadata, Option.of(triggeringInstantTimestamp)));
     } else {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkTable.java
Patch:
@@ -113,6 +113,9 @@ public <R extends SpecificRecordBase> Option<HoodieTableMetadataWriter> getMetad
       // existence after the creation is needed.
       final HoodieTableMetadataWriter metadataWriter = SparkHoodieBackedTableMetadataWriter.create(
           context.getHadoopConf().get(), config, context, actionMetadata, Option.of(triggeringInstantTimestamp));
+      // even with metadata enabled, some index could have been disabled
+      // delete metadata partitions corresponding to such indexes
+      deleteMetadataIndexIfNecessary();
       try {
         if (isMetadataTableExists || metaClient.getFs().exists(new Path(
             HoodieTableMetadata.getMetadataTableBasePath(metaClient.getBasePath())))) {

File: hudi-cli/src/main/java/org/apache/hudi/cli/HoodieTableHeaderFields.java
Patch:
@@ -83,6 +83,8 @@ public class HoodieTableHeaderFields {
   public static final String HEADER_HOODIE_PROPERTY = "Property";
   public static final String HEADER_OLD_VALUE = "Old Value";
   public static final String HEADER_NEW_VALUE = "New Value";
+  public static final String HEADER_TEXT_METAFILE_PRESENT = "Text Metafile present ?";
+  public static final String HEADER_BASE_METAFILE_PRESENT = "Base Metafile present ?";
 
   /**
    * Fields of Savepoints.

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -173,7 +173,8 @@ private void init(HoodieRecord record) {
       try {
         // Save hoodie partition meta in the partition path
         HoodiePartitionMetadata partitionMetadata = new HoodiePartitionMetadata(fs, baseInstantTime,
-            new Path(config.getBasePath()), FSUtils.getPartitionPath(config.getBasePath(), partitionPath));
+            new Path(config.getBasePath()), FSUtils.getPartitionPath(config.getBasePath(), partitionPath),
+            hoodieTable.getPartitionMetafileFormat());
         partitionMetadata.trySave(getPartitionId());
 
         // Since the actual log file written to can be different based on when rollover happens, we use the

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -94,7 +94,8 @@ public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTa
 
     try {
       HoodiePartitionMetadata partitionMetadata = new HoodiePartitionMetadata(fs, instantTime,
-          new Path(config.getBasePath()), FSUtils.getPartitionPath(config.getBasePath(), partitionPath));
+          new Path(config.getBasePath()), FSUtils.getPartitionPath(config.getBasePath(), partitionPath),
+          hoodieTable.getPartitionMetafileFormat());
       partitionMetadata.trySave(getPartitionId());
       createMarkerFile(partitionPath, FSUtils.makeDataFileName(this.instantTime, this.writeToken, this.fileId, hoodieTable.getBaseFileExtension()));
       this.fileWriter = HoodieFileWriterFactory.getFileWriter(instantTime, path, hoodieTable, config,

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -166,7 +166,8 @@ private void init(String fileId, String partitionPath, HoodieBaseFile baseFileTo
       writeStatus.getStat().setPrevCommit(FSUtils.getCommitTime(latestValidFilePath));
 
       HoodiePartitionMetadata partitionMetadata = new HoodiePartitionMetadata(fs, instantTime,
-          new Path(config.getBasePath()), FSUtils.getPartitionPath(config.getBasePath(), partitionPath));
+          new Path(config.getBasePath()), FSUtils.getPartitionPath(config.getBasePath(), partitionPath),
+          hoodieTable.getPartitionMetafileFormat());
       partitionMetadata.trySave(getPartitionId());
 
       String newFileName = FSUtils.makeDataFileName(instantTime, writeToken, fileId, hoodieTable.getBaseFileExtension());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -1074,7 +1074,7 @@ public DirectoryInfo(String relativePath, FileStatus[] fileStatus) {
           if (!status.getPath().getName().equals(HoodieTableMetaClient.METAFOLDER_NAME)) {
             this.subDirectories.add(status.getPath());
           }
-        } else if (status.getPath().getName().equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE)) {
+        } else if (status.getPath().getName().startsWith(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE_PREFIX)) {
           // Presence of partition meta file implies this is a HUDI partition
           this.isHoodiePartition = true;
         } else if (FSUtils.isDataFile(status.getPath())) {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataCreateHandle.java
Patch:
@@ -94,7 +94,8 @@ public HoodieRowDataCreateHandle(HoodieTable table, HoodieWriteConfig writeConfi
               fs,
               instantTime,
               new Path(writeConfig.getBasePath()),
-              FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath));
+              FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath),
+              table.getPartitionMetafileFormat());
       partitionMetadata.trySave(taskPartitionId);
       createMarkerFile(partitionPath, FSUtils.makeDataFileName(this.instantTime, getWriteToken(), this.fileId, table.getBaseFileExtension()));
       this.fileWriter = createNewFileWriter(path, table, writeConfig, rowType);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowCreateHandle.java
Patch:
@@ -93,7 +93,8 @@ public HoodieRowCreateHandle(HoodieTable table, HoodieWriteConfig writeConfig, S
               fs,
               instantTime,
               new Path(writeConfig.getBasePath()),
-              FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath));
+              FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath),
+              table.getPartitionMetafileFormat());
       partitionMetadata.trySave(taskPartitionId);
       createMarkerFile(partitionPath, FSUtils.makeDataFileName(this.instantTime, getWriteToken(), this.fileId, table.getBaseFileExtension()));
       this.fileWriter = createNewFileWriter(path, table, writeConfig, structType);

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java
Patch:
@@ -246,7 +246,7 @@ public static List<String> getAllFoldersWithPartitionMetaFile(FileSystem fs, Str
     final List<String> partitions = new ArrayList<>();
     processFiles(fs, basePathStr, (locatedFileStatus) -> {
       Path filePath = locatedFileStatus.getPath();
-      if (filePath.getName().equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE)) {
+      if (filePath.getName().startsWith(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE_PREFIX)) {
         partitions.add(getRelativePartitionPath(basePath, filePath.getParent()));
       }
       return true;

File: hudi-common/src/main/java/org/apache/hudi/metadata/FileSystemBackedTableMetadata.java
Patch:
@@ -96,7 +96,7 @@ public List<String> getAllPartitionPaths() throws IOException {
               } else if (!fileStatus.getPath().getName().equals(HoodieTableMetaClient.METAFOLDER_NAME)) {
                 pathsToList.add(fileStatus.getPath());
               }
-            } else if (fileStatus.getPath().getName().equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE)) {
+            } else if (fileStatus.getPath().getName().startsWith(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE_PREFIX)) {
               String partitionName = FSUtils.getRelativePartitionPath(new Path(datasetBasePath), fileStatus.getPath().getParent());
               partitionPaths.add(partitionName);
             }

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/FileCreateUtils.java
Patch:
@@ -280,7 +280,7 @@ public static void createPendingInflightCompaction(String basePath, String insta
   public static void createPartitionMetaFile(String basePath, String partitionPath) throws IOException {
     Path parentPath = Paths.get(basePath, partitionPath);
     Files.createDirectories(parentPath);
-    Path metaFilePath = parentPath.resolve(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);
+    Path metaFilePath = parentPath.resolve(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE_PREFIX);
     if (Files.notExists(metaFilePath)) {
       Files.createFile(metaFilePath);
     }
@@ -397,7 +397,7 @@ public static List<Path> getPartitionPaths(Path basePath) throws IOException {
     }
     return Files.list(basePath).filter(entry -> (!entry.getFileName().toString().equals(HoodieTableMetaClient.METAFOLDER_NAME)
         && !entry.getFileName().toString().contains("parquet") && !entry.getFileName().toString().contains("log"))
-        && !entry.getFileName().toString().endsWith(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE)).collect(Collectors.toList());
+        && !entry.getFileName().toString().startsWith(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE_PREFIX)).collect(Collectors.toList());
   }
 
   /**

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java
Patch:
@@ -205,7 +205,7 @@ public static void writePartitionMetadataDeprecated(FileSystem fs, String[] part
    */
   public void writePartitionMetadata(FileSystem fs, String[] partitionPaths, String basePath) {
     for (String partitionPath : partitionPaths) {
-      new HoodiePartitionMetadata(fs, "000", new Path(basePath), new Path(basePath, partitionPath)).trySave(0);
+      new HoodiePartitionMetadata(fs, "000", new Path(basePath), new Path(basePath, partitionPath), Option.empty()).trySave(0);
     }
   }
 

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestTable.java
Patch:
@@ -680,7 +680,7 @@ public FileStatus[] listAllFilesInPartition(String partitionPath) throws IOExcep
           boolean toReturn = true;
           String filePath = entry.getPath().toString();
           String fileName = entry.getPath().getName();
-          if (fileName.equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE) || (!fileName.contains("log") && !fileName.contains("parquet"))
+          if (fileName.startsWith(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE_PREFIX) || (!fileName.contains("log") && !fileName.contains("parquet"))
               || filePath.contains("metadata")) {
             toReturn = false;
           } else {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java
Patch:
@@ -117,8 +117,8 @@ public void snapshot(JavaSparkContext jsc, String baseDir, final String outputDi
         dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile.getPath())));
 
         // also need to copy over partition metadata
-        Path partitionMetaFile =
-            new Path(FSUtils.getPartitionPath(baseDir, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);
+        Path partitionMetaFile = HoodiePartitionMetadata.getPartitionMetafilePath(fs1,
+            FSUtils.getPartitionPath(baseDir, partition)).get();
         if (fs1.exists(partitionMetaFile)) {
           filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));
         }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java
Patch:
@@ -206,9 +206,9 @@ private void exportAsHudi(JavaSparkContext jsc, Config cfg, List<String> partiti
       Stream<HoodieBaseFile> dataFiles = fsView.getLatestBaseFilesBeforeOrOn(partition, latestCommitTimestamp);
       dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile.getPath())));
       // also need to copy over partition metadata
-      Path partitionMetaFile =
-          new Path(FSUtils.getPartitionPath(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);
       FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());
+      Path partitionMetaFile = HoodiePartitionMetadata.getPartitionMetafilePath(fs,
+          FSUtils.getPartitionPath(cfg.sourceBasePath, partition)).get();
       if (fs.exists(partitionMetaFile)) {
         filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));
       }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/BootstrapExecutor.java
Patch:
@@ -185,6 +185,7 @@ private void initializeTable() throws IOException {
       }
     }
     HoodieTableMetaClient.withPropertyBuilder()
+        .fromProperties(props)
         .setTableType(cfg.tableType)
         .setTableName(cfg.targetTableName)
         .setArchiveLogFolder(ARCHIVELOG_FOLDER.defaultValue())

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -468,7 +468,7 @@ public int hashCode() {
               compactSchedulingMinShare, clusterSchedulingMinShare, forceDisableCompaction, checkpoint,
               initialCheckpointProvider, help);
     }
-  
+
     @Override
     public String toString() {
       return "Config{"

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
Patch:
@@ -43,8 +43,8 @@
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.utilities.checkpointing.InitialCheckPointProvider;
 import org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamerMetrics;
-import org.apache.hudi.utilities.exception.HoodieSourcePostProcessException;
 import org.apache.hudi.utilities.exception.HoodieSchemaPostProcessException;
+import org.apache.hudi.utilities.exception.HoodieSourcePostProcessException;
 import org.apache.hudi.utilities.schema.ChainedSchemaPostProcessor;
 import org.apache.hudi.utilities.schema.DelegatingSchemaProvider;
 import org.apache.hudi.utilities.schema.RowBasedSchemaProvider;
@@ -272,6 +272,7 @@ private static SparkConf buildSparkConf(String appName, String defaultMaster, Ma
       sparkConf.set("spark.eventLog.overwrite", "true");
       sparkConf.set("spark.eventLog.enabled", "true");
     }
+    sparkConf.set("spark.ui.port", "8090");
     sparkConf.setIfMissing("spark.driver.maxResultSize", "2g");
     sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
     sparkConf.set("spark.hadoop.mapred.output.compress", "true");

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java
Patch:
@@ -1509,6 +1509,7 @@ private Stream<Pair<String, String>> convertPathToFileIdWithCommitTime(final Hoo
   protected static HoodieCommitMetadata generateCommitMetadata(
       String instantTime, Map<String, List<String>> partitionToFilePaths) {
     HoodieCommitMetadata metadata = new HoodieCommitMetadata();
+    metadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, HoodieTestTable.PHONY_TABLE_SCHEMA);
     partitionToFilePaths.forEach((partitionPath, fileList) -> fileList.forEach(f -> {
       HoodieWriteStat writeStat = new HoodieWriteStat();
       writeStat.setPartitionPath(partitionPath);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java
Patch:
@@ -119,7 +119,7 @@ public void testSuccessfulCompactionBasedOnTime() throws Exception {
   @Test
   public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {
     // Given: make three commits
-    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);
+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60, CompactionTriggerStrategy.NUM_OR_TIME);
     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {
       List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);
       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());
@@ -134,7 +134,7 @@ public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {
       assertEquals(4, metaClient.getActiveTimeline().getWriteTimeline().countInstants());
       // 4th commit, that will trigger compaction because reach the time elapsed
       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();
-      finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);
+      finalInstant = HoodieActiveTimeline.createNewInstantTime(60000);
       createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);
 
       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroWriteSupport.java
Patch:
@@ -44,7 +44,7 @@ public class HoodieAvroWriteSupport extends AvroWriteSupport {
   public static final String HOODIE_BLOOM_FILTER_TYPE_CODE = "hoodie_bloom_filter_type_code";
 
   public HoodieAvroWriteSupport(MessageType schema, Schema avroSchema, Option<BloomFilter> bloomFilterOpt) {
-    super(schema, avroSchema);
+    super(schema, avroSchema, ConvertingGenericData.INSTANCE);
     this.bloomFilterOpt = bloomFilterOpt;
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java
Patch:
@@ -130,8 +130,9 @@ public WriteOperationType getOperationType() {
   public HashMap<String, String> getFileIdAndFullPaths(String basePath) {
     HashMap<String, String> fullPaths = new HashMap<>();
     for (Map.Entry<String, String> entry : getFileIdAndRelativePaths().entrySet()) {
-      String fullPath =
-          (entry.getValue() != null) ? (FSUtils.getPartitionPath(basePath, entry.getValue())).toString() : null;
+      String fullPath = entry.getValue() != null
+          ? FSUtils.getPartitionPath(basePath, entry.getValue()).toString()
+          : null;
       fullPaths.put(entry.getKey(), fullPath);
     }
     return fullPaths;

File: hudi-common/src/main/java/org/apache/hudi/metadata/MetadataPartitionType.java
Patch:
@@ -31,6 +31,8 @@ public enum MetadataPartitionType {
   // FileId prefix used for all file groups in this partition.
   private final String fileIdPrefix;
   // Total file groups
+  // TODO fix: enum should not have any mutable aspect as this compromises whole idea
+  //      of the inum being static, immutable entity
   private int fileGroupCount = 1;
 
   MetadataPartitionType(final String partitionPath, final String fileIdPrefix) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -704,7 +704,7 @@ public void dropMetadataPartitions(List<MetadataPartitionType> metadataPartition
   private MetadataRecordsGenerationParams getRecordsGenerationParams() {
     return new MetadataRecordsGenerationParams(
         dataMetaClient, enabledPartitionTypes, dataWriteConfig.getBloomFilterType(),
-        dataWriteConfig.getBloomIndexParallelism(),
+        dataWriteConfig.getMetadataBloomFilterIndexParallelism(),
         dataWriteConfig.isMetadataColumnStatsIndexEnabled(),
         dataWriteConfig.getColumnStatsIndexParallelism(),
         StringUtils.toList(dataWriteConfig.getColumnsEnabledForColumnStatsIndex()),

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMergedLogRecordReader.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hudi.common.util.SpillableMapUtils;
 import org.apache.hudi.common.util.collection.ExternalSpillableMap;
 import org.apache.hudi.common.util.collection.Pair;
+import org.apache.hudi.internal.schema.InternalSchema;
 
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
@@ -65,7 +66,7 @@ private HoodieMetadataMergedLogRecordReader(FileSystem fs, String basePath, Stri
                                               Option<InstantRange> instantRange, boolean enableFullScan) {
     super(fs, basePath, logFilePaths, readerSchema, latestInstantTime, maxMemorySizeInBytes, false, false, bufferSize,
         spillableMapBasePath, instantRange, false, diskMapType, isBitCaskDiskMapCompressionEnabled, false,
-        enableFullScan, Option.of(partitionName));
+        enableFullScan, Option.of(partitionName), InternalSchema.getEmptyInternalSchema());
     this.mergeKeyFilter = mergeKeyFilter;
     if (enableFullScan) {
       performScan();

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java
Patch:
@@ -196,6 +196,7 @@ protected ClosableIterator<IndexedRecord> lookupRecords(List<String> keys) throw
     //       is appropriately carried over
     Configuration inlineConf = new Configuration(blockContentLoc.getHadoopConf());
     inlineConf.set("fs." + InLineFileSystem.SCHEME + ".impl", InLineFileSystem.class.getName());
+    inlineConf.setClassLoader(Thread.currentThread().getContextClassLoader());
 
     Path inlinePath = InLineFSUtils.getInlineFilePath(
         blockContentLoc.getLogFile().getPath(),

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/rollback/TestMergeOnReadRollbackActionExecutor.java
Patch:
@@ -45,7 +45,6 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.marker.WriteMarkersFactory;
 import org.apache.hudi.testutils.MetadataMergeWriteStatus;
-
 import org.apache.spark.api.java.JavaRDD;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.Assertions;
@@ -125,8 +124,8 @@ public void testMergeOnReadRollbackActionExecutor(boolean isUsingMarkers) throws
 
     for (Map.Entry<String, HoodieRollbackPartitionMetadata> entry : rollbackMetadata.entrySet()) {
       HoodieRollbackPartitionMetadata meta = entry.getValue();
-      assertTrue(meta.getFailedDeleteFiles() == null || meta.getFailedDeleteFiles().size() == 0);
-      assertTrue(meta.getSuccessDeleteFiles() == null || meta.getSuccessDeleteFiles().size() == 0);
+      assertEquals(0, meta.getFailedDeleteFiles().size());
+      assertEquals(0, meta.getSuccessDeleteFiles().size());
     }
 
     //4. assert file group after rollback, and compare to the rollbackstat

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/BaseRestoreActionExecutor.java
Patch:
@@ -136,6 +136,9 @@ private HoodieRestoreMetadata finishRestore(Map<String, List<HoodieRollbackMetad
         .filter(instant -> HoodieActiveTimeline.GREATER_THAN.test(instant.getTimestamp(), restoreInstantTime))
         .collect(Collectors.toList());
     instantsToRollback.forEach(entry -> {
+      if (entry.isCompleted()) {
+        table.getActiveTimeline().deleteCompletedRollback(entry);
+      }
       table.getActiveTimeline().deletePending(new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.ROLLBACK_ACTION, entry.getTimestamp()));
       table.getActiveTimeline().deletePending(new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.ROLLBACK_ACTION, entry.getTimestamp()));
     });

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/utils/TestMetadataConversionUtils.java
Patch:
@@ -259,7 +259,7 @@ private void createReplace(String instantTime, WriteOperationType writeOperation
 
   private void createCleanMetadata(String instantTime) throws IOException {
     HoodieCleanerPlan cleanerPlan = new HoodieCleanerPlan(new HoodieActionInstant("", "", ""), "", new HashMap<>(),
-        CleanPlanV2MigrationHandler.VERSION, new HashMap<>());
+        CleanPlanV2MigrationHandler.VERSION, new HashMap<>(), new ArrayList<>());
     HoodieCleanStat cleanStats = new HoodieCleanStat(
         HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS,
         HoodieTestUtils.DEFAULT_PARTITION_PATHS[new Random().nextInt(HoodieTestUtils.DEFAULT_PARTITION_PATHS.length)],

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkCopyOnWriteTableArchiveWithReplace.java
Patch:
@@ -79,7 +79,7 @@ public void testDeletePartitionAndArchive(boolean metadataEnabled) throws IOExce
       client.startCommitWithTime(instantTime4, HoodieActiveTimeline.REPLACE_COMMIT_ACTION);
       client.deletePartitions(Arrays.asList(DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH), instantTime4);
 
-      // 2nd write batch; 4 commits for the 3rd partition; the 3rd commit to trigger archiving the replace commit
+      // 2nd write batch; 4 commits for the 4th partition; the 4th commit to trigger archiving the replace commit
       for (int i = 5; i < 9; i++) {
         String instantTime = HoodieActiveTimeline.createNewInstantTime(i * 1000);
         client.startCommitWithTime(instantTime);
@@ -97,7 +97,7 @@ public void testDeletePartitionAndArchive(boolean metadataEnabled) throws IOExce
       // verify records
       final HoodieTimeline timeline2 = metaClient.getCommitTimeline().filterCompletedInstants();
       assertEquals(5, countRecordsOptionallySince(jsc(), basePath(), sqlContext(), timeline2, Option.empty()),
-          "should only have the 4 records from the 3rd partition.");
+          "should only have the 5 records from the 3rd partition.");
     }
   }
 }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestHarness.java
Patch:
@@ -687,7 +687,7 @@ public HoodieInstant createCleanMetadata(String instantTime, boolean inflightOnl
 
   public HoodieInstant createCleanMetadata(String instantTime, boolean inflightOnly, boolean isEmpty) throws IOException {
     HoodieCleanerPlan cleanerPlan = new HoodieCleanerPlan(new HoodieActionInstant("", "", ""), "", new HashMap<>(),
-            CleanPlanV2MigrationHandler.VERSION, new HashMap<>());
+            CleanPlanV2MigrationHandler.VERSION, new HashMap<>(), new ArrayList<>());
     if (inflightOnly) {
       HoodieTestTable.of(metaClient).addInflightClean(instantTime, cleanerPlan);
     } else {

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanPlanV1MigrationHandler.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.common.table.timeline.versioning.clean;
 
+import java.util.ArrayList;
 import java.util.HashMap;
 import org.apache.hudi.avro.model.HoodieCleanerPlan;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -61,6 +62,6 @@ public HoodieCleanerPlan downgradeFrom(HoodieCleanerPlan plan) {
             .collect(Collectors.toList()));
         }).collect(Collectors.toMap(Pair::getKey, Pair::getValue));
     return new HoodieCleanerPlan(plan.getEarliestInstantToRetain(), plan.getPolicy(), filesPerPartition, VERSION,
-        new HashMap<>());
+        new HashMap<>(), new ArrayList<>());
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanPlanV2MigrationHandler.java
Patch:
@@ -27,6 +27,7 @@
 
 import org.apache.hadoop.fs.Path;
 
+import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -53,7 +54,7 @@ public HoodieCleanerPlan upgradeFrom(HoodieCleanerPlan plan) {
             new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), e.getKey()), v).toString(), false))
           .collect(Collectors.toList()))).collect(Collectors.toMap(Pair::getKey, Pair::getValue));
     return new HoodieCleanerPlan(plan.getEarliestInstantToRetain(), plan.getPolicy(), new HashMap<>(), VERSION,
-        filePathsPerPartition);
+        filePathsPerPartition, new ArrayList<>());
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java
Patch:
@@ -64,13 +64,13 @@ public static HoodieCleanMetadata convertCleanMetadata(String startCleanTime,
     for (HoodieCleanStat stat : cleanStats) {
       HoodieCleanPartitionMetadata metadata =
           new HoodieCleanPartitionMetadata(stat.getPartitionPath(), stat.getPolicy().name(),
-              stat.getDeletePathPatterns(), stat.getSuccessDeleteFiles(), stat.getFailedDeleteFiles());
+              stat.getDeletePathPatterns(), stat.getSuccessDeleteFiles(), stat.getFailedDeleteFiles(), stat.isPartitionDeleted());
       partitionMetadataMap.put(stat.getPartitionPath(), metadata);
       if ((null != stat.getDeleteBootstrapBasePathPatterns())
           && (!stat.getDeleteBootstrapBasePathPatterns().isEmpty())) {
         HoodieCleanPartitionMetadata bootstrapMetadata = new HoodieCleanPartitionMetadata(stat.getPartitionPath(),
             stat.getPolicy().name(), stat.getDeleteBootstrapBasePathPatterns(), stat.getSuccessDeleteBootstrapBaseFiles(),
-            stat.getFailedDeleteBootstrapBaseFiles());
+            stat.getFailedDeleteBootstrapBaseFiles(), stat.isPartitionDeleted());
         partitionBootstrapMetadataMap.put(stat.getPartitionPath(), bootstrapMetadata);
       }
       totalDeleted += stat.getSuccessDeleteFiles().size();

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestTable.java
Patch:
@@ -291,7 +291,7 @@ public HoodieTestTable addClean(String instantTime, HoodieCleanerPlan cleanerPla
 
   public HoodieTestTable addClean(String instantTime) throws IOException {
     HoodieCleanerPlan cleanerPlan = new HoodieCleanerPlan(new HoodieActionInstant(EMPTY_STRING, EMPTY_STRING, EMPTY_STRING), EMPTY_STRING, new HashMap<>(),
-        CleanPlanV2MigrationHandler.VERSION, new HashMap<>());
+        CleanPlanV2MigrationHandler.VERSION, new HashMap<>(), new ArrayList<>());
     HoodieCleanStat cleanStats = new HoodieCleanStat(
         HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS,
         HoodieTestUtils.DEFAULT_PARTITION_PATHS[RANDOM.nextInt(HoodieTestUtils.DEFAULT_PARTITION_PATHS.length)],
@@ -305,7 +305,7 @@ public HoodieTestTable addClean(String instantTime) throws IOException {
 
   public Pair<HoodieCleanerPlan, HoodieCleanMetadata> getHoodieCleanMetadata(String commitTime, HoodieTestTableState testTableState) {
     HoodieCleanerPlan cleanerPlan = new HoodieCleanerPlan(new HoodieActionInstant(commitTime, CLEAN_ACTION, EMPTY_STRING), EMPTY_STRING, new HashMap<>(),
-        CleanPlanV2MigrationHandler.VERSION, new HashMap<>());
+        CleanPlanV2MigrationHandler.VERSION, new HashMap<>(), new ArrayList<>());
     List<HoodieCleanStat> cleanStats = new ArrayList<>();
     for (Map.Entry<String, List<String>> entry : testTableState.getPartitionToFileIdMapForCleaner(commitTime).entrySet()) {
       cleanStats.add(new HoodieCleanStat(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS,

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableRollback.java
Patch:
@@ -166,7 +166,7 @@ void testRollbackWithDeltaAndCompactionCommit(boolean rollbackUsingMarkers) thro
       /*
        * Write 1 (only inserts)
        */
-      String newCommitTime = "001";
+      String newCommitTime = "000000001";
       client.startCommitWithTime(newCommitTime);
 
       List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 200);
@@ -183,7 +183,7 @@ void testRollbackWithDeltaAndCompactionCommit(boolean rollbackUsingMarkers) thro
 
       Option<HoodieInstant> deltaCommit = metaClient.getActiveTimeline().getDeltaCommitTimeline().firstInstant();
       assertTrue(deltaCommit.isPresent());
-      assertEquals("001", deltaCommit.get().getTimestamp(), "Delta commit should be 001");
+      assertEquals("000000001", deltaCommit.get().getTimestamp(), "Delta commit should be 000000001");
 
       Option<HoodieInstant> commit = metaClient.getActiveTimeline().getCommitTimeline().firstInstant();
       assertFalse(commit.isPresent());
@@ -201,7 +201,7 @@ void testRollbackWithDeltaAndCompactionCommit(boolean rollbackUsingMarkers) thro
       /*
        * Write 2 (inserts + updates - testing failed delta commit)
        */
-      final String commitTime1 = "002";
+      final String commitTime1 = "000000002";
       // WriteClient with custom config (disable small file handling)
       // NOTE: Second writer will have Metadata table ENABLED
       try (SparkRDDWriteClient secondClient = getHoodieWriteClient(getHoodieWriteConfigWithSmallFileHandlingOff(true));) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java
Patch:
@@ -471,7 +471,7 @@ protected void preCommit(HoodieInstant inflightInstant, HoodieCommitMetadata met
     // Important to create this after the lock to ensure the latest commits show up in the timeline without need for reload
     HoodieTable table = createTable(config, hadoopConf);
     TransactionUtils.resolveWriteConflictIfAny(table, this.txnManager.getCurrentTransactionOwner(),
-        Option.of(metadata), config, txnManager.getLastCompletedTransactionOwner());
+        Option.of(metadata), config, txnManager.getLastCompletedTransactionOwner(), false, this.pendingInflightAndRequestedInstants);
   }
 
   @Override

File: hudi-aws/src/main/java/org/apache/hudi/aws/transaction/lock/DynamoDBBasedLockProvider.java
Patch:
@@ -156,9 +156,9 @@ public LockItem getLock() {
 
   private AmazonDynamoDB getDynamoDBClient() {
     String region = this.lockConfiguration.getConfig().getString(DynamoDbBasedLockConfig.DYNAMODB_LOCK_REGION.key());
-    String endpointURL = this.lockConfiguration.getConfig().getString(DynamoDbBasedLockConfig.DYNAMODB_ENDPOINT_URL.key()) == null
-                          ? RegionUtils.getRegion(region).getServiceEndpoint(AmazonDynamoDB.ENDPOINT_PREFIX)
-                          : this.lockConfiguration.getConfig().getString(DynamoDbBasedLockConfig.DYNAMODB_ENDPOINT_URL.key());
+    String endpointURL = this.lockConfiguration.getConfig().containsKey(DynamoDbBasedLockConfig.DYNAMODB_ENDPOINT_URL.key())
+                          ? this.lockConfiguration.getConfig().getString(DynamoDbBasedLockConfig.DYNAMODB_ENDPOINT_URL.key())
+                          : RegionUtils.getRegion(region).getServiceEndpoint(AmazonDynamoDB.ENDPOINT_PREFIX);
     AwsClientBuilder.EndpointConfiguration dynamodbEndpoint =
             new AwsClientBuilder.EndpointConfiguration(endpointURL, region);
     return AmazonDynamoDBClientBuilder.standard()

File: hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSink.java
Patch:
@@ -73,7 +73,7 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
       // bulk_insert mode
       final String writeOperation = this.conf.get(FlinkOptions.OPERATION);
       if (WriteOperationType.fromValue(writeOperation) == WriteOperationType.BULK_INSERT) {
-        return context.isBounded() ? Pipelines.bulkInsert(conf, rowType, dataStream) : Pipelines.append(conf, rowType, dataStream);
+        return Pipelines.bulkInsert(conf, rowType, dataStream);
       }
 
       // Append mode

File: hudi-flink-datasource/hudi-flink/src/test/java/org/apache/hudi/table/ITTestHoodieDataSource.java
Patch:
@@ -1179,7 +1179,7 @@ void testParquetComplexNestedRowTypes(String operation) {
   @ParameterizedTest
   @ValueSource(strings = {"insert", "upsert", "bulk_insert"})
   void testBuiltinFunctionWithCatalog(String operation) {
-    TableEnvironment tableEnv = streamTableEnv;
+    TableEnvironment tableEnv = batchTableEnv;
 
     String hudiCatalogDDL = catalog("hudi_" + operation)
         .catalogPath(tempFile.getAbsolutePath())

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java
Patch:
@@ -389,9 +389,6 @@ private void completeClustering(HoodieReplaceCommitMetadata metadata,
       finalizeWrite(table, clusteringCommitTime, writeStats);
       // Update table's metadata (table)
       updateTableMetadata(table, metadata, clusteringInstant);
-      // Update tables' metadata indexes
-      // NOTE: This overlaps w/ metadata table (above) and will be reconciled in the future
-      table.updateMetadataIndexes(context, writeStats, clusteringCommitTime);
 
       LOG.info("Committing Clustering " + clusteringCommitTime + ". Finished with result " + metadata);
 

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java
Patch:
@@ -115,7 +115,7 @@ static String getSparkShellCommand(String commandFile) {
         .append(" --master local[2] --driver-class-path ").append(HADOOP_CONF_DIR)
         .append(
             " --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client  --driver-memory 1G --executor-memory 1G --num-executors 1 ")
-        .append(" --packages org.apache.spark:spark-avro_2.11:2.4.4 ").append(" -i ").append(commandFile).toString();
+        .append(" -i ").append(commandFile).toString();
   }
 
   static String getPrestoConsoleCommand(String commandFile) {

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/command/ITTestHoodieSyncCommand.java
Patch:
@@ -60,7 +60,7 @@ public void testValidateSync() throws Exception {
   }
 
   private void syncHoodieTable(String hiveTableName, String op) throws Exception {
-    StringBuilder cmdBuilder = new StringBuilder("spark-submit --packages org.apache.spark:spark-avro_2.11:2.4.4 ")
+    StringBuilder cmdBuilder = new StringBuilder("spark-submit")
         .append(" --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer ").append(HUDI_UTILITIES_BUNDLE)
         .append(" --table-type COPY_ON_WRITE ")
         .append(" --base-file-format ").append(HoodieFileFormat.PARQUET.toString())

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java
Patch:
@@ -69,7 +69,6 @@
  * ```
  * spark-submit \
  * --class org.apache.hudi.utilities.HoodieDataTableValidator \
- * --packages org.apache.spark:spark-avro_2.11:2.4.4 \
  * --master spark://xxxx:7077 \
  * --driver-memory 1g \
  * --executor-memory 1g \
@@ -85,7 +84,6 @@
  * ```
  * spark-submit \
  * --class org.apache.hudi.utilities.HoodieDataTableValidator \
- * --packages org.apache.spark:spark-avro_2.11:2.4.4 \
  * --master spark://xxxx:7077 \
  * --driver-memory 1g \
  * --executor-memory 1g \

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java
Patch:
@@ -92,7 +92,6 @@
  * ```
  * spark-submit \
  *  --class org.apache.hudi.utilities.HoodieMetadataTableValidator \
- *  --packages org.apache.spark:spark-avro_2.11:2.4.4 \
  *  --master spark://xxxx:7077 \
  *  --driver-memory 1g \
  *  --executor-memory 1g \
@@ -111,7 +110,6 @@
  * ```
  * spark-submit \
  *  --class org.apache.hudi.utilities.HoodieMetadataTableValidator \
- *  --packages org.apache.spark:spark-avro_2.11:2.4.4 \
  *  --master spark://xxxx:7077 \
  *  --driver-memory 1g \
  *  --executor-memory 1g \

File: hudi-aws/src/main/java/org/apache/hudi/config/DynamoDbBasedLockConfig.java
Patch:
@@ -103,8 +103,8 @@ public class DynamoDbBasedLockConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> DYNAMODB_ENDPOINT_URL = ConfigProperty
       .key(DYNAMODB_BASED_LOCK_PROPERTY_PREFIX + "endpoint_url")
-      .defaultValue("us-east-1")
-      .sinceVersion("0.11.0")
+      .noDefaultValue()
+      .sinceVersion("0.10.1")
       .withDocumentation("For DynamoDB based lock provider, the url endpoint used for Amazon DynamoDB service."
                          + " Useful for development with a local dynamodb instance.");
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieFileWriter.java
Patch:
@@ -37,8 +37,6 @@ public interface HoodieFileWriter<R extends IndexedRecord> {
 
   void writeAvro(String key, R oldRecord) throws IOException;
 
-  long getBytesWritten();
-
   default void prepRecordWithMetadata(R avroRecord, HoodieRecord record, String instantTime, Integer partitionId, AtomicLong recordIndex, String fileName) {
     String seqId = HoodieRecord.generateSequenceId(instantTime, partitionId, recordIndex.getAndIncrement());
     HoodieAvroUtils.addHoodieKeyToRecord((GenericRecord) avroRecord, record.getRecordKey(), record.getPartitionPath(), fileName);

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java
Patch:
@@ -402,7 +402,7 @@ public void testFileSizeUpsertRecords() throws Exception {
         counts++;
       }
     }
-    assertEquals(3, counts, "If the number of records are more than 1150, then there should be a new file");
+    assertEquals(5, counts, "If the number of records are more than 1150, then there should be a new file");
   }
 
   @Test

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieInternalRowParquetWriter.java
Patch:
@@ -56,7 +56,7 @@ public HoodieInternalRowParquetWriter(Path file, HoodieRowParquetConfig parquetC
 
   @Override
   public boolean canWrite() {
-    return fs.getBytesWritten(file) < maxFileSize;
+    return getDataSize() < maxFileSize;
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java
Patch:
@@ -437,7 +437,7 @@ public void testFileSizeUpsertRecords() throws Exception {
         counts++;
       }
     }
-    assertEquals(3, counts, "If the number of records are more than 1150, then there should be a new file");
+    assertEquals(5, counts, "If the number of records are more than 1150, then there should be a new file");
   }
 
   @Test

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java
Patch:
@@ -233,6 +233,9 @@ protected HoodieWriteMetadata<HoodieData<WriteStatus>> executeClustering(HoodieC
     table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());
     table.getMetaClient().reloadActiveTimeline();
 
+    // Disable auto commit. Strategy is only expected to write data in new files.
+    config.setValue(HoodieWriteConfig.AUTO_COMMIT_ENABLE, Boolean.FALSE.toString());
+
     final Schema schema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));
     HoodieWriteMetadata<HoodieData<WriteStatus>> writeMetadata = (
         (ClusteringExecutionStrategy<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>>)

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.table.action.commit;
 
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
@@ -111,7 +112,7 @@ public List<WriteStatus> bulkInsert(List<HoodieRecord<T>> inputRecords,
 
     FileIdPrefixProvider fileIdPrefixProvider = (FileIdPrefixProvider) ReflectionUtils.loadClass(
         config.getFileIdPrefixProviderClassName(),
-        config.getProps());
+        new TypedProperties(config.getProps()));
 
     List<WriteStatus> writeStatuses = new ArrayList<>();
 

File: hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/quickstart/HoodieSparkQuickstart.java
Patch:
@@ -23,6 +23,7 @@
 import static org.apache.spark.sql.SaveMode.Overwrite;
 import java.util.List;
 import org.apache.commons.lang.ArrayUtils;
+import org.apache.hudi.QuickstartUtils;
 import org.apache.hudi.common.model.HoodieAvroPayload;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.examples.quickstart;
+package org.apache.hudi;
 
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.model.HoodieAvroRecord;

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/TestQuickstartUtils.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.examples.quickstart;
+package org.apache.hudi;
 
 import org.apache.hudi.exception.HoodieException;
 import org.junit.jupiter.api.Assertions;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/processor/maxwell/MaxwellJsonKafkaSourcePostProcessor.java
Patch:
@@ -91,9 +91,9 @@ public static class Config {
 
     public static final ConfigProperty<String> PRECOMBINE_FIELD_TYPE_PROP = ConfigProperty
         .key("hoodie.deltastreamer.source.json.kafka.post.processor.maxwell.precombine.field.type")
-        .defaultValue("DATA_STRING")
+        .defaultValue(DATE_STRING.toString())
         .withDocumentation("Data type of the preCombine field. could be NON_TIMESTAMP, DATE_STRING,"
-            + "UNIX_TIMESTAMP or EPOCHMILLISECONDS. DATA_STRING by default ");
+            + "UNIX_TIMESTAMP or EPOCHMILLISECONDS. DATE_STRING by default ");
 
     public static final ConfigProperty<String> PRECOMBINE_FIELD_FORMAT_PROP = ConfigProperty
         .key("hoodie.deltastreamer.source.json.kafka.post.processor.maxwell.precombine.field.format")

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java
Patch:
@@ -227,8 +227,8 @@ protected GenericRecord rewriteRecord(GenericRecord record) {
     return HoodieAvroUtils.rewriteRecord(record, writeSchemaWithMetaFields);
   }
 
-  protected GenericRecord rewriteRecord(GenericRecord record, boolean copyOverMetaFields, GenericRecord fallbackRecord) {
-    return HoodieAvroUtils.rewriteRecord(record, writeSchemaWithMetaFields, copyOverMetaFields, fallbackRecord);
+  protected GenericRecord rewriteRecordWithMetadata(GenericRecord record, String fileName) {
+    return HoodieAvroUtils.rewriteRecordWithMetadata(record, writeSchemaWithMetaFields, fileName);
   }
 
   public abstract List<WriteStatus> close();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hudi.common.testutils.RawTripTestPayload;
 import org.apache.hudi.common.util.BaseFileUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieUpsertException;
 import org.apache.hudi.io.HoodieCreateHandle;
@@ -77,6 +78,7 @@ public void tearDown() throws IOException {
   private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {
     // Create a bunch of records with an old version of schema
     final HoodieWriteConfig config = makeHoodieClientConfig("/exampleSchema.avsc");
+    config.setValue(HoodieCompactionConfig.PRESERVE_COMMIT_METADATA, "false");
     final HoodieSparkTable table = HoodieSparkTable.create(config, context);
     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {
       List<HoodieRecord> insertRecords = new ArrayList<>();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java
Patch:
@@ -780,7 +780,7 @@ public List<HoodieColumnRangeMetadata<String>> getSortedColumnStatsList(
         return allColumnNameList.stream()
             .flatMap(columnName ->
                 tableMetadata.getColumnStats(partitionFileNameList, columnName).values().stream()
-                    .map(stats -> new HoodieColumnRangeMetadata<>(
+                    .map(stats -> HoodieColumnRangeMetadata.create(
                         stats.getFileName(),
                         columnName,
                         stats.getMinValue(),
@@ -799,7 +799,7 @@ public List<HoodieColumnRangeMetadata<String>> getSortedColumnStatsList(
                     metaClient.getHadoopConf(),
                     new Path(new Path(metaClient.getBasePath(), partitionPath), filename),
                     allColumnNameList).stream())
-            .map(rangeMetadata -> new HoodieColumnRangeMetadata<String>(
+            .map(rangeMetadata -> HoodieColumnRangeMetadata.create(
                 rangeMetadata.getFilePath(),
                 rangeMetadata.getColumnName(),
                 // Note: here we ignore the type in the validation,

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/HoodieDeltaStreamerTestBase.java
Patch:
@@ -137,7 +137,7 @@ protected static void prepareInitialConfigs(FileSystem dfs, String dfsBasePath,
     TypedProperties downstreamProps = new TypedProperties();
     downstreamProps.setProperty("include", "base.properties");
     downstreamProps.setProperty("hoodie.datasource.write.recordkey.field", "_row_key");
-    downstreamProps.setProperty("hoodie.datasource.write.partitionpath.field", "not_there");
+    downstreamProps.setProperty("hoodie.datasource.write.partitionpath.field", "partition_path");
 
     // Source schema is the target schema of upstream table
     downstreamProps.setProperty("hoodie.deltastreamer.schemaprovider.source.schema.file", dfsBasePath + "/target.avsc");
@@ -149,7 +149,7 @@ protected static void prepareInitialConfigs(FileSystem dfs, String dfsBasePath,
     invalidProps.setProperty("include", "sql-transformer.properties");
     invalidProps.setProperty("hoodie.datasource.write.keygenerator.class", "invalid");
     invalidProps.setProperty("hoodie.datasource.write.recordkey.field", "_row_key");
-    invalidProps.setProperty("hoodie.datasource.write.partitionpath.field", "not_there");
+    invalidProps.setProperty("hoodie.datasource.write.partitionpath.field", "partition_path");
     invalidProps.setProperty("hoodie.deltastreamer.schemaprovider.source.schema.file", dfsBasePath + "/source.avsc");
     invalidProps.setProperty("hoodie.deltastreamer.schemaprovider.target.schema.file", dfsBasePath + "/target.avsc");
     UtilitiesTestBase.Helpers.savePropsToDFS(invalidProps, dfs, dfsBasePath + "/" + PROPS_FILENAME_TEST_INVALID);

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java
Patch:
@@ -59,6 +59,7 @@
 import static org.apache.hudi.common.util.ValidationUtils.checkArgument;
 import static org.apache.hudi.common.util.ValidationUtils.checkState;
 import static org.apache.hudi.metadata.HoodieTableMetadata.RECORDKEY_PARTITION_LIST;
+import static org.apache.hudi.metadata.HoodieTableMetadataUtil.getPartition;
 
 /**
  * MetadataTable records are persisted with the schema defined in HoodieMetadata.avsc.
@@ -222,7 +223,7 @@ protected HoodieMetadataPayload(String key, int type,
    */
   public static HoodieRecord<HoodieMetadataPayload> createPartitionListRecord(List<String> partitions) {
     Map<String, HoodieMetadataFileInfo> fileInfo = new HashMap<>();
-    partitions.forEach(partition -> fileInfo.put(partition, new HoodieMetadataFileInfo(0L, false)));
+    partitions.forEach(partition -> fileInfo.put(getPartition(partition), new HoodieMetadataFileInfo(0L, false)));
 
     HoodieKey key = new HoodieKey(RECORDKEY_PARTITION_LIST, MetadataPartitionType.FILES.getPartitionPath());
     HoodieMetadataPayload payload = new HoodieMetadataPayload(key.getRecordKey(), METADATA_TYPE_PARTITION_LIST,

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java
Patch:
@@ -123,7 +123,7 @@ public void testUpdateRecords() throws Exception {
     // Prepare the AvroParquetIO
     HoodieWriteConfig config = makeHoodieClientConfig();
     int startInstant = 1;
-    String firstCommitTime = makeNewCommitTime(startInstant++);
+    String firstCommitTime = makeNewCommitTime(startInstant++, "%09d");
     HoodieJavaWriteClient writeClient = getHoodieWriteClient(config);
     writeClient.startCommitWithTime(firstCommitTime);
     metaClient = HoodieTableMetaClient.reload(metaClient);
@@ -185,7 +185,7 @@ public void testUpdateRecords() throws Exception {
 
     List<HoodieRecord> updatedRecords = Arrays.asList(updatedRecord1, insertedRecord1);
 
-    String newCommitTime = makeNewCommitTime(startInstant++);
+    String newCommitTime = makeNewCommitTime(startInstant++, "%09d");
     metaClient = HoodieTableMetaClient.reload(metaClient);
     writeClient.startCommitWithTime(newCommitTime);
     List<WriteStatus> statuses = writeClient.upsert(updatedRecords, newCommitTime);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -180,7 +180,7 @@ public abstract HoodieWriteMetadata<O> insert(HoodieEngineContext context, Strin
    * @return HoodieWriteMetadata
    */
   public abstract HoodieWriteMetadata<O> bulkInsert(HoodieEngineContext context, String instantTime,
-      I records, Option<BulkInsertPartitioner<I>> bulkInsertPartitioner);
+      I records, Option<BulkInsertPartitioner> bulkInsertPartitioner);
 
   /**
    * Deletes a list of {@link HoodieKey}s from the Hoodie table, at the supplied instantTime {@link HoodieKey}s will be
@@ -237,7 +237,7 @@ public abstract HoodieWriteMetadata<O> insertPrepped(HoodieEngineContext context
    * @return HoodieWriteMetadata
    */
   public abstract HoodieWriteMetadata<O> bulkInsertPrepped(HoodieEngineContext context, String instantTime,
-      I preppedRecords,  Option<BulkInsertPartitioner<I>> bulkInsertPartitioner);
+      I preppedRecords,  Option<BulkInsertPartitioner> bulkInsertPartitioner);
 
   /**
    * Replaces all the existing records and inserts the specified new records into Hoodie table at the supplied instantTime,

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java
Patch:
@@ -28,7 +28,7 @@
 /**
  * When file groups in clustering, write records to these file group need to check.
  */
-public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I> {
+public abstract class UpdateStrategy<T extends HoodieRecordPayload, I> {
 
   protected final HoodieEngineContext engineContext;
   protected Set<HoodieFileGroupId> fileGroupsInPendingClustering;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseBulkInsertHelper.java
Patch:
@@ -34,15 +34,15 @@ public abstract class BaseBulkInsertHelper<T extends HoodieRecordPayload, I, K,
   public abstract HoodieWriteMetadata<O> bulkInsert(I inputRecords, String instantTime,
                                                     HoodieTable<T, I, K, O> table, HoodieWriteConfig config,
                                                     BaseCommitActionExecutor<T, I, K, O, R> executor, boolean performDedupe,
-                                                    Option<BulkInsertPartitioner<I>> userDefinedBulkInsertPartitioner);
+                                                    Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner);
 
   /**
    * Only write input records. Does not change timeline/index. Return information about new files created.
    */
   public abstract O bulkInsert(I inputRecords, String instantTime,
                                HoodieTable<T, I, K, O> table, HoodieWriteConfig config,
                                boolean performDedupe,
-                               Option<BulkInsertPartitioner<I>> userDefinedBulkInsertPartitioner,
+                               Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner,
                                boolean addMetadataFields,
                                int parallelism,
                                WriteHandleFactory writeHandleFactory);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java
Patch:
@@ -68,6 +68,7 @@
 import org.slf4j.LoggerFactory;
 
 import javax.annotation.Nonnull;
+
 import java.io.IOException;
 import java.util.Collections;
 import java.util.Iterator;
@@ -231,7 +232,7 @@ public HoodieWriteMetadata<List<WriteStatus>> insert(HoodieEngineContext context
   public HoodieWriteMetadata<List<WriteStatus>> bulkInsert(HoodieEngineContext context,
                                                            String instantTime,
                                                            List<HoodieRecord<T>> records,
-                                                           Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> bulkInsertPartitioner) {
+                                                           Option<BulkInsertPartitioner> bulkInsertPartitioner) {
     throw new HoodieNotSupportedException("BulkInsert is not supported yet");
   }
 
@@ -264,7 +265,7 @@ public HoodieWriteMetadata<List<WriteStatus>> insertPrepped(HoodieEngineContext
   public HoodieWriteMetadata<List<WriteStatus>> bulkInsertPrepped(HoodieEngineContext context,
                                                                   String instantTime,
                                                                   List<HoodieRecord<T>> preppedRecords,
-                                                                  Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> bulkInsertPartitioner) {
+                                                                  Option<BulkInsertPartitioner> bulkInsertPartitioner) {
     throw new HoodieNotSupportedException("BulkInsertPrepped is not supported yet");
   }
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java
Patch:
@@ -114,7 +114,7 @@ public HoodieWriteMetadata<List<WriteStatus>> insert(HoodieEngineContext context
   public HoodieWriteMetadata<List<WriteStatus>> bulkInsert(HoodieEngineContext context,
                                                            String instantTime,
                                                            List<HoodieRecord<T>> records,
-                                                           Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> bulkInsertPartitioner) {
+                                                           Option<BulkInsertPartitioner> bulkInsertPartitioner) {
     return new JavaBulkInsertCommitActionExecutor((HoodieJavaEngineContext) context, config,
         this, instantTime, records, bulkInsertPartitioner).execute();
   }
@@ -152,7 +152,7 @@ public HoodieWriteMetadata<List<WriteStatus>> insertPrepped(HoodieEngineContext
   public HoodieWriteMetadata<List<WriteStatus>> bulkInsertPrepped(HoodieEngineContext context,
                                                                   String instantTime,
                                                                   List<HoodieRecord<T>> preppedRecords,
-                                                                  Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> bulkInsertPartitioner) {
+                                                                  Option<BulkInsertPartitioner> bulkInsertPartitioner) {
     return new JavaBulkInsertPreppedCommitActionExecutor((HoodieJavaEngineContext) context, config,
         this, instantTime, preppedRecords, bulkInsertPartitioner).execute();
   }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaMergeOnReadTable.java
Patch:
@@ -61,7 +61,7 @@ public HoodieWriteMetadata<List<WriteStatus>> upsertPrepped(HoodieEngineContext
   public HoodieWriteMetadata<List<WriteStatus>> bulkInsertPrepped(HoodieEngineContext context,
                                                                   String instantTime,
                                                                   List<HoodieRecord<T>> preppedRecords,
-                                                                  Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> bulkInsertPartitioner) {
+                                                                  Option<BulkInsertPartitioner> bulkInsertPartitioner) {
     return new JavaBulkInsertPreppedCommitActionExecutor((HoodieJavaEngineContext) context, config,
         this, instantTime, preppedRecords, bulkInsertPartitioner).execute();
   }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertCommitActionExecutor.java
Patch:
@@ -36,17 +36,17 @@
 public class JavaBulkInsertCommitActionExecutor<T extends HoodieRecordPayload<T>> extends BaseJavaCommitActionExecutor<T> {
 
   private final List<HoodieRecord<T>> inputRecords;
-  private final Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> bulkInsertPartitioner;
+  private final Option<BulkInsertPartitioner> bulkInsertPartitioner;
 
   public JavaBulkInsertCommitActionExecutor(HoodieJavaEngineContext context, HoodieWriteConfig config, HoodieTable table,
                                             String instantTime, List<HoodieRecord<T>> inputRecords,
-                                            Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> bulkInsertPartitioner) {
+                                            Option<BulkInsertPartitioner> bulkInsertPartitioner) {
     this(context, config, table, instantTime, inputRecords, bulkInsertPartitioner, Option.empty());
   }
 
   public JavaBulkInsertCommitActionExecutor(HoodieJavaEngineContext context, HoodieWriteConfig config, HoodieTable table,
                                             String instantTime, List<HoodieRecord<T>> inputRecords,
-                                            Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> bulkInsertPartitioner,
+                                            Option<BulkInsertPartitioner> bulkInsertPartitioner,
                                             Option<Map<String, String>> extraMetadata) {
     super(context, config, table, instantTime, WriteOperationType.BULK_INSERT, extraMetadata);
     this.inputRecords = inputRecords;

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java
Patch:
@@ -65,7 +65,7 @@ public HoodieWriteMetadata<List<WriteStatus>> bulkInsert(final List<HoodieRecord
                                                            final HoodieWriteConfig config,
                                                            final BaseCommitActionExecutor<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>, R> executor,
                                                            final boolean performDedupe,
-                                                           final Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> userDefinedBulkInsertPartitioner) {
+                                                           final Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner) {
     HoodieWriteMetadata result = new HoodieWriteMetadata();
 
     // It's possible the transition to inflight could have already happened.
@@ -89,7 +89,7 @@ public List<WriteStatus> bulkInsert(List<HoodieRecord<T>> inputRecords,
                                       HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,
                                       HoodieWriteConfig config,
                                       boolean performDedupe,
-                                      Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> userDefinedBulkInsertPartitioner,
+                                      Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner,
                                       boolean useWriterSchema,
                                       int parallelism,
                                       WriteHandleFactory writeHandleFactory) {
@@ -106,6 +106,7 @@ public List<WriteStatus> bulkInsert(List<HoodieRecord<T>> inputRecords,
     BulkInsertPartitioner partitioner = userDefinedBulkInsertPartitioner.isPresent()
         ? userDefinedBulkInsertPartitioner.get()
         : JavaBulkInsertInternalPartitionerFactory.get(config.getBulkInsertSortMode());
+    // only List is supported for Java partitioner, but it is not enforced by BulkInsertPartitioner API. To improve this, TODO HUDI-3463
     repartitionedRecords = (List<HoodieRecord<T>>) partitioner.repartitionRecords(dedupedRecords, parallelism);
 
     FileIdPrefixProvider fileIdPrefixProvider = (FileIdPrefixProvider) ReflectionUtils.loadClass(

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertPreppedCommitActionExecutor.java
Patch:
@@ -36,12 +36,12 @@ public class JavaBulkInsertPreppedCommitActionExecutor<T extends HoodieRecordPay
     extends BaseJavaCommitActionExecutor<T> {
 
   private final List<HoodieRecord<T>> preppedInputRecord;
-  private final Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> userDefinedBulkInsertPartitioner;
+  private final Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner;
 
   public JavaBulkInsertPreppedCommitActionExecutor(HoodieJavaEngineContext context,
                                                    HoodieWriteConfig config, HoodieTable table,
                                                    String instantTime, List<HoodieRecord<T>> preppedInputRecord,
-                                                   Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> userDefinedBulkInsertPartitioner) {
+                                                   Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner) {
     super(context, config, table, instantTime, WriteOperationType.BULK_INSERT);
     this.preppedInputRecord = preppedInputRecord;
     this.userDefinedBulkInsertPartitioner = userDefinedBulkInsertPartitioner;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieReadClient.java
Patch:
@@ -68,7 +68,7 @@ public class HoodieReadClient<T extends HoodieRecordPayload<T>> implements Seria
    * base path pointing to the table. Until, then just always assume a BloomIndex
    */
   private final transient HoodieIndex<?, ?> index;
-  private HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> hoodieTable;
+  private HoodieTable hoodieTable;
   private transient Option<SQLContext> sqlContextOpt;
   private final transient HoodieSparkEngineContext context;
   private final transient Configuration hadoopConf;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkSingleFileSortExecutionStrategy.java
Patch:
@@ -20,6 +20,7 @@
 package org.apache.hudi.client.clustering.run.strategy;
 
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -34,7 +35,6 @@
 import org.apache.avro.Schema;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
-import org.apache.spark.api.java.JavaRDD;
 
 import java.util.List;
 import java.util.Map;
@@ -56,7 +56,7 @@ public SparkSingleFileSortExecutionStrategy(HoodieTable table,
   }
 
   @Override
-  public JavaRDD<WriteStatus> performClusteringWithRecordsRDD(JavaRDD<HoodieRecord<T>> inputRecords,
+  public HoodieData<WriteStatus> performClusteringWithRecordsRDD(HoodieData<HoodieRecord<T>> inputRecords,
                                                               int numOutputGroups,
                                                               String instantTime,
                                                               Map<String, String> strategyParams,
@@ -74,7 +74,7 @@ public JavaRDD<WriteStatus> performClusteringWithRecordsRDD(JavaRDD<HoodieRecord
     // Since clustering will write to single file group using HoodieUnboundedCreateHandle, set max file size to a large value.
     props.put(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE.key(), String.valueOf(Long.MAX_VALUE));
     HoodieWriteConfig newConfig = HoodieWriteConfig.newBuilder().withProps(props).build();
-    return (JavaRDD<WriteStatus>) SparkBulkInsertHelper.newInstance().bulkInsert(inputRecords, instantTime, getHoodieTable(), newConfig,
+    return (HoodieData<WriteStatus>) SparkBulkInsertHelper.newInstance().bulkInsert(inputRecords, instantTime, getHoodieTable(), newConfig,
         false, getPartitioner(strategyParams, schema), true, numOutputGroups, new SingleFileHandleCreateFactory(fileGroupIdList.get(0).getFileId(), preserveHoodieMetadata));
   }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkSortAndSizeExecutionStrategy.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.client.clustering.run.strategy;
 
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -32,7 +33,6 @@
 import org.apache.avro.Schema;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
-import org.apache.spark.api.java.JavaRDD;
 
 import java.util.List;
 import java.util.Map;
@@ -54,7 +54,7 @@ public SparkSortAndSizeExecutionStrategy(HoodieTable table,
   }
 
   @Override
-  public JavaRDD<WriteStatus> performClusteringWithRecordsRDD(final JavaRDD<HoodieRecord<T>> inputRecords, final int numOutputGroups,
+  public HoodieData<WriteStatus> performClusteringWithRecordsRDD(final HoodieData<HoodieRecord<T>> inputRecords, final int numOutputGroups,
                                                               final String instantTime, final Map<String, String> strategyParams, final Schema schema,
                                                               final List<HoodieFileGroupId> fileGroupIdList, final boolean preserveHoodieMetadata) {
     LOG.info("Starting clustering for a group, parallelism:" + numOutputGroups + " commit:" + instantTime);
@@ -64,7 +64,7 @@ public JavaRDD<WriteStatus> performClusteringWithRecordsRDD(final JavaRDD<Hoodie
     props.put(HoodieWriteConfig.AUTO_COMMIT_ENABLE.key(), Boolean.FALSE.toString());
     props.put(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE.key(), String.valueOf(getWriteConfig().getClusteringTargetFileMaxBytes()));
     HoodieWriteConfig newConfig = HoodieWriteConfig.newBuilder().withProps(props).build();
-    return (JavaRDD<WriteStatus>) SparkBulkInsertHelper.newInstance()
+    return (HoodieData<WriteStatus>) SparkBulkInsertHelper.newInstance()
         .bulkInsert(inputRecords, instantTime, getHoodieTable(), newConfig, false, getPartitioner(strategyParams, schema), true, numOutputGroups, new CreateHandleFactory(preserveHoodieMetadata));
   }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkAllowUpdateStrategy.java
Patch:
@@ -19,13 +19,12 @@
 package org.apache.hudi.client.clustering.update.strategy;
 
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
+import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.collection.Pair;
 
-import org.apache.spark.api.java.JavaRDD;
-
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
@@ -42,7 +41,7 @@ public SparkAllowUpdateStrategy(HoodieSparkEngineContext engineContext,
   }
 
   @Override
-  public Pair<JavaRDD<HoodieRecord<T>>, Set<HoodieFileGroupId>> handleUpdate(JavaRDD<HoodieRecord<T>> taggedRecordsRDD) {
+  public Pair<HoodieData<HoodieRecord<T>>, Set<HoodieFileGroupId>> handleUpdate(HoodieData<HoodieRecord<T>> taggedRecordsRDD) {
     List<HoodieFileGroupId> fileGroupIdsWithRecordUpdate = getGroupIdsWithUpdate(taggedRecordsRDD);
     Set<HoodieFileGroupId> fileGroupIdsWithUpdatesAndPendingClustering = fileGroupIdsWithRecordUpdate.stream()
         .filter(f -> fileGroupsInPendingClustering.contains(f))

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.client.clustering.update.strategy;
 
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
+import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
@@ -27,7 +28,6 @@
 
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
-import org.apache.spark.api.java.JavaRDD;
 
 import java.util.Collections;
 import java.util.HashSet;
@@ -47,7 +47,7 @@ public SparkRejectUpdateStrategy(HoodieSparkEngineContext engineContext,
   }
 
   @Override
-  public Pair<JavaRDD<HoodieRecord<T>>, Set<HoodieFileGroupId>> handleUpdate(JavaRDD<HoodieRecord<T>> taggedRecordsRDD) {
+  public Pair<HoodieData<HoodieRecord<T>>, Set<HoodieFileGroupId>> handleUpdate(HoodieData<HoodieRecord<T>> taggedRecordsRDD) {
     List<HoodieFileGroupId> fileGroupIdsWithRecordUpdate = getGroupIdsWithUpdate(taggedRecordsRDD);
     fileGroupIdsWithRecordUpdate.forEach(fileGroupIdWithRecordUpdate -> {
       if (fileGroupsInPendingClustering.contains(fileGroupIdWithRecordUpdate)) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQueryEqualityPreCommitValidator.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.client.validator;
 
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodiePreCommitValidatorConfig;
@@ -28,7 +29,6 @@
 
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
-import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.SQLContext;
@@ -40,7 +40,7 @@
  * 
  * Expects both queries to return same result.
  */
-public class SqlQueryEqualityPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends JavaRDD<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
+public class SqlQueryEqualityPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends HoodieData<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
 
   private static final Logger LOG = LogManager.getLogger(SqlQueryEqualityPreCommitValidator.class);
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQueryInequalityPreCommitValidator.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.client.validator;
 
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodiePreCommitValidatorConfig;
@@ -28,7 +29,6 @@
 
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
-import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.SQLContext;
@@ -40,7 +40,7 @@
  * <p>
  * Expects query results do not match.
  */
-public class SqlQueryInequalityPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends JavaRDD<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
+public class SqlQueryInequalityPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends HoodieData<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
   private static final Logger LOG = LogManager.getLogger(SqlQueryInequalityPreCommitValidator.class);
 
   public SqlQueryInequalityPreCommitValidator(HoodieSparkTable<T> table, HoodieEngineContext engineContext, HoodieWriteConfig config) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQueryPreCommitValidator.java
Patch:
@@ -20,15 +20,16 @@
 
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
+import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieValidationException;
 import org.apache.hudi.table.HoodieSparkTable;
+
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
-import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
@@ -41,7 +42,7 @@
 /**
  * Validator framework to run sql queries and compare table state at different locations.
  */
-public abstract class SqlQueryPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends JavaRDD<WriteStatus>> extends SparkPreCommitValidator<T, I, K, O> {
+public abstract class SqlQueryPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends HoodieData<WriteStatus>> extends SparkPreCommitValidator<T, I, K, O> {
   private static final Logger LOG = LogManager.getLogger(SqlQueryPreCommitValidator.class);
   private static final AtomicInteger TABLE_COUNTER = new AtomicInteger(0);
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQuerySingleResultPreCommitValidator.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.client.validator;
 
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodiePreCommitValidatorConfig;
@@ -28,7 +29,6 @@
 
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
-import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.SQLContext;
 
@@ -40,7 +40,7 @@
  * <p>
  * Example configuration: "query1#expectedResult1;query2#expectedResult2;"
  */
-public class SqlQuerySingleResultPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends JavaRDD<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
+public class SqlQuerySingleResultPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends HoodieData<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {
   private static final Logger LOG = LogManager.getLogger(SqlQueryInequalityPreCommitValidator.class);
 
   public SqlQuerySingleResultPreCommitValidator(HoodieSparkTable<T> table, HoodieEngineContext engineContext, HoodieWriteConfig config) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestHoodieMergeOnReadTable.java
Patch:
@@ -39,6 +39,7 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieClusteringConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
+import org.apache.hudi.data.HoodieJavaRDD;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.index.HoodieIndex.IndexType;
 import org.apache.hudi.metadata.HoodieTableMetadataWriter;
@@ -556,7 +557,7 @@ public void testHandleUpdateWithMultiplePartitions() throws Exception {
       // initialize partitioner
       hoodieTable.getHoodieView().sync();
       BaseSparkDeltaCommitActionExecutor actionExecutor = new SparkDeleteDeltaCommitActionExecutor(context(), cfg, hoodieTable,
-          newDeleteTime, deleteRDD);
+          newDeleteTime, HoodieJavaRDD.of(deleteRDD));
       actionExecutor.getUpsertPartitioner(new WorkloadProfile(buildProfile(deleteRDD)));
       final List<List<WriteStatus>> deleteStatus = jsc().parallelize(Arrays.asList(1)).map(x -> {
         return actionExecutor.handleUpdate(partitionPath, fileId, fewRecordsForDelete.iterator());

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -195,12 +196,12 @@ public void testWriteStatusContentsAfterCompaction() throws Exception {
       String compactionInstantTime = "102";
       table.scheduleCompaction(context, compactionInstantTime, Option.empty());
       table.getMetaClient().reloadActiveTimeline();
-      JavaRDD<WriteStatus> result = (JavaRDD<WriteStatus>) table.compact(
+      HoodieData<WriteStatus> result = (HoodieData<WriteStatus>) table.compact(
           context, compactionInstantTime).getWriteStatuses();
 
       // Verify that all partition paths are present in the WriteStatus result
       for (String partitionPath : dataGen.getPartitionPaths()) {
-        List<WriteStatus> writeStatuses = result.collect();
+        List<WriteStatus> writeStatuses = result.collectAsList();
         assertTrue(writeStatuses.stream()
             .filter(writeStatus -> writeStatus.getStat().getPartitionPath().contentEquals(partitionPath)).count() > 0);
       }

File: hudi-common/src/main/java/org/apache/hudi/common/data/HoodiePairData.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.hudi.common.data;
 
+import org.apache.hudi.common.function.SerializableBiFunction;
 import org.apache.hudi.common.function.SerializableFunction;
 import org.apache.hudi.common.function.SerializablePairFunction;
 import org.apache.hudi.common.util.Option;
@@ -72,6 +73,8 @@ public abstract class HoodiePairData<K, V> implements Serializable {
    */
   public abstract Map<K, Long> countByKey();
 
+  public abstract HoodiePairData<K, V> reduceByKey(SerializableBiFunction<V, V, V> func, int parallelism);
+
   /**
    * @param func serializable map function.
    * @param <O>  output object type.

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieROTablePathFilter.java
Patch:
@@ -45,6 +45,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
 import java.util.stream.Collectors;
 
 /**
@@ -93,7 +94,7 @@ public HoodieROTablePathFilter() {
   }
 
   public HoodieROTablePathFilter(Configuration conf) {
-    this.hoodiePathCache = new HashMap<>();
+    this.hoodiePathCache = new ConcurrentHashMap<>();
     this.nonHoodiePathCache = new HashSet<>();
     this.conf = new SerializableConfiguration(conf);
     this.metaClientCache = new HashMap<>();

File: hudi-flink/src/main/java/org/apache/hudi/streamer/HoodieFlinkStreamer.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.formats.common.TimestampFormat;
 import org.apache.flink.formats.json.JsonRowDataDeserializationSchema;
-import org.apache.flink.runtime.state.filesystem.FsStateBackend;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;
@@ -61,8 +60,9 @@ public static void main(String[] args) throws Exception {
     // There can only be one checkpoint at one time.
     env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);
 
+    env.setStateBackend(cfg.stateBackend);
     if (cfg.flinkCheckPointPath != null) {
-      env.setStateBackend(new FsStateBackend(cfg.flinkCheckPointPath));
+      env.getCheckpointConfig().setCheckpointStorage(cfg.flinkCheckPointPath);
     }
 
     TypedProperties kafkaProps = DFSPropertiesConfiguration.getGlobalProps();

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieOperation.java
Patch:
@@ -40,7 +40,7 @@ public enum HoodieOperation {
   /**
    * Delete operation.
    */
-  DELETE("D", (byte) 4);
+  DELETE("D", (byte) 3);
 
   private final String name;
   private final byte value;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -44,6 +44,7 @@
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ReflectionUtils;
+import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.config.metrics.HoodieMetricsConfig;
 import org.apache.hudi.config.metrics.HoodieMetricsDatadogConfig;
@@ -1552,7 +1553,8 @@ public double getParquetCompressionRatio() {
   }
 
   public CompressionCodecName getParquetCompressionCodec() {
-    return CompressionCodecName.fromConf(getString(HoodieStorageConfig.PARQUET_COMPRESSION_CODEC_NAME));
+    String codecName = getString(HoodieStorageConfig.PARQUET_COMPRESSION_CODEC_NAME);
+    return CompressionCodecName.fromConf(StringUtils.isNullOrEmpty(codecName) ? null : codecName);
   }
 
   public boolean parquetDictionaryEnabled() {

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java
Patch:
@@ -133,7 +133,7 @@ public HoodieMetadataPayload(Option<GenericRecord> recordOpt) {
       // This can be simplified using SpecificData.deepcopy once this bug is fixed
       // https://issues.apache.org/jira/browse/AVRO-1811
       //
-      // NOTE: {@code HoodieMetadataRecord} has to always carry both "key" nad "type" fields
+      // NOTE: {@code HoodieMetadataRecord} has to always carry both "key" and "type" fields
       //       for it to be handled appropriately, therefore these fields have to be reflected
       //       in any (read-)projected schema
       key = record.get(KEY_FIELD_NAME).toString();

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -316,6 +316,8 @@ public static HiveSyncConfig buildHiveSyncConfig(TypedProperties props, String b
     if (props.containsKey(HiveExternalCatalog.CREATED_SPARK_VERSION())) {
       hiveSyncConfig.sparkVersion = props.getString(HiveExternalCatalog.CREATED_SPARK_VERSION());
     }
+    hiveSyncConfig.syncComment = Boolean.valueOf(props.getString(DataSourceWriteOptions.HIVE_SYNC_COMMENT().key(),
+            DataSourceWriteOptions.HIVE_SYNC_COMMENT().defaultValue()));
     return hiveSyncConfig;
   }
 

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java
Patch:
@@ -97,7 +97,7 @@ public HoodieWriteableTestTable forCommit(String instantTime) {
     return (HoodieWriteableTestTable) super.forCommit(instantTime);
   }
 
-  public HoodieWriteableTestTable withInserts(String partition, String fileId, List<HoodieRecord> records, TaskContextSupplier contextSupplier) throws Exception {
+  public Path withInserts(String partition, String fileId, List<HoodieRecord> records, TaskContextSupplier contextSupplier) throws Exception {
     FileCreateUtils.createPartitionMetaFile(basePath, partition);
     String fileName = baseFileName(currentInstantTime, fileId);
 
@@ -151,7 +151,7 @@ public HoodieWriteableTestTable withInserts(String partition, String fileId, Lis
       }
     }
 
-    return this;
+    return baseFilePath;
   }
 
   public Map<String, List<HoodieLogFile>> withLogAppends(List<HoodieRecord> records) throws Exception {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -91,7 +91,7 @@ protected <T extends SpecificRecordBase> void initialize(HoodieEngineContext eng
                                                            Option<String> inflightInstantTimestamp) {
     try {
       if (enabled) {
-        bootstrapIfNeeded(engineContext, dataMetaClient, actionMetadata, inflightInstantTimestamp);
+        initializeIfNeeded(dataMetaClient, actionMetadata, inflightInstantTimestamp);
       }
     } catch (IOException e) {
       LOG.error("Failed to initialize metadata table. Disabling the writer.", e);

File: hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkWriteableTestTable.java
Patch:
@@ -108,7 +108,8 @@ public HoodieFlinkWriteableTestTable withInserts(String partition, String fileId
   }
 
   public HoodieFlinkWriteableTestTable withInserts(String partition, String fileId, List<HoodieRecord> records) throws Exception {
-    return (HoodieFlinkWriteableTestTable) withInserts(partition, fileId, records, new org.apache.hudi.client.FlinkTaskContextSupplier(null));
+    withInserts(partition, fileId, records, new org.apache.hudi.client.FlinkTaskContextSupplier(null));
+    return this;
   }
 
   public Map<String, List<HoodieLogFile>> withLogAppends(List<HoodieRecord> records) throws Exception {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -113,7 +113,7 @@ protected <T extends SpecificRecordBase> void initialize(HoodieEngineContext eng
       });
 
       if (enabled) {
-        bootstrapIfNeeded(engineContext, dataMetaClient, actionMetadata, inflightInstantTimestamp);
+        initializeIfNeeded(dataMetaClient, actionMetadata, inflightInstantTimestamp);
       }
     } catch (IOException e) {
       LOG.error("Failed to initialize metadata table. Disabling the writer.", e);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieMetadataBase.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.client.functional;
 
-import org.apache.hadoop.fs.Path;
 import org.apache.hudi.avro.model.HoodieMetadataRecord;
+import org.apache.hudi.client.HoodieTimelineArchiver;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
@@ -52,8 +52,9 @@
 import org.apache.hudi.metadata.SparkHoodieBackedTableMetadataWriter;
 import org.apache.hudi.table.HoodieSparkTable;
 import org.apache.hudi.table.HoodieTable;
-import org.apache.hudi.client.HoodieTimelineArchiver;
 import org.apache.hudi.testutils.HoodieClientTestHarness;
+
+import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 import org.junit.jupiter.api.AfterEach;
@@ -437,5 +438,4 @@ protected HoodieWriteConfig getMetadataWriteConfig(HoodieWriteConfig writeConfig
     }
     return builder.build();
   }
-
 }

File: hudi-common/src/main/java/org/apache/hudi/metadata/FileSystemBackedTableMetadata.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.metadata;
 
 import org.apache.hudi.avro.model.HoodieMetadataColumnStats;
+import org.apache.hudi.common.bloom.BloomFilter;
 import org.apache.hudi.common.config.SerializableConfiguration;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.fs.FSUtils;
@@ -33,7 +34,6 @@
 import org.apache.hudi.exception.HoodieMetadataException;
 
 import java.io.IOException;
-import java.nio.ByteBuffer;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
@@ -143,13 +143,13 @@ public void reset() {
     // no-op
   }
 
-  public Option<ByteBuffer> getBloomFilter(final String partitionName, final String fileName)
+  public Option<BloomFilter> getBloomFilter(final String partitionName, final String fileName)
       throws HoodieMetadataException {
     throw new HoodieMetadataException("Unsupported operation: getBloomFilter for " + fileName);
   }
 
   @Override
-  public Map<Pair<String, String>, ByteBuffer> getBloomFilters(final List<Pair<String, String>> partitionNameFileNameList)
+  public Map<Pair<String, String>, BloomFilter> getBloomFilters(final List<Pair<String, String>> partitionNameFileNameList)
       throws HoodieMetadataException {
     throw new HoodieMetadataException("Unsupported operation: getBloomFilters!");
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java
Patch:
@@ -709,7 +709,7 @@ public List<BloomFilterData> getSortedBloomFilterList(
             .map(entry -> BloomFilterData.builder()
                 .setPartitionPath(entry.getKey().getKey())
                 .setFilename(entry.getKey().getValue())
-                .setBloomFilter(entry.getValue())
+                .setBloomFilter(ByteBuffer.wrap(entry.getValue().serializeToString().getBytes()))
                 .build())
             .sorted()
             .collect(Collectors.toList());

File: hudi-flink/src/test/java/org/apache/hudi/table/TestHoodieTableFactory.java
Patch:
@@ -83,7 +83,6 @@ void beforeEach() throws IOException {
     this.conf = new Configuration();
     this.conf.setString(FlinkOptions.PATH, tempFile.getAbsolutePath());
     this.conf.setString(FlinkOptions.TABLE_NAME, "t1");
-    this.conf.set(FlinkOptions.COMPACTION_MAX_MEMORY, 1024);
     StreamerUtil.initTableIfNotExists(this.conf);
   }
 

File: hudi-flink/src/test/java/org/apache/hudi/table/TestHoodieTableSource.java
Patch:
@@ -64,7 +64,6 @@ public class TestHoodieTableSource {
   void beforeEach() throws Exception {
     final String path = tempFile.getAbsolutePath();
     conf = TestConfigurations.getDefaultConf(path);
-    conf.set(FlinkOptions.COMPACTION_MAX_MEMORY, 1024);
     TestData.writeData(TestData.DATA_SET_INSERT, conf);
   }
 
@@ -123,7 +122,6 @@ void testGetTableAvroSchema() {
     final String path = tempFile.getAbsolutePath();
     conf = TestConfigurations.getDefaultConf(path);
     conf.setBoolean(FlinkOptions.READ_AS_STREAMING, true);
-    conf.set(FlinkOptions.COMPACTION_MAX_MEMORY, 1024);
 
     HoodieTableSource tableSource = new HoodieTableSource(
         TestConfigurations.TABLE_SCHEMA,

File: hudi-flink/src/test/java/org/apache/hudi/table/format/TestInputFormat.java
Patch:
@@ -73,7 +73,6 @@ void beforeEach(HoodieTableType tableType, Map<String, String> options) throws I
     conf = TestConfigurations.getDefaultConf(tempFile.getAbsolutePath());
     conf.setString(FlinkOptions.TABLE_TYPE, tableType.name());
     conf.setBoolean(FlinkOptions.COMPACTION_ASYNC_ENABLED, false); // close the async compaction
-    conf.set(FlinkOptions.COMPACTION_MAX_MEMORY, 1024);
     options.forEach((key, value) -> conf.setString(key, value));
 
     StreamerUtil.initTableIfNotExists(conf);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -417,6 +417,8 @@ public List<WriteStatus> close() {
         writer = null;
 
         // update final size, once for all log files
+        // TODO we can actually deduce file size purely from AppendResult (based on offset and size
+        //      of the appended block)
         for (WriteStatus status: statuses) {
           long logFileSize = FSUtils.getFileSize(fs, new Path(config.getBasePath(), status.getStat().getPath()));
           status.getStat().setFileSizeInBytes(logFileSize);

File: hudi-common/src/main/java/org/apache/hudi/common/util/CollectionUtils.java
Patch:
@@ -77,8 +77,8 @@ public static <E> List<E> combine(List<E> one, List<E> another) {
    * NOTE: That values associated with overlapping keys from the second map, will override
    *       values from the first one
    */
-  public static <K, V> Map<K, V> combine(Map<K, V> one, Map<K, V> another) {
-    Map<K, V> combined = new HashMap<>(one.size() + another.size());
+  public static <K, V> HashMap<K, V> combine(Map<K, V> one, Map<K, V> another) {
+    HashMap<K, V> combined = new HashMap<>(one.size() + another.size());
     combined.putAll(one);
     combined.putAll(another);
     return combined;

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/FileCreateUtils.java
Patch:
@@ -285,7 +285,7 @@ public static void createPartitionMetaFile(String basePath, String partitionPath
 
   public static void createBaseFile(String basePath, String partitionPath, String instantTime, String fileId)
       throws Exception {
-    createBaseFile(basePath, partitionPath, instantTime, fileId, 0);
+    createBaseFile(basePath, partitionPath, instantTime, fileId, 1);
   }
 
   public static void createBaseFile(String basePath, String partitionPath, String instantTime, String fileId, long length)

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestTable.java
Patch:
@@ -1057,6 +1057,7 @@ public static List<HoodieWriteStat> generateHoodieWriteStatForPartition(Map<Stri
         writeStat.setPartitionPath(partition);
         writeStat.setPath(partition + "/" + fileName);
         writeStat.setTotalWriteBytes(fileIdInfo.getValue());
+        writeStat.setFileSizeInBytes(fileIdInfo.getValue());
         writeStats.add(writeStat);
       }
     }
@@ -1082,6 +1083,7 @@ private static List<HoodieWriteStat> generateHoodieWriteStatForPartitionLogFiles
         writeStat.setPartitionPath(partition);
         writeStat.setPath(partition + "/" + fileName);
         writeStat.setTotalWriteBytes(fileIdInfo.getValue()[1]);
+        writeStat.setFileSizeInBytes(fileIdInfo.getValue()[1]);
         writeStats.add(writeStat);
       }
     }

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestBootstrap.java
Patch:
@@ -20,7 +20,6 @@
 
 import org.apache.hudi.DataSourceWriteOptions;
 import org.apache.hudi.avro.model.HoodieFileStatus;
-import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.bootstrap.BootstrapMode;
 import org.apache.hudi.client.bootstrap.FullRecordBootstrapDataProvider;
 import org.apache.hudi.client.bootstrap.selector.BootstrapModeSelector;
@@ -253,7 +252,8 @@ private void testBootstrapCommon(boolean partitioned, boolean deltaCommit, Effec
             .withBootstrapParallelism(3)
             .withBootstrapModeSelector(bootstrapModeSelectorClass).build())
         .build();
-    SparkRDDWriteClient client = new SparkRDDWriteClient(context, config);
+
+    SparkRDDWriteClientOverride client = new SparkRDDWriteClientOverride(context, config);
     client.bootstrap(Option.empty());
     checkBootstrapResults(totalRecords, schema, bootstrapCommitInstantTs, checkNumRawFiles, numInstantsAfterBootstrap,
         numInstantsAfterBootstrap, timestamp, timestamp, deltaCommit, bootstrapInstants, true);
@@ -272,7 +272,7 @@ private void testBootstrapCommon(boolean partitioned, boolean deltaCommit, Effec
     assertFalse(index.useIndex());
 
     // Run bootstrap again
-    client = new SparkRDDWriteClient(context, config);
+    client = new SparkRDDWriteClientOverride(context, config);
     client.bootstrap(Option.empty());
 
     metaClient.reloadActiveTimeline();

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestOrcBootstrap.java
Patch:
@@ -20,7 +20,6 @@
 
 import org.apache.hudi.DataSourceWriteOptions;
 import org.apache.hudi.avro.model.HoodieFileStatus;
-import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.bootstrap.BootstrapMode;
 import org.apache.hudi.client.bootstrap.FullRecordBootstrapDataProvider;
 import org.apache.hudi.client.bootstrap.selector.BootstrapModeSelector;
@@ -245,7 +244,8 @@ private void testBootstrapCommon(boolean partitioned, boolean deltaCommit, Effec
             .withBootstrapParallelism(3)
             .withBootstrapModeSelector(bootstrapModeSelectorClass).build())
         .build();
-    SparkRDDWriteClient client = new SparkRDDWriteClient(context, config);
+
+    SparkRDDWriteClientOverride client = new SparkRDDWriteClientOverride(context, config);
     client.bootstrap(Option.empty());
     checkBootstrapResults(totalRecords, schema, bootstrapCommitInstantTs, checkNumRawFiles, numInstantsAfterBootstrap,
         numInstantsAfterBootstrap, timestamp, timestamp, deltaCommit, bootstrapInstants, true);
@@ -266,7 +266,7 @@ private void testBootstrapCommon(boolean partitioned, boolean deltaCommit, Effec
     assertFalse(index.useIndex());
 
     // Run bootstrap again
-    client = new SparkRDDWriteClient(context, config);
+    client = new SparkRDDWriteClientOverride(context, config);
     client.bootstrap(Option.empty());
 
     metaClient.reloadActiveTimeline();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
Patch:
@@ -250,7 +250,7 @@ public class HoodieCompactionConfig extends HoodieConfig {
 
   public static final ConfigProperty<Boolean> PRESERVE_COMMIT_METADATA = ConfigProperty
       .key("hoodie.compaction.preserve.commit.metadata")
-      .defaultValue(false)
+      .defaultValue(true)
       .sinceVersion("0.11.0")
       .withDocumentation("When rewriting data, preserves existing hoodie_commit_time");
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -1072,8 +1072,7 @@ public EngineType getEngineType() {
   }
 
   public boolean populateMetaFields() {
-    return Boolean.parseBoolean(getStringOrDefault(HoodieTableConfig.POPULATE_META_FIELDS,
-        HoodieTableConfig.POPULATE_META_FIELDS.defaultValue()));
+    return getBooleanOrDefault(HoodieTableConfig.POPULATE_META_FIELDS);
   }
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecord.java
Patch:
@@ -42,6 +42,8 @@ public abstract class HoodieRecord<T> implements Serializable {
   public static final String OPERATION_METADATA_FIELD = "_hoodie_operation";
   public static final String HOODIE_IS_DELETED = "_hoodie_is_deleted";
 
+  public static int FILENAME_METADATA_FIELD_POS = 4;
+
   public static final List<String> HOODIE_META_COLUMNS =
       CollectionUtils.createImmutableList(COMMIT_TIME_METADATA_FIELD, COMMIT_SEQNO_METADATA_FIELD,
           RECORD_KEY_METADATA_FIELD, PARTITION_PATH_METADATA_FIELD, FILENAME_METADATA_FIELD);

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -172,9 +172,9 @@ public class HoodieTableConfig extends HoodieConfig {
       .noDefaultValue()
       .withDocumentation("Base path of the dataset that needs to be bootstrapped as a Hudi table");
 
-  public static final ConfigProperty<String> POPULATE_META_FIELDS = ConfigProperty
+  public static final ConfigProperty<Boolean> POPULATE_META_FIELDS = ConfigProperty
       .key("hoodie.populate.meta.fields")
-      .defaultValue("true")
+      .defaultValue(true)
       .withDocumentation("When enabled, populates all meta fields. When disabled, no meta fields are populated "
           + "and incremental queries will not be functional. This is only meant to be used for append only/immutable data for batch processing");
 

File: hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java
Patch:
@@ -364,6 +364,7 @@ public void stream(Dataset<Row> streamingInput, String operationType, String che
         .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS.key(), "1")
         .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE().key(), "true")
         .option(DataSourceWriteOptions.ASYNC_CLUSTERING_ENABLE().key(), "true")
+        .option(HoodieCompactionConfig.PRESERVE_COMMIT_METADATA.key(), "false")
         .option(HoodieWriteConfig.TBL_NAME.key(), tableName).option("checkpointLocation", checkpointLocation)
         .outputMode(OutputMode.Append());
 

File: hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/DefaultSource.java
Patch:
@@ -66,7 +66,7 @@ public Optional<DataSourceWriter> createWriter(String writeUUID, StructType sche
     String path = options.get("path").get();
     String tblName = options.get(HoodieWriteConfig.TBL_NAME.key()).get();
     boolean populateMetaFields = options.getBoolean(HoodieTableConfig.POPULATE_META_FIELDS.key(),
-        Boolean.parseBoolean(HoodieTableConfig.POPULATE_META_FIELDS.defaultValue()));
+        HoodieTableConfig.POPULATE_META_FIELDS.defaultValue());
     Map<String, String> properties = options.asMap();
     // Auto set the value of "hoodie.parquet.writelegacyformat.enabled"
     mayBeOverwriteParquetWriteLegacyFormatProp(properties, schema);

File: hudi-spark-datasource/hudi-spark3-common/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java
Patch:
@@ -53,7 +53,7 @@ public Table getTable(StructType schema, Transform[] partitioning, Map<String, S
     String path = properties.get("path");
     String tblName = properties.get(HoodieWriteConfig.TBL_NAME.key());
     boolean populateMetaFields = Boolean.parseBoolean(properties.getOrDefault(HoodieTableConfig.POPULATE_META_FIELDS.key(),
-        HoodieTableConfig.POPULATE_META_FIELDS.defaultValue()));
+        Boolean.toString(HoodieTableConfig.POPULATE_META_FIELDS.defaultValue())));
     boolean arePartitionRecordsSorted = Boolean.parseBoolean(properties.getOrDefault(HoodieInternalConfig.BULKINSERT_ARE_PARTITIONER_RECORDS_SORTED,
         Boolean.toString(HoodieInternalConfig.DEFAULT_BULKINSERT_ARE_PARTITIONER_RECORDS_SORTED)));
     // Create a new map as the properties is an unmodifiableMap on Spark 3.2.0

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -276,7 +276,7 @@ public void refreshTimeline() throws IOException {
           .setPartitionFields(partitionColumns)
           .setRecordKeyFields(props.getProperty(DataSourceWriteOptions.RECORDKEY_FIELD().key()))
           .setPopulateMetaFields(props.getBoolean(HoodieTableConfig.POPULATE_META_FIELDS.key(),
-              Boolean.parseBoolean(HoodieTableConfig.POPULATE_META_FIELDS.defaultValue())))
+              HoodieTableConfig.POPULATE_META_FIELDS.defaultValue()))
           .setKeyGeneratorClassProp(props.getProperty(DataSourceWriteOptions.KEYGENERATOR_CLASS_NAME().key(),
               SimpleKeyGenerator.class.getName()))
           .setPreCombineField(cfg.sourceOrderingField)
@@ -370,7 +370,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource(
           .setPartitionFields(partitionColumns)
           .setRecordKeyFields(props.getProperty(DataSourceWriteOptions.RECORDKEY_FIELD().key()))
           .setPopulateMetaFields(props.getBoolean(HoodieTableConfig.POPULATE_META_FIELDS.key(),
-              Boolean.parseBoolean(HoodieTableConfig.POPULATE_META_FIELDS.defaultValue())))
+              HoodieTableConfig.POPULATE_META_FIELDS.defaultValue()))
           .setKeyGeneratorClassProp(props.getProperty(DataSourceWriteOptions.KEYGENERATOR_CLASS_NAME().key(),
               SimpleKeyGenerator.class.getName()))
           .initTable(new Configuration(jssc.hadoopConfiguration()), cfg.targetBasePath);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableInsertUpdateDelete.java
Patch:
@@ -264,7 +264,7 @@ public void testSimpleInsertUpdateAndDelete(boolean populateMetaFields) throws E
           .map(baseFile -> new Path(baseFile.getPath()).getParent().toString())
           .collect(Collectors.toList());
       List<GenericRecord> recordsRead =
-          HoodieMergeOnReadTestUtils.getRecordsUsingInputFormat(hadoopConf(), inputPaths, basePath(), new JobConf(hadoopConf()), true, false);
+          HoodieMergeOnReadTestUtils.getRecordsUsingInputFormat(hadoopConf(), inputPaths, basePath(), new JobConf(hadoopConf()), true, populateMetaFields);
       // Wrote 20 records and deleted 20 records, so remaining 20-20 = 0
       assertEquals(0, recordsRead.size(), "Must contain 0 records");
     }

File: hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java
Patch:
@@ -143,7 +143,7 @@ public Schema getTableAvroSchemaFromDataFile() {
    * @throws Exception
    */
   public Schema getTableAvroSchema() throws Exception {
-    return getTableAvroSchema(metaClient.getTableConfig().populateMetaFields());
+    return getTableAvroSchema(true);
   }
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstantTimeGenerator.java
Patch:
@@ -69,7 +69,7 @@ public static String createNewInstantTime(long milliseconds) {
     return lastInstantTime.updateAndGet((oldVal) -> {
       String newCommitTime;
       do {
-        if (commitTimeZone.equals(HoodieTimelineTimeZone.UTC.toString())) {
+        if (commitTimeZone.equals(HoodieTimelineTimeZone.UTC)) {
           LocalDateTime now = LocalDateTime.now(ZoneOffset.UTC);
           newCommitTime = now.format(MILLIS_INSTANT_TIME_FORMATTER);
         } else {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndex.java
Patch:
@@ -141,6 +141,6 @@ public void close() {
   }
 
   public enum IndexType {
-    HBASE, INMEMORY, BLOOM, GLOBAL_BLOOM, SIMPLE, GLOBAL_SIMPLE, BUCKET
+    HBASE, INMEMORY, BLOOM, GLOBAL_BLOOM, SIMPLE, GLOBAL_SIMPLE, BUCKET, FLINK_STATE
   }
 }

File: hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java
Patch:
@@ -371,7 +371,7 @@ private String getBucketID(HoodieRecord<?> record) {
    *
    * @param value HoodieRecord
    */
-  private void bufferRecord(HoodieRecord<?> value) {
+  protected void bufferRecord(HoodieRecord<?> value) {
     final String bucketID = getBucketID(value);
 
     DataBucket bucket = this.buckets.computeIfAbsent(bucketID,

File: hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSink.java
Patch:
@@ -84,7 +84,6 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
       // default parallelism
       int parallelism = dataStream.getExecutionConfig().getParallelism();
       DataStream<Object> pipeline;
-
       // bootstrap
       final DataStream<HoodieRecord> hoodieRecordDataStream =
           Pipelines.bootstrap(conf, rowType, parallelism, dataStream, context.isBounded(), overwrite);

File: hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieCatalog.java
Patch:
@@ -376,7 +376,7 @@ public List<CatalogPartitionSpec> listPartitionsByFilter(ObjectPath tablePath, L
   @Override
   public CatalogPartition getPartition(ObjectPath tablePath, CatalogPartitionSpec catalogPartitionSpec)
       throws PartitionNotExistException, CatalogException {
-    return null;
+    throw new PartitionNotExistException(getName(), tablePath, catalogPartitionSpec);
   }
 
   @Override
@@ -409,7 +409,7 @@ public List<String> listFunctions(String databaseName) throws DatabaseNotExistEx
 
   @Override
   public CatalogFunction getFunction(ObjectPath functionPath) throws FunctionNotExistException, CatalogException {
-    return null;
+    throw new FunctionNotExistException(getName(), functionPath);
   }
 
   @Override

File: hudi-flink/src/test/java/org/apache/hudi/utils/factory/ContinuousFileSourceFactory.java
Patch:
@@ -53,7 +53,7 @@ public DynamicTableSource createDynamicTableSource(Context context) {
     Configuration conf = (Configuration) helper.getOptions();
     Path path = new Path(conf.getOptional(FlinkOptions.PATH).orElseThrow(() ->
         new ValidationException("Option [path] should be not empty.")));
-    return new ContinuousFileSource(context.getCatalogTable().getSchema(), path, conf);
+    return new ContinuousFileSource(context.getCatalogTable().getResolvedSchema(), path, conf);
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecord.java
Patch:
@@ -40,6 +40,7 @@ public abstract class HoodieRecord<T> implements Serializable {
   public static final String PARTITION_PATH_METADATA_FIELD = "_hoodie_partition_path";
   public static final String FILENAME_METADATA_FIELD = "_hoodie_file_name";
   public static final String OPERATION_METADATA_FIELD = "_hoodie_operation";
+  public static final String HOODIE_IS_DELETED = "_hoodie_is_deleted";
 
   public static final List<String> HOODIE_META_COLUMNS =
       CollectionUtils.createImmutableList(COMMIT_TIME_METADATA_FIELD, COMMIT_SEQNO_METADATA_FIELD,

File: hudi-common/src/main/java/org/apache/hudi/common/model/OverwriteWithLatestAvroPayload.java
Patch:
@@ -85,7 +85,7 @@ public Option<IndexedRecord> getInsertValue(Schema schema) throws IOException {
    * @returns {@code true} if record represents a delete record. {@code false} otherwise.
    */
   protected boolean isDeleteRecord(GenericRecord genericRecord) {
-    final String isDeleteKey = "_hoodie_is_deleted";
+    final String isDeleteKey = HoodieRecord.HOODIE_IS_DELETED;
     // Modify to be compatible with new version Avro.
     // The new version Avro throws for GenericRecord.get if the field name
     // does not exist in the schema.

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/AvroKafkaSource.java
Patch:
@@ -65,12 +65,12 @@ public AvroKafkaSource(TypedProperties props, JavaSparkContext sparkContext, Spa
       SchemaProvider schemaProvider, HoodieDeltaStreamerMetrics metrics) {
     super(props, sparkContext, sparkSession, schemaProvider);
 
-    props.put(NATIVE_KAFKA_KEY_DESERIALIZER_PROP, StringDeserializer.class);
+    props.put(NATIVE_KAFKA_KEY_DESERIALIZER_PROP, StringDeserializer.class.getName());
     deserializerClassName = props.getString(DataSourceWriteOptions.KAFKA_AVRO_VALUE_DESERIALIZER_CLASS().key(),
             DataSourceWriteOptions.KAFKA_AVRO_VALUE_DESERIALIZER_CLASS().defaultValue());
 
     try {
-      props.put(NATIVE_KAFKA_VALUE_DESERIALIZER_PROP, Class.forName(deserializerClassName));
+      props.put(NATIVE_KAFKA_VALUE_DESERIALIZER_PROP, Class.forName(deserializerClassName).getName());
       if (deserializerClassName.equals(KafkaAvroSchemaDeserializer.class.getName())) {
         if (schemaProvider == null) {
           throw new HoodieIOException("SchemaProvider has to be set to use KafkaAvroSchemaDeserializer");

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JsonKafkaSource.java
Patch:
@@ -52,8 +52,8 @@ public JsonKafkaSource(TypedProperties properties, JavaSparkContext sparkContext
                          SchemaProvider schemaProvider, HoodieDeltaStreamerMetrics metrics) {
     super(properties, sparkContext, sparkSession, schemaProvider);
     this.metrics = metrics;
-    properties.put("key.deserializer", StringDeserializer.class);
-    properties.put("value.deserializer", StringDeserializer.class);
+    properties.put("key.deserializer", StringDeserializer.class.getName());
+    properties.put("value.deserializer", StringDeserializer.class.getName());
     offsetGen = new KafkaOffsetGen(properties);
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/debezium/DebeziumSource.java
Patch:
@@ -82,12 +82,12 @@ public DebeziumSource(TypedProperties props, JavaSparkContext sparkContext,
                         HoodieDeltaStreamerMetrics metrics) {
     super(props, sparkContext, sparkSession, schemaProvider);
 
-    props.put(NATIVE_KAFKA_KEY_DESERIALIZER_PROP, StringDeserializer.class);
+    props.put(NATIVE_KAFKA_KEY_DESERIALIZER_PROP, StringDeserializer.class.getName());
     deserializerClassName = props.getString(DataSourceWriteOptions.KAFKA_AVRO_VALUE_DESERIALIZER_CLASS().key(),
         DataSourceWriteOptions.KAFKA_AVRO_VALUE_DESERIALIZER_CLASS().defaultValue());
 
     try {
-      props.put(NATIVE_KAFKA_VALUE_DESERIALIZER_PROP, Class.forName(deserializerClassName));
+      props.put(NATIVE_KAFKA_VALUE_DESERIALIZER_PROP, Class.forName(deserializerClassName).getName());
     } catch (ClassNotFoundException e) {
       String error = "Could not load custom avro kafka deserializer: " + deserializerClassName;
       LOG.error(error);

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/functional/TestHoodieCombineHiveInputFormat.java
Patch:
@@ -104,7 +104,7 @@ public void multiPartitionReadersRealtimeCombineHoodieInputFormat() throws Excep
     final int numRecords = 1000;
     // Create 3 partitions, each partition holds one parquet file and 1000 records
     List<File> partitionDirs = InputFormatTestUtil
-        .prepareMultiPartitionedParquetTable(tempDir, schema, 3, numRecords, commitTime);
+        .prepareMultiPartitionedParquetTable(tempDir, schema, 3, numRecords, commitTime, HoodieTableType.MERGE_ON_READ);
     InputFormatTestUtil.commit(tempDir, commitTime);
 
     TableDesc tblDesc = Utilities.defaultTd;

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/testutils/InputFormatTestUtil.java
Patch:
@@ -246,9 +246,9 @@ public static File prepareNonPartitionedParquetTable(java.nio.file.Path basePath
   }
 
   public static List<File> prepareMultiPartitionedParquetTable(java.nio.file.Path basePath, Schema schema,
-      int numberPartitions, int numberOfRecordsPerPartition, String commitNumber) throws IOException {
+      int numberPartitions, int numberOfRecordsPerPartition, String commitNumber, HoodieTableType tableType) throws IOException {
     List<File> result = new ArrayList<>();
-    HoodieTestUtils.init(HoodieTestUtils.getDefaultHadoopConf(), basePath.toString());
+    HoodieTestUtils.init(HoodieTestUtils.getDefaultHadoopConf(), basePath.toString(), tableType, HoodieFileFormat.PARQUET);
     for (int i = 0; i < numberPartitions; i++) {
       java.nio.file.Path partitionPath = basePath.resolve(Paths.get(2016 + i + "", "05", "01"));
       setupPartition(basePath, partitionPath);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java
Patch:
@@ -106,8 +106,8 @@ public Pair<Option<Dataset<Row>>, String> fetchNextBatch(Option<String> lastCkpt
             sparkContext, srcPath, numInstantsPerFetch, beginInstant, missingCheckpointStrategy);
 
     if (queryTypeAndInstantEndpts.getValue().getKey().equals(queryTypeAndInstantEndpts.getValue().getValue())) {
-      LOG.warn("Already caught up. Begin Checkpoint was :" + queryTypeAndInstantEndpts.getKey());
-      return Pair.of(Option.empty(), queryTypeAndInstantEndpts.getKey());
+      LOG.warn("Already caught up. Begin Checkpoint was :" + queryTypeAndInstantEndpts.getValue().getKey());
+      return Pair.of(Option.empty(), queryTypeAndInstantEndpts.getValue().getKey());
     }
 
     Dataset<Row> source = null;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/BaseActionExecutor.java
Patch:
@@ -65,7 +65,7 @@ protected final void writeTableMetadata(HoodieCommitMetadata metadata, String ac
    * Writes clean metadata to table metadata.
    * @param metadata clean metadata of interest.
    */
-  protected final void writeTableMetadata(HoodieCleanMetadata metadata) {
+  protected final void writeTableMetadata(HoodieCleanMetadata metadata, String instantTime) {
     table.getMetadataWriter(instantTime).ifPresent(w -> w.update(metadata, instantTime));
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java
Patch:
@@ -126,8 +126,8 @@ public Pair<Option<Dataset<Row>>, String> fetchNextBatch(Option<String> lastCkpt
         numInstantsPerFetch, beginInstant, missingCheckpointStrategy);
 
     if (queryTypeAndInstantEndpts.getValue().getKey().equals(queryTypeAndInstantEndpts.getValue().getValue())) {
-      LOG.warn("Already caught up. Begin Checkpoint was :" + queryTypeAndInstantEndpts.getKey());
-      return Pair.of(Option.empty(), queryTypeAndInstantEndpts.getKey());
+      LOG.warn("Already caught up. Begin Checkpoint was :" + queryTypeAndInstantEndpts.getValue().getKey());
+      return Pair.of(Option.empty(), queryTypeAndInstantEndpts.getValue().getKey());
     }
 
     Dataset<Row> source = null;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -577,6 +577,8 @@ private void testUpsertsInternal(HoodieWriteConfig config,
         TimelineLayoutVersion.CURR_VERSION).build();
     client = getHoodieWriteClient(newConfig);
 
+    client.savepoint("004", "user1","comment1");
+
     client.restoreToInstant("004");
 
     assertFalse(metaClient.reloadActiveTimeline().getRollbackTimeline().lastInstant().isPresent());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseBulkInsertHelper.java
Patch:
@@ -34,15 +34,15 @@ public abstract class BaseBulkInsertHelper<T extends HoodieRecordPayload, I, K,
   public abstract HoodieWriteMetadata<O> bulkInsert(I inputRecords, String instantTime,
                                                     HoodieTable<T, I, K, O> table, HoodieWriteConfig config,
                                                     BaseCommitActionExecutor<T, I, K, O, R> executor, boolean performDedupe,
-                                                    Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner);
+                                                    Option<BulkInsertPartitioner<I>> userDefinedBulkInsertPartitioner);
 
   /**
    * Only write input records. Does not change timeline/index. Return information about new files created.
    */
   public abstract O bulkInsert(I inputRecords, String instantTime,
                                HoodieTable<T, I, K, O> table, HoodieWriteConfig config,
                                boolean performDedupe,
-                               Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner,
+                               Option<BulkInsertPartitioner<I>> userDefinedBulkInsertPartitioner,
                                boolean addMetadataFields,
                                int parallelism,
                                WriteHandleFactory writeHandleFactory);

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaExecutionStrategy.java
Patch:
@@ -121,7 +121,7 @@ public abstract List<WriteStatus> performClusteringWithRecordList(
    * @param schema         Schema of the data including metadata fields.
    * @return empty for now.
    */
-  protected Option<BulkInsertPartitioner<T>> getPartitioner(Map<String, String> strategyParams, Schema schema) {
+  protected Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> getPartitioner(Map<String, String> strategyParams, Schema schema) {
     if (strategyParams.containsKey(PLAN_STRATEGY_SORT_COLUMNS.key())) {
       return Option.of(new JavaCustomColumnsSortPartitioner(
           strategyParams.get(PLAN_STRATEGY_SORT_COLUMNS.key()).split(","),

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java
Patch:
@@ -65,7 +65,7 @@ public HoodieWriteMetadata<List<WriteStatus>> bulkInsert(final List<HoodieRecord
                                                            final HoodieWriteConfig config,
                                                            final BaseCommitActionExecutor<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>, R> executor,
                                                            final boolean performDedupe,
-                                                           final Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {
+                                                           final Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> userDefinedBulkInsertPartitioner) {
     HoodieWriteMetadata result = new HoodieWriteMetadata();
 
     // It's possible the transition to inflight could have already happened.
@@ -89,7 +89,7 @@ public List<WriteStatus> bulkInsert(List<HoodieRecord<T>> inputRecords,
                                       HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table,
                                       HoodieWriteConfig config,
                                       boolean performDedupe,
-                                      Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner,
+                                      Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> userDefinedBulkInsertPartitioner,
                                       boolean useWriterSchema,
                                       int parallelism,
                                       WriteHandleFactory writeHandleFactory) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java
Patch:
@@ -67,7 +67,7 @@ public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(final JavaRDD<Hoodie
                                                               final HoodieWriteConfig config,
                                                               final BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor,
                                                               final boolean performDedupe,
-                                                              final Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {
+                                                              final Option<BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>>> userDefinedBulkInsertPartitioner) {
     HoodieWriteMetadata result = new HoodieWriteMetadata();
 
     //transition bulk_insert state to inflight
@@ -88,7 +88,7 @@ public JavaRDD<WriteStatus> bulkInsert(JavaRDD<HoodieRecord<T>> inputRecords,
                                          HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,
                                          HoodieWriteConfig config,
                                          boolean performDedupe,
-                                         Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner,
+                                         Option<BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>>> userDefinedBulkInsertPartitioner,
                                          boolean useWriterSchema,
                                          int parallelism,
                                          WriteHandleFactory writeHandleFactory) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkBulkInsertDeltaCommitActionExecutor.java
Patch:
@@ -39,17 +39,17 @@ public class SparkBulkInsertDeltaCommitActionExecutor<T extends HoodieRecordPayl
     extends BaseSparkDeltaCommitActionExecutor<T> {
 
   private final JavaRDD<HoodieRecord<T>> inputRecordsRDD;
-  private final Option<BulkInsertPartitioner<T>> bulkInsertPartitioner;
+  private final Option<BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>>> bulkInsertPartitioner;
 
   public SparkBulkInsertDeltaCommitActionExecutor(HoodieSparkEngineContext context, HoodieWriteConfig config, HoodieTable table,
                                                   String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD,
-                                                  Option<BulkInsertPartitioner<T>> bulkInsertPartitioner)  {
+                                                  Option<BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>>> bulkInsertPartitioner)  {
     this(context, config, table, instantTime, inputRecordsRDD, bulkInsertPartitioner, Option.empty());
   }
 
   public SparkBulkInsertDeltaCommitActionExecutor(HoodieSparkEngineContext context, HoodieWriteConfig config, HoodieTable table,
                                                   String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD,
-                                                  Option<BulkInsertPartitioner<T>> bulkInsertPartitioner,
+                                                  Option<BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>>> bulkInsertPartitioner,
                                                   Option<Map<String, String>> extraMetadata) {
     super(context, config, table, instantTime, WriteOperationType.BULK_INSERT, extraMetadata);
     this.inputRecordsRDD = inputRecordsRDD;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkBulkInsertPreppedDeltaCommitActionExecutor.java
Patch:
@@ -37,12 +37,12 @@ public class SparkBulkInsertPreppedDeltaCommitActionExecutor<T extends HoodieRec
     extends BaseSparkDeltaCommitActionExecutor<T> {
 
   private final JavaRDD<HoodieRecord<T>> preppedInputRecordRdd;
-  private final Option<BulkInsertPartitioner<T>> bulkInsertPartitioner;
+  private final Option<BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>>> bulkInsertPartitioner;
 
   public SparkBulkInsertPreppedDeltaCommitActionExecutor(HoodieSparkEngineContext context,
                                                          HoodieWriteConfig config, HoodieTable table,
                                                          String instantTime, JavaRDD<HoodieRecord<T>> preppedInputRecordRdd,
-                                                         Option<BulkInsertPartitioner<T>> bulkInsertPartitioner) {
+                                                         Option<BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>>> bulkInsertPartitioner) {
     super(context, config, table, instantTime, WriteOperationType.BULK_INSERT);
     this.preppedInputRecordRdd = preppedInputRecordRdd;
     this.bulkInsertPartitioner = bulkInsertPartitioner;

File: hudi-flink/src/main/java/org/apache/hudi/table/format/cow/ParquetColumnarRowSplitReader.java
Patch:
@@ -18,11 +18,10 @@
 
 package org.apache.hudi.table.format.cow;
 
-import org.apache.hudi.table.format.cow.data.ColumnarRowData;
-import org.apache.hudi.table.format.cow.vector.VectorizedColumnBatch;
-
 import org.apache.flink.formats.parquet.vector.reader.ColumnReader;
+import org.apache.flink.table.data.ColumnarRowData;
 import org.apache.flink.table.data.vector.ColumnVector;
+import org.apache.flink.table.data.vector.VectorizedColumnBatch;
 import org.apache.flink.table.data.vector.writable.WritableColumnVector;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.LogicalTypeRoot;

File: hudi-flink/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.hudi.table.format.cow.vector.HeapArrayVector;
 import org.apache.hudi.table.format.cow.vector.HeapMapColumnVector;
 import org.apache.hudi.table.format.cow.vector.HeapRowColumnVector;
-import org.apache.hudi.table.format.cow.vector.VectorizedColumnBatch;
 import org.apache.hudi.table.format.cow.vector.reader.ArrayColumnReader;
 import org.apache.hudi.table.format.cow.vector.reader.MapColumnReader;
 import org.apache.hudi.table.format.cow.vector.reader.RowColumnReader;
@@ -41,6 +40,7 @@
 import org.apache.flink.table.data.DecimalData;
 import org.apache.flink.table.data.TimestampData;
 import org.apache.flink.table.data.vector.ColumnVector;
+import org.apache.flink.table.data.vector.VectorizedColumnBatch;
 import org.apache.flink.table.data.vector.heap.HeapBooleanVector;
 import org.apache.flink.table.data.vector.heap.HeapByteVector;
 import org.apache.flink.table.data.vector.heap.HeapBytesVector;

File: hudi-flink/src/main/java/org/apache/hudi/table/format/cow/vector/HeapArrayVector.java
Patch:
@@ -18,9 +18,8 @@
 
 package org.apache.hudi.table.format.cow.vector;
 
-import org.apache.hudi.table.format.cow.data.ColumnarArrayData;
-
 import org.apache.flink.table.data.ArrayData;
+import org.apache.flink.table.data.ColumnarArrayData;
 import org.apache.flink.table.data.vector.ArrayColumnVector;
 import org.apache.flink.table.data.vector.ColumnVector;
 import org.apache.flink.table.data.vector.heap.AbstractHeapVector;

File: hudi-flink/src/main/java/org/apache/hudi/table/format/cow/vector/HeapMapColumnVector.java
Patch:
@@ -18,10 +18,10 @@
 
 package org.apache.hudi.table.format.cow.vector;
 
-import org.apache.hudi.table.format.cow.data.ColumnarMapData;
-
+import org.apache.flink.table.data.ColumnarMapData;
 import org.apache.flink.table.data.MapData;
 import org.apache.flink.table.data.vector.ColumnVector;
+import org.apache.flink.table.data.vector.MapColumnVector;
 import org.apache.flink.table.data.vector.heap.AbstractHeapVector;
 import org.apache.flink.table.data.vector.writable.WritableColumnVector;
 

File: hudi-flink/src/main/java/org/apache/hudi/table/format/cow/vector/HeapRowColumnVector.java
Patch:
@@ -18,8 +18,9 @@
 
 package org.apache.hudi.table.format.cow.vector;
 
-import org.apache.hudi.table.format.cow.data.ColumnarRowData;
-
+import org.apache.flink.table.data.ColumnarRowData;
+import org.apache.flink.table.data.vector.RowColumnVector;
+import org.apache.flink.table.data.vector.VectorizedColumnBatch;
 import org.apache.flink.table.data.vector.heap.AbstractHeapVector;
 import org.apache.flink.table.data.vector.writable.WritableColumnVector;
 

File: hudi-flink/src/test/java/org/apache/hudi/utils/TestSQL.java
Patch:
@@ -58,7 +58,7 @@ private TestSQL() {
       + "(3, array['abc3', 'def3'], map['abc3', 1, 'def3', 3], row(3, 'abc3'))";
 
   public static final String COMPLEX_NESTED_ROW_TYPE_INSERT_T1 = "insert into t1 values\n"
-      + "(1, array['abc1', 'def1'], map['abc1', 1, 'def1', 3], row(array['abc1', 'def1'], row(1, 'abc1'))),\n"
-      + "(2, array['abc2', 'def2'], map['abc2', 1, 'def2', 3], row(array['abc2', 'def2'], row(2, 'abc2'))),\n"
-      + "(3, array['abc3', 'def3'], map['abc3', 1, 'def3', 3], row(array['abc3', 'def3'], row(3, 'abc3')))";
+      + "(1, array['abc1', 'def1'], array[1, 1], map['abc1', 1, 'def1', 3], row(array['abc1', 'def1'], row(1, 'abc1'))),\n"
+      + "(2, array['abc2', 'def2'], array[2, 2], map['abc2', 1, 'def2', 3], row(array['abc2', 'def2'], row(2, 'abc2'))),\n"
+      + "(3, array['abc3', 'def3'], array[3, 3], map['abc3', 1, 'def3', 3], row(array['abc3', 'def3'], row(3, 'abc3')))";
 }

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java
Patch:
@@ -45,7 +45,6 @@
 import org.springframework.stereotype.Component;
 import scala.collection.JavaConverters;
 
-import java.io.File;
 import java.io.FileInputStream;
 import java.io.IOException;
 import java.util.List;
@@ -153,10 +152,12 @@ public String overwriteHoodieProperties(
 
     HoodieTableMetaClient client = HoodieCLI.getTableMetaClient();
     Properties newProps = new Properties();
-    newProps.load(new FileInputStream(new File(overwriteFilePath)));
+    newProps.load(new FileInputStream(overwriteFilePath));
     Map<String, String> oldProps = client.getTableConfig().propsMap();
     Path metaPathDir = new Path(client.getBasePath(), METAFOLDER_NAME);
     HoodieTableConfig.create(client.getFs(), metaPathDir, newProps);
+    // reload new props as checksum would have been added
+    newProps = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient()).getTableConfig().getProps();
 
     TreeSet<String> allPropKeys = new TreeSet<>();
     allPropKeys.addAll(newProps.keySet().stream().map(Object::toString).collect(Collectors.toSet()));

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -1548,7 +1548,7 @@ public void testUpgradeDowngrade() throws IOException {
     assertTrue(currentStatus.getModificationTime() > prevStatus.getModificationTime());
 
     initMetaClient();
-    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.THREE.versionCode());
+    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.FOUR.versionCode());
     assertTrue(fs.exists(new Path(metadataTableBasePath)), "Metadata table should exist");
     FileStatus newStatus = fs.getFileStatus(new Path(metadataTableBasePath));
     assertTrue(oldStatus.getModificationTime() < newStatus.getModificationTime());
@@ -1630,7 +1630,7 @@ public void testRollbackDuringUpgradeForDoubleLocking() throws IOException, Inte
     }
 
     initMetaClient();
-    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.THREE.versionCode());
+    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.FOUR.versionCode());
     assertTrue(fs.exists(new Path(metadataTableBasePath)), "Metadata table should exist");
     FileStatus newStatus = fs.getFileStatus(new Path(metadataTableBasePath));
     assertTrue(oldStatus.getModificationTime() < newStatus.getModificationTime());

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java
Patch:
@@ -56,6 +56,8 @@
 import java.util.zip.DeflaterOutputStream;
 import java.util.zip.InflaterInputStream;
 
+import static org.apache.hudi.common.util.BinaryUtil.generateChecksum;
+
 /**
  * This class provides a disk spillable only map implementation. All of the data is currenly written to one file,
  * without any rollover support. It uses the following : 1) An in-memory map that tracks the key-> latest ValueMetadata.
@@ -223,7 +225,7 @@ private synchronized R put(T key, R value, boolean flush) {
           new BitCaskDiskMap.ValueMetadata(this.filePath, valueSize, filePosition.get(), timestamp));
       byte[] serializedKey = SerializationUtils.serialize(key);
       filePosition
-          .set(SpillableMapUtils.spillToDisk(writeOnlyFileHandle, new FileEntry(SpillableMapUtils.generateChecksum(val),
+          .set(SpillableMapUtils.spillToDisk(writeOnlyFileHandle, new FileEntry(generateChecksum(val),
               serializedKey.length, valueSize, serializedKey, val, timestamp)));
       if (flush) {
         flushToDisk();

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestHoodieTableMetaClient.java
Patch:
@@ -54,6 +54,8 @@ public void checkMetadata() {
     assertEquals(basePath, metaClient.getBasePath(), "Basepath should be the one assigned");
     assertEquals(basePath + "/.hoodie", metaClient.getMetaPath(),
         "Metapath should be ${basepath}/.hoodie");
+    assertTrue(metaClient.getTableConfig().getProps().containsKey(HoodieTableConfig.TABLE_CHECKSUM.key()));
+    assertTrue(HoodieTableConfig.validateChecksum(metaClient.getTableConfig().getProps()));
   }
 
   @Test

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/parser/BaseHoodieDateTimeParser.java
Patch:
@@ -19,7 +19,7 @@
 
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.keygen.TimestampBasedAvroKeyGenerator.Config;
+import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 import org.joda.time.DateTimeZone;
 import org.joda.time.format.DateTimeFormatter;
 
@@ -36,7 +36,7 @@ public BaseHoodieDateTimeParser(TypedProperties config) {
   }
 
   private String initInputDateFormatDelimiter() {
-    String inputDateFormatDelimiter = config.getString(Config.TIMESTAMP_INPUT_DATE_FORMAT_LIST_DELIMITER_REGEX_PROP, ",").trim();
+    String inputDateFormatDelimiter = config.getString(KeyGeneratorOptions.Config.TIMESTAMP_INPUT_DATE_FORMAT_LIST_DELIMITER_REGEX_PROP, ",").trim();
     inputDateFormatDelimiter = inputDateFormatDelimiter.isEmpty() ? "," : inputDateFormatDelimiter;
     return inputDateFormatDelimiter;
   }
@@ -45,7 +45,7 @@ private String initInputDateFormatDelimiter() {
    * Returns the output date format in which the partition paths will be created for the hudi dataset.
    */
   public String getOutputDateFormat() {
-    return config.getString(Config.TIMESTAMP_OUTPUT_DATE_FORMAT_PROP);
+    return config.getString(KeyGeneratorOptions.Config.TIMESTAMP_OUTPUT_DATE_FORMAT_PROP);
   }
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -401,7 +401,7 @@ protected boolean isFileSliceAfterPendingCompaction(FileSlice fileSlice) {
    */
   protected FileSlice filterBaseFileAfterPendingCompaction(FileSlice fileSlice) {
     if (isFileSliceAfterPendingCompaction(fileSlice)) {
-      LOG.info("File Slice (" + fileSlice + ") is in pending compaction");
+      LOG.debug("File Slice (" + fileSlice + ") is in pending compaction");
       // Base file is filtered out of the file-slice as the corresponding compaction
       // instant not completed yet.
       FileSlice transformed =

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/SqlSource.java
Patch:
@@ -48,6 +48,8 @@
  * <p>To fetch and use the latest incremental checkpoint, you need to also set this hoodie_conf for deltastremer jobs:
  *
  * <p>hoodie.write.meta.key.prefixes = 'deltastreamer.checkpoint.key'
+ *
+ * Also, users are expected to set --allow-commit-on-no-checkpoint-change while using this SqlSource.
  */
 public class SqlSource extends RowSource {
   private static final long serialVersionUID = 1L;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/HoodieDeltaStreamerTestBase.java
Patch:
@@ -61,6 +61,7 @@ public class HoodieDeltaStreamerTestBase extends UtilitiesTestBase {
   static final String PROPS_FILENAME_TEST_PARQUET = "test-parquet-dfs-source.properties";
   static final String PROPS_FILENAME_TEST_ORC = "test-orc-dfs-source.properties";
   static final String PROPS_FILENAME_TEST_JSON_KAFKA = "test-json-kafka-dfs-source.properties";
+  static final String PROPS_FILENAME_TEST_SQL_SOURCE = "test-sql-source-source.properties";
   static final String PROPS_FILENAME_TEST_MULTI_WRITER = "test-multi-writer.properties";
   static final String FIRST_PARQUET_FILE_NAME = "1.parquet";
   static final String FIRST_ORC_FILE_NAME = "1.orc";
@@ -71,6 +72,7 @@ public class HoodieDeltaStreamerTestBase extends UtilitiesTestBase {
   static final int ORC_NUM_RECORDS = 5;
   static final int CSV_NUM_RECORDS = 3;
   static final int JSON_KAFKA_NUM_RECORDS = 5;
+  static final int SQL_SOURCE_NUM_RECORDS = 1000;
   String kafkaCheckpointType = "string";
   // Required fields
   static final String TGT_BASE_PATH_PARAM = "--target-base-path";

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestUpgradeDowngradeCommand.java
Patch:
@@ -106,7 +106,7 @@ public void testDowngradeCommand() throws Exception {
     assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.ZERO.versionCode());
     assertTableVersionFromPropertyFile();
 
-    // verify marker files are non existant
+    // verify marker files are non existent
     for (String partitionPath : DEFAULT_PARTITION_PATHS) {
       assertEquals(0, FileCreateUtils.getTotalMarkerFileCount(tablePath, partitionPath, "101", IOType.MERGE));
     }

File: hudi-cli/src/test/java/org/apache/hudi/cli/testutils/HoodieTestCommitMetadataGenerator.java
Patch:
@@ -73,9 +73,9 @@ public static void createCommitFileWithMetadata(String basePath, String commitTi
   }
 
   public static void createCommitFileWithMetadata(String basePath, String commitTime, Configuration configuration,
-      Option<Integer> writes, Option<Integer> updates, Map<String, String> extraMetdata) throws Exception {
+                                                  Option<Integer> writes, Option<Integer> updates, Map<String, String> extraMetadata) throws Exception {
     createCommitFileWithMetadata(basePath, commitTime, configuration, UUID.randomUUID().toString(),
-        UUID.randomUUID().toString(), writes, updates, extraMetdata);
+        UUID.randomUUID().toString(), writes, updates, extraMetadata);
   }
 
   public static void createCommitFileWithMetadata(String basePath, String commitTime, Configuration configuration,

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieReadClient.java
Patch:
@@ -65,7 +65,7 @@ public class HoodieReadClient<T extends HoodieRecordPayload<T>> implements Seria
 
   /**
    * TODO: We need to persist the index type into hoodie.properties and be able to access the index just with a simple
-   * basepath pointing to the table. Until, then just always assume a BloomIndex
+   * base path pointing to the table. Until, then just always assume a BloomIndex
    */
   private final transient HoodieIndex<?, ?> index;
   private HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> hoodieTable;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java
Patch:
@@ -504,7 +504,7 @@ private HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<Wri
   @Override
   protected void preCommit(HoodieInstant inflightInstant, HoodieCommitMetadata metadata) {
     // Create a Hoodie table after startTxn which encapsulated the commits and files visible.
-    // Important to create this after the lock to ensure latest commits show up in the timeline without need for reload
+    // Important to create this after the lock to ensure the latest commits show up in the timeline without need for reload
     HoodieTable table = createTable(config, hadoopConf);
     TransactionUtils.resolveWriteConflictIfAny(table, this.txnManager.getCurrentTransactionOwner(),
         Option.of(metadata), config, txnManager.getLastCompletedTransactionOwner());

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkSizeBasedClusteringPlanStrategy.java
Patch:
@@ -87,7 +87,7 @@ protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String
 
       // Add to the current file-group
       currentGroup.add(currentSlice);
-      // assume each filegroup size is ~= parquet.max.file.size
+      // assume each file group size is ~= parquet.max.file.size
       totalSizeSoFar += currentSlice.getBaseFile().isPresent() ? currentSlice.getBaseFile().get().getFileSize() : writeConfig.getParquetMaxFileSize();
     }
 
@@ -118,7 +118,7 @@ protected Map<String, String> getStrategyParams() {
   @Override
   protected Stream<FileSlice> getFileSlicesEligibleForClustering(final String partition) {
     return super.getFileSlicesEligibleForClustering(partition)
-        // Only files that have basefile size smaller than small file size are eligible.
+        // Only files that have base file size smaller than small file size are eligible.
         .filter(slice -> slice.getBaseFile().map(HoodieBaseFile::getFileSize).orElse(0L) < getWriteConfig().getClusteringSmallFileLimit());
   }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java
Patch:
@@ -37,7 +37,7 @@
 
 /**
  * Update strategy based on following.
- * if some file group have update record, throw exception
+ * if some file groups have update record, throw exception
  */
 public class SparkRejectUpdateStrategy<T extends HoodieRecordPayload<T>> extends UpdateStrategy<T, JavaRDD<HoodieRecord<T>>> {
   private static final Logger LOG = LogManager.getLogger(SparkRejectUpdateStrategy.class);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQuerySingleResultPreCommitValidator.java
Patch:
@@ -35,9 +35,9 @@
 import java.util.List;
 
 /**
- * Validator to run sql queries on new table state and expects a single result. If the result doesnt match expected result,
- * throw validation error. 
- * 
+ * Validator to run sql queries on new table state and expects a single result. If the result does not match expected result,
+ * throw validation error.
+ * <p>
  * Example configuration: "query1#expectedResult1;query2#expectedResult2;"
  */
 public class SqlQuerySingleResultPreCommitValidator<T extends HoodieRecordPayload, I, K, O extends JavaRDD<WriteStatus>> extends SqlQueryPreCommitValidator<T, I, K, O> {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowCreateHandle.java
Patch:
@@ -45,7 +45,7 @@
 import java.util.concurrent.atomic.AtomicLong;
 
 /**
- * Create handle with InternalRow for datasource implemention of bulk insert.
+ * Create handle with InternalRow for datasource implementation of bulk insert.
  */
 public class HoodieRowCreateHandle implements Serializable {
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/NonpartitionedKeyGenerator.java
Patch:
@@ -18,9 +18,10 @@
 
 package org.apache.hudi.keygen;
 
-import org.apache.avro.generic.GenericRecord;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
+
+import org.apache.avro.generic.GenericRecord;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.types.StructType;
@@ -31,7 +32,7 @@
 import java.util.stream.Collectors;
 
 /**
- * Simple Key generator for unpartitioned Hive Tables.
+ * Simple Key generator for non-partitioned Hive Tables.
  */
 public class NonpartitionedKeyGenerator extends BuiltinKeyGenerator {
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.metadata;
 
-import org.apache.avro.specific.SpecificRecordBase;
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
@@ -35,6 +34,7 @@
 import org.apache.hudi.exception.HoodieMetadataException;
 import org.apache.hudi.metrics.DistributedRegistry;
 
+import org.apache.avro.specific.SpecificRecordBase;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
@@ -51,8 +51,8 @@ public class SparkHoodieBackedTableMetadataWriter extends HoodieBackedTableMetad
   /**
    * Return a Spark based implementation of {@code HoodieTableMetadataWriter} which can be used to
    * write to the metadata table.
-   *
-   * If the metadata table does not exist, an attempt is made to bootstrap it but there is no guarantted that
+   * <p>
+   * If the metadata table does not exist, an attempt is made to bootstrap it but there is no guaranteed that
    * table will end up bootstrapping at this time.
    *
    * @param conf

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java
Patch:
@@ -113,9 +113,9 @@ public HoodieBootstrapWriteMetadata execute() {
     validate();
     try {
       HoodieTableMetaClient metaClient = table.getMetaClient();
-      Option<HoodieInstant> completetedInstant =
+      Option<HoodieInstant> completedInstant =
           metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();
-      ValidationUtils.checkArgument(!completetedInstant.isPresent(),
+      ValidationUtils.checkArgument(!completedInstant.isPresent(),
           "Active Timeline is expected to be empty for bootstrap to be performed. "
               + "If you want to re-bootstrap, please rollback bootstrap first !!");
       Map<BootstrapMode, List<Pair<String, List<HoodieFileStatus>>>> partitionSelections = listAndProcessSourcePartitions();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkExecuteClusteringCommitActionExecutor.java
Patch:
@@ -116,7 +116,7 @@ protected String getCommitActionType() {
   protected Map<String, List<String>> getPartitionToReplacedFileIds(HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata) {
     Set<HoodieFileGroupId> newFilesWritten = writeMetadata.getWriteStats().get().stream()
         .map(s -> new HoodieFileGroupId(s.getPartitionPath(), s.getFileId())).collect(Collectors.toSet());
-    // for the below execution strategy, new filegroup id would be same as old filegroup id
+    // for the below execution strategy, new file group id would be same as old file group id
     if (SparkSingleFileSortExecutionStrategy.class.getName().equals(config.getClusteringExecutionStrategyClass())) {
       return ClusteringUtils.getFileGroupsFromClusteringPlan(clusteringPlan)
           .collect(Collectors.groupingBy(fg -> fg.getPartitionPath(), Collectors.mapping(fg -> fg.getFileId(), Collectors.toList())));

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwritePartitioner.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.WorkloadProfile;
+
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
@@ -44,7 +45,7 @@ public SparkInsertOverwritePartitioner(WorkloadProfile profile, HoodieEngineCont
    * Returns a list of small files in the given partition path.
    */
   protected List<SmallFile> getSmallFiles(String partitionPath) {
-    // for overwrite, we ignore all existing files. So dont consider any file to be smallFiles
+    // for overwrite, we ignore all existing files. So do not consider any file to be smallFiles
     return Collections.emptyList();
   }
 }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestClientRollback.java
Patch:
@@ -171,7 +171,7 @@ public void testSavepointAndRollback() throws Exception {
   }
 
   /**
-   * Test Cases for effects of rollbacking completed/inflight commits.
+   * Test Cases for effects of rolling back completed/inflight commits.
    */
   @Test
   public void testRollbackCommit() throws Exception {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientMultiWriter.java
Patch:
@@ -584,7 +584,7 @@ private void createCommitWithInsertsForPartition(HoodieWriteConfig cfg, SparkRDD
   private void createCommitWithInserts(HoodieWriteConfig cfg, SparkRDDWriteClient client,
                                        String prevCommitTime, String newCommitTime, int numRecords,
                                        boolean doCommit) throws Exception {
-    // Finish first base commmit
+    // Finish first base commit
     JavaRDD<WriteStatus> result = insertFirstBatch(cfg, client, newCommitTime, prevCommitTime, numRecords, SparkRDDWriteClient::bulkInsert,
         false, false, numRecords);
     if (doCommit) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java
Patch:
@@ -147,7 +147,7 @@ public void testSchemaCompatibilityBasic() throws Exception {
         + TIP_NESTED_SCHEMA + EXTRA_FIELD_SCHEMA + EXTRA_FIELD_SCHEMA.replace("new_field", "new_new_field")
         + TRIP_SCHEMA_SUFFIX;
     assertTrue(TableSchemaResolver.isSchemaCompatible(TRIP_EXAMPLE_SCHEMA, multipleAddedFieldSchema),
-        "Multiple added fields with defauls are compatible");
+        "Multiple added fields with defaults are compatible");
 
     assertFalse(TableSchemaResolver.isSchemaCompatible(TRIP_EXAMPLE_SCHEMA,
         TRIP_SCHEMA_PREFIX + EXTRA_TYPE_SCHEMA + MAP_TYPE_SCHEMA
@@ -205,7 +205,7 @@ public void testMORTable() throws Exception {
     final List<HoodieRecord> failedRecords = generateInsertsWithSchema("004", numRecords, TRIP_EXAMPLE_SCHEMA_DEVOLVED);
     try {
       // We cannot use insertBatch directly here because we want to insert records
-      // with a devolved schema and insertBatch inserts records using the TRIP_EXMPLE_SCHEMA.
+      // with a devolved schema and insertBatch inserts records using the TRIP_EXAMPLE_SCHEMA.
       writeBatch(client, "005", "004", Option.empty(), "003", numRecords,
           (String s, Integer a) -> failedRecords, SparkRDDWriteClient::insert, false, 0, 0, 0, false);
       fail("Insert with devolved scheme should fail");
@@ -233,7 +233,7 @@ public void testMORTable() throws Exception {
     client = getHoodieWriteClient(hoodieEvolvedWriteConfig);
 
     // We cannot use insertBatch directly here because we want to insert records
-    // with a evolved schemaand insertBatch inserts records using the TRIP_EXMPLE_SCHEMA.
+    // with an evolved schema and insertBatch inserts records using the TRIP_EXAMPLE_SCHEMA.
     final List<HoodieRecord> evolvedRecords = generateInsertsWithSchema("005", numRecords, TRIP_EXAMPLE_SCHEMA_EVOLVED);
     writeBatch(client, "005", "004", Option.empty(), initCommitTime, numRecords,
         (String s, Integer a) -> evolvedRecords, SparkRDDWriteClient::insert, false, 0, 0, 0, false);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/bloom/TestKeyRangeLookupTree.java
Patch:
@@ -80,7 +80,7 @@ public void testFileGroupLookUpManyEntriesWithSameStartValue() {
    * Tests for many duplicate entries in the tree.
    */
   @Test
-  public void testFileGroupLookUpManyDulicateEntries() {
+  public void testFileGroupLookUpManyDuplicateEntries() {
     KeyRangeNode toInsert = new KeyRangeNode(Long.toString(1200), Long.toString(2000), UUID.randomUUID().toString());
     updateExpectedMatchesToTest(toInsert);
     keyRangeLookupTree.insert(toInsert);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/hbase/TestSparkHoodieHBaseIndex.java
Patch:
@@ -191,7 +191,7 @@ public void testTagLocationAndPartitionPathUpdate() throws Exception {
     final String newCommitTime = "001";
     final int numRecords = 10;
     final String oldPartitionPath = "1970/01/01";
-    final String emptyHoodieRecordPayloadClasssName = EmptyHoodieRecordPayload.class.getName();
+    final String emptyHoodieRecordPayloadClassName = EmptyHoodieRecordPayload.class.getName();
 
     List<HoodieRecord> newRecords = dataGen.generateInserts(newCommitTime, numRecords);
     List<HoodieRecord> oldRecords = new LinkedList();
@@ -226,7 +226,7 @@ public void testTagLocationAndPartitionPathUpdate() throws Exception {
       assertEquals(numRecords * 2L, taggedRecords.stream().count());
       // Verify the number of deleted records
       assertEquals(numRecords, taggedRecords.stream().filter(record -> record.getKey().getPartitionPath().equals(oldPartitionPath)
-          && record.getData().getClass().getName().equals(emptyHoodieRecordPayloadClasssName)).count());
+          && record.getData().getClass().getName().equals(emptyHoodieRecordPayloadClassName)).count());
       // Verify the number of inserted records
       assertEquals(numRecords, taggedRecords.stream().filter(record -> !record.getKey().getPartitionPath().equals(oldPartitionPath)).count());
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/TestHoodieTimelineArchiveLog.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.io;
 
-import org.apache.hadoop.fs.FileStatus;
 import org.apache.hudi.avro.model.HoodieRollbackMetadata;
 import org.apache.hudi.client.utils.MetadataConversionUtils;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
@@ -52,6 +51,7 @@
 import org.apache.hudi.testutils.HoodieClientTestHarness;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
@@ -655,7 +655,8 @@ public void testConvertCommitMetadata() throws Exception {
   public void testArchiveTableWithCleanCommits(boolean enableMetadata) throws Exception {
     HoodieWriteConfig writeConfig = initTestTableAndGetWriteConfig(enableMetadata, 2, 4, 2);
 
-    // min archival commits is 2 and max archival commits is 4(either clean commits has to be > 4 or commits has to be greater than 4.
+    // min archival commits is 2 and max archival commits is 4
+    // (either clean commits has to be > 4 or commits has to be greater than 4)
     // and so, after 5th commit, 3 commits will be archived.
     // 1,2,3,4,5,6 : after archival -> 1,5,6 (because, 2,3,4,5 and 6 are clean commits and are eligible for archival)
     // after 7th and 8th commit no-op wrt archival.

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/keygen/factory/TestHoodieSparkKeyGeneratorFactory.java
Patch:
@@ -25,8 +25,8 @@
 import org.apache.hudi.keygen.SimpleKeyGenerator;
 import org.apache.hudi.keygen.TestComplexKeyGenerator;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
-
 import org.apache.hudi.keygen.constant.KeyGeneratorType;
+
 import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.Test;
 
@@ -58,7 +58,7 @@ public void testKeyGeneratorFactory() throws IOException {
     // set both class name and keyGenerator type
     props.put(HoodieWriteConfig.KEYGENERATOR_TYPE.key(), KeyGeneratorType.CUSTOM.name());
     KeyGenerator keyGenerator3 = HoodieSparkKeyGeneratorFactory.createKeyGenerator(props);
-    // KEYGENERATOR_TYPE_PROP was overitten by KEYGENERATOR_CLASS_PROP
+    // KEYGENERATOR_TYPE_PROP was overwritten by KEYGENERATOR_CLASS_PROP
     Assertions.assertEquals(SimpleKeyGenerator.class.getName(), keyGenerator3.getClass().getName());
 
     // set wrong class name

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestConsistencyGuard.java
Patch:
@@ -169,9 +169,9 @@ private ConsistencyGuardConfig getConsistencyGuardConfig() {
     return getConsistencyGuardConfig(3, 10, 10);
   }
 
-  private ConsistencyGuardConfig getConsistencyGuardConfig(int maxChecks, int initalSleep, int maxSleep) {
+  private ConsistencyGuardConfig getConsistencyGuardConfig(int maxChecks, int initialSleep, int maxSleep) {
     return ConsistencyGuardConfig.newBuilder().withConsistencyCheckEnabled(true)
-        .withInitialConsistencyCheckIntervalMs(initalSleep).withMaxConsistencyCheckIntervalMs(maxSleep)
+        .withInitialConsistencyCheckIntervalMs(initialSleep).withMaxConsistencyCheckIntervalMs(maxSleep)
         .withMaxConsistencyChecks(maxChecks).build();
   }
 }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieSparkTable;
 import org.apache.hudi.table.marker.WriteMarkersFactory;
+
 import org.junit.jupiter.api.Test;
 
 import java.util.ArrayList;
@@ -62,7 +63,7 @@ public void testCompactionIsNotScheduledEarly() throws Exception {
       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());
       HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();
 
-      // Then: ensure no compaction is executedm since there are only 2 delta commits
+      // Then: ensure no compaction is executed since there are only 2 delta commits
       assertEquals(2, metaClient.getActiveTimeline().getWriteTimeline().countInstants());
     }
   }
@@ -152,7 +153,7 @@ public void testSuccessfulCompactionBasedOnNumAndTime() throws Exception {
       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());
       HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();
 
-      // Then: ensure no compaction is executedm since there are only 3 delta commits
+      // Then: ensure no compaction is executed since there are only 3 delta commits
       assertEquals(3, metaClient.getActiveTimeline().getWriteTimeline().countInstants());
       // 4th commit, that will trigger compaction
       metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/rollback/TestMergeOnReadRollbackActionExecutor.java
Patch:
@@ -112,7 +112,7 @@ public void testMergeOnReadRollbackActionExecutor(boolean isUsingMarkers) throws
       assertTrue(meta.getSuccessDeleteFiles() == null || meta.getSuccessDeleteFiles().size() == 0);
     }
 
-    //4. assert filegroup after rollback, and compare to the rollbackstat
+    //4. assert file group after rollback, and compare to the rollbackstat
     // assert the first partition data and log file size
     List<HoodieFileGroup> firstPartitionRollBack1FileGroups = table.getFileSystemView().getAllFileGroups(DEFAULT_FIRST_PARTITION_PATH).collect(Collectors.toList());
     assertEquals(1, firstPartitionRollBack1FileGroups.size());

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/marker/TestWriteMarkersBase.java
Patch:
@@ -103,7 +103,7 @@ public void testDeletionWhenMarkerDirNotExists() throws IOException {
   @ParameterizedTest
   @ValueSource(booleans = {true, false})
   public void testDataPathsWhenCreatingOrMerging(boolean isTablePartitioned) throws IOException {
-    // add markfiles
+    // add marker files
     createSomeMarkers(isTablePartitioned);
     // add invalid file
     createInvalidFile(isTablePartitioned ? "2020/06/01" : "", "invalid_file3");

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java
Patch:
@@ -207,7 +207,7 @@ public static List<HoodieBaseFile> getLatestBaseFiles(String basePath, FileSyste
   }
 
   /**
-   * Reads the paths under the a hoodie table out as a DataFrame.
+   * Reads the paths under the hoodie table out as a DataFrame.
    */
   public static Dataset<Row> read(JavaSparkContext jsc, String basePath, SQLContext sqlContext, FileSystem fs,
                                   String... paths) {

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java
Patch:
@@ -275,7 +275,7 @@ public HoodieMetadataPayload preCombine(HoodieMetadataPayload previousRecord) {
         HoodieMetadataBloomFilter combineBloomFilterMetadata = combineBloomFilterMetadata(previousRecord);
         return new HoodieMetadataPayload(key, type, combineBloomFilterMetadata);
       case METADATA_TYPE_COLUMN_STATS:
-        return new HoodieMetadataPayload(key, type, combineColumnStatsMetadatat(previousRecord));
+        return new HoodieMetadataPayload(key, type, combineColumnStatsMetadata(previousRecord));
       default:
         throw new HoodieMetadataException("Unknown type of HoodieMetadataPayload: " + type);
     }
@@ -285,7 +285,7 @@ private HoodieMetadataBloomFilter combineBloomFilterMetadata(HoodieMetadataPaylo
     return this.bloomFilterMetadata;
   }
 
-  private HoodieMetadataColumnStats combineColumnStatsMetadatat(HoodieMetadataPayload previousRecord) {
+  private HoodieMetadataColumnStats combineColumnStatsMetadata(HoodieMetadataPayload previousRecord) {
     return this.columnStatMetadata;
   }
 

File: hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java
Patch:
@@ -164,7 +164,7 @@ public void run() throws Exception {
     ExecutorService executor = Executors.newFixedThreadPool(2);
     int numInitialCommits = 0;
 
-    // thread for spark strucutured streaming
+    // thread for spark structured streaming
     try {
       Future<Void> streamFuture = executor.submit(() -> {
         LOG.info("===== Streaming Starting =====");
@@ -211,7 +211,7 @@ public void run() throws Exception {
     Dataset<Row> inputDF3 = newSpark.read().json(jssc.parallelize(deletes, 2));
     executor = Executors.newFixedThreadPool(2);
 
-    // thread for spark strucutured streaming
+    // thread for spark structured streaming
     try {
       Future<Void> streamFuture = executor.submit(() -> {
         LOG.info("===== Streaming Starting =====");

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/TestDataSourceUtils.java
Patch:
@@ -191,7 +191,7 @@ public void testDoWriteOperationWithUserDefinedBulkInsertPartitioner() throws Ho
 
   @Test
   public void testCreateUserDefinedBulkInsertPartitionerRowsWithInValidPartitioner() throws HoodieException {
-    config = HoodieWriteConfig.newBuilder().withPath("/").withUserDefinedBulkInsertPartitionerClass("NonExistantUserDefinedClass").build();
+    config = HoodieWriteConfig.newBuilder().withPath("/").withUserDefinedBulkInsertPartitionerClass("NonExistentUserDefinedClass").build();
 
     Exception exception = assertThrows(HoodieException.class, () -> {
       DataSourceUtils.createUserDefinedBulkInsertPartitionerWithRows(config);

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/payload/TestAWSDmsAvroPayload.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.avro.generic.GenericData;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
-
 import org.junit.jupiter.api.Test;
 
 import static org.junit.jupiter.api.Assertions.assertFalse;
@@ -98,7 +97,7 @@ public void testDelete() {
 
     try {
       Option<IndexedRecord> outputPayload = payload.combineAndGetUpdateValue(oldRecord, avroSchema);
-      // expect nothing to be comitted to table
+      // expect nothing to be committed to table
       assertFalse(outputPayload.isPresent());
     } catch (Exception e) {
       fail("Unexpected exception");
@@ -123,7 +122,7 @@ public void testPreCombineWithDelete() {
     try {
       OverwriteWithLatestAvroPayload output = payload.preCombine(insertPayload);
       Option<IndexedRecord> outputPayload = output.getInsertValue(avroSchema);
-      // expect nothing to be comitted to table
+      // expect nothing to be committed to table
       assertFalse(outputPayload.isPresent());
     } catch (Exception e) {
       fail("Unexpected exception");

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java
Patch:
@@ -74,7 +74,7 @@ public class TimelineServerPerf implements Serializable {
   public TimelineServerPerf(Config cfg) throws IOException {
     this.cfg = cfg;
     useExternalTimelineServer = (cfg.serverHost != null);
-    TimelineService.Config timelineServiceConf = cfg.getTimelinServerConfig();
+    TimelineService.Config timelineServiceConf = cfg.getTimelineServerConfig();
     this.timelineServer = new TimelineService(
         new HoodieLocalEngineContext(FSUtils.prepareHadoopConf(new Configuration())),
         new Configuration(), timelineServiceConf, FileSystem.get(new Configuration()),
@@ -281,7 +281,7 @@ public static class Config implements Serializable {
         description = " Server Host (Set it for externally managed timeline service")
     public String serverHost = null;
 
-    @Parameter(names = {"--view-storage", "-st"}, description = "View Storage Type. Defaut - SPILLABLE_DISK")
+    @Parameter(names = {"--view-storage", "-st"}, description = "View Storage Type. Default - SPILLABLE_DISK")
     public FileSystemViewStorageType viewStorageType = FileSystemViewStorageType.SPILLABLE_DISK;
 
     @Parameter(names = {"--max-view-mem-per-table", "-mv"},
@@ -310,7 +310,7 @@ public static class Config implements Serializable {
     @Parameter(names = {"--help", "-h"})
     public Boolean help = false;
 
-    public TimelineService.Config getTimelinServerConfig() {
+    public TimelineService.Config getTimelineServerConfig() {
       TimelineService.Config c = new TimelineService.Config();
       c.viewStorageType = viewStorageType;
       c.baseStorePathForFileGroups = baseStorePathForFileGroups;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JsonKafkaSource.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamerMetrics;
 import org.apache.hudi.utilities.exception.HoodieSourceTimeoutException;
 import org.apache.hudi.utilities.schema.SchemaProvider;
@@ -79,7 +80,7 @@ private JavaRDD<String> toRDD(OffsetRange[] offsetRanges) {
             offsetGen.getKafkaParams(),
             offsetRanges,
             LocationStrategies.PreferConsistent())
-        .filter(x -> Objects.nonNull(x.value()))
+        .filter(x -> !StringUtils.isNullOrEmpty((String)x.value()))
         .map(x -> x.value().toString());
   }
 

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -439,7 +439,7 @@ private static int rollbackToSavepoint(JavaSparkContext jsc, String savepointTim
       LOG.info(String.format("The commit \"%s\" rolled back.", savepointTime));
       return 0;
     } catch (Exception e) {
-      LOG.warn(String.format("The commit \"%s\" failed to roll back.", savepointTime));
+      LOG.warn(String.format("The commit \"%s\" failed to roll back.", savepointTime), e);
       return -1;
     }
   }
@@ -451,7 +451,7 @@ private static int deleteSavepoint(JavaSparkContext jsc, String savepointTime, S
       LOG.info(String.format("Savepoint \"%s\" deleted.", savepointTime));
       return 0;
     } catch (Exception e) {
-      LOG.warn(String.format("Failed: Could not delete savepoint \"%s\".", savepointTime));
+      LOG.warn(String.format("Failed: Could not delete savepoint \"%s\".", savepointTime), e);
       return -1;
     }
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieStorageConfig.java
Patch:
@@ -127,7 +127,7 @@ public class HoodieStorageConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> PARQUET_OUTPUT_TIMESTAMP_TYPE = ConfigProperty
           .key("hoodie.parquet.outputtimestamptype")
-          .defaultValue("TIMESTAMP_MILLIS")
+          .defaultValue("TIMESTAMP_MICROS")
           .withDocumentation("Sets spark.sql.parquet.outputTimestampType. Parquet timestamp type to use when Spark writes data to Parquet files.");
 
   public static final ConfigProperty<String> HFILE_COMPRESSION_ALGORITHM_NAME = ConfigProperty

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java
Patch:
@@ -152,7 +152,7 @@ public class HoodieIndexConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> SIMPLE_INDEX_PARALLELISM = ConfigProperty
       .key("hoodie.simple.index.parallelism")
-      .defaultValue("50")
+      .defaultValue("100")
       .withDocumentation("Only applies if index type is SIMPLE. "
           + "This is the amount of parallelism for index lookup, which involves a Spark Shuffle");
 
@@ -568,7 +568,7 @@ public HoodieIndexConfig build() {
     private String getDefaultIndexType(EngineType engineType) {
       switch (engineType) {
         case SPARK:
-          return HoodieIndex.IndexType.BLOOM.name();
+          return HoodieIndex.IndexType.SIMPLE.name();
         case FLINK:
         case JAVA:
           return HoodieIndex.IndexType.INMEMORY.name();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/simple/HoodieSimpleIndex.java
Patch:
@@ -148,7 +148,7 @@ protected HoodiePairData<HoodieKey, HoodieRecordLocation> fetchRecordLocationsFo
   protected HoodiePairData<HoodieKey, HoodieRecordLocation> fetchRecordLocations(
       HoodieEngineContext context, HoodieTable hoodieTable, int parallelism,
       List<Pair<String, HoodieBaseFile>> baseFiles) {
-    int fetchParallelism = Math.max(1, Math.max(baseFiles.size(), parallelism));
+    int fetchParallelism = Math.max(1, Math.min(baseFiles.size(), parallelism));
 
     return context.parallelize(baseFiles, fetchParallelism)
         .flatMap(partitionPathBaseFile -> new HoodieKeyLocationFetchHandle(config, hoodieTable, partitionPathBaseFile, keyGeneratorOpt)

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/config/TestHoodieWriteConfig.java
Patch:
@@ -82,7 +82,7 @@ public void testPropertyLoading(boolean withAlternative) throws IOException {
   public void testDefaultIndexAccordingToEngineType() {
     testEngineSpecificConfig(HoodieWriteConfig::getIndexType,
         constructConfigMap(
-            EngineType.SPARK, HoodieIndex.IndexType.BLOOM,
+            EngineType.SPARK, HoodieIndex.IndexType.SIMPLE,
             EngineType.FLINK, HoodieIndex.IndexType.INMEMORY,
             EngineType.JAVA, HoodieIndex.IndexType.INMEMORY));
   }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.integ.testsuite.dag.nodes;
 
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;
 import org.apache.hudi.integ.testsuite.dag.ExecutionContext;
@@ -49,7 +50,8 @@ public Dataset<Row> getDatasetToValidate(SparkSession session, ExecutionContext
                                            StructType inputSchema) {
     String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + "/*/*/*";
     log.info("Validate data in target hudi path " + hudiPath);
-    Dataset<Row> hudiDf = session.read().format("hudi").load(hudiPath);
+    Dataset<Row> hudiDf = session.read().option(HoodieMetadataConfig.ENABLE.key(), String.valueOf(config.isEnableMetadataValidate()))
+        .format("hudi").load(hudiPath);
     return hudiDf.drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)
             .drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.FILENAME_METADATA_FIELD);
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -105,7 +105,7 @@ public abstract class BaseHoodieWriteClient<T extends HoodieRecordPayload, I, K,
   private static final Logger LOG = LogManager.getLogger(BaseHoodieWriteClient.class);
 
   protected final transient HoodieMetrics metrics;
-  private final transient HoodieIndex<T, ?, ?, ?> index;
+  private final transient HoodieIndex<?, ?> index;
 
   protected transient Timer.Context writeTimer = null;
   protected transient Timer.Context compactionTimer;
@@ -142,7 +142,7 @@ public BaseHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig writ
     this.txnManager = new TransactionManager(config, fs);
   }
 
-  protected abstract HoodieIndex<T, ?, ?, ?> createIndex(HoodieWriteConfig writeConfig);
+  protected abstract HoodieIndex<?, ?> createIndex(HoodieWriteConfig writeConfig);
 
   public void setOperationType(WriteOperationType operationType) {
     this.operationType = operationType;
@@ -1160,7 +1160,7 @@ public HoodieMetrics getMetrics() {
     return metrics;
   }
 
-  public HoodieIndex<T, ?, ?, ?> getIndex() {
+  public HoodieIndex<?, ?> getIndex() {
     return index;
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/HoodieLazyInsertIterable.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.execution;
 
-import java.util.Properties;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.utils.LazyIterableIterator;
 import org.apache.hudi.common.engine.TaskContextSupplier;
@@ -36,6 +35,7 @@
 
 import java.util.Iterator;
 import java.util.List;
+import java.util.Properties;
 import java.util.function.Function;
 
 /**
@@ -87,7 +87,7 @@ public static class HoodieInsertValueGenResult<T extends HoodieRecord> {
     public HoodieInsertValueGenResult(T record, Schema schema, Properties properties) {
       this.record = record;
       try {
-        this.insertValue = record.getData().getInsertValue(schema, properties);
+        this.insertValue = ((HoodieRecordPayload) record.getData()).getInsertValue(schema, properties);
       } catch (Exception e) {
         this.exception = Option.of(e);
       }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java
Patch:
@@ -102,14 +102,14 @@ public static List<Pair<String, HoodieBaseFile>> getLatestBaseFilesForAllPartiti
    * @return the tagged {@link HoodieRecord}
    */
   public static HoodieRecord getTaggedRecord(HoodieRecord inputRecord, Option<HoodieRecordLocation> location) {
-    HoodieRecord record = inputRecord;
+    HoodieRecord<?> record = inputRecord;
     if (location.isPresent()) {
       // When you have a record in multiple files in the same partition, then <row key, record> collection
       // will have 2 entries with the same exact in memory copy of the HoodieRecord and the 2
       // separate filenames that the record is found in. This will result in setting
       // currentLocation 2 times and it will fail the second time. So creating a new in memory
       // copy of the hoodie record.
-      record = new HoodieRecord<>(inputRecord);
+      record = inputRecord.newInstance();
       record.unseal();
       record.setCurrentLocation(location.get());
       record.seal();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/BaseHoodieBloomIndexHelper.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecordLocation;
-import org.apache.hudi.common.util.collection.ImmutablePair;
+import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
@@ -51,7 +51,7 @@ public abstract class BaseHoodieBloomIndexHelper implements Serializable {
   public abstract HoodiePairData<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(
       HoodieWriteConfig config, HoodieEngineContext context, HoodieTable hoodieTable,
       HoodiePairData<String, String> partitionRecordKeyPairs,
-      HoodieData<ImmutablePair<String, HoodieKey>> fileComparisonPairs,
+      HoodieData<Pair<String, HoodieKey>> fileComparisonPairs,
       Map<String, List<BloomIndexFileInfo>> partitionToFileInfo,
       Map<String, Long> recordsPerPartition);
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/ListBasedHoodieBloomIndexHelper.java
Patch:
@@ -57,11 +57,11 @@ public static ListBasedHoodieBloomIndexHelper getInstance() {
   public HoodiePairData<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(
       HoodieWriteConfig config, HoodieEngineContext context, HoodieTable hoodieTable,
       HoodiePairData<String, String> partitionRecordKeyPairs,
-      HoodieData<ImmutablePair<String, HoodieKey>> fileComparisonPairs,
+      HoodieData<Pair<String, HoodieKey>> fileComparisonPairs,
       Map<String, List<BloomIndexFileInfo>> partitionToFileInfo, Map<String, Long> recordsPerPartition) {
     List<Pair<String, HoodieKey>> fileComparisonPairList =
         HoodieList.getList(fileComparisonPairs).stream()
-            .sorted(Comparator.comparing(ImmutablePair::getLeft)).collect(toList());
+            .sorted(Comparator.comparing(Pair::getLeft)).collect(toList());
 
     List<HoodieKeyLookupResult> keyLookupResults = new ArrayList<>();
     Iterator<List<HoodieKeyLookupResult>> iterator = new HoodieBaseBloomIndexCheckFunction(

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -393,7 +393,7 @@ public boolean canWrite(HoodieRecord record) {
 
   @Override
   public void write(HoodieRecord record, Option<IndexedRecord> insertValue) {
-    Option<Map<String, String>> recordMetadata = record.getData().getMetadata();
+    Option<Map<String, String>> recordMetadata = ((HoodieRecordPayload) record.getData()).getMetadata();
     try {
       init(record);
       flushToDiskIfRequired(record);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.io;
 
-import org.apache.avro.Schema;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.fs.FSUtils;
@@ -37,6 +36,7 @@
 import org.apache.hudi.io.storage.HoodieFileWriterFactory;
 import org.apache.hudi.table.HoodieTable;
 
+import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.fs.Path;
@@ -128,7 +128,7 @@ public boolean canWrite(HoodieRecord record) {
    */
   @Override
   public void write(HoodieRecord record, Option<IndexedRecord> avroRecord) {
-    Option recordMetadata = record.getData().getMetadata();
+    Option recordMetadata = ((HoodieRecordPayload) record.getData()).getMetadata();
     if (HoodieOperation.isDelete(record.getOperation())) {
       avroRecord = Option.empty();
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -324,7 +324,7 @@ public void write(GenericRecord oldRecord) {
     if (keyToNewRecords.containsKey(key)) {
       // If we have duplicate records that we are updating, then the hoodie record will be deflated after
       // writing the first record. So make a copy of the record to be merged
-      HoodieRecord<T> hoodieRecord = new HoodieRecord<>(keyToNewRecords.get(key));
+      HoodieRecord<T> hoodieRecord = keyToNewRecords.get(key).newInstance();
       try {
         Option<IndexedRecord> combinedAvroRecord =
             hoodieRecord.getData().combineAndGetUpdateValue(oldRecord,

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java
Patch:
@@ -85,7 +85,7 @@ public void write(GenericRecord oldRecord) {
       }
 
       // This is a new insert
-      HoodieRecord<T> hoodieRecord = new HoodieRecord<>(keyToNewRecords.get(keyToPreWrite));
+      HoodieRecord<T> hoodieRecord = keyToNewRecords.get(keyToPreWrite).newInstance();
       if (writtenRecordKeys.contains(keyToPreWrite)) {
         throw new HoodieUpsertException("Insert/Update not in sorted order");
       }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java
Patch:
@@ -210,7 +210,7 @@ public void write(HoodieRecord record, Option<IndexedRecord> insertValue) {
    * Perform the actual writing of the given record into the backing file.
    */
   public void write(HoodieRecord record, Option<IndexedRecord> avroRecord, Option<Exception> exception) {
-    Option recordMetadata = record.getData().getMetadata();
+    Option recordMetadata = ((HoodieRecordPayload) record.getData()).getMetadata();
     if (exception.isPresent() && exception.get() instanceof Throwable) {
       // Not throwing exception from here, since we don't want to fail the entire job for a single record
       writeStatus.markFailure(record, exception.get(), recordMetadata);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -102,7 +102,7 @@ public abstract class HoodieTable<T extends HoodieRecordPayload, I, K, O> implem
 
   protected final HoodieWriteConfig config;
   protected final HoodieTableMetaClient metaClient;
-  protected final HoodieIndex<T, ?, ?, ?> index;
+  protected final HoodieIndex<?, ?> index;
   private SerializableConfiguration hadoopConfiguration;
   protected final TaskContextSupplier taskContextSupplier;
   private final HoodieTableMetadata metadata;
@@ -128,7 +128,7 @@ protected HoodieTable(HoodieWriteConfig config, HoodieEngineContext context, Hoo
     this.taskContextSupplier = context.getTaskContextSupplier();
   }
 
-  protected abstract HoodieIndex<T, ?, ?, ?> getIndex(HoodieWriteConfig config, HoodieEngineContext context);
+  protected abstract HoodieIndex<?, ?> getIndex(HoodieWriteConfig config, HoodieEngineContext context);
 
   protected HoodieStorageLayout getStorageLayout(HoodieWriteConfig config) {
     return HoodieLayoutFactory.createLayout(config);
@@ -367,7 +367,7 @@ public HoodieActiveTimeline getActiveTimeline() {
   /**
    * Return the index.
    */
-  public HoodieIndex<T, ?, ?, ?> getIndex() {
+  public HoodieIndex<?, ?> getIndex() {
     return index;
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/bootstrap/BootstrapRecordConsumer.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.table.action.bootstrap;
 
 import org.apache.hudi.common.model.HoodieRecord;
+import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.io.HoodieBootstrapHandle;
@@ -39,7 +40,8 @@ public BootstrapRecordConsumer(HoodieBootstrapHandle bootstrapHandle) {
   @Override
   protected void consumeOneRecord(HoodieRecord record) {
     try {
-      bootstrapHandle.write(record, record.getData().getInsertValue(bootstrapHandle.getWriterSchemaWithMetaFields()));
+      bootstrapHandle.write(record, ((HoodieRecordPayload) record.getData())
+          .getInsertValue(bootstrapHandle.getWriterSchemaWithMetaFields()));
     } catch (IOException e) {
       throw new HoodieIOException(e.getMessage(), e);
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseWriteHelper.java
Patch:
@@ -86,5 +86,5 @@ public I deduplicateRecords(
   }
 
   public abstract I deduplicateRecords(
-      I records, HoodieIndex<T, ?, ?, ?> index, int parallelism);
+      I records, HoodieIndex<?, ?> index, int parallelism);
 }

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/io/storage/TestHoodieHFileReaderWriter.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.common.bloom.BloomFilterTypeCode;
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 
@@ -122,7 +123,7 @@ public void testWriteReadHFile(boolean populateMetaFields, boolean testAvroWithM
       record.put("time", Integer.toString(RANDOM.nextInt()));
       record.put("number", i);
       if (testAvroWithMeta) {
-        writer.writeAvroWithMetadata(record, new HoodieRecord(new HoodieKey((String) record.get("_row_key"),
+        writer.writeAvroWithMetadata(record, new HoodieAvroRecord(new HoodieKey((String) record.get("_row_key"),
             Integer.toString((Integer) record.get("number"))), new EmptyHoodieRecordPayload())); // payload does not matter. GenericRecord passed in is what matters
         // only HoodieKey will be looked up from the 2nd arg(HoodieRecord).
       } else {
@@ -170,4 +171,4 @@ private Set<String> getRandomKeys(int count, List<String> keys) {
     }
     return rowKeys;
   }
-}
\ No newline at end of file
+}

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndexFactory.java
Patch:
@@ -46,11 +46,11 @@ public static HoodieIndex createIndex(HoodieFlinkEngineContext context, HoodieWr
     // TODO more indexes to be added
     switch (config.getIndexType()) {
       case INMEMORY:
-        return new FlinkInMemoryStateIndex<>(context, config);
+        return new FlinkInMemoryStateIndex(context, config);
       case BLOOM:
-        return new HoodieBloomIndex<>(config, ListBasedHoodieBloomIndexHelper.getInstance());
+        return new HoodieBloomIndex(config, ListBasedHoodieBloomIndexHelper.getInstance());
       case SIMPLE:
-        return new HoodieSimpleIndex<>(config, Option.empty());
+        return new HoodieSimpleIndex(config, Option.empty());
       default:
         throw new HoodieIndexException("Unsupported index type " + config.getIndexType());
     }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeleteHelper.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.common.data.HoodieList;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.collection.Pair;
@@ -93,7 +94,7 @@ public HoodieWriteMetadata<List<WriteStatus>> execute(String instantTime,
       }
 
       List<HoodieRecord<EmptyHoodieRecordPayload>> dedupedRecords =
-          dedupedKeys.stream().map(key -> new HoodieRecord<>(key, new EmptyHoodieRecordPayload())).collect(Collectors.toList());
+          dedupedKeys.stream().map(key -> new HoodieAvroRecord<>(key, new EmptyHoodieRecordPayload())).collect(Collectors.toList());
       Instant beginTag = Instant.now();
       // perform index look up to get existing location of records
       List<HoodieRecord<EmptyHoodieRecordPayload>> taggedRecords = HoodieList.getList(

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkWriteHelper.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.data.HoodieList;
 import org.apache.hudi.common.engine.HoodieEngineContext;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieOperation;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -89,7 +90,7 @@ protected List<HoodieRecord<T>> tag(List<HoodieRecord<T>> dedupedRecords, Hoodie
 
   @Override
   public List<HoodieRecord<T>> deduplicateRecords(
-      List<HoodieRecord<T>> records, HoodieIndex<T, ?, ?, ?> index, int parallelism) {
+      List<HoodieRecord<T>> records, HoodieIndex<?, ?> index, int parallelism) {
     Map<Object, List<Pair<Object, HoodieRecord<T>>>> keyedRecords = records.stream().map(record -> {
       // If index used is global, then records are expected to differ in their partitionPath
       final Object key = record.getKey().getRecordKey();
@@ -107,7 +108,7 @@ public List<HoodieRecord<T>> deduplicateRecords(
       boolean choosePrev = data1.equals(reducedData);
       HoodieKey reducedKey = choosePrev ? rec1.getKey() : rec2.getKey();
       HoodieOperation operation = choosePrev ? rec1.getOperation() : rec2.getOperation();
-      HoodieRecord<T> hoodieRecord = new HoodieRecord<>(reducedKey, reducedData, operation);
+      HoodieRecord<T> hoodieRecord = new HoodieAvroRecord<>(reducedKey, reducedData, operation);
       // reuse the location from the first record.
       hoodieRecord.setCurrentLocation(rec1.getCurrentLocation());
       return hoodieRecord;

File: hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkWriteableTestTable.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
+import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.log.HoodieLogFormat;
 import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;
@@ -132,7 +133,7 @@ private Pair<String, HoodieLogFile> appendRecordsToLogFile(List<HoodieRecord> gr
       header.put(HeaderMetadataType.SCHEMA, schema.toString());
       logWriter.appendBlock(new HoodieAvroDataBlock(groupedRecords.stream().map(r -> {
         try {
-          GenericRecord val = (GenericRecord) r.getData().getInsertValue(schema).get();
+          GenericRecord val = (GenericRecord) ((HoodieRecordPayload) r.getData()).getInsertValue(schema).get();
           HoodieAvroUtils.addHoodieKeyToRecord(val, r.getRecordKey(), r.getPartitionPath(), "");
           return (IndexedRecord) val;
         } catch (IOException e) {

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaExecutionStrategy.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.client.common.JavaTaskContextSupplier;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.ClusteringOperation;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -237,7 +238,7 @@ private HoodieRecord<T> transform(IndexedRecord indexedRecord) {
     HoodieKey hoodieKey = new HoodieKey(key, partition);
 
     HoodieRecordPayload avroPayload = new RewriteAvroPayload(record);
-    HoodieRecord hoodieRecord = new HoodieRecord(hoodieKey, avroPayload);
+    HoodieRecord hoodieRecord = new HoodieAvroRecord(hoodieKey, avroPayload);
     return hoodieRecord;
   }
 }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieIndexFactory.java
Patch:
@@ -44,9 +44,9 @@ public static HoodieIndex createIndex(HoodieWriteConfig config) {
     // TODO more indexes to be added
     switch (config.getIndexType()) {
       case INMEMORY:
-        return new HoodieInMemoryHashIndex<>(config);
+        return new HoodieInMemoryHashIndex(config);
       case BLOOM:
-        return new HoodieBloomIndex<>(config, ListBasedHoodieBloomIndexHelper.getInstance());
+        return new HoodieBloomIndex(config, ListBasedHoodieBloomIndexHelper.getInstance());
       default:
         throw new HoodieIndexException("Unsupported index type " + config.getIndexType());
     }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaDeleteHelper.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.common.data.HoodieList;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.collection.Pair;
@@ -95,7 +96,7 @@ public HoodieWriteMetadata<List<WriteStatus>> execute(String instantTime,
       }
 
       List<HoodieRecord<EmptyHoodieRecordPayload>> dedupedRecords =
-          dedupedKeys.stream().map(key -> new HoodieRecord<>(key, new EmptyHoodieRecordPayload())).collect(Collectors.toList());
+          dedupedKeys.stream().map(key -> new HoodieAvroRecord<>(key, new EmptyHoodieRecordPayload())).collect(Collectors.toList());
       Instant beginTag = Instant.now();
       // perform index look up to get existing location of records
       List<HoodieRecord<EmptyHoodieRecordPayload>> taggedRecords = HoodieList.getList(

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaWriteHelper.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.data.HoodieList;
 import org.apache.hudi.common.engine.HoodieEngineContext;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
@@ -55,7 +56,7 @@ protected List<HoodieRecord<T>> tag(List<HoodieRecord<T>> dedupedRecords, Hoodie
 
   @Override
   public List<HoodieRecord<T>> deduplicateRecords(
-      List<HoodieRecord<T>> records, HoodieIndex<T, ?, ?, ?> index, int parallelism) {
+      List<HoodieRecord<T>> records, HoodieIndex<?, ?> index, int parallelism) {
     boolean isIndexingGlobal = index.isGlobal();
     Map<Object, List<Pair<Object, HoodieRecord<T>>>> keyedRecords = records.stream().map(record -> {
       HoodieKey hoodieKey = record.getKey();
@@ -70,7 +71,7 @@ public List<HoodieRecord<T>> deduplicateRecords(
       // we cannot allow the user to change the key or partitionPath, since that will affect
       // everything
       // so pick it from one of the records.
-      return new HoodieRecord<T>(rec1.getKey(), reducedData);
+      return new HoodieAvroRecord<T>(rec1.getKey(), reducedData);
     }).orElse(null)).filter(Objects::nonNull).collect(Collectors.toList());
   }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.ClusteringOperation;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -290,7 +291,7 @@ private HoodieRecord<T> transform(IndexedRecord indexedRecord) {
     HoodieKey hoodieKey = new HoodieKey(key, partition);
 
     HoodieRecordPayload avroPayload = new RewriteAvroPayload(record);
-    HoodieRecord hoodieRecord = new HoodieRecord(hoodieKey, avroPayload);
+    HoodieRecord hoodieRecord = new HoodieAvroRecord(hoodieKey, avroPayload);
     return hoodieRecord;
   }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SingleSparkJobExecutionStrategy.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hudi.common.engine.TaskContextSupplier;
 import org.apache.hudi.common.model.ClusteringGroupInfo;
 import org.apache.hudi.common.model.ClusteringOperation;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -181,7 +182,7 @@ private HoodieRecord<T> transform(IndexedRecord indexedRecord) {
     HoodieKey hoodieKey = new HoodieKey(key, partition);
 
     HoodieRecordPayload avroPayload = new RewriteAvroPayload(record);
-    HoodieRecord hoodieRecord = new HoodieRecord(hoodieKey, avroPayload);
+    HoodieRecord hoodieRecord = new HoodieAvroRecord(hoodieKey, avroPayload);
     return hoodieRecord;
   }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/data/HoodieJavaRDD.java
Patch:
@@ -83,8 +83,8 @@ public JavaRDD<T> get() {
   }
 
   @Override
-  public void persist(String storageLevel) {
-    rddData.persist(StorageLevel.fromString(storageLevel));
+  public void persist(String cacheConfig) {
+    rddData.persist(StorageLevel.fromString(cacheConfig));
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndexHelper.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecordLocation;
-import org.apache.hudi.common.util.collection.ImmutablePair;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.data.HoodieJavaPairRDD;
@@ -63,7 +62,7 @@ public static SparkHoodieBloomIndexHelper getInstance() {
   public HoodiePairData<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(
       HoodieWriteConfig config, HoodieEngineContext context, HoodieTable hoodieTable,
       HoodiePairData<String, String> partitionRecordKeyPairs,
-      HoodieData<ImmutablePair<String, HoodieKey>> fileComparisonPairs,
+      HoodieData<Pair<String, HoodieKey>> fileComparisonPairs,
       Map<String, List<BloomIndexFileInfo>> partitionToFileInfo,
       Map<String, Long> recordsPerPartition) {
     JavaRDD<Tuple2<String, HoodieKey>> fileComparisonsRDD =

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/OrcBootstrapMetadataHandler.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.avro.model.HoodieFileStatus;
 import org.apache.hudi.client.bootstrap.BootstrapRecordPayload;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.AvroOrcUtils;
@@ -73,7 +74,7 @@ void executeBootstrap(HoodieBootstrapHandle<?, ?, ?, ?> bootstrapHandle, Path so
         GenericRecord gr = new GenericData.Record(HoodieAvroUtils.RECORD_KEY_SCHEMA);
         gr.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, recKey);
         BootstrapRecordPayload payload = new BootstrapRecordPayload(gr);
-        HoodieRecord rec = new HoodieRecord(new HoodieKey(recKey, partitionPath), payload);
+        HoodieRecord rec = new HoodieAvroRecord(new HoodieKey(recKey, partitionPath), payload);
         return rec;
       });
       wrapper.execute();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/ParquetBootstrapMetadataHandler.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.avro.model.HoodieFileStatus;
 import org.apache.hudi.client.bootstrap.BootstrapRecordPayload;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.ParquetReaderIterator;
@@ -77,7 +78,7 @@ void executeBootstrap(HoodieBootstrapHandle<?, ?, ?, ?> bootstrapHandle,
         GenericRecord gr = new GenericData.Record(HoodieAvroUtils.RECORD_KEY_SCHEMA);
         gr.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, recKey);
         BootstrapRecordPayload payload = new BootstrapRecordPayload(gr);
-        HoodieRecord rec = new HoodieRecord(new HoodieKey(recKey, partitionPath), payload);
+        HoodieRecord rec = new HoodieAvroRecord(new HoodieKey(recKey, partitionPath), payload);
         return rec;
       });
       wrapper.execute();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBucketIndexPartitioner.java
Patch:
@@ -74,7 +74,7 @@ public SparkBucketIndexPartitioner(WorkloadProfile profile,
           " Bucket index partitioner should only be used by BucketIndex other than "
               + table.getIndex().getClass().getSimpleName());
     }
-    this.numBuckets = ((HoodieBucketIndex<T>) table.getIndex()).getNumBuckets();
+    this.numBuckets = ((HoodieBucketIndex) table.getIndex()).getNumBuckets();
     this.indexKeyField = config.getBucketIndexHashField();
     this.totalPartitionPaths = profile.getPartitionPaths().size();
     partitionPaths = new ArrayList<>(profile.getPartitionPaths());

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeleteHelper.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
@@ -93,7 +94,7 @@ public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute(String instantTime,
       }
 
       JavaRDD<HoodieRecord<T>> dedupedRecords =
-          dedupedKeys.map(key -> new HoodieRecord(key, new EmptyHoodieRecordPayload()));
+          dedupedKeys.map(key -> new HoodieAvroRecord(key, new EmptyHoodieRecordPayload()));
       Instant beginTag = Instant.now();
       // perform index loop up to get existing location of records
       JavaRDD<HoodieRecord<T>> taggedRecords = HoodieJavaRDD.getJavaRDD(

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkWriteHelper.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.engine.HoodieEngineContext;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
@@ -58,7 +59,7 @@ protected JavaRDD<HoodieRecord<T>> tag(JavaRDD<HoodieRecord<T>> dedupedRecords,
 
   @Override
   public JavaRDD<HoodieRecord<T>> deduplicateRecords(
-      JavaRDD<HoodieRecord<T>> records, HoodieIndex<T, ?, ?, ?> index, int parallelism) {
+      JavaRDD<HoodieRecord<T>> records, HoodieIndex<?, ?> index, int parallelism) {
     boolean isIndexingGlobal = index.isGlobal();
     return records.mapToPair(record -> {
       HoodieKey hoodieKey = record.getKey();
@@ -70,7 +71,7 @@ public JavaRDD<HoodieRecord<T>> deduplicateRecords(
       T reducedData = (T) rec2.getData().preCombine(rec1.getData());
       HoodieKey reducedKey = rec1.getData().equals(reducedData) ? rec1.getKey() : rec2.getKey();
 
-      return new HoodieRecord<T>(reducedKey, reducedData);
+      return new HoodieAvroRecord<T>(reducedKey, reducedData);
     }, parallelism).map(Tuple2::_2);
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieReadClient.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.client;
 
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.Option;
@@ -209,7 +210,7 @@ private void testTagLocation(HoodieWriteConfig hoodieWriteConfig,
       // since they have been modified in the DAG
       JavaRDD<HoodieRecord> recordRDD =
           jsc.parallelize(result.collect().stream().map(WriteStatus::getWrittenRecords).flatMap(Collection::stream)
-              .map(record -> new HoodieRecord(record.getKey(), null)).collect(Collectors.toList()));
+              .map(record -> new HoodieAvroRecord(record.getKey(), null)).collect(Collectors.toList()));
       // Should have 100 records in table (check using Index), all in locations marked at commit
       HoodieReadClient readClient = getHoodieReadClient(hoodieWriteConfig.getBasePath());
       List<HoodieRecord> taggedRecords = readClient.tagLocation(recordRDD).collect();
@@ -225,7 +226,7 @@ private void testTagLocation(HoodieWriteConfig hoodieWriteConfig,
           numRecords, 200, 2);
       recordRDD =
           jsc.parallelize(result.collect().stream().map(WriteStatus::getWrittenRecords).flatMap(Collection::stream)
-              .map(record -> new HoodieRecord(record.getKey(), null)).collect(Collectors.toList()));
+              .map(record -> new HoodieAvroRecord(record.getKey(), null)).collect(Collectors.toList()));
       // Index should be able to locate all updates in correct locations.
       readClient = getHoodieReadClient(hoodieWriteConfig.getBasePath());
       taggedRecords = readClient.tagLocation(recordRDD).collect();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.client;
 
 import org.apache.hudi.avro.HoodieAvroUtils;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
@@ -497,9 +498,9 @@ private List<HoodieRecord> convertToSchema(List<HoodieRecord> records, String sc
       HoodieKey key = r.getKey();
       GenericRecord payload;
       try {
-        payload = (GenericRecord)r.getData().getInsertValue(HoodieTestDataGenerator.AVRO_SCHEMA).get();
+        payload = (GenericRecord) ((HoodieAvroRecord) r).getData().getInsertValue(HoodieTestDataGenerator.AVRO_SCHEMA).get();
         GenericRecord newPayload = HoodieAvroUtils.rewriteRecord(payload, newSchema);
-        return new HoodieRecord(key, new RawTripTestPayload(newPayload.toString(), key.getRecordKey(), key.getPartitionPath(), schemaStr));
+        return new HoodieAvroRecord(key, new RawTripTestPayload(newPayload.toString(), key.getRecordKey(), key.getPartitionPath(), schemaStr));
       } catch (IOException e) {
         throw new RuntimeException("Conversion to new schema failed");
       }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.client;
 
 import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
@@ -82,7 +83,7 @@ private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IO
       for (String recordStr : recordsStrs) {
         RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);
         insertRecords
-            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));
+            .add(new HoodieAvroRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));
       }
       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()
           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));
@@ -147,7 +148,7 @@ private List<HoodieRecord> buildUpdateRecords(String recordStr, String insertFil
     List<HoodieRecord> updateRecords = new ArrayList<>();
     RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);
     HoodieRecord record =
-        new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);
+        new HoodieAvroRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);
     record.setCurrentLocation(new HoodieRecordLocation("101", insertFileId));
     record.seal();
     updateRecords.add(record);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestDeleteHelper.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.data.HoodieJavaRDD;
-import org.apache.hudi.index.bloom.HoodieBloomIndex;
+import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
 
@@ -66,7 +66,7 @@ private enum CombineTestMode {
   private static final int DELETE_PARALLELISM = 200;
 
   @Mock
-  private HoodieBloomIndex index;
+  private HoodieIndex index;
   @Mock
   private HoodieTable<EmptyHoodieRecordPayload, JavaRDD<HoodieRecord>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table;
   @Mock

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java
Patch:
@@ -175,7 +175,7 @@ public void testWriteStatusContentsAfterCompaction() throws Exception {
 
       List<HoodieRecord> updatedRecords = dataGen.generateUpdates(newCommitTime, records);
       JavaRDD<HoodieRecord> updatedRecordsRDD = jsc.parallelize(updatedRecords, 1);
-      HoodieIndex index = new HoodieBloomIndex<>(config, SparkHoodieBloomIndexHelper.getInstance());
+      HoodieIndex index = new HoodieBloomIndex(config, SparkHoodieBloomIndexHelper.getInstance());
       JavaRDD<HoodieRecord> updatedTaggedRecordsRDD = tagLocation(index, updatedRecordsRDD, table);
 
       writeClient.startCommitWithTime(newCommitTime);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableRollback.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
 import org.apache.hudi.common.model.HoodieFileGroup;
 import org.apache.hudi.common.model.HoodieRecord;
+import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -592,7 +593,7 @@ private void assertRecords(List<HoodieRecord> inputRecords, List<GenericRecord>
     Map<String, GenericRecord> expectedRecords = new HashMap<>();
     inputRecords.forEach(entry -> {
       try {
-        expectedRecords.put(entry.getRecordKey(), ((GenericRecord) entry.getData().getInsertValue(HoodieTestDataGenerator.AVRO_SCHEMA).get()));
+        expectedRecords.put(entry.getRecordKey(), (GenericRecord) ((HoodieRecordPayload) entry.getData()).getInsertValue(HoodieTestDataGenerator.AVRO_SCHEMA).get());
       } catch (IOException e) {
         e.printStackTrace();
       }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestBase.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.common.HoodieCleanStat;
 import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
@@ -274,7 +275,7 @@ private Function<Integer, List<HoodieKey>> wrapDeleteKeysGenFunctionForPreppedCa
       final HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(basePath).setLoadActiveTimelineOnLoad(true).build();
       HoodieSparkTable table = HoodieSparkTable.create(writeConfig, context, metaClient);
       JavaRDD<HoodieRecord> recordsToDelete = jsc.parallelize(records, 1)
-          .map(key -> new HoodieRecord(key, new EmptyHoodieRecordPayload()));
+          .map(key -> new HoodieAvroRecord(key, new EmptyHoodieRecordPayload()));
       JavaRDD<HoodieRecord> taggedRecords = tagLocation(index, recordsToDelete, table);
       return taggedRecords.map(record -> record.getKey()).collect();
     };

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.common.table.log;
 
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -384,7 +385,7 @@ private void processDataBlock(HoodieDataBlock dataBlock, Option<List<String>> ke
    * @param partitionName      - Partition name
    * @return HoodieRecord created from the IndexedRecord
    */
-  protected HoodieRecord<?> createHoodieRecord(final IndexedRecord rec, final HoodieTableConfig hoodieTableConfig,
+  protected HoodieAvroRecord<?> createHoodieRecord(final IndexedRecord rec, final HoodieTableConfig hoodieTableConfig,
                                                final String payloadClassFQN, final String preCombineField,
                                                final boolean withOperationField,
                                                final Option<Pair<String, String>> simpleKeyGenFields,

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieMergedLogRecordScanner.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.common.table.log;
 
 import org.apache.hudi.common.config.HoodieCommonConfig;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieOperation;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -144,7 +145,7 @@ protected void processNextRecord(HoodieRecord<? extends HoodieRecordPayload> hoo
       HoodieRecordPayload combinedValue = hoodieRecord.getData().preCombine(oldValue);
       boolean choosePrev = combinedValue.equals(oldValue);
       HoodieOperation operation = choosePrev ? oldRecord.getOperation() : hoodieRecord.getOperation();
-      records.put(key, new HoodieRecord<>(new HoodieKey(key, hoodieRecord.getPartitionPath()), combinedValue, operation));
+      records.put(key, new HoodieAvroRecord<>(new HoodieKey(key, hoodieRecord.getPartitionPath()), combinedValue, operation));
     } else {
       // Put the record as is
       records.put(key, hoodieRecord);

File: hudi-common/src/main/java/org/apache/hudi/common/util/SpillableMapUtils.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.common.util;
 
 import org.apache.hudi.common.fs.SizeAwareDataOutputStream;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieOperation;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -144,7 +145,7 @@ public static <R> R convertToHoodieRecordPayload(GenericRecord record, String pa
     Object preCombineVal = getPreCombineVal(record, preCombineField);
     HoodieOperation operation = withOperationField
         ? HoodieOperation.fromName(getNullableValAsString(record, HoodieRecord.OPERATION_METADATA_FIELD)) : null;
-    HoodieRecord<? extends HoodieRecordPayload> hoodieRecord = new HoodieRecord<>(new HoodieKey(recKey, partitionPath),
+    HoodieRecord<? extends HoodieRecordPayload> hoodieRecord = new HoodieAvroRecord<>(new HoodieKey(recKey, partitionPath),
         ReflectionUtils.loadPayload(payloadClazz, new Object[]{record, preCombineVal}, GenericRecord.class,
             Comparable.class), operation);
 
@@ -170,7 +171,7 @@ private static Object getPreCombineVal(GenericRecord rec, String preCombineField
    * Utility method to convert bytes to HoodieRecord using schema and payload class.
    */
   public static <R> R generateEmptyPayload(String recKey, String partitionPath, String payloadClazz) {
-    HoodieRecord<? extends HoodieRecordPayload> hoodieRecord = new HoodieRecord<>(new HoodieKey(recKey, partitionPath),
+    HoodieRecord<? extends HoodieRecordPayload> hoodieRecord = new HoodieAvroRecord<>(new HoodieKey(recKey, partitionPath),
         ReflectionUtils.loadPayload(payloadClazz, new Object[] {Option.empty()}, Option.class));
     return (R) hoodieRecord;
   }

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hudi.common.config.SerializableConfiguration;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.FileSlice;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -215,7 +216,7 @@ private List<Pair<String, Option<HoodieRecord<HoodieMetadataPayload>>>> readFrom
           // merge base file record w/ log record if present
           if (logRecords.containsKey(key) && logRecords.get(key).isPresent()) {
             HoodieRecordPayload mergedPayload = logRecords.get(key).get().getData().preCombine(hoodieRecord.getData());
-            result.add(Pair.of(key, Option.of(new HoodieRecord(hoodieRecord.getKey(), mergedPayload))));
+            result.add(Pair.of(key, Option.of(new HoodieAvroRecord(hoodieRecord.getKey(), mergedPayload))));
           } else {
             // only base record
             result.add(Pair.of(key, Option.of(hoodieRecord)));

File: hudi-common/src/test/java/org/apache/hudi/common/fs/inline/TestParquetInLining.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.hudi.common.fs.inline;
 
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.testutils.FileSystemTestUtils;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
@@ -144,7 +145,8 @@ static List<GenericRecord> getParquetHoodieRecords() throws IOException {
     List<HoodieRecord> hoodieRecords = dataGenerator.generateInsertsWithHoodieAvroPayload(commitTime, 10);
     List<GenericRecord> toReturn = new ArrayList<>();
     for (HoodieRecord record : hoodieRecords) {
-      toReturn.add((GenericRecord) record.getData().getInsertValue(HoodieTestDataGenerator.AVRO_SCHEMA).get());
+      toReturn.add((GenericRecord) ((HoodieAvroRecord) record).getData()
+          .getInsertValue(HoodieTestDataGenerator.AVRO_SCHEMA).get());
     }
     return toReturn;
   }

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodieRecord.java
Patch:
@@ -44,7 +44,7 @@ public class TestHoodieRecord {
   public void setUp() throws Exception {
     final List<IndexedRecord> indexedRecords = SchemaTestUtil.generateHoodieTestRecords(0, 1);
     final List<HoodieRecord> hoodieRecords =
-        indexedRecords.stream().map(r -> new HoodieRecord(new HoodieKey(UUID.randomUUID().toString(), "0000/00/00"),
+        indexedRecords.stream().map(r -> new HoodieAvroRecord(new HoodieKey(UUID.randomUUID().toString(), "0000/00/00"),
             new AvroBinaryTestPayload(Option.of((GenericRecord) r)))).collect(Collectors.toList());
     hoodieRecord = hoodieRecords.get(0);
   }

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/SpillableMapTestUtils.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.common.testutils;
 
 import org.apache.hudi.common.model.HoodieAvroPayload;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
@@ -48,7 +49,7 @@ public static List<String> upsertRecords(List<IndexedRecord> iRecords,
       String partitionPath = ((GenericRecord) r).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString();
       recordKeys.add(key);
       HoodieRecord record =
-          new HoodieRecord<>(new HoodieKey(key, partitionPath), new HoodieAvroPayload(Option.of((GenericRecord) r)));
+          new HoodieAvroRecord<>(new HoodieKey(key, partitionPath), new HoodieAvroPayload(Option.of((GenericRecord) r)));
       record.unseal();
       record.setCurrentLocation(new HoodieRecordLocation("DUMMY_COMMIT_TIME", "DUMMY_FILE_ID"));
       record.seal();

File: hudi-examples/src/main/java/org/apache/hudi/examples/common/HoodieExampleDataGenerator.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.model.HoodieAvroPayload;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
@@ -126,7 +127,7 @@ public Stream<HoodieRecord<T>> generateInsertsStream(String commitTime, Integer
       kp.partitionPath = partitionPath;
       existingKeys.put(currSize + i, kp);
       numExistingKeys++;
-      return new HoodieRecord<>(key, generateRandomValue(key, commitTime));
+      return new HoodieAvroRecord<>(key, generateRandomValue(key, commitTime));
     });
   }
 
@@ -149,7 +150,7 @@ public List<HoodieRecord<T>> generateUpdates(String commitTime, Integer n) {
   }
 
   public HoodieRecord<T> generateUpdateRecord(HoodieKey key, String commitTime) {
-    return new HoodieRecord<>(key, generateRandomValue(key, commitTime));
+    return new HoodieAvroRecord<>(key, generateRandomValue(key, commitTime));
   }
 
   private Option<String> convertToString(HoodieRecord<T> record) {

File: hudi-flink/src/main/java/org/apache/hudi/sink/bootstrap/BootstrapOperator.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.FileSlice;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordGlobalLocation;
@@ -251,7 +252,7 @@ protected void loadRecords(String partitionPath) throws Exception {
 
   @SuppressWarnings("unchecked")
   public static HoodieRecord generateHoodieRecord(HoodieKey hoodieKey, FileSlice fileSlice) {
-    HoodieRecord hoodieRecord = new HoodieRecord(hoodieKey, null);
+    HoodieRecord hoodieRecord = new HoodieAvroRecord(hoodieKey, null);
     hoodieRecord.setCurrentLocation(new HoodieRecordGlobalLocation(hoodieKey.getPartitionPath(), fileSlice.getBaseInstantTime(), fileSlice.getFileId()));
     hoodieRecord.seal();
 

File: hudi-flink/src/main/java/org/apache/hudi/sink/partitioner/BucketAssignFunction.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.client.common.HoodieFlinkEngineContext;
 import org.apache.hudi.common.config.SerializableConfiguration;
 import org.apache.hudi.common.model.BaseAvroPayload;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordGlobalLocation;
@@ -180,7 +181,7 @@ private void processRecord(HoodieRecord<?> record, Collector<O> out) throws Exce
         if (globalIndex) {
           // if partition path changes, emit a delete record for old partition path,
           // then update the index state using location with new partition path.
-          HoodieRecord<?> deleteRecord = new HoodieRecord<>(new HoodieKey(recordKey, oldLoc.getPartitionPath()),
+          HoodieRecord<?> deleteRecord = new HoodieAvroRecord<>(new HoodieKey(recordKey, oldLoc.getPartitionPath()),
               payloadCreation.createDeletePayload((BaseAvroPayload) record.getData()));
           deleteRecord.setCurrentLocation(oldLoc.toLocal("U"));
           deleteRecord.seal();

File: hudi-flink/src/main/java/org/apache/hudi/sink/transform/RowDataToHoodieFunction.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.sink.transform;
 
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieOperation;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -111,6 +112,6 @@ private HoodieRecord toHoodieRecord(I record) throws Exception {
 
     HoodieRecordPayload payload = payloadCreation.createPayload(gr);
     HoodieOperation operation = HoodieOperation.fromValue(record.getRowKind().toByteValue());
-    return new HoodieRecord<>(hoodieKey, payload, operation);
+    return new HoodieAvroRecord<>(hoodieKey, payload, operation);
   }
 }

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.config.HoodieCommonConfig;
 import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;
@@ -96,9 +97,9 @@ private HoodieMergedLogRecordScanner getMergedLogRecordScanner() throws IOExcept
 
   private Option<GenericRecord> buildGenericRecordwithCustomPayload(HoodieRecord record) throws IOException {
     if (usesCustomPayload) {
-      return record.getData().getInsertValue(getWriterSchema());
+      return ((HoodieAvroRecord) record).getData().getInsertValue(getWriterSchema());
     } else {
-      return record.getData().getInsertValue(getReaderSchema());
+      return ((HoodieAvroRecord) record).getData().getInsertValue(getReaderSchema());
     }
   }
 

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/AbstractConnectWriter.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieAvroPayload;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.util.Option;
@@ -81,7 +82,7 @@ public void writeRecord(SinkRecord record) throws IOException {
     }
 
     // Tag records with a file ID based on kafka partition and hudi partition.
-    HoodieRecord<?> hoodieRecord = new HoodieRecord<>(keyGenerator.getKey(avroRecord.get()), new HoodieAvroPayload(avroRecord));
+    HoodieRecord<?> hoodieRecord = new HoodieAvroRecord<>(keyGenerator.getKey(avroRecord.get()), new HoodieAvroPayload(avroRecord));
     String fileId = KafkaConnectUtils.hashDigest(String.format("%s-%s", record.kafkaPartition(), hoodieRecord.getPartitionPath()));
     hoodieRecord.unseal();
     hoodieRecord.setCurrentLocation(new HoodieRecordLocation(instantTime, fileId));

File: hudi-kafka-connect/src/test/java/org/apache/hudi/writers/TestAbstractConnectWriter.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.HoodieAvroPayload;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.testutils.SchemaTestUtil;
@@ -168,7 +169,7 @@ protected List<WriteStatus> flushRecords() {
   }
 
   private static HoodieRecord convertToHoodieRecords(IndexedRecord iRecord, String key, String partitionPath) {
-    return new HoodieRecord<>(new HoodieKey(key, partitionPath),
+    return new HoodieAvroRecord<>(new HoodieKey(key, partitionPath),
         new HoodieAvroPayload(Option.of((GenericRecord) iRecord)));
   }
 

File: hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi;
 
 import org.apache.hudi.avro.HoodieAvroUtils;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
@@ -149,7 +150,7 @@ public Stream<HoodieRecord> generateInsertsStream(String randomString, Integer n
         existingKeys.put(currSize + i, key);
         numExistingKeys++;
         try {
-          return new HoodieRecord(key, generateRandomValue(key, randomString));
+          return new HoodieAvroRecord(key, generateRandomValue(key, randomString));
         } catch (IOException e) {
           throw new HoodieIOException(e.getMessage(), e);
         }
@@ -165,7 +166,7 @@ public List<HoodieRecord> generateInserts(Integer n) throws IOException {
     }
 
     public HoodieRecord generateUpdateRecord(HoodieKey key, String randomString) throws IOException {
-      return new HoodieRecord(key, generateRandomValue(key, randomString));
+      return new HoodieAvroRecord(key, generateRandomValue(key, randomString));
     }
 
     /**

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestBootstrap.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
@@ -510,7 +511,7 @@ private static JavaRDD<HoodieRecord> generateInputBatch(JavaSparkContext jsc,
           try {
             String key = gr.get("_row_key").toString();
             String pPath = p.getKey();
-            return new HoodieRecord<>(new HoodieKey(key, pPath), new RawTripTestPayload(gr.toString(), key, pPath,
+            return new HoodieAvroRecord<>(new HoodieKey(key, pPath), new RawTripTestPayload(gr.toString(), key, pPath,
                 HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA));
           } catch (IOException e) {
             throw new HoodieIOException(e.getMessage(), e);

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestOrcBootstrap.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.hudi.common.bootstrap.index.BootstrapIndex;
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.config.TypedProperties;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -425,7 +426,7 @@ private static JavaRDD<HoodieRecord> generateInputBatch(JavaSparkContext jsc,
           try {
             String key = gr.get("_row_key").toString();
             String pPath = p.getKey();
-            return new HoodieRecord<>(new HoodieKey(key, pPath), new RawTripTestPayload(gr.toString(), key, pPath,
+            return new HoodieAvroRecord<>(new HoodieKey(key, pPath), new RawTripTestPayload(gr.toString(), key, pPath,
                 HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA));
           } catch (IOException e) {
             throw new HoodieIOException(e.getMessage(), e);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
@@ -197,7 +198,7 @@ protected JavaRDD<HoodieRecord<HoodieRecordPayload>> buildHoodieRecordsForImport
               LOG.warn("Unable to parse date from partition field. Assuming partition as (" + partitionField + ")");
             }
           }
-          return new HoodieRecord<>(new HoodieKey(rowField.toString(), partitionPath),
+          return new HoodieAvroRecord<>(new HoodieKey(rowField.toString(), partitionPath),
               new HoodieJsonPayload(genericRecord.toString()));
         });
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hudi.client.embedded.EmbeddedTimelineService;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
@@ -451,7 +452,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource(
               KeyGeneratorOptions.KEYGENERATOR_CONSISTENT_LOGICAL_TIMESTAMP_ENABLED.key(),
               Boolean.parseBoolean(KeyGeneratorOptions.KEYGENERATOR_CONSISTENT_LOGICAL_TIMESTAMP_ENABLED.defaultValue()))))
           : DataSourceUtils.createPayload(cfg.payloadClassName, gr);
-      return new HoodieRecord<>(keyGenerator.getKey(gr), payload);
+      return new HoodieAvroRecord<>(keyGenerator.getKey(gr), payload);
     });
 
     return Pair.of(schemaProvider, Pair.of(checkpointStr, records));

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/testutils/UtilitiesTestBase.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.config.TypedProperties;
+import org.apache.hudi.common.model.HoodieAvroRecord;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -378,7 +379,7 @@ public static TypedProperties setupSchemaOnDFSWithAbsoluteScope(String scope, St
 
     public static GenericRecord toGenericRecord(HoodieRecord hoodieRecord, Schema schema) {
       try {
-        Option<IndexedRecord> recordOpt = hoodieRecord.getData().getInsertValue(schema);
+        Option<IndexedRecord> recordOpt = ((HoodieAvroRecord) hoodieRecord).getData().getInsertValue(schema);
         return (GenericRecord) recordOpt.get();
       } catch (IOException e) {
         return null;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java
Patch:
@@ -55,11 +55,11 @@ public HoodieGlobalBloomIndex(HoodieWriteConfig config, BaseHoodieBloomIndexHelp
    * Load all involved files as <Partition, filename> pairs from all partitions in the table.
    */
   @Override
-  List<Pair<String, BloomIndexFileInfo>> loadInvolvedFiles(List<String> partitions, final HoodieEngineContext context,
-                                                           final HoodieTable hoodieTable) {
+  List<Pair<String, BloomIndexFileInfo>> loadColumnRangesFromFiles(List<String> partitions, final HoodieEngineContext context,
+                                                                   final HoodieTable hoodieTable) {
     HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();
     List<String> allPartitionPaths = FSUtils.getAllPartitionPaths(context, config.getMetadataConfig(), metaClient.getBasePath());
-    return super.loadInvolvedFiles(allPartitionPaths, context, hoodieTable);
+    return super.loadColumnRangesFromFiles(allPartitionPaths, context, hoodieTable);
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieKeyLocationFetchHandle.java
Patch:
@@ -47,7 +47,7 @@ public class HoodieKeyLocationFetchHandle<T extends HoodieRecordPayload, I, K, O
 
   public HoodieKeyLocationFetchHandle(HoodieWriteConfig config, HoodieTable<T, I, K, O> hoodieTable,
                                       Pair<String, HoodieBaseFile> partitionPathBaseFilePair, Option<BaseKeyGenerator> keyGeneratorOpt) {
-    super(config, null, hoodieTable, Pair.of(partitionPathBaseFilePair.getLeft(), partitionPathBaseFilePair.getRight().getFileId()));
+    super(config, hoodieTable, Pair.of(partitionPathBaseFilePair.getLeft(), partitionPathBaseFilePair.getRight().getFileId()));
     this.partitionPathBaseFilePair = partitionPathBaseFilePair;
     this.keyGeneratorOpt = keyGeneratorOpt;
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java
Patch:
@@ -108,7 +108,7 @@ public HoodieWriteHandle(HoodieWriteConfig config, String instantTime, String pa
   protected HoodieWriteHandle(HoodieWriteConfig config, String instantTime, String partitionPath, String fileId,
                               HoodieTable<T, I, K, O> hoodieTable, Option<Schema> overriddenSchema,
                               TaskContextSupplier taskContextSupplier) {
-    super(config, instantTime, hoodieTable);
+    super(config, Option.of(instantTime), hoodieTable);
     this.partitionPath = partitionPath;
     this.fileId = fileId;
     this.tableSchema = overriddenSchema.orElseGet(() -> getSpecifiedTableSchema(config));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -768,4 +768,7 @@ public <T extends SpecificRecordBase> Option<HoodieTableMetadataWriter> getMetad
     return Option.empty();
   }
 
+  public HoodieTableMetadata getMetadataTable() {
+    return this.metadata;
+  }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/columnstats/ColumnStatsIndexHelper.java
Patch:
@@ -200,7 +200,7 @@ public static Dataset<Row> buildColumnStatsTableFor(
 
                 indexRow.add(minMaxValue.getLeft());      // min
                 indexRow.add(minMaxValue.getRight());     // max
-                indexRow.add(colMetadata.getNumNulls());
+                indexRow.add(colMetadata.getNullCount());
               });
 
               return Row$.MODULE$.apply(JavaConversions.asScalaBuffer(indexRow));

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedTableMetadata.java
Patch:
@@ -247,7 +247,7 @@ private void verifyMetadataRecordKeyExcludeFromPayloadLogFiles(HoodieTable table
     // Compaction should not be triggered yet. Let's verify no base file
     // and few log files available.
     List<FileSlice> fileSlices = table.getSliceView()
-        .getLatestFileSlices(MetadataPartitionType.FILES.partitionPath()).collect(Collectors.toList());
+        .getLatestFileSlices(MetadataPartitionType.FILES.getPartitionPath()).collect(Collectors.toList());
     if (fileSlices.isEmpty()) {
       throw new IllegalStateException("LogFile slices are not available!");
     }
@@ -322,7 +322,7 @@ private void verifyMetadataMergedRecords(HoodieTableMetaClient metadataMetaClien
         .withBasePath(metadataMetaClient.getBasePath())
         .withLogFilePaths(logFilePaths)
         .withLatestInstantTime(latestCommitTimestamp)
-        .withPartition(MetadataPartitionType.FILES.partitionPath())
+        .withPartition(MetadataPartitionType.FILES.getPartitionPath())
         .withReaderSchema(schema)
         .withMaxMemorySizeInBytes(100000L)
         .withBufferSize(4096)
@@ -351,7 +351,7 @@ private void verifyMetadataMergedRecords(HoodieTableMetaClient metadataMetaClien
   private void verifyMetadataRecordKeyExcludeFromPayloadBaseFiles(HoodieTable table) throws IOException {
     table.getHoodieView().sync();
     List<FileSlice> fileSlices = table.getSliceView()
-        .getLatestFileSlices(MetadataPartitionType.FILES.partitionPath()).collect(Collectors.toList());
+        .getLatestFileSlices(MetadataPartitionType.FILES.getPartitionPath()).collect(Collectors.toList());
     if (!fileSlices.get(0).getBaseFile().isPresent()) {
       throw new IllegalStateException("Base file not available!");
     }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieGlobalBloomIndex.java
Patch:
@@ -109,7 +109,7 @@ public void testLoadInvolvedFiles() throws Exception {
     // intentionally missed the partition "2015/03/12" to see if the GlobalBloomIndex can pick it up
     List<String> partitions = Arrays.asList("2016/01/21", "2016/04/01");
     // partitions will NOT be respected by this loadInvolvedFiles(...) call
-    List<Pair<String, BloomIndexFileInfo>> filesList = index.loadInvolvedFiles(partitions, context, hoodieTable);
+    List<Pair<String, BloomIndexFileInfo>> filesList = index.loadColumnRangesFromFiles(partitions, context, hoodieTable);
     // Still 0, as no valid commit
     assertEquals(0, filesList.size());
 
@@ -118,7 +118,7 @@ public void testLoadInvolvedFiles() throws Exception {
         .withInserts("2015/03/12", "3", record1)
         .withInserts("2015/03/12", "4", record2, record3, record4);
 
-    filesList = index.loadInvolvedFiles(partitions, context, hoodieTable);
+    filesList = index.loadColumnRangesFromFiles(partitions, context, hoodieTable);
     assertEquals(4, filesList.size());
 
     Map<String, BloomIndexFileInfo> filesMap = toFileMap(filesList);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestHarness.java
Patch:
@@ -59,7 +59,6 @@
 import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;
 import org.apache.hudi.metadata.HoodieTableMetadata;
 import org.apache.hudi.metadata.HoodieTableMetadataWriter;
-import org.apache.hudi.metadata.MetadataPartitionType;
 import org.apache.hudi.metadata.SparkHoodieBackedTableMetadataWriter;
 import org.apache.hudi.table.HoodieSparkTable;
 import org.apache.hudi.table.HoodieTable;
@@ -680,7 +679,7 @@ private void runFullValidation(HoodieWriteConfig writeConfig, String metadataTab
     // in the .hoodie folder.
     List<String> metadataTablePartitions = FSUtils.getAllPartitionPaths(engineContext, HoodieTableMetadata.getMetadataTableBasePath(basePath),
         false, false);
-    Assertions.assertEquals(MetadataPartitionType.values().length, metadataTablePartitions.size());
+    Assertions.assertEquals(metadataWriter.getEnabledPartitionTypes().size(), metadataTablePartitions.size());
 
     // Metadata table should automatically compact and clean
     // versions are +1 as autoClean / compaction happens end of commits

File: hudi-common/src/main/java/org/apache/hudi/common/bloom/HoodieDynamicBoundedBloomFilter.java
Patch:
@@ -63,7 +63,7 @@ public class HoodieDynamicBoundedBloomFilter implements BloomFilter {
    * @param serString the serialized string which represents the {@link HoodieDynamicBoundedBloomFilter}
    * @param typeCode  type code of the bloom filter
    */
-  HoodieDynamicBoundedBloomFilter(String serString, BloomFilterTypeCode typeCode) {
+  public HoodieDynamicBoundedBloomFilter(String serString, BloomFilterTypeCode typeCode) {
     // ignoring the type code for now, since we have just one version
     byte[] bytes = Base64CodecUtil.decode(serString);
     DataInputStream dis = new DataInputStream(new ByteArrayInputStream(bytes));

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java
Patch:
@@ -175,11 +175,11 @@ protected String getKeyField() {
     return this.simpleKeyGenFields.get().getKey();
   }
 
-  public void scan() {
+  public synchronized void scan() {
     scan(Option.empty());
   }
 
-  public void scan(Option<List<String>> keys) {
+  public synchronized void scan(Option<List<String>> keys) {
     currentInstantLogBlocks = new ArrayDeque<>();
     progress = 0.0f;
     totalLogFiles = new AtomicLong(0);

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMergedLogRecordReader.java
Patch:
@@ -116,7 +116,7 @@ public static HoodieMetadataMergedLogRecordReader.Builder newBuilder() {
    * @param key Key of the record to retrieve
    * @return {@code HoodieRecord} if key was found else {@code Option.empty()}
    */
-  public List<Pair<String, Option<HoodieRecord<HoodieMetadataPayload>>>> getRecordByKey(String key) {
+  public synchronized List<Pair<String, Option<HoodieRecord<HoodieMetadataPayload>>>> getRecordByKey(String key) {
     return Collections.singletonList(Pair.of(key, Option.ofNullable((HoodieRecord) records.get(key))));
   }
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMetrics.java
Patch:
@@ -41,6 +41,8 @@ public class HoodieMetadataMetrics implements Serializable {
   // Metric names
   public static final String LOOKUP_PARTITIONS_STR = "lookup_partitions";
   public static final String LOOKUP_FILES_STR = "lookup_files";
+  public static final String LOOKUP_BLOOM_FILTERS_METADATA_STR = "lookup_meta_index_bloom_filters";
+  public static final String LOOKUP_COLUMN_STATS_METADATA_STR = "lookup_meta_index_column_ranges";
   public static final String SCAN_STR = "scan";
   public static final String BASEFILE_READ_STR = "basefile_read";
   public static final String INITIALIZE_STR = "initialize";
@@ -77,7 +79,7 @@ private Map<String, String> getStats(HoodieTableFileSystemView fsView, boolean d
     Map<String, String> stats = new HashMap<>();
 
     // Total size of the metadata and count of base/log files
-    for (String metadataPartition : MetadataPartitionType.all()) {
+    for (String metadataPartition : MetadataPartitionType.allPaths()) {
       List<FileSlice> latestSlices = fsView.getLatestFileSlices(metadataPartition).collect(Collectors.toList());
 
       // Total size of the metadata and count of base/log files

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java
Patch:
@@ -639,8 +639,8 @@ public boolean rollback(final String commitInstantTime, Option<HoodiePendingRoll
           .findFirst());
       if (commitInstantOpt.isPresent()) {
         LOG.info("Scheduling Rollback at instant time :" + rollbackInstantTime);
-        Option<HoodieRollbackPlan> rollbackPlanOption = pendingRollbackInfo.map(entry -> Option.of(entry.getRollbackPlan())).orElse(table.scheduleRollback(context, rollbackInstantTime,
-            commitInstantOpt.get(), false, config.shouldRollbackUsingMarkers()));
+        Option<HoodieRollbackPlan> rollbackPlanOption = pendingRollbackInfo.map(entry -> Option.of(entry.getRollbackPlan()))
+            .orElseGet(() -> table.scheduleRollback(context, rollbackInstantTime, commitInstantOpt.get(), false, config.shouldRollbackUsingMarkers()));
         if (rollbackPlanOption.isPresent()) {
           // execute rollback
           HoodieRollbackMetadata rollbackMetadata = table.rollback(context, rollbackInstantTime, commitInstantOpt.get(), true,

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/utils/TestMetadataConversionUtils.java
Patch:
@@ -188,7 +188,6 @@ private void createRollbackMetadata(String instantTime) throws Exception {
     rollbackPartitionMetadata.setPartitionPath("p1");
     rollbackPartitionMetadata.setSuccessDeleteFiles(Arrays.asList("f1"));
     rollbackPartitionMetadata.setFailedDeleteFiles(new ArrayList<>());
-    rollbackPartitionMetadata.setWrittenLogFiles(new HashMap<>());
     rollbackPartitionMetadata.setRollbackLogFiles(new HashMap<>());
     Map<String, HoodieRollbackPartitionMetadata> partitionMetadataMap = new HashMap<>();
     partitionMetadataMap.put("p1", rollbackPartitionMetadata);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestMarkerBasedRollbackStrategy.java
Patch:
@@ -133,7 +133,6 @@ public void testCopyOnWriteRollback(boolean useFileListingMetadata) throws Excep
         assertEquals(1, stat.getSuccessDeleteFiles().size());
         assertEquals(0, stat.getFailedDeleteFiles().size());
         assertEquals(0, stat.getCommandBlocksCount().size());
-        assertEquals(0, stat.getWrittenLogFileSizeMap().size());
       }
     }
   }
@@ -162,8 +161,6 @@ public void testMergeOnReadRollback(boolean useFileListingMetadata) throws Excep
         assertEquals(0, stat.getFailedDeleteFiles().size());
         assertEquals(1, stat.getCommandBlocksCount().size());
         stat.getCommandBlocksCount().forEach((fileStatus, len) -> assertTrue(fileStatus.getPath().getName().contains(HoodieFileFormat.HOODIE_LOG.getFileExtension())));
-        assertEquals(1, stat.getWrittenLogFileSizeMap().size());
-        stat.getWrittenLogFileSizeMap().forEach((fileStatus, len) -> assertTrue(fileStatus.getPath().getName().contains(HoodieFileFormat.HOODIE_LOG.getFileExtension())));
       }
     }
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimelineMetadataUtils.java
Patch:
@@ -77,10 +77,8 @@ public static HoodieRollbackMetadata convertRollbackMetadata(String startRollbac
     for (HoodieRollbackStat stat : rollbackStats) {
       Map<String, Long> rollbackLogFiles = stat.getCommandBlocksCount().keySet().stream()
           .collect(Collectors.toMap(f -> f.getPath().toString(), FileStatus::getLen));
-      Map<String, Long> probableLogFiles = stat.getWrittenLogFileSizeMap().keySet().stream()
-          .collect(Collectors.toMap(f -> f.getPath().toString(), FileStatus::getLen));
       HoodieRollbackPartitionMetadata metadata = new HoodieRollbackPartitionMetadata(stat.getPartitionPath(),
-          stat.getSuccessDeleteFiles(), stat.getFailedDeleteFiles(), rollbackLogFiles, probableLogFiles);
+          stat.getSuccessDeleteFiles(), stat.getFailedDeleteFiles(), rollbackLogFiles);
       partitionMetadataBuilder.put(stat.getPartitionPath(), metadata);
       totalDeleted += stat.getSuccessDeleteFiles().size();
     }

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestTimelineUtils.java
Patch:
@@ -270,8 +270,7 @@ private HoodieRollbackMetadata getRollbackMetadataInstance(String basePath, Stri
     List<HoodieInstant> rollbacks = new ArrayList<>();
     rollbacks.add(new HoodieInstant(false, actionType, commitTs));
 
-    HoodieRollbackStat rollbackStat = new HoodieRollbackStat(partition, deletedFiles, Collections.emptyList(), Collections.emptyMap(),
-        Collections.EMPTY_MAP);
+    HoodieRollbackStat rollbackStat = new HoodieRollbackStat(partition, deletedFiles, Collections.emptyList(), Collections.emptyMap());
     List<HoodieRollbackStat> rollbackStats = new ArrayList<>();
     rollbackStats.add(rollbackStat);
     return TimelineMetadataUtils.convertRollbackMetadata(commitTs, Option.empty(), rollbacks, rollbackStats);

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java
Patch:
@@ -556,7 +556,7 @@ private void performRestore(HoodieInstant instant, List<String> files, String ro
       boolean isRestore) throws IOException {
     Map<String, List<String>> partititonToFiles = deleteFiles(files);
     List<HoodieRollbackStat> rollbackStats = partititonToFiles.entrySet().stream().map(e ->
-        new HoodieRollbackStat(e.getKey(), e.getValue(), new ArrayList<>(), new HashMap<>(), new HashMap<>())
+        new HoodieRollbackStat(e.getKey(), e.getValue(), new ArrayList<>(), new HashMap<>())
     ).collect(Collectors.toList());
 
     List<HoodieInstant> rollbacks = new ArrayList<>();

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestTable.java
Patch:
@@ -354,7 +354,6 @@ public HoodieRollbackMetadata getRollbackMetadata(String instantTimeToDelete, Ma
       rollbackPartitionMetadata.setPartitionPath(entry.getKey());
       rollbackPartitionMetadata.setSuccessDeleteFiles(entry.getValue());
       rollbackPartitionMetadata.setFailedDeleteFiles(new ArrayList<>());
-      rollbackPartitionMetadata.setWrittenLogFiles(getWrittenLogFiles(instantTimeToDelete, entry));
       long rollbackLogFileSize = 50 + RANDOM.nextInt(500);
       String fileId = UUID.randomUUID().toString();
       String logFileName = logFileName(instantTimeToDelete, fileId, 0);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java
Patch:
@@ -83,7 +83,7 @@ public class HoodieClusteringConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> PLAN_STRATEGY_SMALL_FILE_LIMIT = ConfigProperty
       .key(CLUSTERING_STRATEGY_PARAM_PREFIX + "small.file.limit")
-      .defaultValue(String.valueOf(600 * 1024 * 1024L))
+      .defaultValue(String.valueOf(300 * 1024 * 1024L))
       .sinceVersion("0.7.0")
       .withDocumentation("Files smaller than the size specified here are candidates for clustering");
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
Patch:
@@ -200,7 +200,7 @@ public class HoodieCompactionConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> COMPACTION_LAZY_BLOCK_READ_ENABLE = ConfigProperty
       .key("hoodie.compaction.lazy.block.read")
-      .defaultValue("false")
+      .defaultValue("true")
       .withDocumentation("When merging the delta log files, this config helps to choose whether the log blocks "
           + "should be read lazily or not. Choose true to use lazy block reading (low memory usage, but incurs seeks to each block"
           + " header) or false for immediate block read (higher memory usage)");

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -370,6 +370,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource(
           .initTable(new Configuration(jssc.hadoopConfiguration()), cfg.targetBasePath);
     }
 
+    LOG.debug("Checkpoint from config: " + cfg.checkpoint);
     if (!resumeCheckpointStr.isPresent() && cfg.checkpoint != null) {
       resumeCheckpointStr = Option.of(cfg.checkpoint);
     }
@@ -470,6 +471,7 @@ private Option<String> getCheckpointToResume(Option<HoodieTimeline> commitTimeli
       Option<HoodieCommitMetadata> commitMetadataOption = getLatestCommitMetadataWithValidCheckpointInfo(commitTimelineOpt.get());
       if (commitMetadataOption.isPresent()) {
         HoodieCommitMetadata commitMetadata = commitMetadataOption.get();
+        LOG.debug("Checkpoint reset from metadata: " + commitMetadata.getMetadata(CHECKPOINT_RESET_KEY));
         if (cfg.checkpoint != null && (StringUtils.isNullOrEmpty(commitMetadata.getMetadata(CHECKPOINT_RESET_KEY))
             || !cfg.checkpoint.equals(commitMetadata.getMetadata(CHECKPOINT_RESET_KEY)))) {
           resumeCheckpointStr = Option.of(cfg.checkpoint);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/SerializableHoodieRollbackRequest.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.avro.model.HoodieRollbackRequest;
 
+import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
@@ -29,7 +30,7 @@
  * HoodieRollbackRequest in HoodieRollbackPlan (avro pojo) is not operable direclty within spark parallel engine.
  * Hence converting the same to this {@link SerializableHoodieRollbackRequest} and then using it within spark.parallelize.
  */
-public class SerializableHoodieRollbackRequest {
+public class SerializableHoodieRollbackRequest implements Serializable {
 
   private final String partitionPath;
   private final String fileId;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieFileWriterFactory.java
Patch:
@@ -87,7 +87,8 @@ private static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFi
     BloomFilter filter = createBloomFilter(config);
     HoodieHFileConfig hfileConfig = new HoodieHFileConfig(hoodieTable.getHadoopConf(),
         config.getHFileCompressionAlgorithm(), config.getHFileBlockSize(), config.getHFileMaxFileSize(),
-        PREFETCH_ON_OPEN, CACHE_DATA_IN_L1, DROP_BEHIND_CACHE_COMPACTION, filter, HFILE_COMPARATOR);
+        HoodieHFileReader.KEY_FIELD_NAME, PREFETCH_ON_OPEN, CACHE_DATA_IN_L1, DROP_BEHIND_CACHE_COMPACTION,
+        filter, HFILE_COMPARATOR);
 
     return new HoodieHFileWriter<>(instantTime, path, hfileConfig, schema, taskContextSupplier, config.populateMetaFields());
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataKeyGenerator.java
Patch:
@@ -42,7 +42,7 @@ public HoodieTableMetadataKeyGenerator(TypedProperties config) {
 
   @Override
   public String getRecordKey(GenericRecord record) {
-    return KeyGenUtils.getRecordKey(record, HoodieMetadataPayload.SCHEMA_FIELD_ID_KEY, isConsistentLogicalTimestampEnabled());
+    return KeyGenUtils.getRecordKey(record, HoodieMetadataPayload.KEY_FIELD_NAME, isConsistentLogicalTimestampEnabled());
   }
 
   @Override

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/io/storage/TestHoodieHFileReaderWriter.java
Patch:
@@ -103,7 +103,7 @@ private HoodieHFileWriter createHFileWriter(Schema avroSchema, boolean populateM
     String instantTime = "000";
 
     HoodieHFileConfig hoodieHFileConfig = new HoodieHFileConfig(conf, Compression.Algorithm.GZ, 1024 * 1024, 120 * 1024 * 1024,
-        PREFETCH_ON_OPEN, CACHE_DATA_IN_L1, DROP_BEHIND_CACHE_COMPACTION, filter, HFILE_COMPARATOR);
+        HoodieHFileReader.KEY_FIELD_NAME, PREFETCH_ON_OPEN, CACHE_DATA_IN_L1, DROP_BEHIND_CACHE_COMPACTION, filter, HFILE_COMPARATOR);
     return new HoodieHFileWriter(instantTime, filePath, hoodieHFileConfig, avroSchema, mockTaskContextSupplier, populateMetaFields);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -126,7 +126,7 @@ public final class HoodieMetadataConfig extends HoodieConfig {
 
   public static final ConfigProperty<Boolean> POPULATE_META_FIELDS = ConfigProperty
       .key(METADATA_PREFIX + ".populate.meta.fields")
-      .defaultValue(true)
+      .defaultValue(false)
       .sinceVersion("0.10.0")
       .withDocumentation("When enabled, populates all meta fields. When disabled, no meta fields are populated.");
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMergedLogRecordReader.java
Patch:
@@ -139,7 +139,7 @@ public synchronized List<Pair<String, Option<HoodieRecord<HoodieMetadataPayload>
 
   @Override
   protected String getKeyField() {
-    return HoodieMetadataPayload.SCHEMA_FIELD_ID_KEY;
+    return HoodieMetadataPayload.KEY_FIELD_NAME;
   }
 
   /**

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/testutils/InputFormatTestUtil.java
Patch:
@@ -363,7 +363,8 @@ public static HoodieLogFormat.Writer writeDataBlockToLogFile(File partitionDir,
     Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, newCommit);
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, writeSchema.toString());
-    HoodieDataBlock dataBlock = (logBlockType == HoodieLogBlock.HoodieLogBlockType.HFILE_DATA_BLOCK) ? new HoodieHFileDataBlock(records, header) :
+    HoodieDataBlock dataBlock = (logBlockType == HoodieLogBlock.HoodieLogBlockType.HFILE_DATA_BLOCK)
+        ? new HoodieHFileDataBlock(records, header, HoodieRecord.RECORD_KEY_METADATA_FIELD) :
         new HoodieAvroDataBlock(records, header);
     writer.appendBlock(dataBlock);
     return writer;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackRequest.java
Patch:
@@ -20,10 +20,12 @@
 
 import org.apache.hudi.common.util.Option;
 
+import java.io.Serializable;
+
 /**
  * Request for performing one rollback action.
  */
-public class ListingBasedRollbackRequest {
+public class ListingBasedRollbackRequest implements Serializable {
 
   /**
    * Rollback commands, that trigger a specific handling for rollback.

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java
Patch:
@@ -122,13 +122,13 @@ public Pair<Option<Dataset<Row>>, String> fetchNextBatch(Option<String> lastCkpt
     }
 
     String filter = "s3.object.size > 0";
-    if (!StringUtils.isNullOrEmpty(props.getString(Config.S3_KEY_PREFIX))) {
+    if (!StringUtils.isNullOrEmpty(props.getString(Config.S3_KEY_PREFIX, null))) {
       filter = filter + " and s3.object.key like '" + props.getString(Config.S3_KEY_PREFIX) + "%'";
     }
-    if (!StringUtils.isNullOrEmpty(props.getString(Config.S3_IGNORE_KEY_PREFIX))) {
+    if (!StringUtils.isNullOrEmpty(props.getString(Config.S3_IGNORE_KEY_PREFIX, null))) {
       filter = filter + " and s3.object.key not like '" + props.getString(Config.S3_IGNORE_KEY_PREFIX) + "%'";
     }
-    if (!StringUtils.isNullOrEmpty(props.getString(Config.S3_IGNORE_KEY_SUBSTRING))) {
+    if (!StringUtils.isNullOrEmpty(props.getString(Config.S3_IGNORE_KEY_SUBSTRING, null))) {
       filter = filter + " and s3.object.key not like '%" + props.getString(Config.S3_IGNORE_KEY_SUBSTRING) + "%'";
     }
     // add file format filtering by default

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -535,7 +535,7 @@ private List<DirectoryInfo> listAllPartitions(HoodieTableMetaClient datasetMetaC
    * All FileGroups for a given metadata partition has a fixed prefix as per the {@link MetadataPartitionType#getFileIdPrefix()}.
    * Each file group is suffixed with 4 digits with increments of 1 starting with 0000.
    *
-   * Lets say we configure 10 file groups for record level index partittion, and prefix as "record-index-bucket-"
+   * Lets say we configure 10 file groups for record level index partition, and prefix as "record-index-bucket-"
    * File groups will be named as :
    *    record-index-bucket-0000, .... -> ..., record-index-bucket-0009
    */

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -892,7 +892,8 @@ public void testMetadataMultiWriter() throws Exception {
 
     Properties properties = new Properties();
     properties.setProperty(FILESYSTEM_LOCK_PATH_PROP_KEY, basePath + "/.hoodie/.locks");
-    properties.setProperty(LockConfiguration.LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY,"3000");
+    properties.setProperty(LockConfiguration.LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY,"1000");
+    properties.setProperty(LockConfiguration.LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP_KEY,"20");
     HoodieWriteConfig writeConfig = getWriteConfigBuilder(true, true, false)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder()
             .withFailedWritesCleaningPolicy(HoodieFailedWritesCleaningPolicy.LAZY).withAutoClean(false).build())

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -288,7 +288,7 @@ public class HoodieWriteConfig extends HoodieConfig {
   public static final ConfigProperty<String> BULK_INSERT_SORT_MODE = ConfigProperty
       .key("hoodie.bulkinsert.sort.mode")
       .defaultValue(BulkInsertSortMode.GLOBAL_SORT.toString())
-      .withDocumentation("Sorting modes to use for sorting records for bulk insert. This is user when user "
+      .withDocumentation("Sorting modes to use for sorting records for bulk insert. This is use when user "
           + BULKINSERT_USER_DEFINED_PARTITIONER_CLASS_NAME.key() + "is not configured. Available values are - "
           + "GLOBAL_SORT: this ensures best file sizes, with lowest memory overhead at cost of sorting. "
           + "PARTITION_SORT: Strikes a balance by only sorting within a partition, still keeping the memory overhead of writing "

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLayoutConfig.java
Patch:
@@ -80,7 +80,7 @@ public HoodieLayoutConfig build() {
     }
 
     private void setDefault() {
-      if (layoutConfig.getString(HoodieIndexConfig.INDEX_TYPE.key()).equals(HoodieIndex.IndexType.BUCKET.name())) {
+      if (layoutConfig.contains(HoodieIndexConfig.INDEX_TYPE.key()) && layoutConfig.getString(HoodieIndexConfig.INDEX_TYPE.key()).equals(HoodieIndex.IndexType.BUCKET.name())) {
         layoutConfig.setDefaultValue(LAYOUT_TYPE, HoodieStorageLayout.LayoutType.BUCKET.name());
       }
       layoutConfig.setDefaultValue(LAYOUT_TYPE, LAYOUT_TYPE.defaultValue());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBaseBloomIndexCheckFunction.java
Patch:
@@ -49,8 +49,8 @@ public HoodieBaseBloomIndexCheckFunction(HoodieTable hoodieTable, HoodieWriteCon
   }
 
   @Override
-  public Iterator<List<KeyLookupResult>> apply(Iterator<Pair<String, HoodieKey>> fileParitionRecordKeyTripletItr) {
-    return new LazyKeyCheckIterator(fileParitionRecordKeyTripletItr);
+  public Iterator<List<KeyLookupResult>> apply(Iterator<Pair<String, HoodieKey>> filePartitionRecordKeyTripletItr) {
+    return new LazyKeyCheckIterator(filePartitionRecordKeyTripletItr);
   }
 
   class LazyKeyCheckIterator extends LazyIterableIterator<Pair<String, HoodieKey>, List<KeyLookupResult>> {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkValidatorUtils.java
Patch:
@@ -138,7 +138,7 @@ public static Dataset<Row> readRecordsForBaseFiles(SQLContext sqlContext, List<S
   }
 
   /**
-   * Get reads from paritions modified including any inflight commits.
+   * Get reads from partitions modified including any inflight commits.
    * Note that this only works for COW tables
    */
   public static Dataset<Row> getRecordsFromPendingCommits(SQLContext sqlContext, 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndexCheckFunction.java
Patch:
@@ -53,8 +53,8 @@ public HoodieBloomIndexCheckFunction(HoodieTable hoodieTable, HoodieWriteConfig
 
   @Override
   public Iterator<List<KeyLookupResult>> call(Integer partition,
-      Iterator<Tuple2<String, HoodieKey>> fileParitionRecordKeyTripletItr) {
-    return new LazyKeyCheckIterator(fileParitionRecordKeyTripletItr);
+      Iterator<Tuple2<String, HoodieKey>> filePartitionRecordKeyTripletItr) {
+    return new LazyKeyCheckIterator(filePartitionRecordKeyTripletItr);
   }
 
   class LazyKeyCheckIterator extends LazyIterableIterator<Tuple2<String, HoodieKey>, List<KeyLookupResult>> {

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestHoodieDatasetBulkInsertHelper.java
Patch:
@@ -186,14 +186,14 @@ public void testBulkInsertPreCombine(boolean enablePreCombine) {
     }
 
     int metadataRecordKeyIndex = resultSchema.fieldIndex(HoodieRecord.RECORD_KEY_METADATA_FIELD);
-    int metadataParitionPathIndex = resultSchema.fieldIndex(HoodieRecord.PARTITION_PATH_METADATA_FIELD);
+    int metadataPartitionPathIndex = resultSchema.fieldIndex(HoodieRecord.PARTITION_PATH_METADATA_FIELD);
     int metadataCommitTimeIndex = resultSchema.fieldIndex(HoodieRecord.COMMIT_TIME_METADATA_FIELD);
     int metadataCommitSeqNoIndex = resultSchema.fieldIndex(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD);
     int metadataFilenameIndex = resultSchema.fieldIndex(HoodieRecord.FILENAME_METADATA_FIELD);
 
     result.toJavaRDD().foreach(entry -> {
       assertTrue(entry.get(metadataRecordKeyIndex).equals(entry.getAs("_row_key")));
-      assertTrue(entry.get(metadataParitionPathIndex).equals(entry.getAs("partition")));
+      assertTrue(entry.get(metadataPartitionPathIndex).equals(entry.getAs("partition")));
       assertTrue(entry.get(metadataCommitSeqNoIndex).equals(""));
       assertTrue(entry.get(metadataCommitTimeIndex).equals(""));
       assertTrue(entry.get(metadataFilenameIndex).equals(""));

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveStylePartitionValueExtractor.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.List;
 
 /**
- * Extractor for Hive Style Partitioned tables, when the parition folders are key value pairs.
+ * Extractor for Hive Style Partitioned tables, when the partition folders are key value pairs.
  *
  * <p>This implementation extracts the partition value of yyyy-mm-dd from the path of type datestr=yyyy-mm-dd.
  */

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java
Patch:
@@ -131,9 +131,9 @@ public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(JavaS
     long lastCheckpointTime = lastCheckpointStr.map(Long::parseLong).orElse(Long.MIN_VALUE);
     HoodieSparkEngineContext context = new HoodieSparkEngineContext(sparkContext);
     SerializableConfiguration serializedConf = new SerializableConfiguration(fs.getConf());
-    List<String> prunedParitionPaths = pruneDatePartitionPaths(context, fs, props.getString(ROOT_INPUT_PATH_PROP), currentDate);
+    List<String> prunedPartitionPaths = pruneDatePartitionPaths(context, fs, props.getString(ROOT_INPUT_PATH_PROP), currentDate);
 
-    List<FileStatus> eligibleFiles = context.flatMap(prunedParitionPaths,
+    List<FileStatus> eligibleFiles = context.flatMap(prunedPartitionPaths,
         path -> {
           FileSystem fs = new Path(path).getFileSystem(serializedConf.get());
           return listEligibleFiles(fs, new Path(path), lastCheckpointTime).stream();

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java
Patch:
@@ -596,6 +596,7 @@ public static Object getRecordColumnValues(HoodieRecord<? extends HoodieRecordPa
       if (columns.length == 1) {
         return HoodieAvroUtils.getNestedFieldVal(genericRecord, columns[0], true, consistentLogicalTimestampEnabled);
       } else {
+        // TODO this is inefficient, instead we can simply return array of Comparable
         StringBuilder sb = new StringBuilder();
         for (String col : columns) {
           sb.append(HoodieAvroUtils.getNestedFieldValAsString(genericRecord, col, true, consistentLogicalTimestampEnabled));

File: hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java
Patch:
@@ -48,6 +48,7 @@
 import org.apache.hadoop.security.Credentials;
 import org.apache.hadoop.security.token.Token;
 import org.apache.hadoop.util.Progressable;
+import org.apache.hudi.hadoop.FileNameCachingPath;
 
 import java.io.IOException;
 import java.net.URI;
@@ -141,7 +142,7 @@ private static Path convertPathWithScheme(Path oldPath, String newScheme) {
     try {
       newURI = new URI(newScheme, oldURI.getUserInfo(), oldURI.getHost(), oldURI.getPort(), oldURI.getPath(),
           oldURI.getQuery(), oldURI.getFragment());
-      return new Path(newURI);
+      return new FileNameCachingPath(newURI);
     } catch (URISyntaxException e) {
       // TODO - Better Exception handling
       throw new RuntimeException(e);

File: hudi-common/src/main/java/org/apache/hudi/common/util/ObjectSizeCalculator.java
Patch:
@@ -90,7 +90,7 @@ public static long getObjectSize(Object obj) throws UnsupportedOperationExceptio
   private final Map<Class<?>, ClassSizeInfo> classSizeInfos = new IdentityHashMap<>();
 
   private final Set<Object> alreadyVisited = Collections.newSetFromMap(new IdentityHashMap<>());
-  private final Deque<Object> pending = new ArrayDeque<>(16 * 1024);
+  private final Deque<Object> pending = new ArrayDeque<>(64);
   private long size;
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java
Patch:
@@ -464,7 +464,7 @@ protected void postCommit(HoodieTable<T, I, K, O> table, HoodieCommitMetadata me
   }
 
   protected void runTableServicesInline(HoodieTable<T, I, K, O> table, HoodieCommitMetadata metadata, Option<Map<String, String>> extraMetadata) {
-    if (config.inlineTableServices()) {
+    if (config.areAnyTableServicesInline()) {
       if (config.isMetadataTableEnabled()) {
         table.getHoodieView().sync();
       }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/storage/HoodieBucketLayout.java
Patch:
@@ -40,7 +40,8 @@ public class HoodieBucketLayout extends HoodieStorageLayout {
       add(WriteOperationType.DELETE);
       add(WriteOperationType.COMPACT);
       add(WriteOperationType.DELETE_PARTITION);
-    }};
+    }
+  };
 
   public HoodieBucketLayout(HoodieWriteConfig config) {
     super(config);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/MetadataBootstrapHandlerFactory.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.avro.model.HoodieFileStatus;
+
 import static org.apache.hudi.common.model.HoodieFileFormat.ORC;
 import static org.apache.hudi.common.model.HoodieFileFormat.PARQUET;
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/util/DataTypeUtils.java
Patch:
@@ -63,7 +63,8 @@ public class DataTypeUtils {
         // String types
         put(StringType$.class,
             newHashSet(VarcharType$.class, StringType$.class));
-      }};
+      }
+  };
 
   /**
    * Validates whether one {@link StructType} is compatible w/ the other one.

File: hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java
Patch:
@@ -52,6 +52,7 @@
 import org.apache.orc.storage.serde2.io.DateWritable;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.orc.TypeDescription;
+
 import static org.apache.avro.JsonProperties.NULL_VALUE;
 
 /**

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestRatelimiter.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.util.concurrent.TimeUnit;
 import org.junit.jupiter.api.Test;
+
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/DagUtils.java
Patch:
@@ -106,6 +106,7 @@ public static WorkflowDag convertYamlToDag(String yaml) throws IOException {
             Entry<String, JsonNode> dagContentNode = contentItr.next();
             allNodes.put(dagContentNode.getKey(), convertJsonToDagNode(allNodes, dagContentNode.getKey(), dagContentNode.getValue()));
           }
+          break;
         default:
           break;
       }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/BaseValidateDatasetNode.java
Patch:
@@ -161,7 +161,6 @@ private Dataset<Row> getInputDf(ExecutionContext context, SparkSession session,
         .filter("_hoodie_is_deleted != true");
   }
 
-
   private ExpressionEncoder getEncoder(StructType schema) {
     List<Attribute> attributes = JavaConversions.asJavaCollection(schema.toAttributes()).stream()
         .map(Attribute::toAttribute).collect(Collectors.toList());

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/helpers/DFSTestSuitePathSelector.java
Patch:
@@ -38,7 +38,6 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
-import java.util.Optional;
 import java.util.stream.Collectors;
 
 /**
@@ -74,10 +73,10 @@ public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(
       // Say input data is as follow input/1, input/2, input/5 since 3,4 was rolled back and 5 is new generated data
       // checkpoint from the latest commit metadata will be 2 since 3,4 has been rolled back. We need to set the
       // next batch id correctly as 5 instead of 3
-      Optional<String> correctBatchIdDueToRollback = Arrays.stream(fileStatuses)
+      Option<String> correctBatchIdDueToRollback = Option.fromJavaOptional(Arrays.stream(fileStatuses)
           .map(f -> f.getPath().toString().split("/")[f.getPath().toString().split("/").length - 1])
           .filter(bid1 -> Integer.parseInt(bid1) > lastBatchId)
-          .min((bid1, bid2) -> Integer.min(Integer.parseInt(bid1), Integer.parseInt(bid2)));
+          .min((bid1, bid2) -> Integer.min(Integer.parseInt(bid1), Integer.parseInt(bid2))));
       if (correctBatchIdDueToRollback.isPresent() && Integer.parseInt(correctBatchIdDueToRollback.get()) > nextBatchId) {
         nextBatchId = Integer.parseInt(correctBatchIdDueToRollback.get());
       }

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/job/TestHoodieTestSuiteJob.java
Patch:
@@ -35,7 +35,6 @@
 import org.apache.hudi.integ.testsuite.schema.TestSuiteFileBasedSchemaProvider;
 import org.apache.hudi.integ.testsuite.writer.DeltaOutputMode;
 import org.apache.hudi.keygen.TimestampBasedKeyGenerator;
-import org.apache.hudi.utilities.schema.FilebasedSchemaProvider;
 import org.apache.hudi.utilities.sources.AvroDFSSource;
 import org.apache.hudi.utilities.testutils.UtilitiesTestBase;
 

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestMultiPartKeysValueExtractor.java
Patch:
@@ -21,6 +21,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import org.junit.jupiter.api.Test;
+
 import static org.junit.jupiter.api.Assertions.assertEquals;
 
 public class TestMultiPartKeysValueExtractor {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -689,7 +689,7 @@ protected void compactIfNecessary(AbstractHoodieWriteClient writeClient, String
     String latestDeltacommitTime = metadataMetaClient.reloadActiveTimeline().getDeltaCommitTimeline().filterCompletedInstants().lastInstant()
         .get().getTimestamp();
     List<HoodieInstant> pendingInstants = dataMetaClient.reloadActiveTimeline().filterInflightsAndRequested()
-        .findInstantsBefore(latestDeltacommitTime).getInstants().collect(Collectors.toList());
+        .findInstantsBefore(instantTime).getInstants().collect(Collectors.toList());
 
     if (!pendingInstants.isEmpty()) {
       LOG.info(String.format("Cannot compact metadata table as there are %d inflight instants before latest deltacommit %s: %s",

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkCopyOnWriteTableArchiveWithReplace.java
Patch:
@@ -79,8 +79,8 @@ public void testDeletePartitionAndArchive(boolean metadataEnabled) throws IOExce
       client.startCommitWithTime(instantTime4, HoodieActiveTimeline.REPLACE_COMMIT_ACTION);
       client.deletePartitions(Arrays.asList(DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH), instantTime4);
 
-      // 2nd write batch; 3 commits for the 3rd partition; the 3rd commit to trigger archiving the replace commit
-      for (int i = 5; i < 8; i++) {
+      // 2nd write batch; 4 commits for the 3rd partition; the 3rd commit to trigger archiving the replace commit
+      for (int i = 5; i < 9; i++) {
         String instantTime = HoodieActiveTimeline.createNewInstantTime(i * 1000);
         client.startCommitWithTime(instantTime);
         client.insert(jsc().parallelize(dataGen.generateInsertsForPartition(instantTime, 1, DEFAULT_THIRD_PARTITION_PATH), 1), instantTime);
@@ -96,7 +96,7 @@ public void testDeletePartitionAndArchive(boolean metadataEnabled) throws IOExce
 
       // verify records
       final HoodieTimeline timeline2 = metaClient.getCommitTimeline().filterCompletedInstants();
-      assertEquals(4, countRecordsOptionallySince(jsc(), basePath(), sqlContext(), timeline2, Option.empty()),
+      assertEquals(5, countRecordsOptionallySince(jsc(), basePath(), sqlContext(), timeline2, Option.empty()),
           "should only have the 4 records from the 3rd partition.");
     }
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/ComplexAvroKeyGenerator.java
Patch:
@@ -40,11 +40,11 @@ public ComplexAvroKeyGenerator(TypedProperties props) {
 
   @Override
   public String getRecordKey(GenericRecord record) {
-    return KeyGenUtils.getRecordKey(record, getRecordKeyFields());
+    return KeyGenUtils.getRecordKey(record, getRecordKeyFields(), isConsistentLogicalTimestampEnabled());
   }
 
   @Override
   public String getPartitionPath(GenericRecord record) {
-    return KeyGenUtils.getRecordPartitionPath(record, getPartitionPathFields(), hiveStylePartitioning, encodePartitionPath);
+    return KeyGenUtils.getRecordPartitionPath(record, getPartitionPathFields(), hiveStylePartitioning, encodePartitionPath, isConsistentLogicalTimestampEnabled());
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/GlobalAvroDeleteKeyGenerator.java
Patch:
@@ -40,7 +40,7 @@ public GlobalAvroDeleteKeyGenerator(TypedProperties config) {
 
   @Override
   public String getRecordKey(GenericRecord record) {
-    return KeyGenUtils.getRecordKey(record, getRecordKeyFields());
+    return KeyGenUtils.getRecordKey(record, getRecordKeyFields(), isConsistentLogicalTimestampEnabled());
   }
 
   @Override

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/NonpartitionedAvroKeyGenerator.java
Patch:
@@ -57,9 +57,9 @@ public String getRecordKey(GenericRecord record) {
     // 1. if there is only one record key field, the format of record key is just "<value>"
     // 2. if there are multiple record key fields, the format is "<field1>:<value1>,<field2>:<value2>,..."
     if (getRecordKeyFieldNames().size() == 1) {
-      return KeyGenUtils.getRecordKey(record, getRecordKeyFields().get(0));
+      return KeyGenUtils.getRecordKey(record, getRecordKeyFields().get(0), isConsistentLogicalTimestampEnabled());
     }
-    return KeyGenUtils.getRecordKey(record, getRecordKeyFields());
+    return KeyGenUtils.getRecordKey(record, getRecordKeyFields(), isConsistentLogicalTimestampEnabled());
   }
 
   public String getEmptyPartition() {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/SimpleAvroKeyGenerator.java
Patch:
@@ -47,11 +47,11 @@ public SimpleAvroKeyGenerator(TypedProperties props) {
 
   @Override
   public String getRecordKey(GenericRecord record) {
-    return KeyGenUtils.getRecordKey(record, getRecordKeyFields().get(0));
+    return KeyGenUtils.getRecordKey(record, getRecordKeyFields().get(0), isConsistentLogicalTimestampEnabled());
   }
 
   @Override
   public String getPartitionPath(GenericRecord record) {
-    return KeyGenUtils.getPartitionPath(record, getPartitionPathFields().get(0), hiveStylePartitioning, encodePartitionPath);
+    return KeyGenUtils.getPartitionPath(record, getPartitionPathFields().get(0), hiveStylePartitioning, encodePartitionPath, isConsistentLogicalTimestampEnabled());
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataKeyGenerator.java
Patch:
@@ -42,7 +42,7 @@ public HoodieTableMetadataKeyGenerator(TypedProperties config) {
 
   @Override
   public String getRecordKey(GenericRecord record) {
-    return KeyGenUtils.getRecordKey(record, HoodieMetadataPayload.SCHEMA_FIELD_ID_KEY);
+    return KeyGenUtils.getRecordKey(record, HoodieMetadataPayload.SCHEMA_FIELD_ID_KEY, isConsistentLogicalTimestampEnabled());
   }
 
   @Override

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaExecutionStrategy.java
Patch:
@@ -124,7 +124,8 @@ protected Option<BulkInsertPartitioner<T>> getPartitioner(Map<String, String> st
     if (strategyParams.containsKey(PLAN_STRATEGY_SORT_COLUMNS.key())) {
       return Option.of(new JavaCustomColumnsSortPartitioner(
           strategyParams.get(PLAN_STRATEGY_SORT_COLUMNS.key()).split(","),
-          HoodieAvroUtils.addMetadataFields(schema)));
+          HoodieAvroUtils.addMetadataFields(schema),
+          getWriteConfig().isConsistentLogicalTimestampEnabled()));
     } else {
       return Option.empty();
     }

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/execution/bulkinsert/TestJavaBulkInsertInternalPartitioner.java
Patch:
@@ -64,13 +64,13 @@ public void testCustomColumnSortPartitioner(String sortColumnString) throws Exce
 
     List<HoodieRecord> records = generateTestRecordsForBulkInsert(1000);
     testBulkInsertInternalPartitioner(
-        new JavaCustomColumnsSortPartitioner(sortColumns, HoodieTestDataGenerator.AVRO_SCHEMA),
+        new JavaCustomColumnsSortPartitioner(sortColumns, HoodieTestDataGenerator.AVRO_SCHEMA, false),
         records, true, generatePartitionNumRecords(records), Option.of(columnComparator));
   }
 
   private Comparator<HoodieRecord> getCustomColumnComparator(Schema schema, String[] sortColumns) {
     return Comparator.comparing(
-        record -> HoodieAvroUtils.getRecordColumnValues(record, sortColumns, schema).toString());
+        record -> HoodieAvroUtils.getRecordColumnValues(record, sortColumns, schema, false).toString());
   }
 
   private void verifyRecordAscendingOrder(List<HoodieRecord> records,

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java
Patch:
@@ -108,7 +108,6 @@ public HoodieWriteMetadata<JavaRDD<WriteStatus>> performClustering(final HoodieC
     return writeMetadata;
   }
 
-
   /**
    * Execute clustering to write inputRecords into new files as defined by rules in strategy parameters.
    * The number of new file groups created is bounded by numOutputGroups.
@@ -141,7 +140,7 @@ protected Option<BulkInsertPartitioner<T>> getPartitioner(Map<String, String> st
           getWriteConfig(), HoodieAvroUtils.addMetadataFields(schema)));
     } else if (strategyParams.containsKey(PLAN_STRATEGY_SORT_COLUMNS.key())) {
       return Option.of(new RDDCustomColumnsSortPartitioner(strategyParams.get(PLAN_STRATEGY_SORT_COLUMNS.key()).split(","),
-          HoodieAvroUtils.addMetadataFields(schema)));
+          HoodieAvroUtils.addMetadataFields(schema), getWriteConfig().isConsistentLogicalTimestampEnabled()));
     } else {
       return Option.empty();
     }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/execution/bulkinsert/TestBulkInsertInternalPartitioner.java
Patch:
@@ -147,9 +147,9 @@ public void testCustomColumnSortPartitioner() throws Exception {
 
     JavaRDD<HoodieRecord> records1 = generateTestRecordsForBulkInsert(jsc);
     JavaRDD<HoodieRecord> records2 = generateTripleTestRecordsForBulkInsert(jsc);
-    testBulkInsertInternalPartitioner(new RDDCustomColumnsSortPartitioner(sortColumns, HoodieTestDataGenerator.AVRO_SCHEMA),
+    testBulkInsertInternalPartitioner(new RDDCustomColumnsSortPartitioner(sortColumns, HoodieTestDataGenerator.AVRO_SCHEMA, false),
         records1, true, true, generateExpectedPartitionNumRecords(records1), Option.of(columnComparator));
-    testBulkInsertInternalPartitioner(new RDDCustomColumnsSortPartitioner(sortColumns, HoodieTestDataGenerator.AVRO_SCHEMA),
+    testBulkInsertInternalPartitioner(new RDDCustomColumnsSortPartitioner(sortColumns, HoodieTestDataGenerator.AVRO_SCHEMA, false),
         records2, true, true, generateExpectedPartitionNumRecords(records2), Option.of(columnComparator));
 
     HoodieWriteConfig config = HoodieWriteConfig

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/bucket/TestBucketIdentifier.java
Patch:
@@ -45,7 +45,7 @@ public void testBucketIdWithSimpleRecordKey() {
     String indexKeyField = "_row_key";
     GenericRecord record = KeyGeneratorTestUtilities.getRecord();
     HoodieRecord hoodieRecord = new HoodieRecord(
-        new HoodieKey(KeyGenUtils.getRecordKey(record, recordKeyField), ""), null);
+        new HoodieKey(KeyGenUtils.getRecordKey(record, recordKeyField, false), ""), null);
     int bucketId = BucketIdentifier.getBucketId(hoodieRecord, indexKeyField, 8);
     assert bucketId == BucketIdentifier.getBucketId(
         Arrays.asList(record.get(indexKeyField).toString()), 8);
@@ -57,7 +57,7 @@ public void testBucketIdWithComplexRecordKey() {
     String indexKeyField = "_row_key";
     GenericRecord record = KeyGeneratorTestUtilities.getRecord();
     HoodieRecord hoodieRecord = new HoodieRecord(
-        new HoodieKey(KeyGenUtils.getRecordKey(record, recordKeyField), ""), null);
+        new HoodieKey(KeyGenUtils.getRecordKey(record, recordKeyField, false), ""), null);
     int bucketId = BucketIdentifier.getBucketId(hoodieRecord, indexKeyField, 8);
     assert bucketId == BucketIdentifier.getBucketId(
         Arrays.asList(record.get(indexKeyField).toString()), 8);

File: hudi-flink/src/main/java/org/apache/hudi/sink/utils/PayloadCreation.java
Patch:
@@ -77,7 +77,7 @@ public HoodieRecordPayload<?> createPayload(GenericRecord record) throws Excepti
     if (shouldCombine) {
       ValidationUtils.checkState(preCombineField != null);
       Comparable<?> orderingVal = (Comparable<?>) HoodieAvroUtils.getNestedFieldVal(record,
-          preCombineField, false);
+          preCombineField, false, false);
       return (HoodieRecordPayload<?>) constructor.newInstance(record, orderingVal);
     } else {
       return (HoodieRecordPayload<?>) this.constructor.newInstance(Option.of(record));

File: hudi-flink/src/test/java/org/apache/hudi/utils/TestStringToRowDataConverter.java
Patch:
@@ -94,7 +94,7 @@ void testRowDataToAvroStringToRowData() {
         (GenericRecord) converter.convert(AvroSchemaConverter.convertToSchema(rowType), rowData);
     StringToRowDataConverter stringToRowDataConverter =
         new StringToRowDataConverter(rowType.getChildren().toArray(new LogicalType[0]));
-    final String recordKey = KeyGenUtils.getRecordKey(avroRecord, rowType.getFieldNames());
+    final String recordKey = KeyGenUtils.getRecordKey(avroRecord, rowType.getFieldNames(), false);
     final String[] recordKeys = KeyGenUtils.extractRecordKeys(recordKey);
     Object[] convertedKeys = stringToRowDataConverter.convert(recordKeys);
 

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieConfig.java
Patch:
@@ -146,6 +146,9 @@ public <T> Integer getIntOrDefault(ConfigProperty<T> configProperty) {
   }
 
   public <T> Boolean getBoolean(ConfigProperty<T> configProperty) {
+    if (configProperty.hasDefaultValue()) {
+      return getBooleanOrDefault(configProperty);
+    }
     Option<Object> rawValue = getRawValue(configProperty);
     return rawValue.map(v -> Boolean.parseBoolean(v.toString())).orElse(null);
   }

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java
Patch:
@@ -24,9 +24,9 @@
 import java.util.Map;
 
 /**
- * DDLExceutor is the interface which defines the ddl functions for Hive.
+ * DDLExecutor is the interface which defines the ddl functions for Hive.
  * There are two main implementations one is QueryBased other is based on HiveMetaStore
- * QueryBasedDDLExecutor also has two impls namely HiveQL based and other JDBC based.
+ * QueryBasedDDLExecutor also has two implementations namely HiveQL based and other JDBC based.
  */
 public interface DDLExecutor {
   /**

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HiveIncrementalPuller.java
Patch:
@@ -91,6 +91,8 @@ public static class Config implements Serializable {
     public String fromCommitTime;
     @Parameter(names = {"--maxCommits"})
     public int maxCommits = 3;
+    @Parameter(names = {"--fsDefaultFs"})
+    public String fsDefaultFs = "file:///";
     @Parameter(names = {"--help", "-h"}, help = true)
     public Boolean help = false;
   }
@@ -124,6 +126,7 @@ private void validateConfig(Config config) {
 
   public void saveDelta() throws IOException {
     Configuration conf = new Configuration();
+    conf.set("fs.defaultFS",config.fsDefaultFs);
     FileSystem fs = FileSystem.get(conf);
     Statement stmt = null;
     try {

File: hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieCatalog.java
Patch:
@@ -305,7 +305,9 @@ public void createTable(ObjectPath tablePath, CatalogBaseTable catalogTable, boo
     try {
       StreamerUtil.initTableIfNotExists(conf);
       // prepare the non-table-options properties
-      options.put(TableOptionProperties.COMMENT, resolvedTable.getComment());
+      if (!StringUtils.isNullOrWhitespaceOnly(resolvedTable.getComment())) {
+        options.put(TableOptionProperties.COMMENT, resolvedTable.getComment());
+      }
       TableOptionProperties.createProperties(tablePathStr, hadoopConf, options);
     } catch (IOException e) {
       throw new CatalogException(String.format("Initialize table path %s exception.", tablePathStr), e);

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java
Patch:
@@ -178,9 +178,10 @@ public static void createCOWTable(String instantTime, int numberOfPartitions, bo
     createCommitFile(commitMetadata, instantTime);
   }
 
-  public static void createReplaceCommit(String instantTime, String partitions, WriteOperationType type)
+  public static void createReplaceCommit(String instantTime, String partitions, WriteOperationType type, boolean isParquetSchemaSimple, boolean useSchemaFromCommitMetadata)
       throws IOException {
     HoodieReplaceCommitMetadata replaceCommitMetadata = new HoodieReplaceCommitMetadata();
+    addSchemaToCommitMetadata(replaceCommitMetadata, isParquetSchemaSimple, useSchemaFromCommitMetadata);
     replaceCommitMetadata.setOperationType(type);
     Map<String, List<String>> partitionToReplaceFileIds = new HashMap<>();
     partitionToReplaceFileIds.put(partitions, new ArrayList<>());

File: hudi-examples/src/main/java/org/apache/hudi/examples/spark/HoodieSparkBootstrapExample.java
Patch:
@@ -40,7 +40,7 @@ public class HoodieSparkBootstrapExample {
 
   public static void main(String[] args) throws Exception {
     if (args.length < 5) {
-      System.err.println("Usage: HoodieWriteClientExample <tablePath> <tableName>");
+      System.err.println("Usage: HoodieSparkBootstrapExample <recordKey> <tableName> <partitionPath> <preCombineField> <basePath>");
       System.exit(1);
     }
     String recordKey = args[0];

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/HMSDDLExecutor.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.hive.HiveSyncConfig;
 import org.apache.hudi.hive.HoodieHiveSyncException;
 import org.apache.hudi.hive.PartitionValueExtractor;
+import org.apache.hudi.hive.util.HivePartitionUtil;
 import org.apache.hudi.hive.util.HiveSchemaUtil;
 
 import org.apache.hadoop.fs.FileSystem;
@@ -236,7 +237,8 @@ public void dropPartitionsToTable(String tableName, List<String> partitionsToDro
     LOG.info("Drop partitions " + partitionsToDrop.size() + " on " + tableName);
     try {
       for (String dropPartition : partitionsToDrop) {
-        client.dropPartition(syncConfig.databaseName, tableName, dropPartition, false);
+        String partitionClause = HivePartitionUtil.getPartitionClauseForDrop(dropPartition, partitionValueExtractor, syncConfig);
+        client.dropPartition(syncConfig.databaseName, tableName, partitionClause, false);
         LOG.info("Drop partition " + dropPartition + " on " + tableName);
       }
     } catch (TException e) {

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/HiveQueryDDLExecutor.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hudi.hive.util.HivePartitionUtil;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
@@ -136,7 +137,8 @@ public void dropPartitionsToTable(String tableName, List<String> partitionsToDro
     LOG.info("Drop partitions " + partitionsToDrop.size() + " on " + tableName);
     try {
       for (String dropPartition : partitionsToDrop) {
-        metaStoreClient.dropPartition(config.databaseName, tableName, dropPartition, false);
+        String partitionClause = HivePartitionUtil.getPartitionClauseForDrop(dropPartition, partitionValueExtractor, config);
+        metaStoreClient.dropPartition(config.databaseName, tableName, partitionClause, false);
         LOG.info("Drop partition " + dropPartition + " on " + tableName);
       }
     } catch (Exception e) {

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/QueryBasedDDLExecutor.java
Patch:
@@ -46,7 +46,7 @@
 public abstract class QueryBasedDDLExecutor implements DDLExecutor {
   private static final Logger LOG = LogManager.getLogger(QueryBasedDDLExecutor.class);
   private final HiveSyncConfig config;
-  private final PartitionValueExtractor partitionValueExtractor;
+  public final PartitionValueExtractor partitionValueExtractor;
   private final FileSystem fs;
 
   public QueryBasedDDLExecutor(HiveSyncConfig config, FileSystem fs) {
@@ -160,7 +160,7 @@ private StringBuilder getAlterTablePrefix(String tableName) {
     return alterSQL;
   }
 
-  private String getPartitionClause(String partition) {
+  public String getPartitionClause(String partition) {
     List<String> partitionValues = partitionValueExtractor.extractPartitionValuesInPath(partition);
     ValidationUtils.checkArgument(config.partitionFields.size() == partitionValues.size(),
         "Partition key parts " + config.partitionFields + " does not match with partition values " + partitionValues

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -119,7 +119,8 @@ public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTa
 
   @Override
   public boolean canWrite(HoodieRecord record) {
-    return fileWriter.canWrite() && record.getPartitionPath().equals(writeStatus.getPartitionPath());
+    return (fileWriter.canWrite() && record.getPartitionPath().equals(writeStatus.getPartitionPath()))
+        || layoutControlsNumFiles();
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/AbstractWriteHelper.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieRecordPayload;
+import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.exception.HoodieUpsertException;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.table.HoodieTable;
@@ -38,15 +39,15 @@ public HoodieWriteMetadata<O> write(String instantTime,
                                       boolean shouldCombine,
                                       int shuffleParallelism,
                                       BaseCommitActionExecutor<T, I, K, O, R> executor,
-                                      boolean performTagging) {
+                                      WriteOperationType operationType) {
     try {
       // De-dupe/merge if needed
       I dedupedRecords =
           combineOnCondition(shouldCombine, inputRecords, shuffleParallelism, table);
 
       Instant lookupBegin = Instant.now();
       I taggedRecords = dedupedRecords;
-      if (performTagging) {
+      if (table.getIndex().requiresTagging(operationType)) {
         // perform index loop up to get existing location of records
         context.setJobStatus(this.getClass().getSimpleName(), "Tagging");
         taggedRecords = tag(dedupedRecords, context, table);

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertCommitActionExecutor.java
Patch:
@@ -47,6 +47,6 @@ public FlinkInsertCommitActionExecutor(HoodieEngineContext context,
   @Override
   public HoodieWriteMetadata<List<WriteStatus>> execute() {
     return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
-        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);
+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
   }
 }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertOverwriteCommitActionExecutor.java
Patch:
@@ -64,6 +64,6 @@ protected String getCommitActionType() {
   @Override
   public HoodieWriteMetadata<List<WriteStatus>> execute() {
     return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
-        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);
+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
   }
 }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertOverwriteTableCommitActionExecutor.java
Patch:
@@ -45,6 +45,6 @@ public FlinkInsertOverwriteTableCommitActionExecutor(HoodieEngineContext context
   @Override
   public HoodieWriteMetadata<List<WriteStatus>> execute() {
     return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
-        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);
+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
   }
 }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkUpsertCommitActionExecutor.java
Patch:
@@ -47,6 +47,6 @@ public FlinkUpsertCommitActionExecutor(HoodieEngineContext context,
   @Override
   public HoodieWriteMetadata<List<WriteStatus>> execute() {
     return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
-        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, true);
+        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, operationType);
   }
 }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkWriteHelper.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.common.model.HoodieOperation;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
+import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieUpsertException;
 import org.apache.hudi.index.HoodieIndex;
@@ -64,7 +65,7 @@ public static FlinkWriteHelper newInstance() {
   @Override
   public HoodieWriteMetadata<List<WriteStatus>> write(String instantTime, List<HoodieRecord<T>> inputRecords, HoodieEngineContext context,
                                                       HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table, boolean shouldCombine, int shuffleParallelism,
-                                                      BaseCommitActionExecutor<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>, R> executor, boolean performTagging) {
+                                                      BaseCommitActionExecutor<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>, R> executor, WriteOperationType operationType) {
     try {
       Instant lookupBegin = Instant.now();
       Duration indexLookupDuration = Duration.between(lookupBegin, Instant.now());

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/delta/FlinkUpsertDeltaCommitActionExecutor.java
Patch:
@@ -47,6 +47,6 @@ public FlinkUpsertDeltaCommitActionExecutor(HoodieEngineContext context,
   @Override
   public HoodieWriteMetadata execute() {
     return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
-        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, true);
+        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, operationType);
   }
 }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertCommitActionExecutor.java
Patch:
@@ -45,6 +45,6 @@ public JavaInsertCommitActionExecutor(HoodieEngineContext context,
   @Override
   public HoodieWriteMetadata<List<WriteStatus>> execute() {
     return JavaWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
-        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);
+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
   }
 }

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteCommitActionExecutor.java
Patch:
@@ -55,7 +55,7 @@ public JavaInsertOverwriteCommitActionExecutor(HoodieEngineContext context,
   @Override
   public HoodieWriteMetadata<List<WriteStatus>> execute() {
     return JavaWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
-        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);
+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
   }
 
   @Override

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertCommitActionExecutor.java
Patch:
@@ -45,6 +45,6 @@ public JavaUpsertCommitActionExecutor(HoodieEngineContext context,
   @Override
   public HoodieWriteMetadata<List<WriteStatus>> execute() {
     return JavaWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
-        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, true);
+        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, operationType);
   }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/SparkHoodieIndexFactory.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hudi.index.bloom.HoodieBloomIndex;
 import org.apache.hudi.index.bloom.HoodieGlobalBloomIndex;
 import org.apache.hudi.index.bloom.SparkHoodieBloomIndexHelper;
+import org.apache.hudi.index.bucket.HoodieBucketIndex;
 import org.apache.hudi.index.hbase.SparkHoodieHBaseIndex;
 import org.apache.hudi.index.inmemory.HoodieInMemoryHashIndex;
 import org.apache.hudi.index.simple.HoodieGlobalSimpleIndex;
@@ -55,6 +56,8 @@ public static HoodieIndex createIndex(HoodieWriteConfig config) {
         return new SparkHoodieHBaseIndex<>(config);
       case INMEMORY:
         return new HoodieInMemoryHashIndex<>(config);
+      case BUCKET:
+        return new HoodieBucketIndex(config);
       case BLOOM:
         return new HoodieBloomIndex<>(config, SparkHoodieBloomIndexHelper.getInstance());
       case GLOBAL_BLOOM:

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertCommitActionExecutor.java
Patch:
@@ -44,6 +44,6 @@ public SparkInsertCommitActionExecutor(HoodieSparkEngineContext context,
   @Override
   public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {
     return SparkWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
-        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);
+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
   }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkUpsertCommitActionExecutor.java
Patch:
@@ -44,6 +44,6 @@ public SparkUpsertCommitActionExecutor(HoodieSparkEngineContext context,
   @Override
   public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {
     return SparkWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
-        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, true);
+        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, operationType);
   }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/AbstractSparkDeltaCommitActionExecutor.java
Patch:
@@ -74,8 +74,8 @@ public Partitioner getUpsertPartitioner(WorkloadProfile profile) {
   public Iterator<List<WriteStatus>> handleUpdate(String partitionPath, String fileId,
       Iterator<HoodieRecord<T>> recordItr) throws IOException {
     LOG.info("Merging updates for commit " + instantTime + " for file " + fileId);
-
-    if (!table.getIndex().canIndexLogFiles() && mergeOnReadUpsertPartitioner.getSmallFileIds().contains(fileId)) {
+    if (!table.getIndex().canIndexLogFiles() && mergeOnReadUpsertPartitioner != null
+        && mergeOnReadUpsertPartitioner.getSmallFileIds().contains(fileId)) {
       LOG.info("Small file corrections for updates for commit " + instantTime + " for file " + fileId);
       return super.handleUpdate(partitionPath, fileId, recordItr);
     } else {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkInsertDeltaCommitActionExecutor.java
Patch:
@@ -45,6 +45,6 @@ public SparkInsertDeltaCommitActionExecutor(HoodieSparkEngineContext context,
   @Override
   public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {
     return SparkWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
-        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(),this, false);
+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(),this, operationType);
   }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkUpsertDeltaCommitActionExecutor.java
Patch:
@@ -44,6 +44,6 @@ public SparkUpsertDeltaCommitActionExecutor(HoodieSparkEngineContext context,
   @Override
   public HoodieWriteMetadata execute() {
     return SparkWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
-        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(),this, true);
+        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(),this, operationType);
   }
 }

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/util/HiveSchemaUtil.java
Patch:
@@ -471,6 +471,9 @@ public static String generateCreateDDL(String tableName, MessageType storageSche
     if (!config.partitionFields.isEmpty()) {
       sb.append(" PARTITIONED BY (").append(partitionsStr).append(")");
     }
+    if (config.bucketSpec != null) {
+      sb.append(' ' + config.bucketSpec + ' ');
+    }
     sb.append(" ROW FORMAT SERDE '").append(serdeClass).append("'");
     if (serdeProperties != null && !serdeProperties.isEmpty()) {
       sb.append(" WITH SERDEPROPERTIES (").append(propertyToString(serdeProperties)).append(")");

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java
Patch:
@@ -289,6 +289,8 @@ public interface HoodieTimeline extends Serializable {
    */
   Option<byte[]> getInstantDetails(HoodieInstant instant);
 
+  boolean isEmpty(HoodieInstant instant);
+
   /**
    * Check WriteOperationType is DeletePartition.
    */

File: hudi-flink/src/main/java/org/apache/hudi/table/format/cow/ParquetDecimalVector.java
Patch:
@@ -32,9 +32,9 @@
  */
 public class ParquetDecimalVector implements DecimalColumnVector {
 
-  private final ColumnVector vector;
+  public final ColumnVector vector;
 
-  ParquetDecimalVector(ColumnVector vector) {
+  public ParquetDecimalVector(ColumnVector vector) {
     this.vector = vector;
   }
 

File: hudi-aws/src/main/java/org/apache/hudi/aws/transaction/lock/DynamoDBBasedLockProvider.java
Patch:
@@ -199,7 +199,6 @@ private void createLockTableInDynamoDB(AmazonDynamoDB dynamoDB, String tableName
   }
 
   private void checkRequiredProps(final LockConfiguration config) {
-    ValidationUtils.checkArgument(config.getConfig().getString(DynamoDbBasedLockConfig.DYNAMODB_LOCK_BILLING_MODE.key()) != null);
     ValidationUtils.checkArgument(config.getConfig().getString(DynamoDbBasedLockConfig.DYNAMODB_LOCK_TABLE_NAME.key()) != null);
     ValidationUtils.checkArgument(config.getConfig().getString(DynamoDbBasedLockConfig.DYNAMODB_LOCK_REGION.key()) != null);
     ValidationUtils.checkArgument(config.getConfig().getString(DynamoDbBasedLockConfig.DYNAMODB_LOCK_PARTITION_KEY.key()) != null);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/AvroDFSSource.java
Patch:
@@ -45,6 +45,7 @@ public class AvroDFSSource extends AvroSource {
   public AvroDFSSource(TypedProperties props, JavaSparkContext sparkContext, SparkSession sparkSession,
       SchemaProvider schemaProvider) throws IOException {
     super(props, sparkContext, sparkSession, schemaProvider);
+    sparkContext.hadoopConfiguration().set("avro.schema.input.key", schemaProvider.getSourceSchema().toString());
     this.pathSelector = DFSPathSelector
         .createSourceSelector(props, sparkContext.hadoopConfiguration());
   }

File: hudi-common/src/main/java/org/apache/hudi/common/config/LockConfiguration.java
Patch:
@@ -30,14 +30,14 @@ public class LockConfiguration implements Serializable {
   public static final String LOCK_PREFIX = "hoodie.write.lock.";
 
   public static final String LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP_KEY = LOCK_PREFIX + "wait_time_ms_between_retry";
-  public static final String DEFAULT_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS = String.valueOf(5000L);
+  public static final String DEFAULT_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS = String.valueOf(1000L);
 
   public static final String LOCK_ACQUIRE_RETRY_MAX_WAIT_TIME_IN_MILLIS_PROP_KEY = LOCK_PREFIX + "max_wait_time_ms_between_retry";
 
   public static final String LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP_KEY = LOCK_PREFIX + "client.wait_time_ms_between_retry";
 
   public static final String LOCK_ACQUIRE_NUM_RETRIES_PROP_KEY = LOCK_PREFIX + "num_retries";
-  public static final String DEFAULT_LOCK_ACQUIRE_NUM_RETRIES = String.valueOf(3);
+  public static final String DEFAULT_LOCK_ACQUIRE_NUM_RETRIES = String.valueOf(15);
 
   public static final String LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP_KEY = LOCK_PREFIX + "client.num_retries";
 

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/client/transaction/FileSystemBasedLockProviderTestClass.java
Patch:
@@ -76,7 +76,7 @@ public boolean tryLock(long time, TimeUnit unit) {
     try {
       int numRetries = 0;
       while (fs.exists(new Path(lockPath + "/" + LOCK_NAME))
-          && (numRetries <= lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_NUM_RETRIES_PROP_KEY))) {
+          && (numRetries++ <= lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_NUM_RETRIES_PROP_KEY))) {
         Thread.sleep(lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP_KEY));
       }
       synchronized (LOCK_NAME) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/InProcessLockProvider.java
Patch:
@@ -111,7 +111,7 @@ public void close() {
   }
 
   private String getLogMessage(LockState state) {
-    return StringUtils.join(String.valueOf(Thread.currentThread().getId()),
-        state.name(), " local process lock.");
+    return StringUtils.join("Thread ", String.valueOf(Thread.currentThread().getName()), " ",
+        state.name(), " in-process lock.");
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java
Patch:
@@ -217,7 +217,7 @@ private HoodieCleanMetadata runClean(HoodieTable<T, I, K, O> table, HoodieInstan
       throw new HoodieIOException("Failed to clean up after commit", e);
     } finally {
       if (!skipLocking) {
-        this.txnManager.endTransaction();
+        this.txnManager.endTransaction(Option.empty());
       }
     }
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/BaseRestoreActionExecutor.java
Patch:
@@ -112,7 +112,7 @@ private void writeToMetadata(HoodieRestoreMetadata restoreMetadata) {
       this.txnManager.beginTransaction(Option.empty(), Option.empty());
       writeTableMetadata(restoreMetadata);
     } finally {
-      this.txnManager.endTransaction();
+      this.txnManager.endTransaction(Option.empty());
     }
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java
Patch:
@@ -266,7 +266,7 @@ protected void finishRollback(HoodieInstant inflightInstant, HoodieRollbackMetad
       throw new HoodieIOException("Error executing rollback at instant " + instantTime, e);
     } finally {
       if (!skipLocking) {
-        this.txnManager.endTransaction();
+        this.txnManager.endTransaction(Option.empty());
       }
     }
   }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -359,8 +359,8 @@ public void completeCompaction(
       String compactionCommitTime) {
     this.context.setJobStatus(this.getClass().getSimpleName(), "Collect compaction write status and commit compaction");
     List<HoodieWriteStat> writeStats = writeStatuses.stream().map(WriteStatus::getStat).collect(Collectors.toList());
+    final HoodieInstant compactionInstant = new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, compactionCommitTime);
     try {
-      HoodieInstant compactionInstant = new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, compactionCommitTime);
       this.txnManager.beginTransaction(Option.of(compactionInstant), Option.empty());
       finalizeWrite(table, compactionCommitTime, writeStats);
       // commit to data table after committing to metadata table.
@@ -371,7 +371,7 @@ public void completeCompaction(
       LOG.info("Committing Compaction {} finished with result {}.", compactionCommitTime, metadata);
       CompactHelpers.getInstance().completeInflightCompaction(table, compactionCommitTime, metadata);
     } finally {
-      this.txnManager.endTransaction();
+      this.txnManager.endTransaction(Option.of(compactionInstant));
     }
     if (compactionTimer != null) {
       long durationInMs = metrics.getDurationInMs(compactionTimer.stop());

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java
Patch:
@@ -169,11 +169,11 @@ public boolean isDropPartition() {
       }
 
       if (hoodieCommitMetadata.isPresent()
-          && hoodieCommitMetadata.get().getOperationType().equals(WriteOperationType.DELETE_PARTITION)) {
+          && WriteOperationType.DELETE_PARTITION.equals(hoodieCommitMetadata.get().getOperationType())) {
         return true;
       }
     } catch (Exception e) {
-      throw new HoodieSyncException("Failed to read data schema", e);
+      throw new HoodieSyncException("Failed to get commit metadata", e);
     }
     return false;
   }

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -44,7 +44,7 @@ public final class HoodieMetadataConfig extends HoodieConfig {
   // Enable the internal Metadata Table which saves file listings
   public static final ConfigProperty<Boolean> ENABLE = ConfigProperty
       .key(METADATA_PREFIX + ".enable")
-      .defaultValue(false)
+      .defaultValue(true)
       .sinceVersion("0.7.0")
       .withDocumentation("Enable the internal metadata table which serves table metadata like level file listings");
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java
Patch:
@@ -215,7 +215,7 @@ public void updateIndexMetrics(final String action, final long durationInMs) {
   }
 
   String getMetricsName(String action, String metric) {
-    return config == null ? null : String.format("%s.%s.%s", tableName, action, metric);
+    return config == null ? null : String.format("%s.%s.%s", config.getMetricReporterMetricsNamePrefix(), action, metric);
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/Metrics.java
Patch:
@@ -46,7 +46,7 @@ public class Metrics {
 
   private Metrics(HoodieWriteConfig metricConfig) {
     registry = new MetricRegistry();
-    commonMetricPrefix = metricConfig.getTableName();
+    commonMetricPrefix = metricConfig.getMetricReporterMetricsNamePrefix();
     reporter = MetricsReporterFactory.createReporter(metricConfig, registry);
     if (reporter == null) {
       throw new RuntimeException("Cannot initialize Reporter.");

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerMetrics.java
Patch:
@@ -74,7 +74,7 @@ private Timer createTimer(String name) {
   }
 
   String getMetricsName(String action, String metric) {
-    return config == null ? null : String.format("%s.%s.%s", tableName, action, metric);
+    return config == null ? null : String.format("%s.%s.%s", config.getMetricReporterMetricsNamePrefix(), action, metric);
   }
 
   public void updateDeltaStreamerMetrics(long durationInNs) {

File: hudi-flink/src/main/java/org/apache/hudi/sink/compact/CompactionPlanOperator.java
Patch:
@@ -98,7 +98,7 @@ public void notifyCheckpointComplete(long checkpointId) {
   }
 
   private void scheduleCompaction(HoodieFlinkTable<?> table, long checkpointId) throws IOException {
-    // the last instant takes the highest priority.
+    // the first instant takes the highest priority.
     Option<HoodieInstant> firstRequested = table.getActiveTimeline().filterPendingCompactionTimeline()
         .filter(instant -> instant.getState() == HoodieInstant.State.REQUESTED).firstInstant();
     if (!firstRequested.isPresent()) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedTableMetadata.java
Patch:
@@ -61,7 +61,7 @@ public void testTableOperations() throws Exception {
   }
 
   private void doWriteInsertAndUpsert(HoodieTestTable testTable) throws Exception {
-    doWriteInsertAndUpsert(testTable, "0000001", "0000002");
+    doWriteInsertAndUpsert(testTable, "0000001", "0000002", false);
   }
 
   private void verifyBaseMetadataTable() throws IOException {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieMetadataBootstrap.java
Patch:
@@ -235,7 +235,7 @@ private void bootstrapAndVerifyFailure() throws Exception {
   }
 
   private void doWriteInsertAndUpsert(HoodieTestTable testTable) throws Exception {
-    doWriteInsertAndUpsert(testTable, "0000100", "0000101");
+    doWriteInsertAndUpsert(testTable, "0000100", "0000101", false);
   }
 
   private HoodieWriteConfig getWriteConfig(int minArchivalCommits, int maxArchivalCommits) throws Exception {

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadata.java
Patch:
@@ -48,6 +48,7 @@ public interface HoodieTableMetadata extends Serializable, AutoCloseable {
   String RECORDKEY_PARTITION_LIST = "__all_partitions__";
   // The partition name used for non-partitioned tables
   String NON_PARTITIONED_NAME = ".";
+  String EMPTY_PARTITION_NAME = "";
 
   // Base path of the Metadata Table relative to the dataset (.hoodie/metadata)
   static final String METADATA_TABLE_REL_PATH = HoodieTableMetaClient.METAFOLDER_NAME + Path.SEPARATOR + "metadata";

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/FileCreateUtils.java
Patch:
@@ -367,7 +367,9 @@ public static List<Path> getPartitionPaths(Path basePath) throws IOException {
     if (Files.notExists(basePath)) {
       return Collections.emptyList();
     }
-    return Files.list(basePath).filter(entry -> !entry.getFileName().toString().equals(HoodieTableMetaClient.METAFOLDER_NAME)).collect(Collectors.toList());
+    return Files.list(basePath).filter(entry -> (!entry.getFileName().toString().equals(HoodieTableMetaClient.METAFOLDER_NAME)
+        && !entry.getFileName().toString().contains("parquet") && !entry.getFileName().toString().contains("log"))
+        && !entry.getFileName().toString().endsWith(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE)).collect(Collectors.toList());
   }
 
   /**

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -403,7 +403,7 @@ public void testHugeLogFileWrite() throws IOException, URISyntaxException, Inter
     byte[] dataBlockContentBytes = getDataBlock(records, header).getContentBytes();
     HoodieDataBlock reusableDataBlock = new HoodieAvroDataBlock(null, null,
         Option.ofNullable(dataBlockContentBytes), false, 0, dataBlockContentBytes.length,
-        0, getSimpleSchema(), header, new HashMap<>());
+        0, getSimpleSchema(), header, new HashMap<>(), HoodieRecord.RECORD_KEY_METADATA_FIELD);
     long writtenSize = 0;
     int logBlockWrittenNum = 0;
     while (writtenSize < Integer.MAX_VALUE) {

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java
Patch:
@@ -26,7 +26,6 @@
 import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.hive.common.StringInternUtils;
@@ -183,8 +182,8 @@ private InputSplit[] getCombineSplits(JobConf job, int numSplits, Map<Path, Part
         deserializerClassName = part.getDeserializer(job).getClass().getName();
       } catch (Exception e) {
         // ignore
+        LOG.error("Getting deserializer class name error ", e);
       }
-      FileSystem inpFs = path.getFileSystem(job);
 
       // don't combine if inputformat is a SymlinkTextInputFormat
       if (inputFormat instanceof SymlinkTextInputFormat) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -29,10 +29,10 @@
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodieOperation;
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
+import org.apache.hudi.common.model.HoodiePayloadProps;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.common.model.HoodiePayloadProps;
 import org.apache.hudi.common.model.HoodieWriteStat.RuntimeStats;
 import org.apache.hudi.common.model.IOType;
 import org.apache.hudi.common.table.log.AppendResult;
@@ -178,7 +178,7 @@ private void init(HoodieRecord record) {
         LOG.error("Error in update task at commit " + instantTime, e);
         writeStatus.setGlobalError(e);
         throw new HoodieUpsertException("Failed to initialize HoodieAppendHandle for FileId: " + fileId + " on commit "
-            + instantTime + " on HDFS path " + hoodieTable.getMetaClient().getBasePath() + partitionPath, e);
+            + instantTime + " on HDFS path " + hoodieTable.getMetaClient().getBasePath() + "/" + partitionPath, e);
       }
       doInit = false;
     }

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java
Patch:
@@ -284,7 +284,7 @@ private HoodieLogBlock createCorruptBlock() throws IOException {
     long contentPosition = inputStream.getPos();
     byte[] corruptedBytes = HoodieLogBlock.readOrSkipContent(inputStream, corruptedBlockSize, readBlockLazily);
     return HoodieCorruptBlock.getBlock(logFile, inputStream, Option.ofNullable(corruptedBytes), readBlockLazily,
-        contentPosition, corruptedBlockSize, corruptedBlockSize, new HashMap<>(), new HashMap<>());
+        contentPosition, corruptedBlockSize, nextBlockOffset, new HashMap<>(), new HashMap<>());
   }
 
   private boolean isBlockCorrupt(int blocksize) throws IOException {

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieCommandBlock.java
Patch:
@@ -67,10 +67,10 @@ public byte[] getContentBytes() {
   }
 
   public static HoodieLogBlock getBlock(HoodieLogFile logFile, FSDataInputStream inputStream, Option<byte[]> content,
-      boolean readBlockLazily, long position, long blockSize, long blockEndpos, Map<HeaderMetadataType, String> header,
+      boolean readBlockLazily, long position, long blockSize, long blockEndPos, Map<HeaderMetadataType, String> header,
       Map<HeaderMetadataType, String> footer) {
 
     return new HoodieCommandBlock(content, inputStream, readBlockLazily,
-        Option.of(new HoodieLogBlockContentLocation(logFile, position, blockSize, blockEndpos)), header, footer);
+        Option.of(new HoodieLogBlockContentLocation(logFile, position, blockSize, blockEndPos)), header, footer);
   }
 }

File: hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java
Patch:
@@ -257,6 +257,7 @@ public static HoodieTableMetaClient initTableIfNotExists(Configuration conf) thr
     final org.apache.hadoop.conf.Configuration hadoopConf = StreamerUtil.getHadoopConf();
     if (!tableExists(basePath, hadoopConf)) {
       HoodieTableMetaClient metaClient = HoodieTableMetaClient.withPropertyBuilder()
+          .setTableCreateSchema(conf.getString(FlinkOptions.SOURCE_AVRO_SCHEMA))
           .setTableType(conf.getString(FlinkOptions.TABLE_TYPE))
           .setTableName(conf.getString(FlinkOptions.TABLE_NAME))
           .setRecordKeyFields(conf.getString(FlinkOptions.RECORD_KEY_FIELD, null))

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -154,7 +154,7 @@ protected void commit(HoodieData<HoodieRecord> hoodieDataRecords, String partiti
    * The record is tagged with respective file slice's location based on its record key.
    */
   private List<HoodieRecord> prepRecords(List<HoodieRecord> records, String partitionName, int numFileGroups) {
-    List<FileSlice> fileSlices = HoodieTableMetadataUtil.loadPartitionFileGroupsWithLatestFileSlices(metadataMetaClient, partitionName);
+    List<FileSlice> fileSlices = HoodieTableMetadataUtil.loadPartitionFileGroupsWithLatestFileSlices(metadataMetaClient, partitionName, false);
     ValidationUtils.checkArgument(fileSlices.size() == numFileGroups, String.format("Invalid number of file groups: found=%d, required=%d", fileSlices.size(), numFileGroups));
 
     return records.stream().map(r -> {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -169,7 +169,7 @@ protected void commit(HoodieData<HoodieRecord> hoodieDataRecords, String partiti
    * The record is tagged with respective file slice's location based on its record key.
    */
   private JavaRDD<HoodieRecord> prepRecords(JavaRDD<HoodieRecord> recordsRDD, String partitionName, int numFileGroups) {
-    List<FileSlice> fileSlices = HoodieTableMetadataUtil.loadPartitionFileGroupsWithLatestFileSlices(metadataMetaClient, partitionName);
+    List<FileSlice> fileSlices = HoodieTableMetadataUtil.loadPartitionFileGroupsWithLatestFileSlices(metadataMetaClient, partitionName, false);
     ValidationUtils.checkArgument(fileSlices.size() == numFileGroups, String.format("Invalid number of file groups: found=%d, required=%d", fileSlices.size(), numFileGroups));
 
     return recordsRDD.map(r -> {

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -245,7 +245,7 @@ private Pair<HoodieFileReader, HoodieMetadataMergedLogRecordReader> openReadersI
 
         // Metadata is in sync till the latest completed instant on the dataset
         HoodieTimer timer = new HoodieTimer().startTimer();
-        List<FileSlice> latestFileSlices = HoodieTableMetadataUtil.loadPartitionFileGroupsWithLatestFileSlices(metadataMetaClient, partitionName);
+        List<FileSlice> latestFileSlices = HoodieTableMetadataUtil.loadPartitionFileGroupsWithLatestFileSlices(metadataMetaClient, partitionName, true);
         if (latestFileSlices.size() == 0) {
           // empty partition
           return Pair.of(null, null);

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/ConnectTransactionServices.java
Patch:
@@ -32,7 +32,7 @@ public interface ConnectTransactionServices {
 
   String startCommit();
 
-  void endCommit(String commitTime, List<WriteStatus> writeStatuses, Map<String, String> extraMetadata);
+  boolean endCommit(String commitTime, List<WriteStatus> writeStatuses, Map<String, String> extraMetadata);
 
   Map<String, String> fetchLatestExtraCommitMetadata();
 }

File: hudi-kafka-connect/src/test/java/org/apache/hudi/helper/MockConnectTransactionServices.java
Patch:
@@ -46,8 +46,9 @@ public String startCommit() {
   }
 
   @Override
-  public void endCommit(String commitTime, List<WriteStatus> writeStatuses, Map<String, String> extraMetadata) {
+  public boolean endCommit(String commitTime, List<WriteStatus> writeStatuses, Map<String, String> extraMetadata) {
     assertEquals(String.valueOf(this.commitTime), commitTime);
+    return true;
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableRollback.java
Patch:
@@ -143,7 +143,7 @@ void testCOWToMORConvertedTableRollback(boolean rollbackUsingMarkers) throws Exc
   @ValueSource(booleans = {true, false})
   void testRollbackWithDeltaAndCompactionCommit(boolean rollbackUsingMarkers) throws Exception {
     HoodieWriteConfig.Builder cfgBuilder = getConfigBuilder(false, rollbackUsingMarkers, HoodieIndex.IndexType.SIMPLE)
-        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true).build());
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build());
     addConfigsForPopulateMetaFields(cfgBuilder, true);
     HoodieWriteConfig cfg = cfgBuilder.build();
 

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -44,7 +44,7 @@ public final class HoodieMetadataConfig extends HoodieConfig {
   // Enable the internal Metadata Table which saves file listings
   public static final ConfigProperty<Boolean> ENABLE = ConfigProperty
       .key(METADATA_PREFIX + ".enable")
-      .defaultValue(true)
+      .defaultValue(false)
       .sinceVersion("0.7.0")
       .withDocumentation("Enable the internal metadata table which serves table metadata like level file listings");
 

File: hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java
Patch:
@@ -246,7 +246,7 @@ public Option getRecordByKey(String key, Schema readerSchema) throws IOException
 
     synchronized (this) {
       if (keyScanner == null) {
-        keyScanner = reader.getScanner(false, true);
+        keyScanner = reader.getScanner(false, false);
       }
 
       if (keyScanner.seekTo(kv) == 0) {

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java
Patch:
@@ -59,7 +59,6 @@
 import java.math.BigDecimal;
 import java.nio.ByteBuffer;
 import java.nio.charset.StandardCharsets;
-import java.sql.Timestamp;
 import java.time.LocalDate;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -542,8 +541,6 @@ public static Object convertValueForSpecificDataTypes(Schema fieldSchema, Object
   private static Object convertValueForAvroLogicalTypes(Schema fieldSchema, Object fieldValue) {
     if (fieldSchema.getLogicalType() == LogicalTypes.date()) {
       return LocalDate.ofEpochDay(Long.parseLong(fieldValue.toString()));
-    } else if (fieldSchema.getLogicalType() == LogicalTypes.timestampMicros()) {
-      return new Timestamp(Long.parseLong(fieldValue.toString()) / 1000);
     } else if (fieldSchema.getLogicalType() instanceof LogicalTypes.Decimal) {
       Decimal dc = (Decimal) fieldSchema.getLogicalType();
       DecimalConversion decimalConversion = new DecimalConversion();

File: hudi-common/src/main/java/org/apache/hudi/keygen/constant/KeyGeneratorOptions.java
Patch:
@@ -52,7 +52,7 @@ public class KeyGeneratorOptions extends HoodieConfig {
 
   public static final ConfigProperty<String> PARTITIONPATH_FIELD_NAME = ConfigProperty
       .key("hoodie.datasource.write.partitionpath.field")
-      .defaultValue("partitionpath")
+      .noDefaultValue()
       .withDocumentation("Partition path field. Value to be used at the partitionPath component of HoodieKey. "
           + "Actual value ontained by invoking .toString()");
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieUpsertException;
 import org.apache.hudi.keygen.BaseKeyGenerator;
+import org.apache.hudi.keygen.KeyGenUtils;
 import org.apache.hudi.table.HoodieTable;
 
 import org.apache.avro.generic.GenericRecord;
@@ -72,7 +73,7 @@ public HoodieSortedMergeHandle(HoodieWriteConfig config, String instantTime, Hoo
    */
   @Override
   public void write(GenericRecord oldRecord) {
-    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();
+    String key = KeyGenUtils.getRecordKeyFromGenericRecord(oldRecord, keyGeneratorOpt);
 
     // To maintain overall sorted order across updates and inserts, write any new inserts whose keys are less than
     // the oldRecord's key.

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieFileWriterFactory.java
Patch:
@@ -89,7 +89,7 @@ private static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFi
         config.getHFileCompressionAlgorithm(), config.getHFileBlockSize(), config.getHFileMaxFileSize(),
         PREFETCH_ON_OPEN, CACHE_DATA_IN_L1, DROP_BEHIND_CACHE_COMPACTION, filter, HFILE_COMPARATOR);
 
-    return new HoodieHFileWriter<>(instantTime, path, hfileConfig, schema, taskContextSupplier);
+    return new HoodieHFileWriter<>(instantTime, path, hfileConfig, schema, taskContextSupplier, config.populateMetaFields());
   }
 
   private static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFileWriter<R> newOrcFileWriter(

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieMetadataBase.java
Patch:
@@ -292,7 +292,7 @@ protected HoodieWriteConfig.Builder getWriteConfigBuilder(HoodieFailedWritesClea
             .enable(useFileListingMetadata)
             .enableFullScan(enableFullScan)
             .enableMetrics(enableMetrics)
-            .withPopulateMetaFields(false)
+            .withPopulateMetaFields(HoodieMetadataConfig.POPULATE_META_FIELDS.defaultValue())
             .ignoreSpuriousDeletes(validateMetadataPayloadConsistency)
             .build())
         .withMetricsConfig(HoodieMetricsConfig.newBuilder().on(enableMetrics)

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMergedLogRecordReader.java
Patch:
@@ -28,6 +28,8 @@
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.fs.FileSystem;
+
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.util.SpillableMapUtils;
 import org.apache.log4j.LogManager;
@@ -142,7 +144,7 @@ protected String getKeyField() {
    */
   public static class Builder extends HoodieMergedLogRecordScanner.Builder {
     private Set<String> mergeKeyFilter = Collections.emptySet();
-    private boolean enableFullScan;
+    private boolean enableFullScan = HoodieMetadataConfig.ENABLE_FULL_SCAN_LOG_FILES.defaultValue();
     private boolean enableInlineReading;
 
     @Override

File: hudi-flink/src/main/java/org/apache/hudi/sink/partitioner/BucketAssignFunction.java
Patch:
@@ -121,16 +121,16 @@ public void open(Configuration parameters) throws Exception {
         getRuntimeContext().getIndexOfThisSubtask(),
         getRuntimeContext().getMaxNumberOfParallelSubtasks(),
         getRuntimeContext().getNumberOfParallelSubtasks(),
-        ignoreSmallFiles(writeConfig),
+        ignoreSmallFiles(),
         HoodieTableType.valueOf(conf.getString(FlinkOptions.TABLE_TYPE)),
         context,
         writeConfig);
     this.payloadCreation = PayloadCreation.instance(this.conf);
   }
 
-  private boolean ignoreSmallFiles(HoodieWriteConfig writeConfig) {
+  private boolean ignoreSmallFiles() {
     WriteOperationType operationType = WriteOperationType.fromValue(conf.getString(FlinkOptions.OPERATION));
-    return WriteOperationType.isOverwrite(operationType) || writeConfig.allowDuplicateInserts();
+    return WriteOperationType.isOverwrite(operationType);
   }
 
   @Override

File: hudi-flink/src/test/java/org/apache/hudi/sink/partitioner/TestBucketAssigner.java
Patch:
@@ -401,7 +401,7 @@ public void testWriteProfileMetadataCache() throws Exception {
   }
 
   private static String getLastCompleteInstant(WriteProfile profile) {
-    return StreamerUtil.getLastCompletedInstant(profile.getTable().getMetaClient());
+    return StreamerUtil.getLastCompletedInstant(profile.getMetaClient());
   }
 
   private void assertBucketEquals(

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -204,7 +204,9 @@ private HoodieWriteConfig createMetadataWriteConfig(HoodieWriteConfig writeConfi
             .archiveCommitsWith(minCommitsToKeep, maxCommitsToKeep)
             // we will trigger compaction manually, to control the instant times
             .withInlineCompaction(false)
-            .withMaxNumDeltaCommitsBeforeCompaction(writeConfig.getMetadataCompactDeltaCommitMax()).build())
+            .withMaxNumDeltaCommitsBeforeCompaction(writeConfig.getMetadataCompactDeltaCommitMax())
+            // we will trigger archive manually, to ensure only regular writer invokes it
+            .withAutoArchive(false).build())
         .withParallelism(parallelism, parallelism)
         .withDeleteParallelism(parallelism)
         .withRollbackParallelism(parallelism)

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -140,6 +140,7 @@ protected void commit(HoodieData<HoodieRecord> hoodieDataRecords, String partiti
       if (canTriggerTableService) {
         compactIfNecessary(writeClient, instantTime);
         doClean(writeClient, instantTime);
+        writeClient.archive();
       }
     }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -155,6 +155,7 @@ protected void commit(HoodieData<HoodieRecord> hoodieDataRecords, String partiti
       if (canTriggerTableService) {
         compactIfNecessary(writeClient, instantTime);
         doClean(writeClient, instantTime);
+        writeClient.archive();
       }
     }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/spark/OrderingIndexHelper.java
Patch:
@@ -100,7 +100,7 @@ public static Dataset<Row> createOptimizedDataFrameByMapValue(Dataset<Row> df, L
     }
     // only one col to sort, no need to use z-order
     if (sortCols.size() == 1) {
-      return df.repartitionByRange(fieldNum, org.apache.spark.sql.functions.col(sortCols.get(0)));
+      return df.repartitionByRange(fileNum, org.apache.spark.sql.functions.col(sortCols.get(0)));
     }
     Map<Integer, StructField> fieldMap = sortCols
         .stream().collect(Collectors.toMap(e -> Arrays.asList(df.schema().fields()).indexOf(columnsMap.get(e)), e -> columnsMap.get(e)));

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestDFSPropertiesConfiguration.java
Patch:
@@ -145,7 +145,7 @@ public void testIncludes() {
   }
 
   @Test
-  public void testLocalFileSystemLoading() {
+  public void testLocalFileSystemLoading() throws IOException {
     DFSPropertiesConfiguration cfg = new DFSPropertiesConfiguration(dfs.getConf(), new Path(dfsBasePath + "/t1.props"));
 
     cfg.addPropsFromFile(
@@ -156,8 +156,7 @@ public void testLocalFileSystemLoading() {
                     .getResource("props/test.properties")
                     .getPath()
             )
-        )
-    );
+        ));
 
     TypedProperties props = cfg.getProps();
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
 import org.apache.hudi.common.model.HoodieFileFormat;
-import org.apache.hudi.common.model.DefaultHoodieRecordPayload;
+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.model.WriteConcurrencyMode;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.marker.MarkerType;
@@ -103,7 +103,7 @@ public class HoodieWriteConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> WRITE_PAYLOAD_CLASS_NAME = ConfigProperty
       .key("hoodie.datasource.write.payload.class")
-      .defaultValue(DefaultHoodieRecordPayload.class.getName())
+      .defaultValue(OverwriteWithLatestAvroPayload.class.getName())
       .withDocumentation("Payload class used. Override this, if you like to roll your own merge logic, when upserting/inserting. "
           + "This will render any value set for PRECOMBINE_FIELD_OPT_VAL in-effective");
 

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/KafkaConnectWriterProvider.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.HoodieJavaEngineContext;
 import org.apache.hudi.common.config.TypedProperties;
+import org.apache.hudi.common.engine.EngineType;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieAvroPayload;
 import org.apache.hudi.common.util.ReflectionUtils;
@@ -74,6 +75,7 @@ public KafkaConnectWriterProvider(
 
       // Create the write client to write some records in
       writeConfig = HoodieWriteConfig.newBuilder()
+          .withEngineType(EngineType.JAVA)
           .withProperties(connectConfigs.getProps())
           .withFileIdPrefixProviderClassName(KafkaConnectFileIdPrefixProvider.class.getName())
           .withProps(Collections.singletonMap(

File: hudi-kafka-connect/src/test/java/org/apache/hudi/writers/TestBufferedConnectWriter.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.client.HoodieJavaWriteClient;
 import org.apache.hudi.client.common.HoodieJavaEngineContext;
+import org.apache.hudi.common.engine.EngineType;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
@@ -62,6 +63,7 @@ public void setUp() throws Exception {
     configs = KafkaConnectConfigs.newBuilder().build();
     schemaProvider = new TestAbstractConnectWriter.TestSchemaProvider();
     writeConfig = HoodieWriteConfig.newBuilder()
+        .withEngineType(EngineType.JAVA)
         .withPath("/tmp")
         .withSchema(schemaProvider.getSourceSchema().toString())
         .build();

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -466,7 +466,7 @@ public String getHiveStylePartitioningEnable() {
     return getString(HIVE_STYLE_PARTITIONING_ENABLE);
   }
 
-  public String getUrlEncodePartitoning() {
+  public String getUrlEncodePartitioning() {
     return getString(URL_ENCODE_PARTITIONING);
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java
Patch:
@@ -865,7 +865,7 @@ private Option<HoodiePendingRollbackInfo> getPendingRollbackInfo(HoodieTableMeta
   /**
    * Fetch map of pending commits to be rolledback to {@link HoodiePendingRollbackInfo}.
    * @param metaClient instance of {@link HoodieTableMetaClient} to use.
-   * @return map of pending commits to be rolledback instants to Rollback Instnat and Rollback plan Pair.
+   * @return map of pending commits to be rolledback instants to Rollback Instant and Rollback plan Pair.
    */
   protected Map<String, Option<HoodiePendingRollbackInfo>> getPendingRollbackInfos(HoodieTableMetaClient metaClient) {
     return metaClient.getActiveTimeline().filterPendingRollbackTimeline().getInstants().map(

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/RandomFileIdPrefixProvider.java
Patch:
@@ -18,13 +18,12 @@
 
 package org.apache.hudi.table;
 
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.fs.FSUtils;
 
-import java.util.Properties;
-
 public class RandomFileIdPrefixProvider extends FileIdPrefixProvider {
 
-  public RandomFileIdPrefixProvider(Properties props) {
+  public RandomFileIdPrefixProvider(TypedProperties props) {
     super(props);
   }
 

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/KafkaConnectFileIdPrefixProvider.java
Patch:
@@ -18,23 +18,22 @@
 
 package org.apache.hudi.connect;
 
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.connect.utils.KafkaConnectUtils;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.table.FileIdPrefixProvider;
 
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
-import java.util.Properties;
-
 public class KafkaConnectFileIdPrefixProvider extends FileIdPrefixProvider {
 
   public static final String KAFKA_CONNECT_PARTITION_ID = "hudi.kafka.connect.partition";
   private static final Logger LOG = LogManager.getLogger(KafkaConnectFileIdPrefixProvider.class);
 
   private final String kafkaPartition;
 
-  public KafkaConnectFileIdPrefixProvider(Properties props) {
+  public KafkaConnectFileIdPrefixProvider(TypedProperties props) {
     super(props);
     if (!props.containsKey(KAFKA_CONNECT_PARTITION_ID)) {
       LOG.error("Fatal error due to Kafka Connect Partition Id is not set");

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkEnvCommand.java
Patch:
@@ -43,6 +43,7 @@ public void setEnv(@CliOption(key = {"conf"}, help = "Env config to be set") fin
       throw new IllegalArgumentException("Illegal set parameter, please use like [set --conf SPARK_HOME=/usr/etc/spark]");
     }
     env.put(map[0].trim(), map[1].trim());
+    System.setProperty(map[0].trim(), map[1].trim());
   }
 
   @CliCommand(value = "show envs all", help = "Show spark launcher envs")

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestArchivedCommitsCommand.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieSparkTable;
@@ -72,6 +73,8 @@ public void init() throws Exception {
     HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)
         .withSchema(HoodieTestCommitMetadataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().retainCommits(1).archiveCommitsWith(2, 3).build())
+        .withFileSystemViewConfig(FileSystemViewStorageConfig.newBuilder()
+            .withRemoteServerPort(timelineServicePort).build())
         .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build())
         .forTable("test-trip-table").build();
 

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCommitsCommand.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.testutils.HoodieTestUtils;
 import org.apache.hudi.common.util.NumericUtils;
@@ -209,6 +210,8 @@ public void testShowArchivedCommits() throws Exception {
     HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath1)
         .withSchema(HoodieTestCommitMetadataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().retainCommits(1).archiveCommitsWith(2, 3).build())
+        .withFileSystemViewConfig(FileSystemViewStorageConfig.newBuilder()
+            .withRemoteServerPort(timelineServicePort).build())
         .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build())
         .forTable("test-trip-table").build();
 

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCompactionCommand.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.testutils.CompactionTestUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieCompactionConfig;
@@ -159,6 +160,8 @@ private void generateArchive() throws IOException {
     HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)
         .withSchema(HoodieTestCommitMetadataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().retainCommits(1).archiveCommitsWith(2, 3).build())
+        .withFileSystemViewConfig(FileSystemViewStorageConfig.newBuilder()
+            .withRemoteServerPort(timelineServicePort).build())
         .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build())
         .forTable("test-trip-table").build();
     // archive

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -1316,6 +1316,7 @@ public HoodieWriteConfig.Builder getConfigBuilder(String schemaStr, HoodieIndex.
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType).build())
         .withEmbeddedTimelineServerEnabled(true).withFileSystemViewConfig(FileSystemViewStorageConfig.newBuilder()
             .withEnableBackupForRemoteFileSystemView(false) // Fail test if problem connecting to timeline-server
+            .withRemoteServerPort(timelineServicePort)
             .withStorageType(FileSystemViewStorageType.EMBEDDED_KV_STORE).build());
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieMetadataBase.java
Patch:
@@ -85,6 +85,7 @@ public void init(HoodieTableType tableType, boolean enableMetadataTable, boolean
     initSparkContexts("TestHoodieMetadata");
     initFileSystem();
     fs.mkdirs(new Path(basePath));
+    initTimelineService();
     initMetaClient(tableType);
     initTestDataGenerator();
     metadataTableBasePath = HoodieTableMetadata.getMetadataTableBasePath(basePath);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieMetadataBootstrap.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieTableType;
+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.testutils.HoodieMetadataTestTable;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.hudi.common.testutils.HoodieTestTable;
@@ -241,6 +242,8 @@ private HoodieWriteConfig getWriteConfig(int minArchivalCommits, int maxArchival
     return HoodieWriteConfig.newBuilder().withPath(basePath)
         .withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().retainCommits(1).archiveCommitsWith(minArchivalCommits, maxArchivalCommits).build())
+        .withFileSystemViewConfig(FileSystemViewStorageConfig.newBuilder()
+            .withRemoteServerPort(timelineServicePort).build())
         .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build())
         .forTable("test-trip-table").build();
   }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestBase.java
Patch:
@@ -152,6 +152,7 @@ public HoodieWriteConfig.Builder getConfigBuilder(String schemaStr, IndexType in
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType).build())
         .withEmbeddedTimelineServerEnabled(true).withFileSystemViewConfig(FileSystemViewStorageConfig.newBuilder()
             .withEnableBackupForRemoteFileSystemView(false) // Fail test if problem connecting to timeline-server
+            .withRemoteServerPort(timelineServicePort)
             .withStorageType(FileSystemViewStorageType.EMBEDDED_KV_STORE).build());
   }
 

File: hudi-spark-datasource/hudi-spark-common/src/test/java/org/apache/hudi/internal/HoodieBulkInsertInternalWriterTestBase.java
Patch:
@@ -77,7 +77,7 @@ protected HoodieWriteConfig getWriteConfig(boolean populateMetaFields) {
       properties.setProperty(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), SparkDatasetTestUtils.PARTITION_PATH_FIELD_NAME);
       properties.setProperty(HoodieTableConfig.POPULATE_META_FIELDS.key(), "false");
     }
-    return getConfigBuilder(basePath).withProperties(properties).build();
+    return getConfigBuilder(basePath, timelineServicePort).withProperties(properties).build();
   }
 
   protected void assertWriteStatuses(List<HoodieInternalWriteStatus> writeStatuses, int batches, int size,

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java
Patch:
@@ -66,6 +66,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import javax.annotation.Nonnull;
 import java.io.IOException;
 import java.util.Collections;
 import java.util.Iterator;
@@ -244,7 +245,7 @@ public HoodieWriteMetadata deletePartitions(HoodieEngineContext context, String
   }
 
   @Override
-  public void updateStatistics(HoodieEngineContext context, List<HoodieWriteStat> stats, String instantTime, Boolean isOptimizeOperation) {
+  public void updateMetadataIndexes(@Nonnull HoodieEngineContext context, @Nonnull List<HoodieWriteStat> stats, @Nonnull String instantTime) {
     throw new HoodieNotSupportedException("update statistics is not supported yet");
   }
 

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java
Patch:
@@ -71,6 +71,7 @@
 import java.io.IOException;
 import java.util.Collections;
 import java.util.Iterator;
+import javax.annotation.Nonnull;
 import java.util.List;
 import java.util.Map;
 
@@ -170,7 +171,7 @@ public HoodieWriteMetadata<List<WriteStatus>> insertOverwriteTable(HoodieEngineC
   }
 
   @Override
-  public void updateStatistics(HoodieEngineContext context, List<HoodieWriteStat> stats, String instantTime, Boolean isOptimizeOperation) {
+  public void updateMetadataIndexes(@Nonnull HoodieEngineContext context, @Nonnull List<HoodieWriteStat> stats, @Nonnull String instantTime) {
     throw new HoodieNotSupportedException("update statistics is not supported yet");
   }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDSpatialCurveOptimizationSortPartitioner.java
Patch:
@@ -33,7 +33,7 @@
 
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
-import org.apache.spark.ZCurveOptimizeHelper;
+import org.apache.hudi.index.zorder.ZOrderingIndexHelper;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
@@ -79,10 +79,10 @@ private JavaRDD<GenericRecord> prepareGenericRecord(JavaRDD<HoodieRecord<T>> inp
 
     switch (config.getLayoutOptimizationCurveBuildMethod()) {
       case DIRECT:
-        zDataFrame = ZCurveOptimizeHelper.createZIndexedDataFrameByMapValue(originDF, config.getClusteringSortColumns(), numOutputGroups);
+        zDataFrame = ZOrderingIndexHelper.createZIndexedDataFrameByMapValue(originDF, config.getClusteringSortColumns(), numOutputGroups);
         break;
       case SAMPLE:
-        zDataFrame = ZCurveOptimizeHelper.createZIndexedDataFrameBySample(originDF, config.getClusteringSortColumns(), numOutputGroups);
+        zDataFrame = ZOrderingIndexHelper.createZIndexedDataFrameBySample(originDF, config.getClusteringSortColumns(), numOutputGroups);
         break;
       default:
         throw new HoodieException("Not a valid build curve method for doWriteOperation: ");

File: hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java
Patch:
@@ -40,7 +40,7 @@
  */
 public class SparkUtil {
 
-  private static final String DEFAULT_SPARK_MASTER = "yarn";
+  public static final String DEFAULT_SPARK_MASTER = "yarn";
 
   /**
    * TODO: Need to fix a bunch of hardcoded stuff here eg: history server, spark distro.

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java
Patch:
@@ -66,7 +66,7 @@ public class HoodieClusteringJob {
   public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {
     this.cfg = cfg;
     this.jsc = jsc;
-    this.props = cfg.propsFilePath == null
+    this.props = StringUtils.isNullOrEmpty(cfg.propsFilePath)
         ? UtilHelpers.buildProperties(cfg.configs)
         : readConfigFromFileSystem(jsc, cfg);
     this.metaClient = UtilHelpers.createMetaClient(jsc, cfg.basePath, true);

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -360,7 +360,7 @@ private static int doBootstrap(JavaSparkContext jsc, String tableName, String ta
       String payloadClassName, String enableHiveSync, String propsFilePath, List<String> configs) throws IOException {
 
     TypedProperties properties = propsFilePath == null ? UtilHelpers.buildProperties(configs)
-        : UtilHelpers.readConfig(FSUtils.getFs(propsFilePath, jsc.hadoopConfiguration()), new Path(propsFilePath), configs).getProps(true);
+        : UtilHelpers.readConfig(jsc.hadoopConfiguration(), new Path(propsFilePath), configs).getProps(true);
 
     properties.setProperty(HoodieBootstrapConfig.BASE_PATH.key(), sourcePath);
 

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/metrics/TestMetricsReporterFactory.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.hudi.metrics;
 
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.config.HoodieWriteConfig;
 
 import com.codahale.metrics.MetricRegistry;
@@ -57,7 +58,7 @@ public void metricsReporterFactoryShouldReturnReporter() {
   public void metricsReporterFactoryShouldReturnUserDefinedReporter() {
     when(config.getMetricReporterClassName()).thenReturn(DummyMetricsReporter.class.getName());
 
-    Properties props = new Properties();
+    TypedProperties props = new TypedProperties();
     props.setProperty("testKey", "testValue");
 
     when(config.getProps()).thenReturn(props);
@@ -70,7 +71,7 @@ public void metricsReporterFactoryShouldReturnUserDefinedReporter() {
   @Test
   public void metricsReporterFactoryShouldThrowExceptionWhenMetricsReporterClassIsIllegal() {
     when(config.getMetricReporterClassName()).thenReturn(IllegalTestMetricsReporter.class.getName());
-    when(config.getProps()).thenReturn(new Properties());
+    when(config.getProps()).thenReturn(new TypedProperties());
     assertThrows(HoodieException.class, () -> MetricsReporterFactory.createReporter(config, registry));
   }
 

File: hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java
Patch:
@@ -102,7 +102,7 @@ public static TypedProperties getProps(FlinkStreamerConfig cfg) {
       return new TypedProperties();
     }
     return readConfig(
-        FSUtils.getFs(cfg.propsFilePath, getHadoopConf()),
+        getHadoopConf(),
         new Path(cfg.propsFilePath), cfg.configs).getProps();
   }
 
@@ -127,8 +127,8 @@ public static Schema getSourceSchema(org.apache.flink.configuration.Configuratio
   /**
    * Read config from properties file (`--props` option) and cmd line (`--hoodie-conf` option).
    */
-  public static DFSPropertiesConfiguration readConfig(FileSystem fs, Path cfgPath, List<String> overriddenProps) {
-    DFSPropertiesConfiguration conf = new DFSPropertiesConfiguration(fs, cfgPath);
+  public static DFSPropertiesConfiguration readConfig(org.apache.hadoop.conf.Configuration hadoopConfig, Path cfgPath, List<String> overriddenProps) {
+    DFSPropertiesConfiguration conf = new DFSPropertiesConfiguration(hadoopConfig, cfgPath);
     try {
       if (!overriddenProps.isEmpty()) {
         LOG.info("Adding overridden properties to file properties.");

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteJob.java
Patch:
@@ -103,7 +103,7 @@ public HoodieTestSuiteJob(HoodieTestSuiteConfig cfg, JavaSparkContext jsc) throw
     cfg.propsFilePath = FSUtils.addSchemeIfLocalPath(cfg.propsFilePath).toString();
     this.sparkSession = SparkSession.builder().config(jsc.getConf()).enableHiveSupport().getOrCreate();
     this.fs = FSUtils.getFs(cfg.inputBasePath, jsc.hadoopConfiguration());
-    this.props = UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getProps();
+    this.props = UtilHelpers.readConfig(fs.getConf(), new Path(cfg.propsFilePath), cfg.configs).getProps();
     log.info("Creating workload generator with configs : {}", props.toString());
     this.hiveConf = getDefaultHiveConf(jsc.hadoopConfiguration());
     this.keyGenerator = (BuiltinKeyGenerator) HoodieSparkKeyGeneratorFactory.createKeyGenerator(props);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java
Patch:
@@ -112,7 +112,7 @@ private boolean isUpsert() {
   public int dataImport(JavaSparkContext jsc, int retry) {
     this.fs = FSUtils.getFs(cfg.targetPath, jsc.hadoopConfiguration());
     this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)
-        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getProps(true);
+        : UtilHelpers.readConfig(fs.getConf(), new Path(cfg.propsFilePath), cfg.configs).getProps(true);
     LOG.info("Starting data import with configs : " + props.toString());
     int ret = -1;
     try {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java
Patch:
@@ -77,7 +77,7 @@ public HoodieMultiTableDeltaStreamer(Config config, JavaSparkContext jssc) throw
     FileSystem fs = FSUtils.getFs(commonPropsFile, jssc.hadoopConfiguration());
     configFolder = configFolder.charAt(configFolder.length() - 1) == '/' ? configFolder.substring(0, configFolder.length() - 1) : configFolder;
     checkIfPropsFileAndConfigFolderExist(commonPropsFile, configFolder, fs);
-    TypedProperties commonProperties = UtilHelpers.readConfig(fs, new Path(commonPropsFile), new ArrayList<>()).getProps();
+    TypedProperties commonProperties = UtilHelpers.readConfig(fs.getConf(), new Path(commonPropsFile), new ArrayList<String>()).getProps();
     //get the tables to be ingested and their corresponding config files from this properties instance
     populateTableExecutionContextList(commonProperties, configFolder, fs, config);
   }
@@ -116,7 +116,7 @@ private void populateTableExecutionContextList(TypedProperties properties, Strin
       String configProp = Constants.INGESTION_PREFIX + database + Constants.DELIMITER + currentTable + Constants.INGESTION_CONFIG_SUFFIX;
       String configFilePath = properties.getString(configProp, Helpers.getDefaultConfigFilePath(configFolder, database, currentTable));
       checkIfTableConfigFileExists(configFolder, fs, configFilePath);
-      TypedProperties tableProperties = UtilHelpers.readConfig(fs, new Path(configFilePath), new ArrayList<>()).getProps();
+      TypedProperties tableProperties = UtilHelpers.readConfig(fs.getConf(), new Path(configFilePath), new ArrayList<String>()).getProps();
       properties.forEach((k, v) -> {
         if (tableProperties.get(k) == null) {
           tableProperties.setProperty(k.toString(), v.toString());

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java
Patch:
@@ -378,7 +378,7 @@ static void assertAtLeastNReplaceRequests(int minExpected, String tablePath, Fil
   @Test
   public void testProps() {
     TypedProperties props =
-        new DFSPropertiesConfiguration(dfs, new Path(dfsBasePath + "/" + PROPS_FILENAME_TEST_SOURCE)).getProps();
+        new DFSPropertiesConfiguration(dfs.getConf(), new Path(dfsBasePath + "/" + PROPS_FILENAME_TEST_SOURCE)).getProps();
     assertEquals(2, props.getInteger("hoodie.upsert.shuffle.parallelism"));
     assertEquals("_row_key", props.getString("hoodie.datasource.write.recordkey.field"));
     assertEquals("org.apache.hudi.utilities.functional.TestHoodieDeltaStreamer$TestGenerator",
@@ -498,7 +498,7 @@ public void testKafkaConnectCheckpointProvider() throws IOException {
     String checkpointProviderClass = "org.apache.hudi.utilities.checkpointing.KafkaConnectHdfsProvider";
     HoodieDeltaStreamer.Config cfg = TestHelpers.makeDropAllConfig(tableBasePath, WriteOperationType.UPSERT);
     TypedProperties props =
-        new DFSPropertiesConfiguration(dfs, new Path(dfsBasePath + "/" + PROPS_FILENAME_TEST_SOURCE)).getProps();
+        new DFSPropertiesConfiguration(dfs.getConf(), new Path(dfsBasePath + "/" + PROPS_FILENAME_TEST_SOURCE)).getProps();
     props.put("hoodie.deltastreamer.checkpoint.provider.path", bootstrapPath);
     cfg.initialCheckpointProvider = checkpointProviderClass;
     // create regular kafka connect hdfs dirs

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/debezium/MysqlDebeziumSource.java
Patch:
@@ -68,7 +68,6 @@ protected Dataset<Row> processDataset(Dataset<Row> rowDataset) {
               String.format("%s as %s", DebeziumConstants.INCOMING_TS_MS_FIELD, DebeziumConstants.UPSTREAM_PROCESSING_TS_COL_NAME),
               String.format("%s as %s", DebeziumConstants.INCOMING_SOURCE_NAME_FIELD, DebeziumConstants.FLATTENED_SHARD_NAME),
               String.format("%s as %s", DebeziumConstants.INCOMING_SOURCE_TS_MS_FIELD, DebeziumConstants.FLATTENED_TS_COL_NAME),
-              String.format("%s as %s", DebeziumConstants.INCOMING_SOURCE_TXID_FIELD, DebeziumConstants.FLATTENED_TX_ID_COL_NAME),
               String.format("%s as %s", DebeziumConstants.INCOMING_SOURCE_FILE_FIELD, DebeziumConstants.FLATTENED_FILE_COL_NAME),
               String.format("%s as %s", DebeziumConstants.INCOMING_SOURCE_POS_FIELD, DebeziumConstants.FLATTENED_POS_COL_NAME),
               String.format("%s as %s", DebeziumConstants.INCOMING_SOURCE_ROW_FIELD, DebeziumConstants.FLATTENED_ROW_COL_NAME),
@@ -82,7 +81,6 @@ protected Dataset<Row> processDataset(Dataset<Row> rowDataset) {
               String.format("%s as %s", DebeziumConstants.INCOMING_TS_MS_FIELD, DebeziumConstants.UPSTREAM_PROCESSING_TS_COL_NAME),
               String.format("%s as %s", DebeziumConstants.INCOMING_SOURCE_NAME_FIELD, DebeziumConstants.FLATTENED_SHARD_NAME),
               String.format("%s as %s", DebeziumConstants.INCOMING_SOURCE_TS_MS_FIELD, DebeziumConstants.FLATTENED_TS_COL_NAME),
-              String.format("%s as %s", DebeziumConstants.INCOMING_SOURCE_TXID_FIELD, DebeziumConstants.FLATTENED_TX_ID_COL_NAME),
               String.format("%s as %s", DebeziumConstants.INCOMING_SOURCE_FILE_FIELD, DebeziumConstants.FLATTENED_FILE_COL_NAME),
               String.format("%s as %s", DebeziumConstants.INCOMING_SOURCE_POS_FIELD, DebeziumConstants.FLATTENED_POS_COL_NAME),
               String.format("%s as %s", DebeziumConstants.INCOMING_SOURCE_ROW_FIELD, DebeziumConstants.FLATTENED_ROW_COL_NAME),

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
 import org.apache.hudi.common.model.HoodieFileFormat;
-import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
+import org.apache.hudi.common.model.DefaultHoodieRecordPayload;
 import org.apache.hudi.common.model.WriteConcurrencyMode;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.marker.MarkerType;
@@ -102,7 +102,7 @@ public class HoodieWriteConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> WRITE_PAYLOAD_CLASS_NAME = ConfigProperty
       .key("hoodie.datasource.write.payload.class")
-      .defaultValue(OverwriteWithLatestAvroPayload.class.getName())
+      .defaultValue(DefaultHoodieRecordPayload.class.getName())
       .withDocumentation("Payload class used. Override this, if you like to roll your own merge logic, when upserting/inserting. "
           + "This will render any value set for PRECOMBINE_FIELD_OPT_VAL in-effective");
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java
Patch:
@@ -49,7 +49,7 @@ public class SchemaRegistryProvider extends SchemaProvider {
    */
   public static class Config {
 
-    private static final String SRC_SCHEMA_REGISTRY_URL_PROP = "hoodie.deltastreamer.schemaprovider.registry.url";
+    public static final String SRC_SCHEMA_REGISTRY_URL_PROP = "hoodie.deltastreamer.schemaprovider.registry.url";
     private static final String TARGET_SCHEMA_REGISTRY_URL_PROP =
         "hoodie.deltastreamer.schemaprovider.registry.targetUrl";
   }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamerWithMultiWriter.java
Patch:
@@ -19,7 +19,6 @@
 
 package org.apache.hudi.utilities.functional;
 
-import org.apache.hudi.DataSourceWriteOptions;
 import org.apache.hudi.common.config.LockConfiguration;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
@@ -28,7 +27,6 @@
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.config.HoodieCompactionConfig;
-import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.execution.bulkinsert.BulkInsertSortMode;
 import org.apache.hudi.testutils.SparkClientFunctionalTestHarness;
 import org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer;
@@ -37,7 +35,6 @@
 import org.apache.hudi.utilities.testutils.sources.config.SourceConfigs;
 
 import org.apache.hadoop.fs.FileSystem;
-import org.apache.spark.sql.SaveMode;
 import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.Tag;
 import org.junit.jupiter.params.ParameterizedTest;

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -1418,8 +1418,7 @@ public void testPendingClusteringRollback(boolean populateMetaFields) throws Exc
     dataGen = new HoodieTestDataGenerator();
     String commitTime = HoodieActiveTimeline.createNewInstantTime();
     allRecords.addAll(dataGen.generateInserts(commitTime, 200));
-    writeAndVerifyBatch(client, allRecords, commitTime, populateMetaFields);
-
+    assertThrows(HoodieUpsertException.class, () -> writeAndVerifyBatch(client, allRecords, commitTime, populateMetaFields));
     // verify pending clustering can be rolled back (even though there is a completed commit greater than pending clustering)
     client.rollback(pendingClusteringInstant.getTimestamp());
     metaClient.reloadActiveTimeline();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDSpatialCurveOptimizationSortPartitioner.java
Patch:
@@ -39,7 +39,7 @@
 import org.apache.spark.sql.Row;
 
 /**
- * A partitioner that does spartial curve optimization sorting based on specified column values for each RDD partition.
+ * A partitioner that does spatial curve optimization sorting based on specified column values for each RDD partition.
  * support z-curve optimization, hilbert will come soon.
  * @param <T> HoodieRecordPayload type
  */

File: hudi-common/src/test/java/org/apache/hudi/common/fs/inline/TestInLineFileSystemHFileInLining.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.common.fs.inline;
 
 import org.apache.hudi.common.testutils.FileSystemTestUtils;
+import org.apache.hudi.io.storage.HoodieHBaseKVComparator;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataInputStream;
@@ -92,7 +93,7 @@ public void testSimpleInlineFileSystem() throws IOException {
     HFile.Writer writer = HFile.getWriterFactory(inMemoryConf, cacheConf)
         .withOutputStream(fout)
         .withFileContext(meta)
-        .withComparator(new KeyValue.KVComparator())
+        .withComparator(new HoodieHBaseKVComparator())
         .create();
 
     writeRecords(writer);

File: hudi-flink/src/main/java/org/apache/hudi/sink/bulk/BulkInsertWriteFunction.java
Patch:
@@ -111,8 +111,6 @@ public BulkInsertWriteFunction(Configuration config, RowType rowType) {
 
   @Override
   public void open(Configuration parameters) throws IOException {
-    // always use the user classloader
-    Thread.currentThread().setContextClassLoader(getRuntimeContext().getUserCodeClassLoader());
     this.taskID = getRuntimeContext().getIndexOfThisSubtask();
     this.metaClient = StreamerUtil.createMetaClient(this.config);
     this.writeClient = StreamerUtil.createWriteClient(this.config, getRuntimeContext());

File: hudi-flink/src/main/java/org/apache/hudi/sink/common/AbstractStreamWriteFunction.java
Patch:
@@ -125,8 +125,6 @@ public AbstractStreamWriteFunction(Configuration config) {
 
   @Override
   public void initializeState(FunctionInitializationContext context) throws Exception {
-    // always use the user classloader
-    Thread.currentThread().setContextClassLoader(getRuntimeContext().getUserCodeClassLoader());
     this.taskID = getRuntimeContext().getIndexOfThisSubtask();
     this.metaClient = StreamerUtil.createMetaClient(this.config);
     this.writeClient = StreamerUtil.createWriteClient(this.config, getRuntimeContext());

File: hudi-flink/src/main/java/org/apache/hudi/sink/compact/CompactFunction.java
Patch:
@@ -75,8 +75,6 @@ public CompactFunction(Configuration conf) {
 
   @Override
   public void open(Configuration parameters) throws Exception {
-    // always use the user classloader
-    Thread.currentThread().setContextClassLoader(getRuntimeContext().getUserCodeClassLoader());
     this.taskID = getRuntimeContext().getIndexOfThisSubtask();
     this.writeClient = StreamerUtil.createWriteClient(conf, getRuntimeContext());
     if (this.asyncCompaction) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/OneToTwoUpgradeHandler.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.keygen.constant.KeyGeneratorOptions;
 
-import java.util.HashMap;
+import java.util.Hashtable;
 import java.util.Map;
 
 /**
@@ -36,7 +36,7 @@ public class OneToTwoUpgradeHandler implements UpgradeHandler {
   public Map<ConfigProperty, String> upgrade(
       HoodieWriteConfig config, HoodieEngineContext context, String instantTime,
       BaseUpgradeDowngradeHelper upgradeDowngradeHelper) {
-    Map<ConfigProperty, String> tablePropsToAdd = new HashMap<>();
+    Map<ConfigProperty, String> tablePropsToAdd = new Hashtable<>();
     tablePropsToAdd.put(HoodieTableConfig.PARTITION_FIELDS, upgradeDowngradeHelper.getPartitionColumns(config));
     tablePropsToAdd.put(HoodieTableConfig.RECORDKEY_FIELDS, config.getString(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key()));
     tablePropsToAdd.put(HoodieTableConfig.BASE_FILE_FORMAT, config.getString(HoodieTableConfig.BASE_FILE_FORMAT));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
-import java.util.HashMap;
+import java.util.Hashtable;
 import java.util.Map;
 
 /**
@@ -110,7 +110,7 @@ public void run(HoodieTableVersion toVersion, String instantTime) {
 
     // Perform the actual upgrade/downgrade; this has to be idempotent, for now.
     LOG.info("Attempting to move table from version " + fromVersion + " to " + toVersion);
-    Map<ConfigProperty, String> tableProps = new HashMap<>();
+    Map<ConfigProperty, String> tableProps = new Hashtable<>();
     if (fromVersion.versionCode() < toVersion.versionCode()) {
       // upgrade
       while (fromVersion.versionCode() < toVersion.versionCode()) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -2182,7 +2182,8 @@ protected void setDefaults() {
       writeConfig.setDefaultOnCondition(!isCompactionConfigSet,
           HoodieCompactionConfig.newBuilder().fromProperties(writeConfig.getProps()).build());
       writeConfig.setDefaultOnCondition(!isClusteringConfigSet,
-          HoodieClusteringConfig.newBuilder().fromProperties(writeConfig.getProps()).build());
+          HoodieClusteringConfig.newBuilder().withEngineType(engineType)
+              .fromProperties(writeConfig.getProps()).build());
       writeConfig.setDefaultOnCondition(!isMetricsConfigSet, HoodieMetricsConfig.newBuilder().fromProperties(
           writeConfig.getProps()).build());
       writeConfig.setDefaultOnCondition(!isBootstrapConfigSet,

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java
Patch:
@@ -126,13 +126,14 @@ public HoodieWriteMetadata<List<WriteStatus>> execute(List<HoodieRecord<T>> inpu
     return result;
   }
 
-  protected void updateIndex(List<WriteStatus> writeStatuses, HoodieWriteMetadata<List<WriteStatus>> result) {
+  protected List<WriteStatus> updateIndex(List<WriteStatus> writeStatuses, HoodieWriteMetadata<List<WriteStatus>> result) {
     Instant indexStartTime = Instant.now();
     // Update the index back
     List<WriteStatus> statuses = HoodieList.getList(
         table.getIndex().updateLocation(HoodieList.of(writeStatuses), context, table));
     result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));
     result.setWriteStatuses(statuses);
+    return statuses;
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkRecentDaysClusteringPlanStrategy.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;
 import org.apache.hudi.table.HoodieSparkMergeOnReadTable;
+
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
@@ -49,6 +50,7 @@ public SparkRecentDaysClusteringPlanStrategy(HoodieSparkMergeOnReadTable<T> tabl
     super(table, engineContext, writeConfig);
   }
 
+  @Override
   protected List<String> filterPartitionPaths(List<String> partitionPaths) {
     int targetPartitionsForClustering = getWriteConfig().getTargetPartitionsForClustering();
     int skipPartitionsFromLatestForClustering = getWriteConfig().getSkipPartitionsFromLatestForClustering();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java
Patch:
@@ -164,7 +164,7 @@ private int doSchedule(JavaSparkContext jsc) throws Exception {
     // Get schema.
     SparkRDDWriteClient client =
         UtilHelpers.createHoodieClient(jsc, cfg.basePath, "", cfg.parallelism, Option.of(cfg.strategyClassName), props);
-    if (cfg.compactionInstantTime == null) {
+    if (StringUtils.isNullOrEmpty(cfg.compactionInstantTime)) {
       throw new IllegalArgumentException("No instant time is provided for scheduling compaction. "
           + "Please specify the compaction instant time by using --instant-time.");
     }

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java
Patch:
@@ -59,7 +59,7 @@
 /**
  * HoodieInputFormat which understands the Hoodie File Structure and filters files based on the Hoodie Mode. If paths
  * that does not correspond to a hoodie table then they are passed in as is (as what FileInputFormat.listStatus()
- * would do). The JobConf could have paths from multipe Hoodie/Non-Hoodie tables
+ * would do). The JobConf could have paths from multiple Hoodie/Non-Hoodie tables
  */
 @UseRecordReaderFromInputFormat
 @UseFileSplitsFromInputFormat

File: hudi-cli/src/main/java/org/apache/hudi/cli/utils/CommitUtil.java
Patch:
@@ -51,7 +51,7 @@ public static long countNewRecords(HoodieTableMetaClient target, List<String> co
 
   public static String getTimeDaysAgo(int numberOfDays) {
     Date date = Date.from(ZonedDateTime.now().minusDays(numberOfDays).toInstant());
-    return HoodieActiveTimeline.formatInstantTime(date);
+    return HoodieActiveTimeline.formatDate(date);
   }
 
   /**
@@ -61,8 +61,8 @@ public static String getTimeDaysAgo(int numberOfDays) {
    *  b) hours: -1, returns 20200202010000
    */
   public static String addHours(String compactionCommitTime, int hours) throws ParseException {
-    Instant instant = HoodieActiveTimeline.parseInstantTime(compactionCommitTime).toInstant();
+    Instant instant = HoodieActiveTimeline.parseDateFromInstantTime(compactionCommitTime).toInstant();
     ZonedDateTime commitDateTime = ZonedDateTime.ofInstant(instant, ZoneId.systemDefault());
-    return HoodieActiveTimeline.formatInstantTime(Date.from(commitDateTime.plusHours(hours).toInstant()));
+    return HoodieActiveTimeline.formatDate(Date.from(commitDateTime.plusHours(hours).toInstant()));
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java
Patch:
@@ -233,7 +233,7 @@ void emitCommitMetrics(String instantTime, HoodieCommitMetadata metadata, String
 
       if (writeTimer != null) {
         long durationInMs = metrics.getDurationInMs(writeTimer.stop());
-        metrics.updateCommitMetrics(HoodieActiveTimeline.parseInstantTime(instantTime).getTime(), durationInMs,
+        metrics.updateCommitMetrics(HoodieActiveTimeline.parseDateFromInstantTime(instantTime).getTime(), durationInMs,
             metadata, actionType);
         writeTimer = null;
       }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java
Patch:
@@ -184,7 +184,7 @@ private boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy)
   private Long parsedToSeconds(String time) {
     long timestamp;
     try {
-      timestamp = HoodieActiveTimeline.parseInstantTime(time).getTime() / 1000;
+      timestamp = HoodieActiveTimeline.parseDateFromInstantTime(time).getTime() / 1000;
     } catch (ParseException e) {
       throw new HoodieCompactionException(e.getMessage(), e);
     }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -371,7 +371,7 @@ public void completeCompaction(
     if (compactionTimer != null) {
       long durationInMs = metrics.getDurationInMs(compactionTimer.stop());
       try {
-        metrics.updateCommitMetrics(HoodieActiveTimeline.parseInstantTime(compactionCommitTime).getTime(),
+        metrics.updateCommitMetrics(HoodieActiveTimeline.parseDateFromInstantTime(compactionCommitTime).getTime(),
             durationInMs, metadata, HoodieActiveTimeline.COMPACTION_ACTION);
       } catch (ParseException e) {
         throw new HoodieCommitException("Commit time is not of valid format. Failed to commit compaction "

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java
Patch:
@@ -324,7 +324,7 @@ protected void completeCompaction(HoodieCommitMetadata metadata, JavaRDD<WriteSt
     if (compactionTimer != null) {
       long durationInMs = metrics.getDurationInMs(compactionTimer.stop());
       try {
-        metrics.updateCommitMetrics(HoodieActiveTimeline.parseInstantTime(compactionCommitTime).getTime(),
+        metrics.updateCommitMetrics(HoodieActiveTimeline.parseDateFromInstantTime(compactionCommitTime).getTime(),
             durationInMs, metadata, HoodieActiveTimeline.COMPACTION_ACTION);
       } catch (ParseException e) {
         throw new HoodieCommitException("Commit time is not of valid format. Failed to commit compaction "
@@ -405,7 +405,7 @@ private void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<Wr
     if (clusteringTimer != null) {
       long durationInMs = metrics.getDurationInMs(clusteringTimer.stop());
       try {
-        metrics.updateCommitMetrics(HoodieActiveTimeline.parseInstantTime(clusteringCommitTime).getTime(),
+        metrics.updateCommitMetrics(HoodieActiveTimeline.parseDateFromInstantTime(clusteringCommitTime).getTime(),
             durationInMs, metadata, HoodieActiveTimeline.REPLACE_COMMIT_ACTION);
       } catch (ParseException e) {
         throw new HoodieCommitException("Commit time is not of valid format. Failed to commit compaction "

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java
Patch:
@@ -567,7 +567,7 @@ public boolean rollbackCommit(String instantTime) {
          BufferedMutator mutator = hbaseConnection.getBufferedMutator(TableName.valueOf(tableName))) {
       final RateLimiter limiter = RateLimiter.create(multiPutBatchSize, TimeUnit.SECONDS);
 
-      Long rollbackTime = HoodieActiveTimeline.parseInstantTime(instantTime).getTime();
+      Long rollbackTime = HoodieActiveTimeline.parseDateFromInstantTime(instantTime).getTime();
       Long currentTime = new Date().getTime();
       Scan scan = new Scan();
       scan.addFamily(SYSTEM_COLUMN_FAMILY);

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodieWriteStat.java
Patch:
@@ -36,7 +36,7 @@ public class TestHoodieWriteStat {
 
   @Test
   public void testSetPaths() {
-    String instantTime = HoodieActiveTimeline.formatInstantTime(new Date());
+    String instantTime = HoodieActiveTimeline.formatDate(new Date());
     String basePathString = "/data/tables/some-hoodie-table";
     String partitionPathString = "2017/12/31";
     String fileName = UUID.randomUUID().toString();

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestTable.java
Patch:
@@ -148,7 +148,7 @@ public static String makeNewCommitTime() {
   }
 
   public static String makeNewCommitTime(Instant dateTime) {
-    return HoodieActiveTimeline.formatInstantTime(Date.from(dateTime));
+    return HoodieActiveTimeline.formatDate(Date.from(dateTime));
   }
 
   public static List<String> makeIncrementalCommitTimes(int num) {

File: hudi-flink/src/test/java/org/apache/hudi/utils/TestStreamerUtil.java
Patch:
@@ -80,8 +80,9 @@ void testInitTableIfNotExists() throws IOException {
   void testMedianInstantTime() {
     String higher = "20210705125921";
     String lower = "20210705125806";
+    String expectedMedianInstant = "20210705125844499";
     String median1 = StreamerUtil.medianInstantTime(higher, lower).get();
-    assertThat(median1, is("20210705125843"));
+    assertThat(median1, is(expectedMedianInstant));
     // test symmetry
     assertThrows(IllegalArgumentException.class,
         () -> StreamerUtil.medianInstantTime(lower, higher),

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java
Patch:
@@ -241,7 +241,7 @@ private int doScheduleAndCluster(JavaSparkContext jsc) throws Exception {
         HoodieTimeline inflightHoodieTimeline = table.getActiveTimeline().filterPendingReplaceTimeline().filterInflights();
         if (!inflightHoodieTimeline.empty()) {
           HoodieInstant inflightClusteringInstant = inflightHoodieTimeline.lastInstant().get();
-          Date clusteringStartTime = HoodieActiveTimeline.parseInstantTime(inflightClusteringInstant.getTimestamp());
+          Date clusteringStartTime = HoodieActiveTimeline.parseDateFromInstantTime(inflightClusteringInstant.getTimestamp());
           if (clusteringStartTime.getTime() + cfg.maxProcessingTimeMs < System.currentTimeMillis()) {
             // if there has failed clustering, then we will use the failed clustering instant-time to trigger next clustering action which will rollback and clustering.
             LOG.info("Found failed clustering instant at : " + inflightClusteringInstant + "; Will rollback the failed clustering and re-trigger again.");

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java
Patch:
@@ -230,7 +230,7 @@ public void testImportWithUpsert() throws IOException, ParseException {
 
   public List<GenericRecord> createInsertRecords(Path srcFolder) throws ParseException, IOException {
     Path srcFile = new Path(srcFolder.toString(), "file1.parquet");
-    long startTime = HoodieActiveTimeline.parseInstantTime("20170203000000").getTime() / 1000;
+    long startTime = HoodieActiveTimeline.parseDateFromInstantTime("20170203000000").getTime() / 1000;
     List<GenericRecord> records = new ArrayList<GenericRecord>();
     for (long recordNum = 0; recordNum < 96; recordNum++) {
       records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), "0", "rider-" + recordNum,
@@ -247,7 +247,7 @@ public List<GenericRecord> createInsertRecords(Path srcFolder) throws ParseExcep
 
   public List<GenericRecord> createUpsertRecords(Path srcFolder) throws ParseException, IOException {
     Path srcFile = new Path(srcFolder.toString(), "file1.parquet");
-    long startTime = HoodieActiveTimeline.parseInstantTime("20170203000000").getTime() / 1000;
+    long startTime = HoodieActiveTimeline.parseDateFromInstantTime("20170203000000").getTime() / 1000;
     List<GenericRecord> records = new ArrayList<GenericRecord>();
     // 10 for update
     for (long recordNum = 0; recordNum < 11; recordNum++) {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java
Patch:
@@ -241,7 +241,7 @@ private int doScheduleAndCluster(JavaSparkContext jsc) throws Exception {
         HoodieTimeline inflightHoodieTimeline = table.getActiveTimeline().filterPendingReplaceTimeline().filterInflights();
         if (!inflightHoodieTimeline.empty()) {
           HoodieInstant inflightClusteringInstant = inflightHoodieTimeline.lastInstant().get();
-          Date clusteringStartTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(inflightClusteringInstant.getTimestamp());
+          Date clusteringStartTime = HoodieActiveTimeline.parseInstantTime(inflightClusteringInstant.getTimestamp());
           if (clusteringStartTime.getTime() + cfg.maxProcessingTimeMs < System.currentTimeMillis()) {
             // if there has failed clustering, then we will use the failed clustering instant-time to trigger next clustering action which will rollback and clustering.
             LOG.info("Found failed clustering instant at : " + inflightClusteringInstant + "; Will rollback the failed clustering and re-trigger again.");

File: hudi-flink/src/main/java/org/apache/hudi/sink/bulk/BulkInsertWriteFunction.java
Patch:
@@ -111,6 +111,8 @@ public BulkInsertWriteFunction(Configuration config, RowType rowType) {
 
   @Override
   public void open(Configuration parameters) throws IOException {
+    // always use the user classloader
+    Thread.currentThread().setContextClassLoader(getRuntimeContext().getUserCodeClassLoader());
     this.taskID = getRuntimeContext().getIndexOfThisSubtask();
     this.metaClient = StreamerUtil.createMetaClient(this.config);
     this.writeClient = StreamerUtil.createWriteClient(this.config, getRuntimeContext());

File: hudi-flink/src/main/java/org/apache/hudi/sink/common/AbstractStreamWriteFunction.java
Patch:
@@ -125,6 +125,8 @@ public AbstractStreamWriteFunction(Configuration config) {
 
   @Override
   public void initializeState(FunctionInitializationContext context) throws Exception {
+    // always use the user classloader
+    Thread.currentThread().setContextClassLoader(getRuntimeContext().getUserCodeClassLoader());
     this.taskID = getRuntimeContext().getIndexOfThisSubtask();
     this.metaClient = StreamerUtil.createMetaClient(this.config);
     this.writeClient = StreamerUtil.createWriteClient(this.config, getRuntimeContext());

File: hudi-flink/src/main/java/org/apache/hudi/sink/compact/CompactFunction.java
Patch:
@@ -75,6 +75,8 @@ public CompactFunction(Configuration conf) {
 
   @Override
   public void open(Configuration parameters) throws Exception {
+    // always use the user classloader
+    Thread.currentThread().setContextClassLoader(getRuntimeContext().getUserCodeClassLoader());
     this.taskID = getRuntimeContext().getIndexOfThisSubtask();
     this.writeClient = StreamerUtil.createWriteClient(conf, getRuntimeContext());
     if (this.asyncCompaction) {

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java
Patch:
@@ -156,7 +156,7 @@ public String overwriteHoodieProperties(
     newProps.load(new FileInputStream(new File(overwriteFilePath)));
     Map<String, String> oldProps = client.getTableConfig().propsMap();
     Path metaPathDir = new Path(client.getBasePath(), METAFOLDER_NAME);
-    HoodieTableConfig.createHoodieProperties(client.getFs(), metaPathDir, newProps);
+    HoodieTableConfig.create(client.getFs(), metaPathDir, newProps);
 
     TreeSet<String> allPropKeys = new TreeSet<>();
     allPropKeys.addAll(newProps.keySet().stream().map(Object::toString).collect(Collectors.toSet()));

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -377,7 +377,7 @@ public static HoodieTableMetaClient initTableAndGetMetaClient(Configuration hado
     }
 
     initializeBootstrapDirsIfNotExists(hadoopConf, basePath, fs);
-    HoodieTableConfig.createHoodieProperties(fs, metaPathDir, props);
+    HoodieTableConfig.create(fs, metaPathDir, props);
     // We should not use fs.getConf as this might be different from the original configuration
     // used to create the fs in unit tests
     HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(basePath).build();

File: hudi-common/src/test/java/org/apache/hudi/common/bootstrap/TestBootstrapIndex.java
Patch:
@@ -94,7 +94,7 @@ public void testNoOpBootstrapIndex() throws IOException {
     props.put(HoodieTableConfig.BOOTSTRAP_INDEX_ENABLE.key(), "false");
     Properties properties = new Properties();
     properties.putAll(props);
-    HoodieTableConfig.createHoodieProperties(metaClient.getFs(), new Path(metaClient.getMetaPath()), properties);
+    HoodieTableConfig.create(metaClient.getFs(), new Path(metaClient.getMetaPath()), properties);
 
     metaClient = HoodieTableMetaClient.builder().setConf(metaClient.getHadoopConf()).setBasePath(basePath).build();
     BootstrapIndex bootstrapIndex = BootstrapIndex.getBootstrapIndex(metaClient);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java
Patch:
@@ -346,7 +346,8 @@ private void writeToFile(Schema wrapperSchema, List<IndexedRecord> records) thro
     if (records.size() > 0) {
       Map<HeaderMetadataType, String> header = new HashMap<>();
       header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, wrapperSchema.toString());
-      HoodieAvroDataBlock block = new HoodieAvroDataBlock(records, header);
+      final String keyField = table.getMetaClient().getTableConfig().getRecordKeyFieldProp();
+      HoodieAvroDataBlock block = new HoodieAvroDataBlock(records, header, keyField);
       writer.appendBlock(block);
       records.clear();
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java
Patch:
@@ -180,6 +180,7 @@ public List<WriteStatus> compact(HoodieCompactionHandler compactionHandler,
         .withDiskMapType(config.getCommonConfig().getSpillableDiskMapType())
         .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())
         .withOperationField(config.allowOperationMetadataField())
+        .withPartition(operation.getPartitionPath())
         .build();
     if (!scanner.iterator().hasNext()) {
       scanner.close();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java
Patch:
@@ -203,6 +203,7 @@ private JavaRDD<HoodieRecord<T>> readRecordsForGroupWithLogs(JavaSparkContext js
               .withReverseReader(config.getCompactionReverseLogReadEnabled())
               .withBufferSize(config.getMaxDFSStreamBufferSize())
               .withSpillableMapBasePath(config.getSpillableMapBasePath())
+              .withPartition(clusteringOp.getPartitionPath())
               .build();
 
           Option<HoodieFileReader> baseFileReader = StringUtils.isNullOrEmpty(clusteringOp.getDataFilePath())

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/SimpleKeyGenerator.java
Patch:
@@ -47,9 +47,9 @@ public SimpleKeyGenerator(TypedProperties props) {
   SimpleKeyGenerator(TypedProperties props, String recordKeyField, String partitionPathField) {
     super(props);
     this.recordKeyFields = recordKeyField == null
-        ? Collections.emptyList()
-        : Collections.singletonList(recordKeyField);
-    this.partitionPathFields = Collections.singletonList(partitionPathField);
+        ? Collections.emptyList() : Collections.singletonList(recordKeyField);
+    this.partitionPathFields = partitionPathField == null
+        ? Collections.emptyList() : Collections.singletonList(partitionPathField);
     simpleAvroKeyGenerator = new SimpleAvroKeyGenerator(props, recordKeyField, partitionPathField);
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieMetadataBase.java
Patch:
@@ -288,7 +288,9 @@ protected HoodieWriteConfig.Builder getWriteConfigBuilder(HoodieFailedWritesClea
         .withMetadataConfig(HoodieMetadataConfig.newBuilder()
             .enable(useFileListingMetadata)
             .enableFullScan(enableFullScan)
-            .enableMetrics(enableMetrics).build())
+            .enableMetrics(enableMetrics)
+            .withPopulateMetaFields(false)
+            .build())
         .withMetricsConfig(HoodieMetricsConfig.newBuilder().on(enableMetrics)
             .withExecutorMetrics(true).build())
         .withMetricsGraphiteConfig(HoodieMetricsGraphiteConfig.newBuilder()

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.common.config.ConfigProperty;
 import org.apache.hudi.common.config.HoodieConfig;
 import org.apache.hudi.common.model.HoodieFileFormat;
+import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
@@ -364,7 +365,7 @@ public boolean populateMetaFields() {
    * @returns the record key field prop.
    */
   public String getRecordKeyFieldProp() {
-    return getString(RECORDKEY_FIELDS);
+    return getStringOrDefault(RECORDKEY_FIELDS, HoodieRecord.RECORD_KEY_METADATA_FIELD);
   }
 
   public String getKeyGeneratorClassName() {

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java
Patch:
@@ -1672,9 +1672,9 @@ private HoodieDataBlock getDataBlock(HoodieLogBlockType dataBlockType, List<Inde
                                        Map<HeaderMetadataType, String> header) {
     switch (dataBlockType) {
       case AVRO_DATA_BLOCK:
-        return new HoodieAvroDataBlock(records, header);
+        return new HoodieAvroDataBlock(records, header, HoodieRecord.RECORD_KEY_METADATA_FIELD);
       case HFILE_DATA_BLOCK:
-        return new HoodieHFileDataBlock(records, header);
+        return new HoodieHFileDataBlock(records, header, HoodieRecord.RECORD_KEY_METADATA_FIELD);
       default:
         throw new RuntimeException("Unknown data block type " + dataBlockType);
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java
Patch:
@@ -179,6 +179,7 @@ public List<WriteStatus> compact(HoodieCompactionHandler compactionHandler,
         .withSpillableMapBasePath(config.getSpillableMapBasePath())
         .withDiskMapType(config.getCommonConfig().getSpillableDiskMapType())
         .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())
+        .withOperationField(config.allowOperationMetadataField())
         .build();
     if (!scanner.iterator().hasNext()) {
       scanner.close();

File: hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.common.util.ClosableIterator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.configuration.FlinkOptions;
+import org.apache.hudi.configuration.OptionsResolver;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.keygen.KeyGenUtils;
 import org.apache.hudi.table.format.FilePathUtils;
@@ -179,8 +180,7 @@ public void open(MergeOnReadInputSplit split) throws IOException {
       }
     } else if (!split.getBasePath().isPresent()) {
       // log files only
-      if (conf.getBoolean(FlinkOptions.READ_AS_STREAMING)
-          && conf.getBoolean(FlinkOptions.CHANGELOG_ENABLED)) {
+      if (OptionsResolver.emitChangelog(conf)) {
         this.iterator = new LogFileOnlyIterator(getUnMergedLogFileIterator(split));
       } else {
         this.iterator = new LogFileOnlyIterator(getLogFileIterator(split));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -2179,7 +2179,7 @@ protected void setDefaults() {
       writeConfig.setDefaultOnCondition(!isPayloadConfigSet,
           HoodiePayloadConfig.newBuilder().fromProperties(writeConfig.getProps()).build());
       writeConfig.setDefaultOnCondition(!isMetadataConfigSet,
-          HoodieMetadataConfig.newBuilder().fromProperties(writeConfig.getProps()).build());
+          HoodieMetadataConfig.newBuilder().withEngineType(engineType).fromProperties(writeConfig.getProps()).build());
       writeConfig.setDefaultOnCondition(!isLockConfigSet,
           HoodieLockConfig.newBuilder().fromProperties(writeConfig.getProps()).build());
       writeConfig.setDefaultOnCondition(!isPreCommitValidationConfigSet,

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.hudi.client.HoodieJavaWriteClient;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.bloom.BloomFilter;
-import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.engine.EngineType;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieKey;
@@ -115,8 +114,7 @@ private HoodieWriteConfig.Builder makeHoodieClientConfigBuilder() {
     return HoodieWriteConfig.newBuilder()
         .withEngineType(EngineType.JAVA)
         .withPath(basePath)
-        .withSchema(SCHEMA.toString())
-        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build());
+        .withSchema(SCHEMA.toString());
   }
 
   @Test

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRollbacksCommand.java
Patch:
@@ -90,6 +90,7 @@ public void init() throws Exception {
         .withBaseFilesInPartitions(partitionAndFileId);
     // generate two rollback
     HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(tablePath)
+        .withRollbackUsingMarkers(false)
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();
 
     try (AbstractHoodieWriteClient client = new SparkRDDWriteClient(context(), config)) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -120,9 +120,9 @@ public class HoodieWriteConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> ROLLBACK_USING_MARKERS_ENABLE = ConfigProperty
       .key("hoodie.rollback.using.markers")
-      .defaultValue("false")
+      .defaultValue("true")
       .withDocumentation("Enables a more efficient mechanism for rollbacks based on the marker files generated "
-          + "during the writes. Turned off by default.");
+          + "during the writes. Turned on by default.");
 
   public static final ConfigProperty<String> TIMELINE_LAYOUT_VERSION_NUM = ConfigProperty
       .key("hoodie.timeline.layout.version")

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -45,6 +45,7 @@
 import org.apache.hudi.common.table.log.HoodieLogFormat;
 import org.apache.hudi.common.table.log.block.HoodieDeleteBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock.HeaderMetadataType;
+import org.apache.hudi.common.table.marker.MarkerType;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
@@ -180,6 +181,8 @@ private HoodieWriteConfig createMetadataWriteConfig(HoodieWriteConfig writeConfi
         .withAutoCommit(true)
         .withAvroSchemaValidate(true)
         .withEmbeddedTimelineServerEnabled(false)
+        .withMarkersType(MarkerType.DIRECT.name())
+        .withRollbackUsingMarkers(false)
         .withPath(HoodieTableMetadata.getMetadataTableBasePath(writeConfig.getBasePath()))
         .withSchema(HoodieMetadataRecord.getClassSchema().toString())
         .forTable(tableName)

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestClientRollback.java
Patch:
@@ -201,6 +201,7 @@ public void testRollbackCommit() throws Exception {
         .withBaseFilesInPartitions(partitionAndFileId3);
 
     HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)
+        .withRollbackUsingMarkers(false)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder()
             .withFailedWritesCleaningPolicy(HoodieFailedWritesCleaningPolicy.LAZY).build())
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();
@@ -308,6 +309,7 @@ public void testAutoRollbackInflightCommit() throws Exception {
 
     // Set Failed Writes rollback to EAGER
     config = HoodieWriteConfig.newBuilder().withPath(basePath)
+        .withRollbackUsingMarkers(false)
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();
     final String commitTime5 = "20160506030631";
     try (SparkRDDWriteClient client = getHoodieWriteClient(config)) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/rollback/TestMergeOnReadRollbackActionExecutor.java
Patch:
@@ -162,8 +162,8 @@ public void testFailForCompletedInstants() {
   public void testRollbackWhenFirstCommitFail() throws Exception {
 
     HoodieWriteConfig config = HoodieWriteConfig.newBuilder()
+        .withRollbackUsingMarkers(false)
         .withPath(basePath).withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true).build()).build();
-
     try (SparkRDDWriteClient client = getHoodieWriteClient(config)) {
       client.startCommitWithTime("001");
       client.insert(jsc.emptyRDD(), "001");

File: hudi-flink/src/test/java/org/apache/hudi/utils/TestStreamerUtil.java
Patch:
@@ -86,6 +86,8 @@ void testMedianInstantTime() {
     assertThrows(IllegalArgumentException.class,
         () -> StreamerUtil.medianInstantTime(lower, higher),
         "The first argument should have newer instant time");
+    // test very near instant time
+    assertFalse(StreamerUtil.medianInstantTime("20211116115634", "20211116115633").isPresent());
   }
 
   @Test

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackStrategy.java
Patch:
@@ -60,7 +60,7 @@ public List<HoodieRollbackRequest> getRollbackRequests(HoodieInstant instantToRo
       List<ListingBasedRollbackRequest> rollbackRequests = null;
       if (table.getMetaClient().getTableType() == HoodieTableType.COPY_ON_WRITE) {
         rollbackRequests = RollbackUtils.generateRollbackRequestsByListingCOW(context,
-            table.getMetaClient().getBasePath(), config);
+            table.getMetaClient().getBasePath());
       } else {
         rollbackRequests = RollbackUtils
             .generateRollbackRequestsUsingFileListingMOR(instantToRollback, table, context);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/ZeroToOneUpgradeHandler.java
Patch:
@@ -102,7 +102,7 @@ protected void recreateMarkers(final String commitInstantTime,
         // generate rollback stats
         List<ListingBasedRollbackRequest> rollbackRequests;
         if (table.getMetaClient().getTableType() == HoodieTableType.COPY_ON_WRITE) {
-          rollbackRequests = RollbackUtils.generateRollbackRequestsByListingCOW(context, table.getMetaClient().getBasePath(), table.getConfig());
+          rollbackRequests = RollbackUtils.generateRollbackRequestsByListingCOW(context, table.getMetaClient().getBasePath());
         } else {
           rollbackRequests = RollbackUtils.generateRollbackRequestsUsingFileListingMOR(commitInstantOpt.get(), table, context);
         }

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -184,8 +184,9 @@ private List<Pair<String, Option<HoodieRecord<HoodieMetadataPayload>>>> readFrom
     HoodieRecord<HoodieMetadataPayload> hoodieRecord = null;
     // Retrieve record from base file
     if (baseFileReader != null) {
-      HoodieTimer readTimer = new HoodieTimer().startTimer();
+      HoodieTimer readTimer = new HoodieTimer();
       for (String key : keys) {
+        readTimer.startTimer();
         Option<GenericRecord> baseRecord = baseFileReader.getRecordByKey(key);
         if (baseRecord.isPresent()) {
           hoodieRecord = metadataTableConfig.populateMetaFields()

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/IntervalTreeBasedGlobalIndexFileFilter.java
Patch:
@@ -46,8 +46,8 @@ class IntervalTreeBasedGlobalIndexFileFilter implements IndexFileFilter {
   IntervalTreeBasedGlobalIndexFileFilter(final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo) {
     List<BloomIndexFileInfo> allIndexFiles = new ArrayList<>();
 
-    partitionToFileIndexInfo.forEach((parition, bloomIndexFileInfoList) -> bloomIndexFileInfoList.forEach(file -> {
-      fileIdToPartitionPathMap.put(file.getFileId(), parition);
+    partitionToFileIndexInfo.forEach((partition, bloomIndexFileInfoList) -> bloomIndexFileInfoList.forEach(file -> {
+      fileIdToPartitionPathMap.put(file.getFileId(), partition);
       allIndexFiles.add(file);
     }));
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java
Patch:
@@ -564,7 +564,7 @@ private static Type convertToParquetType(
         int scale = ((DecimalType) type).getScale();
         int numBytes = computeMinBytesForDecimalPrecision(precision);
         return Types.primitive(
-            PrimitiveType.PrimitiveTypeName.BINARY, repetition)
+            PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY, repetition)
             .precision(precision)
             .scale(scale)
             .length(numBytes)

File: hudi-flink/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.flink.formats.parquet.vector.reader.BytesColumnReader;
 import org.apache.flink.formats.parquet.vector.reader.ColumnReader;
 import org.apache.flink.formats.parquet.vector.reader.DoubleColumnReader;
-import org.apache.flink.formats.parquet.vector.reader.FixedLenBytesColumnReader;
 import org.apache.flink.formats.parquet.vector.reader.FloatColumnReader;
 import org.apache.flink.formats.parquet.vector.reader.IntColumnReader;
 import org.apache.flink.formats.parquet.vector.reader.LongColumnReader;
@@ -366,7 +365,7 @@ public static WritableColumnVector createWritableColumnVector(
             "TIME_MICROS original type is not ");
         return new HeapTimestampVector(batchSize);
       case DECIMAL:
-        checkArgument(typeName == PrimitiveType.PrimitiveTypeName.BINARY
+        checkArgument(typeName == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY
                 && primitiveType.getOriginalType() == OriginalType.DECIMAL,
             "Unexpected type: %s", typeName);
         return new HeapBytesVector(batchSize);

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -111,7 +111,7 @@ public final class HoodieMetadataConfig extends HoodieConfig {
 
   public static final ConfigProperty<Integer> FILE_LISTING_PARALLELISM_VALUE = ConfigProperty
       .key("hoodie.file.listing.parallelism")
-      .defaultValue(1500)
+      .defaultValue(200)
       .sinceVersion("0.7.0")
       .withDocumentation("Parallelism to use, when listing the table on lake storage.");
 

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java
Patch:
@@ -57,6 +57,7 @@
 import java.math.BigDecimal;
 import java.nio.ByteBuffer;
 import java.nio.charset.StandardCharsets;
+import java.sql.Timestamp;
 import java.time.LocalDate;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -539,6 +540,8 @@ public static Object convertValueForSpecificDataTypes(Schema fieldSchema, Object
   private static Object convertValueForAvroLogicalTypes(Schema fieldSchema, Object fieldValue) {
     if (fieldSchema.getLogicalType() == LogicalTypes.date()) {
       return LocalDate.ofEpochDay(Long.parseLong(fieldValue.toString()));
+    } else if (fieldSchema.getLogicalType() == LogicalTypes.timestampMicros()) {
+      return new Timestamp(Long.parseLong(fieldValue.toString()) / 1000);
     } else if (fieldSchema.getLogicalType() instanceof LogicalTypes.Decimal) {
       Decimal dc = (Decimal) fieldSchema.getLogicalType();
       DecimalConversion decimalConversion = new DecimalConversion();

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.avro.specific.SpecificRecordBase;
 import org.apache.hudi.client.HoodieFlinkWriteClient;
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.data.HoodieData;
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.metrics.Registry;
 import org.apache.hudi.common.model.FileSlice;
@@ -91,8 +92,9 @@ protected <T extends SpecificRecordBase> void initialize(HoodieEngineContext eng
   }
 
   @Override
-  protected void commit(List<HoodieRecord> records, String partitionName, String instantTime, boolean canTriggerTableService) {
+  protected void commit(HoodieData<HoodieRecord> hoodieDataRecords, String partitionName, String instantTime, boolean canTriggerTableService) {
     ValidationUtils.checkState(enabled, "Metadata table cannot be committed to as it is not enabled");
+    List<HoodieRecord> records = (List<HoodieRecord>) hoodieDataRecords.get();
     List<HoodieRecord> recordList = prepRecords(records, partitionName, 1);
 
     try (HoodieFlinkWriteClient writeClient = new HoodieFlinkWriteClient(engineContext, metadataWriteConfig)) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java
Patch:
@@ -750,7 +750,7 @@ public void testReattemptOfFailedClusteringCommit() throws Exception {
 
     // setup clustering config.
     HoodieClusteringConfig clusteringConfig = HoodieClusteringConfig.newBuilder().withClusteringMaxNumGroups(10)
-        .withClusteringSortColumns("_row_key")
+        .withClusteringSortColumns("_row_key").withInlineClustering(true)
         .withClusteringTargetPartitions(0).withInlineClusteringNumCommits(1).build();
 
     HoodieWriteConfig newWriteConfig = getConfigBuilder(TRIP_EXAMPLE_SCHEMA, HoodieIndex.IndexType.BLOOM, HoodieFailedWritesCleaningPolicy.EAGER)

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableClustering.java
Patch:
@@ -93,6 +93,7 @@ void testClustering(boolean doUpdates, boolean populateMetaFields, boolean prese
         .withClusteringConfig(HoodieClusteringConfig.newBuilder()
             .withClusteringMaxNumGroups(10)
             .withClusteringTargetPartitions(0)
+            .withInlineClustering(true)
             .withInlineClusteringNumCommits(1)
             .withPreserveHoodieCommitMetadata(preserveCommitMetadata).build())
         .withRollbackUsingMarkers(false);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataWriter.java
Patch:
@@ -34,8 +34,10 @@ public interface HoodieTableMetadataWriter extends Serializable, AutoCloseable {
    * Update the metadata table due to a COMMIT operation.
    * @param commitMetadata commit metadata of the operation of interest.
    * @param instantTime instant time of the commit.
+   * @param isTableServiceAction true if caller is a table service. false otherwise. Only regular write operations can trigger metadata table services and this argument
+   *                       will assist in this.
    */
-  void update(HoodieCommitMetadata commitMetadata, String instantTime);
+  void update(HoodieCommitMetadata commitMetadata, String instantTime, boolean isTableServiceAction);
 
   /**
    * Update the metadata table due to a CLEAN operation.

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/BaseActionExecutor.java
Patch:
@@ -56,8 +56,8 @@ public BaseActionExecutor(HoodieEngineContext context, HoodieWriteConfig config,
    * Writes commits metadata to table metadata.
    * @param metadata commit metadata of interest.
    */
-  protected final void writeTableMetadata(HoodieCommitMetadata metadata) {
-    table.getMetadataWriter().ifPresent(w -> w.update(metadata, instantTime));
+  protected final void writeTableMetadata(HoodieCommitMetadata metadata, String actionType) {
+    table.getMetadataWriter().ifPresent(w -> w.update(metadata, instantTime, table.isTableServiceAction(actionType)));
   }
 
   /**

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -258,10 +258,10 @@ protected void preWrite(String instantTime, WriteOperationType writeOperationTyp
   }
 
   @Override
-  protected void preCommit(String instantTime, HoodieCommitMetadata metadata) {
+  protected void preCommit(HoodieInstant inflightInstant, HoodieCommitMetadata metadata) {
     this.metadataWriterOption.ifPresent(w -> {
       w.initTableMetadata(); // refresh the timeline
-      w.update(metadata, instantTime);
+      w.update(metadata, inflightInstant.getTimestamp(), getHoodieTable().isTableServiceAction(inflightInstant.getAction()));
     });
   }
 
@@ -406,7 +406,7 @@ private void writeTableMetadata(HoodieTable<T, List<HoodieRecord<T>>, List<Hoodi
       this.txnManager.beginTransaction(Option.of(hoodieInstant), Option.empty());
       // Do not do any conflict resolution here as we do with regular writes. We take the lock here to ensure all writes to metadata table happens within a
       // single lock (single writer). Because more than one write to metadata table will result in conflicts since all of them updates the same partition.
-      table.getMetadataWriter().ifPresent(w -> w.update(commitMetadata, hoodieInstant.getTimestamp()));
+      table.getMetadataWriter().ifPresent(w -> w.update(commitMetadata, hoodieInstant.getTimestamp(), table.isTableServiceAction(hoodieInstant.getAction())));
     } finally {
       this.txnManager.endTransaction();
     }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java
Patch:
@@ -147,7 +147,7 @@ protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMeta
       HoodieCommitMetadata metadata = CommitUtils.buildMetadata(writeStats, result.getPartitionToReplaceFileIds(),
           extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());
 
-      writeTableMetadata(metadata);
+      writeTableMetadata(metadata, actionType);
 
       activeTimeline.saveAsComplete(new HoodieInstant(true, getCommitActionType(), instantTime),
           Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java
Patch:
@@ -208,7 +208,7 @@ protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMeta
       HoodieCommitMetadata metadata = CommitUtils.buildMetadata(writeStats, result.getPartitionToReplaceFileIds(),
           extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());
 
-      writeTableMetadata(metadata);
+      writeTableMetadata(metadata, actionType);
 
       activeTimeline.saveAsComplete(new HoodieInstant(true, getCommitActionType(), instantTime),
           Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java
Patch:
@@ -248,7 +248,7 @@ protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMeta
     metadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, getSchemaToStoreInCommit());
     metadata.setOperationType(operationType);
 
-    writeTableMetadata(metadata);
+    writeTableMetadata(metadata, actionType);
 
     try {
       activeTimeline.saveAsComplete(new HoodieInstant(true, actionType, instantTime),

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java
Patch:
@@ -267,7 +267,7 @@ protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMeta
       HoodieActiveTimeline activeTimeline = table.getActiveTimeline();
       HoodieCommitMetadata metadata = CommitUtils.buildMetadata(writeStats, result.getPartitionToReplaceFileIds(),
           extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());
-      writeTableMetadata(metadata);
+      writeTableMetadata(metadata, actionType);
       activeTimeline.saveAsComplete(new HoodieInstant(true, getCommitActionType(), instantTime),
           Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));
       LOG.info("Committed " + instantTime);

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieHFileRealtimeInputFormat.java
Patch:
@@ -91,14 +91,14 @@ public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSpli
           // For e:g _hoodie_record_key would be missing and merge step would throw exceptions.
           // TO fix this, hoodie columns are appended late at the time record-reader gets built instead of construction
           // time.
-          HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);
           HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf, Option.empty());
 
           this.conf = jobConf;
           this.conf.set(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP, "true");
         }
       }
     }
+    HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);
 
     LOG.info("Creating record reader with readCols :" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)
         + ", Ids :" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java
Patch:
@@ -98,7 +98,6 @@ void addProjectionToJobConf(final RealtimeSplit realtimeSplit, final JobConf job
           // For e:g _hoodie_record_key would be missing and merge step would throw exceptions.
           // TO fix this, hoodie columns are appended late at the time record-reader gets built instead of construction
           // time.
-          HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);
           if (!realtimeSplit.getDeltaLogPaths().isEmpty()) {
             HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf, realtimeSplit.getHoodieVirtualKeyInfo());
           }
@@ -107,6 +106,7 @@ void addProjectionToJobConf(final RealtimeSplit realtimeSplit, final JobConf job
         }
       }
     }
+    HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.avro.model.HoodieClusteringPlan;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
@@ -217,7 +218,7 @@ public void testPartitionWeight() throws Exception {
     final String testPartitionPath = "2016/09/26";
     int totalInsertNum = 2000;
 
-    HoodieWriteConfig config = makeHoodieClientConfigBuilder()
+    HoodieWriteConfig config = makeHoodieClientConfigBuilder().withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build())
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(0)
             .insertSplitSize(totalInsertNum / 2).autoTuneInsertSplits(false).build()).build();
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/Base64CodecUtil.java
Patch:
@@ -26,11 +26,11 @@ public final class Base64CodecUtil {
   /**
    * Decodes data from the input string into using the encoding scheme.
    *
-   * @param serString
+   * @param encodedString - Base64 encoded string to decode
    * @return A newly-allocated byte array containing the decoded bytes.
    */
-  public static byte[] decode(String serString) {
-    return Base64.getDecoder().decode(serString.getBytes(StandardCharsets.UTF_8));
+  public static byte[] decode(String encodedString) {
+    return Base64.getDecoder().decode(encodedString.getBytes(StandardCharsets.UTF_8));
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/CopyOnWriteRestoreActionExecutor.java
Patch:
@@ -58,6 +58,7 @@ protected HoodieRollbackMetadata rollbackInstant(HoodieInstant instantToRollback
         instantToRollback,
         true,
         true,
+        false,
         false);
     return rollbackActionExecutor.execute();
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/MergeOnReadRestoreActionExecutor.java
Patch:
@@ -62,6 +62,7 @@ protected HoodieRollbackMetadata rollbackInstant(HoodieInstant instantToRollback
         instantToRollback,
         true,
         true,
+        false,
         false);
 
     // TODO : Get file status and create a rollback stat and file

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java
Patch:
@@ -424,7 +424,7 @@ protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<W
         this.txnManager.beginTransaction();
         try {
           // Ensure no inflight commits by setting EAGER policy and explicitly cleaning all failed commits
-          this.rollbackFailedWrites(getInstantsToRollback(metaClient, HoodieFailedWritesCleaningPolicy.EAGER));
+          this.rollbackFailedWrites(getInstantsToRollback(metaClient, HoodieFailedWritesCleaningPolicy.EAGER, Option.of(instantTime)), true);
           new UpgradeDowngrade(
               metaClient, config, context, SparkUpgradeDowngradeHelper.getInstance())
               .run(HoodieTableVersion.current(), instantTime);
@@ -434,6 +434,7 @@ protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<W
       } else {
         upgradeDowngrade.run(HoodieTableVersion.current(), instantTime);
       }
+      metaClient.reloadActiveTimeline();
     }
     metaClient.validateTableProperties(config.getProps(), operationType);
     return getTableAndInitCtx(metaClient, operationType, instantTime);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java
Patch:
@@ -159,8 +159,9 @@ public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context,
   public HoodieRollbackMetadata rollback(HoodieEngineContext context,
                                          String rollbackInstantTime,
                                          HoodieInstant commitInstant,
-                                         boolean deleteInstants) {
-    return new MergeOnReadRollbackActionExecutor(context, config, this, rollbackInstantTime, commitInstant, deleteInstants).execute();
+                                         boolean deleteInstants,
+                                         boolean skipLocking) {
+    return new MergeOnReadRollbackActionExecutor(context, config, this, rollbackInstantTime, commitInstant, deleteInstants, skipLocking).execute();
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java
Patch:
@@ -1308,7 +1308,7 @@ public void testCleanMarkerDataFilesOnRollback() throws Exception {
     metaClient.reloadActiveTimeline();
     HoodieInstant rollbackInstant = new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, "000");
     table.scheduleRollback(context, "001", rollbackInstant, false);
-    table.rollback(context, "001", rollbackInstant, true);
+    table.rollback(context, "001", rollbackInstant, true, false);
     final int numTempFilesAfter = testTable.listAllFilesInTempFolder().length;
     assertEquals(0, numTempFilesAfter, "All temp files are deleted.");
   }

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java
Patch:
@@ -45,7 +45,7 @@ public interface HoodieRecordPayload<T extends HoodieRecordPayload> extends Seri
   T preCombine(T oldValue);
 
   /**
-   * When more than one HoodieRecord have the same HoodieKey, this function combines them before attempting to insert/upsert by taking in a property map.
+   * When more than one HoodieRecord have the same HoodieKey in the incoming batch, this function combines them before attempting to insert/upsert by taking in a property map.
    * Implementation can leverage the property to decide their business logic to do preCombine.
    *
    * @param oldValue   instance of the old {@link HoodieRecordPayload} to be combined with.

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestHoodieTableMetaClient.java
Patch:
@@ -59,10 +59,10 @@ public void checkMetadata() {
   @Test
   public void checkSerDe() {
     // check if this object is serialized and de-serialized, we are able to read from the file system
-    HoodieTableMetaClient deseralizedMetaClient =
+    HoodieTableMetaClient deserializedMetaClient =
         HoodieTestUtils.serializeDeserialize(metaClient, HoodieTableMetaClient.class);
-    assertNotNull(deseralizedMetaClient);
-    HoodieActiveTimeline commitTimeline = deseralizedMetaClient.getActiveTimeline();
+    assertNotNull(deserializedMetaClient);
+    HoodieActiveTimeline commitTimeline = deserializedMetaClient.getActiveTimeline();
     HoodieInstant instant = new HoodieInstant(true, HoodieTimeline.COMMIT_ACTION, "1");
     commitTimeline.createNewInstant(instant);
     commitTimeline.saveAsComplete(instant, Option.of("test-detail".getBytes()));

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java
Patch:
@@ -112,7 +112,7 @@ public static HoodieTableMetaClient init(Configuration hadoopConf, String basePa
   }
 
   public static <T extends Serializable> T serializeDeserialize(T object, Class<T> clazz) {
-    // Using Kyro as the default serializer in Spark Jobs
+    // Using Kryo as the default serializer in Spark Jobs
     Kryo kryo = new Kryo();
     kryo.register(HoodieTableMetaClient.class, new JavaSerializer());
 
@@ -122,9 +122,9 @@ public static <T extends Serializable> T serializeDeserialize(T object, Class<T>
     output.close();
 
     Input input = new Input(new ByteArrayInputStream(baos.toByteArray()));
-    T deseralizedObject = kryo.readObject(input, clazz);
+    T deserializedObject = kryo.readObject(input, clazz);
     input.close();
-    return deseralizedObject;
+    return deserializedObject;
   }
 
   public static List<HoodieWriteStat> generateFakeHoodieWriteStat(int limit) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/providers/SparkProvider.java
Patch:
@@ -39,6 +39,8 @@ default SparkConf conf(Map<String, String> overwritingConfigs) {
     SparkConf sparkConf = new SparkConf();
     sparkConf.set("spark.app.name", getClass().getName());
     sparkConf.set("spark.master", "local[*]");
+    sparkConf.set("spark.default.parallelism", "4");
+    sparkConf.set("spark.sql.shuffle.partitions", "4");
     sparkConf.set("spark.driver.maxResultSize", "2g");
     sparkConf.set("spark.hadoop.mapred.output.compress", "true");
     sparkConf.set("spark.hadoop.mapred.output.compression.codec", "true");
@@ -52,4 +54,4 @@ default SparkConf conf(Map<String, String> overwritingConfigs) {
   default SparkConf conf() {
     return conf(Collections.emptyMap());
   }
-}
\ No newline at end of file
+}

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java
Patch:
@@ -160,6 +160,7 @@ public HoodieTestDataGenerator(String[] partitionPaths, Map<Integer, KeyPartitio
     this.existingKeysBySchema = new HashMap<>();
     existingKeysBySchema.put(TRIP_EXAMPLE_SCHEMA, keyPartitionMap);
     numKeysBySchema = new HashMap<>();
+    numKeysBySchema.put(TRIP_EXAMPLE_SCHEMA, keyPartitionMap.size());
   }
 
   /**
@@ -844,8 +845,8 @@ public int getNumExistingKeys(String schemaStr) {
 
   public static class KeyPartition implements Serializable {
 
-    HoodieKey key;
-    String partitionPath;
+    public HoodieKey key;
+    public String partitionPath;
   }
 
   public void close() {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/BaseActionExecutor.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieRecordPayload;
+import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
@@ -72,7 +73,7 @@ protected final void writeTableMetadata(HoodieCleanMetadata metadata) {
    * @param metadata rollback metadata of interest.
    */
   protected final void writeTableMetadata(HoodieRollbackMetadata metadata) {
-    table.getMetadataWriter().ifPresent(w -> w.update(metadata, instantTime));
+    table.getMetadataWriter(Option.of(metadata)).ifPresent(w -> w.update(metadata, instantTime));
   }
 
   /**

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHBaseIndex.java
Patch:
@@ -337,7 +337,7 @@ public void testTagLocationAndPartitionPathUpdateWithExplicitRollback() throws E
   public void testSimpleTagLocationAndUpdateWithRollback() throws Exception {
     // Load to memory
     HoodieWriteConfig config = getConfigBuilder(100, false, false)
-        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build()).build();
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true).build()).build();
     SparkHoodieHBaseIndex index = new SparkHoodieHBaseIndex(config);
     SparkRDDWriteClient writeClient = getHoodieWriteClient(config);
 
@@ -425,7 +425,7 @@ public void testSimpleTagLocationWithInvalidCommit() throws Exception {
   public void testEnsureTagLocationUsesCommitTimeline() throws Exception {
     // Load to memory
     HoodieWriteConfig config = getConfigBuilder(100, false, false)
-        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build()).build();
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true).build()).build();
     SparkHoodieHBaseIndex index = new SparkHoodieHBaseIndex(config);
     SparkRDDWriteClient writeClient = getHoodieWriteClient(config);
 

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -71,7 +71,7 @@ public final class HoodieMetadataConfig extends HoodieConfig {
   // Maximum delta commits before compaction occurs
   public static final ConfigProperty<Integer> COMPACT_NUM_DELTA_COMMITS = ConfigProperty
       .key(METADATA_PREFIX + ".compact.max.delta.commits")
-      .defaultValue(24)
+      .defaultValue(10)
       .sinceVersion("0.7.0")
       .withDocumentation("Controls how often the metadata table is compacted.");
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieMultiTableDeltaStreamer.java
Patch:
@@ -43,7 +43,7 @@
 import static org.junit.jupiter.api.Assertions.assertThrows;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
-public class TestHoodieMultiTableDeltaStreamer extends TestHoodieDeltaStreamerBase {
+public class TestHoodieMultiTableDeltaStreamer extends HoodieDeltaStreamerTestBase {
 
   private static volatile Logger log = LogManager.getLogger(TestHoodieMultiTableDeltaStreamer.class);
 
@@ -237,7 +237,7 @@ public void testTableLevelProperties() throws IOException {
 
   private String populateCommonPropsAndWriteToFile() throws IOException {
     TypedProperties commonProps = new TypedProperties();
-    populateCommonProps(commonProps);
+    populateCommonProps(commonProps, dfsBasePath);
     UtilitiesTestBase.Helpers.savePropsToDFS(commonProps, dfs, dfsBasePath + "/" + PROPS_FILENAME_TEST_PARQUET);
     return PROPS_FILENAME_TEST_PARQUET;
   }

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/client/transaction/TestSimpleConcurrentFileWritesConflictResolutionStrategy.java
Patch:
@@ -314,7 +314,7 @@ private void createCommit(String instantTime) throws Exception {
     commitMetadata.addWriteStat(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, writeStat);
     commitMetadata.setOperationType(WriteOperationType.INSERT);
     HoodieTestTable.of(metaClient)
-        .addCommit(instantTime, commitMetadata)
+        .addCommit(instantTime, Option.of(commitMetadata))
         .withBaseFilesInPartition(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, fileId1, fileId2);
   }
 
@@ -362,7 +362,7 @@ private void createCompaction(String instantTime) throws Exception {
     writeStat.setFileId("file-1");
     commitMetadata.addWriteStat(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, writeStat);
     HoodieTestTable.of(metaClient)
-        .addCommit(instantTime, commitMetadata)
+        .addCommit(instantTime, Option.of(commitMetadata))
         .withBaseFilesInPartition(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, fileId1, fileId2);
   }
 

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/utils/TestMetadataConversionUtils.java
Patch:
@@ -176,7 +176,7 @@ private void createCompactionMetadata(String instantTime) throws Exception {
     commitMetadata.setOperationType(WriteOperationType.COMPACT);
     commitMetadata.setCompacted(true);
     HoodieTestTable.of(metaClient)
-        .addCommit(instantTime, commitMetadata)
+        .addCommit(instantTime, Option.of(commitMetadata))
         .withBaseFilesInPartition(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, fileId1, fileId2);
   }
 
@@ -206,7 +206,7 @@ private void createCommitMetadata(String instantTime) throws Exception {
     commitMetadata.addMetadata("test", "test");
     commitMetadata.setOperationType(WriteOperationType.INSERT);
     HoodieTestTable.of(metaClient)
-        .addCommit(instantTime, commitMetadata)
+        .addCommit(instantTime, Option.of(commitMetadata))
         .withBaseFilesInPartition(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, fileId1, fileId2);
   }
 

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/kafka/KafkaControlAgent.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.connect.kafka;
 
-import org.apache.hudi.connect.transaction.ControlEvent;
+import org.apache.hudi.connect.ControlMessage;
 import org.apache.hudi.connect.transaction.TransactionCoordinator;
 import org.apache.hudi.connect.transaction.TransactionParticipant;
 
@@ -37,5 +37,5 @@ public interface KafkaControlAgent {
 
   void deregisterTransactionCoordinator(TransactionCoordinator coordinator);
 
-  void publishMessage(ControlEvent message);
+  void publishMessage(ControlMessage message);
 }

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/transaction/TransactionCoordinator.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hudi.connect.transaction;
 
+import org.apache.hudi.connect.ControlMessage;
+
 import org.apache.kafka.common.TopicPartition;
 
 /**
@@ -36,5 +38,5 @@ public interface TransactionCoordinator {
   TopicPartition getPartition();
 
   /* Called when a control event is received from the Kafka control topic */
-  void processControlEvent(ControlEvent message);
+  void processControlEvent(ControlMessage message);
 }

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/AbstractConnectWriter.java
Patch:
@@ -91,11 +91,11 @@ public void writeRecord(SinkRecord record) throws IOException {
   }
 
   @Override
-  public List<WriteStatus> close() throws IOException {
+  public List<WriteStatus> close() {
     return flushRecords();
   }
 
   protected abstract void writeHudiRecord(HoodieRecord<?> record);
 
-  protected abstract List<WriteStatus> flushRecords() throws IOException;
+  protected abstract List<WriteStatus> flushRecords();
 }

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/BufferedConnectWriter.java
Patch:
@@ -92,7 +92,7 @@ public void writeHudiRecord(HoodieRecord<?> record) {
   }
 
   @Override
-  public List<WriteStatus> flushRecords() throws IOException {
+  public List<WriteStatus> flushRecords() {
     try {
       LOG.info("Number of entries in MemoryBasedMap => "
           + bufferedRecords.getInMemoryMapNumEntries()
@@ -122,7 +122,7 @@ public List<WriteStatus> flushRecords() throws IOException {
       LOG.info("Flushed hudi records and got writeStatuses: " + writeStatuses);
       return writeStatuses;
     } catch (Exception e) {
-      throw new IOException("Write records failed", e);
+      throw new HoodieIOException("Write records failed", new IOException(e));
     }
   }
 }

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/ConnectWriter.java
Patch:
@@ -27,5 +27,5 @@ public interface ConnectWriter<T> {
 
   void writeRecord(SinkRecord record) throws IOException;
 
-  List<T> close() throws IOException;
+  List<T> close();
 }

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/KafkaConnectConfigs.java
Patch:
@@ -39,6 +39,7 @@
     description = "Configurations for Kafka Connect Sink Connector for Hudi.")
 public class KafkaConnectConfigs extends HoodieConfig {
 
+  public static final int CURRENT_PROTOCOL_VERSION = 0;
   public static final String KAFKA_VALUE_CONVERTER = "value.converter";
 
   public static final ConfigProperty<String> KAFKA_BOOTSTRAP_SERVERS = ConfigProperty

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java
Patch:
@@ -211,7 +211,7 @@ private HoodieCleanMetadata runClean(HoodieTable<T, I, K, O> table, HoodieInstan
 
   /**
    * Update metadata table if available. Any update to metadata table happens within data table lock.
-   * @param cleanMetadata intance of {@link HoodieCleanMetadata} to be applied to metadata.
+   * @param cleanMetadata instance of {@link HoodieCleanMetadata} to be applied to metadata.
    */
   private void writeMetadata(HoodieCleanMetadata cleanMetadata) {
     try {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/BaseRestoreActionExecutor.java
Patch:
@@ -105,7 +105,7 @@ private HoodieRestoreMetadata finishRestore(Map<String, List<HoodieRollbackMetad
 
   /**
    * Update metadata table if available. Any update to metadata table happens within data table lock.
-   * @param restoreMetadata intance of {@link HoodieRestoreMetadata} to be applied to metadata.
+   * @param restoreMetadata instance of {@link HoodieRestoreMetadata} to be applied to metadata.
    */
   private void writeToMetadata(HoodieRestoreMetadata restoreMetadata) {
     try {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java
Patch:
@@ -262,7 +262,7 @@ protected void finishRollback(HoodieInstant inflightInstant, HoodieRollbackMetad
 
   /**
    * Update metadata table if available. Any update to metadata table happens within data table lock.
-   * @param rollbackMetadata intance of {@link HoodieRollbackMetadata} to be applied to metadata.
+   * @param rollbackMetadata instance of {@link HoodieRollbackMetadata} to be applied to metadata.
    */
   private void writeToMetadata(HoodieRollbackMetadata rollbackMetadata) {
     try {

File: hudi-examples/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java
Patch:
@@ -96,7 +96,7 @@ public static void main(String[] args) throws Exception {
     List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);
     List<HoodieRecord<HoodieAvroPayload>> writeRecords =
         recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());
-    client.upsert(writeRecords, newCommitTime);
+    client.insert(writeRecords, newCommitTime);
 
     // updates
     newCommitTime = client.startCommit();

File: hudi-flink/src/main/java/org/apache/hudi/sink/compact/CompactFunction.java
Patch:
@@ -99,7 +99,7 @@ public void processElement(CompactionPlanEvent event, Context context, Collector
 
   private void doCompaction(String instantTime, CompactionOperation compactionOperation, Collector<CompactionCommitEvent> collector) throws IOException {
     List<WriteStatus> writeStatuses = FlinkCompactHelpers.compact(writeClient, instantTime, compactionOperation);
-    collector.collect(new CompactionCommitEvent(instantTime, writeStatuses, taskID));
+    collector.collect(new CompactionCommitEvent(instantTime, compactionOperation.getFileId(), writeStatuses, taskID));
   }
 
   @VisibleForTesting

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/utils/KafkaConnectUtils.java
Patch:
@@ -87,7 +87,7 @@ public static Configuration getDefaultHadoopConf() {
   /**
    * Extract the record fields.
    * @param keyGenerator key generator Instance of the keygenerator.
-   * @return Returns the record key columns seprarated by comma.
+   * @return Returns the record key columns separated by comma.
    */
   public static String getRecordKeyColumns(KeyGenerator keyGenerator) {
     return String.join(",", keyGenerator.getRecordKeyFieldNames());
@@ -99,7 +99,7 @@ public static String getRecordKeyColumns(KeyGenerator keyGenerator) {
    *
    * @param keyGenerator key generator Instance of the keygenerator.
    * @param typedProperties properties from the config.
-   * @return partition columns Returns the partition columns seprarated by comma.
+   * @return partition columns Returns the partition columns separated by comma.
    */
   public static String getPartitionColumns(KeyGenerator keyGenerator, TypedProperties typedProperties) {
 

File: hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java
Patch:
@@ -419,7 +419,7 @@ private boolean flushBucket(DataBucket bucket) {
 
     List<HoodieRecord> records = bucket.writeBuffer();
     ValidationUtils.checkState(records.size() > 0, "Data bucket to flush has no buffering records");
-    if (config.getBoolean(FlinkOptions.INSERT_DROP_DUPS)) {
+    if (config.getBoolean(FlinkOptions.PRE_COMBINE)) {
       records = FlinkWriteHelper.newInstance().deduplicateRecords(records, (HoodieIndex) null, -1);
     }
     bucket.preWrite(records);
@@ -454,7 +454,7 @@ private void flushRemaining(boolean endInput) {
           .forEach(bucket -> {
             List<HoodieRecord> records = bucket.writeBuffer();
             if (records.size() > 0) {
-              if (config.getBoolean(FlinkOptions.INSERT_DROP_DUPS)) {
+              if (config.getBoolean(FlinkOptions.PRE_COMBINE)) {
                 records = FlinkWriteHelper.newInstance().deduplicateRecords(records, (HoodieIndex) null, -1);
               }
               bucket.preWrite(records);

File: hudi-flink/src/main/java/org/apache/hudi/sink/utils/PayloadCreation.java
Patch:
@@ -55,7 +55,7 @@ private PayloadCreation(
   }
 
   public static PayloadCreation instance(Configuration conf) throws Exception {
-    boolean shouldCombine = conf.getBoolean(FlinkOptions.INSERT_DROP_DUPS)
+    boolean shouldCombine = conf.getBoolean(FlinkOptions.PRE_COMBINE)
         || WriteOperationType.fromValue(conf.getString(FlinkOptions.OPERATION)) == WriteOperationType.UPSERT;
     String preCombineField = null;
     final Class<?>[] argTypes;

File: hudi-flink/src/test/java/org/apache/hudi/sink/TestWriteCopyOnWrite.java
Patch:
@@ -275,7 +275,7 @@ public void testInsert() throws Exception {
   @Test
   public void testInsertDuplicates() throws Exception {
     // reset the config option
-    conf.setBoolean(FlinkOptions.INSERT_DROP_DUPS, true);
+    conf.setBoolean(FlinkOptions.PRE_COMBINE, true);
     funcWrapper = new StreamWriteFunctionWrapper<>(tempFile.getAbsolutePath(), conf);
 
     // open the function and ingest data
@@ -470,7 +470,7 @@ public void testInsertWithMiniBatches() throws Exception {
   public void testInsertWithDeduplication() throws Exception {
     // reset the config option
     conf.setDouble(FlinkOptions.WRITE_BATCH_SIZE, 0.0008); // 839 bytes batch size
-    conf.setBoolean(FlinkOptions.INSERT_DROP_DUPS, true);
+    conf.setBoolean(FlinkOptions.PRE_COMBINE, true);
     funcWrapper = new StreamWriteFunctionWrapper<>(tempFile.getAbsolutePath(), conf);
 
     // open the function and ingest data

File: hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java
Patch:
@@ -652,7 +652,7 @@ void testWriteGlobalIndex() {
     String hoodieTableDDL = sql("t1")
         .option(FlinkOptions.PATH, tempFile.getAbsolutePath())
         .option(FlinkOptions.INDEX_GLOBAL_ENABLED, true)
-        .option(FlinkOptions.INSERT_DROP_DUPS, true)
+        .option(FlinkOptions.PRE_COMBINE, true)
         .end();
     streamTableEnv.executeSql(hoodieTableDDL);
 
@@ -674,7 +674,7 @@ void testWriteLocalIndex() {
     String hoodieTableDDL = sql("t1")
         .option(FlinkOptions.PATH, tempFile.getAbsolutePath())
         .option(FlinkOptions.INDEX_GLOBAL_ENABLED, false)
-        .option(FlinkOptions.INSERT_DROP_DUPS, true)
+        .option(FlinkOptions.PRE_COMBINE, true)
         .end();
     streamTableEnv.executeSql(hoodieTableDDL);
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/execution/FlinkLazyInsertIterable.java
Patch:
@@ -65,7 +65,7 @@ protected List<WriteStatus> computeNext() {
     try {
       final Schema schema = new Schema.Parser().parse(hoodieConfig.getSchema());
       bufferedIteratorExecutor =
-          new BoundedInMemoryExecutor<>(hoodieConfig.getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(inputItr), Option.of(getInsertHandler()), getTransformFunction(schema));
+          new BoundedInMemoryExecutor<>(hoodieConfig.getWriteBufferLimitBytes(), new IteratorBasedQueueProducer<>(inputItr), Option.of(getInsertHandler()), getTransformFunction(schema, hoodieConfig));
       final List<WriteStatus> result = bufferedIteratorExecutor.execute();
       assert result != null && !result.isEmpty() && !bufferedIteratorExecutor.isRemaining();
       return result;

File: hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -100,7 +100,7 @@ private FlinkOptions() {
   public static final ConfigOption<Integer> METADATA_COMPACTION_DELTA_COMMITS = ConfigOptions
       .key("metadata.compaction.delta_commits")
       .intType()
-      .defaultValue(24)
+      .defaultValue(10)
       .withDescription("Max delta commits for metadata table to trigger compaction, default 24");
 
   // ------------------------------------------------------------------------

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -55,7 +55,6 @@
 import org.apache.hudi.common.table.view.TableFileSystemView.BaseFileOnlyView;
 import org.apache.hudi.common.table.view.TableFileSystemView.SliceView;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
@@ -711,7 +710,7 @@ public HoodieEngineContext getContext() {
    * @return instance of {@link HoodieTableMetadataWriter}
    */
   public Option<HoodieTableMetadataWriter> getMetadataWriter() {
-    ValidationUtils.checkArgument(config.isMetadataTableEnabled(), "Metadata Table support not enabled in this Table");
+    // Each engine is expected to override this and provide the actual metadata writer if enabled.
     return Option.empty();
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -71,7 +71,7 @@ public final class HoodieMetadataConfig extends HoodieConfig {
   // Maximum delta commits before compaction occurs
   public static final ConfigProperty<Integer> COMPACT_NUM_DELTA_COMMITS = ConfigProperty
       .key(METADATA_PREFIX + ".compact.max.delta.commits")
-      .defaultValue(10)
+      .defaultValue(24)
       .sinceVersion("0.7.0")
       .withDocumentation("Controls how often the metadata table is compacted.");
 

File: hudi-common/src/main/java/org/apache/hudi/common/config/HoodieMetadataConfig.java
Patch:
@@ -71,7 +71,7 @@ public final class HoodieMetadataConfig extends HoodieConfig {
   // Maximum delta commits before compaction occurs
   public static final ConfigProperty<Integer> COMPACT_NUM_DELTA_COMMITS = ConfigProperty
       .key(METADATA_PREFIX + ".compact.max.delta.commits")
-      .defaultValue(24)
+      .defaultValue(10)
       .sinceVersion("0.7.0")
       .withDocumentation("Controls how often the metadata table is compacted.");
 

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/FileSystemViewCommand.java
Patch:
@@ -119,7 +119,7 @@ public String showAllFileSlices(
 
   @CliCommand(value = "show fsview latest", help = "Show latest file-system view")
   public String showLatestFileSlices(
-      @CliOption(key = {"partitionPath"}, help = "A valid paritition path", mandatory = true) String partition,
+      @CliOption(key = {"partitionPath"}, help = "A valid partition path", mandatory = true) String partition,
       @CliOption(key = {"baseFileOnly"}, help = "Only display base file view",
           unspecifiedDefaultValue = "false") boolean baseFileOnly,
       @CliOption(key = {"maxInstant"}, help = "File-Slices upto this instant are displayed",

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieHBaseIndexConfig.java
Patch:
@@ -148,7 +148,7 @@ public class HoodieHBaseIndexConfig extends HoodieConfig {
       .defaultValue(false)
       .withDocumentation("Only applies if index type is HBASE. "
           + "When an already existing record is upserted to a new partition compared to whats in storage, "
-          + "this config when set, will delete old record in old paritition "
+          + "this config when set, will delete old record in old partition "
           + "and will insert it as new record in new partition.");
 
   public static final ConfigProperty<Boolean> ROLLBACK_SYNC_ENABLE = ConfigProperty

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java
Patch:
@@ -321,7 +321,7 @@ private Map<String, String> getSparkSerdeProperties(boolean readAsOptimized) {
   }
 
   /**
-   * Syncs the list of storage parititions passed in (checks if the partition is in hive, if not adds it or if the
+   * Syncs the list of storage partitions passed in (checks if the partition is in hive, if not adds it or if the
    * partition path does not match, it updates the partition path).
    */
   private void syncPartitions(String tableName, List<String> writtenPartitionsSince) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -711,7 +711,7 @@ public HoodieEngineContext getContext() {
    * @return instance of {@link HoodieTableMetadataWriter}
    */
   public Option<HoodieTableMetadataWriter> getMetadataWriter() {
-    ValidationUtils.checkArgument(!config.isMetadataTableEnabled(), "Metadata Table support not enabled in this Table");
+    ValidationUtils.checkArgument(config.isMetadataTableEnabled(), "Metadata Table support not enabled in this Table");
     return Option.empty();
   }
 }

File: hudi-flink/src/test/java/org/apache/hudi/sink/utils/StreamWriteFunctionWrapper.java
Patch:
@@ -142,9 +142,6 @@ public StreamWriteFunctionWrapper(String tablePath, Configuration conf) throws E
   public void openFunction() throws Exception {
     this.coordinator.start();
     this.coordinator.setExecutor(new MockCoordinatorExecutor(coordinatorContext));
-    if (conf.getBoolean(FlinkOptions.METADATA_ENABLED)) {
-      this.coordinator.setMetadataSyncExecutor(new MockCoordinatorExecutor(coordinatorContext));
-    }
     toHoodieFunction = new RowDataToHoodieFunction<>(TestConfigurations.ROW_TYPE, conf);
     toHoodieFunction.setRuntimeContext(runtimeContext);
     toHoodieFunction.open(conf);

File: hudi-flink/src/test/java/org/apache/hudi/source/TestFileIndex.java
Patch:
@@ -27,7 +27,6 @@
 import org.apache.flink.configuration.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
-import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.io.TempDir;
 import org.junit.jupiter.params.ParameterizedTest;
@@ -90,7 +89,8 @@ void testFileListingUsingMetadataNonPartitionedTable() throws Exception {
     assertTrue(fileStatuses[0].getPath().toString().endsWith(HoodieFileFormat.PARQUET.getFileExtension()));
   }
 
-  @Disabled
+  @ParameterizedTest
+  @ValueSource(booleans = {true, false})
   void testFileListingEmptyTable(boolean enableMetadata) {
     Configuration conf = TestConfigurations.getDefaultConf(tempFile.getAbsolutePath());
     conf.setBoolean(FlinkOptions.METADATA_ENABLED, enableMetadata);

File: hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java
Patch:
@@ -74,7 +74,7 @@ public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning,
    * @param inputFormatClass The input format class of this table.
    * @param outputFormatClass The output format class of this table.
    * @param serdeClass The serde class of this table.
-   * @param serdeProperties The serde properites of this table.
+   * @param serdeProperties The serde properties of this table.
    * @param tableProperties The table properties for this table.
    */
   public abstract void createTable(String tableName, MessageType storageSchema,

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieSyncCommand.java
Patch:
@@ -89,7 +89,7 @@ public String validateSync(
   }
 
   private String getString(HoodieTableMetaClient target, HoodieTimeline targetTimeline, HoodieTableMetaClient source, long sourceCount, long targetCount, String sourceLatestCommit)
-          throws IOException {
+      throws IOException {
     List<HoodieInstant> commitsToCatchup = targetTimeline.findInstantsAfter(sourceLatestCommit, Integer.MAX_VALUE)
         .getInstants().collect(Collectors.toList());
     if (commitsToCatchup.isEmpty()) {

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.CleanerUtils;
 import org.apache.hudi.exception.HoodieIOException;
@@ -186,11 +187,11 @@ public void removeCorruptedPendingCleanAction() {
         CleanerUtils.getCleanerPlan(client, instant);
       } catch (AvroRuntimeException e) {
         LOG.warn("Corruption found. Trying to remove corrupted clean instant file: " + instant);
-        FSUtils.deleteInstantFile(client.getFs(), client.getMetaPath(), instant);
+        HoodieActiveTimeline.deleteInstantFile(client.getFs(), client.getMetaPath(), instant);
       } catch (IOException ioe) {
         if (ioe.getMessage().contains("Not an Avro data file")) {
           LOG.warn("Corruption found. Trying to remove corrupted clean instant file: " + instant);
-          FSUtils.deleteInstantFile(client.getFs(), client.getMetaPath(), instant);
+          HoodieActiveTimeline.deleteInstantFile(client.getFs(), client.getMetaPath(), instant);
         } else {
           throw new HoodieIOException(ioe.getMessage(), ioe);
         }

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestArchivedCommitsCommand.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.cli.functional.CLIFunctionalTestHarness;
 import org.apache.hudi.cli.testutils.HoodieTestCommitMetadataGenerator;
 import org.apache.hudi.cli.testutils.HoodieTestCommitUtilities;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
@@ -71,6 +72,7 @@ public void init() throws Exception {
     HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)
         .withSchema(HoodieTestCommitMetadataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().retainCommits(1).archiveCommitsWith(2, 3).build())
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build())
         .forTable("test-trip-table").build();
 
     // Create six commits

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCommitsCommand.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.cli.functional.CLIFunctionalTestHarness;
 import org.apache.hudi.cli.testutils.HoodieTestCommitMetadataGenerator;
 import org.apache.hudi.cli.testutils.HoodieTestReplaceCommitMetadataGenerator;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -208,6 +209,7 @@ public void testShowArchivedCommits() throws Exception {
     HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath1)
         .withSchema(HoodieTestCommitMetadataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().retainCommits(1).archiveCommitsWith(2, 3).build())
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build())
         .forTable("test-trip-table").build();
 
     // generate data and metadata

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCompactionCommand.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.cli.TableHeader;
 import org.apache.hudi.cli.functional.CLIFunctionalTestHarness;
 import org.apache.hudi.cli.testutils.HoodieTestCommitMetadataGenerator;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieAvroPayload;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -158,6 +159,7 @@ private void generateArchive() throws IOException {
     HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)
         .withSchema(HoodieTestCommitMetadataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().retainCommits(1).archiveCommitsWith(2, 3).build())
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build())
         .forTable("test-trip-table").build();
     // archive
     HoodieTableMetaClient metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java
Patch:
@@ -58,7 +58,7 @@ public static Option<HoodieCommitMetadata> resolveWriteConflictIfAny(final Hoodi
     if (config.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {
       ConflictResolutionStrategy resolutionStrategy = config.getWriteConflictResolutionStrategy();
       Stream<HoodieInstant> instantStream = resolutionStrategy.getCandidateInstants(table.getActiveTimeline(), currentTxnOwnerInstant.get(), lastCompletedTxnOwnerInstant);
-      final ConcurrentOperation thisOperation = new ConcurrentOperation(currentTxnOwnerInstant.get(), thisCommitMetadata.get());
+      final ConcurrentOperation thisOperation = new ConcurrentOperation(currentTxnOwnerInstant.get(), thisCommitMetadata.orElse(new HoodieCommitMetadata()));
       instantStream.forEach(instant -> {
         try {
           ConcurrentOperation otherOperation = new ConcurrentOperation(instant, table.getMetaClient());

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java
Patch:
@@ -141,13 +141,14 @@ protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMeta
     result.setWriteStats(writeStats);
     // Finalize write
     finalizeWrite(instantTime, writeStats, result);
-    syncTableMetadata();
     try {
       LOG.info("Committing " + instantTime + ", action Type " + getCommitActionType());
       HoodieActiveTimeline activeTimeline = table.getActiveTimeline();
       HoodieCommitMetadata metadata = CommitUtils.buildMetadata(writeStats, result.getPartitionToReplaceFileIds(),
           extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());
 
+      writeTableMetadata(metadata);
+
       activeTimeline.saveAsComplete(new HoodieInstant(true, getCommitActionType(), instantTime),
           Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));
       LOG.info("Committed " + instantTime);

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java
Patch:
@@ -206,6 +206,8 @@ protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMeta
       HoodieCommitMetadata metadata = CommitUtils.buildMetadata(writeStats, result.getPartitionToReplaceFileIds(),
           extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());
 
+      writeTableMetadata(metadata);
+
       activeTimeline.saveAsComplete(new HoodieInstant(true, getCommitActionType(), instantTime),
           Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));
       LOG.info("Committed " + instantTime);

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteTableCommitActionExecutor.java
Patch:
@@ -52,8 +52,7 @@ protected List<String> getAllExistingFileIds(String partitionPath) {
   protected Map<String, List<String>> getPartitionToReplacedFileIds(HoodieWriteMetadata<List<WriteStatus>> writeResult) {
     Map<String, List<String>> partitionToExistingFileIds = new HashMap<>();
     List<String> partitionPaths = FSUtils.getAllPartitionPaths(context,
-        table.getMetaClient().getBasePath(), config.isMetadataTableEnabled(),
-        config.getFileListingMetadataVerify(), config.shouldAssumeDatePartitioning());
+        table.getMetaClient().getBasePath(), config.isMetadataTableEnabled(), config.shouldAssumeDatePartitioning());
 
     if (partitionPaths != null && partitionPaths.size() > 0) {
       partitionToExistingFileIds = context.mapToPair(partitionPaths,

File: hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.client.HoodieJavaWriteClient;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.bloom.BloomFilter;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.engine.EngineType;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieKey;
@@ -114,7 +115,8 @@ private HoodieWriteConfig.Builder makeHoodieClientConfigBuilder() {
     return HoodieWriteConfig.newBuilder()
         .withEngineType(EngineType.JAVA)
         .withPath(basePath)
-        .withSchema(SCHEMA.toString());
+        .withSchema(SCHEMA.toString())
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build());
   }
 
   @Test

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/upgrade/OneToTwoUpgradeHandler.java
Patch:
@@ -30,4 +30,4 @@ public class OneToTwoUpgradeHandler extends BaseOneToTwoUpgradeHandler {
   String getPartitionColumns(HoodieWriteConfig config) {
     return HoodieSparkUtils.getPartitionColumns(config.getProps());
   }
-}
+}
\ No newline at end of file

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/storage/row/TestHoodieRowCreateHandle.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.io.storage.row;
 
 import org.apache.hudi.client.HoodieInternalWriteStatus;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
@@ -168,7 +169,8 @@ public void testGlobalFailure() throws Exception {
   @Test
   public void testInstantiationFailure() throws IOException {
     // init config and table
-    HoodieWriteConfig cfg = SparkDatasetTestUtils.getConfigBuilder(basePath).withPath("/dummypath/abc/").build();
+    HoodieWriteConfig cfg = SparkDatasetTestUtils.getConfigBuilder(basePath).withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build())
+        .withPath("/dummypath/abc/").build();
     HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);
 
     try {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestHoodieMergeOnReadTable.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.client.HoodieReadClient;
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
@@ -193,7 +194,7 @@ public void testUpsertPartitioner(boolean populateMetaFields) throws Exception {
   @ValueSource(booleans = {true, false})
   public void testLogFileCountsAfterCompaction(boolean populateMetaFields) throws Exception {
     // insert 100 records
-    HoodieWriteConfig.Builder cfgBuilder = getConfigBuilder(true);
+    HoodieWriteConfig.Builder cfgBuilder = getConfigBuilder(true).withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build());
     addConfigsForPopulateMetaFields(cfgBuilder, populateMetaFields);
     HoodieWriteConfig config = cfgBuilder.build();
 
@@ -524,6 +525,7 @@ public void testHandleUpdateWithMultiplePartitions() throws Exception {
       JavaRDD<HoodieRecord> deleteRDD = jsc().parallelize(fewRecordsForDelete, 1);
 
       // initialize partitioner
+      hoodieTable.getHoodieView().sync();
       AbstractSparkDeltaCommitActionExecutor actionExecutor = new SparkDeleteDeltaCommitActionExecutor(context(), cfg, hoodieTable,
           newDeleteTime, deleteRDD);
       actionExecutor.getUpsertPartitioner(new WorkloadProfile(buildProfile(deleteRDD)));

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestAsyncCompaction.java
Patch:
@@ -52,7 +52,7 @@ public class TestAsyncCompaction extends CompactionTestBase {
 
   private HoodieWriteConfig getConfig(Boolean autoCommit) {
     return getConfigBuilder(autoCommit)
-        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true).validate(true).build())
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(true).build())
         .build();
   }
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/rollback/TestMergeOnReadRollbackActionExecutor.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.avro.model.HoodieRollbackPartitionMetadata;
 import org.apache.hudi.client.SparkRDDWriteClient;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieFileGroup;
 import org.apache.hudi.common.model.HoodieLogFile;
@@ -157,7 +158,7 @@ public void testFailForCompletedInstants() {
   @Test
   public void testRollbackWhenFirstCommitFail() throws Exception {
 
-    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).build();
+    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build()).build();
 
     try (SparkRDDWriteClient client = getHoodieWriteClient(config)) {
       client.startCommitWithTime("001");

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableClustering.java
Patch:
@@ -131,6 +131,7 @@ void testClustering(boolean doUpdates, boolean populateMetaFields, boolean prese
       }
 
       HoodieTable hoodieTable = HoodieSparkTable.create(cfg, context(), metaClient);
+      hoodieTable.getHoodieView().sync();
       FileStatus[] allFiles = listAllBaseFilesInPath(hoodieTable);
       // expect 2 base files for each partition
       assertEquals(dataGen.getPartitionPaths().length * 2, allFiles.length);
@@ -146,6 +147,7 @@ void testClustering(boolean doUpdates, boolean populateMetaFields, boolean prese
 
       metaClient = HoodieTableMetaClient.reload(metaClient);
       final HoodieTable clusteredTable = HoodieSparkTable.create(cfg, context(), metaClient);
+      clusteredTable.getHoodieView().sync();
       Stream<HoodieBaseFile> dataFilesToRead = Arrays.stream(dataGen.getPartitionPaths())
           .flatMap(p -> clusteredTable.getBaseFileOnlyView().getLatestBaseFiles(p));
       // verify there should be only one base file per partition after clustering.

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableIncrementalRead.java
Patch:
@@ -20,6 +20,7 @@
 package org.apache.hudi.table.functional;
 
 import org.apache.hudi.client.SparkRDDWriteClient;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
@@ -83,7 +84,7 @@ public void testIncrementalReadsWithCompaction() throws Exception {
     Properties props = new Properties();
     props.setProperty(HoodieTableConfig.BASE_FILE_FORMAT.key(), HoodieFileFormat.PARQUET.toString());
     HoodieTableMetaClient metaClient = getHoodieMetaClient(HoodieTableType.MERGE_ON_READ, props);
-    HoodieWriteConfig cfg = getConfig(true);
+    HoodieWriteConfig cfg = getConfigBuilder(true).withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(false).build()).build();
     try (SparkRDDWriteClient client = getHoodieWriteClient(cfg)) {
 
       /*

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestHoodieSparkMergeOnReadTableInsertUpdateDelete.java
Patch:
@@ -109,6 +109,7 @@ public void testSimpleInsertAndUpdate(HoodieFileFormat fileFormat, boolean popul
       client.compact(compactionCommitTime);
 
       HoodieTable hoodieTable = HoodieSparkTable.create(cfg, context(), metaClient);
+      hoodieTable.getHoodieView().sync();
       FileStatus[] allFiles = listAllBaseFilesInPath(hoodieTable);
       HoodieTableFileSystemView tableView = getHoodieTableFileSystemView(metaClient, hoodieTable.getCompletedCommitsTimeline(), allFiles);
       Stream<HoodieBaseFile> dataFilesToRead = tableView.getLatestBaseFiles();
@@ -238,6 +239,7 @@ public void testSimpleInsertsGeneratedIntoLogFiles() throws Exception {
       writeClient.commit(newCommitTime, statuses);
 
       HoodieTable table = HoodieSparkTable.create(config, context(), metaClient);
+      table.getHoodieView().sync();
       TableFileSystemView.SliceView tableRTFileSystemView = table.getSliceView();
 
       long numLogFiles = 0;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormat.java
Patch:
@@ -56,6 +56,8 @@ public interface HoodieLogFormat {
 
   String UNKNOWN_WRITE_TOKEN = "1-0-1";
 
+  String DEFAULT_WRITE_TOKEN = "0-0-0";
+
   /**
    * Writer interface to allow appending block to this file format.
    */

File: hudi-common/src/main/java/org/apache/hudi/metadata/FileSystemBackedTableMetadata.java
Patch:
@@ -126,13 +126,13 @@ public Map<String, FileStatus[]> getAllFilesInPartitions(List<String> partitionP
   }
 
   @Override
-  public Option<String> getUpdateTime() {
+  public Option<String> getSyncedInstantTime() {
     throw new UnsupportedOperationException();
   }
 
   @Override
-  public boolean isInSync() {
-    return true;
+  public Option<String> getLatestCompactionTime() {
+    throw new UnsupportedOperationException();
   }
 
   @Override

File: hudi-flink/src/main/java/org/apache/hudi/sink/utils/HiveSyncContext.java
Patch:
@@ -79,7 +79,6 @@ private static HiveSyncConfig buildSyncConfig(Configuration conf) {
     hiveSyncConfig.partitionValueExtractorClass = conf.getString(FlinkOptions.HIVE_SYNC_PARTITION_EXTRACTOR_CLASS_NAME);
     hiveSyncConfig.useJdbc = conf.getBoolean(FlinkOptions.HIVE_SYNC_USE_JDBC);
     hiveSyncConfig.useFileListingFromMetadata = conf.getBoolean(FlinkOptions.METADATA_ENABLED);
-    hiveSyncConfig.verifyMetadataFileListing = false;
     hiveSyncConfig.ignoreExceptions = conf.getBoolean(FlinkOptions.HIVE_SYNC_IGNORE_EXCEPTIONS);
     hiveSyncConfig.supportTimestamp = conf.getBoolean(FlinkOptions.HIVE_SYNC_SUPPORT_TIMESTAMP);
     hiveSyncConfig.autoCreateDatabase = conf.getBoolean(FlinkOptions.HIVE_SYNC_AUTO_CREATE_DB);

File: hudi-flink/src/main/java/org/apache/hudi/source/FileIndex.java
Patch:
@@ -175,8 +175,6 @@ private static HoodieMetadataConfig metadataConfig(org.apache.flink.configuratio
 
     // set up metadata.enabled=true in table DDL to enable metadata listing
     properties.put(HoodieMetadataConfig.ENABLE, conf.getBoolean(FlinkOptions.METADATA_ENABLED));
-    properties.put(HoodieMetadataConfig.SYNC_ENABLE, conf.getBoolean(FlinkOptions.METADATA_ENABLED));
-    properties.put(HoodieMetadataConfig.VALIDATE_ENABLE, false);
 
     return HoodieMetadataConfig.newBuilder().fromProperties(properties).build();
   }

File: hudi-flink/src/test/java/org/apache/hudi/sink/TestStreamWriteOperatorCoordinator.java
Patch:
@@ -41,6 +41,7 @@
 import org.apache.hadoop.fs.Path;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.io.TempDir;
 
@@ -180,7 +181,7 @@ public void testHiveSyncInvoked() throws Exception {
     assertDoesNotThrow(() -> coordinator.notifyCheckpointComplete(1));
   }
 
-  @Test
+  @Disabled
   void testSyncMetadataTable() throws Exception {
     // reset
     reset();

File: hudi-flink/src/test/java/org/apache/hudi/source/TestFileIndex.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.flink.configuration.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.io.TempDir;
 import org.junit.jupiter.params.ParameterizedTest;
@@ -89,8 +90,7 @@ void testFileListingUsingMetadataNonPartitionedTable() throws Exception {
     assertTrue(fileStatuses[0].getPath().toString().endsWith(HoodieFileFormat.PARQUET.getFileExtension()));
   }
 
-  @ParameterizedTest
-  @ValueSource(booleans = {true, false})
+  @Disabled
   void testFileListingEmptyTable(boolean enableMetadata) {
     Configuration conf = TestConfigurations.getDefaultConf(tempFile.getAbsolutePath());
     conf.setBoolean(FlinkOptions.METADATA_ENABLED, enableMetadata);

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java
Patch:
@@ -72,7 +72,6 @@
 
 import static org.apache.hudi.common.config.HoodieMetadataConfig.DEFAULT_METADATA_ENABLE_FOR_READERS;
 import static org.apache.hudi.common.config.HoodieMetadataConfig.ENABLE;
-import static org.apache.hudi.common.config.HoodieMetadataConfig.VALIDATE_ENABLE;
 
 public class HoodieInputFormatUtils {
 
@@ -419,7 +418,6 @@ public static Map<HoodieTableMetaClient, List<Path>> groupSnapshotPathsByMetaCli
   public static HoodieMetadataConfig buildMetadataConfig(Configuration conf) {
     return HoodieMetadataConfig.newBuilder()
         .enable(conf.getBoolean(ENABLE.key(), DEFAULT_METADATA_ENABLE_FOR_READERS))
-        .validate(conf.getBoolean(VALIDATE_ENABLE.key(), VALIDATE_ENABLE.defaultValue()))
         .build();
   }
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/reader/DFSHoodieDatasetInputReader.java
Patch:
@@ -88,7 +88,7 @@ protected List<String> getPartitions(Option<Integer> partitionsLimit) throws IOE
     // calls in metrics as they are not part of normal HUDI operation.
     HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);
     List<String> partitionPaths = FSUtils.getAllPartitionPaths(engineContext, metaClient.getBasePath(),
-        HoodieMetadataConfig.DEFAULT_METADATA_ENABLE_FOR_READERS, HoodieMetadataConfig.VALIDATE_ENABLE.defaultValue(), false);
+        HoodieMetadataConfig.DEFAULT_METADATA_ENABLE_FOR_READERS, false);
     // Sort partition so we can pick last N partitions by default
     Collections.sort(partitionPaths);
     if (!partitionPaths.isEmpty()) {

File: hudi-sync/hudi-dla-sync/src/main/java/org/apache/hudi/dla/HoodieDLAClient.java
Patch:
@@ -71,7 +71,7 @@ public class HoodieDLAClient extends AbstractSyncHoodieClient {
 
   public HoodieDLAClient(DLASyncConfig syncConfig, FileSystem fs) {
     super(syncConfig.basePath, syncConfig.assumeDatePartitioning, syncConfig.useFileListingFromMetadata,
-        syncConfig.verifyMetadataFileListing, false, fs);
+        false, fs);
     this.dlaConfig = syncConfig;
     try {
       this.partitionValueExtractor =

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java
Patch:
@@ -62,7 +62,7 @@ public class HoodieHiveClient extends AbstractSyncHoodieClient {
   private final HiveSyncConfig syncConfig;
 
   public HoodieHiveClient(HiveSyncConfig cfg, HiveConf configuration, FileSystem fs) {
-    super(cfg.basePath, cfg.assumeDatePartitioning, cfg.useFileListingFromMetadata, cfg.verifyMetadataFileListing, cfg.withOperationField, fs);
+    super(cfg.basePath, cfg.assumeDatePartitioning, cfg.useFileListingFromMetadata,  cfg.withOperationField, fs);
     this.syncConfig = cfg;
 
     // Support JDBC, HiveQL and metastore based implementations for backwards compatibility. Future users should

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/replication/GlobalHiveSyncConfig.java
Patch:
@@ -38,7 +38,6 @@ public static GlobalHiveSyncConfig copy(GlobalHiveSyncConfig cfg) {
     newConfig.tableName = cfg.tableName;
     newConfig.usePreApacheInputFormat = cfg.usePreApacheInputFormat;
     newConfig.useFileListingFromMetadata = cfg.useFileListingFromMetadata;
-    newConfig.verifyMetadataFileListing = cfg.verifyMetadataFileListing;
     newConfig.supportTimestamp = cfg.supportTimestamp;
     newConfig.decodePartition = cfg.decodePartition;
     newConfig.batchSyncNum = cfg.batchSyncNum;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java
Patch:
@@ -153,7 +153,7 @@ private Option<String> getLatestCommitTimestamp(FileSystem fs, Config cfg) {
   }
 
   private List<String> getPartitions(HoodieEngineContext engineContext, Config cfg) {
-    return FSUtils.getAllPartitionPaths(engineContext, cfg.sourceBasePath, true, false, false);
+    return FSUtils.getAllPartitionPaths(engineContext, cfg.sourceBasePath, true, false);
   }
 
   private void createSuccessTag(FileSystem fs, Config cfg) throws IOException {

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieSnapshotExporter.java
Patch:
@@ -203,6 +203,8 @@ public void testExportDatasetWithNoCommit() throws IOException {
     public void testExportDatasetWithNoPartition() throws IOException {
       // delete all source data
       lfs.delete(new Path(sourcePath + "/" + PARTITION_PATH), true);
+      // delete hudi metadata table too.
+      lfs.delete(new Path(cfg.sourceBasePath + "/" + ".hoodie/metadata"), true);
 
       // export
       final Throwable thrown = assertThrows(HoodieSnapshotExporterException.class, () -> {

File: hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/DefaultSource.java
Patch:
@@ -64,7 +64,7 @@ public Optional<DataSourceWriter> createWriter(String writeUUID, StructType sche
     String tblName = options.get(HoodieWriteConfig.TBL_NAME.key()).get();
     boolean populateMetaFields = options.getBoolean(HoodieTableConfig.POPULATE_META_FIELDS.key(),
         Boolean.parseBoolean(HoodieTableConfig.POPULATE_META_FIELDS.defaultValue()));
-    // 1st arg to createHooodieConfig is not really reuqired to be set. but passing it anyways.
+    // 1st arg to createHoodieConfig is not really required to be set. but passing it anyways.
     HoodieWriteConfig config = DataSourceUtils.createHoodieConfig(options.get(HoodieWriteConfig.AVRO_SCHEMA_STRING.key()).get(), path, tblName, options.asMap());
     boolean arePartitionRecordsSorted = HoodieInternalConfig.getBulkInsertIsPartitionRecordsSorted(
         options.get(HoodieInternalConfig.BULKINSERT_ARE_PARTITIONER_RECORDS_SORTED).isPresent()

File: hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java
Patch:
@@ -53,7 +53,7 @@ public Table getTable(StructType schema, Transform[] partitioning, Map<String, S
         HoodieTableConfig.POPULATE_META_FIELDS.defaultValue()));
     boolean arePartitionRecordsSorted = Boolean.parseBoolean(properties.getOrDefault(HoodieInternalConfig.BULKINSERT_ARE_PARTITIONER_RECORDS_SORTED,
         Boolean.toString(HoodieInternalConfig.DEFAULT_BULKINSERT_ARE_PARTITIONER_RECORDS_SORTED)));
-    // 1st arg to createHooodieConfig is not really reuqired to be set. but passing it anyways.
+    // 1st arg to createHoodieConfig is not really required to be set. but passing it anyways.
     HoodieWriteConfig config = DataSourceUtils.createHoodieConfig(properties.get(HoodieWriteConfig.AVRO_SCHEMA_STRING.key()), path, tblName, properties);
     return new HoodieDataSourceInternalTable(instantTime, config, schema, getSparkSession(),
         getConfiguration(), properties, populateMetaFields, arePartitionRecordsSorted);

File: hudi-common/src/main/java/org/apache/hudi/common/config/ConfigGroups.java
Patch:
@@ -74,7 +74,7 @@ public static String getDescription(Names names) {
             + "Hudi stats and metrics.";
         break;
       case KAFKA_CONNECT:
-        description = "These set of configs are used for Kakfa Connect Sink Connector for writing Hudi Tables";
+        description = "These set of configs are used for Kafka Connect Sink Connector for writing Hudi Tables";
         break;
       default:
         description = "Please fill in the description for Config Group Name: " + names.name;

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/KafkaConnectConfigs.java
Patch:
@@ -36,7 +36,7 @@
 @Immutable
 @ConfigClassProperty(name = "Kafka Sink Connect Configurations",
     groupName = ConfigGroups.Names.KAFKA_CONNECT,
-    description = "Configurations for Kakfa Connect Sink Connector for Hudi.")
+    description = "Configurations for Kafka Connect Sink Connector for Hudi.")
 public class KafkaConnectConfigs extends HoodieConfig {
 
   public static final String KAFKA_VALUE_CONVERTER = "value.converter";

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java
Patch:
@@ -1514,8 +1514,8 @@ private void testDeltaStreamerTransitionFromParquetToKafkaSource(boolean autoRes
 
     prepareParquetDFSSource(true, false, "source_uber.avsc", "target_uber.avsc", PROPS_FILENAME_TEST_PARQUET,
         PARQUET_SOURCE_ROOT, false);
-    // delta streamer w/ parquest source
-    String tableBasePath = dfsBasePath + "/test_dfs_to_kakfa" + testNum;
+    // delta streamer w/ parquet source
+    String tableBasePath = dfsBasePath + "/test_dfs_to_kafka" + testNum;
     HoodieDeltaStreamer deltaStreamer = new HoodieDeltaStreamer(
         TestHelpers.makeConfig(tableBasePath, WriteOperationType.INSERT, ParquetDFSSource.class.getName(),
             Collections.EMPTY_LIST, PROPS_FILENAME_TEST_PARQUET, false,

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
Patch:
@@ -850,7 +850,7 @@ private void verifyOldParquetFileTest(HoodieHiveClient hiveClient, String emptyC
         "Hive Schema should match the table schema + partition field");
     assertEquals(1, hiveClient.scanTablePartitions(HiveTestUtil.hiveSyncConfig.tableName).size(),"Table partitions should match the number of partitions we wrote");
     assertEquals(emptyCommitTime,
-        hiveClient.getLastCommitTimeSynced(HiveTestUtil.hiveSyncConfig.tableName).get(),"The last commit that was sycned should be updated in the TBLPROPERTIES");
+        hiveClient.getLastCommitTimeSynced(HiveTestUtil.hiveSyncConfig.tableName).get(),"The last commit that was synced should be updated in the TBLPROPERTIES");
 
     // make sure correct schema is picked
     Schema schema = SchemaTestUtil.getSimpleSchema();

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java
Patch:
@@ -1295,7 +1295,7 @@ public void testBulkInsertsAndUpsertsWithSQLBasedTransformerFor2StepPipeline() t
         "Table partitions should match the number of partitions we wrote");
     assertEquals(lastInstantForUpstreamTable,
         hiveClient.getLastCommitTimeSynced(hiveSyncConfig.tableName).get(),
-        "The last commit that was sycned should be updated in the TBLPROPERTIES");
+        "The last commit that was synced should be updated in the TBLPROPERTIES");
   }
 
   @Test

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -296,6 +296,9 @@ public static HiveSyncConfig buildHiveSyncConfig(TypedProperties props, String b
             SlashEncodedDayPartitionValueExtractor.class.getName());
     hiveSyncConfig.useJdbc = Boolean.valueOf(props.getString(DataSourceWriteOptions.HIVE_USE_JDBC().key(),
         DataSourceWriteOptions.HIVE_USE_JDBC().defaultValue()));
+    if (props.containsKey(DataSourceWriteOptions.HIVE_SYNC_MODE().key())) {
+      hiveSyncConfig.syncMode = props.getString(DataSourceWriteOptions.HIVE_SYNC_MODE().key());
+    }
     hiveSyncConfig.autoCreateDatabase = Boolean.valueOf(props.getString(DataSourceWriteOptions.HIVE_AUTO_CREATE_DATABASE().key(),
         DataSourceWriteOptions.HIVE_AUTO_CREATE_DATABASE().defaultValue()));
     hiveSyncConfig.ignoreExceptions = Boolean.valueOf(props.getString(DataSourceWriteOptions.HIVE_IGNORE_EXCEPTIONS().key(),

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveMetastoreBasedLockProvider.java
Patch:
@@ -149,7 +149,7 @@ public void close() {
       if (lock != null) {
         hiveClient.unlock(lock.getLockid());
       }
-      hiveClient.close();
+      Hive.closeCurrent();
     } catch (Exception e) {
       LOG.error(generateLogStatement(org.apache.hudi.common.lock.LockState.FAILED_TO_RELEASE, generateLogSuffixString()));
     }

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java
Patch:
@@ -295,7 +295,7 @@ public void close() {
     try {
       ddlExecutor.close();
       if (client != null) {
-        client.close();
+        Hive.closeCurrent();
         client = null;
       }
     } catch (Exception e) {

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/TestCluster.java
Patch:
@@ -47,6 +47,7 @@
 import org.apache.hadoop.hive.metastore.RetryingMetaStoreClient;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hive.service.server.HiveServer2;
 import org.apache.parquet.avro.AvroSchemaConverter;
 import org.apache.parquet.hadoop.ParquetWriter;
@@ -265,7 +266,7 @@ public void startHiveServer2() {
 
   public void shutDown() {
     stopHiveServer2();
-    client.close();
+    Hive.closeCurrent();
     hiveTestService.getHiveMetaStore().stop();
     hdfsTestService.stop();
   }

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieColumnProjectionUtils.java
Patch:
@@ -39,7 +39,7 @@
 import java.util.stream.IntStream;
 
 /**
- * Utility funcitons copied from Hive ColumnProjectionUtils.java.
+ * Utility functions copied from Hive ColumnProjectionUtils.java.
  * Needed to copy as we see NoSuchMethod errors when directly using these APIs with/without Spark.
  * Some of these methods are not available across hive versions.
  */

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java
Patch:
@@ -65,7 +65,7 @@ public HoodieHiveClient(HiveSyncConfig cfg, HiveConf configuration, FileSystem f
     super(cfg.basePath, cfg.assumeDatePartitioning, cfg.useFileListingFromMetadata, cfg.verifyMetadataFileListing, cfg.withOperationField, fs);
     this.syncConfig = cfg;
 
-    // Support JDBC, HiveQL and metastore based implementations for backwards compatiblity. Future users should
+    // Support JDBC, HiveQL and metastore based implementations for backwards compatibility. Future users should
     // disable jdbc and depend on metastore client for all hive registrations
     try {
       if (!StringUtils.isNullOrEmpty(cfg.syncMode)) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/WriteStatus.java
Patch:
@@ -201,6 +201,7 @@ public void setTotalErrorRecords(long totalErrorRecords) {
   public String toString() {
     final StringBuilder sb = new StringBuilder("WriteStatus {");
     sb.append("fileId=").append(fileId);
+    sb.append(", writeStat=").append(stat);
     sb.append(", globalError='").append(globalError).append('\'');
     sb.append(", hasErrors='").append(hasErrors()).append('\'');
     sb.append(", errorCount='").append(totalErrorRecords).append('\'');

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaTable.java
Patch:
@@ -29,9 +29,8 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
-import org.apache.hudi.exception.HoodieNotSupportedException;
-import org.apache.hudi.index.JavaHoodieIndex;
 import org.apache.hudi.index.HoodieIndex;
+import org.apache.hudi.index.JavaHoodieIndex;
 
 import java.util.List;
 
@@ -56,7 +55,7 @@ public static <T extends HoodieRecordPayload> HoodieJavaTable<T> create(HoodieWr
       case COPY_ON_WRITE:
         return new HoodieJavaCopyOnWriteTable<>(config, context, metaClient);
       case MERGE_ON_READ:
-        throw new HoodieNotSupportedException("MERGE_ON_READ is not supported yet");
+        return new HoodieJavaMergeOnReadTable<>(config, context, metaClient);
       default:
         throw new HoodieException("Unsupported table type :" + metaClient.getTableType());
     }

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/KafkaConnectConfigs.java
Patch:
@@ -67,7 +67,7 @@ public class KafkaConnectConfigs extends HoodieConfig {
 
   public static final ConfigProperty<String> COORDINATOR_WRITE_TIMEOUT_SECS = ConfigProperty
       .key("hoodie.kafka.coordinator.write.timeout.secs")
-      .defaultValue("60")
+      .defaultValue("300")
       .withDocumentation("The timeout after sending an END_COMMIT until when "
           + "the coordinator will wait for the write statuses from all the partitions"
           + "to ignore the current commit and start a new commit.");

File: hudi-kafka-connect/src/test/java/org/apache/hudi/writers/TestAbstractConnectWriter.java
Patch:
@@ -148,7 +148,7 @@ private static class AbstractHudiConnectWriterTestWrapper extends AbstractConnec
     private List<HoodieRecord> writtenRecords;
 
     public AbstractHudiConnectWriterTestWrapper(KafkaConnectConfigs connectConfigs, KeyGenerator keyGenerator, SchemaProvider schemaProvider) {
-      super(connectConfigs, keyGenerator, schemaProvider);
+      super(connectConfigs, keyGenerator, schemaProvider, "000");
       writtenRecords = new ArrayList<>();
     }
 
@@ -157,12 +157,12 @@ public List<HoodieRecord> getWrittenRecords() {
     }
 
     @Override
-    protected void writeHudiRecord(HoodieRecord<HoodieAvroPayload> record) {
+    protected void writeHudiRecord(HoodieRecord<?> record) {
       writtenRecords.add(record);
     }
 
     @Override
-    protected List<WriteStatus> flushHudiRecords() {
+    protected List<WriteStatus> flushRecords() {
       return null;
     }
   }

File: hudi-kafka-connect/src/test/java/org/apache/hudi/writers/TestBufferedConnectWriter.java
Patch:
@@ -88,7 +88,7 @@ public void testSimpleWriteAndFlush() throws Exception {
     Mockito.verify(mockHoodieJavaWriteClient, times(0))
         .bulkInsertPreppedRecords(anyList(), eq(COMMIT_TIME), eq(Option.empty()));
 
-    writer.flushHudiRecords();
+    writer.flushRecords();
     final ArgumentCaptor<List<HoodieRecord>> actualRecords = ArgumentCaptor.forClass(List.class);
     Mockito.verify(mockHoodieJavaWriteClient, times(1))
         .bulkInsertPreppedRecords(actualRecords.capture(), eq(COMMIT_TIME), eq(Option.empty()));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java
Patch:
@@ -434,10 +434,10 @@ protected void postCommit(HoodieTable<T, I, K, O> table, HoodieCommitMetadata me
       // Delete the marker directory for the instant.
       WriteMarkersFactory.get(config.getMarkersType(), table, instantTime)
           .quietDeleteMarkerDir(context, config.getMarkersDeleteParallelism());
+      autoCleanOnCommit();
       // We cannot have unbounded commit files. Archive commits if we have to archive
       HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(config, table);
       archiveLog.archiveIfRequired(context);
-      autoCleanOnCommit();
       if (operationType != null && operationType != WriteOperationType.CLUSTER && operationType != WriteOperationType.COMPACT) {
         syncTableMetadata();
       }

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/transaction/ConnectTransactionCoordinator.java
Patch:
@@ -131,6 +131,7 @@ public void start() {
   @Override
   public void stop() {
     kafkaControlClient.deregisterTransactionCoordinator(this);
+    scheduler.shutdownNow();
     hasStarted.set(false);
     if (executorService != null) {
       boolean terminated = false;

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/transaction/TransactionParticipant.java
Patch:
@@ -21,6 +21,8 @@
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.connect.sink.SinkRecord;
 
+import java.io.IOException;
+
 /**
  * Interface for the Participant that
  * manages Writes for a
@@ -35,7 +37,7 @@ public interface TransactionParticipant {
 
   void buffer(SinkRecord record);
 
-  void processRecords();
+  void processRecords() throws IOException;
 
   TopicPartition getPartition();
 

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/AbstractConnectWriter.java
Patch:
@@ -81,11 +81,11 @@ public void writeRecord(SinkRecord record) throws IOException {
   }
 
   @Override
-  public List<WriteStatus> close() {
+  public List<WriteStatus> close() throws IOException {
     return flushHudiRecords();
   }
 
   protected abstract void writeHudiRecord(HoodieRecord<HoodieAvroPayload> record);
 
-  protected abstract List<WriteStatus> flushHudiRecords();
+  protected abstract List<WriteStatus> flushHudiRecords() throws IOException;
 }

File: hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/BufferedConnectWriter.java
Patch:
@@ -28,7 +28,6 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.ExternalSpillableMap;
 import org.apache.hudi.config.HoodieWriteConfig;
-import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.io.IOUtils;
 import org.apache.hudi.keygen.KeyGenerator;
@@ -94,7 +93,7 @@ public void writeHudiRecord(HoodieRecord<HoodieAvroPayload> record) {
   }
 
   @Override
-  public List<WriteStatus> flushHudiRecords() {
+  public List<WriteStatus> flushHudiRecords() throws IOException {
     try {
       LOG.info("Number of entries in MemoryBasedMap => "
           + bufferedRecords.getInMemoryMapNumEntries()
@@ -114,7 +113,7 @@ public List<WriteStatus> flushHudiRecords() {
           + writeStatuses);
       return writeStatuses;
     } catch (Exception e) {
-      throw new HoodieException("Write records failed", e);
+      throw new IOException("Write records failed", e);
     }
   }
 }

File: hudi-kafka-connect/src/test/java/org/apache/hudi/helper/TestKafkaConnect.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.kafka.connect.sink.SinkRecord;
 import org.apache.kafka.connect.sink.SinkTaskContext;
 
+import java.io.IOException;
 import java.util.Arrays;
 import java.util.Map;
 import java.util.Set;
@@ -60,7 +61,7 @@ public boolean isResumed() {
     return !isPaused;
   }
 
-  public int putRecordsToParticipant() {
+  public int putRecordsToParticipant() throws IOException {
     for (int i = 1; i <= NUM_RECORDS_BATCH; i++) {
       participant.buffer(getNextKafkaRecord());
     }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteJob.java
Patch:
@@ -101,7 +101,7 @@ public HoodieTestSuiteJob(HoodieTestSuiteConfig cfg, JavaSparkContext jsc) throw
     this.cfg = cfg;
     this.jsc = jsc;
     cfg.propsFilePath = FSUtils.addSchemeIfLocalPath(cfg.propsFilePath).toString();
-    this.sparkSession = SparkSession.builder().config(jsc.getConf()).getOrCreate();
+    this.sparkSession = SparkSession.builder().config(jsc.getConf()).enableHiveSupport().getOrCreate();
     this.fs = FSUtils.getFs(cfg.inputBasePath, jsc.hadoopConfiguration());
     this.props = UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();
     log.info("Creating workload generator with configs : {}", props.toString());

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/InsertNode.java
Patch:
@@ -60,6 +60,7 @@ protected void generate(DeltaGenerator deltaGenerator) throws Exception {
     if (!config.isDisableGenerate()) {
       log.info("Generating input data for node {}", this.getName());
       this.deltaWriteStatsRDD = deltaGenerator.writeRecords(deltaGenerator.generateInserts(config));
+      this.deltaWriteStatsRDD.cache();
       this.deltaWriteStatsRDD.count();
     }
   }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java
Patch:
@@ -1252,7 +1252,7 @@ private Map<String, List<BootstrapFileMapping>> generateBootstrapIndexAndSourceD
     assertTrue(new File(sourcePath.toString()).exists());
 
     // recreate metaClient with Bootstrap base path
-    metaClient = HoodieTestUtils.init(basePath, getTableType(), sourcePath.toString());
+    metaClient = HoodieTestUtils.init(basePath, getTableType(), sourcePath.toString(), true);
 
     // generate bootstrap index
     Map<String, List<BootstrapFileMapping>> bootstrapMapping = TestBootstrapIndex.generateBootstrapIndex(metaClient, sourcePath.toString(),

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java
Patch:
@@ -63,9 +63,10 @@ public static HoodieTableMetaClient init(String basePath, HoodieTableType tableT
     return init(getDefaultHadoopConf(), basePath, tableType);
   }
 
-  public static HoodieTableMetaClient init(String basePath, HoodieTableType tableType, String bootstrapBasePath) throws IOException {
+  public static HoodieTableMetaClient init(String basePath, HoodieTableType tableType, String bootstrapBasePath, boolean bootstrapIndexEnable) throws IOException {
     Properties props = new Properties();
     props.setProperty(HoodieTableConfig.BOOTSTRAP_BASE_PATH.key(), bootstrapBasePath);
+    props.put(HoodieTableConfig.BOOTSTRAP_INDEX_ENABLE.key(), bootstrapIndexEnable);
     return init(getDefaultHadoopConf(), basePath, tableType, props);
   }
 

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestBootstrap.java
Patch:
@@ -195,9 +195,9 @@ private enum EffectiveMode {
   private void testBootstrapCommon(boolean partitioned, boolean deltaCommit, EffectiveMode mode) throws Exception {
 
     if (deltaCommit) {
-      metaClient = HoodieTestUtils.init(basePath, HoodieTableType.MERGE_ON_READ, bootstrapBasePath);
+      metaClient = HoodieTestUtils.init(basePath, HoodieTableType.MERGE_ON_READ, bootstrapBasePath, true);
     } else {
-      metaClient = HoodieTestUtils.init(basePath, HoodieTableType.COPY_ON_WRITE, bootstrapBasePath);
+      metaClient = HoodieTestUtils.init(basePath, HoodieTableType.COPY_ON_WRITE, bootstrapBasePath, true);
     }
 
     int totalRecords = 100;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -77,7 +77,7 @@
  * Existing data:
  *     rec1_1, rec2_1, rec3_1, rec4_1
  *
- * For every existing record, merge w/ incoming if requried and write to storage.
+ * For every existing record, merge w/ incoming if required and write to storage.
  *    => rec1_1 and rec1_2 is merged to write rec1_2 to storage
  *    => rec2_1 is written as is
  *    => rec3_1 is written as is

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java
Patch:
@@ -189,13 +189,13 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)
 
         // Go over all such buckets, and assign weights as per amount of incoming inserts.
         List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();
-        double curentCumulativeWeight = 0;
+        double currentCumulativeWeight = 0;
         for (int i = 0; i < bucketNumbers.size(); i++) {
           InsertBucket bkt = new InsertBucket();
           bkt.bucketNumber = bucketNumbers.get(i);
           bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();
-          curentCumulativeWeight += bkt.weight;
-          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, curentCumulativeWeight));
+          currentCumulativeWeight += bkt.weight;
+          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, currentCumulativeWeight));
         }
         LOG.info("Total insert buckets for partition path " + partitionPath + " => " + insertBuckets);
         partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java
Patch:
@@ -232,13 +232,13 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)
 
         // Go over all such buckets, and assign weights as per amount of incoming inserts.
         List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();
-        double curentCumulativeWeight = 0;
+        double currentCumulativeWeight = 0;
         for (int i = 0; i < bucketNumbers.size(); i++) {
           InsertBucket bkt = new InsertBucket();
           bkt.bucketNumber = bucketNumbers.get(i);
           bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();
-          curentCumulativeWeight += bkt.weight;
-          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, curentCumulativeWeight));
+          currentCumulativeWeight += bkt.weight;
+          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, currentCumulativeWeight));
         }
         LOG.info("Total insert buckets for partition path " + partitionPath + " => " + insertBuckets);
         partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);

File: hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.exception.HoodieException;
+
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -352,6 +352,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource(
         }
       }
     } else {
+      // initialize the table for the first time.
       String partitionColumns = HoodieSparkUtils.getPartitionColumns(keyGenerator, props);
       HoodieTableMetaClient.withPropertyBuilder()
           .setTableType(cfg.tableType)

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaProvider.java
Patch:
@@ -29,7 +29,8 @@
 import java.io.Serializable;
 
 /**
- * Class to provide schema for reading data and also writing into a Hoodie table.
+ * Class to provide schema for reading data and also writing into a Hoodie table,
+ * used by deltastreamer (runs over Spark).
  */
 @PublicAPIClass(maturity = ApiMaturityLevel.STABLE)
 public abstract class SchemaProvider implements Serializable {

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteWriter.java
Patch:
@@ -171,7 +171,7 @@ public JavaRDD<WriteStatus> insert(Option<String> instantTime) throws Exception
   }
 
   public JavaRDD<WriteStatus> insertOverwrite(Option<String> instantTime) throws Exception {
-    if(cfg.useDeltaStreamer){
+    if (cfg.useDeltaStreamer) {
       return deltaStreamerWrapper.insertOverwrite();
     } else {
       Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> nextBatch = fetchSource();
@@ -181,7 +181,7 @@ public JavaRDD<WriteStatus> insertOverwrite(Option<String> instantTime) throws E
   }
 
   public JavaRDD<WriteStatus> insertOverwriteTable(Option<String> instantTime) throws Exception {
-    if(cfg.useDeltaStreamer){
+    if (cfg.useDeltaStreamer) {
       return deltaStreamerWrapper.insertOverwriteTable();
     } else {
       Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> nextBatch = fetchSource();

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/DagUtils.java
Patch:
@@ -102,7 +102,7 @@ public static WorkflowDag convertYamlToDag(String yaml) throws IOException {
         case DAG_CONTENT:
           JsonNode dagContent = dagNode.getValue();
           Iterator<Entry<String, JsonNode>> contentItr = dagContent.fields();
-          while(contentItr.hasNext()) {
+          while (contentItr.hasNext()) {
             Entry<String, JsonNode> dagContentNode = contentItr.next();
             allNodes.put(dagContentNode.getKey(), convertJsonToDagNode(allNodes, dagContentNode.getKey(), dagContentNode.getValue()));
           }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/DagNode.java
Patch:
@@ -43,7 +43,7 @@ public abstract class DagNode<O> implements Comparable<DagNode<O>> {
 
   public DagNode clone() {
     List<DagNode<O>> tempChildNodes = new ArrayList<>();
-    for(DagNode dagNode: childNodes) {
+    for (DagNode dagNode: childNodes) {
       tempChildNodes.add(dagNode.clone());
     }
     this.childNodes = tempChildNodes;

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/DelayNode.java
Patch:
@@ -37,7 +37,7 @@ public DelayNode(int delayMins) {
 
   @Override
   public void execute(ExecutionContext context, int curItrCount) throws Exception {
-    log.warn("Waiting for "+ delayMins+" mins before going for next test run");
+    log.warn("Waiting for " + delayMins + " mins before going for next test run");
     Thread.sleep(delayMins * 60 * 1000);
   }
 }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateAsyncOperations.java
Patch:
@@ -77,9 +77,9 @@ public void execute(ExecutionContext executionContext, int curItrCount) throws E
         }
 
         if (config.validateArchival() || config.validateClean()) {
-          Pattern ARCHIVE_FILE_PATTERN =
+          final Pattern ARCHIVE_FILE_PATTERN =
               Pattern.compile("\\.commits_\\.archive\\..*");
-          Pattern CLEAN_FILE_PATTERN =
+          final Pattern CLEAN_FILE_PATTERN =
               Pattern.compile(".*\\.clean\\..*");
 
           String metadataPath = executionContext.getHoodieTestSuiteWriter().getCfg().targetBasePath + "/.hoodie";

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java
Patch:
@@ -72,8 +72,8 @@ public void generateDataByHoodieJavaApp(String hiveTableName, String tableType,
     }
 
     // Run Hoodie Java App
-    String cmd = String.format("%s --hive-sync --table-path %s  --hive-url %s  --table-type %s  --hive-table %s" +
-        " --commit-type %s  --table-name %s", HOODIE_GENERATE_APP, hdfsUrl, HIVE_SERVER_JDBC_URL,
+    String cmd = String.format("%s --hive-sync --table-path %s  --hive-url %s  --table-type %s  --hive-table %s"
+             + " --commit-type %s  --table-name %s", HOODIE_GENERATE_APP, hdfsUrl, HIVE_SERVER_JDBC_URL,
         tableType, hiveTableName, commitType, hoodieTableName);
     if (partitionType == PartitionType.MULTI_KEYS_PARTITIONED) {
       cmd = cmd + " --use-multi-partition-keys";

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java
Patch:
@@ -90,7 +90,6 @@ public class ITTestHoodieDemo extends ITTestBase {
           + " --hoodie-conf hoodie.datasource.hive_sync.database=default "
           + " --hoodie-conf hoodie.datasource.hive_sync.table=%s";
 
-
   @AfterEach
   public void clean() throws Exception {
     String hdfsCmd = "hdfs dfs -rm -R ";

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/schema/TestSchemaRegistryProvider.java
Patch:
@@ -51,7 +51,8 @@ private TypedProperties getProps() {
         put("hoodie.deltastreamer.schemaprovider.registry.urlSuffix", "-value");
         put("hoodie.deltastreamer.schemaprovider.registry.url", "http://foo:bar@localhost");
         put("hoodie.deltastreamer.source.kafka.topic", "foo");
-      }};
+      }
+    };
   }
 
   private Schema getExpectedSchema(String response) throws IOException {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/functional/TestMarkerBasedRollbackStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.action.rollback;
+package org.apache.hudi.table.functional;
 
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
@@ -32,6 +32,7 @@
 import org.apache.hudi.common.testutils.HoodieTestTable;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieSparkTable;
+import org.apache.hudi.table.action.rollback.SparkMarkerBasedRollbackStrategy;
 import org.apache.hudi.testutils.HoodieClientTestBase;
 
 import org.apache.hadoop.fs.FileStatus;
@@ -51,6 +52,7 @@
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
+@Tag("functional")
 public class TestMarkerBasedRollbackStrategy extends HoodieClientTestBase {
 
   private static final String TEST_NAME_WITH_PARAMS = "[{index}] Test with listing metadata enable={0}";
@@ -106,7 +108,6 @@ public void testCopyOnWriteRollbackWithTestTable() throws Exception {
     assertEquals(1, stats.stream().mapToInt(r -> r.getFailedDeleteFiles().size()).sum());
   }
 
-  @Tag("functional")
   @ParameterizedTest(name = TEST_NAME_WITH_PARAMS)
   @MethodSource("configParams")
   public void testCopyOnWriteRollback(boolean useFileListingMetadata) throws Exception {
@@ -129,7 +130,6 @@ public void testCopyOnWriteRollback(boolean useFileListingMetadata) throws Excep
     }
   }
 
-  @Tag("functional")
   @ParameterizedTest(name = TEST_NAME_WITH_PARAMS)
   @MethodSource("configParams")
   public void testMergeOnReadRollback(boolean useFileListingMetadata) throws Exception {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/functional/SparkClientFunctionalTestSuite.java
Patch:
@@ -17,15 +17,15 @@
  * under the License.
  */
 
-package org.apache.hudi.client.functional;
+package org.apache.hudi.functional;
 
 import org.junit.platform.runner.JUnitPlatform;
 import org.junit.platform.suite.api.IncludeTags;
 import org.junit.platform.suite.api.SelectPackages;
 import org.junit.runner.RunWith;
 
 @RunWith(JUnitPlatform.class)
-@SelectPackages("org.apache.hudi.client.functional")
+@SelectPackages({"org.apache.hudi.client.functional", "org.apache.hudi.table.functional"})
 @IncludeTags("functional")
 public class SparkClientFunctionalTestSuite {
 

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestHarness.java
Patch:
@@ -385,7 +385,7 @@ public HoodieTableFileSystemView getHoodieTableFileSystemView(HoodieTableMetaCli
     return tableView;
   }
 
-  protected Pair<HashMap<String, WorkloadStat>, WorkloadStat> buildProfile(JavaRDD<HoodieRecord> inputRecordsRDD) {
+  public static Pair<HashMap<String, WorkloadStat>, WorkloadStat> buildProfile(JavaRDD<HoodieRecord> inputRecordsRDD) {
     HashMap<String, WorkloadStat> partitionPathStatMap = new HashMap<>();
     WorkloadStat globalStat = new WorkloadStat();
 

File: hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -196,6 +196,7 @@ private FlinkOptions() {
       .defaultValue(60)// default 1 minute
       .withDescription("Check interval for streaming read of SECOND, default 1 minute");
 
+  public static final String START_COMMIT_EARLIEST = "earliest";
   public static final ConfigOption<String> READ_STREAMING_START_COMMIT = ConfigOptions
       .key("read.streaming.start-commit")
       .stringType()

File: hudi-flink/src/test/java/org/apache/hudi/utils/TestUtils.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.flink.core.fs.Path;
 
 import java.io.File;
+import java.util.Collections;
 
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
@@ -57,8 +58,6 @@ public static String getSplitPartitionPath(MergeOnReadInputSplit split) {
 
   public static StreamReadMonitoringFunction getMonitorFunc(Configuration conf) {
     final String basePath = conf.getString(FlinkOptions.PATH);
-    final HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder()
-        .setConf(StreamerUtil.getHadoopConf()).setBasePath(basePath).build();
-    return new StreamReadMonitoringFunction(conf, new Path(basePath), metaClient, 1024 * 1024L);
+    return new StreamReadMonitoringFunction(conf, new Path(basePath), 1024 * 1024L, Collections.emptySet());
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/common/util/ReflectionUtils.java
Patch:
@@ -51,8 +51,7 @@ public static Class<?> getClass(String clazzName) {
     synchronized (CLAZZ_CACHE) {
       if (!CLAZZ_CACHE.containsKey(clazzName)) {
         try {
-          Class<?> clazz = Class.forName(clazzName, true,
-              Thread.currentThread().getContextClassLoader());
+          Class<?> clazz = Class.forName(clazzName);
           CLAZZ_CACHE.put(clazzName, clazz);
         } catch (ClassNotFoundException e) {
           throw new HoodieException("Unable to load class", e);

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -354,7 +354,6 @@ protected boolean isBaseFileDueToPendingCompaction(HoodieBaseFile baseFile) {
   protected boolean isFileSliceAfterPendingCompaction(FileSlice fileSlice) {
     Option<Pair<String, CompactionOperation>> compactionWithInstantTime =
         getPendingCompactionOperationWithInstant(fileSlice.getFileGroupId());
-    LOG.info("Pending Compaction instant for (" + fileSlice + ") is :" + compactionWithInstantTime);
     return (compactionWithInstantTime.isPresent())
         && fileSlice.getBaseInstantTime().equals(compactionWithInstantTime.get().getKey());
   }

File: hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/HoodieSparkFunctionalTestSuite.java
Patch:
@@ -23,8 +23,8 @@
 import org.junit.runner.RunWith;
 
 @RunWith(JUnitPlatform.class)
-@SelectPackages("org.apache.hudi.hive.functional")
+@SelectPackages("org.apache.hudi.functional")
 @IncludeTags("functional")
 public class HoodieSparkFunctionalTestSuite {
 
-}
\ No newline at end of file
+}

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/embedded/EmbeddedTimelineService.java
Patch:
@@ -18,15 +18,15 @@
 
 package org.apache.hudi.client.embedded;
 
-import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.config.SerializableConfiguration;
+import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.common.table.marker.MarkerType;
 import org.apache.hudi.common.table.view.FileSystemViewManager;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.table.view.FileSystemViewStorageType;
 import org.apache.hudi.common.util.NetworkUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
-import org.apache.hudi.table.marker.MarkerType;
 import org.apache.hudi.timeline.service.TimelineService;
 
 import org.apache.log4j.LogManager;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -30,11 +30,12 @@
 import org.apache.hudi.common.engine.EngineType;
 import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
-import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
+import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.model.WriteConcurrencyMode;
 import org.apache.hudi.common.table.HoodieTableConfig;
+import org.apache.hudi.common.table.marker.MarkerType;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.util.ReflectionUtils;
@@ -46,7 +47,6 @@
 import org.apache.hudi.metrics.datadog.DatadogHttpClient.ApiSite;
 import org.apache.hudi.table.action.compact.CompactionTriggerStrategy;
 import org.apache.hudi.table.action.compact.strategy.CompactionStrategy;
-import org.apache.hudi.table.marker.MarkerType;
 
 import org.apache.hadoop.hbase.io.compress.Compression;
 import org.apache.orc.CompressionKind;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/DirectWriteMarkers.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.common.model.IOType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.HoodieTimer;
+import org.apache.hudi.common.util.MarkerUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
@@ -149,7 +150,7 @@ public Set<String> createdAndMergedDataPaths(HoodieEngineContext context, int pa
   }
 
   private String translateMarkerToDataPath(String markerPath) {
-    String rPath = stripMarkerFolderPrefix(markerPath);
+    String rPath = MarkerUtils.stripMarkerFolderPrefix(markerPath, basePath, instantTime);
     return stripMarkerSuffix(rPath);
   }
 
@@ -158,7 +159,7 @@ public Set<String> allMarkerFilePaths() throws IOException {
     Set<String> markerFiles = new HashSet<>();
     if (doesMarkerDirExist()) {
       FSUtils.processFiles(fs, markerDirPath.toString(), fileStatus -> {
-        markerFiles.add(stripMarkerFolderPrefix(fileStatus.getPath().toString()));
+        markerFiles.add(MarkerUtils.stripMarkerFolderPrefix(fileStatus.getPath().toString(), basePath, instantTime));
         return true;
       }, false);
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/WriteMarkersFactory.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.fs.StorageSchemes;
+import org.apache.hudi.common.table.marker.MarkerType;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.table.HoodieTable;
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/upgrade/ZeroToOneUpgradeHandler.java
Patch:
@@ -19,11 +19,12 @@
 package org.apache.hudi.table.upgrade;
 
 import org.apache.hudi.client.common.HoodieFlinkEngineContext;
-import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.HoodieRollbackStat;
+import org.apache.hudi.common.engine.HoodieEngineContext;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.IOType;
+import org.apache.hudi.common.table.marker.MarkerType;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -37,7 +38,6 @@
 import org.apache.hudi.table.action.rollback.RollbackUtils;
 import org.apache.hudi.table.marker.WriteMarkers;
 import org.apache.hudi.table.marker.WriteMarkersFactory;
-import org.apache.hudi.table.marker.MarkerType;
 
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/MarkerHandler.java
Patch:
@@ -23,8 +23,8 @@
 import org.apache.hudi.common.model.IOType;
 import org.apache.hudi.common.table.view.FileSystemViewManager;
 import org.apache.hudi.timeline.service.TimelineService;
-import org.apache.hudi.timeline.service.handlers.marker.MarkerCreationFuture;
 import org.apache.hudi.timeline.service.handlers.marker.MarkerCreationDispatchingRunnable;
+import org.apache.hudi.timeline.service.handlers.marker.MarkerCreationFuture;
 import org.apache.hudi.timeline.service.handlers.marker.MarkerDirState;
 
 import io.javalin.Context;

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java
Patch:
@@ -205,7 +205,7 @@ private Stream<HoodieInstant> getInstantsToArchive() {
     if (config.isMetadataTableEnabled()) {
       try (HoodieTableMetadata tableMetadata = HoodieTableMetadata.create(table.getContext(), config.getMetadataConfig(),
           config.getBasePath(), FileSystemViewStorageConfig.FILESYSTEM_VIEW_SPILLABLE_DIR.defaultValue())) {
-        Option<String> lastSyncedInstantTime = tableMetadata.getSyncedInstantTime();
+        Option<String> lastSyncedInstantTime = tableMetadata.getUpdateTime();
 
         if (lastSyncedInstantTime.isPresent()) {
           LOG.info("Limiting archiving of instants to last synced instant on metadata table at " + lastSyncedInstantTime.get());

File: hudi-common/src/main/java/org/apache/hudi/metadata/FileSystemBackedTableMetadata.java
Patch:
@@ -126,7 +126,7 @@ public Map<String, FileStatus[]> getAllFilesInPartitions(List<String> partitionP
   }
 
   @Override
-  public Option<String> getSyncedInstantTime() {
+  public Option<String> getUpdateTime() {
     throw new UnsupportedOperationException();
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -119,7 +119,7 @@ public class HoodieWriteConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> TIMELINE_LAYOUT_VERSION = ConfigProperty
       .key("hoodie.timeline.layout.version")
-      .noDefaultValue()
+      .defaultValue(Integer.toString(TimelineLayoutVersion.VERSION_1))
       .sinceVersion("0.5.1")
       .withDocumentation("Controls the layout of the timeline. Version 0 relied on renames, Version 1 (default) models "
           + "the timeline as an immutable log relying only on atomic writes for object storage.");

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -1973,6 +1973,7 @@ private void testRollbackAfterConsistencyCheckFailureUsingFileList(boolean rollb
         getConfigBuilder().withRollbackUsingMarkers(rollbackUsingMarkers).withAutoCommit(false)
             .withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder()
                 .withConsistencyCheckEnabled(true)
+                .withEnableOptimisticConsistencyGuard(enableOptimisticConsistencyGuard)
                 .withOptimisticConsistencyGuardSleepTimeMs(1).build())
             .withProperties(properties).build();
     SparkRDDWriteClient client = getHoodieWriteClient(cfg);
@@ -2204,6 +2205,7 @@ private Pair<Path, JavaRDD<WriteStatus>> testConsistencyCheck(HoodieTableMetaCli
             .withMaxConsistencyCheckIntervalMs(1).withInitialConsistencyCheckIntervalMs(1).withEnableOptimisticConsistencyGuard(enableOptimisticConsistencyGuard).build())
         .build()) : (getConfigBuilder().withAutoCommit(false)
         .withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder().withConsistencyCheckEnabled(true)
+            .withEnableOptimisticConsistencyGuard(enableOptimisticConsistencyGuard)
             .withOptimisticConsistencyGuardSleepTimeMs(1).build())
         .build());
     SparkRDDWriteClient client = getHoodieWriteClient(cfg);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieGlobalBloomIndex.java
Patch:
@@ -177,7 +177,8 @@ public void testExplodeRecordRDDWithFileComparisons() {
 
   @Test
   public void testTagLocation() throws Exception {
-    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).build();
+    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)
+        .withIndexConfig(HoodieIndexConfig.newBuilder().withBloomIndexUpdatePartitionPath(false).build()).build();
     SparkHoodieGlobalBloomIndex index = new SparkHoodieGlobalBloomIndex(config);
     HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);
     HoodieSparkWriteableTestTable testTable = HoodieSparkWriteableTestTable.of(hoodieTable, SCHEMA);

File: hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuardConfig.java
Patch:
@@ -77,7 +77,7 @@ public class ConsistencyGuardConfig extends HoodieConfig {
   // config to enable OptimisticConsistencyGuard in finalizeWrite instead of FailSafeConsistencyGuard
   public static final ConfigProperty<Boolean> ENABLE_OPTIMISTIC_CONSISTENCY_GUARD_PROP = ConfigProperty
       .key("_hoodie.optimistic.consistency.guard.enable")
-      .defaultValue(true)
+      .defaultValue(false)
       .sinceVersion("0.6.0")
       .withDocumentation("Enable consistency guard, which optimistically assumes consistency is achieved after a certain time period.");
 

File: hudi-examples/src/main/java/org/apache/hudi/examples/spark/HoodieWriteClientExample.java
Patch:
@@ -107,7 +107,7 @@ public static void main(String[] args) throws Exception {
       List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);
       List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);
       JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);
-      client.upsert(writeRecords, newCommitTime);
+      client.insert(writeRecords, newCommitTime);
 
       // updates
       newCommitTime = client.startCommit();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkRecentDaysClusteringPlanStrategy.java
Patch:
@@ -51,8 +51,10 @@ public SparkRecentDaysClusteringPlanStrategy(HoodieSparkMergeOnReadTable<T> tabl
 
   protected List<String> filterPartitionPaths(List<String> partitionPaths) {
     int targetPartitionsForClustering = getWriteConfig().getTargetPartitionsForClustering();
+    int skipPartitionsFromLatestForClustering = getWriteConfig().getSkipPartitionsFromLatestForClustering();
     return partitionPaths.stream()
         .sorted(Comparator.reverseOrder())
+        .skip(Math.max(skipPartitionsFromLatestForClustering, 0))
         .limit(targetPartitionsForClustering > 0 ? targetPartitionsForClustering : partitionPaths.size())
         .collect(Collectors.toList());
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewStorageConfig.java
Patch:
@@ -72,7 +72,7 @@ public class FileSystemViewStorageConfig extends HoodieConfig {
 
   public static final ConfigProperty<String> FILESYSTEM_VIEW_SPILLABLE_DIR = ConfigProperty
       .key("hoodie.filesystem.view.spillable.dir")
-      .defaultValue("/tmp/view_map/")
+      .defaultValue("/tmp/")
       .withDocumentation("Path on local storage to use, when file system view is held in a spillable map.");
 
   public static final ConfigProperty<Long> FILESYSTEM_VIEW_SPILLABLE_MEM = ConfigProperty

File: hudi-common/src/test/java/org/apache/hudi/common/util/collection/TestExternalSpillableMap.java
Patch:
@@ -342,6 +342,7 @@ record = records.get(key);
   public void testLargeInsertUpsert() {}
 
   private static Stream<Arguments> testArguments() {
+    // Arguments : 1. Disk Map Type 2. isCompressionEnabled for BitCaskMap
     return Stream.of(
         arguments(ExternalSpillableMap.DiskMapType.BITCASK, false),
         arguments(ExternalSpillableMap.DiskMapType.ROCKS_DB, false),

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -73,6 +73,7 @@ public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTa
         taskContextSupplier);
     writeStatus.setFileId(fileId);
     writeStatus.setPartitionPath(partitionPath);
+    writeStatus.setStat(new HoodieWriteStat());
 
     this.path = makeNewPath(partitionPath);
 
@@ -200,7 +201,7 @@ public List<WriteStatus> close() {
    * @throws IOException if error occurs
    */
   protected void setupWriteStatus() throws IOException {
-    HoodieWriteStat stat = new HoodieWriteStat();
+    HoodieWriteStat stat = writeStatus.getStat();
     stat.setPartitionPath(writeStatus.getPartitionPath());
     stat.setNumWrites(recordsWritten);
     stat.setNumDeletes(recordsDeleted);
@@ -214,7 +215,6 @@ protected void setupWriteStatus() throws IOException {
     RuntimeStats runtimeStats = new RuntimeStats();
     runtimeStats.setTotalCreateTime(timer.endTimer());
     stat.setRuntimeStats(runtimeStats);
-    writeStatus.setStat(stat);
   }
 
   protected long computeTotalWriteBytes() throws IOException {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataCreateHandle.java
Patch:
@@ -86,6 +86,7 @@ public HoodieRowDataCreateHandle(HoodieTable table, HoodieWriteConfig writeConfi
         writeConfig.getWriteStatusFailureFraction());
     writeStatus.setPartitionPath(partitionPath);
     writeStatus.setFileId(fileId);
+    writeStatus.setStat(new HoodieWriteStat());
     try {
       HoodiePartitionMetadata partitionMetadata =
           new HoodiePartitionMetadata(
@@ -145,7 +146,7 @@ public boolean canWrite() {
    */
   public HoodieInternalWriteStatus close() throws IOException {
     fileWriter.close();
-    HoodieWriteStat stat = new HoodieWriteStat();
+    HoodieWriteStat stat = writeStatus.getStat();
     stat.setPartitionPath(partitionPath);
     stat.setNumWrites(writeStatus.getTotalRecords());
     stat.setNumDeletes(0);
@@ -160,7 +161,6 @@ public HoodieInternalWriteStatus close() throws IOException {
     HoodieWriteStat.RuntimeStats runtimeStats = new HoodieWriteStat.RuntimeStats();
     runtimeStats.setTotalCreateTime(currTimer.endTimer());
     stat.setRuntimeStats(runtimeStats);
-    writeStatus.setStat(stat);
     return writeStatus;
   }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowCreateHandle.java
Patch:
@@ -86,6 +86,7 @@ public HoodieRowCreateHandle(HoodieTable table, HoodieWriteConfig writeConfig, S
         writeConfig.getWriteStatusFailureFraction());
     writeStatus.setPartitionPath(partitionPath);
     writeStatus.setFileId(fileId);
+    writeStatus.setStat(new HoodieWriteStat());
     try {
       HoodiePartitionMetadata partitionMetadata =
           new HoodiePartitionMetadata(
@@ -144,7 +145,7 @@ public boolean canWrite() {
    */
   public HoodieInternalWriteStatus close() throws IOException {
     fileWriter.close();
-    HoodieWriteStat stat = new HoodieWriteStat();
+    HoodieWriteStat stat = writeStatus.getStat();
     stat.setPartitionPath(partitionPath);
     stat.setNumWrites(writeStatus.getTotalRecords());
     stat.setNumDeletes(0);
@@ -159,7 +160,6 @@ public HoodieInternalWriteStatus close() throws IOException {
     HoodieWriteStat.RuntimeStats runtimeStats = new HoodieWriteStat.RuntimeStats();
     runtimeStats.setTotalCreateTime(currTimer.endTimer());
     stat.setRuntimeStats(runtimeStats);
-    writeStatus.setStat(stat);
     return writeStatus;
   }
 

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java
Patch:
@@ -150,7 +150,9 @@ protected void syncHoodieTable(String tableName, boolean useRealtimeInputFormat,
     // check if the database exists else create it
     if (cfg.autoCreateDatabase) {
       try {
-        hoodieHiveClient.createDatabase(cfg.databaseName);
+        if (!hoodieHiveClient.doesDataBaseExist(cfg.databaseName)) {
+          hoodieHiveClient.createDatabase(cfg.databaseName);
+        }
       } catch (Exception e) {
         // this is harmless since table creation will fail anyways, creation of DB is needed for in-memory testing
         LOG.warn("Unable to create database", e);

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java
Patch:
@@ -138,8 +138,7 @@ public static void clear() throws IOException, HiveException, MetaException {
       ddlExecutor.runSQL("drop table if exists " + tableName);
     }
     createdTablesSet.clear();
-    ddlExecutor.runSQL("drop database if exists " + hiveSyncConfig.databaseName);
-    ddlExecutor.runSQL("create database " + hiveSyncConfig.databaseName);
+    ddlExecutor.runSQL("drop database if exists " + hiveSyncConfig.databaseName + " cascade");
   }
 
   public static HiveConf getHiveConf() {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java
Patch:
@@ -130,7 +130,7 @@ public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig
   public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig,
                                    Option<EmbeddedTimelineService> timelineService) {
     super(context, writeConfig, timelineService);
-    this.metrics = new HoodieMetrics(config, config.getTableName());
+    this.metrics = new HoodieMetrics(config);
     this.index = createIndex(writeConfig);
     this.txnManager = new TransactionManager(config, fs);
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java
Patch:
@@ -54,9 +54,9 @@ public class HoodieMetrics {
   private Timer clusteringTimer = null;
   private Timer indexTimer = null;
 
-  public HoodieMetrics(HoodieWriteConfig config, String tableName) {
+  public HoodieMetrics(HoodieWriteConfig config) {
     this.config = config;
-    this.tableName = tableName;
+    this.tableName = config.getTableName();
     if (config.isMetricsOn()) {
       Metrics.init(config);
       this.rollbackTimerName = getMetricsName("timer", HoodieTimeline.ROLLBACK_ACTION);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/Metrics.java
Patch:
@@ -53,9 +53,7 @@ private Metrics(HoodieWriteConfig metricConfig) {
     }
     reporter.start();
 
-    Runtime.getRuntime().addShutdownHook(new Thread(() -> {
-      reportAndCloseReporter();
-    }));
+    Runtime.getRuntime().addShutdownHook(new Thread(this::reportAndCloseReporter));
   }
 
   private void reportAndCloseReporter() {

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/metrics/TestHoodieConsoleMetrics.java
Patch:
@@ -41,7 +41,7 @@ public void start() {
     when(config.getTableName()).thenReturn("console_metrics_test");
     when(config.isMetricsOn()).thenReturn(true);
     when(config.getMetricsReporterType()).thenReturn(MetricsReporterType.CONSOLE);
-    new HoodieMetrics(config, "raw_table");
+    new HoodieMetrics(config);
   }
 
   @AfterEach

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/metrics/TestHoodieJmxMetrics.java
Patch:
@@ -52,7 +52,7 @@ public void testRegisterGauge() {
     when(config.getMetricsReporterType()).thenReturn(MetricsReporterType.JMX);
     when(config.getJmxHost()).thenReturn("localhost");
     when(config.getJmxPort()).thenReturn(String.valueOf(NetworkTestUtils.nextFreePort()));
-    new HoodieMetrics(config, "raw_table");
+    new HoodieMetrics(config);
     registerGauge("jmx_metric1", 123L);
     assertEquals("123", Metrics.getInstance().getRegistry().getGauges()
         .get("jmx_metric1").getValue().toString());
@@ -65,7 +65,7 @@ public void testRegisterGaugeByRangerPort() {
     when(config.getMetricsReporterType()).thenReturn(MetricsReporterType.JMX);
     when(config.getJmxHost()).thenReturn("localhost");
     when(config.getJmxPort()).thenReturn(String.valueOf(NetworkTestUtils.nextFreePort()));
-    new HoodieMetrics(config, "raw_table");
+    new HoodieMetrics(config);
     registerGauge("jmx_metric2", 123L);
     assertEquals("123", Metrics.getInstance().getRegistry().getGauges()
         .get("jmx_metric2").getValue().toString());

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/metrics/TestHoodieMetrics.java
Patch:
@@ -50,8 +50,9 @@ public class TestHoodieMetrics {
   @BeforeEach
   void setUp() {
     when(config.isMetricsOn()).thenReturn(true);
+    when(config.getTableName()).thenReturn("raw_table");
     when(config.getMetricsReporterType()).thenReturn(MetricsReporterType.INMEMORY);
-    metrics = new HoodieMetrics(config, "raw_table");
+    metrics = new HoodieMetrics(config);
   }
 
   @AfterEach

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/metrics/prometheus/TestPrometheusReporter.java
Patch:
@@ -46,10 +46,11 @@ void shutdownMetrics() {
   @Test
   public void testRegisterGauge() {
     when(config.isMetricsOn()).thenReturn(true);
+    when(config.getTableName()).thenReturn("foo");
     when(config.getMetricsReporterType()).thenReturn(MetricsReporterType.PROMETHEUS);
     when(config.getPrometheusPort()).thenReturn(9090);
     assertDoesNotThrow(() -> {
-      new HoodieMetrics(config, "raw_table");
+      new HoodieMetrics(config);
     });
   }
 }

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/metrics/prometheus/TestPushGateWayReporter.java
Patch:
@@ -48,6 +48,7 @@ void shutdownMetrics() {
   @Test
   public void testRegisterGauge() {
     when(config.isMetricsOn()).thenReturn(true);
+    when(config.getTableName()).thenReturn("foo");
     when(config.getMetricsReporterType()).thenReturn(MetricsReporterType.PROMETHEUS_PUSHGATEWAY);
     when(config.getPushGatewayHost()).thenReturn("localhost");
     when(config.getPushGatewayPort()).thenReturn(9091);
@@ -57,7 +58,7 @@ public void testRegisterGauge() {
     when(config.getPushGatewayRandomJobNameSuffix()).thenReturn(false);
 
     assertDoesNotThrow(() -> {
-      new HoodieMetrics(config, "raw_table");
+      new HoodieMetrics(config);
     });
 
     registerGauge("pushGateWayReporter_metric", 123L);

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/metrics/TestHoodieConsoleMetrics.java
Patch:
@@ -38,6 +38,7 @@ public class TestHoodieConsoleMetrics {
 
   @BeforeEach
   public void start() {
+    when(config.getTableName()).thenReturn("console_metrics_test");
     when(config.isMetricsOn()).thenReturn(true);
     when(config.getMetricsReporterType()).thenReturn(MetricsReporterType.CONSOLE);
     new HoodieMetrics(config, "raw_table");

File: hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java
Patch:
@@ -207,14 +207,14 @@ public void initializeState(FunctionInitializationContext context) throws Except
             TypeInformation.of(WriteMetadataEvent.class)
         ));
 
+    this.currentInstant = this.writeClient.getLastPendingInstant(this.actionType);
     if (context.isRestored()) {
       restoreWriteMetadata();
     } else {
       sendBootstrapEvent();
     }
     // blocks flushing until the coordinator starts a new instant
     this.confirming = true;
-    this.currentInstant = this.writeClient.getLastPendingInstant(this.actionType);
   }
 
   @Override

File: hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -290,13 +290,13 @@ private FlinkOptions() {
   public static final ConfigOption<Integer> INDEX_BOOTSTRAP_TASKS = ConfigOptions
       .key("write.index_bootstrap.tasks")
       .intType()
-      .defaultValue(4)
+      .noDefaultValue()
       .withDescription("Parallelism of tasks that do index bootstrap, default is 4");
 
   public static final ConfigOption<Integer> BUCKET_ASSIGN_TASKS = ConfigOptions
       .key("write.bucket_assign.tasks")
       .intType()
-      .defaultValue(4)
+      .noDefaultValue()
       .withDescription("Parallelism of tasks that do bucket assign, default is 4");
 
   public static final ConfigOption<Integer> WRITE_TASKS = ConfigOptions

File: hudi-flink/src/main/java/org/apache/hudi/streamer/HoodieFlinkStreamer.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hudi.sink.compact.CompactionPlanEvent;
 import org.apache.hudi.sink.compact.CompactionPlanOperator;
 import org.apache.hudi.sink.partitioner.BucketAssignFunction;
+import org.apache.hudi.sink.partitioner.BucketAssignOperator;
 import org.apache.hudi.sink.transform.RowDataToHoodieFunction;
 import org.apache.hudi.sink.transform.Transformer;
 import org.apache.hudi.util.AvroSchemaConverter;
@@ -43,7 +44,6 @@
 import org.apache.flink.runtime.state.filesystem.FsStateBackend;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
-import org.apache.flink.streaming.api.operators.KeyedProcessOperator;
 import org.apache.flink.streaming.api.operators.ProcessOperator;
 import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;
 import org.apache.flink.table.data.RowData;
@@ -125,7 +125,7 @@ public static void main(String[] args) throws Exception {
         .transform(
             "bucket_assigner",
             TypeInformation.of(HoodieRecord.class),
-            new KeyedProcessOperator<>(new BucketAssignFunction<>(conf)))
+            new BucketAssignOperator<>(new BucketAssignFunction<>(conf)))
         .setParallelism(conf.getInteger(FlinkOptions.BUCKET_ASSIGN_TASKS))
         .uid("uid_bucket_assigner")
         // shuffle by fileId(bucket id)

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestParquet2SparkSchemaUtils.java
Patch:
@@ -35,9 +35,7 @@
 
 public class TestParquet2SparkSchemaUtils {
   private final SparkToParquetSchemaConverter spark2ParquetConverter =
-          new SparkToParquetSchemaConverter(
-                  (Boolean) SQLConf.PARQUET_WRITE_LEGACY_FORMAT().defaultValue().get(),
-                  SQLConf.ParquetOutputTimestampType$.MODULE$.INT96());
+          new SparkToParquetSchemaConverter(new SQLConf());
   private final SparkSqlParser parser = new SparkSqlParser(new SQLConf());
 
   @Test

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/minicluster/HdfsTestService.java
Patch:
@@ -144,6 +144,7 @@ private static Configuration configureDFSCluster(Configuration config, String lo
     String user = System.getProperty("user.name");
     config.set("hadoop.proxyuser." + user + ".groups", "*");
     config.set("hadoop.proxyuser." + user + ".hosts", "*");
+    config.setBoolean("dfs.permissions",false);
     return config;
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -339,8 +339,9 @@ public class HoodieWriteConfig extends HoodieConfig {
       .withDocumentation("");
 
   public static final ConfigProperty<String> EXTERNAL_RECORD_AND_SCHEMA_TRANSFORMATION = ConfigProperty
-      .key(AVRO_SCHEMA + ".externalTransformation")
+      .key(AVRO_SCHEMA.key() + ".external.transformation")
       .defaultValue("false")
+      .withAlternatives(AVRO_SCHEMA.key() + ".externalTransformation")
       .withDocumentation("");
 
   private ConsistencyGuardConfig consistencyGuardConfig;

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java
Patch:
@@ -146,7 +146,7 @@ private Map<String, Set<String>> getPartitionPathToPendingClusteringFileGroupsId
    * @return smallFiles not in clustering
    */
   private List<SmallFile> filterSmallFilesInClustering(final Set<String> pendingClusteringFileGroupsId, final List<SmallFile> smallFiles) {
-    if (this.config.isClusteringEnabled()) {
+    if (!pendingClusteringFileGroupsId.isEmpty()) {
       return smallFiles.stream()
           .filter(smallFile -> !pendingClusteringFileGroupsId.contains(smallFile.location.getFileId())).collect(Collectors.toList());
     } else {

File: hudi-flink/src/test/java/org/apache/hudi/sink/TestWriteCopyOnWrite.java
Patch:
@@ -399,7 +399,7 @@ public void testInsertWithMiniBatches() throws Exception {
 
     Map<String, List<HoodieRecord>> dataBuffer = funcWrapper.getDataBuffer();
     assertThat("Should have 1 data bucket", dataBuffer.size(), is(1));
-    assertThat("3 records expect to flush out as a mini-batch",
+    assertThat("2 records expect to flush out as a mini-batch",
         dataBuffer.values().stream().findFirst().map(List::size).orElse(-1),
         is(2));
 
@@ -461,7 +461,7 @@ public void testInsertWithDeduplication() throws Exception {
 
     Map<String, List<HoodieRecord>> dataBuffer = funcWrapper.getDataBuffer();
     assertThat("Should have 1 data bucket", dataBuffer.size(), is(1));
-    assertThat("3 records expect to flush out as a mini-batch",
+    assertThat("2 records expect to flush out as a mini-batch",
         dataBuffer.values().stream().findFirst().map(List::size).orElse(-1),
         is(2));
 
@@ -525,7 +525,7 @@ public void testInsertWithSmallBufferSize() throws Exception {
 
     Map<String, List<HoodieRecord>> dataBuffer = funcWrapper.getDataBuffer();
     assertThat("Should have 1 data bucket", dataBuffer.size(), is(1));
-    assertThat("3 records expect to flush out as a mini-batch",
+    assertThat("2 records expect to flush out as a mini-batch",
         dataBuffer.values().stream().findFirst().map(List::size).orElse(-1),
         is(2));
 

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/functional/TestHiveMetastoreBasedLockProvider.java
Patch:
@@ -134,10 +134,10 @@ public void testWaitingLock() throws Exception {
     HiveMetastoreBasedLockProvider lockProvider2 = new HiveMetastoreBasedLockProvider(lockConfiguration, hiveConf());
     lockComponent.setOperationType(DataOperationType.NO_TXN);
     Assertions.assertTrue(lockProvider1.acquireLock(lockConfiguration.getConfig()
-        .getLong(LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP), TimeUnit.MILLISECONDS, lockComponent));
+        .getLong(LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY), TimeUnit.MILLISECONDS, lockComponent));
     try {
       boolean acquireStatus = lockProvider2.acquireLock(lockConfiguration.getConfig()
-          .getLong(LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP), TimeUnit.MILLISECONDS, lockComponent);
+          .getLong(LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY), TimeUnit.MILLISECONDS, lockComponent);
       Assertions.assertFalse(acquireStatus);
     } catch (IllegalArgumentException e) {
       // expected
@@ -146,7 +146,7 @@ public void testWaitingLock() throws Exception {
     // create the third HiveMetastoreBasedLockProvider to acquire lock
     HiveMetastoreBasedLockProvider lockProvider3 = new HiveMetastoreBasedLockProvider(lockConfiguration, hiveConf());
     boolean acquireStatus = lockProvider3.acquireLock(lockConfiguration.getConfig()
-        .getLong(LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP), TimeUnit.MILLISECONDS, lockComponent);
+        .getLong(LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY), TimeUnit.MILLISECONDS, lockComponent);
     // we should acquired lock, since lockProvider1 has already released lock
     Assertions.assertTrue(acquireStatus);
     lockProvider3.unlock();

File: hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/DefaultSource.java
Patch:
@@ -67,6 +67,6 @@ public Optional<DataSourceWriter> createWriter(String writeUUID, StructType sche
         options.get(HoodieInternalConfig.BULKINSERT_ARE_PARTITIONER_RECORDS_SORTED).isPresent()
             ? options.get(HoodieInternalConfig.BULKINSERT_ARE_PARTITIONER_RECORDS_SORTED).get() : null);
     return Optional.of(new HoodieDataSourceInternalWriter(instantTime, config, schema, getSparkSession(),
-            getConfiguration(), arePartitionRecordsSorted));
+            getConfiguration(), options, arePartitionRecordsSorted));
   }
 }

File: hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/HoodieWriterCommitMessage.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.internal;
 
 import org.apache.hudi.client.HoodieInternalWriteStatus;
+
 import org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;
 
 import java.util.List;

File: hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java
Patch:
@@ -53,6 +53,6 @@ public Table getTable(StructType schema, Transform[] partitioning, Map<String, S
     // 1st arg to createHooodieConfig is not really reuqired to be set. but passing it anyways.
     HoodieWriteConfig config = DataSourceUtils.createHoodieConfig(properties.get(HoodieWriteConfig.AVRO_SCHEMA.key()), path, tblName, properties);
     return new HoodieDataSourceInternalTable(instantTime, config, schema, getSparkSession(),
-        getConfiguration(), arePartitionRecordsSorted);
+        getConfiguration(), properties, arePartitionRecordsSorted);
   }
 }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/metadata/TestHoodieBackedMetadata.java
Patch:
@@ -204,8 +204,7 @@ public void testOnlyValidPartitionsAdded() throws Exception {
     HoodieTestTable testTable = HoodieTestTable.of(metaClient);
     testTable.withPartitionMetaFiles("p1", "p2", filteredDirectoryOne, filteredDirectoryTwo, filteredDirectoryThree)
         .addCommit("001").withBaseFilesInPartition("p1", 10).withBaseFilesInPartition("p2", 10, 10)
-        .addCommit("002").withBaseFilesInPartition("p1", 10).withBaseFilesInPartition("p2", 10, 10, 10)
-        .addInflightCommit("003").withBaseFilesInPartition("p1", 10).withBaseFilesInPartition("p2", 10);
+        .addCommit("002").withBaseFilesInPartition("p1", 10).withBaseFilesInPartition("p2", 10, 10, 10);
 
     final HoodieWriteConfig writeConfig =
             getWriteConfigBuilder(HoodieFailedWritesCleaningPolicy.NEVER, true, true, false)

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMetrics.java
Patch:
@@ -48,6 +48,8 @@ public class HoodieMetadataMetrics implements Serializable {
   public static final String BASEFILE_READ_STR = "basefile_read";
   public static final String INITIALIZE_STR = "initialize";
   public static final String SYNC_STR = "sync";
+  public static final String REBOOTSTRAP_STR = "rebootstrap";
+  public static final String BOOTSTRAP_ERR_STR = "bootstrap_error";
 
   // Stats names
   public static final String STAT_TOTAL_BASE_FILE_SIZE = "totalBaseFileSizeInBytes";

File: hudi-flink/src/main/java/org/apache/hudi/schema/FilebasedSchemaProvider.java
Patch:
@@ -64,10 +64,10 @@ public FilebasedSchemaProvider(TypedProperties props) {
   }
 
   public FilebasedSchemaProvider(Configuration conf) {
-    final String readSchemaPath = conf.getString(FlinkOptions.READ_AVRO_SCHEMA_PATH);
-    final FileSystem fs = FSUtils.getFs(readSchemaPath, StreamerUtil.getHadoopConf());
+    final String sourceSchemaPath = conf.getString(FlinkOptions.SOURCE_AVRO_SCHEMA_PATH);
+    final FileSystem fs = FSUtils.getFs(sourceSchemaPath, StreamerUtil.getHadoopConf());
     try {
-      this.sourceSchema = new Schema.Parser().parse(fs.open(new Path(readSchemaPath)));
+      this.sourceSchema = new Schema.Parser().parse(fs.open(new Path(sourceSchemaPath)));
     } catch (IOException ioe) {
       throw new HoodieIOException("Error reading schema", ioe);
     }

File: hudi-flink/src/main/java/org/apache/hudi/util/CompactionUtil.java
Patch:
@@ -72,7 +72,7 @@ public static String getCompactionInstantTime(HoodieTableMetaClient metaClient)
   public static void setAvroSchema(Configuration conf, HoodieTableMetaClient metaClient) throws Exception {
     TableSchemaResolver tableSchemaResolver = new TableSchemaResolver(metaClient);
     Schema tableAvroSchema = tableSchemaResolver.getTableAvroSchema(false);
-    conf.setString(FlinkOptions.READ_AVRO_SCHEMA, tableAvroSchema.toString());
+    conf.setString(FlinkOptions.SOURCE_AVRO_SCHEMA, tableAvroSchema.toString());
   }
 
   /**

File: hudi-flink/src/test/java/org/apache/hudi/utils/TestConfigurations.java
Patch:
@@ -144,7 +144,7 @@ public static String getCollectSinkDDL(String tableName, TableSchema tableSchema
   public static Configuration getDefaultConf(String tablePath) {
     Configuration conf = new Configuration();
     conf.setString(FlinkOptions.PATH, tablePath);
-    conf.setString(FlinkOptions.READ_AVRO_SCHEMA_PATH,
+    conf.setString(FlinkOptions.SOURCE_AVRO_SCHEMA_PATH,
         Objects.requireNonNull(Thread.currentThread()
             .getContextClassLoader().getResource("test_read_schema.avsc")).toString());
     conf.setString(FlinkOptions.TABLE_NAME, "TestHoodieTable");
@@ -155,7 +155,7 @@ public static Configuration getDefaultConf(String tablePath) {
   public static FlinkStreamerConfig getDefaultStreamerConf(String tablePath) {
     FlinkStreamerConfig streamerConf = new FlinkStreamerConfig();
     streamerConf.targetBasePath = tablePath;
-    streamerConf.readSchemaFilePath = Objects.requireNonNull(Thread.currentThread()
+    streamerConf.sourceAvroSchemaPath = Objects.requireNonNull(Thread.currentThread()
         .getContextClassLoader().getResource("test_read_schema.avsc")).toString();
     streamerConf.targetTableName = "TestHoodieTable";
     streamerConf.partitionPathField = "partition";

File: hudi-flink/src/main/java/org/apache/hudi/sink/compact/FlinkCompactionConfig.java
Patch:
@@ -86,8 +86,8 @@ public class FlinkCompactionConfig extends Configuration {
   @Parameter(names = {"--compaction-target-io"}, description = "Target IO per compaction (both read and write) for batching compaction, default 512000M.", required = false)
   public Long compactionTargetIo = 512000L;
 
-  @Parameter(names = {"--compaction-tasks"}, description = "Parallelism of tasks that do actual compaction, default is 10", required = false)
-  public Integer compactionTasks = 10;
+  @Parameter(names = {"--compaction-tasks"}, description = "Parallelism of tasks that do actual compaction, default is -1", required = false)
+  public Integer compactionTasks = -1;
 
   /**
    * Transforms a {@code HoodieFlinkCompaction.config} into {@code Configuration}.

File: hudi-flink/src/main/java/org/apache/hudi/sink/compact/HoodieFlinkCompactor.java
Patch:
@@ -112,7 +112,8 @@ public static void main(String[] args) throws Exception {
     }
 
     // get compactionParallelism.
-    int compactionParallelism = Math.min(conf.getInteger(FlinkOptions.COMPACTION_TASKS), compactionPlan.getOperations().size());
+    int compactionParallelism = conf.getInteger(FlinkOptions.COMPACTION_TASKS) == -1
+            ? compactionPlan.getOperations().size() : conf.getInteger(FlinkOptions.COMPACTION_TASKS);
 
     env.addSource(new CompactionPlanSourceFunction(table, instant, compactionPlan, compactionInstantTime))
         .name("compaction_source")

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -91,7 +91,7 @@ public class HoodieWriteConfig extends HoodieConfig {
   public static final ConfigProperty<String> KEYGENERATOR_TYPE_PROP = ConfigProperty
       .key("hoodie.datasource.write.keygenerator.type")
       .defaultValue(KeyGeneratorType.SIMPLE.name())
-      .withDocumentation("");
+      .withDocumentation("Type of build-in key generator, currently support SIMPLE, COMPLEX, TIMESTAMP, CUSTOM, NON_PARTITION, GLOBAL_DELETE");
 
   public static final ConfigProperty<String> ROLLBACK_USING_MARKERS = ConfigProperty
       .key("hoodie.rollback.using.markers")

File: hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -255,13 +255,13 @@ private FlinkOptions() {
           + "Actual value obtained by invoking .toString(), default ''");
 
   public static final ConfigOption<Boolean> URL_ENCODE_PARTITIONING = ConfigOptions
-      .key(KeyGeneratorOptions.URL_ENCODE_PARTITIONING_OPT_KEY)
+      .key(KeyGeneratorOptions.URL_ENCODE_PARTITIONING_OPT_KEY.key())
       .booleanType()
       .defaultValue(false)
       .withDescription("Whether to encode the partition path url, default false");
 
   public static final ConfigOption<Boolean> HIVE_STYLE_PARTITIONING = ConfigOptions
-      .key(KeyGeneratorOptions.HIVE_STYLE_PARTITIONING_OPT_KEY)
+      .key(KeyGeneratorOptions.HIVE_STYLE_PARTITIONING_OPT_KEY.key())
       .booleanType()
       .defaultValue(false)
       .withDescription("Whether to use Hive style partitioning.\n"

File: hudi-flink/src/main/java/org/apache/hudi/streamer/FlinkStreamerConfig.java
Patch:
@@ -162,7 +162,7 @@ public class FlinkStreamerConfig extends Configuration {
   public Boolean utcTimezone = true;
 
   @Parameter(names = {"--write-partition-url-encode"}, description = "Whether to encode the partition path url, default false")
-  public Boolean writePartitionUrlEncode;
+  public Boolean writePartitionUrlEncode = false;
 
   @Parameter(names = {"--hive-style-partitioning"}, description = "Whether to use Hive style partitioning.\n"
       + "If set true, the names of partition folders follow <partition_column_name>=<partition_value> format.\n"

File: hudi-flink/src/main/java/org/apache/hudi/sink/partitioner/profile/WriteProfiles.java
Patch:
@@ -157,10 +157,10 @@ public static Option<HoodieCommitMetadata> getCommitMetadataSafely(
       // make this fail safe.
       LOG.warn("Instant {} was deleted by the cleaner, ignore", instant.getTimestamp());
       return Option.empty();
-    } catch (IOException e) {
+    } catch (Throwable throwable) {
       LOG.error("Get write metadata for table {} with instant {} and path: {} error",
           tableName, instant.getTimestamp(), basePath);
-      throw new HoodieException(e);
+      return Option.empty();
     }
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/AvroKafkaSource.java
Patch:
@@ -63,11 +63,11 @@ public AvroKafkaSource(TypedProperties props, JavaSparkContext sparkContext, Spa
       props.put(NATIVE_KAFKA_VALUE_DESERIALIZER_PROP, KafkaAvroDeserializer.class);
     } else {
       try {
+        props.put(NATIVE_KAFKA_VALUE_DESERIALIZER_PROP, Class.forName(deserializerClassName));
         if (schemaProvider == null) {
           throw new HoodieIOException("SchemaProvider has to be set to use custom Deserializer");
         }
-        props.put(DataSourceWriteOptions.SCHEMA_PROVIDER_CLASS_PROP(), schemaProvider.getClass().getName());
-        props.put(NATIVE_KAFKA_VALUE_DESERIALIZER_PROP, Class.forName(deserializerClassName));
+        props.put(DataSourceWriteOptions.KAFKA_AVRO_VALUE_DESERIALIZER_SCHEMA(), schemaProvider.getSourceSchema().toString());
       } catch (ClassNotFoundException e) {
         String error = "Could not load custom avro kafka deserializer: " + deserializerClassName;
         LOG.error(error);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java
Patch:
@@ -176,7 +176,9 @@ public KafkaOffsetGen(TypedProperties props) {
     props.keySet().stream().filter(prop -> {
       // In order to prevent printing unnecessary warn logs, here filter out the hoodie
       // configuration items before passing to kafkaParams
-      return !prop.toString().startsWith("hoodie.");
+      return !prop.toString().startsWith("hoodie.")
+              // We need to pass some properties to kafka client so that KafkaAvroSchemaDeserializer can use it
+              || prop.toString().startsWith("hoodie.deltastreamer.source.kafka.value.deserializer.");
     }).forEach(prop -> {
       kafkaParams.put(prop.toString(), props.get(prop.toString()));
     });

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java
Patch:
@@ -127,7 +127,7 @@ public HoodieTimeline filterPendingReplaceTimeline() {
   @Override
   public HoodieTimeline filterPendingCompactionTimeline() {
     return new HoodieDefaultTimeline(
-        instants.stream().filter(s -> s.getAction().equals(HoodieTimeline.COMPACTION_ACTION)), details);
+        instants.stream().filter(s -> s.getAction().equals(HoodieTimeline.COMPACTION_ACTION) && !s.isCompleted()), details);
   }
 
   @Override

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/RollbackUtils.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.table.action.rollback;
 
 import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
@@ -88,7 +87,7 @@ static HoodieRollbackStat mergeRollbackStat(HoodieRollbackStat stat1, HoodieRoll
 
   /**
    * Generate all rollback requests that needs rolling back this action without actually performing rollback for COW table type.
-   * @param fs instance of {@link FileSystem} to use.
+   * @param engineContext instance of {@link HoodieEngineContext} to use.
    * @param basePath base path of interest.
    * @param config instance of {@link HoodieWriteConfig} to use.
    * @return {@link List} of {@link ListingBasedRollbackRequest}s thus collected.

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/RowKeyGeneratorHelper.java
Patch:
@@ -135,8 +135,8 @@ public static String getPartitionPathFromRow(Row row, List<String> partitionPath
    * 4 = "StructField(nested_col,StructType(StructField(prop1,StringType,false), StructField(prop2,LongType,false)),false)"
    *
    * the logic fetches the value from field nested_col.prop1.
-   * If any level of the nested field is null, {@link NULL_RECORDKEY_PLACEHOLDER} is returned.
-   * If the field value is an empty String, {@link EMPTY_RECORDKEY_PLACEHOLDER} is returned.
+   * If any level of the nested field is null, {@link KeyGenUtils#NULL_RECORDKEY_PLACEHOLDER} is returned.
+   * If the field value is an empty String, {@link KeyGenUtils#EMPTY_RECORDKEY_PLACEHOLDER} is returned.
    *
    * @param row instance of {@link Row} of interest
    * @param positions tree style positions where the leaf node need to be fetched and returned

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java
Patch:
@@ -63,7 +63,7 @@ class RealtimeUnmergedRecordReader extends AbstractRealtimeRecordReader
    * clients to consume.
    *
    * @param split File split
-   * @param jobConf Job Configuration
+   * @param job Job Configuration
    * @param realReader Parquet Reader
    */
   public RealtimeUnmergedRecordReader(RealtimeSplit split, JobConf job,

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestSqlSource.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.utilities.testutils.sources;
+package org.apache.hudi.utilities.sources;
 
 import org.apache.avro.generic.GenericRecord;
 import org.apache.hadoop.fs.Path;
@@ -26,8 +26,6 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.utilities.deltastreamer.SourceFormatAdapter;
 import org.apache.hudi.utilities.schema.FilebasedSchemaProvider;
-import org.apache.hudi.utilities.sources.InputBatch;
-import org.apache.hudi.utilities.sources.SqlSource;
 import org.apache.hudi.utilities.testutils.UtilitiesTestBase;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.sql.AnalysisException;

File: hudi-flink/src/test/java/org/apache/hudi/sink/TestWriteCopyOnWrite.java
Patch:
@@ -651,7 +651,6 @@ public void testIndexStateBootstrap() throws Exception {
   @Test
   public void testWriteExactlyOnce() throws Exception {
     // reset the config option
-    conf.setBoolean(FlinkOptions.WRITE_EXACTLY_ONCE_ENABLED, true);
     conf.setLong(FlinkOptions.WRITE_COMMIT_ACK_TIMEOUT, 3);
     conf.setDouble(FlinkOptions.WRITE_TASK_MAX_SIZE, 200.0006); // 630 bytes buffer size
     funcWrapper = new StreamWriteFunctionWrapper<>(tempFile.getAbsolutePath(), conf);

File: hudi-flink/src/main/java/org/apache/hudi/sink/compact/CompactionCommitSink.java
Patch:
@@ -122,8 +122,7 @@ private void commitIfNecessary(String instant, List<CompactionCommitEvent> event
 
     // Whether to cleanup the old log file when compaction
     if (!conf.getBoolean(FlinkOptions.CLEAN_ASYNC_ENABLED)) {
-      this.writeClient.startAsyncCleaning();
-      this.writeClient.waitForCleaningFinish();
+      this.writeClient.clean();
     }
 
     // reset the status

File: hudi-flink/src/main/java/org/apache/hudi/sink/compact/CompactionPlanSourceFunction.java
Patch:
@@ -46,7 +46,8 @@
  *   use {@link org.apache.hudi.common.table.timeline.HoodieActiveTimeline#createNewInstantTime()}
  *   as the instant time;</li>
  *   <li>If the timeline has inflight instants,
- *   use the {earliest inflight instant time - 1ms} as the instant time.</li>
+ *   use the median instant time between [last complete instant time, earliest inflight instant time]
+ *   as the instant time.</li>
  * </ul>
  */
 public class CompactionPlanSourceFunction extends AbstractRichFunction implements SourceFunction<CompactionPlanEvent> {

File: hudi-flink/src/main/java/org/apache/hudi/sink/compact/FlinkCompactionConfig.java
Patch:
@@ -101,6 +101,8 @@ public static org.apache.flink.configuration.Configuration toFlinkConfig(FlinkCo
     conf.setInteger(FlinkOptions.COMPACTION_DELTA_SECONDS, config.compactionDeltaSeconds);
     conf.setInteger(FlinkOptions.COMPACTION_MAX_MEMORY, config.compactionMaxMemory);
     conf.setBoolean(FlinkOptions.CLEAN_ASYNC_ENABLED, config.cleanAsyncEnable);
+    // use synchronous compaction always
+    conf.setBoolean(FlinkOptions.COMPACTION_ASYNC_ENABLED, false);
 
     return conf;
   }

File: hudi-flink/src/main/java/org/apache/hudi/streamer/HoodieFlinkStreamer.java
Patch:
@@ -117,16 +117,16 @@ public static void main(String[] args) throws Exception {
         .transform("hoodie_stream_write", TypeInformation.of(Object.class), operatorFactory)
         .uid("uid_hoodie_stream_write")
         .setParallelism(numWriteTask);
-    if (StreamerUtil.needsScheduleCompaction(conf)) {
+    if (StreamerUtil.needsAsyncCompaction(conf)) {
       pipeline.transform("compact_plan_generate",
               TypeInformation.of(CompactionPlanEvent.class),
               new CompactionPlanOperator(conf))
               .uid("uid_compact_plan_generate")
               .setParallelism(1) // plan generate must be singleton
-              .keyBy(event -> event.getOperation().hashCode())
+              .rebalance()
               .transform("compact_task",
                       TypeInformation.of(CompactionCommitEvent.class),
-                      new KeyedProcessOperator<>(new CompactFunction(conf)))
+                      new ProcessOperator<>(new CompactFunction(conf)))
               .setParallelism(conf.getInteger(FlinkOptions.COMPACTION_TASKS))
               .addSink(new CompactionCommitSink(conf))
               .name("compact_commit")

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java
Patch:
@@ -188,7 +188,8 @@ public static void createMORTable(String commitTime, String deltaCommitTime, int
     DateTime dateTime = DateTime.now();
     HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,
         useSchemaFromCommitMetadata, dateTime, commitTime);
-    createdTablesSet.add(hiveSyncConfig.databaseName + "." + hiveSyncConfig.tableName);
+    createdTablesSet
+      .add(hiveSyncConfig.databaseName + "." + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);
     createdTablesSet
         .add(hiveSyncConfig.databaseName + "." + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);
     HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/BaseScheduleCompactionActionExecutor.java
Patch:
@@ -63,7 +63,7 @@ public Option<HoodieCompactionPlan> execute() {
                   + ", Compaction scheduled at " + instantTime));
       // Committed and pending compaction instants should have strictly lower timestamps
       List<HoodieInstant> conflictingInstants = table.getActiveTimeline()
-          .getWriteTimeline().getInstants()
+          .getWriteTimeline().filterCompletedAndCompactionInstants().getInstants()
           .filter(instant -> HoodieTimeline.compareTimestamps(
               instant.getTimestamp(), HoodieTimeline.GREATER_THAN_OR_EQUALS, instantTime))
           .collect(Collectors.toList());

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java
Patch:
@@ -144,7 +144,7 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {
       // Get schema.
       String schemaStr = UtilHelpers.parseSchema(fs, cfg.schemaFile);
 
-      SparkRDDWriteClient client =
+      SparkRDDWriteClient<HoodieRecordPayload> client =
           UtilHelpers.createHoodieClient(jsc, cfg.targetPath, schemaStr, cfg.parallelism, Option.empty(), props);
 
       JavaRDD<HoodieRecord<HoodieRecordPayload>> hoodieRecords = buildHoodieRecordsForImport(jsc, schemaStr);
@@ -206,7 +206,7 @@ protected JavaRDD<HoodieRecord<HoodieRecordPayload>> buildHoodieRecordsForImport
    * @param hoodieRecords Hoodie Records
    * @param <T> Type
    */
-  protected <T extends HoodieRecordPayload> JavaRDD<WriteStatus> load(SparkRDDWriteClient client, String instantTime,
+  protected <T extends HoodieRecordPayload> JavaRDD<WriteStatus> load(SparkRDDWriteClient<T> client, String instantTime,
                                                                       JavaRDD<HoodieRecord<T>> hoodieRecords) {
     switch (cfg.command.toLowerCase()) {
       case "upsert": {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.Option;
 
 import com.beust.jcommander.JCommander;
@@ -131,9 +132,9 @@ public int compact(int retry) {
   private int doCompact(JavaSparkContext jsc) throws Exception {
     // Get schema.
     String schemaStr = UtilHelpers.parseSchema(fs, cfg.schemaFile);
-    SparkRDDWriteClient client =
+    SparkRDDWriteClient<HoodieRecordPayload> client =
         UtilHelpers.createHoodieClient(jsc, cfg.basePath, schemaStr, cfg.parallelism, Option.empty(), props);
-    JavaRDD<WriteStatus> writeResponse = (JavaRDD<WriteStatus>) client.compact(cfg.compactionInstantTime);
+    JavaRDD<WriteStatus> writeResponse = client.compact(cfg.compactionInstantTime);
     return UtilHelpers.handleErrors(jsc, cfg.compactionInstantTime, writeResponse);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java
Patch:
@@ -92,7 +92,9 @@ public HoodieBackedTableMetadata(HoodieEngineContext engineContext, HoodieMetada
   }
 
   private void initIfNeeded() {
-    if (enabled && this.metaClient == null) {
+    if (!enabled) {
+      LOG.info("Metadata table is disabled for " + datasetBasePath);
+    } else if (this.metaClient == null) {
       this.metadataBasePath = HoodieTableMetadata.getMetadataTableBasePath(datasetBasePath);
       try {
         this.metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf.get()).setBasePath(metadataBasePath).build();
@@ -107,8 +109,6 @@ private void initIfNeeded() {
         this.enabled = false;
         this.metaClient = null;
       }
-    } else {
-      LOG.info("Metadata table is disabled.");
     }
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HiveIncrementalPuller.java
Patch:
@@ -180,13 +180,13 @@ private void executeIncrementalSQL(String tempDbTable, String tempDbTablePath, S
     incrementalPullSQLtemplate.add("storedAsClause", storedAsClause);
     String incrementalSQL = new Scanner(new File(config.incrementalSQLFile)).useDelimiter("\\Z").next();
     if (!incrementalSQL.contains(config.sourceDb + "." + config.sourceTable)) {
-      LOG.info("Incremental SQL does not have " + config.sourceDb + "." + config.sourceTable
+      LOG.error("Incremental SQL does not have " + config.sourceDb + "." + config.sourceTable
           + ", which means its pulling from a different table. Fencing this from happening.");
       throw new HoodieIncrementalPullSQLException(
           "Incremental SQL does not have " + config.sourceDb + "." + config.sourceTable);
     }
     if (!incrementalSQL.contains("`_hoodie_commit_time` > '%targetBasePath'")) {
-      LOG.info("Incremental SQL : " + incrementalSQL
+      LOG.error("Incremental SQL : " + incrementalSQL
           + " does not contain `_hoodie_commit_time` > '%targetBasePath'. Please add "
           + "this clause for incremental to work properly.");
       throw new HoodieIncrementalPullSQLException(

File: hudi-common/src/main/java/org/apache/hudi/common/model/OverwriteWithLatestAvroPayload.java
Patch:
@@ -94,6 +94,6 @@ protected boolean isDeleteRecord(GenericRecord genericRecord) {
    * Return true if value equals defaultValue otherwise false.
    */
   public Boolean overwriteField(Object value, Object defaultValue) {
-    return defaultValue == null ? value == null : defaultValue.toString().equals(value.toString());
+    return defaultValue == null ? value == null : defaultValue.toString().equals(String.valueOf(value));
   }
 }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteJob.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.integ.testsuite;
 
 import org.apache.avro.Schema;
-import org.apache.hudi.DataSourceUtils;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
@@ -43,6 +42,7 @@
 import org.apache.hudi.integ.testsuite.reader.DeltaInputType;
 import org.apache.hudi.integ.testsuite.writer.DeltaOutputMode;
 import org.apache.hudi.keygen.BuiltinKeyGenerator;
+import org.apache.hudi.keygen.factory.HoodieSparkKeyGeneratorFactory;
 import org.apache.hudi.utilities.UtilHelpers;
 import org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer;
 
@@ -106,7 +106,7 @@ public HoodieTestSuiteJob(HoodieTestSuiteConfig cfg, JavaSparkContext jsc) throw
     this.props = UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();
     log.info("Creating workload generator with configs : {}", props.toString());
     this.hiveConf = getDefaultHiveConf(jsc.hadoopConfiguration());
-    this.keyGenerator = (BuiltinKeyGenerator) DataSourceUtils.createKeyGenerator(props);
+    this.keyGenerator = (BuiltinKeyGenerator) HoodieSparkKeyGeneratorFactory.createKeyGenerator(props);
 
     metaClient = HoodieTableMetaClient.withPropertyBuilder()
         .setTableType(cfg.tableType)

File: hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/bootstrap/SparkParquetBootstrapDataProvider.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.hudi.keygen.KeyGenerator;
 
 import org.apache.avro.generic.GenericRecord;
+import org.apache.hudi.keygen.factory.HoodieSparkKeyGeneratorFactory;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.rdd.RDD;
 import org.apache.spark.sql.Dataset;
@@ -62,7 +63,7 @@ public JavaRDD<HoodieRecord> generateInputRecords(String tableName, String sourc
 
     Dataset inputDataset = sparkSession.read().parquet(filePaths);
     try {
-      KeyGenerator keyGenerator = DataSourceUtils.createKeyGenerator(props);
+      KeyGenerator keyGenerator = HoodieSparkKeyGeneratorFactory.createKeyGenerator(props);
       String structName = tableName + "_record";
       String namespace = "hoodie." + tableName;
       RDD<GenericRecord> genericRecords = HoodieSparkUtils.createRdd(inputDataset, structName, namespace);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -47,6 +47,7 @@
 import org.apache.hudi.hive.HiveSyncConfig;
 import org.apache.hudi.hive.HiveSyncTool;
 import org.apache.hudi.keygen.KeyGenerator;
+import org.apache.hudi.keygen.factory.HoodieSparkKeyGeneratorFactory;
 import org.apache.hudi.sync.common.AbstractSyncTool;
 import org.apache.hudi.utilities.UtilHelpers;
 import org.apache.hudi.exception.HoodieDeltaStreamerException;
@@ -208,7 +209,7 @@ public DeltaSync(HoodieDeltaStreamer.Config cfg, SparkSession sparkSession, Sche
     registerAvroSchemas(schemaProvider);
 
     this.transformer = UtilHelpers.createTransformer(cfg.transformerClassNames);
-    this.keyGenerator = DataSourceUtils.createKeyGenerator(props);
+    this.keyGenerator = HoodieSparkKeyGeneratorFactory.createKeyGenerator(props);
 
     this.metrics = new HoodieDeltaStreamerMetrics(getHoodieClientConfig(this.schemaProvider));
 

File: hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hudi.common.model.HoodieRecord;
+import org.apache.hudi.common.testutils.NetworkTestUtils;
 import org.apache.hudi.common.testutils.SchemaTestUtil;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.hive.util.ConfigUtils;
@@ -245,7 +246,6 @@ public void testBasicSync(boolean useJdbc, boolean useSchemaFromCommitMetadata)
     hiveClient = new HoodieHiveClient(HiveTestUtil.hiveSyncConfig, HiveTestUtil.getHiveConf(), HiveTestUtil.fileSystem);
     List<Partition> hivePartitions = hiveClient.scanTablePartitions(HiveTestUtil.hiveSyncConfig.tableName);
     List<String> writtenPartitionsSince = hiveClient.getPartitionsWrittenToSince(Option.empty());
-    writtenPartitionsSince.add(newPartition.get(0));
     List<PartitionEvent> partitionEvents = hiveClient.getPartitionEvents(hivePartitions, writtenPartitionsSince);
     assertEquals(1, partitionEvents.size(), "There should be only one partition event");
     assertEquals(PartitionEventType.UPDATE, partitionEvents.iterator().next().eventType,
@@ -687,7 +687,8 @@ public void testConnectExceptionIgnoreConfigSet() throws IOException, URISyntaxE
 
     HiveSyncConfig syncToolConfig = HiveSyncConfig.copy(HiveTestUtil.hiveSyncConfig);
     syncToolConfig.ignoreExceptions = true;
-    syncToolConfig.jdbcUrl = HiveTestUtil.hiveSyncConfig.jdbcUrl.replace("9999","9031");
+    syncToolConfig.jdbcUrl = HiveTestUtil.hiveSyncConfig.jdbcUrl
+        .replace(String.valueOf(HiveTestUtil.hiveTestService.getHiveServerPort()), String.valueOf(NetworkTestUtils.nextFreePort()));
     HiveSyncTool tool = new HiveSyncTool(syncToolConfig, HiveTestUtil.getHiveConf(), HiveTestUtil.fileSystem);
     tool.syncHoodieTable();
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java
Patch:
@@ -65,7 +65,7 @@ public static InputSplit[] getRealtimeSplits(Configuration conf, Stream<FileSpli
     Map<Path, List<FileSplit>> partitionsToParquetSplits =
         fileSplits.collect(Collectors.groupingBy(split -> split.getPath().getParent()));
     // TODO(vc): Should we handle also non-hoodie splits here?
-    Map<Path, HoodieTableMetaClient> partitionsToMetaClient = getTableMetaClientByBasePath(conf, partitionsToParquetSplits.keySet());
+    Map<Path, HoodieTableMetaClient> partitionsToMetaClient = getTableMetaClientByPartitionPath(conf, partitionsToParquetSplits.keySet());
 
     // Create file system cache so metadata table is only instantiated once. Also can benefit normal file listing if
     // partition path is listed twice so file groups will already be loaded in file system
@@ -141,7 +141,7 @@ public static Map<HoodieBaseFile, List<String>> groupLogsByBaseFile(Configuratio
     Map<Path, List<HoodieBaseFile>> partitionsToParquetSplits =
         fileStatuses.stream().collect(Collectors.groupingBy(file -> file.getFileStatus().getPath().getParent()));
     // TODO(vc): Should we handle also non-hoodie splits here?
-    Map<Path, HoodieTableMetaClient> partitionsToMetaClient = getTableMetaClientByBasePath(conf, partitionsToParquetSplits.keySet());
+    Map<Path, HoodieTableMetaClient> partitionsToMetaClient = getTableMetaClientByPartitionPath(conf, partitionsToParquetSplits.keySet());
 
     // for all unique split parents, obtain all delta files based on delta commit timeline,
     // grouped on file id

File: hudi-common/src/main/java/org/apache/hudi/common/model/ActionType.java
Patch:
@@ -22,6 +22,5 @@
  * The supported action types.
  */
 public enum ActionType {
-  //TODO HUDI-1281 make deltacommit part of this
-  commit, savepoint, compaction, clean, rollback, replacecommit
+  commit, savepoint, compaction, clean, rollback, replacecommit, deltacommit
 }

File: hudi-flink/src/main/java/org/apache/hudi/streamer/HoodieFlinkStreamer.java
Patch:
@@ -103,7 +103,7 @@ public static void main(String[] args) throws Exception {
         .uid("uid_bucket_assigner")
         // shuffle by fileId(bucket id)
         .keyBy(record -> record.getCurrentLocation().getFileId())
-        .transform("hoodie_stream_write", null, operatorFactory)
+        .transform("hoodie_stream_write", TypeInformation.of(Object.class), operatorFactory)
         .uid("uid_hoodie_stream_write")
         .setParallelism(numWriteTask);
     if (StreamerUtil.needsScheduleCompaction(conf)) {

File: hudi-flink/src/test/java/org/apache/hudi/sink/StreamWriteITCase.java
Patch:
@@ -125,7 +125,7 @@ public void testWriteToHoodie() throws Exception {
         .uid("uid_bucket_assigner")
         // shuffle by fileId(bucket id)
         .keyBy(record -> record.getCurrentLocation().getFileId())
-        .transform("hoodie_stream_write", null, operatorFactory)
+        .transform("hoodie_stream_write", TypeInformation.of(Object.class), operatorFactory)
         .uid("uid_hoodie_stream_write");
     execEnv.addOperator(dataStream.getTransformation());
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -312,7 +312,7 @@ public synchronized HoodieArchivedTimeline getArchivedTimeline() {
   }
 
   /**
-   * Helper method to initialize a given path as a hoodie table with configs passed in as as Properties.
+   * Helper method to initialize a given path as a hoodie table with configs passed in as Properties.
    *
    * @return Instance of HoodieTableMetaClient
    */

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.cli.testutils.HoodieTestCommitMetadataGenerator;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieTableType;
+import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
@@ -66,7 +67,7 @@ public void init() throws IOException {
     // Create table and connect
     new TableCommand().createTable(
         tablePath, tableName, HoodieTableType.COPY_ON_WRITE.name(),
-        "", TimelineLayoutVersion.VERSION_1, "org.apache.hudi.common.model.HoodieAvroPayload");
+        HoodieTableConfig.DEFAULT_ARCHIVELOG_FOLDER, TimelineLayoutVersion.VERSION_1, "org.apache.hudi.common.model.HoodieAvroPayload");
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -67,6 +67,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
+import static org.apache.hudi.common.table.HoodieTableConfig.DEFAULT_ARCHIVELOG_FOLDER;
 import static org.apache.hudi.metadata.HoodieTableMetadata.METADATA_TABLE_NAME_SUFFIX;
 import static org.apache.hudi.metadata.HoodieTableMetadata.SOLO_COMMIT_TIMESTAMP;
 
@@ -294,7 +295,7 @@ private void bootstrapFromFilesystem(HoodieEngineContext engineContext, HoodieTa
     HoodieTableMetaClient.withPropertyBuilder()
       .setTableType(HoodieTableType.MERGE_ON_READ)
       .setTableName(tableName)
-      .setArchiveLogFolder("archived")
+      .setArchiveLogFolder(DEFAULT_ARCHIVELOG_FOLDER)
       .setPayloadClassName(HoodieMetadataPayload.class.getName())
       .setBaseFileFormat(HoodieFileFormat.HFILE.toString())
       .initTable(hadoopConf.get(), metadataWriteConfig.getBasePath());

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -81,7 +81,7 @@ public class HoodieTableConfig implements Serializable {
   public static final String DEFAULT_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();
   public static final String NO_OP_BOOTSTRAP_INDEX_CLASS = NoOpBootstrapIndex.class.getName();
   public static final String DEFAULT_BOOTSTRAP_INDEX_CLASS = HFileBootstrapIndex.class.getName();
-  public static final String DEFAULT_ARCHIVELOG_FOLDER = "";
+  public static final String DEFAULT_ARCHIVELOG_FOLDER = "archived";
 
   private Properties props;
 

File: hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java
Patch:
@@ -68,11 +68,12 @@
 import java.util.Objects;
 import java.util.Properties;
 
+import static org.apache.hudi.common.table.HoodieTableConfig.DEFAULT_ARCHIVELOG_FOLDER;
+
 /**
  * Utilities for Flink stream read and write.
  */
 public class StreamerUtil {
-  private static final String DEFAULT_ARCHIVE_LOG_FOLDER = "archived";
 
   private static final Logger LOG = LoggerFactory.getLogger(StreamerUtil.class);
 
@@ -266,7 +267,7 @@ public static void initTableIfNotExists(Configuration conf) throws IOException {
           .setTableType(conf.getString(FlinkOptions.TABLE_TYPE))
           .setTableName(conf.getString(FlinkOptions.TABLE_NAME))
           .setPayloadClassName(conf.getString(FlinkOptions.PAYLOAD_CLASS))
-          .setArchiveLogFolder(DEFAULT_ARCHIVE_LOG_FOLDER)
+          .setArchiveLogFolder(DEFAULT_ARCHIVELOG_FOLDER)
           .setTimelineLayoutVersion(1)
           .initTable(hadoopConf, basePath);
       LOG.info("Table initialized under base path {}", basePath);

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteJob.java
Patch:
@@ -61,6 +61,8 @@
 import java.util.List;
 import java.util.Map;
 
+import static org.apache.hudi.common.table.HoodieTableConfig.DEFAULT_ARCHIVELOG_FOLDER;
+
 /**
  * This is the entry point for running a Hudi Test Suite. Although this class has similarities with {@link HoodieDeltaStreamer} this class does not extend it since do not want to create a dependency
  * on the changes in DeltaStreamer.
@@ -109,7 +111,7 @@ public HoodieTestSuiteJob(HoodieTestSuiteConfig cfg, JavaSparkContext jsc) throw
     metaClient = HoodieTableMetaClient.withPropertyBuilder()
         .setTableType(cfg.tableType)
         .setTableName(cfg.targetTableName)
-        .setArchiveLogFolder("archived")
+        .setArchiveLogFolder(DEFAULT_ARCHIVELOG_FOLDER)
         .initTable(jsc.hadoopConfiguration(), cfg.targetBasePath);
 
     if (cfg.cleanInput) {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/BootstrapExecutor.java
Patch:
@@ -49,6 +49,8 @@
 import java.io.Serializable;
 import java.util.HashMap;
 
+import static org.apache.hudi.common.table.HoodieTableConfig.DEFAULT_ARCHIVELOG_FOLDER;
+
 /**
  * Performs bootstrap from a non-hudi source.
  */
@@ -172,7 +174,7 @@ private void initializeTable() throws IOException {
     HoodieTableMetaClient.withPropertyBuilder()
         .setTableType(cfg.tableType)
         .setTableName(cfg.targetTableName)
-        .setArchiveLogFolder("archived")
+        .setArchiveLogFolder(DEFAULT_ARCHIVELOG_FOLDER)
         .setPayloadClassName(cfg.payloadClassName)
         .setBaseFileFormat(cfg.baseFileFormat)
         .setBootstrapIndexClass(cfg.bootstrapIndexClass)

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -89,6 +89,7 @@
 
 import scala.collection.JavaConversions;
 
+import static org.apache.hudi.common.table.HoodieTableConfig.DEFAULT_ARCHIVELOG_FOLDER;
 import static org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.CHECKPOINT_KEY;
 import static org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.CHECKPOINT_RESET_KEY;
 import static org.apache.hudi.config.HoodieCompactionConfig.INLINE_COMPACT_PROP;
@@ -241,7 +242,7 @@ public void refreshTimeline() throws IOException {
       HoodieTableMetaClient.withPropertyBuilder()
           .setTableType(cfg.tableType)
           .setTableName(cfg.targetTableName)
-          .setArchiveLogFolder("archived")
+          .setArchiveLogFolder(DEFAULT_ARCHIVELOG_FOLDER)
           .setPayloadClassName(cfg.payloadClassName)
           .setBaseFileFormat(cfg.baseFileFormat)
           .setPartitionColumns(partitionColumns)
@@ -334,7 +335,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource(
       HoodieTableMetaClient.withPropertyBuilder()
           .setTableType(cfg.tableType)
           .setTableName(cfg.targetTableName)
-          .setArchiveLogFolder("archived")
+          .setArchiveLogFolder(DEFAULT_ARCHIVELOG_FOLDER)
           .setPayloadClassName(cfg.payloadClassName)
           .setBaseFileFormat(cfg.baseFileFormat)
           .setPartitionColumns(partitionColumns)

File: hudi-common/src/main/java/org/apache/hudi/common/engine/EngineType.java
Patch:
@@ -19,7 +19,9 @@
 package org.apache.hudi.common.engine;
 
 /**
- * Hoodie data processing engine. support only Apache Spark and Apache Flink for now.
+ * Hoodie data processing engine.
+ * <p>
+ * Support only Apache Spark, Apache Flink and Java for now.
  */
 public enum EngineType {
   SPARK, FLINK, JAVA

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCommitsCommand.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.cli.TableHeader;
 import org.apache.hudi.cli.testutils.AbstractShellIntegrationTest;
 import org.apache.hudi.cli.testutils.HoodieTestCommitMetadataGenerator;
-import org.apache.hudi.cli.testutils.HoodieTestReplaceCommitMetadatGenerator;
+import org.apache.hudi.cli.testutils.HoodieTestReplaceCommitMetadataGenerator;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -122,7 +122,7 @@ private LinkedHashMap<HoodieInstant, Integer[]> generateMixedData() throws Excep
     for (Map.Entry<HoodieInstant, Integer[]> entry : replaceCommitData.entrySet()) {
       String key = entry.getKey().getTimestamp();
       Integer[] value = entry.getValue();
-      HoodieTestReplaceCommitMetadatGenerator.createReplaceCommitFileWithMetadata(tablePath, key,
+      HoodieTestReplaceCommitMetadataGenerator.createReplaceCommitFileWithMetadata(tablePath, key,
               Option.of(value[0]), Option.of(value[1]), metaClient);
     }
 

File: hudi-cli/src/test/java/org/apache/hudi/cli/testutils/HoodieTestReplaceCommitMetadataGenerator.java
Patch:
@@ -35,15 +35,15 @@
 import static org.apache.hudi.common.testutils.FileCreateUtils.baseFileName;
 import static org.apache.hudi.common.util.CollectionUtils.createImmutableList;
 
-public class HoodieTestReplaceCommitMetadatGenerator extends HoodieTestCommitMetadataGenerator {
+public class HoodieTestReplaceCommitMetadataGenerator extends HoodieTestCommitMetadataGenerator {
   public static void createReplaceCommitFileWithMetadata(String basePath, String commitTime, Option<Integer> writes, Option<Integer> updates,
                                                          HoodieTableMetaClient metaclient) throws Exception {
 
     HoodieReplaceCommitMetadata replaceMetadata = generateReplaceCommitMetadata(basePath, commitTime, UUID.randomUUID().toString(),
         UUID.randomUUID().toString(), writes, updates);
     HoodieRequestedReplaceMetadata requestedReplaceMetadata = getHoodieRequestedReplaceMetadata();
 
-    HoodieTestTable.of(metaclient).addReplaceCommit(commitTime, requestedReplaceMetadata, replaceMetadata);
+    HoodieTestTable.of(metaclient).addReplaceCommit(commitTime, Option.ofNullable(requestedReplaceMetadata), Option.empty(), replaceMetadata);
   }
 
   private static HoodieRequestedReplaceMetadata getHoodieRequestedReplaceMetadata() {

File: hudi-client/hudi-client-common/src/test/java/org/apache/hudi/client/transaction/TestSimpleConcurrentFileWritesConflictResolutionStrategy.java
Patch:
@@ -383,7 +383,7 @@ private void createReplaceRequested(String instantTime) throws Exception {
     requestedReplaceMetadata.setClusteringPlan(clusteringPlan);
     requestedReplaceMetadata.setVersion(TimelineLayoutVersion.CURR_VERSION);
     HoodieTestTable.of(metaClient)
-        .addRequestedReplace(instantTime, requestedReplaceMetadata)
+        .addRequestedReplace(instantTime, Option.of(requestedReplaceMetadata))
         .withBaseFilesInPartition(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, fileId1, fileId2);
   }
 
@@ -413,7 +413,7 @@ private void createReplace(String instantTime, WriteOperationType writeOperation
     requestedReplaceMetadata.setClusteringPlan(clusteringPlan);
     requestedReplaceMetadata.setVersion(TimelineLayoutVersion.CURR_VERSION);
     HoodieTestTable.of(metaClient)
-        .addReplaceCommit(instantTime, requestedReplaceMetadata, replaceMetadata)
+        .addReplaceCommit(instantTime, Option.of(requestedReplaceMetadata), Option.empty(), replaceMetadata)
         .withBaseFilesInPartition(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, fileId1, fileId2);
   }
 

File: hudi-flink/src/main/java/org/apache/hudi/streamer/HoodieFlinkStreamer.java
Patch:
@@ -88,8 +88,8 @@ public static void main(String[] args) throws Exception {
         .name("kafka_source")
         .uid("uid_kafka_source")
         .map(new RowDataToHoodieFunction<>(rowType, conf), TypeInformation.of(HoodieRecord.class))
-        // Key-by partition path, to avoid multiple subtasks write to a partition at the same time
-        .keyBy(HoodieRecord::getPartitionPath)
+        // Key-by record key, to avoid multiple subtasks write to a partition at the same time
+        .keyBy(HoodieRecord::getRecordKey)
         .transform(
             "bucket_assigner",
             TypeInformation.of(HoodieRecord.class),

File: hudi-flink/src/main/java/org/apache/hudi/configuration/FlinkOptions.java
Patch:
@@ -91,7 +91,7 @@ private FlinkOptions() {
       .booleanType()
       .defaultValue(false)
       .withDescription("Whether to update index for the old partition path\n"
-          + "if same key record with different partition path came in, default true");
+          + "if same key record with different partition path came in, default false");
 
   // ------------------------------------------------------------------------
   //  Read Options

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java
Patch:
@@ -85,12 +85,12 @@ void addProjectionToJobConf(final RealtimeSplit realtimeSplit, final JobConf job
     // risk of experiencing race conditions. Hence, we synchronize on the JobConf object here. There is negligible
     // latency incurred here due to the synchronization since get record reader is called once per spilt before the
     // actual heavy lifting of reading the parquet files happen.
-    if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {
+    if (HoodieRealtimeInputFormatUtils.canAddProjectionToJobConf(realtimeSplit, jobConf)) {
       synchronized (jobConf) {
         LOG.info(
             "Before adding Hoodie columns, Projections :" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)
                 + ", Ids :" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));
-        if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {
+        if (HoodieRealtimeInputFormatUtils.canAddProjectionToJobConf(realtimeSplit, jobConf)) {
           // Hive (across all versions) fails for queries like select count(`_hoodie_commit_time`) from table;
           // In this case, the projection fields gets removed. Looking at HiveInputFormat implementation, in some cases
           // hoodie additional projection columns are reset after calling setConf and only natural projections

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -43,7 +43,6 @@
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicReference;
 import java.util.function.Function;
-import java.util.stream.Collectors;
 
 /**
  * Represents the Active Timeline for the Hoodie table. Instants for the last 12 hours (configurable) is in the
@@ -113,7 +112,7 @@ protected HoodieActiveTimeline(HoodieTableMetaClient metaClient, Set<String> inc
     // multiple casts will make this lambda serializable -
     // http://docs.oracle.com/javase/specs/jls/se8/html/jls-15.html#jls-15.16
     this.details = (Function<HoodieInstant, Option<byte[]>> & Serializable) this::getInstantDetails;
-    LOG.info("Loaded instants " + getInstants().collect(Collectors.toList()));
+    LOG.info("Loaded instants upto : " + lastInstant());
   }
 
   public HoodieActiveTimeline(HoodieTableMetaClient metaClient) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -279,7 +279,7 @@ private void ensurePartitionLoadedCorrectly(String partition) {
           long beginLsTs = System.currentTimeMillis();
           FileStatus[] statuses = listPartition(partitionPath);
           long endLsTs = System.currentTimeMillis();
-          LOG.info("#files found in partition (" + partitionPathStr + ") =" + statuses.length + ", Time taken ="
+          LOG.debug("#files found in partition (" + partitionPathStr + ") =" + statuses.length + ", Time taken ="
               + (endLsTs - beginLsTs));
           List<HoodieFileGroup> groups = addFilesToView(statuses);
 
@@ -293,7 +293,7 @@ private void ensurePartitionLoadedCorrectly(String partition) {
         LOG.debug("View already built for Partition :" + partitionPathStr + ", FOUND is ");
       }
       long endTs = System.currentTimeMillis();
-      LOG.info("Time to load partition (" + partitionPathStr + ") =" + (endTs - beginTs));
+      LOG.debug("Time to load partition (" + partitionPathStr + ") =" + (endTs - beginTs));
       return true;
     });
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java
Patch:
@@ -317,7 +317,7 @@ protected boolean isPartitionAvailableInStore(String partitionPath) {
 
   @Override
   protected void storePartitionView(String partitionPath, List<HoodieFileGroup> fileGroups) {
-    LOG.info("Adding file-groups for partition :" + partitionPath + ", #FileGroups=" + fileGroups.size());
+    LOG.debug("Adding file-groups for partition :" + partitionPath + ", #FileGroups=" + fileGroups.size());
     List<HoodieFileGroup> newList = new ArrayList<>(fileGroups);
     partitionToFileGroupsMap.put(partitionPath, newList);
   }

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/RequestHandler.java
Patch:
@@ -136,8 +136,8 @@ private boolean syncIfLocalViewBehind(Context ctx) {
         if (isLocalViewBehind(ctx)) {
           HoodieTimeline localTimeline = viewManager.getFileSystemView(basePath).getTimeline();
           LOG.info("Syncing view as client passed last known instant " + lastKnownInstantFromClient
-              + " as last known instant but server has the folling timeline :"
-              + localTimeline.getInstants().collect(Collectors.toList()));
+              + " as last known instant but server has the following last instant on timeline :"
+              + localTimeline.lastInstant());
           view.sync();
           return true;
         }
@@ -457,7 +457,7 @@ public void handle(@NotNull Context context) throws Exception {
         metricsRegistry.add("TOTAL_CHECK_TIME", finalCheckTimeTaken);
         metricsRegistry.add("TOTAL_API_CALLS", 1);
 
-        LOG.info(String.format(
+        LOG.debug(String.format(
                 "TimeTakenMillis[Total=%d, Refresh=%d, handle=%d, Check=%d], "
                     + "Success=%s, Query=%s, Host=%s, synced=%s",
                 timeTakenMillis, refreshCheckTimeTaken, handleTimeTaken, finalCheckTimeTaken, success,

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -694,7 +694,9 @@ private void registerAvroSchemas(Schema sourceSchema, Schema targetSchema) {
         schemas.add(targetSchema);
       }
 
-      LOG.info("Registering Schema :" + schemas);
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Registering Schema: " + schemas);
+      }
       jssc.sc().getConf().registerAvroSchemas(JavaConversions.asScalaBuffer(schemas).toList());
     }
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/transform/SqlQueryBasedTransformer.java
Patch:
@@ -62,7 +62,7 @@ public Dataset<Row> apply(JavaSparkContext jsc, SparkSession sparkSession, Datas
     LOG.info("Registering tmp table : " + tmpTable);
     rowDataset.registerTempTable(tmpTable);
     String sqlStr = transformerSQL.replaceAll(SRC_PATTERN, tmpTable);
-    LOG.info("SQL Query for transformation : (" + sqlStr + ")");
+    LOG.debug("SQL Query for transformation : (" + sqlStr + ")");
     return sparkSession.sql(sqlStr);
   }
 }

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java
Patch:
@@ -239,7 +239,7 @@ public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsTo
            * those partitions.
            */
           for (Path path : inputPaths) {
-            if (path.toString().contains(s)) {
+            if (path.toString().endsWith(s)) {
               return true;
             }
           }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java
Patch:
@@ -267,7 +267,7 @@ protected static void populateCommonProps(TypedProperties props) {
   protected static void populateCommonKafkaProps(TypedProperties props) {
     //Kafka source properties
     props.setProperty("bootstrap.servers", testUtils.brokerAddress());
-    props.setProperty(Config.KAFKA_AUTO_RESET_OFFSETS, "earliest");
+    props.setProperty(Config.KAFKA_AUTO_OFFSET_RESET, "earliest");
     props.setProperty("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
     props.setProperty("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
     props.setProperty("hoodie.deltastreamer.kafka.source.maxEvents", String.valueOf(5000));
@@ -1352,7 +1352,7 @@ private void prepareJsonKafkaDFSSource(String propsFileName, String autoResetVal
     props.setProperty("hoodie.deltastreamer.source.kafka.topic",topicName);
     props.setProperty("hoodie.deltastreamer.schemaprovider.source.schema.file", dfsBasePath + "/source_uber.avsc");
     props.setProperty("hoodie.deltastreamer.schemaprovider.target.schema.file", dfsBasePath + "/target_uber.avsc");
-    props.setProperty(Config.KAFKA_AUTO_RESET_OFFSETS, autoResetValue);
+    props.setProperty(Config.KAFKA_AUTO_OFFSET_RESET, autoResetValue);
 
     UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + "/" + propsFileName);
   }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestKafkaSource.java
Patch:
@@ -88,7 +88,7 @@ private TypedProperties createPropsForJsonSource(Long maxEventsToReadFromKafkaSo
     TypedProperties props = new TypedProperties();
     props.setProperty("hoodie.deltastreamer.source.kafka.topic", TEST_TOPIC_NAME);
     props.setProperty("bootstrap.servers", testUtils.brokerAddress());
-    props.setProperty(Config.KAFKA_AUTO_RESET_OFFSETS, resetStrategy);
+    props.setProperty(Config.KAFKA_AUTO_OFFSET_RESET, resetStrategy);
     props.setProperty("hoodie.deltastreamer.kafka.source.maxEvents",
         maxEventsToReadFromKafkaSource != null ? String.valueOf(maxEventsToReadFromKafkaSource) :
             String.valueOf(Config.maxEventsFromKafkaSource));

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java
Patch:
@@ -366,7 +366,7 @@ private void writeToFile(Schema wrapperSchema, List<IndexedRecord> records) thro
   }
 
   private IndexedRecord convertToAvroRecord(HoodieTimeline commitTimeline, HoodieInstant hoodieInstant)
-      throws IOException {
+          throws IOException {
     return MetadataConversionUtils.createMetaWrapper(hoodieInstant, metaClient);
   }
 }

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/FileCreateUtils.java
Patch:
@@ -180,8 +180,8 @@ public static void createInflightRollbackFile(String basePath, String instantTim
     createMetaFile(basePath, instantTime, HoodieTimeline.INFLIGHT_ROLLBACK_EXTENSION);
   }
 
-  public static void createRollbackFile(String basePath, String instantTime, HoodieRollbackMetadata rollbackMetadata) throws IOException {
-    createMetaFile(basePath, instantTime, HoodieTimeline.ROLLBACK_EXTENSION, serializeRollbackMetadata(rollbackMetadata).get());
+  public static void createRollbackFile(String basePath, String instantTime, HoodieRollbackMetadata hoodieRollbackMetadata) throws IOException {
+    createMetaFile(basePath, instantTime, HoodieTimeline.ROLLBACK_EXTENSION, serializeRollbackMetadata(hoodieRollbackMetadata).get());
   }
 
   private static void createAuxiliaryMetaFile(String basePath, String instantTime, String suffix) throws IOException {

File: hudi-flink/src/test/java/org/apache/hudi/table/format/TestInputFormat.java
Patch:
@@ -118,6 +118,7 @@ void testReadBaseAndLogFiles() throws Exception {
 
     // write parquet first with compaction
     conf.setBoolean(FlinkOptions.COMPACTION_ASYNC_ENABLED, true);
+    conf.setInteger(FlinkOptions.COMPACTION_DELTA_COMMITS, 1);
     TestData.writeData(TestData.DATA_SET_INSERT, conf);
 
     InputFormat<RowData, ?> inputFormat = this.tableSource.getInputFormat();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/bootstrap/HoodieSparkBootstrapSchemaProvider.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.avro.Schema;
 import org.apache.hadoop.fs.Path;
+import org.apache.hudi.AvroConversionUtils;
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.avro.model.HoodieFileStatus;
 import org.apache.hudi.common.bootstrap.FileStatusUtils;
@@ -29,7 +30,6 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.parquet.schema.MessageType;
-import org.apache.spark.sql.avro.SchemaConverters;
 import org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter;
 import org.apache.spark.sql.internal.SQLConf;
 import org.apache.spark.sql.types.StructType;
@@ -63,6 +63,6 @@ protected Schema getBootstrapSourceSchema(HoodieEngineContext context, List<Pair
     String structName = tableName + "_record";
     String recordNamespace = "hoodie." + tableName;
 
-    return SchemaConverters.toAvroType(sparkSchema, false, structName, recordNamespace);
+    return AvroConversionUtils.convertStructTypeToAvroSchema(sparkSchema, structName, recordNamespace);
   }
 }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestSchemaPostProcessor.java
Patch:
@@ -47,8 +47,8 @@ public class TestSchemaPostProcessor extends UtilitiesTestBase {
   private static String ORIGINAL_SCHEMA = "{\"name\":\"t3_biz_operation_t_driver\",\"type\":\"record\",\"fields\":[{\"name\":\"ums_id_\",\"type\":[\"null\",\"string\"],\"default\":null},"
                                               + "{\"name\":\"ums_ts_\",\"type\":[\"null\",\"string\"],\"default\":null}]}";
 
-  private static String RESULT_SCHEMA = "{\"type\":\"record\",\"name\":\"hoodie_source\",\"namespace\":\"hoodie.source\",\"fields\":[{\"name\":\"ums_id_\",\"type\":[\"string\",\"null\"]},"
-                                            + "{\"name\":\"ums_ts_\",\"type\":[\"string\",\"null\"]}]}";
+  private static String RESULT_SCHEMA = "{\"type\":\"record\",\"name\":\"hoodie_source\",\"namespace\":\"hoodie.source\",\"fields\":[{\"name\":\"ums_id_\",\"type\":[\"null\",\"string\"],"
+                                              + "\"default\":null},{\"name\":\"ums_ts_\",\"type\":[\"null\",\"string\"],\"default\":null}]}";
 
   @Test
   public void testPostProcessor() throws IOException {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java
Patch:
@@ -102,7 +102,7 @@ public static void main(String[] args) {
   }
 
   private boolean isUpsert() {
-    return "upsert".equals(cfg.command.toLowerCase());
+    return "upsert".equalsIgnoreCase(cfg.command);
   }
 
   public int dataImport(JavaSparkContext jsc, int retry) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/exception/HoodieDependentSystemUnavailableException.java
Patch:
@@ -27,8 +27,8 @@ public class HoodieDependentSystemUnavailableException extends HoodieException {
 
   public static final String HBASE = "HBASE";
 
-  public HoodieDependentSystemUnavailableException(String system, String connectURL) {
-    super(getLogMessage(system, connectURL));
+  public HoodieDependentSystemUnavailableException(String system, String connectURL, Throwable t) {
+    super(getLogMessage(system, connectURL), t);
   }
 
   private static String getLogMessage(String system, String connectURL) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java
Patch:
@@ -151,7 +151,7 @@ private Connection getHBaseConnection() {
       return ConnectionFactory.createConnection(hbaseConfig);
     } catch (IOException e) {
       throw new HoodieDependentSystemUnavailableException(HoodieDependentSystemUnavailableException.HBASE,
-          quorum + ":" + port);
+          quorum + ":" + port, e);
     }
   }
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteWriter.java
Patch:
@@ -126,7 +126,7 @@ private boolean allowWriteClientAccess(DagNode dagNode) {
   public RDD<GenericRecord> getNextBatch() throws Exception {
     Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> nextBatch = fetchSource();
     lastCheckpoint = Option.of(nextBatch.getValue().getLeft());
-    JavaRDD <HoodieRecord> inputRDD = nextBatch.getRight().getRight();
+    JavaRDD<HoodieRecord> inputRDD = nextBatch.getRight().getRight();
     return inputRDD.map(r -> (GenericRecord) r.getData()
         .getInsertValue(new Schema.Parser().parse(schema)).get()).rdd();
   }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/converter/DeleteConverter.java
Patch:
@@ -24,8 +24,6 @@
 import org.apache.avro.generic.GenericRecord;
 import org.apache.spark.api.java.JavaRDD;
 
-import java.util.List;
-
 public class DeleteConverter implements Converter<GenericRecord, GenericRecord> {
 
   private final String schemaStr;

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/scheduler/DagScheduler.java
Patch:
@@ -27,7 +27,6 @@
 import org.apache.hudi.metrics.Metrics;
 
 import org.apache.spark.api.java.JavaSparkContext;
-import org.junit.runners.Suite;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/scheduler/SaferSchemaDagScheduler.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.integ.testsuite.dag.scheduler;
 
 import org.apache.hudi.exception.HoodieException;
-import org.apache.hudi.integ.testsuite.dag.scheduler.DagScheduler;
 import org.apache.hudi.integ.testsuite.dag.WorkflowDag;
 import org.apache.hudi.integ.testsuite.dag.WriterContext;
 import org.apache.hudi.integ.testsuite.dag.nodes.DagNode;

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/DeleteGeneratorIterator.java
Patch:
@@ -21,9 +21,7 @@
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
 
-import java.util.ArrayList;
 import java.util.Iterator;
-import java.util.List;
 
 /**
  * Lazy delete record generator.

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/FlexibleSchemaRecordGenerationIterator.java
Patch:
@@ -53,7 +53,7 @@ public FlexibleSchemaRecordGenerationIterator(long maxEntriesToProduce, int minP
       List<String> partitionPathFieldNames, int numPartitions, int startPartition) {
     this.counter = maxEntriesToProduce;
     this.partitionPathFieldNames = new HashSet<>(partitionPathFieldNames);
-    if(partitionPathFieldNames != null && partitionPathFieldNames.size() > 0) {
+    if (partitionPathFieldNames != null && partitionPathFieldNames.size() > 0) {
       this.firstPartitionPathField = partitionPathFieldNames.get(0);
     }
     Schema schema = new Schema.Parser().parse(schemaStr);

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/GenericRecordFullPayloadGenerator.java
Patch:
@@ -27,7 +27,6 @@
 import org.apache.avro.generic.GenericData.Fixed;
 import org.apache.avro.generic.GenericFixed;
 import org.apache.avro.generic.GenericRecord;
-import org.junit.runners.Suite;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/writer/DFSDeltaWriterAdapter.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.hudi.integ.testsuite.writer;
 
-import org.apache.hudi.integ.testsuite.schema.SchemaUtils;
-
 import org.apache.avro.generic.GenericRecord;
 
 import java.io.IOException;

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java
Patch:
@@ -77,7 +77,7 @@ public void generateDataByHoodieJavaApp(String hiveTableName, String tableType,
         tableType, hiveTableName, commitType, hoodieTableName);
     if (partitionType == PartitionType.MULTI_KEYS_PARTITIONED) {
       cmd = cmd + " --use-multi-partition-keys";
-    } else if (partitionType == PartitionType.NON_PARTITIONED){
+    } else if (partitionType == PartitionType.NON_PARTITIONED) {
       cmd = cmd + " --non-partitioned";
     }
     executeCommandStringInDocker(ADHOC_1_CONTAINER, cmd, true);

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java
Patch:
@@ -166,7 +166,7 @@ private TestExecStartResultCallback executeCommandInDocker(String containerName,
     // try to capture stdout and stderr of the stuck process.
 
     boolean completed =
-      dockerClient.execStartCmd(createCmdResponse.getId()).withDetach(false).withTty(false).exec(callback)
+        dockerClient.execStartCmd(createCmdResponse.getId()).withDetach(false).withTty(false).exec(callback)
         .awaitCompletion(540, SECONDS);
     if (!completed) {
       callback.getStderr().flush();

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/converter/TestDeleteConverter.java
Patch:
@@ -28,7 +28,6 @@
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
 
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
 

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/converter/TestUpdateConverter.java
Patch:
@@ -20,7 +20,6 @@
 
 import static junit.framework.TestCase.assertTrue;
 
-import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;

File: hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalTable.java
Patch:
@@ -66,7 +66,8 @@ public Set<TableCapability> capabilities() {
     return new HashSet<TableCapability>() {{
         add(TableCapability.BATCH_WRITE);
         add(TableCapability.TRUNCATE);
-      }};
+      }
+    };
   }
 
   @Override

File: hudi-common/src/main/java/org/apache/hudi/common/util/ReflectionUtils.java
Patch:
@@ -81,7 +81,7 @@ public static <T extends HoodieRecordPayload> T loadPayload(String recordPayload
   }
 
   /**
-   * Creates an instnace of the given class. Use this version when dealing with interface types as constructor args.
+   * Creates an instance of the given class. Use this version when dealing with interface types as constructor args.
    */
   public static Object loadClass(String clazz, Class<?>[] constructorArgTypes, Object... constructorArgs) {
     try {

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCommitsCommand.java
Patch:
@@ -194,7 +194,7 @@ public void testShowArchivedCommits() throws Exception {
     // archived 101 and 102 instants, remove 103 and 104 instant
     data.remove("103");
     data.remove("104");
-    String expected = generateExpectData(3, data);
+    String expected = generateExpectData(1, data);
     expected = removeNonWordAndStripSpace(expected);
     String got = removeNonWordAndStripSpace(cr.getResult().toString());
     assertEquals(expected, got);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/CompactionAdminClient.java
Patch:
@@ -383,7 +383,7 @@ private List<RenameOpResult> runRenamingOps(HoodieTableMetaClient metaClient,
    * @return list of pairs of log-files (old, new) and for each pair, rename must be done to successfully unschedule
    *         compaction.
    */
-  protected List<Pair<HoodieLogFile, HoodieLogFile>> getRenamingActionsForUnschedulingCompactionPlan(
+  public List<Pair<HoodieLogFile, HoodieLogFile>> getRenamingActionsForUnschedulingCompactionPlan(
       HoodieTableMetaClient metaClient, String compactionInstant, int parallelism,
       Option<HoodieTableFileSystemView> fsViewOpt, boolean skipValidation) throws IOException {
     HoodieTableFileSystemView fsView = fsViewOpt.isPresent() ? fsViewOpt.get()

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/TestHoodieTimelineArchiveLog.java
Patch:
@@ -464,7 +464,8 @@ public void testArchiveCommitTimeline() throws IOException {
     assertTrue(result);
     HoodieArchivedTimeline archivedTimeline = metaClient.getArchivedTimeline();
     List<HoodieInstant> archivedInstants = Arrays.asList(instant1, instant2, instant3);
-    assertEquals(new HashSet<>(archivedInstants), archivedTimeline.getInstants().collect(Collectors.toSet()));
+    assertEquals(new HashSet<>(archivedInstants),
+        archivedTimeline.filterCompletedInstants().getInstants().collect(Collectors.toSet()));
     assertFalse(wrapperFs.exists(markerPath));
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieFileGroupId.java
Patch:
@@ -69,7 +69,7 @@ public String toString() {
   public int compareTo(HoodieFileGroupId o) {
     int ret = partitionPath.compareTo(o.partitionPath);
     if (ret == 0) {
-      ret = fileId.compareTo(fileId);
+      ret = fileId.compareTo(o.fileId);
     }
     return ret;
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java
Patch:
@@ -41,7 +41,6 @@ public class HoodieIndexUtils {
    * Fetches Pair of partition path and {@link HoodieBaseFile}s for interested partitions.
    *
    * @param partition   Partition of interest
-   * @param context     Instance of {@link HoodieEngineContext} to use
    * @param hoodieTable Instance of {@link HoodieTable} of interest
    * @return the list of {@link HoodieBaseFile}
    */

File: hudi-common/src/main/java/org/apache/hudi/common/util/ReflectionUtils.java
Patch:
@@ -47,7 +47,7 @@ public class ReflectionUtils {
 
   private static Map<String, Class<?>> clazzCache = new HashMap<>();
 
-  private static Class<?> getClass(String clazzName) {
+  public static Class<?> getClass(String clazzName) {
     if (!clazzCache.containsKey(clazzName)) {
       try {
         Class<?> clazz = Class.forName(clazzName);

File: hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSink.java
Patch:
@@ -69,8 +69,8 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
 
       DataStream<Object> pipeline = dataStream
           .map(new RowDataToHoodieFunction<>(rowType, conf), TypeInformation.of(HoodieRecord.class))
-          // Key-by partition path, to avoid multiple subtasks write to a partition at the same time
-          .keyBy(HoodieRecord::getPartitionPath)
+          // Key-by record key, to avoid multiple subtasks write to a bucket at the same time
+          .keyBy(HoodieRecord::getRecordKey)
           .transform(
               "bucket_assigner",
               TypeInformation.of(HoodieRecord.class),

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java
Patch:
@@ -639,7 +639,7 @@ public HoodieCleanMetadata clean(String cleanInstantTime) throws HoodieIOExcepti
    */
   public HoodieCleanMetadata clean(String cleanInstantTime, boolean scheduleInline) throws HoodieIOException {
     if (scheduleInline) {
-      scheduleCleaningAtInstant(cleanInstantTime, Option.empty());
+      scheduleTableServiceInternal(cleanInstantTime, Option.empty(), TableServiceType.CLEAN);
     }
     LOG.info("Cleaner started");
     final Timer.Context timerContext = metrics.getCleanCtx();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/LockManager.java
Patch:
@@ -93,7 +93,7 @@ public void unlock() {
   public synchronized LockProvider getLockProvider() {
     // Perform lazy initialization of lock provider only if needed
     if (lockProvider == null) {
-      LOG.info("Lock Provider " + writeConfig.getLockProviderClass());
+      LOG.info("LockProvider " + writeConfig.getLockProviderClass());
       lockProvider = (LockProvider) ReflectionUtils.loadClass(writeConfig.getLockProviderClass(),
           lockConfiguration, hadoopConf.get());
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
Patch:
@@ -108,7 +108,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {
   public static final String COMPACTION_REVERSE_LOG_READ_ENABLED_PROP = "hoodie.compaction.reverse.log.read";
   public static final String DEFAULT_COMPACTION_REVERSE_LOG_READ_ENABLED = "false";
   private static final String DEFAULT_CLEANER_POLICY = HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name();
-  public static final String FAILED_WRITES_CLEANER_POLICY_PROP = "hoodie.failed.writes.cleaner.policy";
+  public static final String FAILED_WRITES_CLEANER_POLICY_PROP = "hoodie.cleaner.policy.failed.writes";
   private  static final String DEFAULT_FAILED_WRITES_CLEANER_POLICY =
       HoodieFailedWritesCleaningPolicy.EAGER.name();
   private static final String DEFAULT_AUTO_CLEAN = "true";

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -1424,7 +1424,7 @@ private void validate() {
       if (props.getProperty(WRITE_CONCURRENCY_MODE_PROP)
           .equalsIgnoreCase(WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.name())) {
         ValidationUtils.checkArgument(props.getProperty(HoodieCompactionConfig.FAILED_WRITES_CLEANER_POLICY_PROP)
-            != HoodieFailedWritesCleaningPolicy.EAGER.name());
+            != HoodieFailedWritesCleaningPolicy.EAGER.name(), "To enable optimistic concurrency control, set hoodie.cleaner.policy.failed.writes=LAZY");
       }
     }
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java
Patch:
@@ -289,7 +289,7 @@ protected static TypedProperties prepareMultiWriterProps(String propsFileName) t
 
     props.setProperty("include", "base.properties");
     props.setProperty("hoodie.write.concurrency.mode", "optimistic_concurrency_control");
-    props.setProperty("hoodie.failed.writes.cleaner.policy", "LAZY");
+    props.setProperty("hoodie.cleaner.policy.failed.writes", "LAZY");
     props.setProperty("hoodie.write.lock.provider", "org.apache.hudi.client.transaction.lock.ZookeeperBasedLockProvider");
     props.setProperty("hoodie.write.lock.hivemetastore.database", "testdb1");
     props.setProperty("hoodie.write.lock.hivemetastore.table", "table1");
@@ -298,7 +298,7 @@ protected static TypedProperties prepareMultiWriterProps(String propsFileName) t
     props.setProperty("hoodie.write.lock.wait_time_ms", "1200000");
     props.setProperty("hoodie.write.lock.num_retries", "10");
     props.setProperty("hoodie.write.lock.zookeeper.lock_key", "test_table");
-    props.setProperty("hoodie.write.lock.zookeeper.zk_base_path", "/test");
+    props.setProperty("hoodie.write.lock.zookeeper.base_path", "/test");
 
     UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + "/" + propsFileName);
     return props;

File: hudi-flink/src/test/java/org/apache/hudi/utils/source/ContinuousFileSource.java
Patch:
@@ -33,7 +33,6 @@
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.RowType;
 
-import java.io.File;
 import java.io.IOException;
 import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
@@ -152,7 +151,6 @@ public void cancel() {
 
     private void loadDataBuffer() {
       try {
-        new File(this.path.toString()).exists();
         this.dataBuffer = Files.readAllLines(Paths.get(this.path.toUri()));
       } catch (IOException e) {
         throw new RuntimeException("Read file " + this.path + " error", e);

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java
Patch:
@@ -53,6 +53,7 @@
 import org.apache.hudi.utilities.sources.JsonKafkaSource;
 import org.apache.hudi.utilities.sources.ParquetDFSSource;
 import org.apache.hudi.utilities.sources.TestDataSource;
+import org.apache.hudi.utilities.sources.helpers.KafkaOffsetGen.Config;
 import org.apache.hudi.utilities.testutils.UtilitiesTestBase;
 import org.apache.hudi.utilities.testutils.sources.DistributedTestDataSource;
 import org.apache.hudi.utilities.testutils.sources.config.SourceConfigs;
@@ -259,7 +260,7 @@ protected static void populateCommonProps(TypedProperties props) {
   protected static void populateCommonKafkaProps(TypedProperties props) {
     //Kafka source properties
     props.setProperty("bootstrap.servers", testUtils.brokerAddress());
-    props.setProperty("hoodie.deltastreamer.source.kafka.auto.reset.offsets", "earliest");
+    props.setProperty(Config.KAFKA_AUTO_RESET_OFFSETS, "earliest");
     props.setProperty("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
     props.setProperty("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
     props.setProperty("hoodie.deltastreamer.kafka.source.maxEvents", String.valueOf(5000));
@@ -1344,7 +1345,7 @@ private void prepareJsonKafkaDFSSource(String propsFileName, String autoResetVal
     props.setProperty("hoodie.deltastreamer.source.kafka.topic",topicName);
     props.setProperty("hoodie.deltastreamer.schemaprovider.source.schema.file", dfsBasePath + "/source_uber.avsc");
     props.setProperty("hoodie.deltastreamer.schemaprovider.target.schema.file", dfsBasePath + "/target_uber.avsc");
-    props.setProperty("hoodie.deltastreamer.source.kafka.auto.reset.offsets", autoResetValue);
+    props.setProperty(Config.KAFKA_AUTO_RESET_OFFSETS, autoResetValue);
 
     UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + "/" + propsFileName);
   }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestKafkaSource.java
Patch:
@@ -88,7 +88,7 @@ private TypedProperties createPropsForJsonSource(Long maxEventsToReadFromKafkaSo
     TypedProperties props = new TypedProperties();
     props.setProperty("hoodie.deltastreamer.source.kafka.topic", TEST_TOPIC_NAME);
     props.setProperty("bootstrap.servers", testUtils.brokerAddress());
-    props.setProperty("hoodie.deltastreamer.source.kafka.auto.reset.offsets", resetStrategy);
+    props.setProperty(Config.KAFKA_AUTO_RESET_OFFSETS, resetStrategy);
     props.setProperty("hoodie.deltastreamer.kafka.source.maxEvents",
         maxEventsToReadFromKafkaSource != null ? String.valueOf(maxEventsToReadFromKafkaSource) :
             String.valueOf(Config.maxEventsFromKafkaSource));

File: hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java
Patch:
@@ -66,7 +66,7 @@ public static HoodieCommitMetadata buildMetadata(List<HoodieWriteStat> writeStat
     if (extraMetadata.isPresent()) {
       extraMetadata.get().forEach(commitMetadata::addMetadata);
     }
-    commitMetadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, schemaToStoreInCommit);
+    commitMetadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, schemaToStoreInCommit == null ? "" : schemaToStoreInCommit);
     commitMetadata.setOperationType(operationType);
     return commitMetadata;
   }

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/DataSourceInternalWriterHelper.java
Patch:
@@ -40,6 +40,7 @@
 import org.apache.spark.sql.SparkSession;
 import org.apache.spark.sql.types.StructType;
 
+import java.util.HashMap;
 import java.util.List;
 
 /**
@@ -77,7 +78,7 @@ public void onDataWriterCommit(String message) {
 
   public void commit(List<HoodieWriteStat> writeStatList) {
     try {
-      writeClient.commitStats(instantTime, writeStatList, Option.empty(),
+      writeClient.commitStats(instantTime, writeStatList, Option.of(new HashMap<>()),
           DataSourceUtils.getCommitActionType(operationType, metaClient.getTableType()));
     } catch (Exception ioe) {
       throw new HoodieException(ioe.getMessage(), ioe);

File: hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/DefaultSource.java
Patch:
@@ -60,7 +60,8 @@ public Optional<DataSourceWriter> createWriter(String writeUUID, StructType sche
     String instantTime = options.get(DataSourceInternalWriterHelper.INSTANT_TIME_OPT_KEY).get();
     String path = options.get("path").get();
     String tblName = options.get(HoodieWriteConfig.TABLE_NAME).get();
-    HoodieWriteConfig config = DataSourceUtils.createHoodieConfig(null, path, tblName, options.asMap());
+    // 1st arg to createHooodieConfig is not really reuqired to be set. but passing it anyways.
+    HoodieWriteConfig config = DataSourceUtils.createHoodieConfig(options.get(HoodieWriteConfig.AVRO_SCHEMA).get(), path, tblName, options.asMap());
     return Optional.of(new HoodieDataSourceInternalWriter(instantTime, config, schema, getSparkSession(),
             getConfiguration()));
   }

File: hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java
Patch:
@@ -47,7 +47,8 @@ public Table getTable(StructType schema, Transform[] partitioning, Map<String, S
     String instantTime = properties.get(DataSourceInternalWriterHelper.INSTANT_TIME_OPT_KEY);
     String path = properties.get("path");
     String tblName = properties.get(HoodieWriteConfig.TABLE_NAME);
-    HoodieWriteConfig config = DataSourceUtils.createHoodieConfig(null, path, tblName, properties);
+    // 1st arg to createHooodieConfig is not really reuqired to be set. but passing it anyways.
+    HoodieWriteConfig config = DataSourceUtils.createHoodieConfig(properties.get(HoodieWriteConfig.AVRO_SCHEMA), path, tblName, properties);
     return new HoodieDataSourceInternalTable(instantTime, config, schema, getSparkSession(),
         getConfiguration());
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java
Patch:
@@ -69,7 +69,7 @@ public static Pair<String, String> calculateBeginAndEndInstants(JavaSparkContext
         return lastInstant.map(hoodieInstant -> getStrictlyLowerTimestamp(hoodieInstant.getTimestamp())).orElse("000");
       } else {
         throw new IllegalArgumentException("Missing begin instant for incremental pull. For reading from latest "
-            + "committed instant set hoodie.deltastreamer.source.hoodie.read_latest_on_midding_ckpt to true");
+            + "committed instant set hoodie.deltastreamer.source.hoodieincr.read_latest_on_missing_ckpt to true");
       }
     });
 

File: hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestTable.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.avro.model.HoodieCleanMetadata;
 import org.apache.hudi.avro.model.HoodieCleanerPlan;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
+import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;
@@ -163,8 +164,8 @@ public HoodieTestTable addDeltaCommit(String instantTime) throws Exception {
     return this;
   }
 
-  public HoodieTestTable addReplaceCommit(String instantTime, HoodieReplaceCommitMetadata metadata) throws Exception {
-    createRequestedReplaceCommit(basePath, instantTime);
+  public HoodieTestTable addReplaceCommit(String instantTime, HoodieRequestedReplaceMetadata requestedReplaceMetadata, HoodieReplaceCommitMetadata metadata) throws Exception {
+    createRequestedReplaceCommit(basePath, instantTime, requestedReplaceMetadata);
     createInflightReplaceCommit(basePath, instantTime);
     createReplaceCommit(basePath, instantTime, metadata);
     currentInstantTime = instantTime;

File: hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.util;
 
 import org.apache.hudi.common.model.HoodieRecordLocation;
+import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.TablePathUtils;
 import org.apache.hudi.exception.HoodieException;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java
Patch:
@@ -177,7 +177,7 @@ public KafkaOffsetGen(TypedProperties props) {
     }
     DataSourceUtils.checkRequiredProperties(props, Collections.singletonList(Config.KAFKA_TOPIC_NAME));
     topicName = props.getString(Config.KAFKA_TOPIC_NAME);
-    String kafkaAutoResetOffsetsStr = props.getString(Config.KAFKA_AUTO_RESET_OFFSETS, Config.DEFAULT_KAFKA_AUTO_RESET_OFFSETS.name());
+    String kafkaAutoResetOffsetsStr = props.getString(Config.KAFKA_AUTO_RESET_OFFSETS, Config.DEFAULT_KAFKA_AUTO_RESET_OFFSETS.name().toLowerCase());
     boolean found = false;
     for (KafkaResetOffsetStrategies entry: KafkaResetOffsetStrategies.values()) {
       if (entry.name().toLowerCase().equals(kafkaAutoResetOffsetsStr)) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatReader.java
Patch:
@@ -104,7 +104,7 @@ public boolean hasNext() {
         throw new HoodieIOException("unable to initialize read with log file ", io);
       }
       LOG.info("Moving to the next reader for logfile " + currentReader.getLogFile());
-      return this.currentReader.hasNext();
+      return hasNext();
     }
     return false;
   }

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java
Patch:
@@ -273,7 +273,7 @@ private String printAllCompactions(HoodieDefaultTimeline timeline,
                                      int limit,
                                      boolean headerOnly) {
 
-    Stream<HoodieInstant> instantsStream = timeline.getCommitsAndCompactionTimeline().getReverseOrderedInstants();
+    Stream<HoodieInstant> instantsStream = timeline.getWriteTimeline().getReverseOrderedInstants();
     List<Pair<HoodieInstant, HoodieCompactionPlan>> compactionPlans = instantsStream
             .map(instant -> Pair.of(instant, compactionPlanReader.apply(instant)))
             .filter(pair -> pair.getRight() != null)

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/FileSystemViewCommand.java
Patch:
@@ -249,7 +249,7 @@ private HoodieTableFileSystemView buildFileSystemView(String globRegex, String m
     } else if (excludeCompaction) {
       timeline = metaClient.getActiveTimeline().getCommitsTimeline();
     } else {
-      timeline = metaClient.getActiveTimeline().getCommitsAndCompactionTimeline();
+      timeline = metaClient.getActiveTimeline().getWriteTimeline();
     }
 
     if (!includeInflight) {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
+import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -155,6 +156,7 @@ private HoodieWriteConfig createMetadataWriteConfig(HoodieWriteConfig writeConfi
             .withAutoClean(false)
             .withCleanerParallelism(parallelism)
             .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS)
+            .withFailedWritesCleaningPolicy(HoodieFailedWritesCleaningPolicy.EAGER)
             .retainCommits(writeConfig.getMetadataCleanerCommitsRetained())
             .archiveCommitsWith(writeConfig.getMetadataMinCommitsToKeep(), writeConfig.getMetadataMaxCommitsToKeep())
             // we will trigger compaction manually, to control the instant times

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/BaseScheduleCompactionActionExecutor.java
Patch:
@@ -62,7 +62,7 @@ public Option<HoodieCompactionPlan> execute() {
 
     // Committed and pending compaction instants should have strictly lower timestamps
     List<HoodieInstant> conflictingInstants = table.getActiveTimeline()
-        .getCommitsAndCompactionTimeline().getInstants()
+        .getWriteTimeline().getInstants()
         .filter(instant -> HoodieTimeline.compareTimestamps(
             instant.getTimestamp(), HoodieTimeline.GREATER_THAN_OR_EQUALS, instantTime))
         .collect(Collectors.toList());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/BaseRestoreActionExecutor.java
Patch:
@@ -63,7 +63,7 @@ public HoodieRestoreMetadata execute() {
     restoreTimer.startTimer();
 
     // Get all the commits on the timeline after the provided commit time
-    List<HoodieInstant> instantsToRollback = table.getActiveTimeline().getCommitsAndCompactionTimeline()
+    List<HoodieInstant> instantsToRollback = table.getActiveTimeline().getWriteTimeline()
         .getReverseOrderedInstants()
         .filter(instant -> HoodieActiveTimeline.GREATER_THAN.test(instant.getTimestamp(), restoreInstantTime))
         .collect(Collectors.toList());

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/savepoint/SavepointHelpers.java
Patch:
@@ -52,7 +52,7 @@ public static void validateSavepointRestore(HoodieTable table, String savepointT
     // Make sure the restore was successful
     table.getMetaClient().reloadActiveTimeline();
     Option<HoodieInstant> lastInstant = table.getActiveTimeline()
-        .getCommitsAndCompactionTimeline()
+        .getWriteTimeline()
         .filterCompletedAndCompactionInstants()
         .lastInstant();
     ValidationUtils.checkArgument(lastInstant.isPresent());

File: hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java
Patch:
@@ -57,7 +57,7 @@ public HoodieJavaWriteClient(HoodieEngineContext context,
                                HoodieWriteConfig writeConfig,
                                boolean rollbackPending,
                                Option<EmbeddedTimelineService> timelineService) {
-    super(context, writeConfig, rollbackPending, timelineService);
+    super(context, writeConfig, timelineService);
   }
 
   @Override

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/TestHoodieTimelineArchiveLog.java
Patch:
@@ -408,11 +408,11 @@ public void testArchiveCommitCompactionNoHole() throws IOException {
     HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);
     HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(cfg, table);
 
-    HoodieTimeline timeline = metaClient.getActiveTimeline().getCommitsAndCompactionTimeline();
+    HoodieTimeline timeline = metaClient.getActiveTimeline().getWriteTimeline();
     assertEquals(8, timeline.countInstants(), "Loaded 6 commits and the count should match");
     boolean result = archiveLog.archiveIfRequired(context);
     assertTrue(result);
-    timeline = metaClient.getActiveTimeline().reload().getCommitsAndCompactionTimeline();
+    timeline = metaClient.getActiveTimeline().reload().getWriteTimeline();
     assertFalse(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.COMMIT_ACTION, "100")),
         "Instants before oldest pending compaction can be removed");
     assertEquals(7, timeline.countInstants(),

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/CompactionTestBase.java
Patch:
@@ -262,7 +262,7 @@ protected List<HoodieBaseFile> getCurrentLatestBaseFiles(HoodieTable table) thro
 
   protected List<FileSlice> getCurrentLatestFileSlices(HoodieTable table) {
     HoodieTableFileSystemView view = new HoodieTableFileSystemView(table.getMetaClient(),
-        table.getMetaClient().getActiveTimeline().reload().getCommitsAndCompactionTimeline());
+        table.getMetaClient().getActiveTimeline().reload().getWriteTimeline());
     return Arrays.stream(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS)
         .flatMap(view::getLatestFileSlices).collect(Collectors.toList());
   }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/FunctionalTestHarness.java
Patch:
@@ -56,7 +56,7 @@
 
 public class FunctionalTestHarness implements SparkProvider, DFSProvider, HoodieMetaClientProvider, HoodieWriteClientProvider {
 
-  private static transient SparkSession spark;
+  protected static transient SparkSession spark;
   private static transient SQLContext sqlContext;
   private static transient JavaSparkContext jsc;
   protected static transient HoodieSparkEngineContext context;
@@ -126,7 +126,7 @@ public HoodieTableMetaClient getHoodieMetaClient(Configuration hadoopConf, Strin
 
   @Override
   public SparkRDDWriteClient getHoodieWriteClient(HoodieWriteConfig cfg) throws IOException {
-    return new SparkRDDWriteClient(context(), cfg, false);
+    return new SparkRDDWriteClient(context(), cfg);
   }
 
   @BeforeEach

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCleaningPolicy.java
Patch:
@@ -22,5 +22,5 @@
  * Hoodie cleaning policies.
  */
 public enum HoodieCleaningPolicy {
-  KEEP_LATEST_FILE_VERSIONS, KEEP_LATEST_COMMITS
+  KEEP_LATEST_FILE_VERSIONS, KEEP_LATEST_COMMITS;
 }

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -69,9 +69,7 @@ public class HoodieActiveTimeline extends HoodieDefaultTimeline {
       INFLIGHT_COMPACTION_EXTENSION, REQUESTED_COMPACTION_EXTENSION,
       INFLIGHT_RESTORE_EXTENSION, RESTORE_EXTENSION,
       ROLLBACK_EXTENSION, INFLIGHT_ROLLBACK_EXTENSION,
-      REQUESTED_REPLACE_COMMIT_EXTENSION, INFLIGHT_REPLACE_COMMIT_EXTENSION, REPLACE_COMMIT_EXTENSION
-  ));
-  
+      REQUESTED_REPLACE_COMMIT_EXTENSION, INFLIGHT_REPLACE_COMMIT_EXTENSION, REPLACE_COMMIT_EXTENSION));
   private static final Logger LOG = LogManager.getLogger(HoodieActiveTimeline.class);
   protected HoodieTableMetaClient metaClient;
   private static AtomicReference<String> lastInstantTime = new AtomicReference<>(String.valueOf(Integer.MIN_VALUE));

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimelineUtils.java
Patch:
@@ -49,7 +49,7 @@ public class TimelineUtils {
    * Does not include internal operations such as clean in the timeline.
    */
   public static List<String> getPartitionsWritten(HoodieTimeline timeline) {
-    HoodieTimeline timelineToSync = timeline.getCommitsAndCompactionTimeline();
+    HoodieTimeline timelineToSync = timeline.getWriteTimeline();
     return getAffectedPartitions(timelineToSync);
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -118,7 +118,7 @@ protected void init(HoodieTableMetaClient metaClient, HoodieTimeline visibleActi
    * @param visibleActiveTimeline Visible Active Timeline
    */
   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {
-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();
+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteTimeline();
   }
 
   /**

File: hudi-common/src/test/java/org/apache/hudi/common/table/timeline/TestHoodieActiveTimeline.java
Patch:
@@ -201,7 +201,7 @@ public void testTimelineGetOperations() {
     // return the correct set of Instant
     checkTimeline.accept(timeline.getCommitsTimeline(),
             CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION, HoodieTimeline.REPLACE_COMMIT_ACTION));
-    checkTimeline.accept(timeline.getCommitsAndCompactionTimeline(),
+    checkTimeline.accept(timeline.getWriteTimeline(),
             CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION, HoodieTimeline.COMPACTION_ACTION, HoodieTimeline.REPLACE_COMMIT_ACTION));
     checkTimeline.accept(timeline.getCommitTimeline(),  CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION, HoodieTimeline.REPLACE_COMMIT_ACTION));
     checkTimeline.accept(timeline.getDeltaCommitTimeline(), Collections.singleton(HoodieTimeline.DELTA_COMMIT_ACTION));

File: hudi-flink/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java
Patch:
@@ -111,7 +111,7 @@ public void open() throws Exception {
       TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);
 
       // writeClient
-      writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg), true);
+      writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));
 
       // init table, create it if not exists.
       StreamerUtil.initTableIfNotExists(FlinkOptions.fromStreamerConfig(cfg));

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java
Patch:
@@ -178,7 +178,7 @@ public static FileInputFormat getInputFormat(String path, boolean realtime, Conf
    * @return
    */
   public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {
-    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();
+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getWriteTimeline();
     Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline
         .filterPendingCompactionTimeline().firstInstant();
     if (pendingCompactionInstant.isPresent()) {

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/CompactNode.java
Patch:
@@ -49,7 +49,7 @@ public void execute(ExecutionContext executionContext, int curItrCount) throws E
         HoodieTableMetaClient.builder().setConf(executionContext.getHoodieTestSuiteWriter().getConfiguration()).setBasePath(executionContext.getHoodieTestSuiteWriter().getCfg().targetBasePath)
             .build();
     Option<HoodieInstant> lastInstant = metaClient.getActiveTimeline()
-        .getCommitsAndCompactionTimeline().filterPendingCompactionTimeline().lastInstant();
+        .getWriteTimeline().filterPendingCompactionTimeline().lastInstant();
     if (lastInstant.isPresent()) {
       log.info("Compacting instant {}", lastInstant.get());
       this.result = executionContext.getHoodieTestSuiteWriter().compact(Option.of(lastInstant.get().getTimestamp()));

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -190,7 +190,7 @@ public static HoodieWriteConfig createHoodieConfig(String schemaStr, String base
 
   public static SparkRDDWriteClient createHoodieClient(JavaSparkContext jssc, String schemaStr, String basePath,
                                                        String tblName, Map<String, String> parameters) {
-    return new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jssc), createHoodieConfig(schemaStr, basePath, tblName, parameters), true);
+    return new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jssc), createHoodieConfig(schemaStr, basePath, tblName, parameters));
   }
 
   public static String getCommitActionType(WriteOperationType operation, HoodieTableType tableType) {

File: hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/DataSourceInternalWriterHelper.java
Patch:
@@ -60,7 +60,7 @@ public DataSourceInternalWriterHelper(String instantTime, HoodieWriteConfig writ
       SparkSession sparkSession, Configuration configuration) {
     this.instantTime = instantTime;
     this.operationType = WriteOperationType.BULK_INSERT;
-    this.writeClient  = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(new JavaSparkContext(sparkSession.sparkContext())), writeConfig, true);
+    this.writeClient  = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(new JavaSparkContext(sparkSession.sparkContext())), writeConfig);
     writeClient.setOperationType(operationType);
     writeClient.startCommitWithTime(instantTime);
     this.metaClient = HoodieTableMetaClient.builder().setConf(configuration).setBasePath(writeConfig.getBasePath()).build();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java
Patch:
@@ -69,7 +69,7 @@ public HoodieCleaner(Config cfg, JavaSparkContext jssc) {
 
   public void run() {
     HoodieWriteConfig hoodieCfg = getHoodieClientConfig();
-    SparkRDDWriteClient client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jssc), hoodieCfg, false);
+    SparkRDDWriteClient client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jssc), hoodieCfg);
     client.clean();
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java
Patch:
@@ -85,11 +85,11 @@ public void snapshot(JavaSparkContext jsc, String baseDir, final String outputDi
     final SerializableConfiguration serConf = new SerializableConfiguration(jsc.hadoopConfiguration());
     final HoodieTableMetaClient tableMetadata = HoodieTableMetaClient.builder().setConf(fs.getConf()).setBasePath(baseDir).build();
     final BaseFileOnlyView fsView = new HoodieTableFileSystemView(tableMetadata,
-        tableMetadata.getActiveTimeline().getCommitsAndCompactionTimeline().filterCompletedInstants());
+        tableMetadata.getActiveTimeline().getWriteTimeline().filterCompletedInstants());
     HoodieEngineContext context = new HoodieSparkEngineContext(jsc);
     // Get the latest commit
     Option<HoodieInstant> latestCommit =
-        tableMetadata.getActiveTimeline().getCommitsAndCompactionTimeline().filterCompletedInstants().lastInstant();
+        tableMetadata.getActiveTimeline().getWriteTimeline().filterCompletedInstants().lastInstant();
     if (!latestCommit.isPresent()) {
       LOG.warn("No commits present. Nothing to snapshot");
       return;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java
Patch:
@@ -149,7 +149,7 @@ private boolean outputPathExists(FileSystem fs, Config cfg) throws IOException {
 
   private Option<String> getLatestCommitTimestamp(FileSystem fs, Config cfg) {
     final HoodieTableMetaClient tableMetadata = HoodieTableMetaClient.builder().setConf(fs.getConf()).setBasePath(cfg.sourceBasePath).build();
-    Option<HoodieInstant> latestCommit = tableMetadata.getActiveTimeline().getCommitsAndCompactionTimeline()
+    Option<HoodieInstant> latestCommit = tableMetadata.getActiveTimeline().getWriteTimeline()
         .filterCompletedInstants().lastInstant();
     return latestCommit.isPresent() ? Option.of(latestCommit.get().getTimestamp()) : Option.empty();
   }
@@ -261,7 +261,7 @@ private BaseFileOnlyView getBaseFileOnlyView(JavaSparkContext jsc, Config cfg) {
     FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, jsc.hadoopConfiguration());
     HoodieTableMetaClient tableMetadata = HoodieTableMetaClient.builder().setConf(fs.getConf()).setBasePath(cfg.sourceBasePath).build();
     return new HoodieTableFileSystemView(tableMetadata, tableMetadata
-        .getActiveTimeline().getCommitsAndCompactionTimeline().filterCompletedInstants());
+        .getActiveTimeline().getWriteTimeline().filterCompletedInstants());
   }
 
   public static void main(String[] args) throws IOException {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/BootstrapExecutor.java
Patch:
@@ -138,7 +138,7 @@ public BootstrapExecutor(HoodieDeltaStreamer.Config cfg, JavaSparkContext jssc,
    */
   public void execute() throws IOException {
     initializeTable();
-    SparkRDDWriteClient bootstrapClient = new SparkRDDWriteClient(new HoodieSparkEngineContext(jssc), bootstrapConfig, true);
+    SparkRDDWriteClient bootstrapClient = new SparkRDDWriteClient(new HoodieSparkEngineContext(jssc), bootstrapConfig);
 
     try {
       HashMap<String, String> checkpointCommitMetadata = new HashMap<>();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -598,7 +598,7 @@ private void reInitWriteClient(Schema sourceSchema, Schema targetSchema) throws
       // Close Write client.
       writeClient.close();
     }
-    writeClient = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jssc), hoodieCfg, true, embeddedTimelineService);
+    writeClient = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jssc), hoodieCfg, embeddedTimelineService);
     onInitializingHoodieWriteClient.apply(writeClient);
   }
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieCombineRealtimeRecordReader.java
Patch:
@@ -71,7 +71,7 @@ public boolean next(NullWritable key, ArrayWritable value) throws IOException {
     } else if (recordReaders.size() > 0) {
       this.currentRecordReader.close();
       this.currentRecordReader = recordReaders.remove(0);
-      return this.currentRecordReader.next(key, value);
+      return next(key, value);
     } else {
       return false;
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -767,7 +767,7 @@ public String getDatadogMetricHost() {
 
   public List<String> getDatadogMetricTags() {
     return Arrays.stream(props.getProperty(
-        HoodieMetricsDatadogConfig.DATADOG_METRIC_TAGS).split("\\s*,\\s*")).collect(Collectors.toList());
+        HoodieMetricsDatadogConfig.DATADOG_METRIC_TAGS, ",").split("\\s*,\\s*")).collect(Collectors.toList());
   }
 
   public String getMetricReporterClassName() {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java
Patch:
@@ -371,7 +371,6 @@ public abstract O bulkInsertPreppedRecords(I preppedRecords, final String instan
    * Common method containing steps to be performed before write (upsert/insert/...
    *
    * @param instantTime Instant Time
-   * @param hoodieTable Hoodie Table
    * @return Write Status
    */
   protected void preWrite(String instantTime, WriteOperationType writeOperationType) {
@@ -719,7 +718,7 @@ public abstract void commitCompaction(String compactionInstantTime, O writeStatu
    */
   protected abstract void completeCompaction(HoodieCommitMetadata metadata, O writeStatuses,
                                              HoodieTable<T, I, K, O> table, String compactionCommitTime);
-  
+
   /**
    * Rollback failed compactions. Inflight rollbacks for compactions revert the .inflight file to the .requested file
    *

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteJob.java
Patch:
@@ -82,6 +82,7 @@ public class HoodieTestSuiteJob {
   private BuiltinKeyGenerator keyGenerator;
 
   public HoodieTestSuiteJob(HoodieTestSuiteConfig cfg, JavaSparkContext jsc) throws IOException {
+    log.warn("Running spark job w/ app id " + jsc.sc().applicationId());
     this.cfg = cfg;
     this.jsc = jsc;
     cfg.propsFilePath = FSUtils.addSchemeIfLocalPath(cfg.propsFilePath).toString();

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/ExecutionContext.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  * This wraps the context needed for an execution of
- * a {@link DagNode#execute(ExecutionContext)}.
+ * a {@link DagNode#execute(ExecutionContext, int)}.
  */
 public class ExecutionContext implements Serializable {
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/CleanNode.java
Patch:
@@ -32,7 +32,7 @@ public CleanNode(Config config) {
   }
 
   @Override
-  public void execute(ExecutionContext executionContext) throws Exception {
+  public void execute(ExecutionContext executionContext, int curItrCount) throws Exception {
     log.info("Executing clean node {}", this.getName());
     executionContext.getHoodieTestSuiteWriter().getWriteClient(this).clean();
   }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/CompactNode.java
Patch:
@@ -40,10 +40,11 @@ public CompactNode(Config config) {
    * if it has one.
    *
    * @param executionContext Execution context to run this compaction
+   * @param curItrCount cur interation count.
    * @throws Exception  will be thrown if any error occurred.
    */
   @Override
-  public void execute(ExecutionContext executionContext) throws Exception {
+  public void execute(ExecutionContext executionContext, int curItrCount) throws Exception {
     HoodieTableMetaClient metaClient = new HoodieTableMetaClient(executionContext.getHoodieTestSuiteWriter().getConfiguration(),
         executionContext.getHoodieTestSuiteWriter().getCfg().targetBasePath);
     Option<HoodieInstant> lastInstant = metaClient.getActiveTimeline()

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/DagNode.java
Patch:
@@ -91,9 +91,10 @@ public void setParentNodes(List<DagNode<O>> parentNodes) {
    * Execute the {@link DagNode}.
    *
    * @param context The context needed for an execution of a node.
+   * @param curItrCount iteration count for executing the node.
    * @throws Exception Thrown if the execution failed.
    */
-  public abstract void execute(ExecutionContext context) throws Exception;
+  public abstract void execute(ExecutionContext context, int curItrCount) throws Exception;
 
   public boolean isCompleted() {
     return isCompleted;

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/DelayNode.java
Patch:
@@ -36,7 +36,7 @@ public DelayNode(int delayMins) {
   }
 
   @Override
-  public void execute(ExecutionContext context) throws Exception {
+  public void execute(ExecutionContext context, int curItrCount) throws Exception {
     log.warn("Waiting for "+ delayMins+" mins before going for next test run");
     Thread.sleep(delayMins * 60 * 1000);
   }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/HiveQueryNode.java
Patch:
@@ -43,7 +43,7 @@ public HiveQueryNode(DeltaConfig.Config config) {
   }
 
   @Override
-  public void execute(ExecutionContext executionContext) throws Exception {
+  public void execute(ExecutionContext executionContext, int curItrCount) throws Exception {
     log.info("Executing hive query node {}", this.getName());
     this.hiveServiceProvider.startLocalHiveServiceIfNeeded(executionContext.getHoodieTestSuiteWriter().getConfiguration());
     HiveSyncConfig hiveSyncConfig = DataSourceUtils

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/HiveSyncNode.java
Patch:
@@ -35,7 +35,7 @@ public HiveSyncNode(Config config) {
   }
 
   @Override
-  public void execute(ExecutionContext executionContext) throws Exception {
+  public void execute(ExecutionContext executionContext, int curItrCount) throws Exception {
     log.info("Executing hive sync node");
     this.hiveServiceProvider.startLocalHiveServiceIfNeeded(executionContext.getHoodieTestSuiteWriter().getConfiguration());
     this.hiveServiceProvider.syncToLocalHiveIfNeeded(executionContext.getHoodieTestSuiteWriter());

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/InsertNode.java
Patch:
@@ -39,7 +39,7 @@ public InsertNode(Config config) {
   }
 
   @Override
-  public void execute(ExecutionContext executionContext) throws Exception {
+  public void execute(ExecutionContext executionContext, int curItrCount) throws Exception {
     // if the insert node has schema override set, reinitialize the table with new schema.
     if (this.config.getReinitContext()) {
       log.info(String.format("Reinitializing table with %s", this.config.getOtherConfigs().toString()));

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/RollbackNode.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ValidationUtils;
-import org.apache.hudi.exception.HoodieNotSupportedException;
 import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;
 import org.apache.hudi.integ.testsuite.dag.ExecutionContext;
 import org.apache.hudi.integ.testsuite.helpers.DFSTestSuitePathSelector;
@@ -42,10 +41,11 @@ public RollbackNode(Config config) {
    * Method helps to rollback the last commit instant in the timeline, if it has one.
    *
    * @param executionContext Execution context to perform this rollback
+   * @param curItrCount current iteration count.
    * @throws Exception will be thrown if any error occurred
    */
   @Override
-  public void execute(ExecutionContext executionContext) throws Exception {
+  public void execute(ExecutionContext executionContext, int curItrCount) throws Exception {
     log.info("Executing rollback node {}", this.getName());
     // Can only be done with an instantiation of a new WriteClient hence cannot be done during DeltaStreamer
     // testing for now

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ScheduleCompactNode.java
Patch:
@@ -35,7 +35,7 @@ public ScheduleCompactNode(Config config) {
   }
 
   @Override
-  public void execute(ExecutionContext executionContext) throws Exception {
+  public void execute(ExecutionContext executionContext, int curItrCount) throws Exception {
     log.info("Executing schedule compact node {}", this.getName());
     // Can only be done with an instantiation of a new WriteClient hence cannot be done during DeltaStreamer
     // testing for now
@@ -48,7 +48,7 @@ public void execute(ExecutionContext executionContext) throws Exception {
       HoodieCommitMetadata metadata = org.apache.hudi.common.model.HoodieCommitMetadata.fromBytes(metaClient
           .getActiveTimeline().getInstantDetails(lastInstant.get()).get(), HoodieCommitMetadata.class);
       Option<String> scheduledInstant = executionContext.getHoodieTestSuiteWriter().scheduleCompaction(Option.of(metadata
-              .getExtraMetadata()));
+          .getExtraMetadata()));
       if (scheduledInstant.isPresent()) {
         log.info("Scheduling compaction instant {}", scheduledInstant.get());
       }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/SparkSQLQueryNode.java
Patch:
@@ -42,10 +42,11 @@ public SparkSQLQueryNode(Config config) {
    * Method helps to execute a sparkSql query from a hive table.
    *
    * @param executionContext Execution context to perform this query.
+   * @param curItrCount current iteration count.
    * @throws Exception will be thrown if ant error occurred
    */
   @Override
-  public void execute(ExecutionContext executionContext) throws Exception {
+  public void execute(ExecutionContext executionContext, int curItrCount) throws Exception {
     log.info("Executing spark sql query node");
     this.hiveServiceProvider.startLocalHiveServiceIfNeeded(executionContext.getHoodieTestSuiteWriter().getConfiguration());
     this.hiveServiceProvider.syncToLocalHiveIfNeeded(executionContext.getHoodieTestSuiteWriter());

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateNode.java
Patch:
@@ -40,9 +40,10 @@ public ValidateNode(Config config, Function<List<DagNode>, R> function) {
    * was set to true or default, but the parent nodes have not completed yet.
    *
    * @param executionContext Context to execute this node
+   * @param curItrCount current iteration count.
    */
   @Override
-  public void execute(ExecutionContext executionContext) {
+  public void execute(ExecutionContext executionContext, int curItrCount) {
     if (this.getParentNodes().size() > 0 && (Boolean) this.config.getOtherConfigs().getOrDefault("WAIT_FOR_PARENTS",
         true)) {
       for (DagNode node : (List<DagNode>) this.getParentNodes()) {

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/FlexibleSchemaRecordGenerationIterator.java
Patch:
@@ -70,7 +70,7 @@ public GenericRecord next() {
     if (lastRecord == null) {
       GenericRecord record = partitionPathsNonEmpty
           ? this.generator.getNewPayloadWithTimestamp(this.firstPartitionPathField)
-          : this.generator.getNewPayload();
+          : this.generator.getNewPayload(partitionPathFieldNames);
       lastRecord = record;
       return record;
     } else {

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/GenericRecordFullPayloadGenerator.java
Patch:
@@ -331,7 +331,7 @@ public boolean validate(GenericRecord record) {
    */
   public GenericRecord updateTimestamp(GenericRecord record, String fieldName) {
     long delta = TimeUnit.MILLISECONDS.convert(++partitionIndex % numDatePartitions, TimeUnit.DAYS);
-    record.put(fieldName, System.currentTimeMillis() - delta);
+    record.put(fieldName, (System.currentTimeMillis() - delta)/1000);
     return record;
   }
 

File: hudi-examples/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java
Patch:
@@ -46,9 +46,9 @@
 /**
  * Simple examples of #{@link HoodieJavaWriteClient}.
  *
- * Usage: HoodieWriteClientExample <tablePath> <tableName>
+ * Usage: HoodieJavaWriteClientExample <tablePath> <tableName>
  * <tablePath> and <tableName> describe root path of hudi and table name
- * for example, `HoodieWriteClientExample file:///tmp/hoodie/sample-table hoodie_rt`
+ * for example, `HoodieJavaWriteClientExample file:///tmp/hoodie/sample-table hoodie_rt`
  */
 public class HoodieJavaWriteClientExample {
 
@@ -58,7 +58,7 @@ public class HoodieJavaWriteClientExample {
 
   public static void main(String[] args) throws Exception {
     if (args.length < 2) {
-      System.err.println("Usage: HoodieWriteClientExample <tablePath> <tableName>");
+      System.err.println("Usage: HoodieJavaWriteClientExample <tablePath> <tableName>");
       System.exit(1);
     }
     String tablePath = args[0];

File: hudi-common/src/main/java/org/apache/hudi/common/model/DefaultHoodieRecordPayload.java
Patch:
@@ -42,7 +42,7 @@ public DefaultHoodieRecordPayload(GenericRecord record, Comparable orderingVal)
   }
 
   public DefaultHoodieRecordPayload(Option<GenericRecord> record) {
-    this(record.isPresent() ? record.get() : null, (record1) -> 0); // natural order
+    this(record.isPresent() ? record.get() : null, 0); // natural order
   }
 
   @Override

File: hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java
Patch:
@@ -248,6 +248,6 @@ public static TypedProperties flinkConf2TypedProperties(Configuration conf) {
 
   public static void checkRequiredProperties(TypedProperties props, List<String> checkPropNames) {
     checkPropNames.forEach(prop ->
-        Preconditions.checkState(!props.containsKey(prop), "Required property " + prop + " is missing"));
+        Preconditions.checkState(props.containsKey(prop), "Required property " + prop + " is missing"));
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodiePayloadConfig.java
Patch:
@@ -65,7 +65,7 @@ public Builder withPayloadOrderingField(String payloadOrderingField) {
 
     public HoodiePayloadConfig build() {
       HoodiePayloadConfig config = new HoodiePayloadConfig(props);
-      setDefaultOnCondition(props, !props.containsKey(PAYLOAD_ORDERING_FIELD_PROP), DEFAULT_PAYLOAD_ORDERING_FIELD_VAL,
+      setDefaultOnCondition(props, !props.containsKey(PAYLOAD_ORDERING_FIELD_PROP), PAYLOAD_ORDERING_FIELD_PROP,
           String.valueOf(DEFAULT_PAYLOAD_ORDERING_FIELD_VAL));
       return config;
     }

File: hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java
Patch:
@@ -98,7 +98,7 @@ public static Option<Pair<HoodieInstant, HoodieClusteringPlan>> getClusteringPla
 
   /**
    * Get filegroups to pending clustering instant mapping for all pending clustering plans.
-   * This includes all clustering operattions in 'requested' and 'inflight' states.
+   * This includes all clustering operations in 'requested' and 'inflight' states.
    */
   public static Map<HoodieFileGroupId, HoodieInstant> getAllFileGroupsInPendingClusteringPlans(
       HoodieTableMetaClient metaClient) {

File: hudi-flink/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java
Patch:
@@ -78,7 +78,7 @@ public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord
   private static final String DELIMITER = "_";
   private static final String INSTANT_MARKER_FOLDER_NAME = ".instant_marker";
   private transient boolean isMain = false;
-  private transient AtomicLong recordCounter = new AtomicLong(0);
+  private AtomicLong recordCounter = new AtomicLong(0);
   private StreamingRuntimeContext runtimeContext;
   private int indexOfThisSubtask;
 

File: hudi-flink/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java
Patch:
@@ -109,7 +109,7 @@ public void open() throws Exception {
     fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());
 
     if (isMain) {
-      TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(runtimeContext);
+      TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);
 
       // writeClient
       writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg), true);

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/embedded/EmbeddedTimelineServerHelper.java
Patch:
@@ -49,8 +49,8 @@ public static Option<EmbeddedTimelineService> createEmbeddedTimelineService(
       // Run Embedded Timeline Server
       LOG.info("Starting Timeline service !!");
       Option<String> hostAddr = context.getProperty(EngineProperty.EMBEDDED_SERVER_HOST);
-      timelineServer = Option.of(new EmbeddedTimelineService(context, hostAddr.orElse(null),
-          config.getEmbeddedTimelineServerPort(), config.getClientSpecifiedViewStorageConfig()));
+      timelineServer = Option.of(new EmbeddedTimelineService(context, hostAddr.orElse(null), config.getEmbeddedTimelineServerPort(),
+          config.getMetadataConfig(), config.getClientSpecifiedViewStorageConfig(), config.getBasePath()));
       timelineServer.get().startServer();
       updateWriteConfigWithTimelineServer(timelineServer.get(), config);
     }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataWriter.java
Patch:
@@ -29,7 +29,7 @@
 /**
  * Interface that supports updating metadata for a given table, as actions complete.
  */
-public interface HoodieTableMetadataWriter extends Serializable {
+public interface HoodieTableMetadataWriter extends Serializable, AutoCloseable {
 
   void update(HoodieCommitMetadata commitMetadata, String instantTime);
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/BaseCleanActionExecutor.java
Patch:
@@ -67,7 +67,7 @@ public BaseCleanActionExecutor(HoodieEngineContext context, HoodieWriteConfig co
    */
   HoodieCleanerPlan requestClean(HoodieEngineContext context) {
     try {
-      CleanPlanner<T, I, K, O> planner = new CleanPlanner<>(table, config);
+      CleanPlanner<T, I, K, O> planner = new CleanPlanner<>(context, table, config);
       Option<HoodieInstant> earliestInstant = planner.getEarliestCommitToRetain();
       List<String> partitionsToClean = planner.getPartitionPathsToClean(earliestInstant);
 

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/rollback/FlinkCopyOnWriteRollbackActionExecutor.java
Patch:
@@ -65,7 +65,7 @@ protected BaseRollbackActionExecutor.RollbackStrategy getRollbackStrategy() {
   @Override
   protected List<HoodieRollbackStat> executeRollbackUsingFileListing(HoodieInstant instantToRollback) {
     List<ListingBasedRollbackRequest> rollbackRequests = RollbackUtils.generateRollbackRequestsByListingCOW(
-        context, table.getMetaClient().getFs(), table.getMetaClient().getBasePath(), config);
+        context, table.getMetaClient().getBasePath(), config);
     return new ListingBasedRollbackHelper(table.getMetaClient(), config).performRollback(context, instantToRollback, rollbackRequests);
   }
 }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/upgrade/ZeroToOneUpgradeHandler.java
Patch:
@@ -93,8 +93,7 @@ private static void recreateMarkerFiles(final String commitInstantTime,
         // generate rollback stats
         List<ListingBasedRollbackRequest> rollbackRequests;
         if (table.getMetaClient().getTableType() == HoodieTableType.COPY_ON_WRITE) {
-          rollbackRequests = RollbackUtils.generateRollbackRequestsByListingCOW(context, table.getMetaClient().getFs(),
-              table.getMetaClient().getBasePath(), table.getConfig());
+          rollbackRequests = RollbackUtils.generateRollbackRequestsByListingCOW(context, table.getMetaClient().getBasePath(), table.getConfig());
         } else {
           rollbackRequests = RollbackUtils.generateRollbackRequestsUsingFileListingMOR(commitInstantOpt.get(), table, context);
         }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java
Patch:
@@ -99,8 +99,6 @@ protected void initialize(HoodieEngineContext engineContext, HoodieTableMetaClie
   @Override
   protected void commit(List<HoodieRecord> records, String partitionName, String instantTime) {
     ValidationUtils.checkState(enabled, "Metadata table cannot be committed to as it is not enabled");
-    metadata.closeReaders();
-
     JavaRDD<HoodieRecord> recordRDD = prepRecords(records, partitionName);
 
     try (SparkRDDWriteClient writeClient = new SparkRDDWriteClient(engineContext, metadataWriteConfig, true)) {

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/HoodieSparkMergeOnReadTableCompactor.java
Patch:
@@ -195,8 +195,7 @@ public HoodieCompactionPlan generateCompactionPlan(HoodieEngineContext context,
     // TODO - rollback any compactions in flight
     HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();
     LOG.info("Compacting " + metaClient.getBasePath() + " with commit " + compactionCommitTime);
-    List<String> partitionPaths = FSUtils.getAllPartitionPaths(context, metaClient.getFs(), metaClient.getBasePath(),
-        config.useFileListingMetadata(), config.getFileListingMetadataVerify(), config.shouldAssumeDatePartitioning());
+    List<String> partitionPaths = FSUtils.getAllPartitionPaths(context, config.getMetadataConfig(), metaClient.getBasePath());
 
     // filter the partition paths if needed to reduce list status
     partitionPaths = config.getCompactionStrategy().filterPartitionPaths(config, partitionPaths);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkCopyOnWriteRollbackActionExecutor.java
Patch:
@@ -67,7 +67,7 @@ protected BaseRollbackActionExecutor.RollbackStrategy getRollbackStrategy() {
   @Override
   protected List<HoodieRollbackStat> executeRollbackUsingFileListing(HoodieInstant instantToRollback) {
     List<ListingBasedRollbackRequest> rollbackRequests = RollbackUtils.generateRollbackRequestsByListingCOW(context,
-        table.getMetaClient().getFs(), table.getMetaClient().getBasePath(), config);
+        table.getMetaClient().getBasePath(), config);
     return new ListingBasedRollbackHelper(table.getMetaClient(), config).performRollback(context, instantToRollback, rollbackRequests);
   }
 }

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/upgrade/ZeroToOneUpgradeHandler.java
Patch:
@@ -92,8 +92,7 @@ private static void recreateMarkerFiles(final String commitInstantTime,
         // generate rollback stats
         List<ListingBasedRollbackRequest> rollbackRequests;
         if (table.getMetaClient().getTableType() == HoodieTableType.COPY_ON_WRITE) {
-          rollbackRequests = RollbackUtils.generateRollbackRequestsByListingCOW(context, table.getMetaClient().getFs(),
-              table.getMetaClient().getBasePath(), table.getConfig());
+          rollbackRequests = RollbackUtils.generateRollbackRequestsByListingCOW(context, table.getMetaClient().getBasePath(), table.getConfig());
         } else {
           rollbackRequests = RollbackUtils.generateRollbackRequestsUsingFileListingMOR(commitInstantOpt.get(), table, context);
         }

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestClientRollback.java
Patch:
@@ -101,8 +101,7 @@ public void testSavepointAndRollback() throws Exception {
       assertNoWriteErrors(statuses);
       HoodieWriteConfig config = getConfig();
       List<String> partitionPaths =
-          FSUtils.getAllPartitionPaths(context, fs, cfg.getBasePath(), config.useFileListingMetadata(),
-              config.getFileListingMetadataVerify(), config.shouldAssumeDatePartitioning());
+          FSUtils.getAllPartitionPaths(context, config.getMetadataConfig(), cfg.getBasePath());
       metaClient = HoodieTableMetaClient.reload(metaClient);
       HoodieSparkTable table = HoodieSparkTable.create(getConfig(), context, metaClient);
       final BaseFileOnlyView view1 = table.getBaseFileOnlyView();

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestHoodieMergeOnReadTable.java
Patch:
@@ -887,7 +887,7 @@ public void testMultiRollbackWithDeltaAndCompactionCommit() throws Exception {
   protected HoodieWriteConfig getHoodieWriteConfigWithSmallFileHandlingOff() {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
         .withDeleteParallelism(2)
-        .withAutoCommit(false).withAssumeDatePartitioning(true)
+        .withAutoCommit(false)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024)
             .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(1).build())
         .withEmbeddedTimelineServerEnabled(true)
@@ -1564,7 +1564,7 @@ protected HoodieWriteConfig.Builder getConfigBuilder(Boolean autoCommit, Boolean
                                                        long compactionSmallFileSize, HoodieClusteringConfig clusteringConfig) {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
         .withDeleteParallelism(2)
-        .withAutoCommit(autoCommit).withAssumeDatePartitioning(true)
+        .withAutoCommit(autoCommit)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(compactionSmallFileSize)
             .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(1).build())
         .withStorageConfig(HoodieStorageConfig.newBuilder().hfileMaxFileSize(1024 * 1024 * 1024).parquetMaxFileSize(1024 * 1024 * 1024).build())

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/CompactionTestBase.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.hudi.client.HoodieReadClient;
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.config.HoodieMetadataConfig;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieFileGroupId;
@@ -70,7 +71,8 @@ protected HoodieWriteConfig.Builder getConfigBuilder(Boolean autoCommit) {
     return HoodieWriteConfig.newBuilder().withPath(basePath)
         .withSchema(TRIP_EXAMPLE_SCHEMA)
         .withParallelism(2, 2)
-        .withAutoCommit(autoCommit).withAssumeDatePartitioning(true)
+        .withAutoCommit(autoCommit)
+        .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024 * 1024)
             .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(1).build())
         .withStorageConfig(HoodieStorageConfig.newBuilder()

File: hudi-common/src/main/java/org/apache/hudi/common/metrics/Metric.java
Patch:
@@ -18,9 +18,11 @@
 
 package org.apache.hudi.common.metrics;
 
+import java.io.Serializable;
+
 /**
  * Interface for Hudi Metric Types.
  */
-public interface Metric {
+public interface Metric extends Serializable {
   Long getValue();
 }
\ No newline at end of file

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java
Patch:
@@ -62,7 +62,7 @@ public SpillableMapBasedFileSystemView(HoodieTableMetaClient metaClient, HoodieT
     this.maxMemoryForBootstrapBaseFile = config.getMaxMemoryForBootstrapBaseFile();
     this.maxMemoryForReplaceFileGroups = config.getMaxMemoryForReplacedFileGroups();
     this.maxMemoryForClusteringFileGroups = config.getMaxMemoryForPendingClusteringFileGroups();
-    this.baseStoreDir = config.getBaseStoreDir();
+    this.baseStoreDir = config.getSpillableDir();
     init(metaClient, visibleActiveTimeline);
   }
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/reader/DFSHoodieDatasetInputReader.java
Patch:
@@ -86,9 +86,8 @@ public DFSHoodieDatasetInputReader(JavaSparkContext jsc, String basePath, String
   protected List<String> getPartitions(Option<Integer> partitionsLimit) throws IOException {
     // Using FSUtils.getFS here instead of metaClient.getFS() since we dont want to count these listStatus
     // calls in metrics as they are not part of normal HUDI operation.
-    FileSystem fs = FSUtils.getFs(metaClient.getBasePath(), metaClient.getHadoopConf());
     HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);
-    List<String> partitionPaths = FSUtils.getAllPartitionPaths(engineContext, fs, metaClient.getBasePath(),
+    List<String> partitionPaths = FSUtils.getAllPartitionPaths(engineContext, metaClient.getBasePath(),
         HoodieMetadataConfig.DEFAULT_METADATA_ENABLE_FOR_READERS, HoodieMetadataConfig.DEFAULT_METADATA_VALIDATE, false);
     // Sort partition so we can pick last N partitions by default
     Collections.sort(partitionPaths);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java
Patch:
@@ -98,7 +98,7 @@ public void snapshot(JavaSparkContext jsc, String baseDir, final String outputDi
     LOG.info(String.format("Starting to snapshot latest version files which are also no-late-than %s.",
         latestCommitTimestamp));
 
-    List<String> partitions = FSUtils.getAllPartitionPaths(context, fs, baseDir, useFileListingFromMetadata, verifyMetadataFileListing, shouldAssumeDatePartitioning);
+    List<String> partitions = FSUtils.getAllPartitionPaths(context, baseDir, useFileListingFromMetadata, verifyMetadataFileListing, shouldAssumeDatePartitioning);
     if (partitions.size() > 0) {
       LOG.info(String.format("The job needs to copy %d partitions.", partitions.size()));
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java
Patch:
@@ -129,7 +129,7 @@ public void export(JavaSparkContext jsc, Config cfg) throws IOException {
     LOG.info(String.format("Starting to snapshot latest version files which are also no-late-than %s.",
         latestCommitTimestamp));
 
-    final List<String> partitions = getPartitions(engineContext, fs, cfg);
+    final List<String> partitions = getPartitions(engineContext, cfg);
     if (partitions.isEmpty()) {
       throw new HoodieSnapshotExporterException("The source dataset has 0 partition to snapshot.");
     }
@@ -154,8 +154,8 @@ private Option<String> getLatestCommitTimestamp(FileSystem fs, Config cfg) {
     return latestCommit.isPresent() ? Option.of(latestCommit.get().getTimestamp()) : Option.empty();
   }
 
-  private List<String> getPartitions(HoodieEngineContext engineContext, FileSystem fs, Config cfg) throws IOException {
-    return FSUtils.getAllPartitionPaths(engineContext, fs, cfg.sourceBasePath, true, false, false);
+  private List<String> getPartitions(HoodieEngineContext engineContext, Config cfg) {
+    return FSUtils.getAllPartitionPaths(engineContext, cfg.sourceBasePath, true, false, false);
   }
 
   private void createSuccessTag(FileSystem fs, Config cfg) throws IOException {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java
Patch:
@@ -87,7 +87,7 @@ private void setHostAddrFromSparkConf(SparkConf sparkConf) {
   public void run() throws IOException {
     JavaSparkContext jsc = UtilHelpers.buildSparkContext("hudi-view-perf-" + cfg.basePath, cfg.sparkMaster);
     HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);
-    List<String> allPartitionPaths = FSUtils.getAllPartitionPaths(engineContext, timelineServer.getFs(), cfg.basePath,
+    List<String> allPartitionPaths = FSUtils.getAllPartitionPaths(engineContext, cfg.basePath,
         cfg.useFileListingFromMetadata, cfg.verifyMetadataFileListing, true);
     Collections.shuffle(allPartitionPaths);
     List<String> selected = allPartitionPaths.stream().filter(p -> !p.contains("error")).limit(cfg.maxPartitions)

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java
Patch:
@@ -277,9 +277,9 @@ public HoodieIndexConfig build() {
           BLOOM_INDEX_BUCKETIZED_CHECKING_PROP, DEFAULT_BLOOM_INDEX_BUCKETIZED_CHECKING);
       setDefaultOnCondition(props, !props.containsKey(BLOOM_INDEX_KEYS_PER_BUCKET_PROP),
           BLOOM_INDEX_KEYS_PER_BUCKET_PROP, DEFAULT_BLOOM_INDEX_KEYS_PER_BUCKET);
-      setDefaultOnCondition(props, !props.contains(BLOOM_INDEX_FILTER_TYPE),
+      setDefaultOnCondition(props, !props.containsKey(BLOOM_INDEX_FILTER_TYPE),
           BLOOM_INDEX_FILTER_TYPE, DEFAULT_BLOOM_INDEX_FILTER_TYPE);
-      setDefaultOnCondition(props, !props.contains(HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES),
+      setDefaultOnCondition(props, !props.containsKey(HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES),
           HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES, DEFAULT_HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES);
       setDefaultOnCondition(props, !props.containsKey(SIMPLE_INDEX_PARALLELISM_PROP), SIMPLE_INDEX_PARALLELISM_PROP,
           DEFAULT_SIMPLE_INDEX_PARALLELISM);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/hbase/TestHBaseIndex.java
Patch:
@@ -324,7 +324,7 @@ public void testSimpleTagLocationWithInvalidCommit() throws Exception {
 
     String newCommitTime = writeClient.startCommit();
     // make a commit with 199 records
-    JavaRDD<HoodieRecord> writeRecords = generateAndCommitRecords(writeClient, 199);
+    JavaRDD<HoodieRecord> writeRecords = generateAndCommitRecords(writeClient, 199, newCommitTime);
 
     // make a second commit with a single record
     String invalidCommit = writeClient.startCommit();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/transform/Transformer.java
Patch:
@@ -44,5 +44,5 @@ public interface Transformer {
    * @return Transformed Dataset
    */
   @PublicAPIMethod(maturity = ApiMaturityLevel.STABLE)
-  Dataset apply(JavaSparkContext jsc, SparkSession sparkSession, Dataset<Row> rowDataset, TypedProperties properties);
+  Dataset<Row> apply(JavaSparkContext jsc, SparkSession sparkSession, Dataset<Row> rowDataset, TypedProperties properties);
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java
Patch:
@@ -36,7 +36,6 @@
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
@@ -365,7 +364,6 @@ private void syncFromInstants(HoodieTableMetaClient datasetMetaClient) {
       LOG.info("Syncing " + instantsToSync.size() + " instants to metadata table: " + instantsToSync);
 
       // Read each instant in order and sync it to metadata table
-      final HoodieActiveTimeline timeline = datasetMetaClient.getActiveTimeline();
       for (HoodieInstant instant : instantsToSync) {
         LOG.info("Syncing instant " + instant + " to metadata table");
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkCompactor.java
Patch:
@@ -45,7 +45,7 @@ public HoodieSparkCompactor(AbstractHoodieWriteClient<T, JavaRDD<HoodieRecord<T>
   @Override
   public void compact(HoodieInstant instant) throws IOException {
     LOG.info("Compactor executing compaction " + instant);
-    SparkRDDWriteClient<T> writeClient = (SparkRDDWriteClient<T>)compactionClient;
+    SparkRDDWriteClient<T> writeClient = (SparkRDDWriteClient<T>) compactionClient;
     JavaRDD<WriteStatus> res = writeClient.compact(instant.getTimestamp());
     this.context.setJobStatus(this.getClass().getSimpleName(), "Collect compaction write status");
     long numWriteErrors = res.collect().stream().filter(WriteStatus::hasErrors).count();

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java
Patch:
@@ -207,7 +207,6 @@ List<PartitionEvent> getPartitionEvents(List<Partition> tablePartitions, List<St
     Map<String, String> paths = new HashMap<>();
     for (Partition tablePartition : tablePartitions) {
       List<String> hivePartitionValues = tablePartition.getValues();
-      Collections.sort(hivePartitionValues);
       String fullTablePartitionPath =
           Path.getPathWithoutSchemeAndAuthority(new Path(tablePartition.getSd().getLocation())).toUri().getPath();
       paths.put(String.join(", ", hivePartitionValues), fullTablePartitionPath);
@@ -219,7 +218,6 @@ List<PartitionEvent> getPartitionEvents(List<Partition> tablePartitions, List<St
       String fullStoragePartitionPath = Path.getPathWithoutSchemeAndAuthority(storagePartitionPath).toUri().getPath();
       // Check if the partition values or if hdfs path is the same
       List<String> storagePartitionValues = partitionValueExtractor.extractPartitionValuesInPath(storagePartition);
-      Collections.sort(storagePartitionValues);
       if (!storagePartitionValues.isEmpty()) {
         String storageValue = String.join(", ", storagePartitionValues);
         if (!paths.containsKey(storageValue)) {

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java
Patch:
@@ -428,10 +428,12 @@ public static Object getNestedFieldVal(GenericRecord record, String fieldName, b
 
     if (returnNullIfNotFound) {
       return null;
-    } else {
+    } else if (valueNode.getSchema().getField(parts[i]) == null) {
       throw new HoodieException(
           fieldName + "(Part -" + parts[i] + ") field not found in record. Acceptable fields were :"
               + valueNode.getSchema().getFields().stream().map(Field::name).collect(Collectors.toList()));
+    } else {
+      throw new HoodieException("The value of " + parts[i] + " can not be null");
     }
   }
 

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java
Patch:
@@ -29,7 +29,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.metadata.HoodieBackedTableMetadata;
 import org.apache.hudi.metadata.HoodieTableMetadata;
-import org.apache.hudi.metrics.SparkHoodieBackedTableMetadataWriter;
+import org.apache.hudi.metadata.SparkHoodieBackedTableMetadataWriter;
 
 import org.apache.spark.api.java.JavaSparkContext;
 import org.springframework.shell.core.CommandMarker;
@@ -117,7 +117,7 @@ public String delete() throws Exception {
       // Metadata directory does not exist
     }
 
-    return String.format("Removed Metdata Table from %s", metadataPath);
+    return String.format("Removed Metadata Table from %s", metadataPath);
   }
 
   @CliCommand(value = "metadata init", help = "Update the metadata table from commits since the creation")

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java
Patch:
@@ -44,7 +44,7 @@
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.index.SparkHoodieIndex;
 import org.apache.hudi.metrics.DistributedRegistry;
-import org.apache.hudi.metrics.SparkHoodieBackedTableMetadataWriter;
+import org.apache.hudi.metadata.SparkHoodieBackedTableMetadataWriter;
 import org.apache.hudi.table.BulkInsertPartitioner;
 import org.apache.hudi.table.HoodieSparkTable;
 import org.apache.hudi.table.HoodieTable;

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java
Patch:
@@ -206,7 +206,7 @@ public static String getRelativePartitionPath(Path basePath, Path fullPartitionP
   public static List<String> getAllFoldersWithPartitionMetaFile(FileSystem fs, String basePathStr) throws IOException {
     // If the basePathStr is a folder within the .hoodie directory then we are listing partitions within an
     // internal table.
-    final boolean isMetadataTable = basePathStr.contains(HoodieTableMetaClient.METAFOLDER_NAME);
+    final boolean isMetadataTable = HoodieTableMetadata.isMetadataTable(basePathStr);
     final Path basePath = new Path(basePathStr);
     final List<String> partitions = new ArrayList<>();
     processFiles(fs, basePathStr, (locatedFileStatus) -> {

File: hudi-common/src/main/java/org/apache/hudi/metadata/BaseTableMetadata.java
Patch:
@@ -270,10 +270,10 @@ private void openTimelineScanner() throws IOException {
     }
 
     HoodieTableMetaClient datasetMetaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);
-    List<HoodieInstant> unsyncedInstants = findInstantsToSync(datasetMetaClient);
+    List<HoodieInstant> unSyncedInstants = findInstantsToSync(datasetMetaClient);
     Schema schema = HoodieAvroUtils.addMetadataFields(HoodieMetadataRecord.getClassSchema());
     timelineRecordScanner =
-        new HoodieMetadataMergedInstantRecordScanner(datasetMetaClient, unsyncedInstants, getSyncedInstantTime(), schema, MAX_MEMORY_SIZE_IN_BYTES, spillableMapDirectory, null);
+        new HoodieMetadataMergedInstantRecordScanner(datasetMetaClient, unSyncedInstants, getSyncedInstantTime(), schema, MAX_MEMORY_SIZE_IN_BYTES, spillableMapDirectory, null);
   }
 
   protected List<HoodieInstant> findInstantsToSync() {

File: hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java
Patch:
@@ -171,7 +171,7 @@ public List<String> getFilenames() {
    * Returns the list of filenames deleted as part of this record.
    */
   public List<String> getDeletions() {
-    return filterFileInfoEntries(true).map(e -> e.getKey()).sorted().collect(Collectors.toList());
+    return filterFileInfoEntries(true).map(Map.Entry::getKey).sorted().collect(Collectors.toList());
   }
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -277,7 +277,7 @@ public SyncableFileSystemView getHoodieView() {
   private SyncableFileSystemView getFileSystemViewInternal(HoodieTimeline timeline) {
     if (config.useFileListingMetadata()) {
       FileSystemViewStorageConfig viewConfig = config.getViewStorageConfig();
-      return new HoodieMetadataFileSystemView(metaClient, this.metadata, timeline, viewConfig.isIncrementalTimelineSyncEnabled());
+      return new HoodieMetadataFileSystemView(metaClient, this.metadata(), timeline, viewConfig.isIncrementalTimelineSyncEnabled());
     } else {
       return getViewManager().getFileSystemView(metaClient);
     }

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java
Patch:
@@ -107,7 +107,7 @@ public List<WriteStatus> upsert(List<HoodieRecord<T>> records, String instantTim
         getTableAndInitCtx(WriteOperationType.UPSERT, instantTime);
     table.validateUpsertSchema();
     setOperationType(WriteOperationType.UPSERT);
-    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);
+    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this);
     HoodieWriteMetadata<List<WriteStatus>> result = table.upsert(context, instantTime, records);
     if (result.getIndexLookupDuration().isPresent()) {
       metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());
@@ -126,7 +126,7 @@ public List<WriteStatus> insert(List<HoodieRecord<T>> records, String instantTim
         getTableAndInitCtx(WriteOperationType.INSERT, instantTime);
     table.validateUpsertSchema();
     setOperationType(WriteOperationType.INSERT);
-    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);
+    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this);
     HoodieWriteMetadata<List<WriteStatus>> result = table.insert(context, instantTime, records);
     if (result.getIndexLookupDuration().isPresent()) {
       metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.common.table.log;
 
 import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.common.fs.TimedFSDataInputStream;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;
 import org.apache.hudi.common.table.log.block.HoodieCommandBlock;
@@ -73,8 +74,8 @@ public HoodieLogFileReader(FileSystem fs, HoodieLogFile logFile, Schema readerSc
       boolean readBlockLazily, boolean reverseReader) throws IOException {
     FSDataInputStream fsDataInputStream = fs.open(logFile.getPath(), bufferSize);
     if (fsDataInputStream.getWrappedStream() instanceof FSInputStream) {
-      this.inputStream = new FSDataInputStream(
-          new BufferedFSInputStream((FSInputStream) fsDataInputStream.getWrappedStream(), bufferSize));
+      this.inputStream = new TimedFSDataInputStream(logFile.getPath(), new FSDataInputStream(
+          new BufferedFSInputStream((FSInputStream) fsDataInputStream.getWrappedStream(), bufferSize)));
     } else {
       // fsDataInputStream.getWrappedStream() maybe a BufferedFSInputStream
       // need to wrap in another BufferedFSInputStream the make bufferSize work?

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.io.storage.HoodieHFileReader;
 import org.apache.log4j.LogManager;
@@ -118,6 +119,8 @@ protected byte[] serializeRecords() throws IOException {
         recordKey = record.get(keyField.pos()).toString();
       }
       byte[] recordBytes = HoodieAvroUtils.indexedRecordToBytes(record);
+      ValidationUtils.checkState(!sortedRecordsMap.containsKey(recordKey),
+          "Writing multiple records with same key not supported for " + this.getClass().getName());
       sortedRecordsMap.put(recordKey, recordBytes);
     }
 

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestHoodieLogFileCommand.java
Patch:
@@ -104,7 +104,7 @@ public void init() throws IOException, InterruptedException, URISyntaxException
       header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, INSTANT_TIME);
       header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
       dataBlock = new HoodieAvroDataBlock(records, header);
-      writer = writer.appendBlock(dataBlock);
+      writer.appendBlock(dataBlock);
     } finally {
       if (writer != null) {
         writer.close();
@@ -183,7 +183,7 @@ public void testShowLogFileRecordsWithMerge() throws IOException, InterruptedExc
       header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, INSTANT_TIME);
       header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
       HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);
-      writer = writer.appendBlock(dataBlock);
+      writer.appendBlock(dataBlock);
     } finally {
       if (writer != null) {
         writer.close();

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java
Patch:
@@ -31,6 +31,7 @@
 
 import java.io.IOException;
 import java.util.Iterator;
+import java.util.List;
 import java.util.Map;
 import java.util.PriorityQueue;
 import java.util.Queue;
@@ -101,7 +102,7 @@ public void write(GenericRecord oldRecord) {
   }
 
   @Override
-  public WriteStatus close() {
+  public List<WriteStatus> close() {
     // write out any pending records (this can happen when inserts are turned into updates)
     newRecordKeysSorted.stream().forEach(key -> {
       try {

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java
Patch:
@@ -106,7 +106,7 @@ private Writer openWriter() {
       } else {
         return this.writer;
       }
-    } catch (InterruptedException | IOException e) {
+    } catch (IOException e) {
       throw new HoodieException("Unable to initialize HoodieLogFormat writer", e);
     }
   }
@@ -335,7 +335,7 @@ private void writeToFile(Schema wrapperSchema, List<IndexedRecord> records) thro
       Map<HeaderMetadataType, String> header = new HashMap<>();
       header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, wrapperSchema.toString());
       HoodieAvroDataBlock block = new HoodieAvroDataBlock(records, header);
-      this.writer = writer.appendBlock(block);
+      writer.appendBlock(block);
       records.clear();
     }
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/AbstractMarkerBasedRollbackStrategy.java
Patch:
@@ -108,7 +108,7 @@ protected HoodieRollbackStat undoAppend(String appendBaseFilePath, HoodieInstant
       // generate metadata
       Map<HoodieLogBlock.HeaderMetadataType, String> header = RollbackUtils.generateHeader(instantToRollback.getTimestamp(), instantTime);
       // if update belongs to an existing log file
-      writer = writer.appendBlock(new HoodieCommandBlock(header));
+      writer.appendBlock(new HoodieCommandBlock(header));
     } finally {
       try {
         if (writer != null) {

File: hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java
Patch:
@@ -128,7 +128,7 @@ Map<String, HoodieRollbackStat> maybeDeleteAndCollectStats(HoodieEngineContext c
             if (doDelete) {
               Map<HoodieLogBlock.HeaderMetadataType, String> header = generateHeader(instantToRollback.getTimestamp());
               // if update belongs to an existing log file
-              writer = writer.appendBlock(new HoodieCommandBlock(header));
+              writer.appendBlock(new HoodieCommandBlock(header));
             }
           } catch (IOException | InterruptedException io) {
             throw new HoodieRollbackException("Failed to rollback for instant " + instantToRollback, io);

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java
Patch:
@@ -291,7 +291,7 @@ private BootstrapWriteStatus handleMetadataBootstrap(String srcPartitionPath, St
                                                        HoodieFileStatus srcFileStatus, KeyGeneratorInterface keyGenerator) {
 
     Path sourceFilePath = FileStatusUtils.toPath(srcFileStatus.getPath());
-    HoodieBootstrapHandle bootstrapHandle = new HoodieBootstrapHandle(config, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS,
+    HoodieBootstrapHandle<?,?,?,?> bootstrapHandle = new HoodieBootstrapHandle(config, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS,
         table, partitionPath, FSUtils.createNewFileIdPfx(), table.getTaskContextSupplier());
     Schema avroSchema = null;
     try {
@@ -329,7 +329,8 @@ private BootstrapWriteStatus handleMetadataBootstrap(String srcPartitionPath, St
     } catch (IOException e) {
       throw new HoodieIOException(e.getMessage(), e);
     }
-    BootstrapWriteStatus writeStatus = (BootstrapWriteStatus)bootstrapHandle.getWriteStatus();
+
+    BootstrapWriteStatus writeStatus = (BootstrapWriteStatus) bootstrapHandle.writeStatuses().get(0);
     BootstrapFileMapping bootstrapFileMapping = new BootstrapFileMapping(
         config.getBootstrapSourceBasePath(), srcPartitionPath, partitionPath,
         srcFileStatus, writeStatus.getFileId());

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/AbstractSparkDeltaCommitActionExecutor.java
Patch:
@@ -79,11 +79,10 @@ public Iterator<List<WriteStatus>> handleUpdate(String partitionPath, String fil
       LOG.info("Small file corrections for updates for commit " + instantTime + " for file " + fileId);
       return super.handleUpdate(partitionPath, fileId, recordItr);
     } else {
-      HoodieAppendHandle appendHandle = new HoodieAppendHandle<>(config, instantTime, table,
+      HoodieAppendHandle<?,?,?,?> appendHandle = new HoodieAppendHandle<>(config, instantTime, table,
           partitionPath, fileId, recordItr, taskContextSupplier);
       appendHandle.doAppend();
-      appendHandle.close();
-      return Collections.singletonList(Collections.singletonList(appendHandle.getWriteStatus())).iterator();
+      return Collections.singletonList(appendHandle.close()).iterator();
     }
   }
 

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java
Patch:
@@ -128,7 +128,7 @@ JavaPairRDD<String, HoodieRollbackStat> maybeDeleteAndCollectStats(HoodieEngineC
             if (doDelete) {
               Map<HeaderMetadataType, String> header = generateHeader(instantToRollback.getTimestamp());
               // if update belongs to an existing log file
-              writer = writer.appendBlock(new HoodieCommandBlock(header));
+              writer.appendBlock(new HoodieCommandBlock(header));
             }
           } catch (IOException | InterruptedException io) {
             throw new HoodieRollbackException("Failed to rollback for instant " + instantToRollback, io);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java
Patch:
@@ -84,10 +84,10 @@ private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IO
       }
       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()
           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));
-      HoodieCreateHandle createHandle =
+      HoodieCreateHandle<?,?,?,?> createHandle =
           new HoodieCreateHandle(config, "100", table, insertRecords.get(0).getPartitionPath(), "f1-0", insertRecordMap, supplier);
       createHandle.write();
-      return createHandle.close();
+      return createHandle.close().get(0);
     }).collect();
 
     final Path commitFile = new Path(config.getBasePath() + "/.hoodie/" + HoodieTimeline.makeCommitFileName("100"));

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java
Patch:
@@ -71,7 +71,7 @@ public class HoodieWriteStat implements Serializable {
   private long numInserts;
 
   /**
-   * Total size of file written.
+   * Total number of bytes written.
    */
   private long totalWriteBytes;
 

File: hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormatAppendFailure.java
Patch:
@@ -110,7 +110,7 @@ public void testFailedToGetAppendStreamFromHDFSNameNode()
         .withFileExtension(HoodieArchivedLogFile.ARCHIVE_EXTENSION).withFileId("commits.archive")
         .overBaseCommit("").withFs(fs).build();
 
-    writer = writer.appendBlock(dataBlock);
+    writer.appendBlock(dataBlock);
     // get the current log file version to compare later
     int logFileVersion = writer.getLogFile().getLogVersion();
     Path logFilePath = writer.getLogFile().getPath();

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieDeltaStreamerWrapper.java
Patch:
@@ -25,13 +25,13 @@
 import org.apache.hudi.utilities.deltastreamer.DeltaSync;
 import org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer;
 import org.apache.hudi.utilities.schema.SchemaProvider;
+
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 
 /**
- * Extends the {@link HoodieDeltaStreamer} to expose certain operations helpful in running the Test Suite.
- * This is done to achieve 2 things 1) Leverage some components of {@link HoodieDeltaStreamer} 2)
- * Piggyback on the suite to test {@link HoodieDeltaStreamer}
+ * Extends the {@link HoodieDeltaStreamer} to expose certain operations helpful in running the Test Suite. This is done to achieve 2 things 1) Leverage some components of {@link HoodieDeltaStreamer}
+ * 2) Piggyback on the suite to test {@link HoodieDeltaStreamer}
  */
 public class HoodieDeltaStreamerWrapper extends HoodieDeltaStreamer {
 

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/dag/TestDagUtils.java
Patch:
@@ -47,6 +47,9 @@ public void testConvertDagToYaml() throws Exception {
   public void testConvertYamlToDag() throws Exception {
     WorkflowDag dag = DagUtils.convertYamlToDag(UtilitiesTestBase.Helpers
         .readFileFromAbsolutePath((System.getProperty("user.dir") + "/.." + COW_DAG_DOCKER_DEMO_RELATIVE_PATH)));
+    assertEquals(dag.getDagName(), "unit-test-cow-dag");
+    assertEquals(dag.getRounds(), 1);
+    assertEquals(dag.getIntermittentDelayMins(), 10);
     assertEquals(dag.getNodeList().size(), 1);
     Assertions.assertEquals(((DagNode) dag.getNodeList().get(0)).getParentNodes().size(), 0);
     assertEquals(((DagNode) dag.getNodeList().get(0)).getChildNodes().size(), 1);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/FilebasedSchemaProvider.java
Patch:
@@ -46,9 +46,9 @@ public static class Config {
 
   private final FileSystem fs;
 
-  private final Schema sourceSchema;
+  protected Schema sourceSchema;
 
-  private Schema targetSchema;
+  protected Schema targetSchema;
 
   public FilebasedSchemaProvider(TypedProperties props, JavaSparkContext jssc) {
     super(props, jssc);

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/hbase/TestHBaseIndex.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -119,6 +120,8 @@ public static void init() throws Exception {
   public void setUp() throws Exception {
     hadoopConf = jsc().hadoopConfiguration();
     hadoopConf.addResource(utility.getConfiguration());
+    // reInit the context here to keep the hadoopConf the same with that in this class
+    context = new HoodieSparkEngineContext(jsc());
     metaClient = getHoodieMetaClient(hadoopConf, basePath());
     dataGen = new HoodieTestDataGenerator();
   }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java
Patch:
@@ -845,7 +845,8 @@ protected void setWriteSchemaForDeletes(HoodieTableMetaClient metaClient) {
     try {
       HoodieActiveTimeline activeTimeline = metaClient.getActiveTimeline();
       Option<HoodieInstant> lastInstant =
-          activeTimeline.filterCompletedInstants().filter(s -> s.getAction().equals(metaClient.getCommitActionType()))
+          activeTimeline.filterCompletedInstants().filter(s -> s.getAction().equals(metaClient.getCommitActionType())
+          || s.getAction().equals(HoodieActiveTimeline.REPLACE_COMMIT_ACTION))
               .lastInstant();
       if (lastInstant.isPresent()) {
         HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -225,7 +225,8 @@ public void write(GenericRecord oldRecord) {
       HoodieRecord<T> hoodieRecord = new HoodieRecord<>(keyToNewRecords.get(key));
       try {
         Option<IndexedRecord> combinedAvroRecord =
-            hoodieRecord.getData().combineAndGetUpdateValue(oldRecord, useWriterSchema ? writerSchemaWithMetafields : writerSchema);
+            hoodieRecord.getData().combineAndGetUpdateValue(oldRecord, useWriterSchema ? writerSchemaWithMetafields : writerSchema,
+                config.getPayloadConfig().getProps());
         if (writeUpdateRecord(hoodieRecord, combinedAvroRecord)) {
           /*
            * ONLY WHEN 1) we have an update for this key AND 2) We are able to successfully write the the combined new

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java
Patch:
@@ -442,7 +442,7 @@ public static Object getNestedFieldVal(GenericRecord record, String fieldName, b
    * @param fieldValue avro field value
    * @return field value either converted (for certain data types) or as it is.
    */
-  private static Object convertValueForSpecificDataTypes(Schema fieldSchema, Object fieldValue) {
+  public static Object convertValueForSpecificDataTypes(Schema fieldSchema, Object fieldValue) {
     if (fieldSchema == null) {
       return fieldValue;
     }

File: hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.ReflectionUtils;
 import org.apache.hudi.config.HoodieCompactionConfig;
+import org.apache.hudi.config.HoodiePayloadConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.exception.HoodieNotSupportedException;
@@ -135,6 +136,8 @@ public static HoodieWriteConfig getHoodieClientConfig(HoodieFlinkStreamer.Config
     HoodieWriteConfig.Builder builder =
         HoodieWriteConfig.newBuilder().withEngineType(EngineType.FLINK).withPath(cfg.targetBasePath).combineInput(cfg.filterDupes, true)
             .withCompactionConfig(HoodieCompactionConfig.newBuilder().withPayloadClass(cfg.payloadClassName).build())
+            .withPayloadConfig(HoodiePayloadConfig.newBuilder().withPayloadOrderingField(cfg.sourceOrderingField)
+                .build())
             .forTable(cfg.targetTableName)
             .withAutoCommit(false)
             .withProps(readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -41,6 +41,7 @@
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieCompactionConfig;
+import org.apache.hudi.config.HoodiePayloadConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.hive.HiveSyncConfig;
@@ -619,6 +620,8 @@ private HoodieWriteConfig getHoodieClientConfig(Schema schema) {
             .withCompactionConfig(HoodieCompactionConfig.newBuilder().withPayloadClass(cfg.payloadClassName)
                 // Inline compaction is disabled for continuous mode. otherwise enabled for MOR
                 .withInlineCompaction(cfg.isInlineCompactionEnabled()).build())
+            .withPayloadConfig(HoodiePayloadConfig.newBuilder().withPayloadOrderingField(cfg.sourceOrderingField)
+                .build())
             .forTable(cfg.targetTableName)
             .withAutoCommit(autoCommit).withProps(props);
 

File: hudi-flink/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java
Patch:
@@ -196,7 +196,7 @@ private void doCheck() throws InterruptedException {
         return;
       }
     }
-    throw new InterruptedException("Last instant costs more than ten second, stop task now");
+    throw new InterruptedException(String.format("Last instant costs more than %s second, stop task now", retryTimes * retryInterval));
   }
 
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -460,7 +460,7 @@ protected void reconcileAgainstMarkers(HoodieEngineContext context,
       if (!invalidDataPaths.isEmpty()) {
         LOG.info("Removing duplicate data files created due to spark retries before committing. Paths=" + invalidDataPaths);
         Map<String, List<Pair<String, String>>> invalidPathsByPartition = invalidDataPaths.stream()
-            .map(dp -> Pair.of(new Path(dp).getParent().toString(), new Path(basePath, dp).toString()))
+            .map(dp -> Pair.of(new Path(basePath, dp).getParent().toString(), new Path(basePath, dp).toString()))
             .collect(Collectors.groupingBy(Pair::getKey));
 
         // Ensure all files in delete list is actually present. This is mandatory for an eventually consistent FS.

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java
Patch:
@@ -111,7 +111,7 @@ public void snapshot(JavaSparkContext jsc, String baseDir, final String outputDi
 
         // also need to copy over partition metadata
         Path partitionMetaFile =
-            new Path(new Path(baseDir, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);
+            new Path(FSUtils.getPartitionPath(baseDir, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);
         if (fs1.exists(partitionMetaFile)) {
           filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));
         }
@@ -122,7 +122,7 @@ public void snapshot(JavaSparkContext jsc, String baseDir, final String outputDi
       context.foreach(filesToCopy, tuple -> {
         String partition = tuple._1();
         Path sourceFilePath = new Path(tuple._2());
-        Path toPartitionPath = new Path(outputDir, partition);
+        Path toPartitionPath = FSUtils.getPartitionPath(outputDir, partition);
         FileSystem ifs = FSUtils.getFs(baseDir, serConf.newCopy());
 
         if (!ifs.exists(toPartitionPath)) {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java
Patch:
@@ -208,7 +208,7 @@ private void exportAsHudi(JavaSparkContext jsc, Config cfg, List<String> partiti
       dataFiles.forEach(hoodieDataFile -> filePaths.add(new Tuple2<>(partition, hoodieDataFile.getPath())));
       // also need to copy over partition metadata
       Path partitionMetaFile =
-          new Path(new Path(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);
+          new Path(FSUtils.getPartitionPath(cfg.sourceBasePath, partition), HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE);
       FileSystem fs = FSUtils.getFs(cfg.sourceBasePath, serConf.newCopy());
       if (fs.exists(partitionMetaFile)) {
         filePaths.add(new Tuple2<>(partition, partitionMetaFile.toString()));
@@ -219,7 +219,7 @@ private void exportAsHudi(JavaSparkContext jsc, Config cfg, List<String> partiti
     context.foreach(files, tuple -> {
       String partition = tuple._1();
       Path sourceFilePath = new Path(tuple._2());
-      Path toPartitionPath = new Path(cfg.targetOutputPath, partition);
+      Path toPartitionPath = FSUtils.getPartitionPath(cfg.targetOutputPath, partition);
       FileSystem fs = FSUtils.getFs(cfg.targetOutputPath, serConf.newCopy());
 
       if (!fs.exists(toPartitionPath)) {

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/storage/TestHoodieInternalRowParquetWriter.java
Patch:
@@ -35,7 +35,6 @@
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
 
-import java.io.IOException;
 import java.util.List;
 import java.util.Random;
 import java.util.UUID;
@@ -64,7 +63,7 @@ public void tearDown() throws Exception {
   }
 
   @Test
-  public void endToEndTest() throws IOException {
+  public void endToEndTest() throws Exception {
     HoodieWriteConfig cfg = SparkDatasetTestUtils.getConfigBuilder(basePath).build();
     for (int i = 0; i < 5; i++) {
       // init write support and parquet config

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java
Patch:
@@ -171,7 +171,7 @@ private <T> T executeRequest(String requestPath, Map<String, String> queryParame
         break;
     }
     String content = response.returnContent().asString();
-    return mapper.readValue(content, reference);
+    return (T) mapper.readValue(content, reference);
   }
 
   private Map<String, String> getParamsWithPartitionPath(String partitionPath) {

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/reader/SparkBasedReader.java
Patch:
@@ -20,7 +20,7 @@
 
 import java.util.List;
 import org.apache.avro.generic.GenericRecord;
-import org.apache.hudi.AvroConversionUtils;
+import org.apache.hudi.HoodieSparkUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.utilities.schema.RowBasedSchemaProvider;
 import org.apache.spark.api.java.JavaRDD;
@@ -49,7 +49,7 @@ public static JavaRDD<GenericRecord> readAvro(SparkSession sparkSession, String
         .option(AVRO_SCHEMA_OPTION_KEY, schemaStr)
         .load(JavaConverters.asScalaIteratorConverter(listOfPaths.iterator()).asScala().toSeq());
 
-    return AvroConversionUtils
+    return HoodieSparkUtils
         .createRdd(dataSet.toDF(), structName.orElse(RowBasedSchemaProvider.HOODIE_RECORD_STRUCT_NAME),
             nameSpace.orElse(RowBasedSchemaProvider.HOODIE_RECORD_NAMESPACE))
         .toJavaRDD();
@@ -61,7 +61,7 @@ public static JavaRDD<GenericRecord> readParquet(SparkSession sparkSession, List
     Dataset<Row> dataSet = sparkSession.read()
         .parquet((JavaConverters.asScalaIteratorConverter(listOfPaths.iterator()).asScala().toSeq()));
 
-    return AvroConversionUtils
+    return HoodieSparkUtils
         .createRdd(dataSet.toDF(), structName.orElse(RowBasedSchemaProvider.HOODIE_RECORD_STRUCT_NAME),
             RowBasedSchemaProvider.HOODIE_RECORD_NAMESPACE)
         .toJavaRDD();

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java
Patch:
@@ -61,9 +61,9 @@ public abstract class ITTestBase {
   protected static final String HIVESERVER = "/hiveserver";
   protected static final String PRESTO_COORDINATOR = "/presto-coordinator-1";
   protected static final String HOODIE_WS_ROOT = "/var/hoodie/ws";
-  protected static final String HOODIE_JAVA_APP = HOODIE_WS_ROOT + "/hudi-spark/run_hoodie_app.sh";
-  protected static final String HOODIE_GENERATE_APP = HOODIE_WS_ROOT + "/hudi-spark/run_hoodie_generate_app.sh";
-  protected static final String HOODIE_JAVA_STREAMING_APP = HOODIE_WS_ROOT + "/hudi-spark/run_hoodie_streaming_app.sh";
+  protected static final String HOODIE_JAVA_APP = HOODIE_WS_ROOT + "/hudi-spark-datasource/hudi-spark/run_hoodie_app.sh";
+  protected static final String HOODIE_GENERATE_APP = HOODIE_WS_ROOT + "/hudi-spark-datasource/hudi-spark/run_hoodie_generate_app.sh";
+  protected static final String HOODIE_JAVA_STREAMING_APP = HOODIE_WS_ROOT + "/hudi-spark-datasource/hudi-spark/run_hoodie_streaming_app.sh";
   protected static final String HUDI_HADOOP_BUNDLE =
       HOODIE_WS_ROOT + "/docker/hoodie/hadoop/hive_base/target/hoodie-hadoop-mr-bundle.jar";
   protected static final String HUDI_HIVE_SYNC_BUNDLE =

File: hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/bootstrap/SparkParquetBootstrapDataProvider.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.bootstrap;
 
-import org.apache.hudi.AvroConversionUtils;
 import org.apache.hudi.DataSourceUtils;
+import org.apache.hudi.HoodieSparkUtils;
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.avro.model.HoodieFileStatus;
 import org.apache.hudi.client.bootstrap.FullRecordBootstrapDataProvider;
@@ -65,7 +65,7 @@ public JavaRDD<HoodieRecord> generateInputRecords(String tableName, String sourc
       KeyGenerator keyGenerator = DataSourceUtils.createKeyGenerator(props);
       String structName = tableName + "_record";
       String namespace = "hoodie." + tableName;
-      RDD<GenericRecord> genericRecords = AvroConversionUtils.createRdd(inputDataset, structName, namespace);
+      RDD<GenericRecord> genericRecords = HoodieSparkUtils.createRdd(inputDataset, structName, namespace);
       return genericRecords.toJavaRDD().map(gr -> {
         String orderingVal = HoodieAvroUtils.getNestedFieldValAsString(
             gr, props.getString("hoodie.datasource.write.precombine.field"), false);

File: hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java
Patch:
@@ -43,7 +43,7 @@
 import org.apache.spark.sql.SparkSession;
 import org.apache.spark.sql.streaming.DataStreamWriter;
 import org.apache.spark.sql.streaming.OutputMode;
-import org.apache.spark.sql.streaming.ProcessingTime;
+import org.apache.spark.sql.streaming.Trigger;
 
 import java.util.List;
 import java.util.concurrent.ExecutorService;
@@ -366,7 +366,7 @@ public void stream(Dataset<Row> streamingInput, String operationType, String che
         .outputMode(OutputMode.Append());
 
     updateHiveSyncConfig(writer);
-    StreamingQuery query = writer.trigger(new ProcessingTime(500)).start(tablePath);
+    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);
     query.awaitTermination(streamingDurationInMs);
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
Patch:
@@ -55,7 +55,6 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
-import org.apache.spark.Accumulator;
 import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
@@ -68,6 +67,7 @@
 import org.apache.spark.sql.jdbc.JdbcDialect;
 import org.apache.spark.sql.jdbc.JdbcDialects;
 import org.apache.spark.sql.types.StructType;
+import org.apache.spark.util.LongAccumulator;
 
 import java.io.BufferedReader;
 import java.io.IOException;
@@ -292,7 +292,7 @@ public static SparkRDDWriteClient createHoodieClient(JavaSparkContext jsc, Strin
   }
 
   public static int handleErrors(JavaSparkContext jsc, String instantTime, JavaRDD<WriteStatus> writeResponse) {
-    Accumulator<Integer> errors = jsc.accumulator(0);
+    LongAccumulator errors = jsc.sc().longAccumulator();
     writeResponse.foreach(writeStatus -> {
       if (writeStatus.hasErrors()) {
         errors.add(1);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.utilities.deltastreamer;
 
-import org.apache.hudi.AvroConversionUtils;
 import org.apache.hudi.DataSourceUtils;
+import org.apache.hudi.HoodieSparkUtils;
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.client.SparkRDDWriteClient;
 import org.apache.hudi.client.WriteStatus;
@@ -342,7 +342,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource(
         // pass in the schema for the Row-to-Avro conversion
         // to avoid nullability mismatch between Avro schema and Row schema
         avroRDDOptional = transformed
-            .map(t -> AvroConversionUtils.createRdd(
+            .map(t -> HoodieSparkUtils.createRdd(
                 t, this.userProvidedSchemaProvider.getTargetSchema(),
                 HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD());
         schemaProvider = this.userProvidedSchemaProvider;
@@ -356,7 +356,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource(
                     UtilHelpers.createRowBasedSchemaProvider(r.schema(), props, jssc)))
                 .orElse(dataAndCheckpoint.getSchemaProvider());
         avroRDDOptional = transformed
-            .map(t -> AvroConversionUtils.createRdd(
+            .map(t -> HoodieSparkUtils.createRdd(
                 t, HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD());
       }
     } else {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.hudi.utilities.deltastreamer;
 
 import org.apache.hudi.AvroConversionUtils;
+import org.apache.hudi.HoodieSparkUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.utilities.UtilHelpers;
 import org.apache.hudi.utilities.schema.FilebasedSchemaProvider;
@@ -73,8 +74,8 @@ public InputBatch<JavaRDD<GenericRecord>> fetchNewDataInAvroFormat(Option<String
                   // If the source schema is specified through Avro schema,
                   // pass in the schema for the Row-to-Avro conversion
                   // to avoid nullability mismatch between Avro schema and Row schema
-                  ? AvroConversionUtils.createRdd(rdd, r.getSchemaProvider().getSourceSchema(),
-                  HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD() : AvroConversionUtils.createRdd(rdd,
+                  ? HoodieSparkUtils.createRdd(rdd, r.getSchemaProvider().getSourceSchema(),
+                  HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD() : HoodieSparkUtils.createRdd(rdd,
                   HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD();
             })
             .orElse(null)), r.getCheckpointForNextBatch(), r.getSchemaProvider());

File: hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -243,6 +243,7 @@ private void testDeduplication(
     when(index.isGlobal()).thenReturn(true);
     List<HoodieRecord<RawTripTestPayload>> dedupedRecs = SparkWriteHelper.newInstance().deduplicateRecords(records, index, 1).collect();
     assertEquals(1, dedupedRecs.size());
+    assertEquals(dedupedRecs.get(0).getPartitionPath(), recordThree.getPartitionPath());
     assertNodupesWithinPartition(dedupedRecs);
 
     // non-Global dedup should be done based on both recordKey and partitionPath

File: hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieParquetReader.java
Patch:
@@ -74,7 +74,6 @@ public void close() {
 
   @Override
   public long getTotalRecords() {
-    // TODO Auto-generated method stub
-    return 0;
+    return ParquetUtils.getRowCount(conf, path);
   }
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -273,7 +273,8 @@ public WriteStatus close() {
           insertRecordsWritten++;
         }
       }
-      keyToNewRecords.clear();
+
+      ((ExternalSpillableMap) keyToNewRecords).close();
       writtenRecordKeys.clear();
 
       if (fileWriter != null) {

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteWriter.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.common.HoodieSparkEngineContext;
 import org.apache.hudi.common.model.HoodieRecord;
+import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
@@ -38,7 +39,6 @@
 import org.apache.hudi.integ.testsuite.dag.nodes.RollbackNode;
 import org.apache.hudi.integ.testsuite.dag.nodes.ScheduleCompactNode;
 import org.apache.hudi.integ.testsuite.writer.DeltaWriteStats;
-import org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer.Operation;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
@@ -126,7 +126,7 @@ public Option<String> startCommit() {
 
   public JavaRDD<WriteStatus> upsert(Option<String> instantTime) throws Exception {
     if (cfg.useDeltaStreamer) {
-      return deltaStreamerWrapper.upsert(Operation.UPSERT);
+      return deltaStreamerWrapper.upsert(WriteOperationType.UPSERT);
     } else {
       Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> nextBatch = fetchSource();
       lastCheckpoint = Option.of(nextBatch.getValue().getLeft());

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieTableType;
+import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -378,7 +379,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource(
       return Pair.of(schemaProvider, Pair.of(checkpointStr, jssc.emptyRDD()));
     }
 
-    boolean shouldCombine = cfg.filterDupes || cfg.operation.equals(HoodieDeltaStreamer.Operation.UPSERT);
+    boolean shouldCombine = cfg.filterDupes || cfg.operation.equals(WriteOperationType.UPSERT);
     JavaRDD<GenericRecord> avroRDD = avroRDDOptional.get();
     JavaRDD<HoodieRecord> records = avroRDD.map(gr -> {
       HoodieRecordPayload payload = shouldCombine ? DataSourceUtils.createPayload(cfg.payloadClassName, gr,

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.DataSourceWriteOptions;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
+import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.config.TypedProperties;
@@ -69,7 +70,7 @@ public HoodieMultiTableDeltaStreamer(Config config, JavaSparkContext jssc) throw
     this.jssc = jssc;
     String commonPropsFile = config.propsFilePath;
     String configFolder = config.configFolder;
-    ValidationUtils.checkArgument(!config.filterDupes || config.operation != HoodieDeltaStreamer.Operation.UPSERT,
+    ValidationUtils.checkArgument(!config.filterDupes || config.operation != WriteOperationType.UPSERT,
         "'--filter-dupes' needs to be disabled when '--op' is 'UPSERT' to ensure updates are not missed.");
     FileSystem fs = FSUtils.getFs(commonPropsFile, jssc.hadoopConfiguration());
     configFolder = configFolder.charAt(configFolder.length() - 1) == '/' ? configFolder.substring(0, configFolder.length() - 1) : configFolder;
@@ -268,7 +269,7 @@ public static class Config implements Serializable {
 
     @Parameter(names = {"--op"}, description = "Takes one of these values : UPSERT (default), INSERT (use when input "
         + "is purely new data/inserts to gain speed)", converter = HoodieDeltaStreamer.OperationConverter.class)
-    public HoodieDeltaStreamer.Operation operation = HoodieDeltaStreamer.Operation.UPSERT;
+    public WriteOperationType operation = WriteOperationType.UPSERT;
 
     @Parameter(names = {"--filter-dupes"},
         description = "Should duplicate records from source be dropped/filtered out before insert/bulk-insert")

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java
Patch:
@@ -204,6 +204,7 @@ public static Schema removeMetadataFields(Schema schema) {
     List<Schema.Field> filteredFields = schema.getFields()
                                               .stream()
                                               .filter(field -> !HoodieRecord.HOODIE_META_COLUMNS.contains(field.name()))
+                                              .map(field -> new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal()))
                                               .collect(Collectors.toList());
     Schema filteredSchema = Schema.createRecord(schema.getName(), schema.getDoc(), schema.getNamespace(), false);
     filteredSchema.setFields(filteredFields);

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java
Patch:
@@ -84,7 +84,8 @@ public boolean next(NullWritable aVoid, ArrayWritable arrayWritable) throws IOEx
     if (!result) {
       // if the result is false, then there are no more records
       return false;
-    } else {
+    }
+    if (!deltaRecordMap.isEmpty()) {
       // TODO(VC): Right now, we assume all records in log, have a matching base record. (which
       // would be true until we have a way to index logs too)
       // return from delta records map if we have some match.
@@ -134,8 +135,8 @@ public boolean next(NullWritable aVoid, ArrayWritable arrayWritable) throws IOEx
           throw new RuntimeException(errMsg, re);
         }
       }
-      return true;
     }
+    return true;
   }
 
   @Override

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/WriterContext.java
Patch:
@@ -67,10 +67,11 @@ public void initContext(JavaSparkContext jsc) throws HoodieException {
       this.schemaProvider = UtilHelpers.createSchemaProvider(cfg.schemaProviderClassName, props, jsc);
       String schemaStr = schemaProvider.getSourceSchema().toString();
       this.hoodieTestSuiteWriter = new HoodieTestSuiteWriter(jsc, props, cfg, schemaStr);
+      int inputParallelism = cfg.inputParallelism > 0 ? cfg.inputParallelism : jsc.defaultParallelism();
       this.deltaGenerator = new DeltaGenerator(
           new DFSDeltaConfig(DeltaOutputMode.valueOf(cfg.outputTypeName), DeltaInputType.valueOf(cfg.inputFormatName),
               new SerializableConfiguration(jsc.hadoopConfiguration()), cfg.inputBasePath, cfg.targetBasePath,
-              schemaStr, cfg.limitFileSize),
+              schemaStr, cfg.limitFileSize, inputParallelism, cfg.deleteOldInput),
           jsc, sparkSession, schemaStr, keyGenerator);
       log.info(String.format("Initialized writerContext with: %s", schemaStr));
     } catch (Exception e) {

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/TestDFSHoodieTestSuiteWriterAdapter.java
Patch:
@@ -125,7 +125,7 @@ public void testDFSTwoFilesWriteWithRollover() throws IOException {
   public void testDFSWorkloadSinkWithMultipleFilesFunctional() throws IOException {
     DeltaConfig dfsSinkConfig = new DFSDeltaConfig(DeltaOutputMode.DFS, DeltaInputType.AVRO,
         new SerializableConfiguration(jsc.hadoopConfiguration()), dfsBasePath, dfsBasePath,
-        schemaProvider.getSourceSchema().toString(), 10240L);
+        schemaProvider.getSourceSchema().toString(), 10240L, jsc.defaultParallelism(), false);
     DeltaWriterAdapter<GenericRecord> dfsDeltaWriterAdapter = DeltaWriterFactory
         .getDeltaWriterAdapter(dfsSinkConfig, 1);
     FlexibleSchemaRecordGenerationIterator itr = new FlexibleSchemaRecordGenerationIterator(1000,

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java
Patch:
@@ -284,7 +284,7 @@ protected Option<HoodieWriteMetadata> fullBootstrap(List<Pair<String, List<Hoodi
   protected BaseSparkCommitActionExecutor<T> getBulkInsertActionExecutor(JavaRDD<HoodieRecord> inputRecordsRDD) {
     return new SparkBulkInsertCommitActionExecutor((HoodieSparkEngineContext) context, new HoodieWriteConfig.Builder().withProps(config.getProps())
         .withSchema(bootstrapSchema).build(), table, HoodieTimeline.FULL_BOOTSTRAP_INSTANT_TS,
-        inputRecordsRDD, extraMetadata);
+        inputRecordsRDD, Option.empty(), extraMetadata);
   }
 
   private BootstrapWriteStatus handleMetadataBootstrap(String srcPartitionPath, String partitionPath,

File: hudi-spark/src/main/java/org/apache/hudi/keygen/CustomKeyGenerator.java
Patch:
@@ -118,7 +118,6 @@ private String getPartitionPath(Option<GenericRecord> record, Option<Row> row) {
       partitionPath.append(DEFAULT_PARTITION_PATH_SEPARATOR);
     }
     partitionPath.deleteCharAt(partitionPath.length() - 1);
-
     return partitionPath.toString();
   }
 

File: hudi-spark/src/main/java/org/apache/hudi/keygen/KeyGenerator.java
Patch:
@@ -41,7 +41,7 @@ public abstract class KeyGenerator implements Serializable, SparkKeyGeneratorInt
   private static final String STRUCT_NAME = "hoodieRowTopLevelField";
   private static final String NAMESPACE = "hoodieRow";
 
-  protected transient TypedProperties config;
+  protected TypedProperties config;
   private transient Function1<Object, Object> converterFn = null;
 
   protected KeyGenerator(TypedProperties config) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieMergedLogRecordScanner.java
Patch:
@@ -88,7 +88,7 @@ public HoodieMergedLogRecordScanner(FileSystem fs, String basePath, List<String>
       LOG.info("Number of entries in DiskBasedMap in ExternalSpillableMap => " + records.getDiskBasedMapNumEntries());
       LOG.info("Size of file spilled to disk => " + records.getSizeOfFileOnDiskInBytes());
     } catch (IOException e) {
-      throw new HoodieIOException("IOException when reading log file ");
+      throw new HoodieIOException("IOException when reading log file ", e);
     }
   }
 

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/AbstractMergeHelper.java
Patch:
@@ -94,7 +94,7 @@ protected Iterator<GenericRecord> getMergingIterator(HoodieTable<T, I, K, O> tab
                                                                                                Schema readSchema, boolean externalSchemaTransformation) throws IOException {
     Path externalFilePath = new Path(baseFile.getBootstrapBaseFile().get().getPath());
     Configuration bootstrapFileConfig = new Configuration(table.getHadoopConf());
-    HoodieFileReader<GenericRecord> bootstrapReader = HoodieFileReaderFactory.<T, GenericRecord>getFileReader(bootstrapFileConfig, externalFilePath);
+    HoodieFileReader<GenericRecord> bootstrapReader = HoodieFileReaderFactory.<GenericRecord>getFileReader(bootstrapFileConfig, externalFilePath);
     Schema bootstrapReadSchema;
     if (externalSchemaTransformation) {
       bootstrapReadSchema = bootstrapReader.getSchema();

File: hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkMergeHelper.java
Patch:
@@ -76,7 +76,7 @@ public void runMerge(HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>
     }
 
     BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;
-    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<T, GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());
+    HoodieFileReader<GenericRecord> reader = HoodieFileReaderFactory.<GenericRecord>getFileReader(cfgForHoodieFile, mergeHandle.getOldFilePath());
     try {
       final Iterator<GenericRecord> readerIterator;
       if (baseFile.getBootstrapBaseFile().isPresent()) {

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
-import org.apache.hudi.exception.SchemaCompatabilityException;
+import org.apache.hudi.exception.SchemaCompatibilityException;
 
 import org.apache.avro.Conversions.DecimalConversion;
 import org.apache.avro.JsonProperties;
@@ -321,7 +321,7 @@ private static GenericRecord rewrite(GenericRecord record, LinkedHashSet<Field>
       }
     }
     if (!GenericData.get().validate(newSchema, newRecord)) {
-      throw new SchemaCompatabilityException(
+      throw new SchemaCompatibilityException(
           "Unable to validate the rewritten record " + record + " against schema " + newSchema);
     }
     return newRecord;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java
Patch:
@@ -319,7 +319,7 @@ private boolean readMagic() throws IOException {
       boolean hasMagic = hasNextMagic();
       if (!hasMagic) {
         throw new CorruptedLogFileException(
-            logFile + "could not be read. Did not find the magic bytes at the start of the block");
+            logFile + " could not be read. Did not find the magic bytes at the start of the block");
       }
       return hasMagic;
     } catch (EOFException e) {

File: hudi-common/src/main/java/org/apache/hudi/common/util/SerializationUtils.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.hudi.common.util;
 
-import org.apache.hudi.exception.HoodieSerializationException;
-
 import com.esotericsoftware.kryo.Kryo;
 import com.esotericsoftware.kryo.io.Input;
 import com.esotericsoftware.kryo.io.Output;
@@ -72,7 +70,6 @@ public static byte[] serialize(final Object obj) throws IOException {
    * @param objectData the serialized object, must not be null
    * @return the deserialized object
    * @throws IllegalArgumentException if {@code objectData} is {@code null}
-   * @throws HoodieSerializationException (runtime) if the serialization fails
    */
   public static <T> T deserialize(final byte[] objectData) {
     if (objectData == null) {

File: hudi-common/src/test/java/org/apache/hudi/avro/TestHoodieAvroUtils.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.avro.JsonProperties;
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.exception.SchemaCompatabilityException;
+import org.apache.hudi.exception.SchemaCompatibilityException;
 
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericData;
@@ -147,7 +147,7 @@ public void testNonNullableFieldWithoutDefault() {
     rec.put("non_pii_col", "val1");
     rec.put("pii_col", "val2");
     rec.put("timestamp", 3.5);
-    assertThrows(SchemaCompatabilityException.class, () -> HoodieAvroUtils.rewriteRecord(rec, new Schema.Parser().parse(SCHEMA_WITH_NON_NULLABLE_FIELD)));
+    assertThrows(SchemaCompatibilityException.class, () -> HoodieAvroUtils.rewriteRecord(rec, new Schema.Parser().parse(SCHEMA_WITH_NON_NULLABLE_FIELD)));
   }
 
   @Test

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/bootstrap/FullRecordBootstrapDataProvider.java
Patch:
@@ -45,7 +45,7 @@ public FullRecordBootstrapDataProvider(TypedProperties props, HoodieEngineContex
   }
 
   /**
-   * Generates a list of input partition and files and returns a RDD representing source.
+   * Generates a list of input partition and files and returns a collection representing source.
    * @param tableName Hudi Table Name
    * @param sourceBasePath Source Base Path
    * @param partitionPaths Partition Paths

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndex.java
Patch:
@@ -62,7 +62,7 @@ public abstract I tagLocation(I records, HoodieEngineContext context,
    * TODO(vc): We may need to propagate the record as well in a WriteStatus class
    */
   @PublicAPIMethod(maturity = ApiMaturityLevel.STABLE)
-  public abstract O updateLocation(O writeStatusRDD, HoodieEngineContext context,
+  public abstract O updateLocation(O writeStatuses, HoodieEngineContext context,
                                    HoodieTable<T, I, K, O> hoodieTable) throws HoodieIndexException;
 
   /**

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java
Patch:
@@ -73,7 +73,7 @@ public static List<Pair<String, HoodieBaseFile>> getLatestBaseFilesForAllPartiti
   public static HoodieRecord getTaggedRecord(HoodieRecord inputRecord, Option<HoodieRecordLocation> location) {
     HoodieRecord record = inputRecord;
     if (location.isPresent()) {
-      // When you have a record in multiple files in the same partition, then rowKeyRecordPairRDD
+      // When you have a record in multiple files in the same partition, then <row key, record> collection
       // will have 2 entries with the same exact in memory copy of the HoodieRecord and the 2
       // separate filenames that the record is found in. This will result in setting
       // currentLocation 2 times and it will fail the second time. So creating a new in memory

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/BulkInsertPartitioner.java
Patch:
@@ -35,7 +35,7 @@ public interface BulkInsertPartitioner<I> {
   I repartitionRecords(I records, int outputSparkPartitions);
 
   /**
-   * @return {@code true} if the records within a RDD partition are sorted; {@code false} otherwise.
+   * @return {@code true} if the records within a partition are sorted; {@code false} otherwise.
    */
   boolean arePartitionRecordsSorted();
 }

File: hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/AbstractWriteHelper.java
Patch:
@@ -32,7 +32,7 @@
 public abstract class AbstractWriteHelper<T extends HoodieRecordPayload, I, K, O, R> {
 
   public HoodieWriteMetadata<O> write(String instantTime,
-                                      I inputRecordsRDD,
+                                      I inputRecords,
                                       HoodieEngineContext context,
                                       HoodieTable<T, I, K, O> table,
                                       boolean shouldCombine,
@@ -42,7 +42,7 @@ public HoodieWriteMetadata<O> write(String instantTime,
     try {
       // De-dupe/merge if needed
       I dedupedRecords =
-          combineOnCondition(shouldCombine, inputRecordsRDD, shuffleParallelism, table);
+          combineOnCondition(shouldCombine, inputRecords, shuffleParallelism, table);
 
       Instant lookupBegin = Instant.now();
       I taggedRecords = dedupedRecords;
@@ -79,7 +79,7 @@ public I combineOnCondition(
    *
    * @param records     hoodieRecords to deduplicate
    * @param parallelism parallelism or partitions to be used while reducing/deduplicating
-   * @return RDD of HoodieRecord already be deduplicated
+   * @return Collection of HoodieRecord already be deduplicated
    */
   public I deduplicateRecords(
       I records, HoodieTable<T, I, K, O> table, int parallelism) {

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieDeltaStreamerWrapper.java
Patch:
@@ -66,6 +66,7 @@ public JavaRDD<WriteStatus> compact() throws Exception {
 
   public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> fetchSource() throws Exception {
     DeltaSync service = deltaSyncService.get().getDeltaSync();
+    service.refreshTimeline();
     return service.readFromSource(service.getCommitTimelineOpt());
   }
 

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/CleanNode.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.integ.testsuite.dag.nodes;
 
+import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;
 import org.apache.hudi.integ.testsuite.dag.ExecutionContext;
 
 /**
@@ -26,7 +27,8 @@
  */
 public class CleanNode extends DagNode<Boolean> {
 
-  public CleanNode() {
+  public CleanNode(Config config) {
+    this.config = config;
   }
 
   @Override

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/GenericRecordPartialPayloadGenerator.java
Patch:
@@ -38,7 +38,7 @@ public GenericRecordPartialPayloadGenerator(Schema schema, int minPayloadSize) {
   }
 
   @Override
-  protected GenericRecord convert(Schema schema) {
+  protected GenericRecord getNewPayload(Schema schema) {
     GenericRecord record = super.convertPartial(schema);
     return record;
   }

File: hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/reader/DFSHoodieDatasetInputReader.java
Patch:
@@ -132,7 +132,7 @@ private JavaRDD<GenericRecord> fetchRecordsFromDataset(Option<Integer> numPartit
       Option<Long> numRecordsToUpdate, Option<Double> percentageRecordsPerFile) throws IOException {
     log.info("NumPartitions : {}, NumFiles : {}, numRecordsToUpdate : {}, percentageRecordsPerFile : {}",
         numPartitions, numFiles, numRecordsToUpdate, percentageRecordsPerFile);
-    List<String> partitionPaths = getPartitions(numPartitions);
+    final List<String> partitionPaths = getPartitions(numPartitions);
     // Read all file slices in the partition
     JavaPairRDD<String, Iterator<FileSlice>> partitionToFileSlice = getPartitionToFileSlice(metaClient,
         partitionPaths);
@@ -156,7 +156,7 @@ private JavaRDD<GenericRecord> fetchRecordsFromDataset(Option<Integer> numPartit
     }
     // Adjust the number of files to read per partition based on the requested partition & file counts
     Map<String, Integer> adjustedPartitionToFileIdCountMap = getFilesToReadPerPartition(partitionToFileSlice,
-        getPartitions(numPartitions).size(), numFilesToUpdate);
+        partitionPaths.size(), numFilesToUpdate);
     JavaRDD<GenericRecord> updates = projectSchema(generateUpdates(adjustedPartitionToFileIdCountMap,
         partitionToFileSlice, numFilesToUpdate, (int) numRecordsToUpdatePerFile));
     if (numRecordsToUpdate.isPresent() && numFiles.isPresent() && numFiles.get() != 0 && numRecordsToUpdate.get()

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -219,7 +219,7 @@ public DeltaSync(HoodieDeltaStreamer.Config cfg, SparkSession sparkSession, Sche
    *
    * @throws IOException in case of any IOException
    */
-  private void refreshTimeline() throws IOException {
+  public void refreshTimeline() throws IOException {
     if (fs.exists(new Path(cfg.targetBasePath))) {
       HoodieTableMetaClient meta = new HoodieTableMetaClient(new Configuration(fs.getConf()), cfg.targetBasePath,
           cfg.payloadClassName);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DFSPathSelector.java
Patch:
@@ -78,6 +78,7 @@ public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(Optio
       while (fitr.hasNext()) {
         LocatedFileStatus fileStatus = fitr.next();
         if (fileStatus.isDirectory()
+            || fileStatus.getLen() == 0
             || IGNORE_FILEPREFIX_LIST.stream().anyMatch(pfx -> fileStatus.getPath().getName().startsWith(pfx))) {
           continue;
         }

File: hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java
Patch:
@@ -40,7 +40,7 @@
  */
 public class SparkUtil {
 
-  private static final String DEFAULT_SPARK_MASTER = "yarn-client";
+  private static final String DEFAULT_SPARK_MASTER = "yarn";
 
   /**
    * TODO: Need to fix a bunch of hardcoded stuff here eg: history server, spark distro.

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/BoundedInMemoryQueue.java
Patch:
@@ -57,7 +57,7 @@ public class BoundedInMemoryQueue<I, O> implements Iterable<O> {
   /** Rate used for sampling records to determine avg record size in bytes. **/
   public static final int RECORD_SAMPLING_RATE = 64;
 
-  /** Maximum records that will be cached **/
+  /** Maximum records that will be cached. **/
   private static final int RECORD_CACHING_LIMIT = 128 * 1024;
 
   private static final Logger LOG = LogManager.getLogger(BoundedInMemoryQueue.class);

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/BootstrapCommand.java
Patch:
@@ -179,7 +179,7 @@ private List<Comparable[]> convertBootstrapSourceFileMapping(List<BootstrapFileM
     final List<Comparable[]> rows = new ArrayList<>();
     for (BootstrapFileMapping mapping : mappingList) {
       rows.add(new Comparable[] {mapping.getPartitionPath(), mapping.getFileId(),
-          mapping.getBootstrapBasePath(), mapping.getBootstrapPartitionPath(), mapping.getBoostrapFileStatus().getPath().getUri()});
+          mapping.getBootstrapBasePath(), mapping.getBootstrapPartitionPath(), mapping.getBootstrapFileStatus().getPath().getUri()});
     }
     return rows;
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -126,12 +126,12 @@ protected List<HoodieFileGroup> addFilesToView(FileStatus[] statuses) {
       if (!isPartitionAvailableInStore(partition)) {
         if (bootstrapIndex.useIndex()) {
           try (BootstrapIndex.IndexReader reader = bootstrapIndex.createReader()) {
-            LOG.info("Boostrap Index available for partition " + partition);
+            LOG.info("Bootstrap Index available for partition " + partition);
             List<BootstrapFileMapping> sourceFileMappings =
                 reader.getSourceFileMappingForPartition(partition);
             addBootstrapBaseFileMapping(sourceFileMappings.stream()
                 .map(s -> new BootstrapBaseFileMapping(new HoodieFileGroupId(s.getPartitionPath(),
-                    s.getFileId()), s.getBoostrapFileStatus())));
+                    s.getFileId()), s.getBootstrapFileStatus())));
           }
         }
         storePartitionView(partition, value);

File: hudi-common/src/main/java/org/apache/hudi/common/util/queue/BoundedInMemoryQueueProducer.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.common.util.queue;
 
 /**
- * Producer for BoundedInMemoryQueue. Memory Bounded Buffer supports multiple producers single consumer pattern.
+ * Producer for {@link BoundedInMemoryQueue}. Memory Bounded Buffer supports multiple producers single consumer pattern.
  *
  * @param <I> Input type for buffer items produced
  */

File: hudi-common/src/test/java/org/apache/hudi/common/bootstrap/TestBootstrapIndex.java
Patch:
@@ -156,7 +156,7 @@ private void validateBootstrapIndex(Map<String, List<BootstrapFileMapping>> boot
           assertEquals(x.getFileId(), res.getFileId());
           assertEquals(x.getPartitionPath(), res.getPartitionPath());
           assertEquals(BOOTSTRAP_BASE_PATH, res.getBootstrapBasePath());
-          assertEquals(x.getBoostrapFileStatus(), res.getBoostrapFileStatus());
+          assertEquals(x.getBootstrapFileStatus(), res.getBootstrapFileStatus());
           assertEquals(x.getBootstrapPartitionPath(), res.getBootstrapPartitionPath());
         });
       });

File: hudi-spark/src/test/java/org/apache/hudi/client/TestBootstrap.java
Patch:
@@ -309,7 +309,7 @@ public void testMetadataBootstrapWithUpdatesMOR() throws Exception {
   }
 
   @Test
-  public void testFullBoostrapOnlyCOW() throws Exception {
+  public void testFullBootstrapOnlyCOW() throws Exception {
     testBootstrapCommon(true, false, EffectiveMode.FULL_BOOTSTRAP_MODE);
   }
 
@@ -319,7 +319,7 @@ public void testFullBootstrapWithUpdatesMOR() throws Exception {
   }
 
   @Test
-  public void testMetaAndFullBoostrapCOW() throws Exception {
+  public void testMetaAndFullBootstrapCOW() throws Exception {
     testBootstrapCommon(true, false, EffectiveMode.MIXED_BOOTSTRAP_MODE);
   }
 

File: hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java
Patch:
@@ -56,7 +56,7 @@ public static class DataGenerator {
     private static final String[] DEFAULT_PARTITION_PATHS =
         {DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH, DEFAULT_THIRD_PARTITION_PATH};
     static String TRIP_EXAMPLE_SCHEMA = "{\"type\": \"record\",\"name\": \"triprec\",\"fields\": [ "
-        + "{\"name\": \"ts\",\"type\": \"double\"},{\"name\": \"uuid\", \"type\": \"string\"},"
+        + "{\"name\": \"ts\",\"type\": \"long\"},{\"name\": \"uuid\", \"type\": \"string\"},"
         + "{\"name\": \"rider\", \"type\": \"string\"},{\"name\": \"driver\", \"type\": \"string\"},"
         + "{\"name\": \"begin_lat\", \"type\": \"double\"},{\"name\": \"begin_lon\", \"type\": \"double\"},"
         + "{\"name\": \"end_lat\", \"type\": \"double\"},{\"name\": \"end_lon\", \"type\": \"double\"},"

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieBootstrapConfig.java
Patch:
@@ -37,6 +37,7 @@ public class HoodieBootstrapConfig extends DefaultHoodieConfig {
   public static final String BOOTSTRAP_BASE_PATH_PROP = "hoodie.bootstrap.base.path";
   public static final String BOOTSTRAP_MODE_SELECTOR = "hoodie.bootstrap.mode.selector";
   public static final String FULL_BOOTSTRAP_INPUT_PROVIDER = "hoodie.bootstrap.full.input.provider";
+  public static final String DEFAULT_FULL_BOOTSTRAP_INPUT_PROVIDER = "org.apache.hudi.bootstrap.SparkParquetBootstrapDataProvider";
   public static final String BOOTSTRAP_KEYGEN_CLASS = "hoodie.bootstrap.keygen.class";
   public static final String BOOTSTRAP_PARTITION_PATH_TRANSLATOR_CLASS =
       "hoodie.bootstrap.partitionpath.translator.class";
@@ -135,6 +136,8 @@ public HoodieBootstrapConfig build() {
       BootstrapMode.valueOf(props.getProperty(BOOTSTRAP_MODE_SELECTOR_REGEX_MODE));
       setDefaultOnCondition(props, !props.containsKey(BOOTSTRAP_INDEX_CLASS_PROP), BOOTSTRAP_INDEX_CLASS_PROP,
           DEFAULT_BOOTSTRAP_INDEX_CLASS);
+      setDefaultOnCondition(props, !props.containsKey(FULL_BOOTSTRAP_INPUT_PROVIDER), FULL_BOOTSTRAP_INPUT_PROVIDER,
+          DEFAULT_FULL_BOOTSTRAP_INPUT_PROVIDER);
       return config;
     }
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -20,6 +20,7 @@
 
 import org.apache.hudi.AvroConversionUtils;
 import org.apache.hudi.DataSourceUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.config.TypedProperties;
@@ -351,7 +352,7 @@ public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource(
     JavaRDD<GenericRecord> avroRDD = avroRDDOptional.get();
     JavaRDD<HoodieRecord> records = avroRDD.map(gr -> {
       HoodieRecordPayload payload = DataSourceUtils.createPayload(cfg.payloadClassName, gr,
-          (Comparable) DataSourceUtils.getNestedFieldVal(gr, cfg.sourceOrderingField, false));
+          (Comparable) HoodieAvroUtils.getNestedFieldVal(gr, cfg.sourceOrderingField, false));
       return new HoodieRecord<>(keyGenerator.getKey(gr), payload);
     });
 

File: hudi-client/src/test/java/org/apache/hudi/index/TestHoodieIndex.java
Patch:
@@ -437,7 +437,7 @@ HoodieWriteConfig.Builder getConfigBuilder(String schemaStr) {
    */
   private HoodieWriteConfig.Builder getConfigBuilder(String schemaStr, IndexType indexType) {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(schemaStr)
-        .withParallelism(2, 2).withBulkInsertParallelism(2).withFinalizeWriteParallelism(2)
+        .withParallelism(2, 2).withBulkInsertParallelism(2).withFinalizeWriteParallelism(2).withDeleteParallelism(2)
         .withWriteStatusClass(MetadataMergeWriteStatus.class)
         .withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder().withConsistencyCheckEnabled(true).build())
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024).build())

File: hudi-client/src/test/java/org/apache/hudi/index/hbase/TestHBaseIndex.java
Patch:
@@ -462,7 +462,7 @@ private HoodieWriteConfig getConfig(int hbaseIndexBatchSize) {
 
   private HoodieWriteConfig.Builder getConfigBuilder(int hbaseIndexBatchSize) {
     return HoodieWriteConfig.newBuilder().withPath(basePath()).withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)
-        .withParallelism(1, 1)
+        .withParallelism(1, 1).withDeleteParallelism(1)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024)
             .withInlineCompaction(false).build())
         .withAutoCommit(false).withStorageConfig(HoodieStorageConfig.newBuilder()

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieKeyLocationFetchHandle.java
Patch:
@@ -174,7 +174,7 @@ public HoodieWriteConfig.Builder getConfigBuilder() {
    */
   private HoodieWriteConfig.Builder getConfigBuilder(String schemaStr) {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(schemaStr)
-        .withParallelism(2, 2).withBulkInsertParallelism(2).withFinalizeWriteParallelism(2)
+        .withParallelism(2, 2).withBulkInsertParallelism(2).withFinalizeWriteParallelism(2).withDeleteParallelism(2)
         .withWriteStatusClass(MetadataMergeWriteStatus.class)
         .withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder().withConsistencyCheckEnabled(true).build())
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024).build())

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieMergeHandle.java
Patch:
@@ -309,6 +309,7 @@ private Dataset<Row> getRecords() {
   HoodieWriteConfig.Builder getConfigBuilder() {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)
         .withParallelism(2, 2)
+        .withDeleteParallelism(2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024).build())
         .withStorageConfig(HoodieStorageConfig.newBuilder().hfileMaxFileSize(1024 * 1024).parquetMaxFileSize(1024 * 1024).build())
         .forTable("test-trip-table")

File: hudi-client/src/test/java/org/apache/hudi/table/TestCleaner.java
Patch:
@@ -207,7 +207,7 @@ private void testInsertAndCleanByVersions(
     HoodieWriteConfig cfg = getConfigBuilder()
         .withCompactionConfig(HoodieCompactionConfig.newBuilder()
             .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(maxVersions).build())
-        .withParallelism(1, 1).withBulkInsertParallelism(1).withFinalizeWriteParallelism(1)
+        .withParallelism(1, 1).withBulkInsertParallelism(1).withFinalizeWriteParallelism(1).withDeleteParallelism(1)
         .withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder().withConsistencyCheckEnabled(true).build())
         .build();
     try (HoodieWriteClient client = getHoodieWriteClient(cfg);) {
@@ -368,7 +368,7 @@ private void testInsertAndCleanByCommits(
     HoodieWriteConfig cfg = getConfigBuilder()
         .withCompactionConfig(HoodieCompactionConfig.newBuilder()
             .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(maxCommits).build())
-        .withParallelism(1, 1).withBulkInsertParallelism(1).withFinalizeWriteParallelism(1)
+        .withParallelism(1, 1).withBulkInsertParallelism(1).withFinalizeWriteParallelism(1).withDeleteParallelism(1)
         .withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder().withConsistencyCheckEnabled(true).build())
         .build();
     HoodieWriteClient client = getHoodieWriteClient(cfg);

File: hudi-client/src/test/java/org/apache/hudi/table/TestHoodieMergeOnReadTable.java
Patch:
@@ -803,6 +803,7 @@ public void testMultiRollbackWithDeltaAndCompactionCommit() throws Exception {
 
   protected HoodieWriteConfig getHoodieWriteConfigWithSmallFileHandlingOff() {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
+        .withDeleteParallelism(2)
         .withAutoCommit(false).withAssumeDatePartitioning(true)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024)
             .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(1).build())
@@ -1466,6 +1467,7 @@ protected HoodieWriteConfig.Builder getConfigBuilder(Boolean autoCommit, HoodieI
 
   protected HoodieWriteConfig.Builder getConfigBuilder(Boolean autoCommit, Boolean rollbackUsingMarkers, HoodieIndex.IndexType indexType) {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
+        .withDeleteParallelism(2)
         .withAutoCommit(autoCommit).withAssumeDatePartitioning(true)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024 * 1024)
             .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(1).build())

File: hudi-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestBase.java
Patch:
@@ -125,7 +125,7 @@ public HoodieWriteConfig.Builder getConfigBuilder(String schemaStr) {
    */
   public HoodieWriteConfig.Builder getConfigBuilder(String schemaStr, IndexType indexType) {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(schemaStr)
-        .withParallelism(2, 2).withBulkInsertParallelism(2).withFinalizeWriteParallelism(2)
+        .withParallelism(2, 2).withBulkInsertParallelism(2).withFinalizeWriteParallelism(2).withDeleteParallelism(2)
         .withTimelineLayoutVersion(TimelineLayoutVersion.CURR_VERSION)
         .withWriteStatusClass(MetadataMergeWriteStatus.class)
         .withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder().withConsistencyCheckEnabled(true).build())

File: hudi-client/src/test/java/org/apache/hudi/testutils/SparkDatasetTestUtils.java
Patch:
@@ -165,6 +165,7 @@ public static InternalRow getInternalRowWithError(String partitionPath) {
   public static HoodieWriteConfig.Builder getConfigBuilder(String basePath) {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)
         .withParallelism(2, 2)
+        .withDeleteParallelism(2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024).build())
         .withStorageConfig(HoodieStorageConfig.newBuilder().hfileMaxFileSize(1024 * 1024).parquetMaxFileSize(1024 * 1024).build())
         .forTable("test-trip-table")

File: hudi-examples/src/main/java/org/apache/hudi/examples/spark/HoodieWriteClientExample.java
Patch:
@@ -88,7 +88,8 @@ public static void main(String[] args) throws Exception {
 
       // Create the write client to write some records in
       HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)
-              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2).forTable(tableName)
+              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
+              .withDeleteParallelism(2).forTable(tableName)
               .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())
               .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();
       HoodieWriteClient<HoodieAvroPayload> client = new HoodieWriteClient<>(jsc, cfg);

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/testsuite/reader/TestDFSHoodieDatasetInputReader.java
Patch:
@@ -113,6 +113,7 @@ private HoodieWriteConfig.Builder makeHoodieClientConfigBuilder() throws Excepti
     // Prepare the AvroParquetIO
     return HoodieWriteConfig.newBuilder().withPath(dfsBasePath)
         .withParallelism(2, 2)
+        .withDeleteParallelism(2)
         .withSchema(HoodieTestDataGenerator
             .TRIP_EXAMPLE_SCHEMA);
   }

File: hudi-spark/src/test/java/HoodieJavaApp.java
Patch:
@@ -199,6 +199,7 @@ public void run() throws Exception {
     Dataset<Row> inputDF3 = spark.read().json(jssc.parallelize(deletes, 2));
     writer = inputDF3.write().format("org.apache.hudi").option("hoodie.insert.shuffle.parallelism", "2")
         .option("hoodie.upsert.shuffle.parallelism", "2")
+        .option("hoodie.delete.shuffle.parallelism", "2")
         .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType) // Hoodie Table Type
         .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), "delete")
         .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), "_row_key")

File: hudi-spark/src/test/java/HoodieJavaStreamingApp.java
Patch:
@@ -354,6 +354,7 @@ public void stream(Dataset<Row> streamingInput, String operationType, String che
 
     DataStreamWriter<Row> writer = streamingInput.writeStream().format("org.apache.hudi")
         .option("hoodie.insert.shuffle.parallelism", "2").option("hoodie.upsert.shuffle.parallelism", "2")
+        .option("hoodie.delete.shuffle.parallelism", "2")
         .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), operationType)
         .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)
         .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), "_row_key")

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
Patch:
@@ -247,6 +247,7 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jsc, String
         HoodieWriteConfig.newBuilder().withPath(basePath)
             .withParallelism(parallelism, parallelism)
             .withBulkInsertParallelism(parallelism)
+            .withDeleteParallelism(parallelism)
             .withSchema(schemaStr).combineInput(true, true).withCompactionConfig(compactionConfig)
             .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())
             .withProps(properties).build();

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieSnapshotExporter.java
Patch:
@@ -111,6 +111,7 @@ private HoodieWriteConfig getHoodieWriteConfig(String basePath) {
         .withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)
         .withParallelism(2, 2)
         .withBulkInsertParallelism(2)
+        .withDeleteParallelism(2)
         .forTable(TABLE_NAME)
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(IndexType.BLOOM).build())
         .build();

File: hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java
Patch:
@@ -157,8 +157,7 @@ private void syncSchema(String tableName, boolean tableExists, boolean useRealTi
     if (!tableExists) {
       LOG.info("Hive table " + tableName + " is not found. Creating it");
       HoodieFileFormat baseFileFormat = HoodieFileFormat.valueOf(cfg.baseFileFormat.toUpperCase());
-      String inputFormatClassName = HoodieInputFormatUtils.getInputFormatClassName(baseFileFormat, useRealTimeInputFormat,
-          new Configuration());
+      String inputFormatClassName = HoodieInputFormatUtils.getInputFormatClassName(baseFileFormat, useRealTimeInputFormat);
 
       if (baseFileFormat.equals(HoodieFileFormat.PARQUET) && cfg.usePreApacheInputFormat) {
         // Parquet input format had an InputFormat class visible under the old naming scheme.

File: hudi-client/src/main/java/org/apache/hudi/table/action/compact/HoodieMergeOnReadTableCompactor.java
Patch:
@@ -135,15 +135,14 @@ private List<WriteStatus> compact(HoodieCopyOnWriteTable hoodieCopyOnWriteTable,
 
     // Compacting is very similar to applying updates to existing file
     Iterator<List<WriteStatus>> result;
-    // If the dataFile is present, there is a base parquet file present, perform updates else perform inserts into a
-    // new base parquet file.
+    // If the dataFile is present, perform updates else perform inserts into a new base file.
     if (oldDataFileOpt.isPresent()) {
       result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),
               operation.getFileId(), scanner.getRecords(),
           oldDataFileOpt.get());
     } else {
       result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),
-          scanner.iterator());
+          scanner.getRecords());
     }
     Iterable<List<WriteStatus>> resultIterable = () -> result;
     return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {

File: hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -1261,7 +1261,8 @@ private HoodieWriteConfig getSmallInsertWriteConfig(int insertSplitSize, boolean
                 .insertSplitSize(insertSplitSize).build())
         .withStorageConfig(
             HoodieStorageConfig.newBuilder()
-                .limitFileSize(dataGen.getEstimatedFileSizeInBytes(200)).build())
+                .hfileMaxFileSize(dataGen.getEstimatedFileSizeInBytes(200))
+                .parquetMaxFileSize(dataGen.getEstimatedFileSizeInBytes(200)).build())
         .build();
   }
 }

File: hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java
Patch:
@@ -411,7 +411,7 @@ public void testCopyOnWriteTable() throws Exception {
   private void checkReadRecords(String instantTime, int numExpectedRecords) throws IOException {
     if (tableType == HoodieTableType.COPY_ON_WRITE) {
       HoodieTimeline timeline = metaClient.reloadActiveTimeline().getCommitTimeline();
-      assertEquals(numExpectedRecords, HoodieClientTestUtils.readSince(basePath, sqlContext, timeline, instantTime).count());
+      assertEquals(numExpectedRecords, HoodieClientTestUtils.countRecordsSince(jsc, basePath, sqlContext, timeline, instantTime));
     } else {
       // TODO: This code fails to read records under the following conditions:
       // 1. No parquet files yet (i.e. no compaction done yet)

File: hudi-client/src/test/java/org/apache/hudi/index/TestHoodieIndex.java
Patch:
@@ -441,7 +441,7 @@ private HoodieWriteConfig.Builder getConfigBuilder(String schemaStr, IndexType i
         .withWriteStatusClass(MetadataMergeWriteStatus.class)
         .withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder().withConsistencyCheckEnabled(true).build())
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024).build())
-        .withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1024 * 1024).build())
+        .withStorageConfig(HoodieStorageConfig.newBuilder().hfileMaxFileSize(1024 * 1024).parquetMaxFileSize(1024 * 1024).build())
         .forTable("test-trip-table")
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType).build())
         .withEmbeddedTimelineServerEnabled(true).withFileSystemViewConfig(FileSystemViewStorageConfig.newBuilder()

File: hudi-client/src/test/java/org/apache/hudi/index/hbase/TestHBaseIndex.java
Patch:
@@ -465,7 +465,8 @@ private HoodieWriteConfig.Builder getConfigBuilder(int hbaseIndexBatchSize) {
         .withParallelism(1, 1)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024)
             .withInlineCompaction(false).build())
-        .withAutoCommit(false).withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1024 * 1024).build())
+        .withAutoCommit(false).withStorageConfig(HoodieStorageConfig.newBuilder()
+            .hfileMaxFileSize(1024 * 1024).parquetMaxFileSize(1024 * 1024).build())
         .forTable("test-trip-table")
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.HBASE)
             .withHBaseIndexConfig(new HoodieHBaseIndexConfig.Builder()

File: hudi-client/src/test/java/org/apache/hudi/index/hbase/TestHBaseQPSResourceAllocator.java
Patch:
@@ -76,7 +76,8 @@ private HoodieWriteConfig.Builder getConfigBuilder(HoodieHBaseIndexConfig hoodie
         .withParallelism(1, 1)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024)
             .withInlineCompaction(false).build())
-        .withAutoCommit(false).withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1024 * 1024).build())
+        .withAutoCommit(false).withStorageConfig(HoodieStorageConfig.newBuilder()
+            .hfileMaxFileSize(1000 * 1024).parquetMaxFileSize(1024 * 1024).build())
         .forTable("test-trip-table").withIndexConfig(HoodieIndexConfig.newBuilder()
             .withIndexType(HoodieIndex.IndexType.HBASE).withHBaseIndexConfig(hoodieHBaseIndexConfig).build());
   }

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieKeyLocationFetchHandle.java
Patch:
@@ -178,7 +178,7 @@ private HoodieWriteConfig.Builder getConfigBuilder(String schemaStr) {
         .withWriteStatusClass(MetadataMergeWriteStatus.class)
         .withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder().withConsistencyCheckEnabled(true).build())
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024).build())
-        .withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1024 * 1024).build())
+        .withStorageConfig(HoodieStorageConfig.newBuilder().hfileMaxFileSize(1024 * 1024).parquetMaxFileSize(1024 * 1024).build())
         .forTable("test-trip-table")
         .withIndexConfig(HoodieIndexConfig.newBuilder().build())
         .withEmbeddedTimelineServerEnabled(true).withFileSystemViewConfig(FileSystemViewStorageConfig.newBuilder()

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieMergeHandle.java
Patch:
@@ -310,7 +310,7 @@ HoodieWriteConfig.Builder getConfigBuilder() {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)
         .withParallelism(2, 2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024).build())
-        .withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1024 * 1024).build())
+        .withStorageConfig(HoodieStorageConfig.newBuilder().hfileMaxFileSize(1024 * 1024).parquetMaxFileSize(1024 * 1024).build())
         .forTable("test-trip-table")
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())
         .withBulkInsertParallelism(2).withWriteStatusClass(TestWriteStatus.class);

File: hudi-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java
Patch:
@@ -71,7 +71,8 @@ private UpsertPartitioner getUpsertPartitioner(int smallFileSize, int numInserts
     HoodieWriteConfig config = makeHoodieClientConfigBuilder()
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(smallFileSize)
             .insertSplitSize(100).autoTuneInsertSplits(autoSplitInserts).build())
-        .withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1000 * 1024).build()).build();
+        .withStorageConfig(HoodieStorageConfig.newBuilder().hfileMaxFileSize(1000 * 1024).parquetMaxFileSize(1000 * 1024).build())
+        .build();
 
     FileCreateUtils.createCommit(basePath, "001");
     FileCreateUtils.createDataFile(basePath, testPartitionPath, "001", "file1", fileSize);

File: hudi-client/src/test/java/org/apache/hudi/table/action/compact/CompactionTestBase.java
Patch:
@@ -72,7 +72,8 @@ protected HoodieWriteConfig.Builder getConfigBuilder(Boolean autoCommit) {
         .withAutoCommit(autoCommit).withAssumeDatePartitioning(true)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024 * 1024)
             .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(1).build())
-        .withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1024 * 1024 * 1024).build())
+        .withStorageConfig(HoodieStorageConfig.newBuilder()
+            .hfileMaxFileSize(1024 * 1024 * 1024).parquetMaxFileSize(1024 * 1024 * 1024).build())
         .forTable("test-trip-table")
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())
         .withEmbeddedTimelineServerEnabled(true).withFileSystemViewConfig(FileSystemViewStorageConfig.newBuilder()
@@ -194,7 +195,7 @@ protected void executeCompaction(String compactionInstantTime, HoodieWriteClient
     assertEquals(latestCompactionCommitTime, compactionInstantTime,
         "Expect compaction instant time to be the latest commit time");
     assertEquals(expectedNumRecs,
-        HoodieClientTestUtils.readSince(basePath, sqlContext, timeline, "000").count(),
+        HoodieClientTestUtils.countRecordsSince(jsc, basePath, sqlContext, timeline, "000"),
         "Must contain expected records");
 
   }

File: hudi-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java
Patch:
@@ -95,7 +95,7 @@ private HoodieWriteConfig.Builder getConfigBuilder() {
         .withParallelism(2, 2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024)
             .withInlineCompaction(false).build())
-        .withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1024 * 1024).build())
+        .withStorageConfig(HoodieStorageConfig.newBuilder().hfileMaxFileSize(1024 * 1024).parquetMaxFileSize(1024 * 1024).build())
         .withMemoryConfig(HoodieMemoryConfig.newBuilder().withMaxDFSStreamBufferSize(1 * 1024 * 1024).build())
         .forTable("test-trip-table")
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build());

File: hudi-client/src/test/java/org/apache/hudi/testutils/SparkDatasetTestUtils.java
Patch:
@@ -166,7 +166,7 @@ public static HoodieWriteConfig.Builder getConfigBuilder(String basePath) {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)
         .withParallelism(2, 2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024).build())
-        .withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1024 * 1024).build())
+        .withStorageConfig(HoodieStorageConfig.newBuilder().hfileMaxFileSize(1024 * 1024).parquetMaxFileSize(1024 * 1024).build())
         .forTable("test-trip-table")
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())
         .withBulkInsertParallelism(2);

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDataBlock.java
Patch:
@@ -77,6 +77,8 @@ public static HoodieLogBlock getBlock(HoodieLogBlockType logDataBlockFormat, Lis
     switch (logDataBlockFormat) {
       case AVRO_DATA_BLOCK:
         return new HoodieAvroDataBlock(recordList, header);
+      case HFILE_DATA_BLOCK:
+        return new HoodieHFileDataBlock(recordList, header);
       default:
         throw new HoodieException("Data block format " + logDataBlockFormat + " not implemented");
     }

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieLogBlock.java
Patch:
@@ -110,7 +110,7 @@ public Option<byte[]> getContent() {
    * Type of the log block WARNING: This enum is serialized as the ordinal. Only add new enums at the end.
    */
   public enum HoodieLogBlockType {
-    COMMAND_BLOCK, DELETE_BLOCK, CORRUPT_BLOCK, AVRO_DATA_BLOCK
+    COMMAND_BLOCK, DELETE_BLOCK, CORRUPT_BLOCK, AVRO_DATA_BLOCK, HFILE_DATA_BLOCK
   }
 
   /**

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/InputPathHandler.java
Patch:
@@ -51,7 +51,7 @@ public class InputPathHandler {
   private final List<Path> snapshotPaths;
   private final List<Path> nonHoodieInputPaths;
 
-  InputPathHandler(Configuration conf, Path[] inputPaths, List<String> incrementalTables) throws IOException {
+  public InputPathHandler(Configuration conf, Path[] inputPaths, List<String> incrementalTables) throws IOException {
     this.conf = conf;
     tableMetaClientMap = new HashMap<>();
     snapshotPaths = new ArrayList<>();

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java
Patch:
@@ -255,7 +255,7 @@ private void saveUpLogs() {
     try {
       // save up the Hive log files for introspection
       String hiveLogStr =
-          executeCommandStringInDocker(HIVESERVER, "cat /tmp/root/hive.log |  grep -i exception -A 10 -B 5", true).getStdout().toString();
+          executeCommandStringInDocker(HIVESERVER, "cat /tmp/root/hive.log |  grep -i exception -A 10 -B 5", false).getStdout().toString();
       String filePath = System.getProperty("java.io.tmpdir") + "/" + System.currentTimeMillis() + "-hive.log";
       FileIOUtils.writeStringToFile(hiveLogStr, filePath);
       LOG.info("Hive log saved up at  : " + filePath);

File: hudi-spark/src/main/java/org/apache/hudi/keygen/ComplexKeyGenerator.java
Patch:
@@ -37,9 +37,9 @@ public class ComplexKeyGenerator extends BuiltinKeyGenerator {
   public ComplexKeyGenerator(TypedProperties props) {
     super(props);
     this.recordKeyFields = Arrays.stream(props.getString(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY())
-        .split(",")).map(String::trim).collect(Collectors.toList());
+        .split(",")).map(String::trim).filter(s -> !s.isEmpty()).collect(Collectors.toList());
     this.partitionPathFields = Arrays.stream(props.getString(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY())
-        .split(",")).map(String::trim).collect(Collectors.toList());
+        .split(",")).map(String::trim).filter(s -> !s.isEmpty()).collect(Collectors.toList());
   }
 
   @Override

File: hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -39,7 +39,7 @@
 import org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.keygen.KeyGenerator;
-import org.apache.hudi.keygen.parser.HoodieDateTimeParser;
+import org.apache.hudi.keygen.parser.AbstractHoodieDateTimeParser;
 import org.apache.hudi.table.BulkInsertPartitioner;
 
 import org.apache.avro.LogicalTypes;
@@ -172,9 +172,9 @@ public static KeyGenerator createKeyGenerator(TypedProperties props) throws IOEx
   /**
    * Create a date time parser class for TimestampBasedKeyGenerator, passing in any configs needed.
    */
-  public static HoodieDateTimeParser createDateTimeParser(TypedProperties props, String parserClass) throws IOException {
+  public static AbstractHoodieDateTimeParser createDateTimeParser(TypedProperties props, String parserClass) throws IOException {
     try {
-      return (HoodieDateTimeParser) ReflectionUtils.loadClass(parserClass, props);
+      return (AbstractHoodieDateTimeParser) ReflectionUtils.loadClass(parserClass, props);
     } catch (Throwable e) {
       throw new IOException("Could not load date time parser class " + parserClass, e);
     }

File: hudi-spark/src/main/java/org/apache/hudi/keygen/RowKeyGeneratorHelper.java
Patch:
@@ -146,7 +146,7 @@ public static Object getNestedFieldVal(Row row, List<Integer> positions) {
         }
         valueToProcess = (Row) valueToProcess.get(positions.get(index));
       } else { // last index
-        if (valueToProcess.getAs(positions.get(index)).toString().isEmpty()) {
+        if (null != valueToProcess.getAs(positions.get(index)) && valueToProcess.getAs(positions.get(index)).toString().isEmpty()) {
           toReturn = EMPTY_RECORDKEY_PLACEHOLDER;
           break;
         }

File: hudi-spark/src/main/java/org/apache/hudi/HoodieDataSourceHelpers.java
Patch:
@@ -74,7 +74,7 @@ public static HoodieTimeline allCompletedCommitsCompactions(FileSystem fs, Strin
     if (metaClient.getTableType().equals(HoodieTableType.MERGE_ON_READ)) {
       return metaClient.getActiveTimeline().getTimelineOfActions(
           CollectionUtils.createSet(HoodieActiveTimeline.COMMIT_ACTION,
-              HoodieActiveTimeline.DELTA_COMMIT_ACTION));
+              HoodieActiveTimeline.DELTA_COMMIT_ACTION)).filterCompletedInstants();
     } else {
       return metaClient.getCommitTimeline().filterCompletedInstants();
     }

File: hudi-spark/src/test/java/HoodieJavaStreamingApp.java
Patch:
@@ -273,7 +273,9 @@ private void waitTillNCommits(FileSystem fs, int numCommits, int timeoutSecs, in
   public int addInputAndValidateIngestion(SparkSession spark, FileSystem fs, String srcPath,
       int initialCommits, int expRecords,
       Dataset<Row> inputDF1, Dataset<Row> inputDF2, boolean instantTimeValidation) throws Exception {
-    inputDF1.write().mode(SaveMode.Append).json(srcPath);
+    // Ensure, we always write only one file. This is very important to ensure a single batch is reliably read
+    // atomically by one iteration of spark streaming.
+    inputDF1.coalesce(1).write().mode(SaveMode.Append).json(srcPath);
 
     int numExpCommits = initialCommits + 1;
     // wait for spark streaming to process one microbatch

File: hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieFileWriterFactory.java
Patch:
@@ -58,7 +58,7 @@ private static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFi
     HoodieAvroWriteSupport writeSupport =
         new HoodieAvroWriteSupport(new AvroSchemaConverter().convert(schema), schema, filter);
 
-    HoodieParquetConfig parquetConfig = new HoodieParquetConfig(writeSupport, config.getParquetCompressionCodec(),
+    HoodieAvroParquetConfig parquetConfig = new HoodieAvroParquetConfig(writeSupport, config.getParquetCompressionCodec(),
         config.getParquetBlockSize(), config.getParquetPageSize(), config.getParquetMaxFileSize(),
         hoodieTable.getHadoopConf(), config.getParquetCompressionRatio());
 

File: hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieParquetWriter.java
Patch:
@@ -52,7 +52,7 @@ public class HoodieParquetWriter<T extends HoodieRecordPayload, R extends Indexe
   private final String instantTime;
   private final SparkTaskContextSupplier sparkTaskContextSupplier;
 
-  public HoodieParquetWriter(String instantTime, Path file, HoodieParquetConfig parquetConfig,
+  public HoodieParquetWriter(String instantTime, Path file, HoodieAvroParquetConfig parquetConfig,
       Schema schema, SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {
     super(HoodieWrapperFileSystem.convertToHoodiePath(file, parquetConfig.getHadoopConf()),
         ParquetFileWriter.Mode.CREATE, parquetConfig.getWriteSupport(), parquetConfig.getCompressionCodecName(),

File: hudi-client/src/main/java/org/apache/hudi/table/action/bootstrap/BootstrapCommitActionExecutor.java
Patch:
@@ -53,7 +53,7 @@
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.execution.SparkBoundedInMemoryExecutor;
 import org.apache.hudi.io.HoodieBootstrapHandle;
-import org.apache.hudi.keygen.KeyGenerator;
+import org.apache.hudi.keygen.KeyGeneratorInterface;
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.WorkloadProfile;
 import org.apache.hudi.table.action.HoodieWriteMetadata;
@@ -225,7 +225,7 @@ protected CommitActionExecutor<T> getBulkInsertActionExecutor(JavaRDD<HoodieReco
   }
 
   private BootstrapWriteStatus handleMetadataBootstrap(String srcPartitionPath, String partitionPath,
-                                                       HoodieFileStatus srcFileStatus, KeyGenerator keyGenerator) {
+                                                       HoodieFileStatus srcFileStatus, KeyGeneratorInterface keyGenerator) {
 
     Path sourceFilePath = FileStatusUtils.toPath(srcFileStatus.getPath());
     HoodieBootstrapHandle bootstrapHandle = new HoodieBootstrapHandle(config, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS,
@@ -311,7 +311,7 @@ private JavaRDD<BootstrapWriteStatus> runMetadataBootstrap(List<Pair<String, Lis
 
     TypedProperties properties = new TypedProperties();
     properties.putAll(config.getProps());
-    KeyGenerator keyGenerator  = (KeyGenerator) ReflectionUtils.loadClass(config.getBootstrapKeyGeneratorClass(),
+    KeyGeneratorInterface keyGenerator  = (KeyGeneratorInterface) ReflectionUtils.loadClass(config.getBootstrapKeyGeneratorClass(),
         properties);
     BootstrapPartitionPathTranslator translator = (BootstrapPartitionPathTranslator) ReflectionUtils.loadClass(
         config.getBootstrapPartitionPathTranslatorClass(), properties);

File: hudi-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java
Patch:
@@ -39,7 +39,7 @@
 import org.apache.hudi.config.HoodieStorageConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.io.IOType;
-import org.apache.hudi.io.storage.HoodieParquetConfig;
+import org.apache.hudi.io.storage.HoodieAvroParquetConfig;
 import org.apache.hudi.io.storage.HoodieParquetWriter;
 
 import org.apache.avro.Schema;
@@ -255,7 +255,7 @@ public static String writeParquetFile(String basePath, String partitionPath, Str
     HoodieAvroWriteSupport writeSupport =
         new HoodieAvroWriteSupport(new AvroSchemaConverter().convert(schema), schema, filter);
     String instantTime = FSUtils.getCommitTime(filename);
-    HoodieParquetConfig config = new HoodieParquetConfig(writeSupport, CompressionCodecName.GZIP,
+    HoodieAvroParquetConfig config = new HoodieAvroParquetConfig(writeSupport, CompressionCodecName.GZIP,
         ParquetWriter.DEFAULT_BLOCK_SIZE, ParquetWriter.DEFAULT_PAGE_SIZE, 120 * 1024 * 1024,
         HoodieTestUtils.getDefaultHadoopConf(), Double.valueOf(HoodieStorageConfig.DEFAULT_STREAM_COMPRESSION_RATIO));
     HoodieParquetWriter writer =

File: hudi-spark/src/test/java/HoodieJavaApp.java
Patch:
@@ -151,7 +151,7 @@ public void run() throws Exception {
         .option(DataSourceWriteOptions.KEYGENERATOR_CLASS_OPT_KEY(),
             nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()
                 : SimpleKeyGenerator.class.getCanonicalName())
-        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_KEY(), "false")
+        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), "false")
         // This will remove any existing data at path below, and create a
         .mode(SaveMode.Overwrite);
 
@@ -178,7 +178,7 @@ public void run() throws Exception {
             nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()
                 : SimpleKeyGenerator.class.getCanonicalName()) // Add Key Extractor
         .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, "1")
-        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_KEY(), "false")
+        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), "false")
         .option(HoodieWriteConfig.TABLE_NAME, tableName).mode(SaveMode.Append);
 
     updateHiveSyncConfig(writer);
@@ -204,7 +204,7 @@ public void run() throws Exception {
             nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()
                 : SimpleKeyGenerator.class.getCanonicalName()) // Add Key Extractor
         .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, "1")
-        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_KEY(), "false")
+        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), "false")
         .option(HoodieWriteConfig.TABLE_NAME, tableName).mode(SaveMode.Append);
 
     updateHiveSyncConfig(writer);

File: hudi-spark/src/test/java/HoodieJavaStreamingApp.java
Patch:
@@ -358,7 +358,7 @@ public void stream(Dataset<Row> streamingInput, String operationType, String che
         .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), "partition")
         .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), "timestamp")
         .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, "1")
-        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_KEY(), "true")
+        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), "true")
         .option(HoodieWriteConfig.TABLE_NAME, tableName).option("checkpointLocation", checkpointLocation)
         .outputMode(OutputMode.Append());
 

File: hudi-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestBase.java
Patch:
@@ -71,7 +71,7 @@
  */
 public class HoodieClientTestBase extends HoodieClientTestHarness {
 
-  private static final Logger LOG = LogManager.getLogger(HoodieClientTestBase.class);
+  protected static final Logger LOG = LogManager.getLogger(HoodieClientTestBase.class);
 
   @BeforeEach
   public void setUp() throws Exception {

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java
Patch:
@@ -69,7 +69,7 @@ private HoodieMergedLogRecordScanner getMergedLogRecordScanner() throws IOExcept
         split.getDeltaLogPaths(),
         usesCustomPayload ? getWriterSchema() : getReaderSchema(),
         split.getMaxCommitTime(),
-        getMaxCompactionMemoryInBytes(),
+        HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes(jobConf),
         Boolean.parseBoolean(jobConf.get(HoodieRealtimeConfig.COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP, HoodieRealtimeConfig.DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED)),
         false,
         jobConf.getInt(HoodieRealtimeConfig.MAX_DFS_STREAM_BUFFER_SIZE_PROP, HoodieRealtimeConfig.DEFAULT_MAX_DFS_STREAM_BUFFER_SIZE),

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -577,7 +577,7 @@ protected Pair<CompletableFuture, ExecutorService> startService() {
             try {
               long start = System.currentTimeMillis();
               Pair<Option<String>, JavaRDD<WriteStatus>> scheduledCompactionInstantAndRDD = deltaSync.syncOnce();
-              if (scheduledCompactionInstantAndRDD.getLeft().isPresent()) {
+              if (null != scheduledCompactionInstantAndRDD && scheduledCompactionInstantAndRDD.getLeft().isPresent()) {
                 LOG.info("Enqueuing new pending compaction instant (" + scheduledCompactionInstantAndRDD.getLeft() + ")");
                 asyncCompactService.enqueuePendingCompaction(new HoodieInstant(State.REQUESTED,
                     HoodieTimeline.COMPACTION_ACTION, scheduledCompactionInstantAndRDD.getLeft().get()));

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java
Patch:
@@ -278,7 +278,8 @@ public static class Config implements Serializable {
         + "hoodie client for importing")
     public String propsFilePath = null;
     @Parameter(names = {"--hoodie-conf"}, description = "Any configuration that can be set in the properties file "
-        + "(using the CLI parameter \"--propsFilePath\") can also be passed command line using this parameter")
+        + "(using the CLI parameter \"--props\") can also be passed command line using this parameter. This can be repeated",
+            splitter = IdentitySplitter.class)
     public List<String> configs = new ArrayList<>();
     @Parameter(names = {"--help", "-h"}, help = true)
     public Boolean help = false;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java
Patch:
@@ -88,7 +88,8 @@ public static class Config implements Serializable {
     public String propsFilePath = null;
 
     @Parameter(names = {"--hoodie-conf"}, description = "Any configuration that can be set in the properties file "
-        + "(using the CLI parameter \"--propsFilePath\") can also be passed command line using this parameter")
+        + "(using the CLI parameter \"--props\") can also be passed command line using this parameter. This can be repeated",
+            splitter = IdentitySplitter.class)
     public List<String> configs = new ArrayList<>();
 
     @Parameter(names = {"--spark-master"}, description = "spark master to use.")

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java
Patch:
@@ -90,7 +90,8 @@ public static class Config implements Serializable {
     public String propsFilePath = null;
 
     @Parameter(names = {"--hoodie-conf"}, description = "Any configuration that can be set in the properties file "
-        + "(using the CLI parameter \"--propsFilePath\") can also be passed command line using this parameter")
+        + "(using the CLI parameter \"--props\") can also be passed command line using this parameter. This can be repeated",
+            splitter = IdentitySplitter.class)
     public List<String> configs = new ArrayList<>();
   }
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.ValidationUtils;
+import org.apache.hudi.utilities.IdentitySplitter;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.utilities.UtilHelpers;
 import org.apache.hudi.utilities.schema.SchemaRegistryProvider;
@@ -226,7 +227,8 @@ public static class Config implements Serializable {
         "file://" + System.getProperty("user.dir") + "/src/test/resources/delta-streamer-config/dfs-source.properties";
 
     @Parameter(names = {"--hoodie-conf"}, description = "Any configuration that can be set in the properties file "
-        + "(using the CLI parameter \"--props\") can also be passed command line using this parameter")
+        + "(using the CLI parameter \"--props\") can also be passed command line using this parameter. This can be repeated",
+            splitter = IdentitySplitter.class)
     public List<String> configs = new ArrayList<>();
 
     @Parameter(names = {"--source-class"},

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/BulkInsertHelper.java
Patch:
@@ -51,7 +51,7 @@ public static <T extends HoodieRecordPayload<T>> HoodieWriteMetadata bulkInsert(
 
     if (performDedupe) {
       dedupedRecords = WriteHelper.combineOnCondition(config.shouldCombineBeforeInsert(), inputRecords,
-          config.getInsertShuffleParallelism(), ((HoodieTable<T>)table));
+          config.getBulkInsertShuffleParallelism(), ((HoodieTable<T>)table));
     }
 
     final JavaRDD<HoodieRecord<T>> repartitionedRecords;

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestArchivedCommitsCommand.java
Patch:
@@ -92,7 +92,7 @@ public void init() throws IOException {
 
     // archive
     HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(cfg, hadoopConf);
-    archiveLog.archiveIfRequired();
+    archiveLog.archiveIfRequired(jsc);
   }
 
   @AfterEach

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCommitsCommand.java
Patch:
@@ -176,7 +176,7 @@ public void testShowArchivedCommits() throws IOException {
     // archive
     metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());
     HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(cfg, jsc.hadoopConfiguration());
-    archiveLog.archiveIfRequired();
+    archiveLog.archiveIfRequired(jsc);
 
     CommandResult cr = getShell().executeCommand(String.format("commits showarchived --startTs %s --endTs %s", "100", "104"));
     assertTrue(cr.isSuccess());

File: hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java
Patch:
@@ -337,7 +337,7 @@ protected void postCommit(HoodieTable<?> table, HoodieCommitMetadata metadata, S
     try {
 
       // Delete the marker directory for the instant.
-      new MarkerFiles(table, instantTime).quietDeleteMarkerDir();
+      new MarkerFiles(table, instantTime).quietDeleteMarkerDir(jsc, config.getMarkersDeleteParallelism());
 
       // Do an inline compaction if enabled
       if (config.isInlineCompaction()) {
@@ -349,7 +349,7 @@ protected void postCommit(HoodieTable<?> table, HoodieCommitMetadata metadata, S
       }
       // We cannot have unbounded commit files. Archive commits if we have to archive
       HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(config, hadoopConf);
-      archiveLog.archiveIfRequired();
+      archiveLog.archiveIfRequired(jsc);
       autoCleanOnCommit(instantTime);
     } catch (IOException ioe) {
       throw new HoodieIOException(ioe.getMessage(), ioe);

File: hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java
Patch:
@@ -113,7 +113,7 @@ public HoodieRollbackMetadata execute() {
     }
 
     // Finally, remove the marker files post rollback.
-    new MarkerFiles(table, instantToRollback.getTimestamp()).quietDeleteMarkerDir();
+    new MarkerFiles(table, instantToRollback.getTimestamp()).quietDeleteMarkerDir(jsc, config.getMarkersDeleteParallelism());
 
     return rollbackMetadata;
   }

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java
Patch:
@@ -132,7 +132,7 @@ private Schema constructHiveOrderedSchema(Schema writerSchema, Map<String, Field
     Schema hiveSchema = Schema.createRecord(writerSchema.getName(), writerSchema.getDoc(), writerSchema.getNamespace(),
         writerSchema.isError());
     hiveSchema.setFields(hiveSchemaFields);
-    LOG.info("HIVE Schema is :" + hiveSchema.toString(true));
+    LOG.debug("HIVE Schema is :" + hiveSchema.toString(true));
     return hiveSchema;
   }
 

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java
Patch:
@@ -65,9 +65,9 @@ public class HoodieIndexConfig extends DefaultHoodieConfig {
   public static final String SIMPLE_INDEX_USE_CACHING_PROP = "hoodie.simple.index.use.caching";
   public static final String DEFAULT_SIMPLE_INDEX_USE_CACHING = "true";
   public static final String SIMPLE_INDEX_PARALLELISM_PROP = "hoodie.simple.index.parallelism";
-  public static final String DEFAULT_SIMPLE_INDEX_PARALLELISM = "0";
+  public static final String DEFAULT_SIMPLE_INDEX_PARALLELISM = "50";
   public static final String GLOBAL_SIMPLE_INDEX_PARALLELISM_PROP = "hoodie.global.simple.index.parallelism";
-  public static final String DEFAULT_GLOBAL_SIMPLE_INDEX_PARALLELISM = "0";
+  public static final String DEFAULT_GLOBAL_SIMPLE_INDEX_PARALLELISM = "100";
 
   // 1B bloom filter checks happen in 250 seconds. 500ms to read a bloom filter.
   // 10M checks in 2500ms, thus amortizing the cost of reading bloom filter across partitions.

File: hudi-spark/src/test/java/HoodieJavaGenerateApp.java
Patch:
@@ -30,7 +30,7 @@
 import org.apache.hudi.keygen.NonpartitionedKeyGenerator;
 import org.apache.hudi.keygen.SimpleKeyGenerator;
 import org.apache.hudi.testutils.DataSourceTestUtils;
-import org.apache.hudi.testutils.HoodieTestDataGenerator;
+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 import org.apache.spark.api.java.JavaSparkContext;

File: hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -589,7 +589,7 @@ private Dataset<org.apache.spark.sql.Row> getAllRows(String[] fullPartitionPaths
   }
 
   private String[] getFullPartitionPaths() {
-   return getFullPartitionPaths(dataGen.getPartitionPaths());
+    return getFullPartitionPaths(dataGen.getPartitionPaths());
   }
 
   private String[] getFullPartitionPaths(String[] relativePartitionPaths) {

File: hudi-client/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java
Patch:
@@ -65,7 +65,7 @@ private HoodieCompactionPlan scheduleCompaction() {
     int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()
         .findInstantsAfter(deltaCommitsSinceTs, Integer.MAX_VALUE).countInstants();
     if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {
-      LOG.info("Not running compaction as only " + deltaCommitsSinceLastCompaction
+      LOG.info("Not scheduling compaction as only " + deltaCommitsSinceLastCompaction
           + " delta commits was found since last compaction " + deltaCommitsSinceTs + ". Waiting for "
           + config.getInlineCompactDeltaCommitMax());
       return new HoodieCompactionPlan();

File: hudi-client/src/test/java/org/apache/hudi/testutils/HoodieTestDataGenerator.java
Patch:
@@ -226,7 +226,7 @@ public TestRawTripPayload generatePayloadForShortTripSchema(HoodieKey key, Strin
   public static TestRawTripPayload generateRandomDeleteValue(HoodieKey key, String instantTime) throws IOException {
     GenericRecord rec = generateGenericRecord(key.getRecordKey(), "rider-" + instantTime, "driver-" + instantTime, 0.0,
         true, false);
-    return new TestRawTripPayload(rec.toString(), key.getRecordKey(), key.getPartitionPath(), TRIP_EXAMPLE_SCHEMA);
+    return new TestRawTripPayload(Option.of(rec.toString()), key.getRecordKey(), key.getPartitionPath(), TRIP_EXAMPLE_SCHEMA, true);
   }
 
   /**

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestArchivedCommitsCommand.java
Patch:
@@ -91,8 +91,8 @@ public void init() throws IOException {
     metaClient.getActiveTimeline().reload().getAllCommitsTimeline().filterCompletedInstants();
 
     // archive
-    HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(cfg, metaClient);
-    archiveLog.archiveIfRequired(hadoopConf);
+    HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(cfg, hadoopConf);
+    archiveLog.archiveIfRequired();
   }
 
   @AfterEach

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCommitsCommand.java
Patch:
@@ -175,8 +175,8 @@ public void testShowArchivedCommits() throws IOException {
 
     // archive
     metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());
-    HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(cfg, metaClient);
-    archiveLog.archiveIfRequired(jsc.hadoopConfiguration());
+    HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(cfg, jsc.hadoopConfiguration());
+    archiveLog.archiveIfRequired();
 
     CommandResult cr = getShell().executeCommand(String.format("commits showarchived --startTs %s --endTs %s", "100", "104"));
     assertTrue(cr.isSuccess());

File: hudi-client/src/main/java/org/apache/hudi/table/action/restore/CopyOnWriteRestoreActionExecutor.java
Patch:
@@ -48,7 +48,8 @@ protected HoodieRollbackMetadata rollbackInstant(HoodieInstant instantToRollback
         HoodieActiveTimeline.createNewInstantTime(),
         instantToRollback,
         true,
-        true);
+        true,
+        false);
     if (!instantToRollback.getAction().equals(HoodieTimeline.COMMIT_ACTION)) {
       throw new HoodieRollbackException("Unsupported action in rollback instant:" + instantToRollback);
     }

File: hudi-client/src/main/java/org/apache/hudi/table/action/restore/MergeOnReadRestoreActionExecutor.java
Patch:
@@ -47,7 +47,8 @@ protected HoodieRollbackMetadata rollbackInstant(HoodieInstant instantToRollback
         HoodieActiveTimeline.createNewInstantTime(),
         instantToRollback,
         true,
-        true);
+        true,
+        false);
 
     switch (instantToRollback.getAction()) {
       case HoodieTimeline.COMMIT_ACTION:

File: hudi-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java
Patch:
@@ -68,7 +68,7 @@ private UpsertPartitioner getUpsertPartitioner(int smallFileSize, int numInserts
             .insertSplitSize(100).autoTuneInsertSplits(autoSplitInserts).build())
         .withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1000 * 1024).build()).build();
 
-    HoodieClientTestUtils.fakeCommitFile(basePath, "001");
+    HoodieClientTestUtils.fakeCommit(basePath, "001");
     HoodieClientTestUtils.fakeDataFile(basePath, testPartitionPath, "001", "file1", fileSize);
     metaClient = HoodieTableMetaClient.reload(metaClient);
     HoodieCopyOnWriteTable table = (HoodieCopyOnWriteTable) HoodieTable.create(metaClient, config, hadoopConf);

File: hudi-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestHarness.java
Patch:
@@ -37,7 +37,6 @@
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
-import org.apache.hudi.table.HoodieTable;
 
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.SQLContext;
@@ -71,7 +70,6 @@ public abstract class HoodieClientTestHarness extends HoodieCommonTestHarness im
   protected transient HoodieWriteClient writeClient;
   protected transient HoodieReadClient readClient;
   protected transient HoodieTableFileSystemView tableView;
-  protected transient HoodieTable hoodieTable;
 
   protected final SparkTaskContextSupplier supplier = new SparkTaskContextSupplier();
 

File: hudi-client/src/test/java/org/apache/hudi/testutils/HoodieMergeOnReadTestUtils.java
Patch:
@@ -90,7 +90,7 @@ public static List<GenericRecord> getRecordsUsingInputFormat(Configuration conf,
     }).reduce((a, b) -> {
       a.addAll(b);
       return a;
-    }).orElse(new ArrayList<GenericRecord>());
+    }).orElse(new ArrayList<>());
   }
 
   private static void setPropsForInputFormat(FileInputFormat inputFormat, JobConf jobConf, Schema schema,

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieSnapshotExporter.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hudi.utilities.functional;
 
+import org.apache.hadoop.fs.LocatedFileStatus;
+import org.apache.hadoop.fs.RemoteIterator;
 import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.common.model.HoodieAvroPayload;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -35,9 +37,7 @@
 import org.apache.hudi.utilities.exception.HoodieSnapshotExporterException;
 
 import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.LocatedFileStatus;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RemoteIterator;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 import org.apache.spark.api.java.JavaRDD;
@@ -92,7 +92,6 @@ public void init() throws Exception {
     JavaRDD<HoodieRecord> recordsRDD = jsc().parallelize(records, 1);
     hdfsWriteClient.bulkInsert(recordsRDD, COMMIT_TIME);
     hdfsWriteClient.close();
-
     RemoteIterator<LocatedFileStatus> itr = dfs().listFiles(new Path(sourcePath), true);
     while (itr.hasNext()) {
       LOG.info(">>> Prepared test file: " + itr.next().getPath());

File: hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java
Patch:
@@ -179,7 +179,7 @@ public List<HoodieRecord> generateUpdates(Integer n) throws IOException {
      */
     public List<String> generateDeletes(List<Row> rows) {
       return rows.stream().map(row ->
-          convertToString(row.getAs("uuid"), row.getAs("partitionPath"))).filter(os -> os.isPresent()).map(os -> os.get())
+          convertToString(row.getAs("uuid"), row.getAs("partitionpath"))).filter(os -> os.isPresent()).map(os -> os.get())
           .collect(Collectors.toList());
     }
 

File: hudi-client/src/test/java/org/apache/hudi/client/TestClientRollback.java
Patch:
@@ -42,6 +42,7 @@
 import java.util.List;
 import java.util.stream.Collectors;
 
+import static org.apache.hudi.testutils.Assertions.assertNoWriteErrors;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertThrows;

File: hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -72,6 +72,7 @@
 
 import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_0;
 import static org.apache.hudi.common.util.ParquetUtils.readRowKeysFromParquet;
+import static org.apache.hudi.testutils.Assertions.assertNoWriteErrors;
 import static org.apache.hudi.testutils.HoodieTestDataGenerator.NULL_SCHEMA;
 import static org.apache.hudi.testutils.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;
 import static org.junit.jupiter.api.Assertions.assertEquals;

File: hudi-client/src/test/java/org/apache/hudi/client/TestHoodieReadClient.java
Patch:
@@ -37,6 +37,7 @@
 import java.util.List;
 import java.util.stream.Collectors;
 
+import static org.apache.hudi.testutils.Assertions.assertNoWriteErrors;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertThrows;
 

File: hudi-client/src/test/java/org/apache/hudi/table/TestCleaner.java
Patch:
@@ -82,6 +82,7 @@
 import scala.Tuple3;
 
 import static org.apache.hudi.common.testutils.HoodieTestUtils.DEFAULT_PARTITION_PATHS;
+import static org.apache.hudi.testutils.Assertions.assertNoWriteErrors;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertNull;

File: hudi-client/src/test/java/org/apache/hudi/table/action/compact/TestAsyncCompaction.java
Patch:
@@ -61,6 +61,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
+import static org.apache.hudi.testutils.Assertions.assertNoWriteErrors;
 import static org.apache.hudi.testutils.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertFalse;

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -34,7 +34,7 @@
 import org.apache.hudi.common.model.HoodieWriteStat.RuntimeStats;
 import org.apache.hudi.common.table.log.HoodieLogFormat;
 import org.apache.hudi.common.table.log.HoodieLogFormat.Writer;
-import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;
+import org.apache.hudi.common.table.log.block.HoodieDataBlock;
 import org.apache.hudi.common.table.log.block.HoodieDeleteBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock.HeaderMetadataType;
@@ -207,7 +207,7 @@ private void doAppend(Map<HeaderMetadataType, String> header) {
       header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, instantTime);
       header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, writerSchema.toString());
       if (recordList.size() > 0) {
-        writer = writer.appendBlock(new HoodieAvroDataBlock(recordList, header));
+        writer = writer.appendBlock(HoodieDataBlock.getBlock(hoodieTable.getLogDataBlockFormat(), recordList, header));
         recordList.clear();
       }
       if (keysToDelete.size() > 0) {

File: hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieFileWriter.java
Patch:
@@ -24,7 +24,7 @@
 
 import java.io.IOException;
 
-public interface HoodieStorageWriter<R extends IndexedRecord> {
+public interface HoodieFileWriter<R extends IndexedRecord> {
 
   void writeAvroWithMetadata(R newRecord, HoodieRecord record) throws IOException;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/deltacommit/DeltaCommitActionExecutor.java
Patch:
@@ -83,7 +83,7 @@ public Iterator<List<WriteStatus>> handleUpdate(String partitionPath, String fil
   @Override
   public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)
       throws Exception {
-    // If canIndexLogFiles, write inserts to log files else write inserts to parquet files
+    // If canIndexLogFiles, write inserts to log files else write inserts to base files
     if (table.getIndex().canIndexLogFiles()) {
       return new LazyInsertIterable<>(recordItr, config, instantTime, (HoodieTable<T>)table, idPfx,
           sparkTaskContextSupplier, new AppendHandleFactory<>());

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordScanner.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;
 import org.apache.hudi.common.table.log.block.HoodieCommandBlock;
+import org.apache.hudi.common.table.log.block.HoodieDataBlock;
 import org.apache.hudi.common.table.log.block.HoodieDeleteBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -267,7 +268,7 @@ private boolean isNewInstantBlock(HoodieLogBlock logBlock) {
    * Iterate over the GenericRecord in the block, read the hoodie key and partition path and call subclass processors to
    * handle it.
    */
-  private void processAvroDataBlock(HoodieAvroDataBlock dataBlock) throws Exception {
+  private void processDataBlock(HoodieDataBlock dataBlock) throws Exception {
     // TODO (NA) - Implement getRecordItr() in HoodieAvroDataBlock and use that here
     List<IndexedRecord> recs = dataBlock.getRecords();
     totalLogRecords.addAndGet(recs.size());
@@ -302,7 +303,7 @@ private void processQueuedBlocksForInstant(Deque<HoodieLogBlock> lastBlocks, int
       HoodieLogBlock lastBlock = lastBlocks.pollLast();
       switch (lastBlock.getBlockType()) {
         case AVRO_DATA_BLOCK:
-          processAvroDataBlock((HoodieAvroDataBlock) lastBlock);
+          processDataBlock((HoodieAvroDataBlock) lastBlock);
           break;
         case DELETE_BLOCK:
           Arrays.stream(((HoodieDeleteBlock) lastBlock).getKeysToDelete()).forEach(this::processNextDeletedKey);

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java
Patch:
@@ -193,7 +193,7 @@ private HoodieLogBlock readBlock() throws IOException {
         if (nextBlockVersion.getVersion() == HoodieLogFormatVersion.DEFAULT_VERSION) {
           return HoodieAvroDataBlock.getBlock(content, readerSchema);
         } else {
-          return HoodieAvroDataBlock.getBlock(logFile, inputStream, Option.ofNullable(content), readBlockLazily,
+          return new HoodieAvroDataBlock(logFile, inputStream, Option.ofNullable(content), readBlockLazily,
               contentPosition, contentLength, blockEndPos, readerSchema, header, footer);
         }
       case DELETE_BLOCK:

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/LogReaderUtils.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;
-import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;
+import org.apache.hudi.common.table.log.block.HoodieDataBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock.HeaderMetadataType;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
@@ -49,8 +49,8 @@ private static Schema readSchemaFromLogFileInReverse(FileSystem fs, HoodieActive
     HoodieTimeline completedTimeline = activeTimeline.getCommitsTimeline().filterCompletedInstants();
     while (reader.hasPrev()) {
       HoodieLogBlock block = reader.prev();
-      if (block instanceof HoodieAvroDataBlock) {
-        HoodieAvroDataBlock lastBlock = (HoodieAvroDataBlock) block;
+      if (block instanceof HoodieDataBlock) {
+        HoodieDataBlock lastBlock = (HoodieDataBlock) block;
         if (completedTimeline
             .containsOrBeforeTimelineStarts(lastBlock.getLogBlockHeader().get(HeaderMetadataType.INSTANT_TIME))) {
           writerSchema = new Schema.Parser().parse(lastBlock.getLogBlockHeader().get(HeaderMetadataType.SCHEMA));

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieLogBlock.java
Patch:
@@ -84,9 +84,7 @@ public byte[] getMagic() {
     throw new HoodieException("No implementation was provided");
   }
 
-  public HoodieLogBlockType getBlockType() {
-    throw new HoodieException("No implementation was provided");
-  }
+  public abstract HoodieLogBlockType getBlockType();
 
   public long getLogBlockLength() {
     throw new HoodieException("No implementation was provided");

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -121,7 +121,8 @@ protected List<HoodieFileGroup> addFilesToView(FileStatus[] statuses) {
       }
     });
     long storePartitionsTs = timer.endTimer();
-    LOG.info("addFilesToView: NumFiles=" + statuses.length + ", FileGroupsCreationTime=" + fgBuildTimeTakenMs
+    LOG.info("addFilesToView: NumFiles=" + statuses.length + ", NumFileGroups=" + fileGroups.size()
+        + ", FileGroupsCreationTime=" + fgBuildTimeTakenMs
         + ", StoreTimeTaken=" + storePartitionsTs);
     return fileGroups;
   }

File: hudi-common/src/main/java/org/apache/hudi/common/util/ParquetReaderIterator.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.client.utils;
+package org.apache.hudi.common.util;
 
 import org.apache.hudi.common.util.queue.BoundedInMemoryQueue;
 import org.apache.hudi.exception.HoodieIOException;

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestParquetReaderIterator.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.client.utils;
+package org.apache.hudi.common.util;
 
 import org.apache.hudi.exception.HoodieIOException;
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.hadoop;
 
+import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
@@ -101,7 +102,8 @@ public FileStatus[] listStatus(JobConf job) throws IOException {
       setInputPaths(job, snapshotPaths.toArray(new Path[snapshotPaths.size()]));
       FileStatus[] fileStatuses = super.listStatus(job);
       Map<HoodieTableMetaClient, List<FileStatus>> groupedFileStatus =
-          HoodieInputFormatUtils.groupFileStatusForSnapshotPaths(fileStatuses, tableMetaClientMap.values());
+          HoodieInputFormatUtils.groupFileStatusForSnapshotPaths(fileStatuses,
+              HoodieFileFormat.PARQUET.getFileExtension(), tableMetaClientMap.values());
       LOG.info("Found a total of " + groupedFileStatus.size() + " groups");
       for (Map.Entry<HoodieTableMetaClient, List<FileStatus>> entry : groupedFileStatus.entrySet()) {
         List<FileStatus> result = HoodieInputFormatUtils.filterFileStatusForSnapshotMode(job, entry.getKey(), entry.getValue());

File: hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java
Patch:
@@ -35,6 +35,9 @@ public class HiveSyncConfig implements Serializable {
   @Parameter(names = {"--table"}, description = "name of the target table in Hive", required = true)
   public String tableName;
 
+  @Parameter(names = {"--base-file-format"}, description = "Format of the base files (PARQUET (or) HFILE)")
+  public String baseFileFormat = "PARQUET";
+
   @Parameter(names = {"--user"}, description = "Hive username", required = true)
   public String hiveUser;
 

File: hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -270,7 +270,7 @@ public static JavaRDD<HoodieRecord> dropDuplicates(JavaSparkContext jssc, JavaRD
     return dropDuplicates(jssc, incomingHoodieRecords, writeConfig);
   }
 
-  public static HiveSyncConfig buildHiveSyncConfig(TypedProperties props, String basePath) {
+  public static HiveSyncConfig buildHiveSyncConfig(TypedProperties props, String basePath, String baseFileFormat) {
     checkRequiredProperties(props, Collections.singletonList(DataSourceWriteOptions.HIVE_TABLE_OPT_KEY()));
     HiveSyncConfig hiveSyncConfig = new HiveSyncConfig();
     hiveSyncConfig.basePath = basePath;
@@ -280,6 +280,7 @@ public static HiveSyncConfig buildHiveSyncConfig(TypedProperties props, String b
     hiveSyncConfig.databaseName = props.getString(DataSourceWriteOptions.HIVE_DATABASE_OPT_KEY(),
         DataSourceWriteOptions.DEFAULT_HIVE_DATABASE_OPT_VAL());
     hiveSyncConfig.tableName = props.getString(DataSourceWriteOptions.HIVE_TABLE_OPT_KEY());
+    hiveSyncConfig.baseFileFormat = baseFileFormat;
     hiveSyncConfig.hiveUser =
         props.getString(DataSourceWriteOptions.HIVE_USER_OPT_KEY(), DataSourceWriteOptions.DEFAULT_HIVE_USER_OPT_VAL());
     hiveSyncConfig.hivePass =

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -205,7 +205,7 @@ private void refreshTimeline() throws IOException {
     } else {
       this.commitTimelineOpt = Option.empty();
       HoodieTableMetaClient.initTableType(new Configuration(jssc.hadoopConfiguration()), cfg.targetBasePath,
-          cfg.tableType, cfg.targetTableName, "archived", cfg.payloadClassName);
+          cfg.tableType, cfg.targetTableName, "archived", cfg.payloadClassName, cfg.baseFileFormat);
     }
   }
 
@@ -274,7 +274,7 @@ private Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource
       }
     } else {
       HoodieTableMetaClient.initTableType(new Configuration(jssc.hadoopConfiguration()), cfg.targetBasePath,
-          cfg.tableType, cfg.targetTableName, "archived", cfg.payloadClassName);
+          cfg.tableType, cfg.targetTableName, "archived", cfg.payloadClassName, cfg.baseFileFormat);
     }
 
     if (!resumeCheckpointStr.isPresent() && cfg.checkpoint != null) {
@@ -474,7 +474,7 @@ private String startCommit() {
    */
   private void syncHive() {
     if (cfg.enableHiveSync) {
-      HiveSyncConfig hiveSyncConfig = DataSourceUtils.buildHiveSyncConfig(props, cfg.targetBasePath);
+      HiveSyncConfig hiveSyncConfig = DataSourceUtils.buildHiveSyncConfig(props, cfg.targetBasePath, cfg.baseFileFormat);
       LOG.info("Syncing target hoodie table with hive table(" + hiveSyncConfig.tableName + "). Hive metastore URL :"
           + hiveSyncConfig.jdbcUrl + ", basePath :" + cfg.targetBasePath);
       new HiveSyncTool(hiveSyncConfig, new HiveConf(conf, HiveConf.class), fs).syncHoodieTable();

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java
Patch:
@@ -44,6 +44,7 @@
 import org.apache.parquet.schema.MessageType;
 
 import java.io.IOException;
+import java.nio.ByteBuffer;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.LinkedHashSet;
@@ -146,7 +147,7 @@ public static Writable avroToArrayWritable(Object value, Schema schema) {
       case STRING:
         return new Text(value.toString());
       case BYTES:
-        return new BytesWritable((byte[]) value);
+        return new BytesWritable(((ByteBuffer)value).array());
       case INT:
         return new IntWritable((Integer) value);
       case LONG:

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
Patch:
@@ -103,7 +103,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {
   private static final String DEFAULT_AUTO_CLEAN = "true";
   private static final String DEFAULT_INLINE_COMPACT = "false";
   private static final String DEFAULT_INCREMENTAL_CLEANER = "true";
-  private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = "1";
+  private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = "5";
   private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = "3";
   private static final String DEFAULT_CLEANER_COMMITS_RETAINED = "10";
   private static final String DEFAULT_MAX_COMMITS_TO_KEEP = "30";

File: hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.common.table.TableSchemaResolver;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieInsertException;
@@ -467,6 +468,7 @@ private List<HoodieRecord> convertToSchema(List<HoodieRecord> records, String sc
   private HoodieWriteConfig getWriteConfig(String schema) {
     return getConfigBuilder(schema)
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(IndexType.INMEMORY).build())
+        .withCompactionConfig(HoodieCompactionConfig.newBuilder().withMaxNumDeltaCommitsBeforeCompaction(1).build())
         .withAvroSchemaValidate(true)
         .build();
   }

File: hudi-spark/src/test/java/HoodieJavaApp.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.HoodieDataSourceHelpers;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
+import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.hive.MultiPartKeysValueExtractor;
 import org.apache.hudi.hive.NonPartitionedExtractor;
@@ -173,6 +174,7 @@ public void run() throws Exception {
         .option(DataSourceWriteOptions.KEYGENERATOR_CLASS_OPT_KEY(),
             nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()
                 : SimpleKeyGenerator.class.getCanonicalName()) // Add Key Extractor
+        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, "1")
         .option(HoodieWriteConfig.TABLE_NAME, tableName).mode(SaveMode.Append);
 
     updateHiveSyncConfig(writer);
@@ -197,6 +199,7 @@ public void run() throws Exception {
         .option(DataSourceWriteOptions.KEYGENERATOR_CLASS_OPT_KEY(),
             nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()
                 : SimpleKeyGenerator.class.getCanonicalName()) // Add Key Extractor
+        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, "1")
         .option(HoodieWriteConfig.TABLE_NAME, tableName).mode(SaveMode.Append);
 
     updateHiveSyncConfig(writer);

File: hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java
Patch:
@@ -490,7 +490,7 @@ public HoodieCleanMetadata clean(String cleanInstantTime) throws HoodieIOExcepti
     LOG.info("Cleaner started");
     final Timer.Context context = metrics.getCleanCtx();
     HoodieCleanMetadata metadata = HoodieTable.create(config, hadoopConf).clean(jsc, cleanInstantTime);
-    if (context != null) {
+    if (context != null && metadata != null) {
       long durationMs = metrics.getDurationInMs(context.stop());
       metrics.updateCleanMetrics(durationMs, metadata.getTotalFilesDeleted());
       LOG.info("Cleaned " + metadata.getTotalFilesDeleted() + " files"

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java
Patch:
@@ -80,7 +80,7 @@ public RealtimeUnmergedRecordReader(HoodieRealtimeFileSplit split, JobConf job,
         false, jobConf.getInt(MAX_DFS_STREAM_BUFFER_SIZE_PROP, DEFAULT_MAX_DFS_STREAM_BUFFER_SIZE), record -> {
           // convert Hoodie log record to Hadoop AvroWritable and buffer
           GenericRecord rec = (GenericRecord) record.getData().getInsertValue(getReaderSchema()).get();
-          ArrayWritable aWritable = (ArrayWritable) avroToArrayWritable(rec, getWriterSchema());
+          ArrayWritable aWritable = (ArrayWritable) avroToArrayWritable(rec, getHiveSchema());
           this.executor.getQueue().insertRecord(aWritable);
     });
     // Start reading and buffering

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java
Patch:
@@ -183,7 +183,7 @@ private List<FileStatus> listStatusForIncrementalMode(
       return null;
     }
     String incrementalInputPaths = partitionsToList.stream()
-        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)
+        .map(s -> StringUtils.isNullOrEmpty(s) ? tableMetaClient.getBasePath() : tableMetaClient.getBasePath() + Path.SEPARATOR + s)
         .filter(s -> {
           /*
            * Ensure to return only results from the original input path that has incremental changes

File: hudi-client/src/test/java/org/apache/hudi/client/TestMultiFS.java
Patch:
@@ -63,9 +63,7 @@ public void setUp() throws Exception {
 
   @AfterEach
   public void tearDown() throws Exception {
-    cleanupSparkContexts();
-    cleanupDFS();
-    cleanupTestDataGenerator();
+    cleanupResources();
   }
 
   protected HoodieWriteConfig getHoodieWriteConfig(String basePath) {

File: hudi-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java
Patch:
@@ -61,8 +61,7 @@ public void setUp() throws Exception {
 
   @AfterEach
   public void tearDown() throws IOException {
-    cleanupSparkContexts();
-    cleanupFileSystem();
+    cleanupResources();
   }
 
   @Test

File: hudi-client/src/test/java/org/apache/hudi/execution/TestBoundedInMemoryQueue.java
Patch:
@@ -72,8 +72,7 @@ public void setUp() throws Exception {
 
   @AfterEach
   public void tearDown() throws Exception {
-    cleanupTestDataGenerator();
-    cleanupExecutorService();
+    cleanupResources();
   }
 
   // Test to ensure that we are reading all records from queue iterator in the same order

File: hudi-client/src/test/java/org/apache/hudi/execution/TestSparkBoundedInMemoryExecutor.java
Patch:
@@ -53,7 +53,7 @@ public void setUp() throws Exception {
 
   @AfterEach
   public void tearDown() throws Exception {
-    cleanupTestDataGenerator();
+    cleanupResources();
   }
 
   @Test

File: hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieBloomIndex.java
Patch:
@@ -97,9 +97,7 @@ public void setUp() throws Exception {
 
   @AfterEach
   public void tearDown() throws Exception {
-    cleanupSparkContexts();
-    cleanupFileSystem();
-    cleanupClients();
+    cleanupResources();
   }
 
   private HoodieWriteConfig makeConfig(boolean rangePruning, boolean treeFiltering, boolean bucketizedChecking) {

File: hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieGlobalBloomIndex.java
Patch:
@@ -80,9 +80,8 @@ public void setUp() throws Exception {
   }
 
   @AfterEach
-  public void tearDown() {
-    cleanupSparkContexts();
-    cleanupClients();
+  public void tearDown() throws IOException {
+    cleanupResources();
   }
 
   @Test

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieCommitArchiveLog.java
Patch:
@@ -69,8 +69,7 @@ public void init() throws Exception {
 
   @AfterEach
   public void clean() throws IOException {
-    cleanupDFS();
-    cleanupSparkContexts();
+    cleanupResources();
   }
 
   @Test

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieKeyLocationFetchHandle.java
Patch:
@@ -82,9 +82,7 @@ public void setUp() throws Exception {
 
   @AfterEach
   public void tearDown() throws IOException {
-    cleanupSparkContexts();
-    cleanupFileSystem();
-    cleanupClients();
+    cleanupResources();
   }
 
   @Test

File: hudi-client/src/test/java/org/apache/hudi/table/TestConsistencyGuard.java
Patch:
@@ -44,7 +44,7 @@ public void setup() {
 
   @AfterEach
   public void tearDown() throws Exception {
-    cleanupFileSystem();
+    cleanupResources();
   }
 
   @Test

File: hudi-client/src/test/java/org/apache/hudi/table/action/compact/TestAsyncCompaction.java
Patch:
@@ -523,7 +523,7 @@ private List<WriteStatus> createNextDeltaCommit(String instantTime, List<HoodieR
   private List<HoodieBaseFile> getCurrentLatestDataFiles(HoodieTable table, HoodieWriteConfig cfg) throws IOException {
     FileStatus[] allFiles = HoodieTestUtils.listAllDataFilesInPath(table.getMetaClient().getFs(), cfg.getBasePath());
     HoodieTableFileSystemView view =
-        new HoodieTableFileSystemView(table.getMetaClient(), table.getCompletedCommitsTimeline(), allFiles);
+        getHoodieTableFileSystemView(table.getMetaClient(), table.getCompletedCommitsTimeline(), allFiles);
     return view.getLatestBaseFiles().collect(Collectors.toList());
   }
 

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java
Patch:
@@ -117,7 +117,7 @@ private boolean syncIfLocalViewBehind(Context ctx) {
       synchronized (view) {
         if (isLocalViewBehind(ctx)) {
           HoodieTimeline localTimeline = viewManager.getFileSystemView(basePath).getTimeline();
-          LOG.warn("Syncing view as client passed last known instant " + lastKnownInstantFromClient
+          LOG.info("Syncing view as client passed last known instant " + lastKnownInstantFromClient
               + " as last known instant but server has the folling timeline :"
               + localTimeline.getInstants().collect(Collectors.toList()));
           view.sync();

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestSavepointsCommand.java
Patch:
@@ -75,8 +75,9 @@ public void testShowSavepoints() throws IOException {
     String[][] rows = Arrays.asList("100", "101", "102", "103").stream().sorted(Comparator.reverseOrder())
         .map(instant -> new String[]{instant}).toArray(String[][]::new);
     String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_SAVEPOINT_TIME}, rows);
-
-    assertEquals(expected, cr.getResult().toString());
+    expected = removeNonWordAndStripSpace(expected);
+    String got = removeNonWordAndStripSpace(cr.getResult().toString());
+    assertEquals(expected, got);
   }
 
   /**

File: hudi-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.client;
 
+import java.io.IOException;
 import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
@@ -59,8 +60,9 @@ public void setUp() throws Exception {
   }
 
   @AfterEach
-  public void tearDown() {
+  public void tearDown() throws IOException {
     cleanupSparkContexts();
+    cleanupFileSystem();
   }
 
   @Test

File: hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieBloomIndex.java
Patch:
@@ -99,7 +99,7 @@ public void setUp() throws Exception {
   public void tearDown() throws Exception {
     cleanupSparkContexts();
     cleanupFileSystem();
-    cleanupMetaClient();
+    cleanupClients();
   }
 
   private HoodieWriteConfig makeConfig(boolean rangePruning, boolean treeFiltering, boolean bucketizedChecking) {

File: hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieGlobalBloomIndex.java
Patch:
@@ -82,7 +82,7 @@ public void setUp() throws Exception {
   @AfterEach
   public void tearDown() {
     cleanupSparkContexts();
-    cleanupMetaClient();
+    cleanupClients();
   }
 
   @Test

File: hudi-client/src/test/java/org/apache/hudi/index/hbase/TestHBaseQPSResourceAllocator.java
Patch:
@@ -59,7 +59,7 @@ public void setUp() throws Exception {
   @AfterEach
   public void tearDown() throws Exception {
     cleanupSparkContexts();
-    cleanupMetaClient();
+    cleanupClients();
     if (utility != null) {
       utility.shutdownMiniCluster();
     }

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieKeyLocationFetchHandle.java
Patch:
@@ -84,7 +84,7 @@ public void setUp() throws Exception {
   public void tearDown() throws IOException {
     cleanupSparkContexts();
     cleanupFileSystem();
-    cleanupMetaClient();
+    cleanupClients();
   }
 
   @Test

File: hudi-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java
Patch:
@@ -85,7 +85,7 @@ public void setUp() throws Exception {
   @AfterEach
   public void tearDown() throws Exception {
     cleanupSparkContexts();
-    cleanupMetaClient();
+    cleanupClients();
     cleanupFileSystem();
     cleanupTestDataGenerator();
   }
@@ -129,7 +129,7 @@ public void testUpdateRecords() throws Exception {
     // Prepare the AvroParquetIO
     HoodieWriteConfig config = makeHoodieClientConfig();
     String firstCommitTime = HoodieTestUtils.makeNewCommitTime();
-    HoodieWriteClient writeClient = new HoodieWriteClient(jsc, config);
+    HoodieWriteClient writeClient = getHoodieWriteClient(config);
     writeClient.startCommitWithTime(firstCommitTime);
     metaClient = HoodieTableMetaClient.reload(metaClient);
 

File: hudi-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java
Patch:
@@ -62,7 +62,7 @@ public void setUp() throws Exception {
   @AfterEach
   public void tearDown() throws Exception {
     cleanupSparkContexts();
-    cleanupMetaClient();
+    cleanupClients();
     cleanupFileSystem();
     cleanupTestDataGenerator();
   }

File: hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestService.java
Patch:
@@ -154,6 +154,7 @@ private HiveConf configureHive(Configuration conf, String localHiveLocation) thr
     File derbyLogFile = new File(localHiveDir, "derby.log");
     derbyLogFile.createNewFile();
     setSystemProperty("derby.stream.error.file", derbyLogFile.getPath());
+    setSystemProperty("derby.system.home", localHiveDir.getAbsolutePath());
     conf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname,
         Files.createTempDirectory(System.currentTimeMillis() + "-").toFile().getAbsolutePath());
     conf.set("datanucleus.schema.autoCreateTables", "true");

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
Patch:
@@ -96,7 +96,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {
   private static final String DEFAULT_CLEANER_POLICY = HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name();
   private static final String DEFAULT_AUTO_CLEAN = "true";
   private static final String DEFAULT_INLINE_COMPACT = "false";
-  private static final String DEFAULT_INCREMENTAL_CLEANER = "false";
+  private static final String DEFAULT_INCREMENTAL_CLEANER = "true";
   private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = "1";
   private static final String DEFAULT_CLEANER_FILE_VERSIONS_RETAINED = "3";
   private static final String DEFAULT_CLEANER_COMMITS_RETAINED = "10";

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -82,7 +82,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {
   private static final String DEFAULT_FINALIZE_WRITE_PARALLELISM = DEFAULT_PARALLELISM;
 
   private static final String EMBEDDED_TIMELINE_SERVER_ENABLED = "hoodie.embed.timeline.server";
-  private static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_ENABLED = "false";
+  private static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_ENABLED = "true";
 
   private static final String FAIL_ON_TIMELINE_ARCHIVING_ENABLED_PROP = "hoodie.fail.on.timeline.archiving";
   private static final String DEFAULT_FAIL_ON_TIMELINE_ARCHIVING_ENABLED = "true";

File: hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestService.java
Patch:
@@ -105,6 +105,7 @@ public HiveServer2 start() throws IOException {
     executorService = Executors.newSingleThreadExecutor();
     tServer = startMetaStore(bindIP, metastorePort, serverConf);
 
+    serverConf.set("hive.in.test", "true");
     hiveServer = startHiveServer(serverConf);
 
     String serverHostname;

File: hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java
Patch:
@@ -75,13 +75,13 @@ public class TestTableSchemaEvolution extends TestHoodieClientBase {
       + TRIP_SCHEMA_SUFFIX;
 
   @BeforeEach
-  public void setUp() throws Exception {
+  public void setUp() throws IOException {
     initResources();
   }
 
   @AfterEach
-  public void tearDown() {
-    cleanupSparkContexts();
+  public void tearDown() throws IOException {
+    cleanupResources();
   }
 
   @Test

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -270,7 +270,7 @@ private static int compact(JavaSparkContext jsc, String basePath, String tableNa
     cfg.propsFilePath = propsFilePath;
     cfg.configs = configs;
     jsc.getConf().set("spark.executor.memory", sparkMemory);
-    return new HoodieCompactor(cfg).compact(jsc, retry);
+    return new HoodieCompactor(jsc, cfg).compact(retry);
   }
 
   private static int deduplicatePartitionPath(JavaSparkContext jsc, String duplicatedPartitionPath,

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java
Patch:
@@ -55,7 +55,7 @@ public static class Config implements Serializable {
     public String basePath = null;
     @Parameter(names = {"--table-name", "-tn"}, description = "Table name", required = true)
     public String tableName = null;
-    @Parameter(names = {"--instant-time", "-sp"}, description = "Compaction Instant time", required = true)
+    @Parameter(names = {"--instant-time", "-it"}, description = "Compaction Instant time", required = true)
     public String compactionInstantTime = null;
     @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for hoodie insert", required = true)
     public int parallelism = 1;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java
Patch:
@@ -193,7 +193,7 @@ public OffsetRange[] getNextOffsetRanges(Option<String> lastCheckpointStr, long
             fromOffsets = consumer.endOffsets(topicPartitions);
             break;
           default:
-            throw new HoodieNotSupportedException("Auto reset value must be one of 'smallest' or 'largest' ");
+            throw new HoodieNotSupportedException("Auto reset value must be one of 'earliest' or 'latest' ");
         }
       }
 

File: hudi-client/src/test/java/org/apache/hudi/metrics/TestHoodieJmxMetrics.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.metrics;
 
+import org.apache.hudi.common.testutils.NetworkTestUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 
 import org.junit.jupiter.api.Test;
@@ -39,7 +40,7 @@ public void testRegisterGauge() {
     when(config.isMetricsOn()).thenReturn(true);
     when(config.getMetricsReporterType()).thenReturn(MetricsReporterType.JMX);
     when(config.getJmxHost()).thenReturn("localhost");
-    when(config.getJmxPort()).thenReturn("9889");
+    when(config.getJmxPort()).thenReturn(String.valueOf(NetworkTestUtils.nextFreePort()));
     new HoodieMetrics(config, "raw_table");
     registerGauge("jmx_metric1", 123L);
     assertEquals("123", Metrics.getInstance().getRegistry().getGauges()
@@ -51,7 +52,7 @@ public void testRegisterGaugeByRangerPort() {
     when(config.isMetricsOn()).thenReturn(true);
     when(config.getMetricsReporterType()).thenReturn(MetricsReporterType.JMX);
     when(config.getJmxHost()).thenReturn("localhost");
-    when(config.getJmxPort()).thenReturn("1000-5000");
+    when(config.getJmxPort()).thenReturn(String.valueOf(NetworkTestUtils.nextFreePort()));
     new HoodieMetrics(config, "raw_table");
     registerGauge("jmx_metric2", 123L);
     assertEquals("123", Metrics.getInstance().getRegistry().getGauges()

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java
Patch:
@@ -86,7 +86,7 @@ JavaRDD<Tuple2<String, HoodieKey>> explodeRecordRDDWithFileComparisons(
       JavaPairRDD<String, String> partitionRecordKeyPairRDD) {
 
     IndexFileFilter indexFileFilter =
-        config.getBloomIndexPruneByRanges() ? new IntervalTreeBasedGlobalIndexFileFilter(partitionToFileIndexInfo)
+        config.useBloomIndexTreebasedFilter() ? new IntervalTreeBasedGlobalIndexFileFilter(partitionToFileIndexInfo)
             : new ListBasedGlobalIndexFileFilter(partitionToFileIndexInfo);
 
     return partitionRecordKeyPairRDD.map(partitionRecordKeyPair -> {

File: hudi-client/src/main/java/org/apache/hudi/execution/BulkInsertMapFunction.java
Patch:
@@ -50,7 +50,7 @@ public BulkInsertMapFunction(String instantTime, HoodieWriteConfig config, Hoodi
 
   @Override
   public Iterator<List<WriteStatus>> call(Integer partition, Iterator<HoodieRecord<T>> sortedRecordItr) {
-    return new CopyOnWriteLazyInsertIterable<>(sortedRecordItr, config, instantTime, hoodieTable,
+    return new LazyInsertIterable<>(sortedRecordItr, config, instantTime, hoodieTable,
         fileIDPrefixes.get(partition), hoodieTable.getSparkTaskContextSupplier());
   }
 }

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/CommitActionExecutor.java
Patch:
@@ -29,7 +29,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieUpsertException;
-import org.apache.hudi.execution.CopyOnWriteLazyInsertIterable;
+import org.apache.hudi.execution.LazyInsertIterable;
 import org.apache.hudi.execution.SparkBoundedInMemoryExecutor;
 import org.apache.hudi.io.HoodieMergeHandle;
 import org.apache.hudi.table.HoodieTable;
@@ -132,7 +132,7 @@ public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRec
       LOG.info("Empty partition");
       return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();
     }
-    return new CopyOnWriteLazyInsertIterable<>(recordItr, config, instantTime, (HoodieTable<T>)table, idPfx,
+    return new LazyInsertIterable<>(recordItr, config, instantTime, (HoodieTable<T>)table, idPfx,
         sparkTaskContextSupplier);
   }
 

File: hudi-client/src/test/java/org/apache/hudi/execution/TestBoundedInMemoryExecutor.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer;
 import org.apache.hudi.config.HoodieWriteConfig;
-import org.apache.hudi.execution.CopyOnWriteLazyInsertIterable.HoodieInsertValueGenResult;
+import org.apache.hudi.execution.LazyInsertIterable.HoodieInsertValueGenResult;
 
 import org.apache.avro.generic.IndexedRecord;
 import org.junit.After;
@@ -37,7 +37,7 @@
 
 import scala.Tuple2;
 
-import static org.apache.hudi.execution.CopyOnWriteLazyInsertIterable.getTransformFunction;
+import static org.apache.hudi.execution.LazyInsertIterable.getTransformFunction;
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.when;
 

File: hudi-client/src/test/java/org/apache/hudi/execution/TestBoundedInMemoryQueue.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.hudi.common.util.queue.FunctionBasedQueueProducer;
 import org.apache.hudi.common.util.queue.IteratorBasedQueueProducer;
 import org.apache.hudi.exception.HoodieException;
-import org.apache.hudi.execution.CopyOnWriteLazyInsertIterable.HoodieInsertValueGenResult;
+import org.apache.hudi.execution.LazyInsertIterable.HoodieInsertValueGenResult;
 
 import org.apache.avro.generic.IndexedRecord;
 import org.junit.After;
@@ -53,7 +53,7 @@
 
 import scala.Tuple2;
 
-import static org.apache.hudi.execution.CopyOnWriteLazyInsertIterable.getTransformFunction;
+import static org.apache.hudi.execution.LazyInsertIterable.getTransformFunction;
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.when;
 

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java
Patch:
@@ -42,7 +42,7 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
-import org.apache.hudi.table.compact.OperationResult;
+import org.apache.hudi.table.action.compact.OperationResult;
 import org.apache.hudi.utilities.UtilHelpers;
 
 import org.apache.hadoop.fs.FSDataInputStream;

File: hudi-cli/src/main/java/org/apache/hudi/cli/utils/CommitUtil.java
Patch:
@@ -39,7 +39,7 @@ public class CommitUtil {
 
   public static long countNewRecords(HoodieTableMetaClient target, List<String> commitsToCatchup) throws IOException {
     long totalNew = 0;
-    HoodieTimeline timeline = target.getActiveTimeline().reload().getCommitTimeline().filterCompletedInstants();
+    HoodieTimeline timeline = target.reloadActiveTimeline().getCommitTimeline().filterCompletedInstants();
     for (String commit : commitsToCatchup) {
       HoodieCommitMetadata c = HoodieCommitMetadata.fromBytes(
           timeline.getInstantDetails(new HoodieInstant(false, HoodieTimeline.COMMIT_ACTION, commit)).get(),

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestArchivedCommitsCommand.java
Patch:
@@ -30,7 +30,7 @@
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
-import org.apache.hudi.table.HoodieCommitArchiveLog;
+import org.apache.hudi.table.HoodieTimelineArchiveLog;
 
 import org.junit.After;
 import org.junit.Before;
@@ -91,7 +91,7 @@ public void init() throws IOException {
     metaClient.getActiveTimeline().reload().getAllCommitsTimeline().filterCompletedInstants();
 
     // archive
-    HoodieCommitArchiveLog archiveLog = new HoodieCommitArchiveLog(cfg, metaClient);
+    HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(cfg, metaClient);
     archiveLog.archiveIfRequired(jsc);
   }
 

File: hudi-cli/src/test/java/org/apache/hudi/cli/common/HoodieTestCommitUtilities.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.avro.model.HoodieWriteStat;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
-import org.apache.hudi.table.HoodieCommitArchiveLog;
+import org.apache.hudi.table.HoodieTimelineArchiveLog;
 
 import java.util.LinkedHashMap;
 import java.util.List;
@@ -36,7 +36,7 @@ public class HoodieTestCommitUtilities {
    */
   public static org.apache.hudi.avro.model.HoodieCommitMetadata convertAndOrderCommitMetadata(
       HoodieCommitMetadata hoodieCommitMetadata) {
-    return orderCommitMetadata(HoodieCommitArchiveLog.convertCommitMetadata(hoodieCommitMetadata));
+    return orderCommitMetadata(HoodieTimelineArchiveLog.convertCommitMetadata(hoodieCommitMetadata));
   }
 
   /**

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
Patch:
@@ -22,8 +22,8 @@
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.util.ValidationUtils;
-import org.apache.hudi.table.compact.strategy.CompactionStrategy;
-import org.apache.hudi.table.compact.strategy.LogFileSizeBasedCompactionStrategy;
+import org.apache.hudi.table.action.compact.strategy.CompactionStrategy;
+import org.apache.hudi.table.action.compact.strategy.LogFileSizeBasedCompactionStrategy;
 
 import javax.annotation.concurrent.Immutable;
 

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hudi.common.util.ReflectionUtils;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.metrics.MetricsReporterType;
-import org.apache.hudi.table.compact.strategy.CompactionStrategy;
+import org.apache.hudi.table.action.compact.strategy.CompactionStrategy;
 
 import org.apache.parquet.hadoop.metadata.CompressionCodecName;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/HoodieWriteMetadata.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.action.commit;
+package org.apache.hudi.table.action;
 
 import java.util.List;
 import org.apache.hudi.client.WriteStatus;

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java
Patch:
@@ -39,6 +39,7 @@
 import org.apache.hudi.table.WorkloadStat;
 import org.apache.hudi.table.action.BaseActionExecutor;
 
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 import org.apache.spark.Partitioner;
@@ -165,8 +166,6 @@ protected void updateIndexAndCommitIfNeeded(JavaRDD<WriteStatus> writeStatusRDD,
         (HoodieTable<T>)table);
     result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));
     result.setWriteStatuses(statuses);
-
-    // Trigger the insert and collect statuses
     commitOnAutoCommit(result);
   }
 
@@ -207,7 +206,6 @@ private void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetada
     try {
       activeTimeline.saveAsComplete(new HoodieInstant(true, actionType, instantTime),
           Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));
-
       LOG.info("Committed " + instantTime);
     } catch (IOException e) {
       throw new HoodieCommitException("Failed to complete commit " + config.getBasePath() + " at time " + instantTime,

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/BulkInsertCommitActionExecutor.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.UserDefinedBulkInsertPartitioner;
 
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/BulkInsertHelper.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.UserDefinedBulkInsertPartitioner;
 
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 
 import java.util.List;

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/BulkInsertPreppedCommitActionExecutor.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.UserDefinedBulkInsertPartitioner;
 
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/DeleteCommitActionExecutor.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/DeleteHelper.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.hudi.table.HoodieTable;
 import org.apache.hudi.table.WorkloadProfile;
 
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 
@@ -58,8 +59,8 @@ private static  <T extends HoodieRecordPayload<T>> JavaRDD<HoodieKey> deduplicat
   }
 
   public static <T extends HoodieRecordPayload<T>> HoodieWriteMetadata execute(String instantTime,
-      JavaRDD<HoodieKey> keys, JavaSparkContext jsc, HoodieWriteConfig config, HoodieTable<T> table,
-      CommitActionExecutor<T> deleteExecutor) {
+                                                                               JavaRDD<HoodieKey> keys, JavaSparkContext jsc, HoodieWriteConfig config, HoodieTable<T> table,
+                                                                               CommitActionExecutor<T> deleteExecutor) {
     try {
       HoodieWriteMetadata result = null;
       // De-dupe/merge if needed

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertCommitActionExecutor.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertPreppedCommitActionExecutor.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/UpsertCommitActionExecutor.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPreppedCommitActionExecutor.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.compact;
+package org.apache.hudi.table.action.compact;
 
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.client.WriteStatus;

File: hudi-client/src/main/java/org/apache/hudi/table/action/compact/HoodieMergeOnReadTableCompactor.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.compact;
+package org.apache.hudi.table.action.compact;
 
 import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
@@ -42,7 +42,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieCopyOnWriteTable;
 import org.apache.hudi.table.HoodieTable;
-import org.apache.hudi.table.compact.strategy.CompactionStrategy;
+import org.apache.hudi.table.action.compact.strategy.CompactionStrategy;
 
 import org.apache.avro.Schema;
 import org.apache.hadoop.fs.FileSystem;
@@ -71,7 +71,6 @@
  * passes it through a CompactionFilter and executes all the compactions and writes a new version of base files and make
  * a normal commit
  *
- * @see HoodieCompactor
  */
 public class HoodieMergeOnReadTableCompactor implements HoodieCompactor {
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/compact/OperationResult.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.compact;
+package org.apache.hudi.table.action.compact;
 
 import org.apache.hudi.common.util.Option;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/compact/strategy/BoundedIOCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.compact.strategy;
+package org.apache.hudi.table.action.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/main/java/org/apache/hudi/table/action/compact/strategy/BoundedPartitionAwareCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.compact.strategy;
+package org.apache.hudi.table.action.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/main/java/org/apache/hudi/table/action/compact/strategy/CompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.compact.strategy;
+package org.apache.hudi.table.action.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
@@ -26,7 +26,7 @@
 import org.apache.hudi.common.util.CompactionUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
-import org.apache.hudi.table.compact.HoodieMergeOnReadTableCompactor;
+import org.apache.hudi.table.action.compact.HoodieMergeOnReadTableCompactor;
 
 import java.io.Serializable;
 import java.util.HashMap;

File: hudi-client/src/main/java/org/apache/hudi/table/action/compact/strategy/DayBasedCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.compact.strategy;
+package org.apache.hudi.table.action.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/main/java/org/apache/hudi/table/action/compact/strategy/LogFileSizeBasedCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.compact.strategy;
+package org.apache.hudi.table.action.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/main/java/org/apache/hudi/table/action/compact/strategy/UnBoundedCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.compact.strategy;
+package org.apache.hudi.table.action.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/main/java/org/apache/hudi/table/action/compact/strategy/UnBoundedPartitionAwareCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.compact.strategy;
+package org.apache.hudi.table.action.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/main/java/org/apache/hudi/table/action/deltacommit/BulkInsertDeltaCommitActionExecutor.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hudi.table.UserDefinedBulkInsertPartitioner;
 
 import org.apache.hudi.table.action.commit.BulkInsertHelper;
-import org.apache.hudi.table.action.commit.HoodieWriteMetadata;
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/deltacommit/BulkInsertPreppedDeltaCommitActionExecutor.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hudi.table.UserDefinedBulkInsertPartitioner;
 
 import org.apache.hudi.table.action.commit.BulkInsertHelper;
-import org.apache.hudi.table.action.commit.HoodieWriteMetadata;
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/deltacommit/DeleteDeltaCommitActionExecutor.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hudi.table.HoodieTable;
 
 import org.apache.hudi.table.action.commit.DeleteHelper;
-import org.apache.hudi.table.action.commit.HoodieWriteMetadata;
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/deltacommit/InsertDeltaCommitActionExecutor.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
-import org.apache.hudi.table.action.commit.HoodieWriteMetadata;
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.hudi.table.action.commit.WriteHelper;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;

File: hudi-client/src/main/java/org/apache/hudi/table/action/deltacommit/InsertPreppedDeltaCommitActionExecutor.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
-import org.apache.hudi.table.action.commit.HoodieWriteMetadata;
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/deltacommit/UpsertDeltaCommitActionExecutor.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
-import org.apache.hudi.table.action.commit.HoodieWriteMetadata;
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.hudi.table.action.commit.WriteHelper;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;

File: hudi-client/src/main/java/org/apache/hudi/table/action/deltacommit/UpsertPreppedDeltaCommitActionExecutor.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
-import org.apache.hudi.table.action.commit.HoodieWriteMetadata;
+import org.apache.hudi.table.action.HoodieWriteMetadata;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 

File: hudi-client/src/main/java/org/apache/hudi/table/action/rollback/MergeOnReadRollbackActionExecutor.java
Patch:
@@ -118,7 +118,7 @@ private List<RollbackRequest> generateRollbackRequests(JavaSparkContext jsc, Hoo
         config.shouldAssumeDatePartitioning());
     int sparkPartitions = Math.max(Math.min(partitions.size(), config.getRollbackParallelism()), 1);
     return jsc.parallelize(partitions, Math.min(partitions.size(), sparkPartitions)).flatMap(partitionPath -> {
-      HoodieActiveTimeline activeTimeline = table.getActiveTimeline().reload();
+      HoodieActiveTimeline activeTimeline = table.getMetaClient().reloadActiveTimeline();
       List<RollbackRequest> partitionRollbackRequests = new ArrayList<>();
       switch (instantToRollback.getAction()) {
         case HoodieTimeline.COMMIT_ACTION:

File: hudi-client/src/test/java/org/apache/hudi/client/TestClientRollback.java
Patch:
@@ -58,7 +58,7 @@ public class TestClientRollback extends TestHoodieClientBase {
   public void testSavepointAndRollback() throws Exception {
     HoodieWriteConfig cfg = getConfigBuilder().withCompactionConfig(HoodieCompactionConfig.newBuilder()
         .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(1).build()).build();
-    try (HoodieWriteClient client = getHoodieWriteClient(cfg);) {
+    try (HoodieWriteClient client = getHoodieWriteClient(cfg)) {
       HoodieTestDataGenerator.writePartitionMetadata(fs, HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS, basePath);
 
       /**

File: hudi-client/src/test/java/org/apache/hudi/client/TestCompactionAdminClient.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
-import org.apache.hudi.table.compact.OperationResult;
+import org.apache.hudi.table.action.compact.OperationResult;
 
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;

File: hudi-client/src/test/java/org/apache/hudi/table/action/compact/TestAsyncCompaction.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.compact;
+package org.apache.hudi.table.action.compact;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/test/java/org/apache/hudi/table/action/compact/strategy/TestHoodieCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.table.compact.strategy;
+package org.apache.hudi.table.action.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.common.model.HoodieBaseFile;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -185,7 +185,7 @@ public static class Config implements Serializable {
         "file://" + System.getProperty("user.dir") + "/src/test/resources/delta-streamer-config/dfs-source.properties";
 
     @Parameter(names = {"--hoodie-conf"}, description = "Any configuration that can be set in the properties file "
-        + "(using the CLI parameter \"--propsFilePath\") can also be passed command line using this parameter")
+        + "(using the CLI parameter \"--props\") can also be passed command line using this parameter")
     public List<String> configs = new ArrayList<>();
 
     @Parameter(names = {"--source-class"},

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java
Patch:
@@ -226,7 +226,7 @@ public static class Config implements Serializable {
         "file://" + System.getProperty("user.dir") + "/src/test/resources/delta-streamer-config/dfs-source.properties";
 
     @Parameter(names = {"--hoodie-conf"}, description = "Any configuration that can be set in the properties file "
-        + "(using the CLI parameter \"--propsFilePath\") can also be passed command line using this parameter")
+        + "(using the CLI parameter \"--props\") can also be passed command line using this parameter")
     public List<String> configs = new ArrayList<>();
 
     @Parameter(names = {"--source-class"},

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java
Patch:
@@ -256,8 +256,7 @@ public static class Config implements Serializable {
         + ". Allows transforming raw source Dataset to a target Dataset (conforming to target schema) before "
         + "writing. Default : Not set. E:g - org.apache.hudi.utilities.transform.SqlQueryBasedTransformer (which "
         + "allows a SQL query templated to be passed as a transformation function). "
-        + "Pass a comma-separated list of subclass names to chain the transformations.",
-        converter = HoodieDeltaStreamer.TransformersConverter.class)
+        + "Pass a comma-separated list of subclass names to chain the transformations.")
     public List<String> transformerClassNames = null;
 
     @Parameter(names = {"--source-limit"}, description = "Maximum amount of data to read from source. "

File: hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertBucket.java
Patch:
@@ -32,10 +32,10 @@ public class InsertBucket implements Serializable {
 
   @Override
   public String toString() {
-    final StringBuilder sb = new StringBuilder("WorkloadStat {");
+    final StringBuilder sb = new StringBuilder("InsertBucket {");
     sb.append("bucketNumber=").append(bucketNumber).append(", ");
     sb.append("weight=").append(weight);
     sb.append('}');
     return sb.toString();
   }
-}
\ No newline at end of file
+}

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -191,8 +191,6 @@ private Option<IndexedRecord> getIndexedRecord(HoodieRecord<T> hoodieRecord) {
     return Option.empty();
   }
 
-  // TODO (NA) - Perform a writerSchema check of current input record with the last writerSchema on log file
-  // to make sure we don't append records with older (shorter) writerSchema than already appended
   public void doAppend() {
     while (recordItr.hasNext()) {
       HoodieRecord record = recordItr.next();

File: hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -199,15 +199,15 @@ private void testDeduplication(
     String recordKey = UUID.randomUUID().toString();
     HoodieKey keyOne = new HoodieKey(recordKey, "2018-01-01");
     HoodieRecord<TestRawTripPayload> recordOne =
-        new HoodieRecord(keyOne, HoodieTestDataGenerator.generateRandomValue(keyOne, newCommitTime));
+        new HoodieRecord(keyOne, dataGen.generateRandomValue(keyOne, newCommitTime));
 
     HoodieKey keyTwo = new HoodieKey(recordKey, "2018-02-01");
     HoodieRecord recordTwo =
-        new HoodieRecord(keyTwo, HoodieTestDataGenerator.generateRandomValue(keyTwo, newCommitTime));
+        new HoodieRecord(keyTwo, dataGen.generateRandomValue(keyTwo, newCommitTime));
 
     // Same key and partition as keyTwo
     HoodieRecord recordThree =
-        new HoodieRecord(keyTwo, HoodieTestDataGenerator.generateRandomValue(keyTwo, newCommitTime));
+        new HoodieRecord(keyTwo, dataGen.generateRandomValue(keyTwo, newCommitTime));
 
     JavaRDD<HoodieRecord<TestRawTripPayload>> records =
         jsc.parallelize(Arrays.asList(recordOne, recordTwo, recordThree), 1);

File: hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;
 import org.apache.hudi.hive.HoodieHiveClient.PartitionEvent;
 import org.apache.hudi.hive.HoodieHiveClient.PartitionEvent.PartitionEventType;
-import org.apache.hudi.hive.util.SchemaUtil;
+import org.apache.hudi.hive.util.HiveSchemaUtil;
 
 import com.beust.jcommander.JCommander;
 import org.apache.hadoop.conf.Configuration;
@@ -158,7 +158,7 @@ private void syncSchema(String tableName, boolean tableExists, boolean useRealTi
     } else {
       // Check if the table schema has evolved
       Map<String, String> tableSchema = hoodieHiveClient.getTableSchema(tableName);
-      SchemaDifference schemaDiff = SchemaUtil.getSchemaDifference(schema, tableSchema, cfg.partitionFields);
+      SchemaDifference schemaDiff = HiveSchemaUtil.getSchemaDifference(schema, tableSchema, cfg.partitionFields);
       if (!schemaDiff.isEmpty()) {
         LOG.info("Schema difference found for " + tableName);
         hoodieHiveClient.updateTableDefinition(tableName, schema);

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java
Patch:
@@ -212,7 +212,7 @@ public static Schema generateProjectionSchema(Schema writeSchema, Map<String, Fi
         throw new HoodieException("Field " + fn + " not found in log schema. Query cannot proceed! "
             + "Derived Schema Fields: " + new ArrayList<>(schemaFieldsMap.keySet()));
       } else {
-        projectedFields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultValue()));
+        projectedFields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal()));
       }
     }
 
@@ -367,7 +367,7 @@ private Schema constructHiveOrderedSchema(Schema writerSchema, Map<String, Field
       Field field = schemaFieldsMap.get(columnName.toLowerCase());
 
       if (field != null) {
-        hiveSchemaFields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultValue()));
+        hiveSchemaFields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal()));
       } else {
         // Hive has some extra virtual columns like BLOCK__OFFSET__INSIDE__FILE which do not exist in table schema.
         // They will get skipped as they won't be found in the original schema.

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.common.config.TypedProperties;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.utilities.UtilHelpers;
 import org.apache.hudi.utilities.schema.SchemaRegistryProvider;
@@ -66,6 +67,8 @@ public HoodieMultiTableDeltaStreamer(Config config, JavaSparkContext jssc) throw
     this.jssc = jssc;
     String commonPropsFile = config.propsFilePath;
     String configFolder = config.configFolder;
+    ValidationUtils.checkArgument(!config.filterDupes || config.operation != HoodieDeltaStreamer.Operation.UPSERT,
+        "'--filter-dupes' needs to be disabled when '--op' is 'UPSERT' to ensure updates are not missed.");
     FileSystem fs = FSUtils.getFs(commonPropsFile, jssc.hadoopConfiguration());
     configFolder = configFolder.charAt(configFolder.length() - 1) == '/' ? configFolder.substring(0, configFolder.length() - 1) : configFolder;
     checkIfPropsFileAndConfigFolderExist(commonPropsFile, configFolder, fs);

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/TimelineService.java
Patch:
@@ -36,7 +36,7 @@
 import java.io.Serializable;
 
 /**
- * A stand alone timeline service exposing File-System View interfaces to clients.
+ * A standalone timeline service exposing File-System View interfaces to clients.
  */
 public class TimelineService {
 

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CleansCommand.java
Patch:
@@ -139,7 +139,7 @@ public String runClean(@CliOption(key = "sparkMemory", unspecifiedDefaultValue =
     SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
 
     String cmd = SparkMain.SparkCommand.CLEAN.toString();
-    sparkLauncher.addAppArgs(cmd, metaClient.getBasePath(), master, propsFilePath, sparkMemory);
+    sparkLauncher.addAppArgs(cmd, master, sparkMemory, metaClient.getBasePath(), propsFilePath);
     UtilHelpers.validateAndAddProperties(configs, sparkLauncher);
     Process process = sparkLauncher.launch();
     InputStreamConsumer.captureOutput(process);

File: hudi-client/src/main/java/org/apache/hudi/client/HoodieReadClient.java
Patch:
@@ -96,7 +96,7 @@ public HoodieReadClient(JavaSparkContext jsc, HoodieWriteConfig clientConfig) {
     final String basePath = clientConfig.getBasePath();
     // Create a Hoodie table which encapsulated the commits and files visible
     HoodieTableMetaClient metaClient = new HoodieTableMetaClient(jsc.hadoopConfiguration(), basePath, true);
-    this.hoodieTable = HoodieTable.getHoodieTable(metaClient, clientConfig, jsc);
+    this.hoodieTable = HoodieTable.create(metaClient, clientConfig, jsc);
     this.index = HoodieIndex.createIndex(clientConfig, jsc);
     this.sqlContextOpt = Option.empty();
   }

File: hudi-client/src/main/java/org/apache/hudi/table/HoodieCommitArchiveLog.java
Patch:
@@ -138,7 +138,7 @@ private Stream<HoodieInstant> getInstantsToArchive(JavaSparkContext jsc) {
     int maxCommitsToKeep = config.getMaxCommitsToKeep();
     int minCommitsToKeep = config.getMinCommitsToKeep();
 
-    HoodieTable table = HoodieTable.getHoodieTable(metaClient, config, jsc);
+    HoodieTable table = HoodieTable.create(metaClient, config, jsc);
 
     // GroupBy each action and limit each action timeline to maxCommitsToKeep
     // TODO: Handle ROLLBACK_ACTION in future

File: hudi-client/src/main/java/org/apache/hudi/table/HoodieMergeOnReadTable.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
+import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieTimeline;
@@ -84,8 +85,8 @@ public class HoodieMergeOnReadTable<T extends HoodieRecordPayload> extends Hoodi
   // UpsertPartitioner for MergeOnRead table type
   private MergeOnReadUpsertPartitioner mergeOnReadUpsertPartitioner;
 
-  public HoodieMergeOnReadTable(HoodieWriteConfig config, JavaSparkContext jsc) {
-    super(config, jsc);
+  HoodieMergeOnReadTable(HoodieWriteConfig config, JavaSparkContext jsc, HoodieTableMetaClient metaClient) {
+    super(config, jsc, metaClient);
   }
 
   @Override

File: hudi-client/src/main/java/org/apache/hudi/table/compact/HoodieMergeOnReadTableCompactor.java
Patch:
@@ -90,7 +90,7 @@ public JavaRDD<WriteStatus> compact(JavaSparkContext jsc, HoodieCompactionPlan c
     }
     HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();
     // Compacting is very similar to applying updates to existing file
-    HoodieCopyOnWriteTable table = new HoodieCopyOnWriteTable(config, jsc);
+    HoodieCopyOnWriteTable table = new HoodieCopyOnWriteTable(config, jsc, metaClient);
     List<CompactionOperation> operations = compactionPlan.getOperations().stream()
         .map(CompactionOperation::convertFromAvroRecordInstance).collect(toList());
     LOG.info("Compactor compacting " + operations + " files");

File: hudi-client/src/test/java/org/apache/hudi/client/TestClientRollback.java
Patch:
@@ -99,7 +99,7 @@ public void testSavepointAndRollback() throws Exception {
       List<String> partitionPaths =
           FSUtils.getAllPartitionPaths(fs, cfg.getBasePath(), getConfig().shouldAssumeDatePartitioning());
       metaClient = HoodieTableMetaClient.reload(metaClient);
-      HoodieTable table = HoodieTable.getHoodieTable(metaClient, getConfig(), jsc);
+      HoodieTable table = HoodieTable.create(metaClient, getConfig(), jsc);
       final BaseFileOnlyView view1 = table.getBaseFileOnlyView();
 
       List<HoodieBaseFile> dataFiles = partitionPaths.stream().flatMap(s -> {
@@ -124,7 +124,7 @@ public void testSavepointAndRollback() throws Exception {
       assertNoWriteErrors(statuses);
 
       metaClient = HoodieTableMetaClient.reload(metaClient);
-      table = HoodieTable.getHoodieTable(metaClient, getConfig(), jsc);
+      table = HoodieTable.create(metaClient, getConfig(), jsc);
       final BaseFileOnlyView view2 = table.getBaseFileOnlyView();
 
       dataFiles = partitionPaths.stream().flatMap(s -> view2.getAllBaseFiles(s).filter(f -> f.getCommitTime().equals("004"))).collect(Collectors.toList());
@@ -143,7 +143,7 @@ public void testSavepointAndRollback() throws Exception {
       client.rollbackToSavepoint(savepoint.getTimestamp());
 
       metaClient = HoodieTableMetaClient.reload(metaClient);
-      table = HoodieTable.getHoodieTable(metaClient, getConfig(), jsc);
+      table = HoodieTable.create(metaClient, getConfig(), jsc);
       final BaseFileOnlyView view3 = table.getBaseFileOnlyView();
       dataFiles = partitionPaths.stream().flatMap(s -> {
         return view3.getAllBaseFiles(s).filter(f -> f.getCommitTime().equals("002"));

File: hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -800,7 +800,7 @@ public void testCommitWritesRelativePaths() throws Exception {
     HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).build();
     try (HoodieWriteClient client = getHoodieWriteClient(cfg);) {
       HoodieTableMetaClient metaClient = new HoodieTableMetaClient(jsc.hadoopConfiguration(), basePath);
-      HoodieTable table = HoodieTable.getHoodieTable(metaClient, cfg, jsc);
+      HoodieTable table = HoodieTable.create(metaClient, cfg, jsc);
 
       String instantTime = "000";
       client.startCommitWithTime(instantTime);

File: hudi-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java
Patch:
@@ -31,11 +31,11 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.HoodieCreateHandle;
 import org.apache.hudi.io.HoodieMergeHandle;
-import org.apache.hudi.table.HoodieCopyOnWriteTable;
 
 import org.apache.avro.generic.GenericRecord;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
+import org.apache.hudi.table.HoodieTable;
 import org.apache.parquet.avro.AvroReadSupport;
 import org.junit.After;
 import org.junit.Assert;
@@ -67,7 +67,7 @@ public void tearDown() {
   public void testSchemaEvolutionOnUpdate() throws Exception {
     // Create a bunch of records with a old version of schema
     final HoodieWriteConfig config = makeHoodieClientConfig("/exampleSchema.txt");
-    final HoodieCopyOnWriteTable table = new HoodieCopyOnWriteTable(config, jsc);
+    final HoodieTable<?> table = HoodieTable.create(config, jsc);
 
     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {
       String recordStr1 = "{\"_row_key\":\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\","
@@ -102,7 +102,7 @@ public void testSchemaEvolutionOnUpdate() throws Exception {
     final WriteStatus insertResult = statuses.get(0);
     String fileId = insertResult.getFileId();
 
-    final HoodieCopyOnWriteTable table2 = new HoodieCopyOnWriteTable(config2, jsc);
+    final HoodieTable table2 = HoodieTable.create(config2, jsc);
     Assert.assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {
       // New content with values for the newly added field
       String recordStr1 = "{\"_row_key\":\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\","

File: hudi-client/src/test/java/org/apache/hudi/io/storage/TestHoodieStorageWriterFactory.java
Patch:
@@ -44,7 +44,7 @@ public void testGetStorageWriter() throws IOException {
     final String instantTime = "100";
     final Path parquetPath = new Path(basePath + "/partition/path/f1_1-0-1_000.parquet");
     final HoodieWriteConfig cfg = getConfig();
-    HoodieTable table = HoodieTable.getHoodieTable(metaClient, cfg, jsc);
+    HoodieTable table = HoodieTable.create(metaClient, cfg, jsc);
     SparkTaskContextSupplier supplier = new SparkTaskContextSupplier();
     HoodieStorageWriter<IndexedRecord> parquetWriter = HoodieStorageWriterFactory.getStorageWriter(instantTime,
         parquetPath, table, cfg, HoodieTestDataGenerator.AVRO_SCHEMA, supplier);

File: hudi-client/src/test/java/org/apache/hudi/table/compact/TestAsyncCompaction.java
Patch:
@@ -119,7 +119,7 @@ public void testRollbackForInflightCompaction() throws Exception {
 
       // Reload and rollback inflight compaction
       metaClient = new HoodieTableMetaClient(jsc.hadoopConfiguration(), cfg.getBasePath());
-      HoodieTable hoodieTable = HoodieTable.getHoodieTable(metaClient, cfg, jsc);
+      HoodieTable hoodieTable = HoodieTable.create(metaClient, cfg, jsc);
       // hoodieTable.rollback(jsc,
       //    new HoodieInstant(true, HoodieTimeline.COMPACTION_ACTION, compactionInstantTime), false);
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java
Patch:
@@ -39,8 +39,9 @@ public class CleanerUtils {
   public static final Integer CLEAN_METADATA_VERSION_2 = CleanV2MigrationHandler.VERSION;
   public static final Integer LATEST_CLEAN_METADATA_VERSION = CLEAN_METADATA_VERSION_2;
 
-  public static HoodieCleanMetadata convertCleanMetadata(HoodieTableMetaClient metaClient,
-      String startCleanTime, Option<Long> durationInMs, List<HoodieCleanStat> cleanStats) {
+  public static HoodieCleanMetadata convertCleanMetadata(String startCleanTime,
+                                                         Option<Long> durationInMs,
+                                                         List<HoodieCleanStat> cleanStats) {
     Map<String, HoodieCleanPartitionMetadata> partitionMetadataMap = new HashMap<>();
     int totalDeleted = 0;
     String earliestCommitToRetain = null;

File: hudi-common/src/test/java/org/apache/hudi/common/model/HoodieTestUtils.java
Patch:
@@ -328,7 +328,7 @@ public static void createCleanFiles(HoodieTableMetaClient metaClient, String bas
       // Create the clean metadata
 
       HoodieCleanMetadata cleanMetadata =
-          CleanerUtils.convertCleanMetadata(metaClient, instantTime, Option.of(0L), Collections.singletonList(cleanStats));
+          CleanerUtils.convertCleanMetadata(instantTime, Option.of(0L), Collections.singletonList(cleanStats));
       // Write empty clean metadata
       os.write(TimelineMetadataUtils.serializeCleanMetadata(cleanMetadata).get());
     }

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java
Patch:
@@ -414,8 +414,7 @@ private void performClean(String instant, List<String> files, String cleanInstan
 
     HoodieInstant cleanInflightInstant = new HoodieInstant(true, HoodieTimeline.CLEAN_ACTION, cleanInstant);
     metaClient.getActiveTimeline().createNewInstant(cleanInflightInstant);
-    HoodieCleanMetadata cleanMetadata = CleanerUtils
-        .convertCleanMetadata(metaClient, cleanInstant, Option.empty(), cleanStats);
+    HoodieCleanMetadata cleanMetadata = CleanerUtils.convertCleanMetadata(cleanInstant, Option.empty(), cleanStats);
     metaClient.getActiveTimeline().saveAsComplete(cleanInflightInstant,
         TimelineMetadataUtils.serializeCleanMetadata(cleanMetadata));
   }

File: hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java
Patch:
@@ -198,7 +198,8 @@ private List<String> constructChangePartitions(String tableName, List<String> pa
     for (String partition : partitions) {
       String partitionClause = getPartitionClause(partition);
       Path partitionPath = FSUtils.getPartitionPath(syncConfig.basePath, partition);
-      String fullPartitionPath = partitionPath.toUri().getScheme().equals(StorageSchemes.HDFS.getScheme())
+      String partitionScheme = partitionPath.toUri().getScheme();
+      String fullPartitionPath = StorageSchemes.HDFS.getScheme().equals(partitionScheme)
               ? FSUtils.getDFSFullPartitionPath(fs, partitionPath) : partitionPath.toString();
       String changePartition =
           alterTable + " PARTITION (" + partitionClause + ") SET LOCATION '" + fullPartitionPath + "'";
@@ -505,6 +506,7 @@ private List<CommandProcessorResponse> updateHiveSQLs(List<String> sqls) {
     try {
       final long startTime = System.currentTimeMillis();
       ss = SessionState.start(configuration);
+      ss.setCurrentDatabase(syncConfig.databaseName);
       hiveDriver = new org.apache.hadoop.hive.ql.Driver(configuration);
       final long endTime = System.currentTimeMillis();
       LOG.info(String.format("Time taken to start SessionState and create Driver: %s ms", (endTime - startTime)));

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RollbacksCommand.java
Patch:
@@ -120,7 +120,7 @@ public String showRollback(
   /**
    * An Active timeline containing only rollbacks.
    */
-  class RollbackTimeline extends HoodieActiveTimeline {
+  static class RollbackTimeline extends HoodieActiveTimeline {
 
     public RollbackTimeline(HoodieTableMetaClient metaClient) {
       super(metaClient, CollectionUtils.createImmutableSet(HoodieTimeline.ROLLBACK_EXTENSION));

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkEnvCommand.java
Patch:
@@ -34,7 +34,7 @@
 @Component
 public class SparkEnvCommand implements CommandMarker {
 
-  public static Map<String, String> env = new HashMap<String, String>();
+  public static Map<String, String> env = new HashMap<>();
 
   @CliCommand(value = "set", help = "Set spark launcher env to cli")
   public void setEnv(@CliOption(key = {"conf"}, help = "Env config to be set") final String confMap) {
@@ -49,8 +49,8 @@ public void setEnv(@CliOption(key = {"conf"}, help = "Env config to be set") fin
   public String showAllEnv() {
     String[][] rows = new String[env.size()][2];
     int i = 0;
-    for (String key: env.keySet()) {
-      rows[i] = new String[]{key, env.get(key)};
+    for (Map.Entry<String, String> entry: env.entrySet()) {
+      rows[i] = new String[]{entry.getKey(), entry.getValue()};
       i++;
     }
     return HoodiePrintHelper.print(new String[] {"key", "value"}, rows);

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/StatsCommand.java
Patch:
@@ -131,9 +131,9 @@ public String fileSizeStats(
     }
 
     List<Comparable[]> rows = new ArrayList<>();
-    for (String instantTime : commitHistoMap.keySet()) {
-      Snapshot s = commitHistoMap.get(instantTime).getSnapshot();
-      rows.add(printFileSizeHistogram(instantTime, s));
+    for (Map.Entry<String, Histogram> entry : commitHistoMap.entrySet()) {
+      Snapshot s = entry.getValue().getSnapshot();
+      rows.add(printFileSizeHistogram(entry.getKey(), s));
     }
     Snapshot s = globalHistogram.getSnapshot();
     rows.add(printFileSizeHistogram("ALL", s));

File: hudi-client/src/main/java/org/apache/hudi/client/HoodieCleanClient.java
Patch:
@@ -47,6 +47,7 @@
 
 public class HoodieCleanClient<T extends HoodieRecordPayload> extends AbstractHoodieClient {
 
+  private static final long serialVersionUID = 1L;
   private static final Logger LOG = LogManager.getLogger(HoodieCleanClient.class);
   private final transient HoodieMetrics metrics;
 

File: hudi-client/src/main/java/org/apache/hudi/client/HoodieReadClient.java
Patch:
@@ -57,6 +57,7 @@
  */
 public class HoodieReadClient<T extends HoodieRecordPayload> implements Serializable {
 
+  private static final long serialVersionUID = 1L;
   private static final Logger LOG = LogManager.getLogger(HoodieReadClient.class);
 
   /**

File: hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java
Patch:
@@ -93,6 +93,7 @@
  */
 public class HoodieWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T> {
 
+  private static final long serialVersionUID = 1L;
   private static final Logger LOG = LogManager.getLogger(HoodieWriteClient.class);
   private static final String LOOKUP_STR = "lookup";
   private final boolean rollbackPending;

File: hudi-client/src/main/java/org/apache/hudi/client/WriteStatus.java
Patch:
@@ -35,6 +35,7 @@
  */
 public class WriteStatus implements Serializable {
 
+  private static final long serialVersionUID = 1L;
   private static final long RANDOM_SEED = 9038412832L;
 
   private final HashMap<HoodieKey, Throwable> errors = new HashMap<>();

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieHBaseIndexConfig.java
Patch:
@@ -61,7 +61,7 @@ public class HoodieHBaseIndexConfig extends DefaultHoodieConfig {
    * value based on global indexing throughput needs and most importantly, how much the HBase installation in use is
    * able to tolerate without Region Servers going down.
    */
-  public static String HBASE_MAX_QPS_PER_REGION_SERVER_PROP = "hoodie.index.hbase.max.qps.per.region.server";
+  public static final String HBASE_MAX_QPS_PER_REGION_SERVER_PROP = "hoodie.index.hbase.max.qps.per.region.server";
   /**
    * Default batch size, used only for Get, but computed for Put.
    */

File: hudi-client/src/main/java/org/apache/hudi/table/compact/strategy/BoundedPartitionAwareCompactionStrategy.java
Patch:
@@ -39,7 +39,7 @@
  */
 public class BoundedPartitionAwareCompactionStrategy extends DayBasedCompactionStrategy {
 
-  SimpleDateFormat dateFormat = new SimpleDateFormat(datePartitionFormat);
+  SimpleDateFormat dateFormat = new SimpleDateFormat(DATE_PARTITION_FORMAT);
 
   @Override
   public List<HoodieCompactionOperation> orderAndFilter(HoodieWriteConfig writeConfig,

File: hudi-client/src/main/java/org/apache/hudi/table/compact/strategy/DayBasedCompactionStrategy.java
Patch:
@@ -39,14 +39,14 @@
 public class DayBasedCompactionStrategy extends CompactionStrategy {
 
   // For now, use SimpleDateFormat as default partition format
-  protected static String datePartitionFormat = "yyyy/MM/dd";
+  protected static final String DATE_PARTITION_FORMAT = "yyyy/MM/dd";
   // Sorts compaction in LastInFirstCompacted order
   protected static Comparator<String> comparator = (String leftPartition, String rightPartition) -> {
     try {
       leftPartition = getPartitionPathWithoutPartitionKeys(leftPartition);
       rightPartition = getPartitionPathWithoutPartitionKeys(rightPartition);
-      Date left = new SimpleDateFormat(datePartitionFormat, Locale.ENGLISH).parse(leftPartition);
-      Date right = new SimpleDateFormat(datePartitionFormat, Locale.ENGLISH).parse(rightPartition);
+      Date left = new SimpleDateFormat(DATE_PARTITION_FORMAT, Locale.ENGLISH).parse(leftPartition);
+      Date right = new SimpleDateFormat(DATE_PARTITION_FORMAT, Locale.ENGLISH).parse(rightPartition);
       return left.after(right) ? -1 : right.after(left) ? 1 : 0;
     } catch (ParseException e) {
       throw new HoodieException("Invalid Partition Date Format", e);

File: hudi-common/src/main/java/org/apache/hudi/common/config/SerializableConfiguration.java
Patch:
@@ -30,6 +30,7 @@
  */
 public class SerializableConfiguration implements Serializable {
 
+  private static final long serialVersionUID = 1L;
   private transient Configuration configuration;
 
   public SerializableConfiguration(Configuration configuration) {

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieBaseFile.java
Patch:
@@ -31,6 +31,7 @@
  */
 public class HoodieBaseFile implements Serializable {
 
+  private static final long serialVersionUID = 1L;
   private transient FileStatus fileStatus;
   private final String fullPath;
   private long fileLen;

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieLogFile.java
Patch:
@@ -37,6 +37,7 @@
  */
 public class HoodieLogFile implements Serializable {
 
+  private static final long serialVersionUID = 1L;
   public static final String DELTA_EXTENSION = ".log";
   public static final Integer LOGFILE_BASE_VERSION = 1;
 
@@ -129,6 +130,7 @@ public static Comparator<HoodieLogFile> getReverseLogFileComparator() {
    */
   public static class LogFileComparator implements Comparator<HoodieLogFile>, Serializable {
 
+    private static final long serialVersionUID = 1L;
     private transient Comparator<String> writeTokenComparator;
 
     private Comparator<String> getWriteTokenComparator() {

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRollingStatMetadata.java
Patch:
@@ -52,7 +52,7 @@ public HoodieRollingStatMetadata(String actionType) {
     this.actionType = actionType;
   }
 
-  class RollingStatsHashMap<K, V> extends HashMap<K, V> {
+  static class RollingStatsHashMap<K, V> extends HashMap<K, V> {
 
     @Override
     public V put(K key, V value) {

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java
Patch:
@@ -48,6 +48,7 @@
  */
 public class HoodieDefaultTimeline implements HoodieTimeline {
 
+  private static final long serialVersionUID = 1L;
   private static final Logger LOG = LogManager.getLogger(HoodieDefaultTimeline.class);
 
   private static final String HASHING_ALGORITHM = "SHA-256";

File: hudi-common/src/main/java/org/apache/hudi/common/util/HoodieTimer.java
Patch:
@@ -32,7 +32,7 @@ public class HoodieTimer {
   // Ordered stack of TimeInfo's to make sure stopping the timer returns the correct elapsed time
   Deque<TimeInfo> timeInfoDeque = new ArrayDeque<>();
 
-  class TimeInfo {
+  static class TimeInfo {
 
     // captures the startTime of the code block
     long startTime;

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/DiskBasedMap.java
Patch:
@@ -55,7 +55,7 @@
  */
 public final class DiskBasedMap<T extends Serializable, R extends Serializable> implements Map<T, R>, Iterable<R> {
 
-  public static int BUFFER_SIZE = 128 * 1024;  // 128 KB
+  public static final int BUFFER_SIZE = 128 * 1024;  // 128 KB
   private static final Logger LOG = LogManager.getLogger(DiskBasedMap.class);
   // Stores the key and corresponding value's latest metadata spilled to disk
   private final Map<T, ValueMetadata> valueMetadataMap;

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieROTablePathFilter.java
Patch:
@@ -51,6 +51,7 @@
  */
 public class HoodieROTablePathFilter implements PathFilter, Serializable {
 
+  private static final long serialVersionUID = 1L;
   private static final Logger LOG = LogManager.getLogger(HoodieROTablePathFilter.class);
 
   /**

File: hudi-hive-sync/src/main/java/org/apache/hudi/hive/SlashEncodedDayPartitionValueExtractor.java
Patch:
@@ -33,6 +33,7 @@
  */
 public class SlashEncodedDayPartitionValueExtractor implements PartitionValueExtractor {
 
+  private static final long serialVersionUID = 1L;
   private transient DateTimeFormatter dtfOut;
 
   public SlashEncodedDayPartitionValueExtractor() {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java
Patch:
@@ -66,6 +66,7 @@
  */
 public class HDFSParquetImporter implements Serializable {
 
+  private static final long serialVersionUID = 1L;
   private static final Logger LOG = LogManager.getLogger(HDFSParquetImporter.class);
 
   private static final DateTimeFormatter PARTITION_FORMATTER = DateTimeFormatter.ofPattern("yyyy/MM/dd")

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/Compactor.java
Patch:
@@ -37,6 +37,7 @@
  */
 public class Compactor implements Serializable {
 
+  private static final long serialVersionUID = 1L;
   private static final Logger LOG = LogManager.getLogger(Compactor.class);
 
   private transient HoodieWriteClient compactionClient;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -83,9 +83,10 @@
  */
 public class DeltaSync implements Serializable {
 
+  private static final long serialVersionUID = 1L;
   private static final Logger LOG = LogManager.getLogger(DeltaSync.class);
-  public static String CHECKPOINT_KEY = "deltastreamer.checkpoint.key";
-  public static String CHECKPOINT_RESET_KEY = "deltastreamer.checkpoint.reset_key";
+  public static final String CHECKPOINT_KEY = "deltastreamer.checkpoint.key";
+  public static final String CHECKPOINT_RESET_KEY = "deltastreamer.checkpoint.reset_key";
 
   /**
    * Delta Sync Config.

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -77,6 +77,7 @@
  */
 public class HoodieDeltaStreamer implements Serializable {
 
+  private static final long serialVersionUID = 1L;
   private static final Logger LOG = LogManager.getLogger(HoodieDeltaStreamer.class);
 
   public static String CHECKPOINT_KEY = "deltastreamer.checkpoint.key";
@@ -302,6 +303,7 @@ public static void main(String[] args) throws Exception {
    */
   public static class DeltaSyncService extends AbstractDeltaStreamerService {
 
+    private static final long serialVersionUID = 1L;
     /**
      * Delta Sync Config.
      */
@@ -489,6 +491,7 @@ public TypedProperties getProps() {
    */
   public static class AsyncCompactService extends AbstractDeltaStreamerService {
 
+    private static final long serialVersionUID = 1L;
     private final int maxConcurrentCompaction;
     private transient Compactor compactor;
     private transient JavaSparkContext jssc;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java
Patch:
@@ -58,6 +58,7 @@
 
 public class TimelineServerPerf implements Serializable {
 
+  private static final long serialVersionUID = 1L;
   private static final Logger LOG = LogManager.getLogger(TimelineServerPerf.class);
   private final Config cfg;
   private transient TimelineService timelineServer;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HiveIncrPullSource.java
Patch:
@@ -59,6 +59,8 @@
  */
 public class HiveIncrPullSource extends AvroSource {
 
+  private static final long serialVersionUID = 1L;
+
   private static final Logger LOG = LogManager.getLogger(HiveIncrPullSource.class);
 
   private final transient FileSystem fs;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/AvroConvertor.java
Patch:
@@ -33,6 +33,7 @@
  */
 public class AvroConvertor implements Serializable {
 
+  private static final long serialVersionUID = 1L;
   /**
    * To be lazily inited on executors.
    */

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DFSPathSelector.java
Patch:
@@ -85,7 +85,7 @@ public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(Optio
       long maxModificationTime = Long.MIN_VALUE;
       List<FileStatus> filteredFiles = new ArrayList<>();
       for (FileStatus f : eligibleFiles) {
-        if (lastCheckpointStr.isPresent() && f.getModificationTime() <= Long.valueOf(lastCheckpointStr.get())) {
+        if (lastCheckpointStr.isPresent() && f.getModificationTime() <= Long.valueOf(lastCheckpointStr.get()).longValue()) {
           // skip processed files
           continue;
         }

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/BucketizedBloomCheckPartitioner.java
Patch:
@@ -145,7 +145,7 @@ public int getPartition(Object key) {
     final Pair<String, String> parts = (Pair<String, String>) key;
     final long hashOfKey = NumericUtils.getMessageDigestHash("MD5", parts.getRight());
     final List<Integer> candidatePartitions = fileGroupToPartitions.get(parts.getLeft());
-    final int idx = (int) Math.floorMod(hashOfKey, candidatePartitions.size());
+    final int idx = (int) Math.floorMod((int) hashOfKey, candidatePartitions.size());
     assert idx >= 0;
     return candidatePartitions.get(idx);
   }

File: hudi-cli/src/main/java/org/apache/hudi/cli/HoodieCLI.java
Patch:
@@ -20,10 +20,10 @@
 
 import org.apache.hudi.cli.utils.SparkTempViewProvider;
 import org.apache.hudi.cli.utils.TempViewProvider;
-import org.apache.hudi.common.model.TimelineLayoutVersion;
+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.ConsistencyGuardConfig;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.ConsistencyGuardConfig;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 
 import org.apache.hadoop.conf.Configuration;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/ArchivedCommitsCommand.java
Patch:
@@ -24,11 +24,11 @@
 import org.apache.hudi.cli.HoodiePrintHelper;
 import org.apache.hudi.cli.TableHeader;
 import org.apache.hudi.common.model.HoodieLogFile;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.log.HoodieLogFormat;
 import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;
 import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CommitsCommand.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieArchivedTimeline;
 import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/FileSystemViewCommand.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.view.HoodieTableFileSystemView;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieSyncCommand.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.cli.utils.CommitUtil;
 import org.apache.hudi.cli.utils.HiveUtil;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.exception.HoodieException;
 

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 
 import org.apache.hadoop.fs.Path;
 import org.apache.spark.launcher.SparkLauncher;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SavepointsCommand.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.cli.utils.InputStreamConsumer;
 import org.apache.hudi.cli.utils.SparkUtil;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.config.HoodieIndexConfig;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.cli.DedupeSparkJob;
 import org.apache.hudi.cli.utils.SparkUtil;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/StatsCommand.java
Patch:
@@ -22,10 +22,10 @@
 import org.apache.hudi.cli.HoodiePrintHelper;
 import org.apache.hudi.cli.TableHeader;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.NumericUtils;
 
 import com.codahale.metrics.Histogram;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/TableCommand.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.cli.TableHeader;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.ConsistencyGuardConfig;
+import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.exception.TableNotFoundException;
 
 import org.springframework.shell.core.CommandMarker;

File: hudi-cli/src/main/java/org/apache/hudi/cli/utils/CommitUtil.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 

File: hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.cli.commands.SparkEnvCommand;
 import org.apache.hudi.cli.commands.SparkMain;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.StringUtils;
 
 import org.apache.spark.SparkConf;

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestArchivedCommitsCommand.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.hudi.cli.common.HoodieTestCommitUtilities;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestTableCommand.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.cli.HoodieCLI;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.ConsistencyGuardConfig;
+import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.junit.Before;
 import org.junit.Test;
 import org.springframework.shell.core.CommandResult;

File: hudi-cli/src/test/java/org/apache/hudi/cli/common/HoodieTestCommitMetadataGenerator.java
Patch:
@@ -27,9 +27,9 @@
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.CollectionUtils;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.exception.HoodieIOException;
 
 import java.io.IOException;

File: hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieClient.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.client.embedded.EmbeddedTimelineService;
 import org.apache.hudi.client.utils.ClientUtils;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 

File: hudi-client/src/main/java/org/apache/hudi/client/embedded/EmbeddedTimelineService.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.client.embedded;
 
-import org.apache.hudi.common.SerializableConfiguration;
+import org.apache.hudi.common.config.SerializableConfiguration;
 import org.apache.hudi.common.table.view.FileSystemViewManager;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.table.view.FileSystemViewStorageType;

File: hudi-client/src/main/java/org/apache/hudi/client/utils/ClientUtils.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.client.utils;
 
-import org.apache.hudi.common.model.TimelineLayoutVersion;
+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.config;
 
+import org.apache.hudi.common.config.DefaultHoodieConfig;
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.util.ValidationUtils;

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieHBaseIndexConfig.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.config;
 
+import org.apache.hudi.common.config.DefaultHoodieConfig;
 import org.apache.hudi.index.hbase.DefaultHBaseQPSResourceAllocator;
 
 import java.io.File;

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java
Patch:
@@ -18,7 +18,8 @@
 
 package org.apache.hudi.config;
 
-import org.apache.hudi.common.bloom.filter.BloomFilterTypeCode;
+import org.apache.hudi.common.bloom.BloomFilterTypeCode;
+import org.apache.hudi.common.config.DefaultHoodieConfig;
 import org.apache.hudi.index.HoodieIndex;
 
 import javax.annotation.concurrent.Immutable;

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieMemoryConfig.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hudi.config;
 
+import org.apache.hudi.common.config.DefaultHoodieConfig;
+
 import javax.annotation.concurrent.Immutable;
 
 import java.io.File;

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieMetricsConfig.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.config;
 
+import org.apache.hudi.common.config.DefaultHoodieConfig;
 import org.apache.hudi.metrics.MetricsReporterType;
 
 import javax.annotation.concurrent.Immutable;

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieStorageConfig.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.hudi.config;
 
+import org.apache.hudi.common.config.DefaultHoodieConfig;
+
 import javax.annotation.concurrent.Immutable;
 
 import java.io.File;

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -20,10 +20,11 @@
 
 import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.common.config.DefaultHoodieConfig;
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
-import org.apache.hudi.common.model.TimelineLayoutVersion;
+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
-import org.apache.hudi.common.util.ConsistencyGuardConfig;
+import org.apache.hudi.common.fs.ConsistencyGuardConfig;
 import org.apache.hudi.common.util.ReflectionUtils;
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.table.compact.strategy.CompactionStrategy;

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieIOException;

File: hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ReflectionUtils;

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -30,15 +30,15 @@
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.HoodieWriteStat.RuntimeStats;
-import org.apache.hudi.common.table.TableFileSystemView.SliceView;
+import org.apache.hudi.common.table.view.TableFileSystemView.SliceView;
 import org.apache.hudi.common.table.log.HoodieLogFormat;
 import org.apache.hudi.common.table.log.HoodieLogFormat.Writer;
 import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;
 import org.apache.hudi.common.table.log.block.HoodieDeleteBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock.HeaderMetadataType;
-import org.apache.hudi.common.util.FSUtils;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieAppendException;

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.HoodieWriteStat.RuntimeStats;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieInsertException;

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieKeyLookupHandle.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.io;
 
-import org.apache.hudi.common.bloom.filter.BloomFilter;
+import org.apache.hudi.common.bloom.BloomFilter;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieTableType;

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -29,8 +29,8 @@
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.HoodieWriteStat.RuntimeStats;
 import org.apache.hudi.common.util.DefaultSizeEstimator;
-import org.apache.hudi.common.util.FSUtils;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.HoodieRecordSizeEstimator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.ExternalSpillableMap;

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java
Patch:
@@ -22,8 +22,8 @@
 import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.common.util.FSUtils;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.HoodieTimer;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ReflectionUtils;

File: hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieParquetWriter.java
Patch:
@@ -20,11 +20,11 @@
 
 import org.apache.hudi.avro.HoodieAvroWriteSupport;
 import org.apache.hudi.client.SparkTaskContextSupplier;
-import org.apache.hudi.common.io.storage.HoodieWrapperFileSystem;
+import org.apache.hudi.common.fs.HoodieWrapperFileSystem;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.common.util.FSUtils;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;

File: hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieStorageWriterFactory.java
Patch:
@@ -20,10 +20,10 @@
 
 import org.apache.hudi.avro.HoodieAvroWriteSupport;
 import org.apache.hudi.client.SparkTaskContextSupplier;
-import org.apache.hudi.common.bloom.filter.BloomFilter;
-import org.apache.hudi.common.bloom.filter.BloomFilterFactory;
+import org.apache.hudi.common.bloom.BloomFilter;
+import org.apache.hudi.common.bloom.BloomFilterFactory;
 import org.apache.hudi.common.model.HoodieRecordPayload;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 

File: hudi-client/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.metrics;
 
 import org.apache.hudi.common.model.HoodieCommitMetadata;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.config.HoodieWriteConfig;
 
 import com.codahale.metrics.Timer;

File: hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java
Patch:
@@ -32,11 +32,11 @@
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieRollingStatMetadata;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.NumericUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;

File: hudi-client/src/main/java/org/apache/hudi/table/HoodieMergeOnReadTable.java
Patch:
@@ -28,11 +28,11 @@
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.table.view.SyncableFileSystemView;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-client/src/main/java/org/apache/hudi/table/compact/strategy/CompactionStrategy.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.util.CompactionUtils;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.compact.HoodieMergeOnReadTableCompactor;

File: hudi-client/src/main/java/org/apache/hudi/table/rollback/RollbackHelper.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hudi.common.table.log.block.HoodieCommandBlock.HoodieCommandBlockTypeEnum;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock.HeaderMetadataType;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieRollbackException;

File: hudi-client/src/test/java/HoodieClientExample.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;

File: hudi-client/src/test/java/org/apache/hudi/client/TestClientRollback.java
Patch:
@@ -24,9 +24,9 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.TableFileSystemView.BaseFileOnlyView;
+import org.apache.hudi.common.table.view.TableFileSystemView.BaseFileOnlyView;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-client/src/test/java/org/apache/hudi/client/TestMultiFS.java
Patch:
@@ -26,9 +26,9 @@
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;

File: hudi-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java
Patch:
@@ -24,8 +24,8 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieTestUtils;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.FileIOUtils;
 import org.apache.hudi.common.util.ParquetUtils;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-client/src/test/java/org/apache/hudi/common/HoodieClientTestHarness.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.common.minicluster.HdfsTestService;
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;

File: hudi-client/src/test/java/org/apache/hudi/common/HoodieMergeOnReadTestUtils.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.common;
 
 import org.apache.hudi.common.model.HoodieTestUtils;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;
 
 import org.apache.avro.Schema;

File: hudi-client/src/test/java/org/apache/hudi/common/config/TestHoodieWriteConfig.java
Patch:
@@ -16,8 +16,10 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.config;
+package org.apache.hudi.common.config;
 
+import org.apache.hudi.config.HoodieCompactionConfig;
+import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.config.HoodieWriteConfig.Builder;
 
 import org.junit.Test;

File: hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieGlobalBloomIndex.java
Patch:
@@ -26,9 +26,9 @@
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.FileIOUtils;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieCommitArchiveLog.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.model.WriteOperationType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieArchivedTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieMergeHandle.java
Patch:
@@ -26,9 +26,9 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieStorageConfig;

File: hudi-client/src/test/java/org/apache/hudi/table/TestConsistencyGuard.java
Patch:
@@ -20,9 +20,9 @@
 
 import org.apache.hudi.common.HoodieClientTestHarness;
 import org.apache.hudi.common.HoodieClientTestUtils;
-import org.apache.hudi.common.util.ConsistencyGuard;
-import org.apache.hudi.common.util.ConsistencyGuardConfig;
-import org.apache.hudi.common.util.FailSafeConsistencyGuard;
+import org.apache.hudi.common.fs.ConsistencyGuard;
+import org.apache.hudi.common.fs.ConsistencyGuardConfig;
+import org.apache.hudi.common.fs.FailSafeConsistencyGuard;
 
 import org.apache.hadoop.fs.Path;
 import org.junit.After;

File: hudi-client/src/test/java/org/apache/hudi/table/TestCopyOnWriteTable.java
Patch:
@@ -26,14 +26,14 @@
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.TestRawTripPayload;
 import org.apache.hudi.common.TestRawTripPayload.MetadataMergeWriteStatus;
-import org.apache.hudi.common.bloom.filter.BloomFilter;
+import org.apache.hudi.common.bloom.BloomFilter;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.FileIOUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ParquetUtils;

File: hudi-client/src/test/java/org/apache/hudi/table/TestHoodieRecordSizing.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieWriteStat;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 

File: hudi-client/src/test/java/org/apache/hudi/table/compact/TestAsyncCompaction.java
Patch:
@@ -33,7 +33,7 @@
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;

File: hudi-client/src/test/java/org/apache/hudi/table/compact/TestHoodieCompactor.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieMemoryConfig;

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.avro;
 
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.exception.HoodieIOException;

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroWriteSupport.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.avro;
 
-import org.apache.hudi.common.bloom.filter.BloomFilter;
-import org.apache.hudi.common.bloom.filter.HoodieDynamicBoundedBloomFilter;
+import org.apache.hudi.common.bloom.BloomFilter;
+import org.apache.hudi.common.bloom.HoodieDynamicBoundedBloomFilter;
 
 import org.apache.avro.Schema;
 import org.apache.parquet.avro.AvroWriteSupport;

File: hudi-common/src/main/java/org/apache/hudi/common/bloom/BloomFilter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.bloom.filter;
+package org.apache.hudi.common.bloom;
 
 /**
  * A Bloom filter interface.

File: hudi-common/src/main/java/org/apache/hudi/common/bloom/BloomFilterFactory.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.bloom.filter;
+package org.apache.hudi.common.bloom;
 
 import org.apache.hadoop.util.hash.Hash;
 

File: hudi-common/src/main/java/org/apache/hudi/common/bloom/BloomFilterTypeCode.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.bloom.filter;
+package org.apache.hudi.common.bloom;
 
 /**
  * Bloom filter type codes.

File: hudi-common/src/main/java/org/apache/hudi/common/bloom/BloomFilterUtils.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.bloom.filter;
+package org.apache.hudi.common.bloom;
 
 /**
  * Bloom filter utils.

File: hudi-common/src/main/java/org/apache/hudi/common/bloom/HoodieDynamicBoundedBloomFilter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.bloom.filter;
+package org.apache.hudi.common.bloom;
 
 import org.apache.hudi.common.util.Base64CodecUtil;
 import org.apache.hudi.exception.HoodieIndexException;

File: hudi-common/src/main/java/org/apache/hudi/common/bloom/InternalDynamicBloomFilter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.bloom.filter;
+package org.apache.hudi.common.bloom;
 
 import org.apache.hadoop.util.bloom.BloomFilter;
 import org.apache.hadoop.util.bloom.Key;

File: hudi-common/src/main/java/org/apache/hudi/common/bloom/InternalFilter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.bloom.filter;
+package org.apache.hudi.common.bloom;
 
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.util.bloom.HashFunction;

File: hudi-common/src/main/java/org/apache/hudi/common/bloom/SimpleBloomFilter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.bloom.filter;
+package org.apache.hudi.common.bloom;
 
 import org.apache.hudi.common.util.Base64CodecUtil;
 import org.apache.hudi.exception.HoodieIndexException;

File: hudi-common/src/main/java/org/apache/hudi/common/config/DFSPropertiesConfiguration.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.common.config;
 
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;

File: hudi-common/src/main/java/org/apache/hudi/common/config/DefaultHoodieConfig.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.config;
+package org.apache.hudi.common.config;
 
 import java.io.Serializable;
 import java.util.Properties;

File: hudi-common/src/main/java/org/apache/hudi/common/config/SerializableConfiguration.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common;
+package org.apache.hudi.common.config;
 
 import org.apache.hadoop.conf.Configuration;
 

File: hudi-common/src/main/java/org/apache/hudi/common/config/TypedProperties.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.common.config;
 
 import java.io.Serializable;
 import java.util.Arrays;

File: hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuard.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.common.fs;
 
 import org.apache.hadoop.fs.Path;
 

File: hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuardConfig.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.common.fs;
 
-import org.apache.hudi.config.DefaultHoodieConfig;
+import org.apache.hudi.common.config.DefaultHoodieConfig;
 
 import java.io.File;
 import java.io.FileReader;

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java
Patch:
@@ -16,13 +16,15 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.common.fs;
 
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
+import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;

File: hudi-common/src/main/java/org/apache/hudi/common/fs/FailSafeConsistencyGuard.java
Patch:
@@ -16,11 +16,12 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.common.fs;
 
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 

File: hudi-common/src/main/java/org/apache/hudi/common/fs/NoOpConsistencyGuard.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.common.fs;
 
 import org.apache.hadoop.fs.Path;
 

File: hudi-common/src/main/java/org/apache/hudi/common/fs/SizeAwareDataInputStream.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.storage;
+package org.apache.hudi.common.fs;
 
 import java.io.DataInputStream;
 import java.io.IOException;

File: hudi-common/src/main/java/org/apache/hudi/common/fs/SizeAwareDataOutputStream.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util.collection.io.storage;
+package org.apache.hudi.common.fs;
 
 import java.io.BufferedOutputStream;
 import java.io.DataOutputStream;

File: hudi-common/src/main/java/org/apache/hudi/common/fs/SizeAwareFSDataOutputStream.java
Patch:
@@ -16,9 +16,8 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.io.storage;
+package org.apache.hudi.common.fs;
 
-import org.apache.hudi.common.util.ConsistencyGuard;
 import org.apache.hudi.exception.HoodieException;
 
 import org.apache.hadoop.fs.FSDataOutputStream;

File: hudi-common/src/main/java/org/apache/hudi/common/fs/StorageSchemes.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.storage;
+package org.apache.hudi.common.fs;
 
 import java.util.Arrays;
 

File: hudi-common/src/main/java/org/apache/hudi/common/fs/inline/InLineFSUtils.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.inline.fs;
+package org.apache.hudi.common.fs.inline;
 
 import org.apache.hadoop.fs.Path;
 

File: hudi-common/src/main/java/org/apache/hudi/common/fs/inline/InLineFileSystem.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.inline.fs;
+package org.apache.hudi.common.fs.inline;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataInputStream;

File: hudi-common/src/main/java/org/apache/hudi/common/fs/inline/InLineFsDataInputStream.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.inline.fs;
+package org.apache.hudi.common.fs.inline;
 
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.ReadOption;

File: hudi-common/src/main/java/org/apache/hudi/common/fs/inline/InMemoryFileSystem.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.inline.fs;
+package org.apache.hudi.common.fs.inline;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataInputStream;

File: hudi-common/src/main/java/org/apache/hudi/common/model/BaseAvroPayload.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/CompactionOperation.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.common.model;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 
 import org.apache.hadoop.fs.Path;

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroPayload.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieIOException;
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieBaseFile.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 
 import com.fasterxml.jackson.annotation.JsonAutoDetect;
 import com.fasterxml.jackson.annotation.JsonIgnoreProperties;

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieFileGroup.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieLogFile.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;

File: hudi-common/src/main/java/org/apache/hudi/common/model/OverwriteWithLatestAvroPayload.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.Option;
 
 import org.apache.avro.Schema;

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
-import org.apache.hudi.common.model.TimelineLayoutVersion;
+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieIOException;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordScanner.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;
 import org.apache.hudi.common.table.log.block.HoodieCommandBlock;
 import org.apache.hudi.common.table.log.block.HoodieDeleteBlock;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock.HeaderMetadataType;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock.HoodieLogBlockType;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.exception.CorruptedLogFileException;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormat.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java
Patch:
@@ -19,11 +19,11 @@
 package org.apache.hudi.common.table.log;
 
 import org.apache.hudi.common.model.HoodieLogFile;
-import org.apache.hudi.common.storage.StorageSchemes;
+import org.apache.hudi.common.fs.StorageSchemes;
 import org.apache.hudi.common.table.log.HoodieLogFormat.Writer;
 import org.apache.hudi.common.table.log.HoodieLogFormat.WriterBuilder;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/LogReaderUtils.java
Patch:
@@ -16,12 +16,12 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.common.table.log;
 
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.log.HoodieLogFormat;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;
 import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java
Patch:
@@ -19,8 +19,8 @@
 package org.apache.hudi.common.table.log.block;
 
 import org.apache.hudi.common.model.HoodieLogFile;
-import org.apache.hudi.common.storage.SizeAwareDataInputStream;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.common.fs.SizeAwareDataInputStream;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieIOException;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDeleteBlock.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieLogFile;
-import org.apache.hudi.common.storage.SizeAwareDataInputStream;
+import org.apache.hudi.common.fs.SizeAwareDataInputStream;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.SerializationUtils;
 import org.apache.hudi.exception.HoodieIOException;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieLogBlock.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.hudi.common.table.timeline;
 
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
 import org.apache.hudi.common.util.FileIOUtils;
 import org.apache.hudi.common.util.Option;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieArchivedTimeline.java
Patch:
@@ -26,7 +26,6 @@
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
 import org.apache.hudi.common.table.log.HoodieLogFormat;
 import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;
 import org.apache.hudi.common.util.Option;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.common.table.timeline;
 
-import org.apache.hudi.common.table.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
 import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.Option;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstant.java
Patch:
@@ -18,9 +18,8 @@
 
 package org.apache.hudi.common.table.timeline;
 
-import org.apache.hudi.common.table.HoodieTimeline;
 import org.apache.hudi.common.util.CollectionUtils;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 
 import org.apache.hadoop.fs.FileStatus;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java
Patch:
@@ -16,11 +16,10 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.table;
+package org.apache.hudi.common.table.timeline;
 
 import org.apache.hudi.common.model.HoodieTableType;
-import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;
-import org.apache.hudi.common.table.timeline.HoodieInstant;
+import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimelineDiffHelper.java
Patch:
@@ -16,11 +16,10 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.common.table.timeline;
 
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
+import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
 
 import org.apache.log4j.LogManager;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimelineLayout.java
Patch:
@@ -16,10 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.table;
+package org.apache.hudi.common.table.timeline;
 
-import org.apache.hudi.common.model.TimelineLayoutVersion;
-import org.apache.hudi.common.table.timeline.HoodieInstant;
+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.util.collection.Pair;
 
 import java.io.Serializable;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/TimelineDTO.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.common.table.timeline.dto;
 
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;
 
 import com.fasterxml.jackson.annotation.JsonIgnoreProperties;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/AbstractMigratorBase.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.versioning;
+package org.apache.hudi.common.table.timeline.versioning;
 
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/MetadataMigrator.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.versioning;
+package org.apache.hudi.common.table.timeline.versioning;
 
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.ValidationUtils;

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/TimelineLayoutVersion.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.model;
+package org.apache.hudi.common.table.timeline.versioning;
 
 import org.apache.hudi.common.util.ValidationUtils;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/VersionMigrator.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.versioning;
+package org.apache.hudi.common.table.timeline.versioning;
 
 import java.io.Serializable;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanMetadataMigrator.java
Patch:
@@ -16,11 +16,11 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.versioning.clean;
+package org.apache.hudi.common.table.timeline.versioning.clean;
 
 import org.apache.hudi.avro.model.HoodieCleanMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.versioning.MetadataMigrator;
+import org.apache.hudi.common.table.timeline.versioning.MetadataMigrator;
 
 import java.util.Arrays;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanV1MigrationHandler.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.versioning.clean;
+package org.apache.hudi.common.table.timeline.versioning.clean;
 
 import org.apache.hudi.avro.model.HoodieCleanMetadata;
 import org.apache.hudi.avro.model.HoodieCleanPartitionMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
-import org.apache.hudi.common.versioning.AbstractMigratorBase;
+import org.apache.hudi.common.table.timeline.versioning.AbstractMigratorBase;
 
 import org.apache.hadoop.fs.Path;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanV2MigrationHandler.java
Patch:
@@ -16,14 +16,14 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.versioning.clean;
+package org.apache.hudi.common.table.timeline.versioning.clean;
 
 import org.apache.hudi.avro.model.HoodieCleanMetadata;
 import org.apache.hudi.avro.model.HoodieCleanPartitionMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
-import org.apache.hudi.common.versioning.AbstractMigratorBase;
+import org.apache.hudi.common.table.timeline.versioning.AbstractMigratorBase;
 
 import org.apache.hadoop.fs.Path;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/compaction/CompactionPlanMigrator.java
Patch:
@@ -16,11 +16,11 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.versioning.compaction;
+package org.apache.hudi.common.table.timeline.versioning.compaction;
 
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.versioning.MetadataMigrator;
+import org.apache.hudi.common.table.timeline.versioning.MetadataMigrator;
 
 import java.util.Arrays;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/compaction/CompactionV1MigrationHandler.java
Patch:
@@ -16,14 +16,14 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.versioning.compaction;
+package org.apache.hudi.common.table.timeline.versioning.compaction;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.ValidationUtils;
-import org.apache.hudi.common.versioning.AbstractMigratorBase;
+import org.apache.hudi.common.table.timeline.versioning.AbstractMigratorBase;
 
 import org.apache.hadoop.fs.Path;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/compaction/CompactionV2MigrationHandler.java
Patch:
@@ -16,13 +16,13 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.versioning.compaction;
+package org.apache.hudi.common.table.timeline.versioning.compaction;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.ValidationUtils;
-import org.apache.hudi.common.versioning.AbstractMigratorBase;
+import org.apache.hudi.common.table.timeline.versioning.AbstractMigratorBase;
 
 import org.apache.hadoop.fs.Path;
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -25,11 +25,10 @@
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.CompactionUtils;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.HoodieTimer;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ValidationUtils;

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java
Patch:
@@ -18,10 +18,9 @@
 
 package org.apache.hudi.common.table.view;
 
-import org.apache.hudi.common.SerializableConfiguration;
+import org.apache.hudi.common.config.SerializableConfiguration;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.Functions.Function2;
 
 import org.apache.log4j.LogManager;

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewStorageConfig.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.common.table.view;
 
 import org.apache.hudi.common.util.ValidationUtils;
-import org.apache.hudi.config.DefaultHoodieConfig;
+import org.apache.hudi.common.config.DefaultHoodieConfig;
 
 import java.io.File;
 import java.io.FileReader;

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java
Patch:
@@ -22,8 +22,7 @@
 import org.apache.hudi.common.model.HoodieFileGroup;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.TableFileSystemView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/PriorityBasedFileSystemView.java
Patch:
@@ -22,8 +22,7 @@
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieFileGroup;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Functions.Function0;
 import org.apache.hudi.common.util.Functions.Function1;

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java
Patch:
@@ -23,8 +23,7 @@
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieFileGroup;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.dto.BaseFileDTO;
 import org.apache.hudi.common.table.timeline.dto.CompactionOpDTO;

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java
Patch:
@@ -25,9 +25,9 @@
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.RocksDBDAO;
+import org.apache.hudi.common.util.collection.RocksDBDAO;
 import org.apache.hudi.common.util.RocksDBSchemaHelper;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.common.model.HoodieFileGroup;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.DefaultSizeEstimator;
 import org.apache.hudi.common.util.collection.ExternalSpillableMap;
 import org.apache.hudi.common.util.collection.Pair;

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/SyncableFileSystemView.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.table;
+package org.apache.hudi.common.table.view;
 
-import org.apache.hudi.common.table.TableFileSystemView.BaseFileOnlyView;
-import org.apache.hudi.common.table.TableFileSystemView.SliceView;
+import org.apache.hudi.common.table.view.TableFileSystemView.BaseFileOnlyView;
+import org.apache.hudi.common.table.view.TableFileSystemView.SliceView;
 
 /**
  * A consolidated file-system view interface exposing both complete slice and basefile only views along with

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/TableFileSystemView.java
Patch:
@@ -16,13 +16,14 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.table;
+package org.apache.hudi.common.table.view;
 
 import org.apache.hudi.common.model.CompactionOperation;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieFileGroup;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/SpillableMapUtils.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.collection.DiskBasedMap.FileEntry;
-import org.apache.hudi.common.util.collection.io.storage.SizeAwareDataOutputStream;
+import org.apache.hudi.common.fs.SizeAwareDataOutputStream;
 import org.apache.hudi.exception.HoodieCorruptedDataException;
 
 import org.apache.avro.generic.GenericRecord;

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/DiskBasedMap.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.common.util.BufferedRandomAccessFile;
 import org.apache.hudi.common.util.SerializationUtils;
 import org.apache.hudi.common.util.SpillableMapUtils;
-import org.apache.hudi.common.util.collection.io.storage.SizeAwareDataOutputStream;
+import org.apache.hudi.common.fs.SizeAwareDataOutputStream;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.exception.HoodieNotSupportedException;

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDBBasedMap.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.common.util.collection;
 
-import org.apache.hudi.common.util.RocksDBDAO;
 import org.apache.hudi.exception.HoodieNotSupportedException;
 
 import java.io.Serializable;

File: hudi-common/src/test/java/org/apache/hudi/avro/TestHoodieAvroUtils.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.avro;
 
 import org.apache.avro.Schema;
 import org.codehaus.jackson.JsonNode;

File: hudi-common/src/test/java/org/apache/hudi/common/HoodieCommonTestHarness.java
Patch:
@@ -21,8 +21,8 @@
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.table.view.SyncableFileSystemView;
 import org.apache.hudi.common.table.view.HoodieTableFileSystemView;
 
 import org.junit.Rule;

File: hudi-common/src/test/java/org/apache/hudi/common/bloom/TestBloomFilter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.bloom.filter;
+package org.apache.hudi.common.bloom;
 
 import org.junit.Assert;
 import org.junit.Test;

File: hudi-common/src/test/java/org/apache/hudi/common/bloom/TestInternalDynamicBloomFilter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.bloom.filter;
+package org.apache.hudi.common.bloom;
 
 import org.apache.hadoop.util.hash.Hash;
 import org.junit.Assert;

File: hudi-common/src/test/java/org/apache/hudi/common/fs/TestFSUtils.java
Patch:
@@ -16,13 +16,13 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.common.fs;
 
 import org.apache.hudi.common.HoodieCommonTestHarness;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.exception.HoodieException;
 

File: hudi-common/src/test/java/org/apache/hudi/common/fs/inline/FileSystemTestUtils.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.inline.fs;
+package org.apache.hudi.common.fs.inline;
 
 import org.apache.hadoop.fs.Path;
 

File: hudi-common/src/test/java/org/apache/hudi/common/fs/inline/TestInLineFileSystem.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.inline.fs;
+package org.apache.hudi.common.fs.inline;
 
 import org.apache.hudi.common.util.collection.Pair;
 
@@ -38,8 +38,8 @@
 import java.util.Arrays;
 import java.util.List;
 
-import static org.apache.hudi.common.inline.fs.FileSystemTestUtils.RANDOM;
-import static org.apache.hudi.common.inline.fs.FileSystemTestUtils.getRandomOuterFSPath;
+import static org.apache.hudi.common.fs.inline.FileSystemTestUtils.RANDOM;
+import static org.apache.hudi.common.fs.inline.FileSystemTestUtils.getRandomOuterFSPath;
 
 /**
  * Tests {@link InLineFileSystem}.

File: hudi-common/src/test/java/org/apache/hudi/common/fs/inline/TestInMemoryFileSystem.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.inline.fs;
+package org.apache.hudi.common.fs.inline;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataOutputStream;
@@ -27,8 +27,8 @@
 import java.io.IOException;
 import java.net.URI;
 
-import static org.apache.hudi.common.inline.fs.FileSystemTestUtils.RANDOM;
-import static org.apache.hudi.common.inline.fs.FileSystemTestUtils.getRandomOuterInMemPath;
+import static org.apache.hudi.common.fs.inline.FileSystemTestUtils.RANDOM;
+import static org.apache.hudi.common.fs.inline.FileSystemTestUtils.getRandomOuterInMemPath;
 
 /**
  * Unit tests {@link InMemoryFileSystem}.

File: hudi-common/src/test/java/org/apache/hudi/common/minicluster/HdfsTestService.java
Patch:
@@ -54,7 +54,7 @@ public class HdfsTestService {
   private MiniDFSCluster miniDfsCluster;
 
   public HdfsTestService() throws IOException {
-    workDir = Files.createTempDirectory("temp").getName(0).toString();
+    workDir = Files.createTempDirectory("temp").toFile().getAbsolutePath();
   }
 
   public Configuration getHadoopConf() {

File: hudi-common/src/test/java/org/apache/hudi/common/model/AvroBinaryTestPayload.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieIOException;
 

File: hudi-common/src/test/java/org/apache/hudi/common/model/TestHoodieWriteStat.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.model;
 
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 
 import org.apache.hadoop.fs.Path;
 import org.junit.Test;

File: hudi-common/src/test/java/org/apache/hudi/common/storage/TestStorageSchemes.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.common.storage;
 
+import org.apache.hudi.common.fs.StorageSchemes;
 import org.junit.Test;
 
 import static org.junit.Assert.assertFalse;

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestHoodieTableMetaClient.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.util.Option;
 
 import org.junit.Before;

File: hudi-common/src/test/java/org/apache/hudi/common/table/TestTimelineLayout.java
Patch:
@@ -18,10 +18,12 @@
 
 package org.apache.hudi.common.table;
 
-import org.apache.hudi.common.model.TimelineLayoutVersion;
+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
 
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.TimelineLayout;
 import org.junit.Assert;
 import org.junit.Test;
 

File: hudi-common/src/test/java/org/apache/hudi/common/table/log/TestHoodieLogFormat.java
Patch:
@@ -34,8 +34,8 @@
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock.HeaderMetadataType;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock.HoodieLogBlockType;
-import org.apache.hudi.common.util.FSUtils;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.SchemaTestUtil;
 import org.apache.hudi.exception.CorruptedLogFileException;
 

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestPriorityBasedFileSystemView.java
Patch:
@@ -22,9 +22,8 @@
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieFileGroup;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
-import org.apache.hudi.common.table.string.MockHoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.MockHoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.ImmutablePair;

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestRocksDBBasedIncrementalFSViewSync.java
Patch:
@@ -19,8 +19,7 @@
 package org.apache.hudi.common.table.view;
 
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 
 import java.io.IOException;
 

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestRocksDbBasedFileSystemView.java
Patch:
@@ -18,8 +18,7 @@
 
 package org.apache.hudi.common.table.view;
 
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 
 import java.io.IOException;
 

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestSpillableMapBasedFileSystemView.java
Patch:
@@ -18,8 +18,7 @@
 
 package org.apache.hudi.common.table.view;
 
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 
 /**
  * Tests spillable map based file system view {@link SyncableFileSystemView}.

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestSpillableMapBasedIncrementalFSViewSync.java
Patch:
@@ -19,8 +19,7 @@
 package org.apache.hudi.common.table.view;
 
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 
 /**
  * Tests spillable map based incremental fs view sync.

File: hudi-common/src/test/java/org/apache/hudi/common/util/SchemaTestUtil.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.common.util;
 
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.avro.MercifulJsonConverter;
 import org.apache.hudi.common.model.HoodieAvroPayload;
 import org.apache.hudi.common.model.HoodieKey;

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestCompactionUtils.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.common.HoodieCommonTestHarness;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieFileGroupId;
@@ -29,7 +30,7 @@
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.CompactionTestUtils.TestHoodieBaseFile;
 import org.apache.hudi.common.util.collection.Pair;
-import org.apache.hudi.common.versioning.compaction.CompactionPlanMigrator;
+import org.apache.hudi.common.table.timeline.versioning.compaction.CompactionPlanMigrator;
 
 import org.apache.hadoop.fs.Path;
 import org.junit.Assert;

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestDFSPropertiesConfiguration.java
Patch:
@@ -23,6 +23,8 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hudi.common.config.DFSPropertiesConfiguration;
+import org.apache.hudi.common.config.TypedProperties;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;

File: hudi-common/src/test/java/org/apache/hudi/common/util/collection/TestDiskBasedMap.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.HoodieRecordSizeEstimator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.SchemaTestUtil;

File: hudi-common/src/test/java/org/apache/hudi/common/util/collection/TestExternalSpillableMap.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.util.DefaultSizeEstimator;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.HoodieRecordSizeEstimator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.SchemaTestUtil;

File: hudi-common/src/test/java/org/apache/hudi/common/util/collection/TestRocksDBManager.java
Patch:
@@ -16,10 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.common.util;
+package org.apache.hudi.common.util.collection;
 
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
-import org.apache.hudi.common.util.collection.Pair;
 
 import org.junit.AfterClass;
 import org.junit.Assert;

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java
Patch:
@@ -45,8 +45,8 @@
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.TableFileSystemView.BaseFileOnlyView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.table.view.TableFileSystemView.BaseFileOnlyView;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.view.HoodieTableFileSystemView;
 import org.apache.hudi.common.util.StringUtils;

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieROTablePathFilter.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.hadoop;
 
-import org.apache.hudi.common.SerializableConfiguration;
+import org.apache.hudi.common.config.SerializableConfiguration;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java
Patch:
@@ -20,8 +20,8 @@
 
 import org.apache.hudi.common.model.HoodieAvroPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.HoodieAvroUtils;
-import org.apache.hudi.common.util.LogReaderUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
+import org.apache.hudi.common.table.log.LogReaderUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java
Patch:
@@ -22,11 +22,11 @@
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.view.HoodieTableFileSystemView;
 import org.apache.hudi.common.util.CollectionUtils;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.exception.HoodieException;

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java
Patch:
@@ -21,8 +21,8 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;
-import org.apache.hudi.common.util.FSUtils;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.Option;
 
 import org.apache.avro.generic.GenericRecord;

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.common.table.log.HoodieUnMergedLogRecordScanner;
 import org.apache.hudi.common.util.DefaultSizeEstimator;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.queue.BoundedInMemoryExecutor;
 import org.apache.hudi.common.util.queue.BoundedInMemoryQueueProducer;

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/InputFormatTestUtil.java
Patch:
@@ -20,8 +20,8 @@
 
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieTestUtils;
-import org.apache.hudi.common.util.FSUtils;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.SchemaTestUtil;
 
 import org.apache.avro.Schema;

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestHoodieParquetInputFormat.java
Patch:
@@ -38,7 +38,7 @@
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.model.HoodieWriteStat;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.junit.Before;
 import org.junit.Rule;
 import org.junit.Test;

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/realtime/TestHoodieRealtimeRecordReader.java
Patch:
@@ -22,16 +22,16 @@
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.HoodieTestUtils;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.log.HoodieLogFormat;
 import org.apache.hudi.common.table.log.HoodieLogFormat.Writer;
 import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;
 import org.apache.hudi.common.table.log.block.HoodieCommandBlock;
 import org.apache.hudi.common.table.log.block.HoodieCommandBlock.HoodieCommandBlockTypeEnum;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
 import org.apache.hudi.common.table.log.block.HoodieLogBlock.HeaderMetadataType;
-import org.apache.hudi.common.util.FSUtils;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.SchemaTestUtil;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;

File: hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.hive;
 
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.InvalidTableException;
 import org.apache.hudi.hadoop.HoodieParquetInputFormat;

File: hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java
Patch:
@@ -22,11 +22,11 @@
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.model.HoodieTableType;
-import org.apache.hudi.common.storage.StorageSchemes;
+import org.apache.hudi.common.fs.StorageSchemes;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;

File: hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.ReflectionUtils;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-spark/src/main/java/org/apache/hudi/HoodieDataSourceHelpers.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.CollectionUtils;

File: hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
-import org.apache.hudi.common.util.HoodieAvroUtils;
+import org.apache.hudi.avro.HoodieAvroUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieIOException;
 

File: hudi-spark/src/main/java/org/apache/hudi/keygen/ComplexKeyGenerator.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.DataSourceUtils;
 import org.apache.hudi.DataSourceWriteOptions;
 import org.apache.hudi.common.model.HoodieKey;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieKeyException;
 
 import org.apache.avro.generic.GenericRecord;

File: hudi-spark/src/main/java/org/apache/hudi/keygen/GlobalDeleteKeyGenerator.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.hudi.DataSourceUtils;
 import org.apache.hudi.DataSourceWriteOptions;
 import org.apache.hudi.common.model.HoodieKey;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieKeyException;
 
 /**

File: hudi-spark/src/main/java/org/apache/hudi/keygen/KeyGenerator.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.keygen;
 
 import org.apache.hudi.common.model.HoodieKey;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 
 import org.apache.avro.generic.GenericRecord;
 

File: hudi-spark/src/main/java/org/apache/hudi/keygen/NonpartitionedKeyGenerator.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.DataSourceUtils;
 import org.apache.hudi.common.model.HoodieKey;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieKeyException;
 
 import org.apache.avro.generic.GenericRecord;

File: hudi-spark/src/main/java/org/apache/hudi/keygen/SimpleKeyGenerator.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.DataSourceUtils;
 import org.apache.hudi.DataSourceWriteOptions;
 import org.apache.hudi.common.model.HoodieKey;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieKeyException;
 
 import org.apache.avro.generic.GenericRecord;

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.timeline.service;
 
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.table.view.SyncableFileSystemView;
 import org.apache.hudi.common.table.timeline.dto.CompactionOpDTO;
 import org.apache.hudi.common.table.timeline.dto.BaseFileDTO;
 import org.apache.hudi.common.table.timeline.dto.FileGroupDTO;

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/TimelineService.java
Patch:
@@ -18,11 +18,11 @@
 
 package org.apache.hudi.timeline.service;
 
-import org.apache.hudi.common.SerializableConfiguration;
+import org.apache.hudi.common.config.SerializableConfiguration;
 import org.apache.hudi.common.table.view.FileSystemViewManager;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.table.view.FileSystemViewStorageType;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 
 import com.beust.jcommander.JCommander;
 import com.beust.jcommander.Parameter;

File: hudi-timeline-service/src/test/java/org/apache/hudi/timeline/table/view/TestRemoteHoodieTableFileSystemView.java
Patch:
@@ -18,9 +18,9 @@
 
 package org.apache.hudi.timeline.table.view;
 
-import org.apache.hudi.common.SerializableConfiguration;
-import org.apache.hudi.common.table.HoodieTimeline;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.config.SerializableConfiguration;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
+import org.apache.hudi.common.table.view.SyncableFileSystemView;
 import org.apache.hudi.common.table.view.FileSystemViewManager;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.table.view.FileSystemViewStorageType;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java
Patch:
@@ -26,9 +26,9 @@
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieIOException;
 
 import com.beust.jcommander.IValueValidator;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java
Patch:
@@ -19,8 +19,8 @@
 package org.apache.hudi.utilities;
 
 import org.apache.hudi.client.HoodieWriteClient;
-import org.apache.hudi.common.util.FSUtils;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.config.HoodieWriteConfig;
 
 import com.beust.jcommander.JCommander;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.client.CompactionAdminClient.ValidationOpResult;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 
 import com.beust.jcommander.JCommander;
 import com.beust.jcommander.Parameter;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java
Patch:
@@ -20,9 +20,9 @@
 
 import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 
 import com.beust.jcommander.JCommander;
 import com.beust.jcommander.Parameter;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
Patch:
@@ -22,10 +22,10 @@
 import org.apache.hudi.AvroConversionUtils;
 import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.client.WriteStatus;
-import org.apache.hudi.common.util.DFSPropertiesConfiguration;
+import org.apache.hudi.common.config.DFSPropertiesConfiguration;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ReflectionUtils;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/adhoc/UpgradePayloadFromUberToApache.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableConfig;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.config.HoodieCompactionConfig;
 
 import com.beust.jcommander.JCommander;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -29,10 +29,10 @@
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -22,13 +22,13 @@
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
 import org.apache.hudi.common.util.CompactionUtils;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/keygen/TimestampBasedKeyGenerator.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.DataSourceUtils;
 import org.apache.hudi.keygen.SimpleKeyGenerator;
 import org.apache.hudi.common.model.HoodieKey;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieKeyException;
 import org.apache.hudi.exception.HoodieNotSupportedException;
 import org.apache.hudi.utilities.exception.HoodieDeltaStreamerException;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java
Patch:
@@ -20,11 +20,11 @@
 
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.SyncableFileSystemView;
+import org.apache.hudi.common.table.view.SyncableFileSystemView;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.table.view.FileSystemViewStorageType;
 import org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.timeline.service.TimelineService;
 import org.apache.hudi.utilities.UtilHelpers;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/FilebasedSchemaProvider.java
Patch:
@@ -19,8 +19,8 @@
 package org.apache.hudi.utilities.schema;
 
 import org.apache.hudi.DataSourceUtils;
-import org.apache.hudi.common.util.FSUtils;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.fs.FSUtils;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieIOException;
 
 import org.apache.avro.Schema;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/JdbcbasedSchemaProvider.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.schema;
 
 import org.apache.avro.Schema;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.utilities.UtilHelpers;
 import org.apache.spark.api.java.JavaSparkContext;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/NullTargetSchemaRegistryProvider.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.utilities.schema;
 
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 
 import org.apache.avro.Schema;
 import org.apache.spark.api.java.JavaSparkContext;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/RowBasedSchemaProvider.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.schema;
 
 import org.apache.hudi.AvroConversionUtils;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 
 import org.apache.avro.Schema;
 import org.apache.spark.api.java.JavaSparkContext;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaProvider.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.utilities.schema;
 
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 
 import org.apache.avro.Schema;
 import org.apache.spark.api.java.JavaSparkContext;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.schema;
 
 import org.apache.hudi.DataSourceUtils;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieIOException;
 
 import com.fasterxml.jackson.databind.JsonNode;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/AvroDFSSource.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 import org.apache.hudi.utilities.sources.helpers.DFSPathSelector;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/AvroKafkaSource.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 import org.apache.hudi.utilities.sources.helpers.KafkaOffsetGen;
 import org.apache.hudi.utilities.sources.helpers.KafkaOffsetGen.CheckpointUtils;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/AvroSource.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.utilities.sources;
 
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 
 import org.apache.avro.generic.GenericRecord;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/CsvDFSSource.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 import org.apache.hudi.utilities.sources.helpers.DFSPathSelector;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HiveIncrPullSource.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.DataSourceUtils;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.utilities.HiveIncrementalPuller;
 import org.apache.hudi.utilities.schema.SchemaProvider;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.hudi.DataSourceUtils;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor;
 import org.apache.hudi.utilities.schema.SchemaProvider;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JsonDFSSource.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 import org.apache.hudi.utilities.sources.helpers.DFSPathSelector;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JsonKafkaSource.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 import org.apache.hudi.utilities.sources.helpers.KafkaOffsetGen;
 import org.apache.hudi.utilities.sources.helpers.KafkaOffsetGen.CheckpointUtils;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JsonSource.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.utilities.sources;
 
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 
 import org.apache.spark.api.java.JavaRDD;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/ParquetDFSSource.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 import org.apache.hudi.utilities.sources.helpers.DFSPathSelector;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/RowSource.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.utilities.schema.RowBasedSchemaProvider;
 import org.apache.hudi.utilities.schema.SchemaProvider;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/Source.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 
 import org.apache.spark.api.java.JavaSparkContext;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DFSPathSelector.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.hudi.utilities.sources.helpers;
 
 import org.apache.hudi.DataSourceUtils;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.collection.ImmutablePair;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieIOException;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources.helpers;
 
 import org.apache.hudi.common.table.HoodieTableMetaClient;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ValidationUtils;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.DataSourceUtils;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieNotSupportedException;
 
 import org.apache.kafka.clients.consumer.KafkaConsumer;
@@ -50,7 +50,7 @@ public class KafkaOffsetGen {
   public static class CheckpointUtils {
 
     /**
-     * Reconstruct checkpoint from string.
+     * Reconstruct checkpoint from timeline.
      */
     public static HashMap<TopicPartition, Long> strToOffsets(String checkpointStr) {
       HashMap<TopicPartition, Long> offsetMap = new HashMap<>();

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/transform/AWSDmsTransformer.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.transform;
 
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.payload.AWSDmsAvroPayload;
 
 import org.apache.spark.api.java.JavaSparkContext;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/transform/FlatteningTransformer.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.utilities.transform;
 
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/transform/SqlQueryBasedTransformer.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.utilities.transform;
 
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/transform/Transformer.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.utilities.transform;
 
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.Dataset;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.minicluster.HdfsTestService;
 import org.apache.hudi.common.model.HoodieTestUtils;
-import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.table.timeline.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 
 import org.apache.avro.generic.GenericRecord;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotCopier.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.common.HoodieCommonTestHarness;
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.model.HoodieTestUtils;
-import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.fs.FSUtils;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestJdbcbasedSchemaProvider.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.utilities;
 
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.utilities.schema.JdbcbasedSchemaProvider;
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestTimestampBasedKeyGenerator.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.DataSourceWriteOptions;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.util.SchemaTestUtil;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.keygen.TimestampBasedKeyGenerator;
 import org.junit.Before;
 import org.junit.Test;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/UtilitiesTestBase.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.hive.HiveSyncConfig;
 import org.apache.hudi.hive.HoodieHiveClient;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/AbstractBaseTestSource.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.common.util.collection.RocksDBBasedMap;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.utilities.schema.SchemaProvider;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/DistributedTestDataSource.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 import org.apache.hudi.utilities.sources.config.TestSourceConfig;
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestCsvDFSSource.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.UtilitiesTestBase;
 import org.apache.hudi.utilities.schema.FilebasedSchemaProvider;
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestDataSource.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 
 import org.apache.avro.generic.GenericRecord;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestJsonDFSSource.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.UtilitiesTestBase;
 
 import org.apache.hadoop.fs.Path;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestKafkaSource.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.hudi.AvroConversionUtils;
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 import org.apache.hudi.utilities.UtilitiesTestBase;
 import org.apache.hudi.utilities.deltastreamer.SourceFormatAdapter;
 import org.apache.hudi.utilities.schema.FilebasedSchemaProvider;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestParquetDFSSource.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.utilities.sources;
 
 import org.apache.hudi.common.model.HoodieRecord;
-import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.config.TypedProperties;
 
 import org.apache.hadoop.fs.Path;
 import org.junit.Before;

File: hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java
Patch:
@@ -499,7 +499,7 @@ public String toString() {
   }
 
   /**
-   * Helper class for an insert bucket along with the weight [0.0, 0.1] that defines the amount of incoming inserts that
+   * Helper class for an insert bucket along with the weight [0.0, 1.0] that defines the amount of incoming inserts that
    * should be allocated to the bucket.
    */
   class InsertBucket implements Serializable {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java
Patch:
@@ -180,7 +180,7 @@ public OffsetRange[] getNextOffsetRanges(Option<String> lastCheckpointStr, long
               .map(x -> new TopicPartition(x.topic(), x.partition())).collect(Collectors.toSet());
 
       // Determine the offset ranges to read from
-      if (lastCheckpointStr.isPresent()) {
+      if (lastCheckpointStr.isPresent() && !lastCheckpointStr.get().isEmpty()) {
         fromOffsets = checkupValidOffsets(consumer, lastCheckpointStr, topicPartitions);
       } else {
         KafkaResetOffsetStrategies autoResetValue = KafkaResetOffsetStrategies

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieDeltaStreamer.java
Patch:
@@ -102,7 +102,7 @@ public class TestHoodieDeltaStreamer extends UtilitiesTestBase {
   private static final String PROPS_FILENAME_TEST_INVALID = "test-invalid.properties";
   private static final String PROPS_FILENAME_TEST_CSV = "test-csv-dfs-source.properties";
   private static final String PROPS_FILENAME_TEST_PARQUET = "test-parquet-dfs-source.properties";
-  private static final String PARQUET_SOURCE_ROOT = dfsBasePath + "/parquetFiles";
+  private static String PARQUET_SOURCE_ROOT;
   private static final int PARQUET_NUM_RECORDS = 5;
   private static final int CSV_NUM_RECORDS = 3;
   private static final Logger LOG = LogManager.getLogger(TestHoodieDeltaStreamer.class);
@@ -112,6 +112,7 @@ public class TestHoodieDeltaStreamer extends UtilitiesTestBase {
   @BeforeClass
   public static void initClass() throws Exception {
     UtilitiesTestBase.initClass(true);
+    PARQUET_SOURCE_ROOT = dfsBasePath + "/parquetFiles";
 
     // prepare the configs.
     UtilitiesTestBase.Helpers.copyToDFS("delta-streamer-config/base.properties", dfs, dfsBasePath + "/base.properties");

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -303,7 +303,6 @@ private Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource
                 t, HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD());
       }
 
-      // Use Transformed Row's schema if not overridden
       // Use Transformed Row's schema if not overridden. If target schema is not specified
       // default to RowBasedSchemaProvider
       schemaProvider = this.schemaProvider == null || this.schemaProvider.getTargetSchema() == null

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -124,7 +124,7 @@ public void sync() throws Exception {
         throw ex;
       } finally {
         deltaSyncService.close();
-        LOG.info("Shut down deltastreamer");
+        LOG.info("Shut down delta streamer");
       }
     }
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/Source.java
Patch:
@@ -22,8 +22,6 @@
 import org.apache.hudi.common.util.TypedProperties;
 import org.apache.hudi.utilities.schema.SchemaProvider;
 
-import org.apache.log4j.LogManager;
-import org.apache.log4j.Logger;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.SparkSession;
 
@@ -33,7 +31,6 @@
  * Represents a source from which we can tail data. Assumes a constructor that takes properties.
  */
 public abstract class Source<T> implements Serializable {
-  private static final Logger LOG = LogManager.getLogger(Source.class);
 
   public enum SourceType {
     JSON, AVRO, ROW

File: hudi-common/src/main/java/org/apache/hudi/common/versioning/compaction/CompactionV2MigrationHandler.java
Patch:
@@ -55,7 +55,7 @@ public HoodieCompactionPlan upgradeFrom(HoodieCompactionPlan input) {
       v2CompactionOperationList = input.getOperations().stream().map(inp ->
         HoodieCompactionOperation.newBuilder().setBaseInstantTime(inp.getBaseInstantTime())
             .setFileId(inp.getFileId()).setPartitionPath(inp.getPartitionPath()).setMetrics(inp.getMetrics())
-            .setDataFilePath(new Path(inp.getDataFilePath()).getName()).setDeltaFilePaths(
+            .setDataFilePath(inp.getDataFilePath() == null ? null : new Path(inp.getDataFilePath()).getName()).setDeltaFilePaths(
                 inp.getDeltaFilePaths().stream().map(s -> new Path(s).getName()).collect(Collectors.toList()))
         .build()).collect(Collectors.toList());
     }

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestCompactionUtils.java
Patch:
@@ -84,6 +84,8 @@ public void testUpgradeDowngrade() {
     HoodieCompactionPlan newPlan = migrator.upgradeToLatest(plan, plan.getVersion());
     Assert.assertEquals(LATEST_COMPACTION_METADATA_VERSION, newPlan.getVersion());
     testFileSlicesCompactionPlanEquality(inputAndPlan.getKey(), newPlan);
+    HoodieCompactionPlan latestPlan = migrator.migrateToVersion(oldPlan, oldPlan.getVersion(), newPlan.getVersion());
+    testFileSlicesCompactionPlanEquality(inputAndPlan.getKey(), latestPlan);
   }
 
   @Test

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/TableCommand.java
Patch:
@@ -54,7 +54,7 @@ public String connect(
           help = "Enable eventual consistency") final boolean eventuallyConsistent,
       @CliOption(key = {"initialCheckIntervalMs"}, unspecifiedDefaultValue = "2000",
           help = "Initial wait time for eventual consistency") final Integer initialConsistencyIntervalMs,
-      @CliOption(key = {"maxCheckIntervalMs"}, unspecifiedDefaultValue = "300000",
+      @CliOption(key = {"maxWaitIntervalMs"}, unspecifiedDefaultValue = "300000",
           help = "Max wait time for eventual consistency") final Integer maxConsistencyIntervalMs,
       @CliOption(key = {"maxCheckIntervalMs"}, unspecifiedDefaultValue = "7",
           help = "Max checks for eventual consistency") final Integer maxConsistencyChecks)

File: hudi-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java
Patch:
@@ -40,6 +40,7 @@
 import org.junit.After;
 import org.junit.Assert;
 import org.junit.Before;
+import org.junit.Test;
 //import org.junit.Test;
 
 import java.util.ArrayList;
@@ -55,14 +56,15 @@ public void setUp() throws Exception {
     initPath();
     HoodieTestUtils.init(HoodieTestUtils.getDefaultHadoopConf(), basePath);
     initSparkContexts("TestUpdateSchemaEvolution");
+    initFileSystem();
   }
 
   @After
   public void tearDown() {
     cleanupSparkContexts();
   }
 
-  //@Test
+  @Test
   public void testSchemaEvolutionOnUpdate() throws Exception {
     // Create a bunch of records with a old version of schema
     final HoodieWriteConfig config = makeHoodieClientConfig("/exampleSchema.txt");

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieDeltaStreamer.java
Patch:
@@ -422,8 +422,8 @@ private void testUpsertsContinuousMode(HoodieTableType tableType, String tempDir
       } else {
         TestHelpers.assertAtleastNCompactionCommits(5, tableBasePath, dfs);
       }
-      TestHelpers.assertRecordCount(totalRecords + 200, tableBasePath + "/*/*.parquet", sqlContext);
-      TestHelpers.assertDistanceCount(totalRecords + 200, tableBasePath + "/*/*.parquet", sqlContext);
+      TestHelpers.assertRecordCount(totalRecords, tableBasePath + "/*/*.parquet", sqlContext);
+      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + "/*/*.parquet", sqlContext);
       return true;
     }, 180);
     ds.shutdownGracefully();

File: hudi-client/src/main/java/org/apache/hudi/table/HoodieCommitArchiveLog.java
Patch:
@@ -331,7 +331,7 @@ private IndexedRecord convertToAvroRecord(HoodieTimeline commitTimeline, HoodieI
     return archivedMetaWrapper;
   }
 
-  private org.apache.hudi.avro.model.HoodieCommitMetadata commitMetadataConverter(
+  public org.apache.hudi.avro.model.HoodieCommitMetadata commitMetadataConverter(
       HoodieCommitMetadata hoodieCommitMetadata) {
     ObjectMapper mapper = new ObjectMapper();
     // Need this to ignore other public get() methods

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.cli.commands;
 
-import org.apache.hudi.CompactionAdminClient.RenameOpResult;
-import org.apache.hudi.CompactionAdminClient.ValidationOpResult;
+import org.apache.hudi.client.CompactionAdminClient.RenameOpResult;
+import org.apache.hudi.client.CompactionAdminClient.ValidationOpResult;
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.cli.HoodieCLI;
@@ -42,7 +42,7 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
-import org.apache.hudi.func.OperationResult;
+import org.apache.hudi.table.compact.OperationResult;
 import org.apache.hudi.utilities.UtilHelpers;
 
 import org.apache.hadoop.fs.FSDataInputStream;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SavepointsCommand.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.cli.commands;
 
-import org.apache.hudi.HoodieWriteClient;
+import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.cli.HoodieCLI;
 import org.apache.hudi.cli.HoodiePrintHelper;
 import org.apache.hudi.cli.utils.InputStreamConsumer;

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
Patch:
@@ -18,15 +18,15 @@
 
 package org.apache.hudi.cli.commands;
 
-import org.apache.hudi.HoodieWriteClient;
+import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.cli.DedupeSparkJob;
 import org.apache.hudi.cli.utils.SparkUtil;
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
-import org.apache.hudi.io.compact.strategy.UnBoundedCompactionStrategy;
+import org.apache.hudi.table.compact.strategy.UnBoundedCompactionStrategy;
 import org.apache.hudi.utilities.HDFSParquetImporter;
 import org.apache.hudi.utilities.HDFSParquetImporter.Config;
 import org.apache.hudi.utilities.HoodieCleaner;

File: hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.cli.utils;
 
-import org.apache.hudi.HoodieWriteClient;
+import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.cli.commands.SparkEnvCommand;
 import org.apache.hudi.cli.commands.SparkMain;
 import org.apache.hudi.common.util.FSUtils;

File: hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieClient.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi;
+package org.apache.hudi.client;
 
 import org.apache.hudi.client.embedded.EmbeddedTimelineService;
 import org.apache.hudi.client.utils.ClientUtils;

File: hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java
Patch:
@@ -16,9 +16,10 @@
  * limitations under the License.
  */
 
-package org.apache.hudi;
+package org.apache.hudi.client;
 
 import java.util.Collections;
+
 import org.apache.hudi.avro.model.HoodieRollbackMetadata;
 import org.apache.hudi.client.embedded.EmbeddedTimelineService;
 import org.apache.hudi.common.HoodieRollbackStat;

File: hudi-client/src/main/java/org/apache/hudi/client/CompactionAdminClient.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi;
+package org.apache.hudi.client;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
@@ -40,7 +40,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
-import org.apache.hudi.func.OperationResult;
+import org.apache.hudi.table.compact.OperationResult;
 
 import com.google.common.base.Preconditions;
 import org.apache.hadoop.fs.FileStatus;

File: hudi-client/src/main/java/org/apache/hudi/client/HoodieCleanClient.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi;
+package org.apache.hudi.client;
 
 import org.apache.hudi.avro.model.HoodieCleanMetadata;
 import org.apache.hudi.avro.model.HoodieCleanerPlan;
@@ -78,7 +78,7 @@ public void clean() throws HoodieIOException {
    * @param startCleanTime Cleaner Instant Timestamp
    * @throws HoodieIOException in case of any IOException
    */
-  protected HoodieCleanMetadata clean(String startCleanTime) throws HoodieIOException {
+  public HoodieCleanMetadata clean(String startCleanTime) throws HoodieIOException {
     // Create a Hoodie table which encapsulated the commits and files visible
     final HoodieTable<T> table = HoodieTable.getHoodieTable(createMetaClient(true), config, jsc);
 
@@ -136,7 +136,7 @@ protected Option<HoodieCleanerPlan> scheduleClean(String startCleanTime) {
    * @param table Hoodie Table
    * @param cleanInstant Cleaner Instant
    */
-  protected HoodieCleanMetadata runClean(HoodieTable<T> table, HoodieInstant cleanInstant) {
+  public HoodieCleanMetadata runClean(HoodieTable<T> table, HoodieInstant cleanInstant) {
     try {
       HoodieCleanerPlan cleanerPlan = CleanerUtils.getCleanerPlan(table.getMetaClient(), cleanInstant);
       return runClean(table, cleanInstant, cleanerPlan);

File: hudi-client/src/main/java/org/apache/hudi/client/HoodieReadClient.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi;
+package org.apache.hudi.client;
 
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.client.embedded.EmbeddedTimelineService;

File: hudi-client/src/main/java/org/apache/hudi/client/WriteStatus.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi;
+package org.apache.hudi.client;
 
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;

File: hudi-client/src/main/java/org/apache/hudi/client/utils/LazyIterableIterator.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.func;
+package org.apache.hudi.client.utils;
 
 import java.util.Iterator;
 

File: hudi-client/src/main/java/org/apache/hudi/client/utils/ParquetReaderIterator.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.func;
+package org.apache.hudi.client.utils;
 
 import org.apache.hudi.common.util.queue.BoundedInMemoryQueue;
 import org.apache.hudi.exception.HoodieIOException;

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
Patch:
@@ -20,8 +20,8 @@
 
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
-import org.apache.hudi.io.compact.strategy.CompactionStrategy;
-import org.apache.hudi.io.compact.strategy.LogFileSizeBasedCompactionStrategy;
+import org.apache.hudi.table.compact.strategy.CompactionStrategy;
+import org.apache.hudi.table.compact.strategy.LogFileSizeBasedCompactionStrategy;
 
 import com.google.common.base.Preconditions;
 

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -18,15 +18,15 @@
 
 package org.apache.hudi.config;
 
-import org.apache.hudi.HoodieWriteClient;
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.HoodieWriteClient;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
 import org.apache.hudi.common.model.TimelineLayoutVersion;
 import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;
 import org.apache.hudi.common.util.ConsistencyGuardConfig;
 import org.apache.hudi.common.util.ReflectionUtils;
 import org.apache.hudi.index.HoodieIndex;
-import org.apache.hudi.io.compact.strategy.CompactionStrategy;
+import org.apache.hudi.table.compact.strategy.CompactionStrategy;
 import org.apache.hudi.metrics.MetricsReporterType;
 
 import org.apache.parquet.hadoop.metadata.CompressionCodecName;

File: hudi-client/src/main/java/org/apache/hudi/execution/BulkInsertMapFunction.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.func;
+package org.apache.hudi.execution;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-client/src/main/java/org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java
Patch:
@@ -16,9 +16,10 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.func;
+package org.apache.hudi.execution;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
+import org.apache.hudi.client.utils.LazyIterableIterator;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.Option;

File: hudi-client/src/main/java/org/apache/hudi/execution/MergeOnReadLazyInsertIterable.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.func;
+package org.apache.hudi.execution;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-client/src/main/java/org/apache/hudi/execution/SparkBoundedInMemoryExecutor.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.func;
+package org.apache.hudi.execution;
 
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.queue.BoundedInMemoryExecutor;

File: hudi-client/src/main/java/org/apache/hudi/index/HoodieIndex.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.index;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;

File: hudi-client/src/main/java/org/apache/hudi/index/InMemoryHashIndex.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.index;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.index.bloom;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndexCheckFunction.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIndexException;
-import org.apache.hudi.func.LazyIterableIterator;
+import org.apache.hudi.client.utils.LazyIterableIterator;
 import org.apache.hudi.io.HoodieKeyLookupHandle;
 import org.apache.hudi.io.HoodieKeyLookupHandle.KeyLookupResult;
 import org.apache.hudi.table.HoodieTable;

File: hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.index.hbase;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.io;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.FileSlice;
 import org.apache.hudi.common.model.HoodieDeltaWriteStat;
 import org.apache.hudi.common.model.HoodieKey;

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.io;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.io;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
 import org.apache.hudi.common.model.HoodieRecord;

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.io;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.FSUtils;

File: hudi-client/src/main/java/org/apache/hudi/table/HoodieCommitArchiveLog.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.io;
+package org.apache.hudi.table;
 
 import org.apache.hudi.avro.model.HoodieArchivedMetaEntry;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
@@ -45,7 +45,6 @@
 import org.apache.hudi.exception.HoodieCommitException;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
-import org.apache.hudi.table.HoodieTable;
 
 import com.fasterxml.jackson.databind.DeserializationFeature;
 import com.fasterxml.jackson.databind.ObjectMapper;

File: hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.table;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.avro.model.HoodieCleanerPlan;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.avro.model.HoodieSavepointMetadata;

File: hudi-client/src/main/java/org/apache/hudi/table/compact/HoodieCompactor.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.io.compact;
+package org.apache.hudi.table.compact;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-client/src/main/java/org/apache/hudi/table/compact/HoodieMergeOnReadTableCompactor.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.io.compact;
+package org.apache.hudi.table.compact;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.common.model.CompactionOperation;
@@ -37,7 +37,7 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
-import org.apache.hudi.io.compact.strategy.CompactionStrategy;
+import org.apache.hudi.table.compact.strategy.CompactionStrategy;
 import org.apache.hudi.table.HoodieCopyOnWriteTable;
 import org.apache.hudi.table.HoodieTable;
 

File: hudi-client/src/main/java/org/apache/hudi/table/compact/OperationResult.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.func;
+package org.apache.hudi.table.compact;
 
 import org.apache.hudi.common.util.Option;
 

File: hudi-client/src/main/java/org/apache/hudi/table/compact/strategy/BoundedIOCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.io.compact.strategy;
+package org.apache.hudi.table.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/main/java/org/apache/hudi/table/compact/strategy/BoundedPartitionAwareCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.io.compact.strategy;
+package org.apache.hudi.table.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/main/java/org/apache/hudi/table/compact/strategy/CompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.io.compact.strategy;
+package org.apache.hudi.table.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
@@ -26,7 +26,7 @@
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
-import org.apache.hudi.io.compact.HoodieMergeOnReadTableCompactor;
+import org.apache.hudi.table.compact.HoodieMergeOnReadTableCompactor;
 
 import java.io.Serializable;
 import java.util.HashMap;

File: hudi-client/src/main/java/org/apache/hudi/table/compact/strategy/DayBasedCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.io.compact.strategy;
+package org.apache.hudi.table.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/main/java/org/apache/hudi/table/compact/strategy/LogFileSizeBasedCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.io.compact.strategy;
+package org.apache.hudi.table.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/main/java/org/apache/hudi/table/compact/strategy/UnBoundedCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.io.compact.strategy;
+package org.apache.hudi.table.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/main/java/org/apache/hudi/table/compact/strategy/UnBoundedPartitionAwareCompactionStrategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.io.compact.strategy;
+package org.apache.hudi.table.compact.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;

File: hudi-client/src/test/java/HoodieClientExample.java
Patch:
@@ -16,8 +16,8 @@
  * limitations under the License.
  */
 
-import org.apache.hudi.HoodieWriteClient;
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.HoodieWriteClient;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.HoodieClientTestUtils;
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.model.HoodieAvroPayload;

File: hudi-client/src/test/java/org/apache/hudi/client/TestClientRollback.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi;
+package org.apache.hudi.client;
 
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.model.HoodieCleaningPolicy;

File: hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi;
+package org.apache.hudi.client;
 
 import org.apache.hudi.common.HoodieClientTestUtils;
 import org.apache.hudi.common.HoodieTestDataGenerator;
@@ -408,7 +408,7 @@ public void testUpsertToDiffPartitionGlobalIndex() throws Exception {
     assertNoWriteErrors(statuses);
 
     // check the partition metadata is written out
-    assertPartitionMetadata(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS, fs);
+    assertPartitionMetadataForRecords(inserts1, fs);
     String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];
     for (int i = 0; i < fullPartitionPaths.length; i++) {
       fullPartitionPaths[i] = String.format("%s/%s/*", basePath, dataGen.getPartitionPaths()[i]);
@@ -430,7 +430,7 @@ public void testUpsertToDiffPartitionGlobalIndex() throws Exception {
     assertNoWriteErrors(statuses1);
 
     // check the partition metadata is written out
-    assertPartitionMetadata(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS, fs);
+    assertPartitionMetadataForRecords(updates1, fs);
     // Check the entire dataset has all records still
     fullPartitionPaths = new String[dataGen.getPartitionPaths().length];
     for (int i = 0; i < fullPartitionPaths.length; i++) {

File: hudi-client/src/test/java/org/apache/hudi/client/TestHoodieReadClient.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi;
+package org.apache.hudi.client;
 
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;

File: hudi-client/src/test/java/org/apache/hudi/client/TestMultiFS.java
Patch:
@@ -16,8 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hudi;
+package org.apache.hudi.client;
 
+import org.apache.hudi.common.HoodieClientTestHarness;
 import org.apache.hudi.common.HoodieClientTestUtils;
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.model.HoodieAvroPayload;

File: hudi-client/src/test/java/org/apache/hudi/client/TestWriteStatus.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi;
+package org.apache.hudi.client;
 
 import org.apache.hudi.common.model.HoodieRecord;
 

File: hudi-client/src/test/java/org/apache/hudi/client/utils/TestParquetReaderIterator.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.hudi.func;
+package org.apache.hudi.client.utils;
 
 import org.apache.hudi.exception.HoodieIOException;
 

File: hudi-client/src/test/java/org/apache/hudi/common/HoodieClientTestUtils.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.common;
 
-import org.apache.hudi.HoodieReadClient;
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.HoodieReadClient;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.avro.HoodieAvroWriteSupport;
 import org.apache.hudi.common.bloom.filter.BloomFilter;
 import org.apache.hudi.common.bloom.filter.BloomFilterFactory;

File: hudi-client/src/test/java/org/apache/hudi/common/TestRawTripPayload.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common;
 
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.avro.MercifulJsonConverter;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;

File: hudi-client/src/test/java/org/apache/hudi/index/TestHBaseQPSResourceAllocator.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.index;
 
-import org.apache.hudi.HoodieClientTestHarness;
+import org.apache.hudi.common.HoodieClientTestHarness;
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieCompactionConfig;

File: hudi-client/src/test/java/org/apache/hudi/index/TestHbaseIndex.java
Patch:
@@ -18,9 +18,9 @@
 
 package org.apache.hudi.index;
 
-import org.apache.hudi.HoodieClientTestHarness;
-import org.apache.hudi.HoodieWriteClient;
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.common.HoodieClientTestHarness;
+import org.apache.hudi.client.HoodieWriteClient;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieWriteStat;

File: hudi-client/src/test/java/org/apache/hudi/index/TestHoodieIndex.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.index;
 
-import org.apache.hudi.HoodieClientTestHarness;
+import org.apache.hudi.common.HoodieClientTestHarness;
 import org.apache.hudi.config.HoodieHBaseIndexConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieBloomIndex.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.index.bloom;
 
-import org.apache.hudi.HoodieClientTestHarness;
+import org.apache.hudi.common.HoodieClientTestHarness;
 import org.apache.hudi.common.HoodieClientTestUtils;
 import org.apache.hudi.common.TestRawTripPayload;
 import org.apache.hudi.common.bloom.filter.BloomFilter;

File: hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieGlobalBloomIndex.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.index.bloom;
 
-import org.apache.hudi.HoodieClientTestHarness;
+import org.apache.hudi.common.HoodieClientTestHarness;
 import org.apache.hudi.common.HoodieClientTestUtils;
 import org.apache.hudi.common.TestRawTripPayload;
 import org.apache.hudi.common.model.EmptyHoodieRecordPayload;

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieCommitArchiveLog.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.io;
 
-import org.apache.hudi.HoodieClientTestHarness;
+import org.apache.hudi.common.HoodieClientTestHarness;
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -32,6 +32,7 @@
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
+import org.apache.hudi.table.HoodieCommitArchiveLog;
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieMergeHandle.java
Patch:
@@ -18,9 +18,9 @@
 
 package org.apache.hudi.io;
 
-import org.apache.hudi.HoodieClientTestHarness;
-import org.apache.hudi.HoodieWriteClient;
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.common.HoodieClientTestHarness;
+import org.apache.hudi.client.HoodieWriteClient;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.HoodieClientTestUtils;
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.model.HoodieRecord;

File: hudi-client/src/test/java/org/apache/hudi/io/storage/TestHoodieStorageWriterFactory.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.io.storage;
 
-import org.apache.hudi.TestHoodieClientBase;
+import org.apache.hudi.client.TestHoodieClientBase;
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;

File: hudi-client/src/test/java/org/apache/hudi/table/TestConsistencyGuard.java
Patch:
@@ -16,8 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.hudi;
+package org.apache.hudi.table;
 
+import org.apache.hudi.common.HoodieClientTestHarness;
 import org.apache.hudi.common.HoodieClientTestUtils;
 import org.apache.hudi.common.util.ConsistencyGuard;
 import org.apache.hudi.common.util.ConsistencyGuardConfig;

File: hudi-client/src/test/java/org/apache/hudi/table/TestCopyOnWriteTable.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.table;
 
-import org.apache.hudi.HoodieClientTestHarness;
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.common.HoodieClientTestHarness;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.HoodieClientTestUtils;
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.TestRawTripPayload;

File: hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -18,6 +18,9 @@
 
 package org.apache.hudi;
 
+import org.apache.hudi.client.HoodieReadClient;
+import org.apache.hudi.client.HoodieWriteClient;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.client.embedded.EmbeddedTimelineService;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.utilities;
 
-import org.apache.hudi.HoodieWriteClient;
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.HoodieWriteClient;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.HoodieJsonPayload;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.utilities;
 
-import org.apache.hudi.HoodieWriteClient;
+import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.TypedProperties;
 import org.apache.hudi.config.HoodieWriteConfig;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java
Patch:
@@ -18,9 +18,9 @@
 
 package org.apache.hudi.utilities;
 
-import org.apache.hudi.CompactionAdminClient;
-import org.apache.hudi.CompactionAdminClient.RenameOpResult;
-import org.apache.hudi.CompactionAdminClient.ValidationOpResult;
+import org.apache.hudi.client.CompactionAdminClient;
+import org.apache.hudi.client.CompactionAdminClient.RenameOpResult;
+import org.apache.hudi.client.CompactionAdminClient.ValidationOpResult;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.FSUtils;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.utilities;
 
-import org.apache.hudi.HoodieWriteClient;
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.HoodieWriteClient;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.TypedProperties;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
Patch:
@@ -20,8 +20,8 @@
 
 import org.apache.avro.Schema;
 import org.apache.hudi.AvroConversionUtils;
-import org.apache.hudi.HoodieWriteClient;
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.HoodieWriteClient;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.util.DFSPropertiesConfiguration;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ReflectionUtils;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/Compactor.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.utilities.deltastreamer;
 
-import org.apache.hudi.HoodieWriteClient;
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.HoodieWriteClient;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieException;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -20,9 +20,9 @@
 
 import org.apache.hudi.AvroConversionUtils;
 import org.apache.hudi.DataSourceUtils;
-import org.apache.hudi.HoodieWriteClient;
+import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.keygen.KeyGenerator;
-import org.apache.hudi.WriteStatus;
+import org.apache.hudi.client.WriteStatus;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.utilities.deltastreamer;
 
-import org.apache.hudi.HoodieWriteClient;
+import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
 import org.apache.hudi.common.table.HoodieTableMetaClient;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.utilities;
 
-import org.apache.hudi.HoodieReadClient;
-import org.apache.hudi.HoodieWriteClient;
+import org.apache.hudi.client.HoodieReadClient;
+import org.apache.hudi.client.HoodieWriteClient;
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.minicluster.HdfsTestService;
 import org.apache.hudi.common.model.HoodieTestUtils;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/keygen/TimestampBasedKeyGenerator.java
Patch:
@@ -103,7 +103,7 @@ public HoodieKey getKey(GenericRecord record) {
         unixTime = ((Float) partitionVal).longValue();
       } else if (partitionVal instanceof Long) {
         unixTime = (Long) partitionVal;
-      } else if (partitionVal instanceof String) {
+      } else if (partitionVal instanceof CharSequence) {
         unixTime = inputDateFormat.parse(partitionVal.toString()).getTime() / 1000;
       } else {
         throw new HoodieNotSupportedException(

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -485,7 +485,7 @@ public TypedProperties getProps() {
   }
 
   /**
-   * Async Compactor Service tha runs in separate thread. Currently, only one compactor is allowed to run at any time.
+   * Async Compactor Service that runs in separate thread. Currently, only one compactor is allowed to run at any time.
    */
   public static class AsyncCompactService extends AbstractDeltaStreamerService {
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/AbstractDeltaStreamerService.java
Patch:
@@ -32,7 +32,7 @@
 import java.util.function.Function;
 
 /**
- * Base Class for running delta-sync/compaction in separate thread and controlling their life-cyle.
+ * Base Class for running delta-sync/compaction in separate thread and controlling their life-cycle.
  */
 public abstract class AbstractDeltaStreamerService implements Serializable {
 

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CommitsCommand.java
Patch:
@@ -268,11 +268,11 @@ public String showCommitPartitions(@CliOption(key = {"commit"}, help = "Commit t
       for (HoodieWriteStat stat : stats) {
         if (stat.getPrevCommit().equals(HoodieWriteStat.NULL_COMMIT)) {
           totalFilesAdded += 1;
-          totalRecordsInserted += stat.getNumWrites();
         } else {
           totalFilesUpdated += 1;
           totalRecordsUpdated += stat.getNumUpdateWrites();
         }
+        totalRecordsInserted += stat.getNumInserts();
         totalBytesWritten += stat.getTotalWriteBytes();
         totalWriteErrors += stat.getTotalWriteErrors();
       }

File: hudi-client/src/test/java/org/apache/hudi/index/TestHBaseQPSResourceAllocator.java
Patch:
@@ -116,7 +116,7 @@ private HoodieWriteConfig.Builder getConfigBuilder(HoodieHBaseIndexConfig hoodie
 
   private HoodieHBaseIndexConfig getConfigWithResourceAllocator(Option<String> resourceAllocatorClass) {
     HoodieHBaseIndexConfig.Builder builder = new HoodieHBaseIndexConfig.Builder()
-        .hbaseZkPort(Integer.valueOf(hbaseConfig.get("hbase.zookeeper.property.clientPort")))
+        .hbaseZkPort(Integer.parseInt(hbaseConfig.get("hbase.zookeeper.property.clientPort")))
         .hbaseZkQuorum(hbaseConfig.get("hbase.zookeeper.quorum")).hbaseTableName(tableName).hbaseIndexGetBatchSize(100);
     if (resourceAllocatorClass.isPresent()) {
       builder.withQPSResourceAllocatorType(resourceAllocatorClass.get());

File: hudi-client/src/test/java/org/apache/hudi/index/TestHbaseIndex.java
Patch:
@@ -418,7 +418,7 @@ private HoodieWriteConfig.Builder getConfigBuilder() {
         .forTable("test-trip-table")
         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.HBASE)
             .withHBaseIndexConfig(new HoodieHBaseIndexConfig.Builder()
-                .hbaseZkPort(Integer.valueOf(hbaseConfig.get("hbase.zookeeper.property.clientPort")))
+                .hbaseZkPort(Integer.parseInt(hbaseConfig.get("hbase.zookeeper.property.clientPort")))
                 .hbaseZkQuorum(hbaseConfig.get("hbase.zookeeper.quorum")).hbaseTableName(tableName)
                 .hbaseIndexGetBatchSize(100).build())
             .build());

File: hudi-client/src/main/java/org/apache/hudi/client/utils/ClientUtils.java
Patch:
@@ -37,6 +37,7 @@ public class ClientUtils {
   public static HoodieTableMetaClient createMetaClient(JavaSparkContext jsc, HoodieWriteConfig config,
       boolean loadActiveTimelineOnLoad) {
     return new HoodieTableMetaClient(jsc.hadoopConfiguration(), config.getBasePath(), loadActiveTimelineOnLoad,
-        config.getConsistencyGuardConfig(), Option.of(new TimelineLayoutVersion(config.getTimelineLayoutVersion())));
+        config.getConsistencyGuardConfig(),
+        Option.of(new TimelineLayoutVersion(config.getTimelineLayoutVersion())));
   }
 }

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
Patch:
@@ -757,6 +757,7 @@ public HoodieWriteConfig build() {
       // Ensure Layout Version is good
       new TimelineLayoutVersion(Integer.parseInt(layoutVersion));
 
+
       // Build WriteConfig at the end
       HoodieWriteConfig config = new HoodieWriteConfig(props);
       Preconditions.checkArgument(config.getBasePath() != null);

File: hudi-client/src/test/java/org/apache/hudi/TestHoodieClientBase.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
 import org.apache.hudi.common.model.HoodieRecord;
+import org.apache.hudi.common.model.TimelineLayoutVersion;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.HoodieTimeline;
 import org.apache.hudi.common.table.SyncableFileSystemView;
@@ -145,6 +146,7 @@ HoodieWriteConfig.Builder getConfigBuilder(String schemaStr) {
   HoodieWriteConfig.Builder getConfigBuilder(String schemaStr, IndexType indexType) {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(schemaStr)
         .withParallelism(2, 2).withBulkInsertParallelism(2).withFinalizeWriteParallelism(2)
+        .withTimelineLayoutVersion(TimelineLayoutVersion.CURR_VERSION)
         .withWriteStatusClass(MetadataMergeWriteStatus.class)
         .withConsistencyGuardConfig(ConsistencyGuardConfig.newBuilder().withConsistencyCheckEnabled(true).build())
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024).build())

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java
Patch:
@@ -107,13 +107,13 @@ public String compactionsAll(
         try {
           // This could be a completed compaction. Assume a compaction request file is present but skip if fails
           compactionPlan = AvroUtils.deserializeCompactionPlan(
-              activeTimeline.readPlanAsBytes(
+              activeTimeline.readCompactionPlanAsBytes(
                   HoodieTimeline.getCompactionRequestedInstant(instant.getTimestamp())).get());
         } catch (HoodieIOException ioe) {
           // SKIP
         }
       } else {
-        compactionPlan = AvroUtils.deserializeCompactionPlan(activeTimeline.readPlanAsBytes(
+        compactionPlan = AvroUtils.deserializeCompactionPlan(activeTimeline.readCompactionPlanAsBytes(
             HoodieTimeline.getCompactionRequestedInstant(instant.getTimestamp())).get());
       }
 
@@ -156,7 +156,7 @@ public String compactionShow(
     HoodieTableMetaClient client = checkAndGetMetaClient();
     HoodieActiveTimeline activeTimeline = client.getActiveTimeline();
     HoodieCompactionPlan compactionPlan = AvroUtils.deserializeCompactionPlan(
-        activeTimeline.readPlanAsBytes(
+        activeTimeline.readCompactionPlanAsBytes(
             HoodieTimeline.getCompactionRequestedInstant(compactionInstantTime)).get());
 
     List<Comparable[]> rows = new ArrayList<>();

File: hudi-client/src/main/java/org/apache/hudi/CompactionAdminClient.java
Patch:
@@ -220,7 +220,7 @@ public List<RenameOpResult> repairCompaction(String compactionInstant, int paral
   private static HoodieCompactionPlan getCompactionPlan(HoodieTableMetaClient metaClient, String compactionInstant)
       throws IOException {
     return AvroUtils.deserializeCompactionPlan(
-            metaClient.getActiveTimeline().readPlanAsBytes(
+            metaClient.getActiveTimeline().readCompactionPlanAsBytes(
                     HoodieTimeline.getCompactionRequestedInstant(compactionInstant)).get());
   }
 

File: hudi-client/src/main/java/org/apache/hudi/HoodieWriteClient.java
Patch:
@@ -953,7 +953,7 @@ public void commitCompaction(String compactionInstantTime, JavaRDD<WriteStatus>
     HoodieTable<T> table = HoodieTable.getHoodieTable(metaClient, config, jsc);
     HoodieActiveTimeline timeline = metaClient.getActiveTimeline();
     HoodieCompactionPlan compactionPlan = AvroUtils.deserializeCompactionPlan(
-        timeline.readPlanAsBytes(HoodieTimeline.getCompactionRequestedInstant(compactionInstantTime)).get());
+        timeline.readCompactionPlanAsBytes(HoodieTimeline.getCompactionRequestedInstant(compactionInstantTime)).get());
     // Merge extra meta-data passed by user with the one already in inflight compaction
     Option<Map<String, String>> mergedMetaData = extraMetadata.map(m -> {
       Map<String, String> merged = new HashMap<>();

File: hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java
Patch:
@@ -72,7 +72,7 @@ public static HoodieCleanMetadata getCleanerMetadata(HoodieTableMetaClient metaC
       throws IOException {
     CleanMetadataMigrator metadataMigrator = new CleanMetadataMigrator(metaClient);
     HoodieCleanMetadata cleanMetadata = AvroUtils.deserializeHoodieCleanMetadata(
-        metaClient.getActiveTimeline().readPlanAsBytes(cleanInstant).get());
+        metaClient.getActiveTimeline().readCleanerInfoAsBytes(cleanInstant).get());
     return metadataMigrator.upgradeToLatest(cleanMetadata, cleanMetadata.getVersion());
   }
 
@@ -85,7 +85,7 @@ public static HoodieCleanMetadata getCleanerMetadata(HoodieTableMetaClient metaC
    */
   public static HoodieCleanerPlan getCleanerPlan(HoodieTableMetaClient metaClient, HoodieInstant cleanInstant)
       throws IOException {
-    return AvroUtils.deserializeAvroMetadata(metaClient.getActiveTimeline().readPlanAsBytes(cleanInstant).get(),
+    return AvroUtils.deserializeAvroMetadata(metaClient.getActiveTimeline().readCleanerInfoAsBytes(cleanInstant).get(),
         HoodieCleanerPlan.class);
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/common/util/CompactionUtils.java
Patch:
@@ -140,7 +140,7 @@ public static HoodieCompactionPlan getCompactionPlan(HoodieTableMetaClient metaC
       throws IOException {
     CompactionPlanMigrator migrator = new CompactionPlanMigrator(metaClient);
     HoodieCompactionPlan compactionPlan = AvroUtils.deserializeCompactionPlan(
-        metaClient.getActiveTimeline().readPlanAsBytes(
+        metaClient.getActiveTimeline().readCompactionPlanAsBytes(
             HoodieTimeline.getCompactionRequestedInstant(compactionInstant)).get());
     return migrator.upgradeToLatest(compactionPlan, compactionPlan.getVersion());
   }

File: hudi-client/src/main/java/org/apache/hudi/CompactionAdminClient.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.hudi.client.embedded.EmbeddedTimelineService;
 import org.apache.hudi.common.model.CompactionOperation;
 import org.apache.hudi.common.model.FileSlice;
-import org.apache.hudi.common.model.HoodieDataFile;
+import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieFileGroupId;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
@@ -293,7 +293,7 @@ private ValidationOpResult validateCompactionOperation(HoodieTableMetaClient met
                 .filter(fs -> fs.getFileId().equals(operation.getFileId())).findFirst());
         if (fileSliceOptional.isPresent()) {
           FileSlice fs = fileSliceOptional.get();
-          Option<HoodieDataFile> df = fs.getDataFile();
+          Option<HoodieBaseFile> df = fs.getBaseFile();
           if (operation.getDataFileName().isPresent()) {
             String expPath = metaClient.getFs()
                 .getFileStatus(
@@ -448,7 +448,7 @@ public List<Pair<HoodieLogFile, HoodieLogFile>> getRenamingActionsForUnschedulin
         .orElse(HoodieLogFile.LOGFILE_BASE_VERSION - 1);
     String logExtn = fileSliceForCompaction.getLogFiles().findFirst().map(lf -> "." + lf.getFileExtension())
         .orElse(HoodieLogFile.DELTA_EXTENSION);
-    String parentPath = fileSliceForCompaction.getDataFile().map(df -> new Path(df.getPath()).getParent().toString())
+    String parentPath = fileSliceForCompaction.getBaseFile().map(df -> new Path(df.getPath()).getParent().toString())
         .orElse(fileSliceForCompaction.getLogFiles().findFirst().map(lf -> lf.getPath().getParent().toString()).get());
     for (HoodieLogFile toRepair : logFilesToRepair) {
       int version = maxUsedVersion + 1;

File: hudi-client/src/main/java/org/apache/hudi/HoodieReadClient.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.client.embedded.EmbeddedTimelineService;
-import org.apache.hudi.common.model.HoodieDataFile;
+import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;
@@ -136,8 +136,8 @@ private void assertSqlContext() {
 
   private Option<String> convertToDataFilePath(Option<Pair<String, String>> partitionPathFileIDPair) {
     if (partitionPathFileIDPair.isPresent()) {
-      HoodieDataFile dataFile = hoodieTable.getROFileSystemView()
-          .getLatestDataFile(partitionPathFileIDPair.get().getLeft(), partitionPathFileIDPair.get().getRight()).get();
+      HoodieBaseFile dataFile = hoodieTable.getBaseFileOnlyView()
+          .getLatestBaseFile(partitionPathFileIDPair.get().getLeft(), partitionPathFileIDPair.get().getRight()).get();
       return Option.of(dataFile.getPath());
     } else {
       return Option.empty();

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java
Patch:
@@ -245,8 +245,8 @@ List<Tuple2<String, BloomIndexFileInfo>> loadInvolvedFiles(List<String> partitio
               hoodieTable.getMetaClient().getCommitsTimeline().filterCompletedInstants().lastInstant();
           List<Pair<String, String>> filteredFiles = new ArrayList<>();
           if (latestCommitTime.isPresent()) {
-            filteredFiles = hoodieTable.getROFileSystemView()
-                .getLatestDataFilesBeforeOrOn(partitionPath, latestCommitTime.get().getTimestamp())
+            filteredFiles = hoodieTable.getBaseFileOnlyView()
+                .getLatestBaseFilesBeforeOrOn(partitionPath, latestCommitTime.get().getTimestamp())
                 .map(f -> Pair.of(partitionPath, f.getFileId())).collect(toList());
           }
           return filteredFiles.iterator();

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -29,7 +29,7 @@
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieWriteStat;
 import org.apache.hudi.common.model.HoodieWriteStat.RuntimeStats;
-import org.apache.hudi.common.table.TableFileSystemView.RealtimeView;
+import org.apache.hudi.common.table.TableFileSystemView.SliceView;
 import org.apache.hudi.common.table.log.HoodieLogFormat;
 import org.apache.hudi.common.table.log.HoodieLogFormat.Writer;
 import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;
@@ -117,7 +117,7 @@ private void init(HoodieRecord record) {
     if (doInit) {
       this.partitionPath = record.getPartitionPath();
       // extract some information from the first record
-      RealtimeView rtView = hoodieTable.getRTFileSystemView();
+      SliceView rtView = hoodieTable.getSliceView();
       Option<FileSlice> fileSlice = rtView.getLatestFileSlice(partitionPath, fileId);
       // Set the base commit time as the current commitTime for new inserts into log files
       String baseInstantTime = instantTime;

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieKeyLookupHandle.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.io;
 
 import org.apache.hudi.common.bloom.filter.BloomFilter;
-import org.apache.hudi.common.model.HoodieDataFile;
+import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.util.HoodieTimer;
@@ -113,7 +113,7 @@ public KeyLookupResult getLookupResult() {
       LOG.debug("#The candidate row keys for " + partitionPathFilePair + " => " + candidateRecordKeys);
     }
 
-    HoodieDataFile dataFile = getLatestDataFile();
+    HoodieBaseFile dataFile = getLatestDataFile();
     List<String> matchingKeys =
         checkCandidatesAgainstFile(hoodieTable.getHadoopConf(), candidateRecordKeys, new Path(dataFile.getPath()));
     LOG.info(

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieRangeInfoHandle.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.io;
 
-import org.apache.hudi.common.model.HoodieDataFile;
+import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieRecordPayload;
 import org.apache.hudi.common.util.ParquetUtils;
 import org.apache.hudi.common.util.collection.Pair;
@@ -38,7 +38,7 @@ public HoodieRangeInfoHandle(HoodieWriteConfig config, HoodieTable<T> hoodieTabl
   }
 
   public String[] getMinMaxKeys() {
-    HoodieDataFile dataFile = getLatestDataFile();
+    HoodieBaseFile dataFile = getLatestDataFile();
     return ParquetUtils.readMinMaxRecordKeys(hoodieTable.getHadoopConf(), new Path(dataFile.getPath()));
   }
 }

File: hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/LogFileSizeBasedCompactionStrategy.java
Patch:
@@ -20,7 +20,7 @@
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
-import org.apache.hudi.common.model.HoodieDataFile;
+import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -43,7 +43,7 @@ public class LogFileSizeBasedCompactionStrategy extends BoundedIOCompactionStrat
   private static final String TOTAL_LOG_FILE_SIZE = "TOTAL_LOG_FILE_SIZE";
 
   @Override
-  public Map<String, Double> captureMetrics(HoodieWriteConfig config, Option<HoodieDataFile> dataFile,
+  public Map<String, Double> captureMetrics(HoodieWriteConfig config, Option<HoodieBaseFile> dataFile,
       String partitionPath, List<HoodieLogFile> logFiles) {
     Map<String, Double> metrics = super.captureMetrics(config, dataFile, partitionPath, logFiles);
 

File: hudi-client/src/test/java/org/apache/hudi/TestCompactionAdminClient.java
Patch:
@@ -275,7 +275,7 @@ private List<Pair<HoodieLogFile, HoodieLogFile>> validateUnSchedulePlan(Compacti
     // Expect all file-slice whose base-commit is same as compaction commit to contain no new Log files
     newFsView.getLatestFileSlicesBeforeOrOn(HoodieTestUtils.DEFAULT_PARTITION_PATHS[0], compactionInstant, true)
         .filter(fs -> fs.getBaseInstantTime().equals(compactionInstant)).forEach(fs -> {
-          Assert.assertFalse("No Data file must be present", fs.getDataFile().isPresent());
+          Assert.assertFalse("No Data file must be present", fs.getBaseFile().isPresent());
           Assert.assertEquals("No Log Files", 0, fs.getLogFiles().count());
         });
 
@@ -336,7 +336,7 @@ private void validateUnScheduleFileId(CompactionAdminClient client, String inges
     newFsView.getLatestFileSlicesBeforeOrOn(HoodieTestUtils.DEFAULT_PARTITION_PATHS[0], compactionInstant, true)
         .filter(fs -> fs.getBaseInstantTime().equals(compactionInstant))
         .filter(fs -> fs.getFileId().equals(op.getFileId())).forEach(fs -> {
-          Assert.assertFalse("No Data file must be present", fs.getDataFile().isPresent());
+          Assert.assertFalse("No Data file must be present", fs.getBaseFile().isPresent());
           Assert.assertEquals("No Log Files", 0, fs.getLogFiles().count());
         });
 

File: hudi-client/src/test/java/org/apache/hudi/TestHoodieClientBase.java
Patch:
@@ -157,7 +157,7 @@ HoodieWriteConfig.Builder getConfigBuilder(String schemaStr, IndexType indexType
 
   protected HoodieTable getHoodieTable(HoodieTableMetaClient metaClient, HoodieWriteConfig config) {
     HoodieTable table = HoodieTable.getHoodieTable(metaClient, config, jsc);
-    ((SyncableFileSystemView) (table.getRTFileSystemView())).reset();
+    ((SyncableFileSystemView) (table.getSliceView())).reset();
     return table;
   }
 

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieCompactor.java
Patch:
@@ -157,7 +157,7 @@ public void testWriteStatusContentsAfterCompaction() throws Exception {
       table = HoodieTable.getHoodieTable(metaClient, config, jsc);
       for (String partitionPath : dataGen.getPartitionPaths()) {
         List<FileSlice> groupedLogFiles =
-            table.getRTFileSystemView().getLatestFileSlices(partitionPath).collect(Collectors.toList());
+            table.getSliceView().getLatestFileSlices(partitionPath).collect(Collectors.toList());
         for (FileSlice fileSlice : groupedLogFiles) {
           assertEquals("There should be 1 log file written for every data file", 1, fileSlice.getLogFiles().count());
         }
@@ -185,7 +185,7 @@ protected HoodieTableType getTableType() {
     return HoodieTableType.MERGE_ON_READ;
   }
 
-  // TODO - after modifying HoodieReadClient to support realtime tables - add more tests to make
+  // TODO - after modifying HoodieReadClient to support mor tables - add more tests to make
   // sure the data read is the updated data (compaction correctness)
   // TODO - add more test cases for compactions after a failed commit/compaction
 }

File: hudi-client/src/test/java/org/apache/hudi/io/strategy/TestHoodieCompactionStrategy.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.hudi.io.strategy;
 
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
-import org.apache.hudi.common.model.HoodieDataFile;
+import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodieLogFile;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
@@ -239,7 +239,7 @@ private List<HoodieCompactionOperation> createCompactionOperations(HoodieWriteCo
     List<HoodieCompactionOperation> operations = new ArrayList<>(sizesMap.size());
 
     sizesMap.forEach((k, v) -> {
-      HoodieDataFile df = TestHoodieDataFile.newDataFile(k);
+      HoodieBaseFile df = TestHoodieBaseFile.newDataFile(k);
       String partitionPath = keyToPartitionMap.get(k);
       List<HoodieLogFile> logFiles = v.stream().map(TestHoodieLogFile::newLogFile).collect(Collectors.toList());
       operations.add(new HoodieCompactionOperation(df.getCommitTime(),

File: hudi-common/src/main/java/org/apache/hudi/common/model/CompactionOperation.java
Patch:
@@ -60,7 +60,7 @@ public CompactionOperation(String fileId, String partitionPath, String baseInsta
     this.metrics = metrics;
   }
 
-  public CompactionOperation(Option<HoodieDataFile> dataFile, String partitionPath, List<HoodieLogFile> logFiles,
+  public CompactionOperation(Option<HoodieBaseFile> dataFile, String partitionPath, List<HoodieLogFile> logFiles,
       Map<String, Double> metrics) {
     if (dataFile.isPresent()) {
       this.baseInstantTime = dataFile.get().getCommitTime();
@@ -111,9 +111,9 @@ public HoodieFileGroupId getFileGroupId() {
     return id;
   }
 
-  public Option<HoodieDataFile> getBaseFile(String basePath, String partitionPath) {
+  public Option<HoodieBaseFile> getBaseFile(String basePath, String partitionPath) {
     Path dirPath = FSUtils.getPartitionPath(basePath, partitionPath);
-    return dataFileName.map(df -> new HoodieDataFile(new Path(dirPath, df).toString()));
+    return dataFileName.map(df -> new HoodieBaseFile(new Path(dirPath, df).toString()));
   }
 
   /**

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieTableType.java
Patch:
@@ -27,9 +27,6 @@
  * <p>
  * MERGE_ON_READ - Speeds up upserts, by delaying merge until enough work piles up.
  * <p>
- * In the future, following might be added.
- * <p>
- * SIMPLE_LSM - A simple 2 level LSM tree.
  */
 public enum HoodieTableType {
   COPY_ON_WRITE, MERGE_ON_READ

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -299,7 +299,7 @@ public static HoodieTableMetaClient initTableType(Configuration hadoopConf, Stri
   }
 
   /**
-   * Helper method to initialize a given path, as a given storage type and table name.
+   * Helper method to initialize a given path, as a given type and table name.
    */
   public static HoodieTableMetaClient initTableType(Configuration hadoopConf, String basePath,
       HoodieTableType tableType, String tableName, String payloadClassName) throws IOException {
@@ -437,7 +437,7 @@ public String getCommitActionType() {
       case MERGE_ON_READ:
         return HoodieActiveTimeline.DELTA_COMMIT_ACTION;
       default:
-        throw new HoodieException("Could not commit on unknown storage type " + this.getTableType());
+        throw new HoodieException("Could not commit on unknown table type " + this.getTableType());
     }
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/util/CompactionUtils.java
Patch:
@@ -69,8 +69,8 @@ public static HoodieCompactionOperation buildFromFileSlice(String partitionPath,
     builder.setFileId(fileSlice.getFileId());
     builder.setBaseInstantTime(fileSlice.getBaseInstantTime());
     builder.setDeltaFilePaths(fileSlice.getLogFiles().map(lf -> lf.getPath().getName()).collect(Collectors.toList()));
-    if (fileSlice.getDataFile().isPresent()) {
-      builder.setDataFilePath(fileSlice.getDataFile().get().getFileName());
+    if (fileSlice.getBaseFile().isPresent()) {
+      builder.setDataFilePath(fileSlice.getBaseFile().get().getFileName());
     }
 
     if (metricsCaptureFunction.isPresent()) {

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieROTablePathFilter.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.hadoop;
 
-import org.apache.hudi.common.model.HoodieDataFile;
+import org.apache.hudi.common.model.HoodieBaseFile;
 import org.apache.hudi.common.model.HoodiePartitionMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.view.HoodieTableFileSystemView;
@@ -139,14 +139,14 @@ public boolean accept(Path path) {
           HoodieTableMetaClient metaClient = new HoodieTableMetaClient(fs.getConf(), baseDir.toString());
           HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,
               metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants(), fs.listStatus(folder));
-          List<HoodieDataFile> latestFiles = fsView.getLatestDataFiles().collect(Collectors.toList());
+          List<HoodieBaseFile> latestFiles = fsView.getLatestBaseFiles().collect(Collectors.toList());
           // populate the cache
           if (!hoodiePathCache.containsKey(folder.toString())) {
             hoodiePathCache.put(folder.toString(), new HashSet<>());
           }
           LOG.info("Based on hoodie metadata from base path: " + baseDir.toString() + ", caching " + latestFiles.size()
               + " files under " + folder);
-          for (HoodieDataFile lfile : latestFiles) {
+          for (HoodieBaseFile lfile : latestFiles) {
             hoodiePathCache.get(folder.toString()).add(new Path(lfile.getPath()));
           }
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java
Patch:
@@ -336,7 +336,7 @@ private static Schema addPartitionFields(Schema schema, List<String> partitionin
    */
   private void init() throws IOException {
     Schema schemaFromLogFile =
-        LogReaderUtils.readLatestSchemaFromLogFiles(split.getBasePath(), split.getDeltaFilePaths(), jobConf);
+        LogReaderUtils.readLatestSchemaFromLogFiles(split.getBasePath(), split.getDeltaLogPaths(), jobConf);
     if (schemaFromLogFile == null) {
       writerSchema = new AvroSchemaConverter().convert(baseFileSchema);
       LOG.debug("Writer Schema From Parquet => " + writerSchema.getFields());
@@ -360,7 +360,7 @@ private void init() throws IOException {
 
     readerSchema = generateProjectionSchema(writerSchema, schemaFieldsMap, projectionFields);
     LOG.info(String.format("About to read compacted logs %s for base split %s, projecting cols %s",
-        split.getDeltaFilePaths(), split.getPath(), projectionFields));
+        split.getDeltaLogPaths(), split.getPath(), projectionFields));
   }
 
   private Schema constructHiveOrderedSchema(Schema writerSchema, Map<String, Field> schemaFieldsMap) {

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java
Patch:
@@ -61,7 +61,7 @@ private HoodieMergedLogRecordScanner getMergedLogRecordScanner() throws IOExcept
     // but can return records for completed commits > the commit we are trying to read (if using
     // readCommit() API)
     return new HoodieMergedLogRecordScanner(FSUtils.getFs(split.getPath().toString(), jobConf), split.getBasePath(),
-        split.getDeltaFilePaths(), usesCustomPayload ? getWriterSchema() : getReaderSchema(), split.getMaxCommitTime(),
+        split.getDeltaLogPaths(), usesCustomPayload ? getWriterSchema() : getReaderSchema(), split.getMaxCommitTime(),
         getMaxCompactionMemoryInBytes(),
         Boolean
             .valueOf(jobConf.get(COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP, DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED)),

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java
Patch:
@@ -77,7 +77,7 @@ public RealtimeUnmergedRecordReader(HoodieRealtimeFileSplit split, JobConf job,
     // Consumer of this record reader
     this.iterator = this.executor.getQueue().iterator();
     this.logRecordScanner = new HoodieUnMergedLogRecordScanner(FSUtils.getFs(split.getPath().toString(), jobConf),
-        split.getBasePath(), split.getDeltaFilePaths(), getReaderSchema(), split.getMaxCommitTime(),
+        split.getBasePath(), split.getDeltaLogPaths(), getReaderSchema(), split.getMaxCommitTime(),
         Boolean
             .valueOf(jobConf.get(COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP, DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED)),
         false, jobConf.getInt(MAX_DFS_STREAM_BUFFER_SIZE_PROP, DEFAULT_MAX_DFS_STREAM_BUFFER_SIZE), record -> {

File: hudi-hive/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java
Patch:
@@ -68,6 +68,9 @@ public class HiveSyncConfig implements Serializable {
   @Parameter(names = {"--use-jdbc"}, description = "Hive jdbc connect url")
   public Boolean useJdbc = true;
 
+  @Parameter(names = {"--skip-ro-suffix"}, description = "Skip the `_ro` suffix for Read optimized table, when registering")
+  public Boolean skipROSuffix = false;
+
   @Parameter(names = {"--help", "-h"}, help = true)
   public Boolean help = false;
 

File: hudi-hive/src/main/java/org/apache/hudi/hive/util/SchemaUtil.java
Patch:
@@ -391,7 +391,7 @@ public static String generateSchemaString(MessageType storageSchema, List<String
     return columns.toString();
   }
 
-  public static String generateCreateDDL(MessageType storageSchema, HiveSyncConfig config, String inputFormatClass,
+  public static String generateCreateDDL(String tableName, MessageType storageSchema, HiveSyncConfig config, String inputFormatClass,
       String outputFormatClass, String serdeClass) throws IOException {
     Map<String, String> hiveSchema = convertParquetSchemaToHiveSchema(storageSchema);
     String columns = generateSchemaString(storageSchema, config.partitionFields);
@@ -406,7 +406,7 @@ public static String generateCreateDDL(MessageType storageSchema, HiveSyncConfig
     String partitionsStr = partitionFields.stream().collect(Collectors.joining(","));
     StringBuilder sb = new StringBuilder("CREATE EXTERNAL TABLE  IF NOT EXISTS ");
     sb = sb.append(HIVE_ESCAPE_CHARACTER).append(config.databaseName).append(HIVE_ESCAPE_CHARACTER)
-            .append(".").append(HIVE_ESCAPE_CHARACTER).append(config.tableName).append(HIVE_ESCAPE_CHARACTER);
+            .append(".").append(HIVE_ESCAPE_CHARACTER).append(tableName).append(HIVE_ESCAPE_CHARACTER);
     sb = sb.append("( ").append(columns).append(")");
     if (!config.partitionFields.isEmpty()) {
       sb = sb.append(" PARTITIONED BY (").append(partitionsStr).append(")");

File: hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -138,8 +138,8 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jssc, String
                                                      String tblName, Map<String, String> parameters) {
 
     // inline compaction is on by default for MOR
-    boolean inlineCompact = parameters.get(DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY())
-        .equals(DataSourceWriteOptions.MOR_STORAGE_TYPE_OPT_VAL());
+    boolean inlineCompact = parameters.get(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY())
+        .equals(DataSourceWriteOptions.MOR_TABLE_TYPE_OPT_VAL());
 
     // insert/bulk-insert combining to be true, if filtering for duplicates
     boolean combineInserts = Boolean.parseBoolean(parameters.get(DataSourceWriteOptions.INSERT_DROP_DUPS_OPT_KEY()));

File: hudi-spark/src/test/java/HoodieJavaStreamingApp.java
Patch:
@@ -203,7 +203,7 @@ public void show(SparkSession spark, FileSystem fs, Dataset<Row> inputDF1, Datas
        * Consume incrementally, only changes in commit 2 above. Currently only supported for COPY_ON_WRITE TABLE
        */
       Dataset<Row> hoodieIncViewDF = spark.read().format("org.apache.hudi")
-          .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY(), DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL())
+          .option(DataSourceReadOptions.QUERY_TYPE_OPT_KEY(), DataSourceReadOptions.QUERY_TYPE_INCREMENTAL_OPT_VAL())
           // Only changes in write 2 above
           .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY(), commitInstantTime1)
           // For incremental view, pass in the root/base path of dataset
@@ -224,7 +224,7 @@ public void stream(Dataset<Row> streamingInput) throws Exception {
 
     DataStreamWriter<Row> writer = streamingInput.writeStream().format("org.apache.hudi")
         .option("hoodie.insert.shuffle.parallelism", "2").option("hoodie.upsert.shuffle.parallelism", "2")
-        .option(DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY(), tableType)
+        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)
         .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), "_row_key")
         .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), "partition")
         .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), "timestamp")

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -208,7 +208,7 @@ private void refreshTimeline() throws IOException {
     } else {
       this.commitTimelineOpt = Option.empty();
       HoodieTableMetaClient.initTableType(new Configuration(jssc.hadoopConfiguration()), cfg.targetBasePath,
-          cfg.storageType, cfg.targetTableName, "archived", cfg.payloadClassName);
+          cfg.tableType, cfg.targetTableName, "archived", cfg.payloadClassName);
     }
   }
 
@@ -270,7 +270,7 @@ private Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource
       }
     } else {
       HoodieTableMetaClient.initTableType(new Configuration(jssc.hadoopConfiguration()), cfg.targetBasePath,
-          cfg.storageType, cfg.targetTableName, "archived", cfg.payloadClassName);
+          cfg.tableType, cfg.targetTableName, "archived", cfg.payloadClassName);
     }
 
     if (!resumeCheckpointStr.isPresent() && cfg.checkpoint != null) {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SchedulerConfGenerator.java
Patch:
@@ -71,7 +71,7 @@ public static Map<String, String> getSparkSchedulingConfigs(HoodieDeltaStreamer.
 
     Map<String, String> additionalSparkConfigs = new HashMap<>();
     if (sparkSchedulerMode.isPresent() && "FAIR".equals(sparkSchedulerMode.get()) && cfg.continuousMode
-        && cfg.storageType.equals(HoodieTableType.MERGE_ON_READ.name())) {
+        && cfg.tableType.equals(HoodieTableType.MERGE_ON_READ.name())) {
       String sparkSchedulingConfFile = generateAndStoreConfig(cfg.deltaSyncSchedulingWeight,
           cfg.compactSchedulingWeight, cfg.deltaSyncSchedulingMinShare, cfg.compactSchedulingMinShare);
       additionalSparkConfigs.put(SPARK_SCHEDULER_ALLOCATION_FILE_KEY, sparkSchedulingConfFile);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java
Patch:
@@ -115,7 +115,7 @@ public Pair<Option<Dataset<Row>>, String> fetchNextBatch(Option<String> lastCkpt
 
     // Do Incr pull. Set end instant if available
     DataFrameReader reader = sparkSession.read().format("org.apache.hudi")
-        .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY(), DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL())
+        .option(DataSourceReadOptions.QUERY_TYPE_OPT_KEY(), DataSourceReadOptions.QUERY_TYPE_INCREMENTAL_OPT_VAL())
         .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY(), instantEndpts.getLeft())
         .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY(), instantEndpts.getRight());
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestSchedulerConfGenerator.java
Patch:
@@ -43,12 +43,12 @@ public void testGenerateSparkSchedulingConf() throws Exception {
     assertNull("continuousMode is false", configs.get(SchedulerConfGenerator.SPARK_SCHEDULER_ALLOCATION_FILE_KEY));
 
     cfg.continuousMode = true;
-    cfg.storageType = HoodieTableType.COPY_ON_WRITE.name();
+    cfg.tableType = HoodieTableType.COPY_ON_WRITE.name();
     configs = SchedulerConfGenerator.getSparkSchedulingConfigs(cfg);
-    assertNull("storageType is not MERGE_ON_READ",
+    assertNull("table type is not MERGE_ON_READ",
         configs.get(SchedulerConfGenerator.SPARK_SCHEDULER_ALLOCATION_FILE_KEY));
 
-    cfg.storageType = HoodieTableType.MERGE_ON_READ.name();
+    cfg.tableType = HoodieTableType.MERGE_ON_READ.name();
     configs = SchedulerConfGenerator.getSparkSchedulingConfigs(cfg);
     assertNotNull("all satisfies", configs.get(SchedulerConfGenerator.SPARK_SCHEDULER_ALLOCATION_FILE_KEY));
   }

File: hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java
Patch:
@@ -172,7 +172,7 @@ public List<HoodieRecord> generateUpdates(Integer n) throws IOException {
     }
 
     /**
-     * Generates delete records for the passed in rows
+     * Generates delete records for the passed in rows.
      *
      * @param rows List of {@link Row}s for which delete record need to be generated
      * @return list of hoodie records to delete

File: hudi-spark/src/test/java/DataSourceTestUtils.java
Patch:
@@ -34,7 +34,9 @@ public static Option<String> convertToString(HoodieRecord record) {
     try {
       String str = ((TestRawTripPayload) record.getData()).getJsonData();
       str = "{" + str.substring(str.indexOf("\"timestamp\":"));
-      return Option.of(str.replaceAll("}", ", \"partition\": \"" + record.getPartitionPath() + "\"}"));
+      // Remove the last } bracket
+      str = str.substring(0, str.length() - 1);
+      return Option.of(str + ", \"partition\": \"" + record.getPartitionPath() + "\"}");
     } catch (IOException e) {
       return Option.empty();
     }

File: hudi-spark/src/test/java/HoodieJavaApp.java
Patch:
@@ -212,8 +212,8 @@ public void run() throws Exception {
         .load(tablePath + (nonPartitionedTable ? "/*" : "/*/*/*/*"));
     hoodieROViewDF.registerTempTable("hoodie_ro");
     spark.sql("describe hoodie_ro").show();
-    // all trips whose fare was greater than 2.
-    spark.sql("select fare, begin_lon, begin_lat, timestamp from hoodie_ro where fare > 2.0").show();
+    // all trips whose fare amount was greater than 2.
+    spark.sql("select fare.amount, begin_lon, begin_lat, timestamp from hoodie_ro where fare.amount > 2.0").show();
 
     if (tableType.equals(HoodieTableType.COPY_ON_WRITE.name())) {
       /**

File: hudi-spark/src/test/java/HoodieJavaStreamingApp.java
Patch:
@@ -195,8 +195,8 @@ public void show(SparkSession spark, FileSystem fs, Dataset<Row> inputDF1, Datas
         .load(tablePath + "/*/*/*/*");
     hoodieROViewDF.registerTempTable("hoodie_ro");
     spark.sql("describe hoodie_ro").show();
-    // all trips whose fare was greater than 2.
-    spark.sql("select fare, begin_lon, begin_lat, timestamp from hoodie_ro where fare > 2.0").show();
+    // all trips whose fare amount was greater than 2.
+    spark.sql("select fare.amount, begin_lon, begin_lat, timestamp from hoodie_ro where fare.amount > 2.0").show();
 
     if (tableType.equals(HoodieTableType.COPY_ON_WRITE.name())) {
       /**

File: hudi-hive/src/main/java/org/apache/hudi/hive/util/SchemaUtil.java
Patch:
@@ -174,6 +174,8 @@ private static String convertField(final Type parquetType) {
         final DecimalMetadata decimalMetadata = parquetType.asPrimitiveType().getDecimalMetadata();
         return field.append("DECIMAL(").append(decimalMetadata.getPrecision()).append(" , ")
             .append(decimalMetadata.getScale()).append(")").toString();
+      } else if (originalType == OriginalType.DATE) {
+        return field.append("DATE").toString();
       }
       // TODO - fix the method naming here
       return parquetPrimitiveTypeName.convert(new PrimitiveType.PrimitiveTypeNameConverter<String, RuntimeException>() {

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java
Patch:
@@ -107,7 +107,7 @@ static String getSparkShellCommand(String commandFile) {
         .append(" --master local[2] --driver-class-path ").append(HADOOP_CONF_DIR)
         .append(
             " --conf spark.sql.hive.convertMetastoreParquet=false --deploy-mode client  --driver-memory 1G --executor-memory 1G --num-executors 1 ")
-        .append(" --packages com.databricks:spark-avro_2.11:4.0.0 ").append(" -i ").append(commandFile).toString();
+        .append(" --packages org.apache.spark:spark-avro_2.11:2.4.4 ").append(" -i ").append(commandFile).toString();
   }
 
   static String getPrestoConsoleCommand(String commandFile) {

File: hudi-spark/src/test/java/HoodieJavaApp.java
Patch:
@@ -104,6 +104,7 @@ public void run() throws Exception {
     SparkSession spark = SparkSession.builder().appName("Hoodie Spark APP")
         .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer").master("local[1]").getOrCreate();
     JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());
+    spark.sparkContext().setLogLevel("WARN");
     FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());
 
     // Generator of some records to be loaded in.

File: hudi-client/src/test/java/org/apache/hudi/TestCleaner.java
Patch:
@@ -910,7 +910,7 @@ public void testCleanMarkerDataFilesOnRollback() throws IOException {
    * Test CLeaner Stat when there are no partition paths.
    */
   @Test
-  public void testCleaningWithZeroPartitonPaths() throws IOException {
+  public void testCleaningWithZeroPartitionPaths() throws IOException {
     HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder()
             .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(2).build())

File: hudi-client/src/test/java/org/apache/hudi/table/TestCopyOnWriteTable.java
Patch:
@@ -416,7 +416,7 @@ private UpsertPartitioner getUpsertPartitioner(int smallFileSize, int numInserts
     WorkloadProfile profile = new WorkloadProfile(jsc.parallelize(records));
     HoodieCopyOnWriteTable.UpsertPartitioner partitioner =
         (HoodieCopyOnWriteTable.UpsertPartitioner) table.getUpsertPartitioner(profile);
-    assertEquals("Update record should have gone to the 1 update partiton", 0, partitioner.getPartition(
+    assertEquals("Update record should have gone to the 1 update partition", 0, partitioner.getPartition(
         new Tuple2<>(updateRecords.get(0).getKey(), Option.ofNullable(updateRecords.get(0).getCurrentLocation()))));
     return partitioner;
   }

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java
Patch:
@@ -329,7 +329,7 @@ private void testCleans(SyncableFileSystemView view, List<String> newCleanerInst
         Assert.assertEquals(State.COMPLETED, view.getLastInstant().get().getState());
         Assert.assertEquals(HoodieTimeline.CLEAN_ACTION, view.getLastInstant().get().getAction());
         partitions.forEach(p -> {
-          LOG.info("PARTTITION : " + p);
+          LOG.info("PARTITION : " + p);
           LOG.info("\tFileSlices :" + view.getAllFileSlices(p).collect(Collectors.toList()));
         });
 

File: hudi-hive/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java
Patch:
@@ -133,7 +133,7 @@ public HoodieTimeline getActiveTimeline() {
   }
 
   /**
-   * Add the (NEW) partitons to the table.
+   * Add the (NEW) partitions to the table.
    */
   void addPartitionsToTable(List<String> partitionsToAdd) {
     if (partitionsToAdd.isEmpty()) {

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java
Patch:
@@ -175,7 +175,8 @@ public long fetchTotalInsertRecordsWritten() {
     long totalInsertRecordsWritten = 0;
     for (List<HoodieWriteStat> stats : partitionToWriteStats.values()) {
       for (HoodieWriteStat stat : stats) {
-        if (stat.getPrevCommit() != null && stat.getPrevCommit().equalsIgnoreCase("null")) {
+        // determine insert rows in every file
+        if (stat.getPrevCommit() != null) {
           totalInsertRecordsWritten += stat.getNumInserts();
         }
       }

File: hudi-cli/src/main/java/org/apache/hudi/cli/HoodieCLI.java
Patch:
@@ -48,7 +48,7 @@ public class HoodieCLI {
    * Enum for CLI state.
    */
   public enum CLIState {
-    INIT, DATASET, SYNC
+    INIT, TABLE, SYNC
   }
 
   public static void setConsistencyGuardConfig(ConsistencyGuardConfig config) {
@@ -100,7 +100,7 @@ public static void connectTo(String basePath, Integer layoutVersion) {
    */
   public static HoodieTableMetaClient getTableMetaClient() {
     if (tableMetadata == null) {
-      throw new NullPointerException("There is no hudi dataset. Please use connect command to set dataset first");
+      throw new NullPointerException("There is no hudi table. Please use connect command to set table first");
     }
     return tableMetadata;
   }

File: hudi-cli/src/main/java/org/apache/hudi/cli/HoodiePrompt.java
Patch:
@@ -37,7 +37,7 @@ public String getPrompt() {
       switch (HoodieCLI.state) {
         case INIT:
           return "hudi->";
-        case DATASET:
+        case TABLE:
           return "hudi:" + tableName + "->";
         case SYNC:
           return "hudi:" + tableName + " <==> " + HoodieCLI.syncTableMetadata.getTableConfig().getTableName() + "->";

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java
Patch:
@@ -145,7 +145,7 @@ public String compactionsAll(
   @CliCommand(value = "compaction show", help = "Shows compaction details for a specific compaction instant")
   public String compactionShow(
       @CliOption(key = "instant", mandatory = true,
-          help = "Base path for the target hoodie dataset") final String compactionInstantTime,
+          help = "Base path for the target hoodie table") final String compactionInstantTime,
       @CliOption(key = {"limit"}, help = "Limit commits",
           unspecifiedDefaultValue = "-1") final Integer limit,
       @CliOption(key = {"sortBy"}, help = "Sorting Field", unspecifiedDefaultValue = "") final String sortByField,
@@ -212,7 +212,7 @@ public String compact(
       @CliOption(key = "sparkMemory", unspecifiedDefaultValue = "4G",
           help = "Spark executor memory") final String sparkMemory,
       @CliOption(key = "retry", unspecifiedDefaultValue = "1", help = "Number of retries") final String retry,
-      @CliOption(key = "compactionInstant", help = "Base path for the target hoodie dataset") String compactionInstantTime,
+      @CliOption(key = "compactionInstant", help = "Base path for the target hoodie table") String compactionInstantTime,
       @CliOption(key = "propsFilePath", help = "path to properties file on localfs or dfs with configurations for hoodie client for compacting",
         unspecifiedDefaultValue = "") final String propsFilePath,
       @CliOption(key = "hoodieConfigs", help = "Any configuration that can be set in the properties file can be passed here in the form of an array",
@@ -471,7 +471,7 @@ private String getRenamesToBePrinted(List<RenameOpResult> res, Integer limit, St
       if (result.get()) {
         System.out.println("All renames successfully completed to " + operation + " done !!");
       } else {
-        System.out.println("Some renames failed. DataSet could be in inconsistent-state. Try running compaction repair");
+        System.out.println("Some renames failed. table could be in inconsistent-state. Try running compaction repair");
       }
 
       List<Comparable[]> rows = new ArrayList<>();

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java
Patch:
@@ -65,7 +65,7 @@ public String deduplicate(
     return "Deduplication failed ";
   }
 
-  @CliCommand(value = "repair addpartitionmeta", help = "Add partition metadata to a dataset, if not present")
+  @CliCommand(value = "repair addpartitionmeta", help = "Add partition metadata to a table, if not present")
   public String addPartitionMeta(
       @CliOption(key = {"dryrun"}, help = "Should we actually add or just print what would be done",
           unspecifiedDefaultValue = "true") final boolean dryRun)

File: hudi-client/src/main/java/org/apache/hudi/HoodieReadClient.java
Patch:
@@ -62,15 +62,15 @@ public class HoodieReadClient<T extends HoodieRecordPayload> extends AbstractHoo
 
   /**
    * TODO: We need to persist the index type into hoodie.properties and be able to access the index just with a simple
-   * basepath pointing to the dataset. Until, then just always assume a BloomIndex
+   * basepath pointing to the table. Until, then just always assume a BloomIndex
    */
   private final transient HoodieIndex<T> index;
   private final HoodieTimeline commitTimeline;
   private HoodieTable hoodieTable;
   private transient Option<SQLContext> sqlContextOpt;
 
   /**
-   * @param basePath path to Hoodie dataset
+   * @param basePath path to Hoodie table
    */
   public HoodieReadClient(JavaSparkContext jsc, String basePath, Option<EmbeddedTimelineService> timelineService) {
     this(jsc, HoodieWriteConfig.newBuilder().withPath(basePath)
@@ -80,7 +80,7 @@ public HoodieReadClient(JavaSparkContext jsc, String basePath, Option<EmbeddedTi
   }
 
   /**
-   * @param basePath path to Hoodie dataset
+   * @param basePath path to Hoodie table
    */
   public HoodieReadClient(JavaSparkContext jsc, String basePath) {
     this(jsc, basePath, Option.empty());

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java
Patch:
@@ -302,7 +302,7 @@ public boolean isImplicitWithStorage() {
 
   /**
    * For each incoming record, produce N output records, 1 each for each file against which the record's key needs to be
-   * checked. For datasets, where the keys have a definite insert order (e.g: timestamp as prefix), the number of files
+   * checked. For tables, where the keys have a definite insert order (e.g: timestamp as prefix), the number of files
    * to be compared gets cut down a lot from range pruning.
    *
    * Sub-partition to ensure the records can be looked up against files & also prune file<=>record comparisons based on

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java
Patch:
@@ -43,7 +43,7 @@
 import scala.Tuple2;
 
 /**
- * This filter will only work with hoodie dataset since it will only load partitions with .hoodie_partition_metadata
+ * This filter will only work with hoodie table since it will only load partitions with .hoodie_partition_metadata
  * file in it.
  */
 public class HoodieGlobalBloomIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {
@@ -71,7 +71,7 @@ List<Tuple2<String, BloomIndexFileInfo>> loadInvolvedFiles(List<String> partitio
 
   /**
    * For each incoming record, produce N output records, 1 each for each file against which the record's key needs to be
-   * checked. For datasets, where the keys have a definite insert order (e.g: timestamp as prefix), the number of files
+   * checked. For tables, where the keys have a definite insert order (e.g: timestamp as prefix), the number of files
    * to be compared gets cut down a lot from range pruning.
    * <p>
    * Sub-partition to ensure the records can be looked up against files & also prune file<=>record comparisons based on

File: hudi-client/src/main/java/org/apache/hudi/io/compact/HoodieRealtimeTableCompactor.java
Patch:
@@ -75,9 +75,9 @@
 public class HoodieRealtimeTableCompactor implements HoodieCompactor {
 
   private static final Logger LOG = LogManager.getLogger(HoodieRealtimeTableCompactor.class);
-  // Accumulator to keep track of total log files for a dataset
+  // Accumulator to keep track of total log files for a table
   private AccumulatorV2<Long, Long> totalLogFiles;
-  // Accumulator to keep track of total log file slices for a dataset
+  // Accumulator to keep track of total log file slices for a table
   private AccumulatorV2<Long, Long> totalFileSlices;
 
   @Override

File: hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/BoundedPartitionAwareCompactionStrategy.java
Patch:
@@ -34,7 +34,7 @@
 
 /**
  * This strategy ensures that the last N partitions are picked up even if there are later partitions created for the
- * dataset. lastNPartitions is defined as the N partitions before the currentDate. currentDay = 2018/01/01 The dataset
+ * table. lastNPartitions is defined as the N partitions before the currentDate. currentDay = 2018/01/01 The table
  * has partitions for 2018/02/02 and 2018/03/03 beyond the currentDay This strategy will pick up the following
  * partitions for compaction : (2018/01/01, allPartitionsInRange[(2018/01/01 - lastNPartitions) to 2018/01/01),
  * 2018/02/02, 2018/03/03)

File: hudi-client/src/test/java/HoodieClientExample.java
Patch:
@@ -132,7 +132,7 @@ public void run() throws Exception {
     client.delete(deleteRecords, newCommitTime);
 
     /**
-     * Schedule a compaction and also perform compaction on a MOR dataset
+     * Schedule a compaction and also perform compaction on a MOR table
      */
     if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {
       Option<String> instant = client.scheduleCompaction(Option.empty());

File: hudi-client/src/test/java/org/apache/hudi/common/HoodieClientTestUtils.java
Patch:
@@ -178,7 +178,7 @@ public static Dataset<Row> readCommit(String basePath, SQLContext sqlContext, Ho
   }
 
   /**
-   * Obtain all new data written into the Hoodie dataset since the given timestamp.
+   * Obtain all new data written into the Hoodie table since the given timestamp.
    */
   public static Dataset<Row> readSince(String basePath, SQLContext sqlContext, HoodieTimeline commitTimeline,
                                        String lastCommitTime) {
@@ -195,7 +195,7 @@ public static Dataset<Row> readSince(String basePath, SQLContext sqlContext, Hoo
   }
 
   /**
-   * Reads the paths under the a hoodie dataset out as a DataFrame.
+   * Reads the paths under the a hoodie table out as a DataFrame.
    */
   public static Dataset<Row> read(JavaSparkContext jsc, String basePath, SQLContext sqlContext, FileSystem fs,
                                   String... paths) {
@@ -212,7 +212,7 @@ public static Dataset<Row> read(JavaSparkContext jsc, String basePath, SQLContex
       }
       return sqlContext.read().parquet(filteredPaths.toArray(new String[filteredPaths.size()]));
     } catch (Exception e) {
-      throw new HoodieException("Error reading hoodie dataset as a dataframe", e);
+      throw new HoodieException("Error reading hoodie table as a dataframe", e);
     }
   }
 

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieCommitArchiveLog.java
Patch:
@@ -76,7 +76,7 @@ public void clean() throws IOException {
   }
 
   @Test
-  public void testArchiveEmptyDataset() throws IOException {
+  public void testArchiveEmptyTable() throws IOException {
     HoodieWriteConfig cfg =
         HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)
             .withParallelism(2, 2).forTable("test-trip-table").build();
@@ -87,7 +87,7 @@ public void testArchiveEmptyDataset() throws IOException {
   }
 
   @Test
-  public void testArchiveDatasetWithArchival() throws IOException {
+  public void testArchiveTableWithArchival() throws IOException {
     HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(basePath)
         .withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().retainCommits(1).archiveCommitsWith(2, 4).build())
@@ -229,7 +229,7 @@ public void testArchiveDatasetWithArchival() throws IOException {
   }
 
   @Test
-  public void testArchiveDatasetWithNoArchival() throws IOException {
+  public void testArchiveTableWithNoArchival() throws IOException {
     HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(basePath)
         .withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2).forTable("test-trip-table")
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().retainCommits(1).archiveCommitsWith(2, 5).build())

File: hudi-client/src/test/java/org/apache/hudi/table/TestMergeOnReadTable.java
Patch:
@@ -292,7 +292,7 @@ public void testSimpleInsertUpdateAndDelete() throws Exception {
   }
 
   @Test
-  public void testCOWToMORConvertedDatasetRollback() throws Exception {
+  public void testCOWToMORConvertedTableRollback() throws Exception {
 
     // Set TableType to COW
     HoodieTestUtils.init(jsc.hadoopConfiguration(), basePath, HoodieTableType.COPY_ON_WRITE);

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroPayload.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  * This is a payload to wrap a existing Hoodie Avro Record. Useful to create a HoodieRecord over existing GenericRecords
- * in a hoodie datasets (useful in compactions)
+ * in a hoodie tables (useful in compactions)
  */
 public class HoodieAvroPayload implements HoodieRecordPayload<HoodieAvroPayload> {
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java
Patch:
@@ -28,7 +28,7 @@
 import java.util.Map;
 
 /**
- * Every Hoodie dataset has an implementation of the <code>HoodieRecordPayload</code> This abstracts out callbacks which
+ * Every Hoodie table has an implementation of the <code>HoodieRecordPayload</code> This abstracts out callbacks which
  * depend on record specific logic.
  */
 public interface HoodieRecordPayload<T extends HoodieRecordPayload> extends Serializable {

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRollingStatMetadata.java
Patch:
@@ -27,7 +27,7 @@
 import java.util.Map;
 
 /**
- * This class holds statistics about files belonging to a dataset.
+ * This class holds statistics about files belonging to a table.
  */
 public class HoodieRollingStatMetadata implements Serializable {
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTimeline.java
Patch:
@@ -31,7 +31,7 @@
 import java.util.stream.Stream;
 
 /**
- * HoodieTimeline is a view of meta-data instants in the hoodie dataset. Instants are specific points in time
+ * HoodieTimeline is a view of meta-data instants in the hoodie table. Instants are specific points in time
  * represented as HoodieInstant.
  * <p>
  * Timelines are immutable once created and operations create new instance of timelines which filter on the instants and

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -47,7 +47,7 @@
 import java.util.stream.Stream;
 
 /**
- * Represents the Active Timeline for the HoodieDataset. Instants for the last 12 hours (configurable) is in the
+ * Represents the Active Timeline for the Hoodie table. Instants for the last 12 hours (configurable) is in the
  * ActiveTimeline and the rest are Archived. ActiveTimeline is a special timeline that allows for creation of instants
  * on the timeline.
  * <p>

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieArchivedTimeline.java
Patch:
@@ -38,7 +38,7 @@
 import java.util.stream.Collectors;
 
 /**
- * Represents the Archived Timeline for the HoodieDataset. Instants for the last 12 hours (configurable) is in the
+ * Represents the Archived Timeline for the Hoodie table. Instants for the last 12 hours (configurable) is in the
  * ActiveTimeline and the rest are in ArchivedTimeline.
  * <p>
  * </p>

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstant.java
Patch:
@@ -30,7 +30,7 @@
 import java.util.Objects;
 
 /**
- * A Hoodie Instant represents a action done on a hoodie dataset. All actions start with a inflight instant and then
+ * A Hoodie Instant represents a action done on a hoodie table. All actions start with a inflight instant and then
  * create a completed instant after done.
  *
  * @see HoodieTimeline

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -60,7 +60,7 @@
 /**
  * Common thread-safe implementation for multiple TableFileSystemView Implementations. Provides uniform handling of (a)
  * Loading file-system views from underlying file-system (b) Pending compaction operations and changing file-system
- * views based on that (c) Thread-safety in loading and managing file system views for this dataset. (d) resetting
+ * views based on that (c) Thread-safety in loading and managing file system views for this table. (d) resetting
  * file-system views The actual mechanism of fetching file slices from different view storages is delegated to
  * sub-classes.
  */

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java
Patch:
@@ -94,7 +94,7 @@ public class RemoteHoodieTableFileSystemView implements SyncableFileSystemView,
   public static final String TIMELINE = String.format("%s/%s", BASE_URL, "timeline/instants/all");
 
   // POST Requests
-  public static final String REFRESH_DATASET = String.format("%s/%s", BASE_URL, "refresh/");
+  public static final String REFRESH_TABLE = String.format("%s/%s", BASE_URL, "refresh/");
 
   public static final String PARTITION_PARAM = "partition";
   public static final String BASEPATH_PARAM = "basepath";
@@ -381,7 +381,7 @@ public Stream<HoodieFileGroup> getAllFileGroups(String partitionPath) {
   public boolean refresh() {
     Map<String, String> paramsMap = getParams();
     try {
-      return executeRequest(REFRESH_DATASET, paramsMap, new TypeReference<Boolean>() {}, RequestMethod.POST);
+      return executeRequest(REFRESH_TABLE, paramsMap, new TypeReference<Boolean>() {}, RequestMethod.POST);
     } catch (IOException e) {
       throw new HoodieRemoteException(e);
     }

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java
Patch:
@@ -46,7 +46,7 @@
 import java.util.stream.Stream;
 
 /**
- * A file-system view implementation on top of embedded Rocks DB store. For each DataSet : 3 column Family is added for
+ * A file-system view implementation on top of embedded Rocks DB store. For each table : 3 column Family is added for
  * storing (1) File-Slices and Data Files for View lookups (2) Pending compaction operations (3) Partitions tracked
  *
  * Fine-grained retrieval API to fetch latest file-slice and data-file which are common operations for
@@ -135,7 +135,7 @@ void removePendingCompactionOperations(Stream<Pair<String, CompactionOperation>>
 
   @Override
   protected void resetViewState() {
-    LOG.info("Deleting all rocksdb data associated with dataset filesystem view");
+    LOG.info("Deleting all rocksdb data associated with table filesystem view");
     rocksDB.close();
     rocksDB = new RocksDBDAO(metaClient.getBasePath(), config.getRocksdbBasePath());
   }

File: hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBSchemaHelper.java
Patch:
@@ -29,7 +29,7 @@
 /**
  * Helper class to generate Key and column names for rocksdb based view
  *
- * For RocksDB, 3 colFamilies are used for storing file-system view for each dataset. (a) View (b) Partitions Cached (c)
+ * For RocksDB, 3 colFamilies are used for storing file-system view for each table. (a) View (b) Partitions Cached (c)
  * Pending Compactions
  *
  *

File: hudi-common/src/main/java/org/apache/hudi/exception/HoodieIOException.java
Patch:
@@ -22,7 +22,7 @@
 
 /**
  * <p>
- * Exception thrown for dataset IO-related failures.
+ * Exception thrown for table IO-related failures.
  * </p>
  */
 public class HoodieIOException extends HoodieException {

File: hudi-common/src/test/java/org/apache/hudi/common/model/HoodieTestUtils.java
Patch:
@@ -118,7 +118,7 @@ public static HoodieTableMetaClient init(Configuration hadoopConf, String basePa
     properties.setProperty(HoodieTableConfig.HOODIE_TABLE_NAME_PROP_NAME, RAW_TRIPS_TEST_NAME);
     properties.setProperty(HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME, tableType.name());
     properties.setProperty(HoodieTableConfig.HOODIE_PAYLOAD_CLASS_PROP_NAME, HoodieAvroPayload.class.getName());
-    return HoodieTableMetaClient.initDatasetAndGetMetaClient(hadoopConf, basePath, properties);
+    return HoodieTableMetaClient.initTableAndGetMetaClient(hadoopConf, basePath, properties);
   }
 
   public static String makeNewCommitTime() {

File: hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java
Patch:
@@ -353,7 +353,7 @@ private void testCleans(SyncableFileSystemView view, List<String> newCleanerInst
    * @param isDeltaCommit is Delta Commit ?
    * @param instantsToFiles List of files associated with each instant
    * @param rolledBackInstants List of rolled-back instants
-   * @param emptyRestoreInstant Restore instant at which dataset becomes empty
+   * @param emptyRestoreInstant Restore instant at which table becomes empty
    */
   private void testRestore(SyncableFileSystemView view, List<String> newRestoreInstants, boolean isDeltaCommit,
       Map<String, List<String>> instantsToFiles, List<String> rolledBackInstants, String emptyRestoreInstant,

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java
Patch:
@@ -60,7 +60,7 @@
 import java.util.stream.Stream;
 
 /**
- * Input Format, that provides a real-time view of data in a Hoodie dataset.
+ * Input Format, that provides a real-time view of data in a Hoodie table.
  */
 @UseFileSplitsFromInputFormat
 public class HoodieParquetRealtimeInputFormat extends HoodieParquetInputFormat implements Configurable {

File: hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestRecordReaderValueIterator.java
Patch:
@@ -34,7 +34,7 @@ public class TestRecordReaderValueIterator {
 
   @Test
   public void testValueIterator() {
-    String[] values = new String[] {"hoodie", "efficient", "new project", "realtime", "spark", "dataset",};
+    String[] values = new String[] {"hoodie", "efficient", "new project", "realtime", "spark", "table",};
     List<Pair<Integer, String>> entries =
         IntStream.range(0, values.length).boxed().map(idx -> Pair.of(idx, values[idx])).collect(Collectors.toList());
     TestRecordReader reader = new TestRecordReader(entries);

File: hudi-hive/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java
Patch:
@@ -44,7 +44,7 @@ public class HiveSyncConfig implements Serializable {
   @Parameter(names = {"--jdbc-url"}, description = "Hive jdbc connect url", required = true)
   public String jdbcUrl;
 
-  @Parameter(names = {"--base-path"}, description = "Basepath of hoodie dataset to sync", required = true)
+  @Parameter(names = {"--base-path"}, description = "Basepath of hoodie table to sync", required = true)
   public String basePath;
 
   @Parameter(names = "--partitioned-by", description = "Fields in the schema partitioned by")

File: hudi-hive/src/main/java/org/apache/hudi/hive/util/SchemaUtil.java
Patch:
@@ -95,7 +95,7 @@ public static SchemaDifference getSchemaDifference(MessageType storageSchema, Ma
         expectedType = expectedType.replaceAll("`", "");
 
         if (!tableColumnType.equalsIgnoreCase(expectedType)) {
-          // check for incremental datasets, the schema type change is allowed as per evolution
+          // check for incremental queries, the schema type change is allowed as per evolution
           // rules
           if (!isSchemaTypeUpdateAllowed(tableColumnType, expectedType)) {
             throw new HoodieHiveSyncException("Could not convert field Type from " + tableColumnType + " to "

File: hudi-hive/src/test/java/org/apache/hudi/hive/TestUtil.java
Patch:
@@ -152,7 +152,7 @@ public static void shutdown() {
     }
   }
 
-  static void createCOWDataset(String commitTime, int numberOfPartitions)
+  static void createCOWTable(String commitTime, int numberOfPartitions)
       throws IOException, InitializationError, URISyntaxException {
     Path path = new Path(hiveSyncConfig.basePath);
     FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));
@@ -166,7 +166,7 @@ static void createCOWDataset(String commitTime, int numberOfPartitions)
     createCommitFile(commitMetadata, commitTime);
   }
 
-  static void createMORDataset(String commitTime, String deltaCommitTime, int numberOfPartitions)
+  static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions)
       throws IOException, InitializationError, URISyntaxException, InterruptedException {
     Path path = new Path(hiveSyncConfig.basePath);
     FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));

File: hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
-import org.apache.hudi.exception.DatasetNotFoundException;
+import org.apache.hudi.exception.TableNotFoundException;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieNotSupportedException;
 import org.apache.hudi.hive.HiveSyncConfig;
@@ -186,8 +186,8 @@ public static JavaRDD<HoodieRecord> dropDuplicates(JavaSparkContext jssc, JavaRD
       client = new HoodieReadClient<>(jssc, writeConfig, timelineService);
       return client.tagLocation(incomingHoodieRecords)
           .filter(r -> !((HoodieRecord<HoodieRecordPayload>) r).isCurrentLocationKnown());
-    } catch (DatasetNotFoundException e) {
-      // this will be executed when there is no hoodie dataset yet
+    } catch (TableNotFoundException e) {
+      // this will be executed when there is no hoodie table yet
       // so no dups to drop
       return incomingHoodieRecords;
     } finally {

File: hudi-spark/src/main/java/org/apache/hudi/HoodieDataSourceHelpers.java
Patch:
@@ -36,7 +36,7 @@
 public class HoodieDataSourceHelpers {
 
   /**
-   * Checks if the Hoodie dataset has new data since given timestamp. This can be subsequently fed to an incremental
+   * Checks if the Hoodie table has new data since given timestamp. This can be subsequently fed to an incremental
    * view read, to perform incremental processing.
    */
   public static boolean hasNewCommits(FileSystem fs, String basePath, String commitTimestamp) {

File: hudi-spark/src/main/java/org/apache/hudi/payload/AWSDmsAvroPayload.java
Patch:
@@ -38,7 +38,7 @@
  * - For updates against the source table, records contain full after image with `Op=U`
  * - For deletes against the source table, records contain full before image with `Op=D`
  *
- * This payload implementation will issue matching insert, delete, updates against the hudi dataset
+ * This payload implementation will issue matching insert, delete, updates against the hudi table
  *
  */
 public class AWSDmsAvroPayload extends OverwriteWithLatestAvroPayload {

File: hudi-spark/src/test/java/HoodieJavaApp.java
Patch:
@@ -45,7 +45,7 @@
 import java.util.List;
 
 /**
- * Sample program that writes & reads hoodie datasets via the Spark datasource.
+ * Sample program that writes & reads hoodie tables via the Spark datasource.
  */
 public class HoodieJavaApp {
 

File: hudi-spark/src/test/java/HoodieJavaStreamingApp.java
Patch:
@@ -45,7 +45,7 @@
 import java.util.concurrent.Future;
 
 /**
- * Sample program that writes & reads hoodie datasets via the Spark datasource streaming.
+ * Sample program that writes & reads hoodie tables via the Spark datasource streaming.
  */
 public class HoodieJavaStreamingApp {
 

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java
Patch:
@@ -77,7 +77,7 @@ public void register() {
   }
 
   /**
-   * Determines if local view of dataset's timeline is behind that of client's view.
+   * Determines if local view of table's timeline is behind that of client's view.
    */
   private boolean isLocalViewBehind(Context ctx) {
     String basePath = ctx.queryParam(RemoteHoodieTableFileSystemView.BASEPATH_PARAM);
@@ -284,9 +284,9 @@ private void registerFileSlicesAPI() {
       writeValueAsString(ctx, dtos);
     }, true));
 
-    app.post(RemoteHoodieTableFileSystemView.REFRESH_DATASET, new ViewHandler(ctx -> {
+    app.post(RemoteHoodieTableFileSystemView.REFRESH_TABLE, new ViewHandler(ctx -> {
       boolean success = sliceHandler
-          .refreshDataset(ctx.validatedQueryParam(RemoteHoodieTableFileSystemView.BASEPATH_PARAM).getOrThrow());
+          .refreshTable(ctx.validatedQueryParam(RemoteHoodieTableFileSystemView.BASEPATH_PARAM).getOrThrow());
       writeValueAsString(ctx, success);
     }, false));
   }

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/FileSliceHandler.java
Patch:
@@ -89,7 +89,7 @@ public List<FileGroupDTO> getAllFileGroups(String basePath, String partitionPath
         .collect(Collectors.toList());
   }
 
-  public boolean refreshDataset(String basePath) {
+  public boolean refreshTable(String basePath) {
     viewManager.clearFileSystemView(basePath);
     return true;
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HiveIncrementalPuller.java
Patch:
@@ -234,7 +234,7 @@ private void executeStatement(String sql, Statement stmt) throws SQLException {
   }
 
   private String inferCommitTime(FileSystem fs) throws SQLException, IOException {
-    LOG.info("FromCommitTime not specified. Trying to infer it from Hoodie dataset " + config.targetDb + "."
+    LOG.info("FromCommitTime not specified. Trying to infer it from Hoodie table " + config.targetDb + "."
         + config.targetTable);
     String targetDataLocation = getTableLocation(config.targetDb, config.targetTable);
     return scanForCommitTime(fs, targetDataLocation);

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java
Patch:
@@ -82,7 +82,7 @@ private HoodieWriteConfig getHoodieClientConfig() throws Exception {
 
   public static class Config implements Serializable {
 
-    @Parameter(names = {"--target-base-path"}, description = "base path for the hoodie dataset to be cleaner.",
+    @Parameter(names = {"--target-base-path"}, description = "base path for the hoodie table to be cleaner.",
         required = true)
     public String basePath;
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java
Patch:
@@ -144,7 +144,7 @@ public static class Config implements Serializable {
 
     @Parameter(names = {"--operation", "-op"}, description = "Operation", required = true)
     public Operation operation = Operation.VALIDATE;
-    @Parameter(names = {"--base-path", "-bp"}, description = "Base path for the dataset", required = true)
+    @Parameter(names = {"--base-path", "-bp"}, description = "Base path for the table", required = true)
     public String basePath = null;
     @Parameter(names = {"--instant-time", "-in"}, description = "Compaction Instant time", required = false)
     public String compactionInstantTime = null;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java
Patch:
@@ -51,7 +51,7 @@ public HoodieCompactor(Config cfg) {
   }
 
   public static class Config implements Serializable {
-    @Parameter(names = {"--base-path", "-sp"}, description = "Base path for the dataset", required = true)
+    @Parameter(names = {"--base-path", "-sp"}, description = "Base path for the table", required = true)
     public String basePath = null;
     @Parameter(names = {"--table-name", "-tn"}, description = "Table name", required = true)
     public String tableName = null;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
Patch:
@@ -224,7 +224,7 @@ public static int handleErrors(JavaSparkContext jsc, String instantTime, JavaRDD
       }
     });
     if (errors.value() == 0) {
-      LOG.info(String.format("Dataset imported into hoodie dataset with %s instant time.", instantTime));
+      LOG.info(String.format("Table imported into hoodie with %s instant time.", instantTime));
       return 0;
     }
     LOG.error(String.format("Import failed with %d errors.", errors.value()));

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/adhoc/UpgradePayloadFromUberToApache.java
Patch:
@@ -42,7 +42,7 @@
 /**
  * This is an one-time use class meant for migrating the configuration for "hoodie.compaction.payload.class" in
  * .hoodie/hoodie.properties from com.uber.hoodie to org.apache.hudi . It takes in a file containing base-paths for a set
- * of hudi datasets and does the migration
+ * of hudi tables and does the migration
  */
 public class UpgradePayloadFromUberToApache implements Serializable {
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -79,7 +79,7 @@
 import static org.apache.hudi.utilities.schema.RowBasedSchemaProvider.HOODIE_RECORD_STRUCT_NAME;
 
 /**
- * Sync's one batch of data to hoodie dataset.
+ * Sync's one batch of data to hoodie table.
  */
 public class DeltaSync implements Serializable {
 
@@ -103,12 +103,12 @@ public class DeltaSync implements Serializable {
   private transient SchemaProvider schemaProvider;
 
   /**
-   * Allows transforming source to target dataset before writing.
+   * Allows transforming source to target table before writing.
    */
   private transient Transformer transformer;
 
   /**
-   * Extract the key for the target dataset.
+   * Extract the key for the target table.
    */
   private KeyGenerator keyGenerator;
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/NullTargetSchemaRegistryProvider.java
Patch:
@@ -24,7 +24,7 @@
 import org.apache.spark.api.java.JavaSparkContext;
 
 /**
- * Schema provider that will force DeltaStreamer to infer target schema from the dataset. It can be used with SQL or
+ * Schema provider that will force DeltaStreamer to infer target schema from the table. It can be used with SQL or
  * Flattening transformers to avoid having a target schema in the schema registry.
  */
 public class NullTargetSchemaRegistryProvider extends SchemaRegistryProvider {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaProvider.java
Patch:
@@ -42,7 +42,7 @@ protected SchemaProvider(TypedProperties props, JavaSparkContext jssc) {
   public abstract Schema getSourceSchema();
 
   public Schema getTargetSchema() {
-    // by default, use source schema as target for hoodie dataset as well
+    // by default, use source schema as target for hoodie table as well
     return getSourceSchema();
   }
 }

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java
Patch:
@@ -85,7 +85,7 @@ public static void cleanupClass() throws Exception {
    * Test successful data import with retries.
    */
   @Test
-  public void testDatasetImportWithRetries() throws Exception {
+  public void testImportWithRetries() throws Exception {
     JavaSparkContext jsc = null;
     try {
       jsc = getJavaSparkContext();

File: hudi-spark/src/main/java/org/apache/hudi/ComplexKeyGenerator.java
Patch:
@@ -61,7 +61,7 @@ public HoodieKey getKey(GenericRecord record) {
     boolean keyIsNullEmpty = true;
     StringBuilder recordKey = new StringBuilder();
     for (String recordKeyField : recordKeyFields) {
-      String recordKeyValue = DataSourceUtils.getNullableNestedFieldValAsString(record, recordKeyField);
+      String recordKeyValue = DataSourceUtils.getNestedFieldValAsString(record, recordKeyField, true);
       if (recordKeyValue == null) {
         recordKey.append(recordKeyField + ":" + NULL_RECORDKEY_PLACEHOLDER + ",");
       } else if (recordKeyValue.isEmpty()) {
@@ -79,7 +79,7 @@ public HoodieKey getKey(GenericRecord record) {
 
     StringBuilder partitionPath = new StringBuilder();
     for (String partitionPathField : partitionPathFields) {
-      String fieldVal = DataSourceUtils.getNullableNestedFieldValAsString(record, partitionPathField);
+      String fieldVal = DataSourceUtils.getNestedFieldValAsString(record, partitionPathField, true);
       if (fieldVal == null || fieldVal.isEmpty()) {
         partitionPath.append(hiveStylePartitioning ? partitionPathField + "=" + DEFAULT_PARTITION_PATH
                 : DEFAULT_PARTITION_PATH);

File: hudi-spark/src/main/java/org/apache/hudi/NonpartitionedKeyGenerator.java
Patch:
@@ -37,7 +37,7 @@ public NonpartitionedKeyGenerator(TypedProperties props) {
 
   @Override
   public HoodieKey getKey(GenericRecord record) {
-    String recordKey = DataSourceUtils.getNullableNestedFieldValAsString(record, recordKeyField);
+    String recordKey = DataSourceUtils.getNestedFieldValAsString(record, recordKeyField, true);
     if (recordKey == null || recordKey.isEmpty()) {
       throw new HoodieKeyException("recordKey value: \"" + recordKey + "\" for field: \"" + recordKeyField + "\" cannot be null or empty.");
     }

File: hudi-spark/src/main/java/org/apache/hudi/SimpleKeyGenerator.java
Patch:
@@ -51,12 +51,12 @@ public HoodieKey getKey(GenericRecord record) {
       throw new HoodieKeyException("Unable to find field names for record key or partition path in cfg");
     }
 
-    String recordKey = DataSourceUtils.getNullableNestedFieldValAsString(record, recordKeyField);
+    String recordKey = DataSourceUtils.getNestedFieldValAsString(record, recordKeyField, true);
     if (recordKey == null || recordKey.isEmpty()) {
       throw new HoodieKeyException("recordKey value: \"" + recordKey + "\" for field: \"" + recordKeyField + "\" cannot be null or empty.");
     }
 
-    String partitionPath = DataSourceUtils.getNullableNestedFieldValAsString(record, partitionPathField);
+    String partitionPath = DataSourceUtils.getNestedFieldValAsString(record, partitionPathField, true);
     if (partitionPath == null || partitionPath.isEmpty()) {
       partitionPath = DEFAULT_PARTITION_PATH;
     }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
Patch:
@@ -313,7 +313,7 @@ private Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource
     JavaRDD<GenericRecord> avroRDD = avroRDDOptional.get();
     JavaRDD<HoodieRecord> records = avroRDD.map(gr -> {
       HoodieRecordPayload payload = DataSourceUtils.createPayload(cfg.payloadClassName, gr,
-          (Comparable) DataSourceUtils.getNestedFieldVal(gr, cfg.sourceOrderingField));
+          (Comparable) DataSourceUtils.getNestedFieldVal(gr, cfg.sourceOrderingField, false));
       return new HoodieRecord<>(keyGenerator.getKey(gr), payload);
     });
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HiveIncrementalPuller.java
Patch:
@@ -112,7 +112,7 @@ public HiveIncrementalPuller(Config config) throws IOException {
     this.config = config;
     validateConfig(config);
     String templateContent =
-        FileIOUtils.readAsUTFString(this.getClass().getResourceAsStream("IncrementalPull.sqltemplate"));
+        FileIOUtils.readAsUTFString(this.getClass().getResourceAsStream("/IncrementalPull.sqltemplate"));
     incrementalPullSQLtemplate = new ST(templateContent);
   }
 

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkEnvCommand.java
Patch:
@@ -46,7 +46,7 @@ public void setEnv(@CliOption(key = {"conf"}, help = "Env config to be set") fin
     env.put(map[0].trim(), map[1].trim());
   }
 
-  @CliCommand(value = "show env all", help = "Show spark launcher envs")
+  @CliCommand(value = "show envs all", help = "Show spark launcher envs")
   public String showAllEnv() {
     String[][] rows = new String[env.size()][2];
     int i = 0;

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java
Patch:
@@ -156,7 +156,7 @@ public void snapshot(JavaSparkContext jsc, String baseDir, final String outputDi
     // Create the _SUCCESS tag
     Path successTagPath = new Path(outputDir + "/_SUCCESS");
     if (!fs.exists(successTagPath)) {
-      LOG.info(String.format("Creating _SUCCESS under targetBasePath: $s", outputDir));
+      LOG.info(String.format("Creating _SUCCESS under targetBasePath: %s", outputDir));
       fs.createNewFile(successTagPath);
     }
   }

File: hudi-cli/src/main/java/org/apache/hudi/cli/HoodieHistoryFileNameProvider.java
Patch:
@@ -30,6 +30,7 @@
 @Order(Ordered.HIGHEST_PRECEDENCE)
 public class HoodieHistoryFileNameProvider extends DefaultHistoryFileNameProvider {
 
+  @Override
   public String getHistoryFileName() {
     return "hoodie-cmd.log";
   }

File: hudi-cli/src/main/java/org/apache/hudi/cli/HoodieSplashScreen.java
Patch:
@@ -50,14 +50,17 @@ public class HoodieSplashScreen extends DefaultBannerProvider {
       + "*                                                                 *" + OsUtils.LINE_SEPARATOR
       + "===================================================================" + OsUtils.LINE_SEPARATOR;
 
+  @Override
   public String getBanner() {
     return screen;
   }
 
+  @Override
   public String getVersion() {
     return "1.0";
   }
 
+  @Override
   public String getWelcomeMessage() {
     return "Welcome to Apache Hudi CLI. Please type help if you are looking for help. ";
   }

File: hudi-client/src/main/java/org/apache/hudi/AbstractHoodieClient.java
Patch:
@@ -72,6 +72,7 @@ protected AbstractHoodieClient(JavaSparkContext jsc, HoodieWriteConfig clientCon
   /**
    * Releases any resources used by the client.
    */
+  @Override
   public void close() {
     stopEmbeddedServerView(true);
   }

File: hudi-client/src/main/java/org/apache/hudi/HoodieWriteClient.java
Patch:
@@ -973,6 +973,7 @@ private void rollbackInternal(String commitToRollback) {
   /**
    * Releases any resources used by the client.
    */
+  @Override
   public void close() {
     // Stop timeline-server if running
     super.close();

File: hudi-client/src/main/java/org/apache/hudi/func/SparkBoundedInMemoryExecutor.java
Patch:
@@ -48,6 +48,7 @@ public SparkBoundedInMemoryExecutor(final HoodieWriteConfig hoodieConfig, Bounde
     this.sparkThreadTaskContext = TaskContext.get();
   }
 
+  @Override
   public void preExecute() {
     // Passing parent thread's TaskContext to newly launched thread for it to access original TaskContext properties.
     TaskContext$.MODULE$.setTaskContext(sparkThreadTaskContext);

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/BloomIndexFileInfo.java
Patch:
@@ -88,6 +88,7 @@ public int hashCode() {
     return Objects.hashCode(fileId, minRecordKey, maxRecordKey);
   }
 
+  @Override
   public String toString() {
     final StringBuilder sb = new StringBuilder("BloomIndexFileInfo {");
     sb.append(" fileId=").append(fileId);

File: hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java
Patch:
@@ -154,6 +154,7 @@ private Connection getHBaseConnection() {
    */
   private void addShutDownHook() {
     Runtime.getRuntime().addShutdownHook(new Thread() {
+      @Override
       public void run() {
         try {
           hbaseConnection.close();
@@ -167,6 +168,7 @@ public void run() {
   /**
    * Ensure that any resources used for indexing are released here.
    */
+  @Override
   public void close() {
     this.hBaseIndexQPSResourceAllocator.releaseQPSResources();
   }

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java
Patch:
@@ -94,6 +94,7 @@ public boolean canWrite(HoodieRecord record) {
   /**
    * Perform the actual writing of the given record into the backing file.
    */
+  @Override
   public void write(HoodieRecord record, Option<IndexedRecord> avroRecord) {
     Option recordMetadata = record.getData().getMetadata();
     try {

File: hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieParquetWriter.java
Patch:
@@ -93,6 +93,7 @@ public void writeAvroWithMetadata(R avroRecord, HoodieRecord record) throws IOEx
     writeSupport.add(record.getRecordKey());
   }
 
+  @Override
   public boolean canWrite() {
     return fs.getBytesWritten(file) < maxFileSize;
   }

File: hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java
Patch:
@@ -279,6 +279,7 @@ public Iterator<List<WriteStatus>> handleInsertPartition(String commitTime, Inte
    * @param jsc JavaSparkContext
    * @return Cleaner Plan
    */
+  @Override
   public HoodieCleanerPlan scheduleClean(JavaSparkContext jsc) {
     try {
       HoodieCleanHelper cleaner = new HoodieCleanHelper(this, config);

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java
Patch:
@@ -108,6 +108,7 @@ public HoodieLogFile getLogFile() {
    */
   private void addShutDownHook() {
     Runtime.getRuntime().addShutdownHook(new Thread() {
+      @Override
       public void run() {
         try {
           close();

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java
Patch:
@@ -107,6 +107,7 @@ public FileSystem getFs() {
     return fs;
   }
 
+  @Override
   public HoodieLogFile getLogFile() {
     return logFile;
   }
@@ -212,6 +213,7 @@ private void flush() throws IOException {
     output.hsync();
   }
 
+  @Override
   public long getCurrentSize() throws IOException {
     if (output == null) {
       throw new IllegalStateException("Cannot get current size as the underlying stream has been closed already");

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -145,6 +145,7 @@ public HoodieTimeline getCommitsTimeline() {
    * timeline * With Async compaction a requested/inflight compaction-instant is a valid baseInstant for a file-slice as
    * there could be delta-commits with that baseInstant.
    */
+  @Override
   public HoodieTimeline getCommitsAndCompactionTimeline() {
     return getTimelineOfActions(Sets.newHashSet(COMMIT_ACTION, DELTA_COMMIT_ACTION, COMPACTION_ACTION));
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -182,6 +182,7 @@ protected List<HoodieFileGroup> buildFileGroups(Stream<HoodieDataFile> dataFileS
   /**
    * Clears the partition Map and reset view states.
    */
+  @Override
   public final void reset() {
     try {
       writeLock.lock();
@@ -380,6 +381,7 @@ public final Option<HoodieDataFile> getDataFileOn(String partitionStr, String in
   /**
    * Get Latest data file for a partition and file-Id.
    */
+  @Override
   public final Option<HoodieDataFile> getLatestDataFile(String partitionStr, String fileId) {
     try {
       readLock.lock();
@@ -434,6 +436,7 @@ public final Stream<FileSlice> getLatestFileSlices(String partitionStr) {
   /**
    * Get Latest File Slice for a given fileId in a given partition.
    */
+  @Override
   public final Option<FileSlice> getLatestFileSlice(String partitionStr, String fileId) {
     try {
       readLock.lock();

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java
Patch:
@@ -205,13 +205,15 @@ public Stream<HoodieFileGroup> fetchAllStoredFileGroups() {
     });
   }
 
+  @Override
   public void close() {
     closed = true;
     super.reset();
     partitionToFileGroupsMap = null;
     fgIdToPendingCompaction = null;
   }
 
+  @Override
   public boolean isClosed() {
     return closed;
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java
Patch:
@@ -76,6 +76,7 @@ protected Map<String, List<HoodieFileGroup>> createPartitionToFileGroups() {
     }
   }
 
+  @Override
   protected Map<HoodieFileGroupId, Pair<String, CompactionOperation>> createFileIdToPendingCompactionMap(
       Map<HoodieFileGroupId, Pair<String, CompactionOperation>> fgIdToPendingCompaction) {
     try {
@@ -91,6 +92,7 @@ protected Map<HoodieFileGroupId, Pair<String, CompactionOperation>> createFileId
     }
   }
 
+  @Override
   public Stream<HoodieFileGroup> getAllFileGroups() {
     return ((ExternalSpillableMap) partitionToFileGroupsMap).valueStream()
         .flatMap(fg -> ((List<HoodieFileGroup>) fg).stream());

File: hudi-common/src/main/java/org/apache/hudi/common/util/ObjectSizeCalculator.java
Patch:
@@ -124,6 +124,7 @@ public static long getObjectSize(Object obj) throws UnsupportedOperationExceptio
 
   private final LoadingCache<Class<?>, ClassSizeInfo> classSizeInfos =
       CacheBuilder.newBuilder().build(new CacheLoader<Class<?>, ClassSizeInfo>() {
+        @Override
         public ClassSizeInfo load(Class<?> clazz) {
           return new ClassSizeInfo(clazz);
         }

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/DiskBasedMap.java
Patch:
@@ -125,6 +125,7 @@ private void initFile(File writeOnlyFile) throws IOException {
    */
   private void addShutDownHook() {
     Runtime.getRuntime().addShutdownHook(new Thread() {
+      @Override
       public void run() {
         try {
           if (writeOnlyFileHandle != null) {

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/LazyFileIterable.java
Patch:
@@ -115,6 +115,7 @@ private void close() {
 
     private void addShutdownHook() {
       Runtime.getRuntime().addShutdownHook(new Thread() {
+        @Override
         public void run() {
           close();
         }

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java
Patch:
@@ -173,10 +173,12 @@ private Map<HoodieTableMetaClient, List<FileStatus>> groupFileStatus(FileStatus[
     return grouped;
   }
 
+  @Override
   public void setConf(Configuration conf) {
     this.conf = conf;
   }
 
+  @Override
   public Configuration getConf() {
     return conf;
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -554,6 +554,7 @@ private HoodieInstant fetchNextCompactionInstant() throws InterruptedException {
     /**
      * Start Compaction Service.
      */
+    @Override
     protected Pair<CompletableFuture, ExecutorService> startService() {
       ExecutorService executor = Executors.newFixedThreadPool(maxConcurrentCompaction);
       List<CompletableFuture<Boolean>> compactionFutures =

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -472,8 +472,8 @@ public void saveToCleanRequested(HoodieInstant instant, Option<byte[]> content)
     Preconditions.checkArgument(instant.getState().equals(State.REQUESTED));
     // Write workload to auxiliary folder
     createFileInAuxiliaryFolder(instant, content);
-    // Plan is only stored in auxiliary folder
-    createFileInMetaPath(instant.getFileName(), Option.empty(), false);
+    // Plan is only stored in auxiliary folder, here stored in meta path
+    createFileInMetaPath(instant.getFileName(), content, false);
   }
 
   private void createFileInMetaPath(String filename, Option<byte[]> content, boolean allowOverwrite) {

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieCommitArchiveLog.java
Patch:
@@ -252,7 +252,7 @@ public void archive(List<HoodieInstant> instants) throws HoodieCommitException {
 
         // filter empty instant, like *.commit.requested
         byte[] instantDetails = commitTimeline.getInstantDetails(hoodieInstant).get();
-        if (instantDetails == null || instantDetails.length == 0 ) {
+        if (instantDetails == null || instantDetails.length == 0) {
           continue;
         }
 

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieTableType.java
Patch:
@@ -21,14 +21,14 @@
 /**
  * Type of the Hoodie Table.
  * <p>
- * Currently, 1 type is supported
+ * Currently, 2 types are supported.
  * <p>
  * COPY_ON_WRITE - Performs upserts by versioning entire files, with later versions containing newer value of a record.
  * <p>
- * In the future, following might be added.
- * <p>
  * MERGE_ON_READ - Speeds up upserts, by delaying merge until enough work piles up.
  * <p>
+ * In the future, following might be added.
+ * <p>
  * SIMPLE_LSM - A simple 2 level LSM tree.
  */
 public enum HoodieTableType {

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java
Patch:
@@ -215,7 +215,7 @@ public String showLogFileRecords(
           if (n instanceof HoodieAvroDataBlock) {
             HoodieAvroDataBlock blk = (HoodieAvroDataBlock) n;
             List<IndexedRecord> records = blk.getRecords();
-            for(IndexedRecord record : records) {
+            for (IndexedRecord record : records) {
               if (allRecords.size() >= limit) {
                 break;
               }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -74,7 +74,7 @@
  *
  * In continuous mode, DeltaStreamer runs in loop-mode going through the below operations (a) pull-from-source (b)
  * write-to-sink (c) Schedule Compactions if needed (d) Conditionally Sync to Hive each cycle. For MOR table with
- * continuous mode enabled, a seperate compactor thread is allocated to execute compactions
+ * continuous mode enabled, a separate compactor thread is allocated to execute compactions
  */
 public class HoodieDeltaStreamer implements Serializable {
 

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java
Patch:
@@ -227,7 +227,7 @@ public String showLogFileRecords(
         }
       }
     }
-    String[][] rows = new String[allRecords.size() + 1][];
+    String[][] rows = new String[allRecords.size()][];
     int i = 0;
     for (IndexedRecord record : allRecords) {
       String[] data = new String[1];

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java
Patch:
@@ -77,7 +77,7 @@ public String addPartitionMeta(
     List<String> partitionPaths =
         FSUtils.getAllPartitionFoldersThreeLevelsDown(HoodieCLI.fs, client.getBasePath());
     Path basePath = new Path(client.getBasePath());
-    String[][] rows = new String[partitionPaths.size() + 1][];
+    String[][] rows = new String[partitionPaths.size()][];
 
     int ind = 0;
     for (String partition : partitionPaths) {

File: hudi-cli/src/main/java/org/apache/hudi/cli/Table.java
Patch:
@@ -85,7 +85,7 @@ public Table add(List<Comparable> row) {
   /**
    * Add all rows.
    * 
-   * @param rows Rows to be aded
+   * @param rows Rows to be added
    * @return
    */
   public Table addAll(List<List<Comparable>> rows) {

File: hudi-client/src/main/java/org/apache/hudi/HoodieWriteClient.java
Patch:
@@ -92,7 +92,7 @@
  * Hoodie Write Client helps you build datasets on HDFS [insert()] and then perform efficient mutations on an HDFS
  * dataset [upsert()]
  * <p>
- * Note that, at any given time, there can only be one Spark job performing these operatons on a Hoodie dataset.
+ * Note that, at any given time, there can only be one Spark job performing these operations on a Hoodie dataset.
  */
 public class HoodieWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieClient {
 

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java
Patch:
@@ -105,7 +105,6 @@ public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD,
       recordRDD.unpersist(); // unpersist the input Record RDD
       keyFilenamePairRDD.unpersist();
     }
-
     return taggedRecordRDD;
   }
 
@@ -321,8 +320,9 @@ JavaRDD<Tuple2<String, HoodieKey>> explodeRecordRDDWithFileComparisons(
       String recordKey = partitionRecordKeyPair._2();
       String partitionPath = partitionRecordKeyPair._1();
 
-      return indexFileFilter.getMatchingFiles(partitionPath, recordKey).stream()
-          .map(matchingFile -> new Tuple2<>(matchingFile, new HoodieKey(recordKey, partitionPath)))
+      return indexFileFilter.getMatchingFilesAndPartition(partitionPath, recordKey).stream()
+          .map(partitionFileIdPair -> new Tuple2<>(partitionFileIdPair.getRight(),
+              new HoodieKey(recordKey, partitionPath)))
           .collect(Collectors.toList());
     }).flatMap(List::iterator);
   }

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/KeyRangeNode.java
Patch:
@@ -73,8 +73,8 @@ public String toString() {
    *
    * @param that the {@link KeyRangeNode} to be compared with
    * @return the result of comparison. 0 if both min and max are equal in both. 1 if this {@link KeyRangeNode} is
-   *         greater than the {@code that} keyRangeNode. -1 if {@code that} keyRangeNode is greater than this
-   *         {@link KeyRangeNode}
+   * greater than the {@code that} keyRangeNode. -1 if {@code that} keyRangeNode is greater than this {@link
+   * KeyRangeNode}
    */
   @Override
   public int compareTo(KeyRangeNode that) {

File: hudi-client/src/main/java/org/apache/hudi/HoodieWriteClient.java
Patch:
@@ -89,7 +89,7 @@
 import scala.Tuple2;
 
 /**
- * Hoodie Write Client helps you build datasets on HDFS [insert()] and then perform efficient mutations on a HDFS
+ * Hoodie Write Client helps you build datasets on HDFS [insert()] and then perform efficient mutations on an HDFS
  * dataset [upsert()]
  * <p>
  * Note that, at any given time, there can only be one Spark job performing these operatons on a Hoodie dataset.

File: hudi-common/src/test/java/org/apache/hudi/common/bloom/filter/TestBloomFilter.java
Patch:
@@ -39,7 +39,7 @@ public class TestBloomFilter {
 
   private final String versionToTest;
 
-  // name attribute is optional, provide an unique name for test
+  // name attribute is optional, provide a unique name for test
   // multiple parameters, uses Collection<Object[]>
   @Parameters()
   public static Collection<Object[]> data() {

File: hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBDAO.java
Patch:
@@ -121,7 +121,7 @@ protected void log(InfoLogLevel infoLogLevel, String logMsg) {
         managedDescriptorMap.put(familyNameFromDescriptor, descriptor);
       }
     } catch (RocksDBException | IOException re) {
-      LOG.error("Got exception opening rocks db instance ", re);
+      LOG.error("Got exception opening Rocks DB instance ", re);
       throw new HoodieException(re);
     }
   }

File: hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
Patch:
@@ -67,7 +67,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {
   public static final String COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = "hoodie.copyonwrite.insert.auto.split";
   // its off by default
   public static final String DEFAULT_COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = String.valueOf(true);
-  // This value is used as a guessimate for the record size, if we can't determine this from
+  // This value is used as a guesstimate for the record size, if we can't determine this from
   // previous commits
   public static final String COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE = "hoodie.copyonwrite.record.size.estimate";
   // Used to determine how much more can be packed into a small file, before it exceeds the size

File: hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java
Patch:
@@ -109,7 +109,7 @@ private static PairFlatMapFunction<Iterator<Tuple2<String, String>>, String, Par
         Tuple2<String, String> partitionDelFileTuple = iter.next();
         String partitionPath = partitionDelFileTuple._1();
         String delFileName = partitionDelFileTuple._2();
-        Path deletePath = new Path(new Path(basePath, partitionPath), delFileName);
+        Path deletePath = FSUtils.getPartitionPath(FSUtils.getPartitionPath(basePath, partitionPath), delFileName);
         String deletePathStr = deletePath.toString();
         Boolean deletedFileResult = deleteFileAndGetResult(fs, deletePathStr);
         if (!partitionCleanStatMap.containsKey(partitionPath)) {

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/Source.java
Patch:
@@ -36,7 +36,7 @@ public abstract class Source<T> implements Serializable {
   private static final Logger LOG = LogManager.getLogger(Source.class);
 
   public enum SourceType {
-    JSON, AVRO, ROW, PARQUET
+    JSON, AVRO, ROW
   }
 
   protected transient TypedProperties props;

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/UtilitiesTestBase.java
Patch:
@@ -61,7 +61,6 @@
 /**
  * Abstract test that provides a dfs & spark contexts.
  *
- * TODO(vc): this needs to be done across the board.
  */
 public class UtilitiesTestBase {
 

File: hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestDFSSource.java
Patch:
@@ -139,7 +139,7 @@ public void testParquetDFSSource() throws IOException {
 
     TypedProperties props = new TypedProperties();
     props.setProperty("hoodie.deltastreamer.source.dfs.root", dfsBasePath + "/parquetFiles");
-    ParquetSource parquetDFSSource = new ParquetDFSSource(props, jsc, sparkSession, schemaProvider);
+    ParquetDFSSource parquetDFSSource = new ParquetDFSSource(props, jsc, sparkSession, schemaProvider);
     SourceFormatAdapter parquetSource = new SourceFormatAdapter(parquetDFSSource);
 
     // 1. Extract without any checkpoint => get all the data, respecting sourceLimit

File: hudi-client/src/test/java/org/apache/hudi/table/TestCopyOnWriteTable.java
Patch:
@@ -20,11 +20,11 @@
 
 import org.apache.hudi.HoodieClientTestHarness;
 import org.apache.hudi.WriteStatus;
-import org.apache.hudi.common.BloomFilter;
 import org.apache.hudi.common.HoodieClientTestUtils;
 import org.apache.hudi.common.HoodieTestDataGenerator;
 import org.apache.hudi.common.TestRawTripPayload;
 import org.apache.hudi.common.TestRawTripPayload.MetadataMergeWriteStatus;
+import org.apache.hudi.common.bloom.filter.BloomFilter;
 import org.apache.hudi.common.model.HoodieKey;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordLocation;

File: hudi-client/src/test/java/org/apache/hudi/common/HoodieMergeOnReadTestUtils.java
Patch:
@@ -46,7 +46,8 @@ public class HoodieMergeOnReadTestUtils {
   public static List<GenericRecord> getRecordsUsingInputFormat(List<String> inputPaths, String basePath)
       throws IOException {
     JobConf jobConf = new JobConf();
-    Schema schema = HoodieAvroUtils.addMetadataFields(Schema.parse(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA));
+    Schema schema = HoodieAvroUtils.addMetadataFields(
+        new Schema.Parser().parse(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA));
     HoodieParquetRealtimeInputFormat inputFormat = new HoodieParquetRealtimeInputFormat();
     setPropsForInputFormat(inputFormat, jobConf, schema, basePath);
     return inputPaths.stream().map(path -> {

File: hudi-client/src/test/java/org/apache/hudi/func/TestUpdateMapFunction.java
Patch:
@@ -100,7 +100,7 @@ public void testSchemaEvolutionOnUpdate() throws Exception {
     // Now try an update with an evolved schema
     // Evolved schema does not have guarantee on preserving the original field ordering
     final HoodieWriteConfig config2 = makeHoodieClientConfig("/exampleEvolvedSchema.txt");
-    final Schema schema = Schema.parse(config2.getSchema());
+    final Schema schema = new Schema.Parser().parse(config2.getSchema());
     final WriteStatus insertResult = statuses.get(0);
     String fileId = insertResult.getFileId();
 

File: hudi-common/src/main/java/org/apache/hudi/common/io/storage/SizeAwareFSDataOutputStream.java
Patch:
@@ -43,7 +43,7 @@ public class SizeAwareFSDataOutputStream extends FSDataOutputStream {
 
   public SizeAwareFSDataOutputStream(Path path, FSDataOutputStream out, ConsistencyGuard consistencyGuard,
       Runnable closeCallback) throws IOException {
-    super(out);
+    super(out, null);
     this.path = path;
     this.closeCallback = closeCallback;
     this.consistencyGuard = consistencyGuard;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java
Patch:
@@ -62,7 +62,7 @@ public HoodieAvroDataBlock(@Nonnull List<IndexedRecord> records, @Nonnull Map<He
       @Nonnull Map<HeaderMetadataType, String> footer) {
     super(header, footer, Option.empty(), Option.empty(), null, false);
     this.records = records;
-    this.schema = Schema.parse(super.getLogBlockHeader().get(HeaderMetadataType.SCHEMA));
+    this.schema = new Schema.Parser().parse(super.getLogBlockHeader().get(HeaderMetadataType.SCHEMA));
   }
 
   public HoodieAvroDataBlock(@Nonnull List<IndexedRecord> records, @Nonnull Map<HeaderMetadataType, String> header) {
@@ -97,7 +97,7 @@ public byte[] getContentBytes() throws IOException {
       createRecordsFromContentBytes();
     }
 
-    Schema schema = Schema.parse(super.getLogBlockHeader().get(HeaderMetadataType.SCHEMA));
+    Schema schema = new Schema.Parser().parse(super.getLogBlockHeader().get(HeaderMetadataType.SCHEMA));
     GenericDatumWriter<IndexedRecord> writer = new GenericDatumWriter<>(schema);
     ByteArrayOutputStream baos = new ByteArrayOutputStream();
     DataOutputStream output = new DataOutputStream(baos);

File: hudi-common/src/main/java/org/apache/hudi/common/util/LogReaderUtils.java
Patch:
@@ -51,7 +51,7 @@ private static Schema readSchemaFromLogFileInReverse(FileSystem fs, HoodieActive
         HoodieAvroDataBlock lastBlock = (HoodieAvroDataBlock) block;
         if (completedTimeline
             .containsOrBeforeTimelineStarts(lastBlock.getLogBlockHeader().get(HeaderMetadataType.INSTANT_TIME))) {
-          writerSchema = Schema.parse(lastBlock.getLogBlockHeader().get(HeaderMetadataType.SCHEMA));
+          writerSchema = new Schema.Parser().parse(lastBlock.getLogBlockHeader().get(HeaderMetadataType.SCHEMA));
           break;
         }
       }

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java
Patch:
@@ -141,7 +141,7 @@ public boolean empty() {
 
   @Override
   public int countInstants() {
-    return new Long(instants.stream().count()).intValue();
+    return instants.size();
   }
 
   @Override

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
Patch:
@@ -190,7 +190,9 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jsc, String
             .withCompactionStrategy(ReflectionUtils.loadClass(strategy)).build())
         .orElse(HoodieCompactionConfig.newBuilder().withInlineCompaction(false).build());
     HoodieWriteConfig config =
-        HoodieWriteConfig.newBuilder().withPath(basePath).withParallelism(parallelism, parallelism)
+        HoodieWriteConfig.newBuilder().withPath(basePath)
+            .withParallelism(parallelism, parallelism)
+            .withBulkInsertParallelism(parallelism)
             .withSchema(schemaStr).combineInput(true, true).withCompactionConfig(compactionConfig)
             .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())
             .withProps(properties).build();

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java
Patch:
@@ -329,7 +329,7 @@ private void init() throws IOException {
     // Add partitioning fields to writer schema for resulting row to contain null values for these fields
     String partitionFields = jobConf.get(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, "");
     List<String> partitioningFields =
-        partitionFields.length() > 0 ? Arrays.stream(partitionFields.split(",")).collect(Collectors.toList())
+        partitionFields.length() > 0 ? Arrays.stream(partitionFields.split("/")).collect(Collectors.toList())
             : new ArrayList<>();
     writerSchema = addPartitionFields(writerSchema, partitioningFields);
     List<String> projectionFields = orderFields(jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR),

File: hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java
Patch:
@@ -88,8 +88,8 @@ public void testDemo() throws Exception {
   }
 
   private void setupDemo() throws Exception {
-    List<String> cmds = new ImmutableList.Builder<String>().add("hdfs dfsadmin -safemode wait") // handle NN going into
-                                                                                                // safe mode at times
+    List<String> cmds = new ImmutableList.Builder<String>()
+        .add("hdfs dfsadmin -safemode wait") // handle NN going into safe mode at times
         .add("hdfs dfs -mkdir -p " + HDFS_DATA_DIR)
         .add("hdfs dfs -copyFromLocal -f " + INPUT_BATCH_PATH1 + " " + HDFS_BATCH_PATH1)
         .add("/bin/bash " + DEMO_CONTAINER_SCRIPT).build();

File: hudi-hive/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
Patch:
@@ -39,6 +39,7 @@
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.Types;
 import org.joda.time.DateTime;
+import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
 import org.junit.runner.RunWith;
@@ -65,7 +66,7 @@ public void setUp() throws IOException, InterruptedException, URISyntaxException
     TestUtil.setUp();
   }
 
-  @Before
+  @After
   public void teardown() throws IOException, InterruptedException {
     TestUtil.clear();
   }

File: hudi-client/src/test/java/org/apache/hudi/common/TestRawTripPayload.java
Patch:
@@ -95,8 +95,8 @@ public Option<IndexedRecord> getInsertValue(Schema schema) throws IOException {
     if (isDeleted) {
       return Option.empty();
     } else {
-      MercifulJsonConverter jsonConverter = new MercifulJsonConverter(schema);
-      return Option.of(jsonConverter.convert(getJsonData()));
+      MercifulJsonConverter jsonConverter = new MercifulJsonConverter();
+      return Option.of(jsonConverter.convert(getJsonData(), schema));
     }
   }
 

File: hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroWriteSupport.java
Patch:
@@ -34,7 +34,7 @@ public class HoodieAvroWriteSupport extends AvroWriteSupport {
   private String minRecordKey;
   private String maxRecordKey;
 
-
+  public static final String OLD_HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY = "com.uber.hoodie.bloomfilter";
   public static final String HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY = "org.apache.hudi.bloomfilter";
   public static final String HOODIE_MIN_RECORD_KEY_FOOTER = "hoodie_min_record_key";
   public static final String HOODIE_MAX_RECORD_KEY_FOOTER = "hoodie_max_record_key";

File: hudi-common/src/main/java/org/apache/hudi/common/HoodieJsonPayload.java
Patch:
@@ -56,8 +56,8 @@ public Option<IndexedRecord> combineAndGetUpdateValue(IndexedRecord oldRec, Sche
 
   @Override
   public Option<IndexedRecord> getInsertValue(Schema schema) throws IOException {
-    MercifulJsonConverter jsonConverter = new MercifulJsonConverter(schema);
-    return Option.of(jsonConverter.convert(getJsonData()));
+    MercifulJsonConverter jsonConverter = new MercifulJsonConverter();
+    return Option.of(jsonConverter.convert(getJsonData(), schema));
   }
 
   private String getJsonData() throws IOException {

File: hudi-common/src/test/java/org/apache/hudi/common/util/SchemaTestUtil.java
Patch:
@@ -167,7 +167,7 @@ public static Schema getComplexEvolvedSchema() throws IOException {
   public static GenericRecord generateAvroRecordFromJson(Schema schema, int recordNumber, String commitTime,
       String fileId) throws IOException {
     TestRecord record = new TestRecord(commitTime, recordNumber, fileId);
-    MercifulJsonConverter converter = new MercifulJsonConverter(schema);
-    return converter.convert(record.toJsonString());
+    MercifulJsonConverter converter = new MercifulJsonConverter();
+    return converter.convert(record.toJsonString(), schema);
   }
 }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/AvroConvertor.java
Patch:
@@ -75,15 +75,15 @@ private void initInjection() {
 
   private void initJsonConvertor() {
     if (jsonConverter == null) {
-      jsonConverter = new MercifulJsonConverter(schema);
+      jsonConverter = new MercifulJsonConverter();
     }
   }
 
 
   public GenericRecord fromJson(String json) throws IOException {
     initSchema();
     initJsonConvertor();
-    return jsonConverter.convert(json);
+    return jsonConverter.convert(json, schema);
   }
 
   public Schema getSchema() {

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -140,7 +140,8 @@ private void init(HoodieRecord record) {
         throw new HoodieUpsertException("Failed to initialize HoodieAppendHandle for FileId: " + fileId + " on commit "
             + instantTime + " on HDFS path " + hoodieTable.getMetaClient().getBasePath() + partitionPath, e);
       }
-      Path path = new Path(partitionPath, writer.getLogFile().getFileName());
+      Path path = partitionPath.length() == 0 ? new Path(writer.getLogFile().getFileName())
+          : new Path(partitionPath, writer.getLogFile().getFileName());
       writeStatus.getStat().setPath(path.toString());
       doInit = false;
     }

File: hudi-client/src/test/java/org/apache/hudi/common/HoodieTestDataGenerator.java
Patch:
@@ -78,6 +78,7 @@ public class HoodieTestDataGenerator {
       + "{\"name\": \"begin_lat\", \"type\": \"double\"}," + "{\"name\": \"begin_lon\", \"type\": \"double\"},"
       + "{\"name\": \"end_lat\", \"type\": \"double\"}," + "{\"name\": \"end_lon\", \"type\": \"double\"},"
       + "{\"name\":\"fare\",\"type\": \"double\"}]}";
+  public static String TRIP_HIVE_COLUMN_TYPES = "double,string,string,string,double,double,double,double,double";
   public static Schema avroSchema = new Schema.Parser().parse(TRIP_EXAMPLE_SCHEMA);
   public static Schema avroSchemaWithMetadataFields = HoodieAvroUtils.addMetadataFields(avroSchema);
 

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java
Patch:
@@ -101,7 +101,7 @@ public boolean next(NullWritable aVoid, ArrayWritable arrayWritable) throws IOEx
         }
         // we assume, a later safe record in the log, is newer than what we have in the map &
         // replace it.
-        ArrayWritable aWritable = (ArrayWritable) avroToArrayWritable(recordToReturn, getWriterSchema());
+        ArrayWritable aWritable = (ArrayWritable) avroToArrayWritable(recordToReturn, getHiveSchema());
         Writable[] replaceValue = aWritable.get();
         if (LOG.isDebugEnabled()) {
           LOG.debug(String.format("key %s, base values: %s, log values: %s", key, arrayWritableToString(arrayWritable),

File: hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java
Patch:
@@ -40,8 +40,8 @@ public class SparkUtil {
   public static SparkLauncher initLauncher(String propertiesFile) throws URISyntaxException {
     String currentJar = new File(SparkUtil.class.getProtectionDomain().getCodeSource().getLocation().toURI().getPath())
         .getAbsolutePath();
-    SparkLauncher sparkLauncher = new SparkLauncher().setAppResource(currentJar)
-        .setMainClass(SparkMain.class.getName());
+    SparkLauncher sparkLauncher =
+        new SparkLauncher().setAppResource(currentJar).setMainClass(SparkMain.class.getName());
 
     if (!StringUtils.isNullOrEmpty(propertiesFile)) {
       sparkLauncher.setPropertiesFile(propertiesFile);

File: hudi-client/src/main/java/org/apache/hudi/exception/HoodieAppendException.java
Patch:
@@ -19,8 +19,9 @@
 package org.apache.hudi.exception;
 
 /**
- * <p> Exception thrown for any higher level errors when <code>HoodieClient</code> is doing a delta
- * commit </p>
+ * <p>
+ * Exception thrown for any higher level errors when <code>HoodieClient</code> is doing a delta commit
+ * </p>
  */
 public class HoodieAppendException extends HoodieException {
 

File: hudi-client/src/main/java/org/apache/hudi/exception/HoodieCommitException.java
Patch:
@@ -19,7 +19,8 @@
 package org.apache.hudi.exception;
 
 /**
- * <p> Exception thrown for any higher level errors when <code>HoodieClient</code> is doing a Commit
+ * <p>
+ * Exception thrown for any higher level errors when <code>HoodieClient</code> is doing a Commit
  * </p>
  */
 public class HoodieCommitException extends HoodieException {

File: hudi-client/src/main/java/org/apache/hudi/exception/HoodieDependentSystemUnavailableException.java
Patch:
@@ -20,7 +20,9 @@
 
 
 /**
- * <p> Exception thrown when dependent system is not available </p>
+ * <p>
+ * Exception thrown when dependent system is not available
+ * </p>
  */
 public class HoodieDependentSystemUnavailableException extends HoodieException {
 

File: hudi-client/src/main/java/org/apache/hudi/exception/HoodieInsertException.java
Patch:
@@ -19,8 +19,9 @@
 package org.apache.hudi.exception;
 
 /**
- * <p> Exception thrown for any higher level errors when <code>HoodieClient</code> is doing a bulk
- * insert </p>
+ * <p>
+ * Exception thrown for any higher level errors when <code>HoodieClient</code> is doing a bulk insert
+ * </p>
  */
 public class HoodieInsertException extends HoodieException {
 

File: hudi-client/src/main/java/org/apache/hudi/exception/HoodieUpsertException.java
Patch:
@@ -19,8 +19,9 @@
 package org.apache.hudi.exception;
 
 /**
- * <p> Exception thrown for any higher level errors when <code>HoodieClient</code> is doing a
- * incremental upsert </p>
+ * <p>
+ * Exception thrown for any higher level errors when <code>HoodieClient</code> is doing a incremental upsert
+ * </p>
  */
 public class HoodieUpsertException extends HoodieException {
 

File: hudi-client/src/main/java/org/apache/hudi/func/ParquetReaderIterator.java
Patch:
@@ -25,8 +25,8 @@
 import org.apache.parquet.hadoop.ParquetReader;
 
 /**
- * This class wraps a parquet reader and provides an iterator based api to
- * read from a parquet file. This is used in {@link BoundedInMemoryQueue}
+ * This class wraps a parquet reader and provides an iterator based api to read from a parquet file. This is used in
+ * {@link BoundedInMemoryQueue}
  */
 public class ParquetReaderIterator<T> implements Iterator<T> {
 

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/BucketizedBloomCheckPartitioner.java
Patch:
@@ -35,6 +35,7 @@
  * Partitions bloom filter checks by spreading out comparisons across buckets of work.
  *
  * Each bucket incurs the following cost
+ * 
  * <pre>
  *   1) Read bloom filter from file footer
  *   2) Check keys against bloom filter
@@ -47,6 +48,7 @@
  * could bound the amount of skew to std_dev(numberOfBucketsPerPartition) * cost of (3), lower than sort partitioning.
  *
  * Approach has two goals :
+ * 
  * <pre>
  *   1) Pack as many buckets from same file group into same partition, to amortize cost of (1) and (2) further
  *   2) Spread buckets across partitions evenly to achieve skew reduction
@@ -76,8 +78,7 @@ public BucketizedBloomCheckPartitioner(int targetPartitions, Map<String, Long> f
 
     Map<String, Integer> bucketsPerFileGroup = new HashMap<>();
     // Compute the buckets needed per file group, using simple uniform distribution
-    fileGroupToComparisons.forEach((f, c) ->
-        bucketsPerFileGroup.put(f, (int) Math.ceil((c * 1.0) / keysPerBucket)));
+    fileGroupToComparisons.forEach((f, c) -> bucketsPerFileGroup.put(f, (int) Math.ceil((c * 1.0) / keysPerBucket)));
     int totalBuckets = bucketsPerFileGroup.values().stream().mapToInt(i -> i).sum();
     // If totalBuckets > targetPartitions, no need to have extra partitions
     this.partitions = Math.min(targetPartitions, totalBuckets);

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/IntervalTreeBasedIndexFileFilter.java
Patch:
@@ -48,8 +48,8 @@ class IntervalTreeBasedIndexFileFilter implements IndexFileFilter {
       KeyRangeLookupTree lookUpTree = new KeyRangeLookupTree();
       bloomIndexFiles.forEach(indexFileInfo -> {
         if (indexFileInfo.hasKeyRanges()) {
-          lookUpTree.insert(new KeyRangeNode(indexFileInfo.getMinRecordKey(),
-              indexFileInfo.getMaxRecordKey(), indexFileInfo.getFileId()));
+          lookUpTree.insert(new KeyRangeNode(indexFileInfo.getMinRecordKey(), indexFileInfo.getMaxRecordKey(),
+              indexFileInfo.getFileId()));
         } else {
           if (!partitionToFilesWithNoRanges.containsKey(partition)) {
             partitionToFilesWithNoRanges.put(partition, new HashSet<>());

File: hudi-client/src/main/java/org/apache/hudi/index/bloom/ListBasedGlobalIndexFileFilter.java
Patch:
@@ -30,8 +30,7 @@ class ListBasedGlobalIndexFileFilter extends ListBasedIndexFileFilter {
    *
    * @param partitionToFileIndexInfo Map of partition to List of {@link BloomIndexFileInfo}
    */
-  ListBasedGlobalIndexFileFilter(
-      Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo) {
+  ListBasedGlobalIndexFileFilter(Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo) {
     super(partitionToFileIndexInfo);
   }
 

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java
Patch:
@@ -61,8 +61,7 @@ public HoodieWriteHandle(HoodieWriteConfig config, String instantTime, String fi
     this.writerSchema = createHoodieWriteSchema(originalSchema);
     this.timer = new HoodieTimer().startTimer();
     this.writeStatus = (WriteStatus) ReflectionUtils.loadClass(config.getWriteStatusClassName(),
-        !hoodieTable.getIndex().isImplicitWithStorage(),
-        config.getWriteStatusFailureFraction());
+        !hoodieTable.getIndex().isImplicitWithStorage(), config.getWriteStatusFailureFraction());
   }
 
   /**
@@ -104,7 +103,7 @@ protected void createMarkerFile(String partitionPath) {
   }
 
   /**
-   * THe marker path will be  <base-path>/.hoodie/.temp/<instant_ts>/2019/04/25/filename
+   * THe marker path will be <base-path>/.hoodie/.temp/<instant_ts>/2019/04/25/filename
    */
   private Path makeNewMarkerPath(String partitionPath) {
     Path markerRootPath = new Path(hoodieTable.getMetaClient().getMarkerFolderPath(instantTime));

File: hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/BoundedIOCompactionStrategy.java
Patch:
@@ -25,8 +25,8 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 
 /**
- * CompactionStrategy which looks at total IO to be done for the compaction (read + write) and
- * limits the list of compactions to be under a configured limit on the IO
+ * CompactionStrategy which looks at total IO to be done for the compaction (read + write) and limits the list of
+ * compactions to be under a configured limit on the IO
  *
  * @see CompactionStrategy
  */

File: hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/UnBoundedCompactionStrategy.java
Patch:
@@ -24,9 +24,8 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 
 /**
- * UnBoundedCompactionStrategy will not change ordering or filter any compaction. It is a
- * pass-through and will compact all the base files which has a log file. This usually means
- * no-intelligence on compaction.
+ * UnBoundedCompactionStrategy will not change ordering or filter any compaction. It is a pass-through and will compact
+ * all the base files which has a log file. This usually means no-intelligence on compaction.
  *
  * @see CompactionStrategy
  */

File: hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieParquetConfig.java
Patch:
@@ -32,9 +32,8 @@ public class HoodieParquetConfig {
   private Configuration hadoopConf;
   private double compressionRatio;
 
-  public HoodieParquetConfig(HoodieAvroWriteSupport writeSupport,
-      CompressionCodecName compressionCodecName, int blockSize, int pageSize, long maxFileSize,
-      Configuration hadoopConf, double compressionRatio) {
+  public HoodieParquetConfig(HoodieAvroWriteSupport writeSupport, CompressionCodecName compressionCodecName,
+      int blockSize, int pageSize, long maxFileSize, Configuration hadoopConf, double compressionRatio) {
     this.writeSupport = writeSupport;
     this.compressionCodecName = compressionCodecName;
     this.blockSize = blockSize;

File: hudi-client/src/main/java/org/apache/hudi/metrics/MetricsReporterType.java
Patch:
@@ -19,8 +19,7 @@
 package org.apache.hudi.metrics;
 
 /**
- * Types of the reporter. Right now we only support Graphite. We can include JMX and CSV in the
- * future.
+ * Types of the reporter. Right now we only support Graphite. We can include JMX and CSV in the future.
  */
 public enum MetricsReporterType {
   GRAPHITE, INMEMORY

File: hudi-client/src/test/java/org/apache/hudi/TestWriteStatus.java
Patch:
@@ -28,14 +28,14 @@
 
 public class TestWriteStatus {
   @Test
-  public void testFailureFraction() throws IOException  {
+  public void testFailureFraction() throws IOException {
     WriteStatus status = new WriteStatus(true, 0.1);
     Throwable t = new Exception("some error in writing");
     for (int i = 0; i < 1000; i++) {
       status.markFailure(Mockito.mock(HoodieRecord.class), t, null);
     }
     assertTrue(status.getFailedRecords().size() > 0);
-    assertTrue(status.getFailedRecords().size() < 150); //150 instead of 100, to prevent flaky test
+    assertTrue(status.getFailedRecords().size() < 150); // 150 instead of 100, to prevent flaky test
     assertTrue(status.hasErrors());
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/SerializableConfiguration.java
Patch:
@@ -57,8 +57,7 @@ private void readObject(ObjectInputStream in) throws IOException {
   @Override
   public String toString() {
     StringBuilder str = new StringBuilder();
-    configuration.iterator().forEachRemaining(e ->
-        str.append(String.format("%s => %s \n", e.getKey(), e.getValue())));
+    configuration.iterator().forEachRemaining(e -> str.append(String.format("%s => %s \n", e.getKey(), e.getValue())));
     return configuration.toString();
   }
 }

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCleaningPolicy.java
Patch:
@@ -19,6 +19,5 @@
 package org.apache.hudi.common.model;
 
 public enum HoodieCleaningPolicy {
-  KEEP_LATEST_FILE_VERSIONS,
-  KEEP_LATEST_COMMITS
+  KEEP_LATEST_FILE_VERSIONS, KEEP_LATEST_COMMITS
 }

File: hudi-common/src/main/java/org/apache/hudi/common/model/HoodieTableType.java
Patch:
@@ -23,14 +23,13 @@
  * <p>
  * Currently, 1 type is supported
  * <p>
- * COPY_ON_WRITE - Performs upserts by versioning entire files, with later versions containing newer
- * value of a record.
+ * COPY_ON_WRITE - Performs upserts by versioning entire files, with later versions containing newer value of a record.
  * <p>
  * In the future, following might be added.
  * <p>
  * MERGE_ON_READ - Speeds up upserts, by delaying merge until enough work piles up.
  * <p>
- * SIMPLE_LSM    - A simple 2 level LSM tree.
+ * SIMPLE_LSM - A simple 2 level LSM tree.
  */
 public enum HoodieTableType {
   COPY_ON_WRITE, MERGE_ON_READ

File: hudi-common/src/main/java/org/apache/hudi/common/storage/StorageSchemes.java
Patch:
@@ -33,8 +33,7 @@ public enum StorageSchemes {
   // Apache Ignite FS
   IGNITE("igfs", true),
   // AWS S3
-  S3A("s3a", false),
-  S3("s3", false),
+  S3A("s3a", false), S3("s3", false),
   // Google Cloud Storage
   GCS("gs", false),
   // View FS for federated setups. If federating across cloud stores, then append support is false

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatVersion.java
Patch:
@@ -19,8 +19,7 @@
 package org.apache.hudi.common.table.log;
 
 /**
- * Implements logic to determine behavior for feature flags for
- * {@link HoodieLogFormat.LogFormatVersion}.
+ * Implements logic to determine behavior for feature flags for {@link HoodieLogFormat.LogFormatVersion}.
  */
 final class HoodieLogFormatVersion extends HoodieLogFormat.LogFormatVersion {
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieUnMergedLogRecordScanner.java
Patch:
@@ -29,9 +29,8 @@ public class HoodieUnMergedLogRecordScanner extends AbstractHoodieLogRecordScann
 
   private final LogRecordScannerCallback callback;
 
-  public HoodieUnMergedLogRecordScanner(FileSystem fs, String basePath,
-      List<String> logFilePaths, Schema readerSchema, String latestInstantTime,
-      boolean readBlocksLazily, boolean reverseReader, int bufferSize,
+  public HoodieUnMergedLogRecordScanner(FileSystem fs, String basePath, List<String> logFilePaths, Schema readerSchema,
+      String latestInstantTime, boolean readBlocksLazily, boolean reverseReader, int bufferSize,
       LogRecordScannerCallback callback) {
     super(fs, basePath, logFilePaths, readerSchema, latestInstantTime, readBlocksLazily, reverseReader, bufferSize);
     this.callback = callback;

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlockVersion.java
Patch:
@@ -19,8 +19,8 @@
 package org.apache.hudi.common.table.log.block;
 
 /**
- * A set of feature flags associated with a data log block format. Versions are changed when the log
- * block format changes. TODO(na) - Implement policies around major/minor versions
+ * A set of feature flags associated with a data log block format. Versions are changed when the log block format
+ * changes. TODO(na) - Implement policies around major/minor versions
  */
 final class HoodieAvroDataBlockVersion extends HoodieLogBlockVersion {
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieCommandBlockVersion.java
Patch:
@@ -19,8 +19,8 @@
 package org.apache.hudi.common.table.log.block;
 
 /**
- * A set of feature flags associated with a command log block format. Versions are changed when the
- * log block format changes. TODO(na) - Implement policies around major/minor versions
+ * A set of feature flags associated with a command log block format. Versions are changed when the log block format
+ * changes. TODO(na) - Implement policies around major/minor versions
  */
 final class HoodieCommandBlockVersion extends HoodieLogBlockVersion {
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDeleteBlockVersion.java
Patch:
@@ -19,8 +19,8 @@
 package org.apache.hudi.common.table.log.block;
 
 /**
- * A set of feature flags associated with a delete log block format. Versions are changed when the
- * log block format changes. TODO(na) - Implement policies around major/minor versions
+ * A set of feature flags associated with a delete log block format. Versions are changed when the log block format
+ * changes. TODO(na) - Implement policies around major/minor versions
  */
 final class HoodieDeleteBlockVersion extends HoodieLogBlockVersion {
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/FileGroupDTO.java
Patch:
@@ -50,8 +50,8 @@ public static FileGroupDTO fromFileGroup(HoodieFileGroup fileGroup) {
   }
 
   public static HoodieFileGroup toFileGroup(FileGroupDTO dto, HoodieTableMetaClient metaClient) {
-    HoodieFileGroup fileGroup = new HoodieFileGroup(dto.partition, dto.id,
-        TimelineDTO.toTimeline(dto.timeline, metaClient));
+    HoodieFileGroup fileGroup =
+        new HoodieFileGroup(dto.partition, dto.id, TimelineDTO.toTimeline(dto.timeline, metaClient));
     dto.slices.stream().map(FileSliceDTO::toFileSlice).forEach(fileSlice -> fileGroup.addFileSlice(fileSlice));
     return fileGroup;
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/TimelineDTO.java
Patch:
@@ -39,7 +39,7 @@ public static TimelineDTO fromTimeline(HoodieTimeline timeline) {
   }
 
   public static HoodieTimeline toTimeline(TimelineDTO dto, HoodieTableMetaClient metaClient) {
-    //TODO:  For Now, we will assume, only active-timeline will be transferred.
+    // TODO: For Now, we will assume, only active-timeline will be transferred.
     return new HoodieDefaultTimeline(dto.instants.stream().map(InstantDTO::toInstant),
         metaClient.getActiveTimeline()::getInstantDetails);
   }

File: hudi-common/src/main/java/org/apache/hudi/common/util/DFSPropertiesConfiguration.java
Patch:
@@ -63,7 +63,7 @@ private String[] splitProperty(String line) {
     int ind = line.indexOf('=');
     String k = line.substring(0, ind).trim();
     String v = line.substring(ind + 1).trim();
-    return new String[]{k, v};
+    return new String[] {k, v};
   }
 
   private void visitFile(Path file) {
@@ -82,6 +82,7 @@ private void visitFile(Path file) {
 
   /**
    * Add properties from input stream
+   * 
    * @param reader Buffered Reader
    * @throws IOException
    */

File: hudi-common/src/main/java/org/apache/hudi/common/util/DefaultSizeEstimator.java
Patch:
@@ -20,12 +20,13 @@
 
 /**
  * Default implementation of size-estimator that uses Twitter's ObjectSizeCalculator
+ * 
  * @param <T>
  */
 public class DefaultSizeEstimator<T> implements SizeEstimator<T> {
 
   @Override
-  public long sizeEstimate(T t)  {
+  public long sizeEstimate(T t) {
     return ObjectSizeCalculator.getObjectSize(t);
   }
 }

File: hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/TimelineService.java
Patch:
@@ -155,7 +155,7 @@ public FileSystem getFs() {
   public static void main(String[] args) throws Exception {
     final Config cfg = new Config();
     JCommander cmd = new JCommander(cfg, args);
-    if (cfg.help || args.length == 0) {
+    if (cfg.help) {
       cmd.usage();
       System.exit(1);
     }

File: hudi-client/src/main/java/org/apache/hudi/client/embedded/EmbeddedTimelineService.java
Patch:
@@ -68,7 +68,7 @@ private FileSystemViewManager createViewManager() {
   }
 
   public void startServer() throws IOException {
-    server = new TimelineService(0, viewManager, hadoopConf.get());
+    server = new TimelineService(0, viewManager, hadoopConf.newCopy());
     serverPort = server.startService();
     logger.info("Started embedded timeline server at " + hostAddr + ":" + serverPort);
   }

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -202,7 +202,7 @@ public HoodieTableConfig getTableConfig() {
    */
   public HoodieWrapperFileSystem getFs() {
     if (fs == null) {
-      FileSystem fileSystem = FSUtils.getFs(metaPath, hadoopConf.get());
+      FileSystem fileSystem = FSUtils.getFs(metaPath, hadoopConf.newCopy());
       Preconditions.checkArgument(!(fileSystem instanceof HoodieWrapperFileSystem),
           "File System not expected to be that of HoodieWrapperFileSystem");
       fs = new HoodieWrapperFileSystem(fileSystem, consistencyGuardConfig.isConsistencyCheckEnabled()

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java
Patch:
@@ -105,7 +105,7 @@ public void snapshot(JavaSparkContext jsc, String baseDir, final String outputDi
 
       jsc.parallelize(partitions, partitions.size()).flatMap(partition -> {
         // Only take latest version files <= latestCommit.
-        FileSystem fs1 = FSUtils.getFs(baseDir, serConf.get());
+        FileSystem fs1 = FSUtils.getFs(baseDir, serConf.newCopy());
         List<Tuple2<String, String>> filePaths = new ArrayList<>();
         Stream<HoodieDataFile> dataFiles = fsView.getLatestDataFilesBeforeOrOn(partition,
             latestCommitTimestamp);
@@ -124,7 +124,7 @@ public void snapshot(JavaSparkContext jsc, String baseDir, final String outputDi
         String partition = tuple._1();
         Path sourceFilePath = new Path(tuple._2());
         Path toPartitionPath = new Path(outputDir, partition);
-        FileSystem ifs = FSUtils.getFs(baseDir, serConf.get());
+        FileSystem ifs = FSUtils.getFs(baseDir, serConf.newCopy());
 
         if (!ifs.exists(toPartitionPath)) {
           ifs.mkdirs(toPartitionPath);

File: hudi-client/src/test/java/org/apache/hudi/HoodieClientTestHarness.java
Patch:
@@ -200,7 +200,7 @@ protected void initTableType() throws IOException {
       throw new IllegalStateException("The Spark context has not been initialized.");
     }
 
-    HoodieTestUtils.initTableType(jsc.hadoopConfiguration(), basePath, getTableType());
+    HoodieTestUtils.init(jsc.hadoopConfiguration(), basePath, getTableType());
   }
 
   /**

File: hudi-client/src/test/java/org/apache/hudi/TestCleaner.java
Patch:
@@ -515,7 +515,7 @@ public void testKeepLatestFileVersionsMOR() throws IOException {
             HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(1).build())
         .build();
 
-    HoodieTableMetaClient metaClient = HoodieTestUtils.initTableType(jsc.hadoopConfiguration(), basePath,
+    HoodieTableMetaClient metaClient = HoodieTestUtils.init(jsc.hadoopConfiguration(), basePath,
         HoodieTableType.MERGE_ON_READ);
 
     // Make 3 files, one base file and 2 log files associated with base file
@@ -858,7 +858,7 @@ public void testKeepLatestVersionsWithPendingCompactions() throws IOException {
    */
   public void testPendingCompactions(HoodieWriteConfig config, int expNumFilesDeleted,
       int expNumFilesUnderCompactionDeleted) throws IOException {
-    HoodieTableMetaClient metaClient = HoodieTestUtils.initTableType(jsc.hadoopConfiguration(), basePath,
+    HoodieTableMetaClient metaClient = HoodieTestUtils.init(jsc.hadoopConfiguration(), basePath,
         HoodieTableType.MERGE_ON_READ);
     String[] instants = new String[]{"000", "001", "003", "005", "007", "009", "011", "013"};
     String[] compactionInstants = new String[]{"002", "004", "006", "008", "010"};

File: hudi-client/src/test/java/org/apache/hudi/TestCompactionAdminClient.java
Patch:
@@ -53,7 +53,7 @@ public class TestCompactionAdminClient extends TestHoodieClientBase {
   public void setUp() throws Exception {
     initTempFolderAndPath();
     initSparkContexts();
-    metaClient = HoodieTestUtils.initTableType(HoodieTestUtils.getDefaultHadoopConf(), basePath, MERGE_ON_READ);
+    metaClient = HoodieTestUtils.init(HoodieTestUtils.getDefaultHadoopConf(), basePath, MERGE_ON_READ);
     client = new CompactionAdminClient(jsc, basePath);
   }
 

File: hudi-client/src/test/java/org/apache/hudi/io/TestHoodieCompactor.java
Patch:
@@ -62,7 +62,7 @@ public void setUp() throws Exception {
     initTempFolderAndPath();
     hadoopConf = HoodieTestUtils.getDefaultHadoopConf();
     fs = FSUtils.getFs(basePath, hadoopConf);
-    HoodieTestUtils.initTableType(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);
+    HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);
     initTestDataGenerator();
   }
 
@@ -96,7 +96,7 @@ private HoodieWriteConfig.Builder getConfigBuilder() {
 
   @Test(expected = HoodieNotSupportedException.class)
   public void testCompactionOnCopyOnWriteFail() throws Exception {
-    HoodieTestUtils.initTableType(hadoopConf, basePath, HoodieTableType.COPY_ON_WRITE);
+    HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.COPY_ON_WRITE);
     HoodieTableMetaClient metaClient = new HoodieTableMetaClient(jsc.hadoopConfiguration(), basePath);
 
     HoodieTable table = HoodieTable.getHoodieTable(metaClient, getConfig(), jsc);

File: hudi-client/src/test/java/org/apache/hudi/table/TestMergeOnReadTable.java
Patch:
@@ -85,7 +85,7 @@ public void init() throws IOException {
     jsc.hadoopConfiguration().addResource(dfs.getConf());
     initTempFolderAndPath();
     dfs.mkdirs(new Path(basePath));
-    HoodieTestUtils.initTableType(jsc.hadoopConfiguration(), basePath, HoodieTableType.MERGE_ON_READ);
+    HoodieTestUtils.init(jsc.hadoopConfiguration(), basePath, HoodieTableType.MERGE_ON_READ);
     initTestDataGenerator();
   }
 
@@ -294,7 +294,7 @@ public void testSimpleInsertUpdateAndDelete() throws Exception {
   public void testCOWToMORConvertedDatasetRollback() throws Exception {
 
     //Set TableType to COW
-    HoodieTestUtils.initTableType(jsc.hadoopConfiguration(), basePath, HoodieTableType.COPY_ON_WRITE);
+    HoodieTestUtils.init(jsc.hadoopConfiguration(), basePath, HoodieTableType.COPY_ON_WRITE);
 
     HoodieWriteConfig cfg = getConfig(true);
     try (HoodieWriteClient client = getWriteClient(cfg);) {
@@ -330,7 +330,7 @@ public void testCOWToMORConvertedDatasetRollback() throws Exception {
       assertNoWriteErrors(statuses);
 
       //Set TableType to MOR
-      HoodieTestUtils.initTableType(jsc.hadoopConfiguration(), basePath, HoodieTableType.MERGE_ON_READ);
+      HoodieTestUtils.init(jsc.hadoopConfiguration(), basePath, HoodieTableType.MERGE_ON_READ);
 
       //rollback a COW commit when TableType is MOR
       client.rollback(newCommitTime);

File: hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
Patch:
@@ -273,7 +273,7 @@ public static HoodieTableMetaClient initTableType(Configuration hadoopConf, Stri
     properties.put(HoodieTableConfig.HOODIE_TABLE_NAME_PROP_NAME, tableName);
     properties.put(HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME, type.name());
     properties.put(HoodieTableConfig.HOODIE_ARCHIVELOG_FOLDER_PROP_NAME, archiveLogFolder);
-    return HoodieTableMetaClient.initializePathAsHoodieDataset(hadoopConf, basePath, properties);
+    return HoodieTableMetaClient.initDatasetAndGetMetaClient(hadoopConf, basePath, properties);
   }
 
   /**
@@ -287,7 +287,7 @@ public static HoodieTableMetaClient initTableType(Configuration hadoopConf, Stri
     if (tableType == HoodieTableType.MERGE_ON_READ) {
       properties.setProperty(HoodieTableConfig.HOODIE_PAYLOAD_CLASS_PROP_NAME, payloadClassName);
     }
-    return HoodieTableMetaClient.initializePathAsHoodieDataset(hadoopConf, basePath, properties);
+    return HoodieTableMetaClient.initDatasetAndGetMetaClient(hadoopConf, basePath, properties);
   }
 
   /**
@@ -296,7 +296,7 @@ public static HoodieTableMetaClient initTableType(Configuration hadoopConf, Stri
    *
    * @return Instance of HoodieTableMetaClient
    */
-  public static HoodieTableMetaClient initializePathAsHoodieDataset(Configuration hadoopConf,
+  public static HoodieTableMetaClient initDatasetAndGetMetaClient(Configuration hadoopConf,
       String basePath, Properties props) throws IOException {
     log.info("Initializing " + basePath + " as hoodie dataset " + basePath);
     Path basePathDir = new Path(basePath);

File: hudi-common/src/test/java/org/apache/hudi/common/table/log/HoodieLogFormatTest.java
Patch:
@@ -113,7 +113,7 @@ public void setUp() throws IOException, InterruptedException {
     assertTrue(fs.mkdirs(new Path(folder.getRoot().getPath())));
     this.partitionPath = new Path(folder.getRoot().getPath());
     this.basePath = folder.getRoot().getParent();
-    HoodieTestUtils.initTableType(MiniClusterUtil.configuration, basePath, HoodieTableType.MERGE_ON_READ);
+    HoodieTestUtils.init(MiniClusterUtil.configuration, basePath, HoodieTableType.MERGE_ON_READ);
   }
 
   @After

File: hudi-common/src/test/java/org/apache/hudi/common/util/TestCompactionUtils.java
Patch:
@@ -65,7 +65,7 @@ public class TestCompactionUtils {
 
   @Before
   public void init() throws IOException {
-    metaClient = HoodieTestUtils.initTableType(getDefaultHadoopConf(),
+    metaClient = HoodieTestUtils.init(getDefaultHadoopConf(),
         tmpFolder.getRoot().getAbsolutePath(), HoodieTableType.MERGE_ON_READ);
     basePath = metaClient.getBasePath();
   }

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java
Patch:
@@ -131,7 +131,7 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {
       properties.put(HoodieTableConfig.HOODIE_TABLE_NAME_PROP_NAME, cfg.tableName);
       properties.put(HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME, cfg.tableType);
       HoodieTableMetaClient
-          .initializePathAsHoodieDataset(jsc.hadoopConfiguration(), cfg.targetPath, properties);
+          .initDatasetAndGetMetaClient(jsc.hadoopConfiguration(), cfg.targetPath, properties);
 
       HoodieWriteClient client = UtilHelpers.createHoodieClient(jsc, cfg.targetPath, schemaStr,
           cfg.parallelism, Option.empty(), props);

File: hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieBloomIndex.java
Patch:
@@ -91,6 +91,7 @@ public TestHoodieBloomIndex(boolean rangePruning, boolean treeFiltering, boolean
   public void setUp() throws Exception {
     initSparkContexts("TestHoodieBloomIndex");
     initTempFolderAndPath();
+    initFileSystem();
     HoodieTestUtils.init(jsc.hadoopConfiguration(), basePath);
     // We have some records to be tagged (two different partitions)
     schemaStr = FileIOUtils.readAsUTFString(getClass().getResourceAsStream("/exampleSchema.txt"));
@@ -100,6 +101,7 @@ public void setUp() throws Exception {
   @After
   public void tearDown() throws Exception {
     cleanupSparkContexts();
+    cleanupFileSystem();
     cleanupTempFolderAndPath();
   }
 

File: hudi-client/src/test/java/org/apache/hudi/table/TestCopyOnWriteTable.java
Patch:
@@ -75,13 +75,15 @@ public void setUp() throws Exception {
     initTempFolderAndPath();
     initTableType();
     initTestDataGenerator();
+    initFileSystem();
   }
 
   @After
   public void tearDown() throws Exception {
     cleanupSparkContexts();
     cleanupTempFolderAndPath();
     cleanupTableType();
+    cleanupFileSystem();
     cleanupTestDataGenerator();
   }
 

File: hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -250,7 +250,7 @@ private void ensurePartitionLoadedCorrectly(String partition) {
   /**
    * Helper to convert file-status to data-files
    *
-   * @param statuses List of Fole-Status
+   * @param statuses List of File-Status
    */
   private Stream<HoodieDataFile> convertFileStatusesToDataFiles(FileStatus[] statuses) {
     Predicate<FileStatus> roFilePredicate = fileStatus ->

File: hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java
Patch:
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.cli.commands;
 
-import com.beust.jcommander.internal.Maps;
 import com.fasterxml.jackson.databind.ObjectMapper;
+import com.google.common.collect.Maps;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -50,12 +50,12 @@
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieMemoryConfig;
 import org.apache.hudi.hive.util.SchemaUtil;
+import org.apache.parquet.avro.AvroSchemaConverter;
 import org.springframework.shell.core.CommandMarker;
 import org.springframework.shell.core.annotation.CliAvailabilityIndicator;
 import org.springframework.shell.core.annotation.CliCommand;
 import org.springframework.shell.core.annotation.CliOption;
 import org.springframework.stereotype.Component;
-import parquet.avro.AvroSchemaConverter;
 import scala.Tuple2;
 import scala.Tuple3;
 

File: hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
Patch:
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.io;
 
-import com.beust.jcommander.internal.Maps;
+import com.google.common.collect.Maps;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Iterator;

File: hudi-client/src/test/java/org/apache/hudi/io/strategy/TestHoodieCompactionStrategy.java
Patch:
@@ -21,8 +21,8 @@
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
 
-import com.beust.jcommander.internal.Lists;
 import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import java.text.SimpleDateFormat;
 import java.util.ArrayList;
@@ -248,7 +248,7 @@ private List<HoodieCompactionOperation> createCompactionOperations(HoodieWriteCo
 
   private List<HoodieCompactionOperation> createCompactionOperations(HoodieWriteConfig config,
       Map<Long, List<Long>> sizesMap, Map<Long, String> keyToPartitionMap) {
-    List<HoodieCompactionOperation> operations = Lists.newArrayList(sizesMap.size());
+    List<HoodieCompactionOperation> operations = new ArrayList<>(sizesMap.size());
 
     sizesMap.forEach((k, v) -> {
       HoodieDataFile df = TestHoodieDataFile.newDataFile(k);

File: hudi-common/src/main/java/org/apache/hudi/common/util/DefaultSizeEstimator.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.hudi.common.util;
 
-import com.twitter.common.objectsize.ObjectSizeCalculator;
-
 /**
  * Default implementation of size-estimator that uses Twitter's ObjectSizeCalculator
  * @param <T>

File: hudi-common/src/main/java/org/apache/hudi/common/util/HoodieRecordSizeEstimator.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.common.util;
 
-import com.twitter.common.objectsize.ObjectSizeCalculator;
 import org.apache.avro.Schema;
 import org.apache.hudi.common.model.HoodieRecord;
 import org.apache.hudi.common.model.HoodieRecordPayload;

File: hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java
Patch:
@@ -27,7 +27,6 @@
 import java.util.function.Function;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
-import org.apache.commons.collections.CollectionUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hudi.avro.HoodieAvroWriteSupport;
@@ -71,7 +70,7 @@ public static Set<String> readRowKeysFromParquet(Configuration configuration, Pa
    */
   public static Set<String> filterParquetRowKeys(Configuration configuration, Path filePath, Set<String> filter) {
     Option<RecordKeysFilterFunction> filterFunction = Option.empty();
-    if (CollectionUtils.isNotEmpty(filter)) {
+    if (filter != null && !filter.isEmpty()) {
       filterFunction = Option.of(new RecordKeysFilterFunction(filter));
     }
     Configuration conf = new Configuration(configuration);

File: hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.common.util.collection;
 
-import com.twitter.common.objectsize.ObjectSizeCalculator;
 import java.io.IOException;
 import java.io.Serializable;
 import java.util.ArrayList;
@@ -30,6 +29,7 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.stream.Stream;
+import org.apache.hudi.common.util.ObjectSizeCalculator;
 import org.apache.hudi.common.util.SizeEstimator;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.log4j.LogManager;

File: hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java
Patch:
@@ -53,9 +53,9 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
-import parquet.avro.AvroSchemaConverter;
-import parquet.hadoop.ParquetFileReader;
-import parquet.schema.MessageType;
+import org.apache.parquet.avro.AvroSchemaConverter;
+import org.apache.parquet.hadoop.ParquetFileReader;
+import org.apache.parquet.schema.MessageType;
 
 /**
  * Record Reader implementation to merge fresh avro data with base parquet data, to support real

File: hudi-hive/src/main/java/org/apache/hudi/hive/SchemaDifference.java
Patch:
@@ -25,7 +25,7 @@
 import com.google.common.collect.Maps;
 import java.util.List;
 import java.util.Map;
-import parquet.schema.MessageType;
+import org.apache.parquet.schema.MessageType;
 
 /**
  * Represents the schema difference between the storage schema and hive table schema

File: hudi-hive/src/test/java/org/apache/hudi/hive/util/HiveTestService.java
Patch:
@@ -41,6 +41,8 @@
 import org.apache.hadoop.hive.thrift.TUGIContainingTransport;
 import org.apache.hive.service.server.HiveServer2;
 import org.apache.hudi.common.model.HoodieTestUtils;
+import org.apache.log4j.LogManager;
+import org.apache.log4j.Logger;
 import org.apache.thrift.TProcessor;
 import org.apache.thrift.protocol.TBinaryProtocol;
 import org.apache.thrift.server.TServer;
@@ -52,12 +54,10 @@
 import org.apache.thrift.transport.TTransport;
 import org.apache.thrift.transport.TTransportException;
 import org.apache.thrift.transport.TTransportFactory;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 public class HiveTestService {
 
-  private static final Logger LOG = LoggerFactory.getLogger(HiveTestService.class);
+  private static Logger LOG = LogManager.getLogger(HiveTestService.class);
 
   private static final int CONNECTION_TIMEOUT = 30000;
 

File: hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -166,7 +166,7 @@ public static class Config implements Serializable {
     public String propsFilePath =
         "file://" + System.getProperty("user.dir") + "/src/test/resources/delta-streamer-config/dfs-source.properties";
 
-    @Parameter(names = {"--hudi-conf"}, description = "Any configuration that can be set in the properties file "
+    @Parameter(names = {"--hoodie-conf"}, description = "Any configuration that can be set in the properties file "
         + "(using the CLI parameter \"--propsFilePath\") can also be passed command line using this parameter")
     public List<String> configs = new ArrayList<>();
 

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/view/AbstractTableFileSystemView.java
Patch:
@@ -216,7 +216,7 @@ private void ensurePartitionLoadedCorrectly(String partition) {
           log.info("Building file system view for partition (" + partitionPathStr + ")");
 
           // Create the path if it does not exist already
-          Path partitionPath = new Path(metaClient.getBasePath(), partitionPathStr);
+          Path partitionPath = FSUtils.getPartitionPath(metaClient.getBasePath(), partitionPathStr);
           FSUtils.createPathIfNotExists(metaClient.getFs(), partitionPath);
           long beginLsTs = System.currentTimeMillis();
           FileStatus[] statuses = metaClient.getFs().listStatus(partitionPath);

File: hoodie-client/src/main/java/com/uber/hoodie/index/bloom/HoodieBloomIndexCheckFunction.java
Patch:
@@ -177,7 +177,7 @@ protected List<KeyLookupResult> computeNext() {
             checkAndAddCandidates(recordKey);
           } else {
             // do the actual checking of file & break out
-            ret.add(new KeyLookupResult(currentFile, checkAgainstCurrentFile()));
+            ret.add(new KeyLookupResult(currentFile, currentPartitionPath, checkAgainstCurrentFile()));
             initState(fileName, partitionPath);
             checkAndAddCandidates(recordKey);
             break;
@@ -186,7 +186,7 @@ protected List<KeyLookupResult> computeNext() {
 
         // handle case, where we ran out of input, close pending work, update return val
         if (!inputItr.hasNext()) {
-          ret.add(new KeyLookupResult(currentFile, checkAgainstCurrentFile()));
+          ret.add(new KeyLookupResult(currentFile, currentPartitionPath, checkAgainstCurrentFile()));
         }
       } catch (Throwable e) {
         if (e instanceof HoodieException) {

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -243,7 +243,7 @@ public void sync() throws Exception {
     JavaRDD<GenericRecord> avroRDD = avroRDDOptional.get();
     JavaRDD<HoodieRecord> records = avroRDD.map(gr -> {
       HoodieRecordPayload payload = DataSourceUtils.createPayload(cfg.payloadClassName, gr,
-          (Comparable) gr.get(cfg.sourceOrderingField));
+          (Comparable) DataSourceUtils.getNestedFieldVal(gr, cfg.sourceOrderingField));
       return new HoodieRecord<>(keyGenerator.getKey(gr), payload);
     });
 

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/keygen/TimestampBasedKeyGenerator.java
Patch:
@@ -83,7 +83,7 @@ public TimestampBasedKeyGenerator(TypedProperties config) {
 
   @Override
   public HoodieKey getKey(GenericRecord record) {
-    Object partitionVal = record.get(partitionPathField);
+    Object partitionVal = DataSourceUtils.getNestedFieldVal(record, partitionPathField);
     SimpleDateFormat partitionPathFormat = new SimpleDateFormat(outputDateFormat);
     partitionPathFormat.setTimeZone(TimeZone.getTimeZone("GMT"));
 
@@ -102,7 +102,7 @@ public HoodieKey getKey(GenericRecord record) {
             "Unexpected type for partition field: " + partitionVal.getClass().getName());
       }
 
-      return new HoodieKey(record.get(recordKeyField).toString(),
+      return new HoodieKey(DataSourceUtils.getNestedFieldValAsString(record, recordKeyField),
           partitionPathFormat.format(new Date(unixTime * 1000)));
     } catch (ParseException pe) {
       throw new HoodieDeltaStreamerException(

File: hoodie-client/src/main/java/com/uber/hoodie/HoodieWriteClient.java
Patch:
@@ -1348,7 +1348,8 @@ private Optional<String> forceCompact(Optional<Map<String, String>> extraMetadat
     Optional<String> compactionInstantTimeOpt = scheduleCompaction(extraMetadata);
     compactionInstantTimeOpt.ifPresent(compactionInstantTime -> {
       try {
-        compact(compactionInstantTime);
+        // inline compaction should auto commit as the user is never given control
+        compact(compactionInstantTime, true);
       } catch (IOException ioe) {
         throw new HoodieIOException(ioe.getMessage(), ioe);
       }

File: hoodie-client/src/main/java/com/uber/hoodie/WriteStatus.java
Patch:
@@ -89,7 +89,8 @@ public void markSuccess(HoodieRecord record, Optional<Map<String, String>> optio
    * HoodieRecord} before deflation.
    */
   public void markFailure(HoodieRecord record, Throwable t, Optional<Map<String, String>> optionalRecordMetadata) {
-    if (random.nextDouble() <= failureFraction) {
+    if (failedRecords.isEmpty() || (random.nextDouble() <= failureFraction)) {
+      // Guaranteed to have at-least one error
       failedRecords.add(record);
       errors.put(record.getKey(), t);
     }

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieAppendHandle.java
Patch:
@@ -251,7 +251,7 @@ public WriteStatus close() {
       writeStatus.getStat().setNumInserts(insertRecordsWritten);
       writeStatus.getStat().setNumDeletes(recordsDeleted);
       writeStatus.getStat().setTotalWriteBytes(estimatedNumberOfBytesWritten);
-      writeStatus.getStat().setTotalWriteErrors(writeStatus.getFailedRecords().size());
+      writeStatus.getStat().setTotalWriteErrors(writeStatus.getTotalErrorRecords());
       RuntimeStats runtimeStats = new RuntimeStats();
       runtimeStats.setTotalUpsertTime(timer.endTimer());
       writeStatus.getStat().setRuntimeStats(runtimeStats);

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieCreateHandle.java
Patch:
@@ -166,7 +166,7 @@ public WriteStatus close() {
       stat.setFileId(writeStatus.getFileId());
       stat.setPaths(new Path(config.getBasePath()), path, tempPath);
       stat.setTotalWriteBytes(FSUtils.getFileSize(fs, getStorageWriterPath()));
-      stat.setTotalWriteErrors(writeStatus.getFailedRecords().size());
+      stat.setTotalWriteErrors(writeStatus.getTotalErrorRecords());
       RuntimeStats runtimeStats = new RuntimeStats();
       runtimeStats.setTotalCreateTime(timer.endTimer());
       stat.setRuntimeStats(runtimeStats);

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieMergeHandle.java
Patch:
@@ -281,7 +281,7 @@ public WriteStatus close() {
       writeStatus.getStat().setNumDeletes(recordsDeleted);
       writeStatus.getStat().setNumUpdateWrites(updatedRecordsWritten);
       writeStatus.getStat().setNumInserts(insertRecordsWritten);
-      writeStatus.getStat().setTotalWriteErrors(writeStatus.getFailedRecords().size());
+      writeStatus.getStat().setTotalWriteErrors(writeStatus.getTotalErrorRecords());
       RuntimeStats runtimeStats = new RuntimeStats();
       runtimeStats.setTotalUpsertTime(timer.endTimer());
       writeStatus.getStat().setRuntimeStats(runtimeStats);

File: hoodie-common/src/main/java/com/uber/hoodie/common/util/queue/BoundedInMemoryExecutor.java
Patch:
@@ -93,7 +93,7 @@ public ExecutorCompletionService<Boolean> startProducers() {
           preExecute();
           producer.produce(queue);
         } catch (Exception e) {
-          logger.error("error consuming records", e);
+          logger.error("error producing records", e);
           queue.markAsFailed(e);
           throw e;
         } finally {

File: hoodie-common/src/main/java/com/uber/hoodie/common/util/queue/BoundedInMemoryQueue.java
Patch:
@@ -207,6 +207,9 @@ private Optional<O> readNextRecord() {
         throw new HoodieException(e);
       }
     }
+    // Check one more time here as it is possible producer errored out and closed immediately
+    throwExceptionIfFailed();
+
     if (newRecord != null && newRecord.isPresent()) {
       return newRecord;
     } else {

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -326,8 +326,8 @@ private void registerAvroSchemas(SchemaProvider schemaProvider) {
 
   private HoodieWriteConfig getHoodieClientConfig(SchemaProvider schemaProvider) {
     HoodieWriteConfig.Builder builder =
-        HoodieWriteConfig.newBuilder().combineInput(true, true).withPath(cfg.targetBasePath)
-            .withAutoCommit(false)
+        HoodieWriteConfig.newBuilder().withPath(cfg.targetBasePath)
+            .withAutoCommit(false).combineInput(cfg.filterDupes, true)
             .withCompactionConfig(HoodieCompactionConfig.newBuilder()
                 .withPayloadClass(cfg.payloadClassName)
                 // turn on inline compaction by default, for MOR tables

File: hoodie-client/src/main/java/com/uber/hoodie/config/HoodieWriteConfig.java
Patch:
@@ -324,8 +324,8 @@ public boolean getBloomIndexUseCaching() {
     return Boolean.parseBoolean(props.getProperty(HoodieIndexConfig.BLOOM_INDEX_USE_CACHING_PROP));
   }
 
-  public int getNumBucketsPerPartition() {
-    return Integer.parseInt(props.getProperty(HoodieIndexConfig.BUCKETED_INDEX_NUM_BUCKETS_PROP));
+  public boolean useBloomIndexTreebasedFilter() {
+    return Boolean.parseBoolean(props.getProperty(HoodieIndexConfig.BLOOM_INDEX_TREE_BASED_FILTER_PROP));
   }
 
   public StorageLevel getBloomIndexInputStorageLevel() {

File: hoodie-client/src/main/java/com/uber/hoodie/index/bloom/HoodieBloomIndex.java
Patch:
@@ -38,7 +38,6 @@
 import com.uber.hoodie.exception.MetadataNotFoundException;
 import com.uber.hoodie.index.HoodieIndex;
 import com.uber.hoodie.table.HoodieTable;
-
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
@@ -304,9 +303,9 @@ public boolean isImplicitWithStorage() {
   JavaPairRDD<String, Tuple2<String, HoodieKey>> explodeRecordRDDWithFileComparisons(
       final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,
       JavaPairRDD<String, String> partitionRecordKeyPairRDD) {
-    IndexFileFilter indexFileFilter = config.getBloomIndexPruneByRanges()
+    IndexFileFilter indexFileFilter = config.useBloomIndexTreebasedFilter()
         ? new IntervalTreeBasedIndexFileFilter(partitionToFileIndexInfo)
-        : new SimpleIndexFileFilter(partitionToFileIndexInfo);
+        : new ListBasedIndexFileFilter(partitionToFileIndexInfo);
     return partitionRecordKeyPairRDD.map(partitionRecordKeyPair -> {
       String recordKey = partitionRecordKeyPair._2();
       String partitionPath = partitionRecordKeyPair._1();

File: hoodie-client/src/main/java/com/uber/hoodie/index/bloom/ListBasedGlobalIndexFileFilter.java
Patch:
@@ -23,14 +23,14 @@
 import java.util.Map;
 import java.util.Set;
 
-class SimpleGlobalIndexFileFilter extends SimpleIndexFileFilter {
+class ListBasedGlobalIndexFileFilter extends ListBasedIndexFileFilter {
 
   /**
-   * Instantiates {@link SimpleGlobalIndexFileFilter}
+   * Instantiates {@link ListBasedGlobalIndexFileFilter}
    *
    * @param partitionToFileIndexInfo Map of partition to List of {@link BloomIndexFileInfo}
    */
-  SimpleGlobalIndexFileFilter(
+  ListBasedGlobalIndexFileFilter(
       Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo) {
     super(partitionToFileIndexInfo);
   }

File: hoodie-client/src/main/java/com/uber/hoodie/index/bloom/ListBasedIndexFileFilter.java
Patch:
@@ -27,16 +27,16 @@
  * Simple implementation of {@link IndexFileFilter}. Sequentially goes through every index file in a given partition to
  * search for potential index files to be searched for a given record key.
  */
-class SimpleIndexFileFilter implements IndexFileFilter {
+class ListBasedIndexFileFilter implements IndexFileFilter {
 
   final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo;
 
   /**
-   * Instantiates {@link SimpleIndexFileFilter}
+   * Instantiates {@link ListBasedIndexFileFilter}
    *
    * @param partitionToFileIndexInfo Map of partition to List of {@link BloomIndexFileInfo}
    */
-  SimpleIndexFileFilter(final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo) {
+  ListBasedIndexFileFilter(final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo) {
     this.partitionToFileIndexInfo = partitionToFileIndexInfo;
   }
 

File: hoodie-client/src/main/java/com/uber/hoodie/io/compact/strategy/DayBasedCompactionStrategy.java
Patch:
@@ -38,9 +38,9 @@
 public class DayBasedCompactionStrategy extends CompactionStrategy {
 
   // For now, use SimpleDateFormat as default partition format
-  private static String datePartitionFormat = "yyyy/MM/dd";
+  protected static String datePartitionFormat = "yyyy/MM/dd";
   // Sorts compaction in LastInFirstCompacted order
-  private static Comparator<String> comparator = (String leftPartition,
+  protected static Comparator<String> comparator = (String leftPartition,
       String rightPartition) -> {
     try {
       Date left = new SimpleDateFormat(datePartitionFormat, Locale.ENGLISH)

File: hoodie-client/src/main/java/com/uber/hoodie/config/HoodieCompactionConfig.java
Patch:
@@ -47,8 +47,8 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {
   public static final String MIN_COMMITS_TO_KEEP_PROP = "hoodie.keep.min.commits";
   // Upsert uses this file size to compact new data onto existing files..
   public static final String PARQUET_SMALL_FILE_LIMIT_BYTES = "hoodie.parquet.small.file.limit";
-  // Turned off by default
-  public static final String DEFAULT_PARQUET_SMALL_FILE_LIMIT_BYTES = String.valueOf(0);
+  // By default, treat any file <= 100MB as a small file.
+  public static final String DEFAULT_PARQUET_SMALL_FILE_LIMIT_BYTES = String.valueOf(104857600);
   /**
    * Configs related to specific table types
    **/

File: hoodie-client/src/main/java/com/uber/hoodie/io/storage/HoodieWrapperFileSystem.java
Patch:
@@ -536,8 +536,9 @@ public void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile) throws IOE
   }
 
   @Override
-  public void close() throws IOException {
-    fileSystem.close();
+  public void close() {
+    // Don't close the underlying `fileSystem` object. This will end up closing it for every thread since it
+    // could be cached across jvm. We don't own that object anyway.
   }
 
   @Override

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieIOHandle.java
Patch:
@@ -61,7 +61,9 @@ public HoodieIOHandle(HoodieWriteConfig config, String commitTime, HoodieTable<T
     this.originalSchema = new Schema.Parser().parse(config.getSchema());
     this.writerSchema = createHoodieWriteSchema(originalSchema);
     this.timer = new HoodieTimer().startTimer();
-    this.writeStatus = ReflectionUtils.loadClass(config.getWriteStatusClassName());
+    this.writeStatus = (WriteStatus) ReflectionUtils.loadClass(config.getWriteStatusClassName(),
+        !hoodieTable.getIndex().isImplicitWithStorage(),
+        config.getWriteStatusFailureFraction());
   }
 
   /**

File: hoodie-client/src/test/java/com/uber/hoodie/TestHoodieClientBase.java
Patch:
@@ -24,6 +24,7 @@
 import com.uber.hoodie.common.HoodieCleanStat;
 import com.uber.hoodie.common.HoodieClientTestUtils;
 import com.uber.hoodie.common.HoodieTestDataGenerator;
+import com.uber.hoodie.common.TestRawTripPayload;
 import com.uber.hoodie.common.model.HoodiePartitionMetadata;
 import com.uber.hoodie.common.model.HoodieRecord;
 import com.uber.hoodie.common.model.HoodieTableType;
@@ -144,6 +145,7 @@ HoodieWriteConfig.Builder getConfigBuilder() {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)
         .withParallelism(2, 2)
         .withBulkInsertParallelism(2).withFinalizeWriteParallelism(2)
+        .withWriteStatusClass(TestRawTripPayload.MetadataMergeWriteStatus.class)
         .withConsistencyCheckEnabled(true)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024).build())
         .withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1024 * 1024).build())

File: hoodie-client/src/test/java/com/uber/hoodie/index/TestHbaseIndex.java
Patch:
@@ -337,7 +337,7 @@ public void testsHBasePutAccessParallelism() {
   }
 
   private WriteStatus getSampleWriteStatus(final int numInserts, final int numUpdateWrites) {
-    final WriteStatus writeStatus = new WriteStatus();
+    final WriteStatus writeStatus = new WriteStatus(false, 0.1);
     HoodieWriteStat hoodieWriteStat = new HoodieWriteStat();
     hoodieWriteStat.setNumInserts(numInserts);
     hoodieWriteStat.setNumUpdateWrites(numUpdateWrites);

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieMergeHandle.java
Patch:
@@ -76,6 +76,9 @@ public HoodieMergeHandle(HoodieWriteConfig config, String commitTime, HoodieTabl
             .filter(dataFile -> dataFile.getFileId().equals(fileId)).findFirst());
   }
 
+  /**
+   * Called by compactor code path
+   */
   public HoodieMergeHandle(HoodieWriteConfig config, String commitTime, HoodieTable<T> hoodieTable,
       Map<String, HoodieRecord<T>> keyToNewRecords, String fileId, Optional<HoodieDataFile> dataFileToBeMerged) {
     super(config, commitTime, hoodieTable);

File: hoodie-client/src/main/java/com/uber/hoodie/config/HoodieWriteConfig.java
Patch:
@@ -176,11 +176,11 @@ public int getCleanerCommitsRetained() {
   }
 
   public int getMaxCommitsToKeep() {
-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP));
+    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP_PROP));
   }
 
   public int getMinCommitsToKeep() {
-    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP));
+    return Integer.parseInt(props.getProperty(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP_PROP));
   }
 
   public int getParquetSmallFileLimit() {

File: hoodie-client/src/test/java/com/uber/hoodie/config/HoodieWriteConfigTest.java
Patch:
@@ -34,8 +34,9 @@ public class HoodieWriteConfigTest {
   public void testPropertyLoading() throws IOException {
     Builder builder = HoodieWriteConfig.newBuilder().withPath("/tmp");
     Map<String, String> params = Maps.newHashMap();
-    params.put(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP, "5");
-    params.put(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP, "2");
+    params.put(HoodieCompactionConfig.CLEANER_COMMITS_RETAINED_PROP, "1");
+    params.put(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP_PROP, "5");
+    params.put(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP_PROP, "2");
     ByteArrayOutputStream outStream = saveParamsIntoOutputStream(params);
     ByteArrayInputStream inputStream = new ByteArrayInputStream(outStream.toByteArray());
     try {

File: hoodie-client/src/main/java/com/uber/hoodie/func/CopyOnWriteLazyInsertIterable.java
Patch:
@@ -95,7 +95,7 @@ protected List<WriteStatus> computeNext() {
     BoundedInMemoryExecutor<HoodieRecord<T>,
         HoodieInsertValueGenResult<HoodieRecord>, List<WriteStatus>> bufferedIteratorExecutor = null;
     try {
-      final Schema schema = HoodieIOHandle.createHoodieWriteSchema(hoodieConfig);
+      final Schema schema = new Schema.Parser().parse(hoodieConfig.getSchema());
       bufferedIteratorExecutor =
           new SparkBoundedInMemoryExecutor<>(hoodieConfig, inputItr,
               getInsertHandler(), getTransformFunction(schema));

File: hoodie-client/src/main/java/com/uber/hoodie/table/HoodieCopyOnWriteTable.java
Patch:
@@ -199,7 +199,7 @@ protected Iterator<List<WriteStatus>> handleUpdateInternal(HoodieMergeHandle ups
       throw new HoodieUpsertException(
           "Error in finding the old file path at commit " + commitTime + " for fileId: " + fileId);
     } else {
-      AvroReadSupport.setAvroReadSchema(getHadoopConf(), upsertHandle.getSchema());
+      AvroReadSupport.setAvroReadSchema(getHadoopConf(), upsertHandle.getWriterSchema());
       BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;
       try (ParquetReader<IndexedRecord> reader = AvroParquetReader.<IndexedRecord>builder(upsertHandle.getOldFilePath())
           .withConf(getHadoopConf()).build()) {

File: hoodie-common/src/main/java/com/uber/hoodie/common/model/HoodieAvroPayload.java
Patch:
@@ -62,7 +62,6 @@ public Optional<IndexedRecord> getInsertValue(Schema schema) throws IOException
     if (recordBytes.length == 0) {
       return Optional.empty();
     }
-    Optional<GenericRecord> record = Optional.of(HoodieAvroUtils.bytesToAvro(recordBytes, schema));
-    return record.map(r -> HoodieAvroUtils.rewriteRecord(r, schema));
+    return Optional.of(HoodieAvroUtils.bytesToAvro(recordBytes, schema));
   }
 }

File: hoodie-spark/src/main/java/com/uber/hoodie/OverwriteWithLatestAvroPayload.java
Patch:
@@ -66,7 +66,6 @@ public Optional<IndexedRecord> combineAndGetUpdateValue(IndexedRecord currentVal
 
   @Override
   public Optional<IndexedRecord> getInsertValue(Schema schema) throws IOException {
-    return Optional.of(HoodieAvroUtils.rewriteRecord(HoodieAvroUtils.bytesToAvro(recordBytes, Schema.parse(schemaStr)),
-        schema));
+    return Optional.of(HoodieAvroUtils.bytesToAvro(recordBytes, schema));
   }
 }

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/HoodieTimeline.java
Patch:
@@ -98,7 +98,7 @@ public interface HoodieTimeline extends Serializable {
   HoodieTimeline filterCompletedAndCompactionInstants();
 
   /**
-   * Filter this timeline to just include inflight and requested compaction instants
+   * Filter this timeline to just include requested and inflight compaction instants
    * @return
    */
   HoodieTimeline filterPendingCompactionTimeline();

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -262,6 +262,7 @@ public HoodieInstant revertCompactionInflightToRequested(HoodieInstant inflightI
     Preconditions.checkArgument(inflightInstant.isInflight());
     HoodieInstant requestedInstant =
         new HoodieInstant(State.REQUESTED, COMPACTION_ACTION, inflightInstant.getTimestamp());
+    // Pass empty data since it is read from the corresponding .aux/.compaction instant file
     transitionState(inflightInstant, requestedInstant, Optional.empty());
     return requestedInstant;
   }
@@ -310,7 +311,7 @@ private void transitionState(HoodieInstant fromInstant, HoodieInstant toInstant,
     Preconditions.checkArgument(fromInstant.getTimestamp().equals(toInstant.getTimestamp()));
     Path commitFilePath = new Path(metaClient.getMetaPath(), toInstant.getFileName());
     try {
-      // open a new file and write the commit metadata in
+      // Re-create the .inflight file by opening a new file and write the commit metadata in
       Path inflightCommitFile = new Path(metaClient.getMetaPath(), fromInstant.getFileName());
       createFileInMetaPath(fromInstant.getFileName(), data);
       boolean success = metaClient.getFs().rename(inflightCommitFile, commitFilePath);

File: hoodie-client/src/test/java/com/uber/hoodie/table/TestMergeOnReadTable.java
Patch:
@@ -326,8 +326,8 @@ public void testSimpleInsertUpdateAndDelete() throws Exception {
 
     List<String> dataFiles = roView.getLatestDataFiles().map(hf -> hf.getPath()).collect(Collectors.toList());
     List<GenericRecord> recordsRead = HoodieMergeOnReadTestUtils.getRecordsUsingInputFormat(dataFiles, basePath);
-    //Wrote 40 records and deleted 20 records, so remaining 40-20 = 20
-    assertEquals("Must contain 20 records", 20, recordsRead.size());
+    //Wrote 20 records and deleted 20 records, so remaining 20-20 = 0
+    assertEquals("Must contain 0 records", 0, recordsRead.size());
   }
 
   @Test

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/log/AbstractHoodieLogRecordScanner.java
Patch:
@@ -19,6 +19,7 @@
 import static com.uber.hoodie.common.table.log.block.HoodieLogBlock.HeaderMetadataType.INSTANT_TIME;
 import static com.uber.hoodie.common.table.log.block.HoodieLogBlock.HoodieLogBlockType.CORRUPT_BLOCK;
 
+import com.uber.hoodie.common.model.HoodieKey;
 import com.uber.hoodie.common.model.HoodieLogFile;
 import com.uber.hoodie.common.model.HoodieRecord;
 import com.uber.hoodie.common.model.HoodieRecordPayload;
@@ -63,7 +64,7 @@ public abstract class AbstractHoodieLogRecordScanner {
   private static final Logger log = LogManager.getLogger(AbstractHoodieLogRecordScanner.class);
 
   // Reader schema for the records
-  private final Schema readerSchema;
+  protected final Schema readerSchema;
   // Latest valid instant time
   // Log-Blocks belonging to inflight delta-instants are filtered-out using this high-watermark.
   private final String latestInstantTime;
@@ -291,7 +292,7 @@ protected abstract void processNextRecord(HoodieRecord<? extends HoodieRecordPay
    *
    * @param key Deleted record key
    */
-  protected abstract void processNextDeletedKey(String key);
+  protected abstract void processNextDeletedKey(HoodieKey key);
 
   /**
    * Process the set of log blocks belonging to the last instant which is read fully.

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/log/HoodieUnMergedLogRecordScanner.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.uber.hoodie.common.table.log;
 
+import com.uber.hoodie.common.model.HoodieKey;
 import com.uber.hoodie.common.model.HoodieRecord;
 import com.uber.hoodie.common.model.HoodieRecordPayload;
 import java.util.List;
@@ -43,7 +44,7 @@ protected void processNextRecord(HoodieRecord<? extends HoodieRecordPayload> hoo
   }
 
   @Override
-  protected void processNextDeletedKey(String key) {
+  protected void processNextDeletedKey(HoodieKey key) {
     throw new IllegalStateException("Not expected to see delete records in this log-scan mode. Check Job Config");
   }
 

File: hoodie-client/src/test/java/com/uber/hoodie/io/strategy/TestHoodieCompactionStrategy.java
Patch:
@@ -143,10 +143,10 @@ public void testPartitionAwareCompactionSimple() {
 
   private List<HoodieCompactionOperation> createCompactionOperations(HoodieWriteConfig config,
       Map<Long, List<Long>> sizesMap) {
-    Map<Long, String> keyToParitionMap = sizesMap.entrySet().stream().map(e ->
+    Map<Long, String> keyToPartitionMap = sizesMap.entrySet().stream().map(e ->
         Pair.of(e.getKey(), partitionPaths[new Random().nextInt(partitionPaths.length - 1)]))
         .collect(Collectors.toMap(Pair::getKey, Pair::getValue));
-    return createCompactionOperations(config, sizesMap, keyToParitionMap);
+    return createCompactionOperations(config, sizesMap, keyToPartitionMap);
   }
 
   private List<HoodieCompactionOperation> createCompactionOperations(HoodieWriteConfig config,

File: hoodie-common/src/main/java/com/uber/hoodie/common/model/HoodieRecordLocation.java
Patch:
@@ -20,7 +20,7 @@
 import java.io.Serializable;
 
 /**
- * Location of a HoodieRecord within the parition it belongs to. Ultimately, this points to an
+ * Location of a HoodieRecord within the partition it belongs to. Ultimately, this points to an
  * actual file on disk
  */
 public class HoodieRecordLocation implements Serializable {

File: hoodie-hive/src/main/java/com/uber/hoodie/hive/util/SchemaUtil.java
Patch:
@@ -410,12 +410,12 @@ public static String generateCreateDDL(MessageType storageSchema, HiveSyncConfig
           .append(getPartitionKeyType(hiveSchema, partitionKey)).toString());
     }
 
-    String paritionsStr = partitionFields.stream().collect(Collectors.joining(","));
+    String partitionsStr = partitionFields.stream().collect(Collectors.joining(","));
     StringBuilder sb = new StringBuilder("CREATE EXTERNAL TABLE  IF NOT EXISTS ");
     sb = sb.append(config.databaseName).append(".").append(config.tableName);
     sb = sb.append("( ").append(columns).append(")");
     if (!config.partitionFields.isEmpty()) {
-      sb = sb.append(" PARTITIONED BY (").append(paritionsStr).append(")");
+      sb = sb.append(" PARTITIONED BY (").append(partitionsStr).append(")");
     }
     sb = sb.append(" ROW FORMAT SERDE '").append(serdeClass).append("'");
     sb = sb.append(" STORED AS INPUTFORMAT '").append(inputFormatClass).append("'");

File: hoodie-client/src/main/java/com/uber/hoodie/table/HoodieCopyOnWriteTable.java
Patch:
@@ -726,7 +726,7 @@ private void assignInserts(WorkloadProfile profile) {
               insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;
             }
 
-            int insertBuckets = (int) Math.max(totalUnassignedInserts / insertRecordsPerBucket, 1L);
+            int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);
             logger.info(
                 "After small file assignment: unassignedInserts => " + totalUnassignedInserts
                     + ", totalInsertBuckets => " + insertBuckets + ", recordsPerBucket => "

File: hoodie-client/src/main/java/com/uber/hoodie/config/HoodieCompactionConfig.java
Patch:
@@ -63,7 +63,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {
   public static final String COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS =
       "hoodie.copyonwrite.insert" + ".auto.split";
   // its off by default
-  public static final String DEFAULT_COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = String.valueOf(false);
+  public static final String DEFAULT_COPY_ON_WRITE_TABLE_AUTO_SPLIT_INSERTS = String.valueOf(true);
   // This value is used as a guessimate for the record size, if we can't determine this from
   // previous commits
   public static final String COPY_ON_WRITE_TABLE_RECORD_SIZE_ESTIMATE =

File: hoodie-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/HoodieInputFormat.java
Patch:
@@ -23,6 +23,7 @@
 import com.uber.hoodie.common.table.TableFileSystemView;
 import com.uber.hoodie.common.table.timeline.HoodieInstant;
 import com.uber.hoodie.common.table.view.HoodieTableFileSystemView;
+import com.uber.hoodie.exception.DatasetNotFoundException;
 import com.uber.hoodie.exception.HoodieIOException;
 import com.uber.hoodie.exception.InvalidDatasetException;
 import java.io.IOException;
@@ -161,7 +162,7 @@ private Map<HoodieTableMetaClient, List<FileStatus>> groupFileStatus(FileStatus[
           metadata = getTableMetaClient(status.getPath().getFileSystem(conf),
               status.getPath().getParent());
           nonHoodieBasePath = null;
-        } catch (InvalidDatasetException e) {
+        } catch (DatasetNotFoundException | InvalidDatasetException e) {
           LOG.info("Handling a non-hoodie path " + status.getPath());
           metadata = null;
           nonHoodieBasePath = status.getPath().getParent().toString();

File: hoodie-client/src/main/java/com/uber/hoodie/table/HoodieCopyOnWriteTable.java
Patch:
@@ -195,7 +195,7 @@ protected Iterator<List<WriteStatus>> handleUpdateInternal(HoodieMergeHandle ups
           "Error in finding the old file path at commit " + commitTime + " for fileId: " + fileId);
     } else {
       AvroReadSupport.setAvroReadSchema(getHadoopConf(), upsertHandle.getSchema());
-      ParquetReader<IndexedRecord> reader = AvroParquetReader.builder(upsertHandle.getOldFilePath())
+      ParquetReader<IndexedRecord> reader = AvroParquetReader.<IndexedRecord>builder(upsertHandle.getOldFilePath())
           .withConf(getHadoopConf()).build();
       BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;
       try {

File: hoodie-client/src/test/java/com/uber/hoodie/TestHoodieClientBase.java
Patch:
@@ -143,6 +143,8 @@ protected HoodieWriteConfig getConfig() {
   HoodieWriteConfig.Builder getConfigBuilder() {
     return HoodieWriteConfig.newBuilder().withPath(basePath).withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)
         .withParallelism(2, 2)
+        .withBulkInsertParallelism(2).withFinalizeWriteParallelism(2)
+        .withConsistencyCheckEnabled(true)
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024).build())
         .withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1024 * 1024).build())
         .forTable("test-trip-table")

File: hoodie-client/src/test/java/com/uber/hoodie/index/TestHbaseIndex.java
Patch:
@@ -24,6 +24,7 @@
 
 import com.uber.hoodie.HoodieWriteClient;
 import com.uber.hoodie.WriteStatus;
+import com.uber.hoodie.common.HoodieClientTestUtils;
 import com.uber.hoodie.common.HoodieTestDataGenerator;
 import com.uber.hoodie.common.model.HoodieRecord;
 import com.uber.hoodie.common.model.HoodieTestUtils;
@@ -50,7 +51,6 @@
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.junit.After;
@@ -101,8 +101,7 @@ public static void init() throws Exception {
     hbaseConfig = utility.getConnection().getConfiguration();
     utility.createTable(TableName.valueOf(tableName), Bytes.toBytes("_s"));
     // Initialize a local spark env
-    SparkConf sparkConf = new SparkConf().setAppName("TestHbaseIndex").setMaster("local[1]");
-    jsc = new JavaSparkContext(sparkConf);
+    jsc = new JavaSparkContext(HoodieClientTestUtils.getSparkConfForTest("TestHbaseIndex"));
     jsc.hadoopConfiguration().addResource(utility.getConfiguration());
   }
 

File: hoodie-utilities/src/test/java/com/uber/hoodie/utilities/TestHoodieDeltaStreamer.java
Patch:
@@ -89,6 +89,7 @@ static HoodieDeltaStreamer.Config makeConfig(String basePath, Operation op) {
       HoodieDeltaStreamer.Config cfg = new HoodieDeltaStreamer.Config();
       cfg.targetBasePath = basePath;
       cfg.targetTableName = "hoodie_trips";
+      cfg.storageType = "COPY_ON_WRITE";
       cfg.sourceClassName = TestDataSource.class.getName();
       cfg.operation = op;
       cfg.sourceOrderingField = "timestamp";

File: hoodie-common/src/test/java/com/uber/hoodie/common/util/SchemaTestUtil.java
Patch:
@@ -47,7 +47,7 @@
 public class SchemaTestUtil {
 
   public static Schema getSimpleSchema() throws IOException {
-    return new Schema.Parser().parse(SchemaTestUtil.class.getResourceAsStream("/simple-test.avro"));
+    return new Schema.Parser().parse(SchemaTestUtil.class.getResourceAsStream("/simple-test.avsc"));
   }
 
   public static List<IndexedRecord> generateTestRecords(int from, int limit) throws IOException, URISyntaxException {
@@ -163,7 +163,7 @@ public static List<HoodieRecord> updateHoodieTestRecordsWithoutHoodieMetadata(Li
   }
 
   public static Schema getEvolvedSchema() throws IOException {
-    return new Schema.Parser().parse(SchemaTestUtil.class.getResourceAsStream("/simple-test-evolved.avro"));
+    return new Schema.Parser().parse(SchemaTestUtil.class.getResourceAsStream("/simple-test-evolved.avsc"));
   }
 
   public static List<IndexedRecord> generateEvolvedTestRecords(int from, int limit)
@@ -172,7 +172,7 @@ public static List<IndexedRecord> generateEvolvedTestRecords(int from, int limit
   }
 
   public static Schema getComplexEvolvedSchema() throws IOException {
-    return new Schema.Parser().parse(SchemaTestUtil.class.getResourceAsStream("/complex-test-evolved.avro"));
+    return new Schema.Parser().parse(SchemaTestUtil.class.getResourceAsStream("/complex-test-evolved.avsc"));
   }
 
   public static GenericRecord generateAvroRecordFromJson(Schema schema, int recordNumber, String commitTime,

File: hoodie-hadoop-mr/src/test/java/com/uber/hoodie/hadoop/HoodieInputFormatTest.java
Patch:
@@ -176,7 +176,7 @@ public void testIncrementalWithMultipleCommits() throws IOException {
   //TODO enable this after enabling predicate pushdown
   public void testPredicatePushDown() throws IOException {
     // initial commit
-    Schema schema = InputFormatTestUtil.readSchema("/sample1.avro");
+    Schema schema = InputFormatTestUtil.readSchema("/sample1.avsc");
     String commit1 = "20160628071126";
     File partitionDir = InputFormatTestUtil.prepareParquetDataset(basePath, schema, 1, 10, commit1);
     InputFormatTestUtil.commit(basePath, commit1);

File: hoodie-client/src/main/java/com/uber/hoodie/HoodieReadClient.java
Patch:
@@ -25,6 +25,7 @@
 import com.uber.hoodie.common.table.HoodieTimeline;
 import com.uber.hoodie.common.util.CompactionUtils;
 import com.uber.hoodie.common.util.FSUtils;
+import com.uber.hoodie.common.util.collection.Pair;
 import com.uber.hoodie.config.HoodieIndexConfig;
 import com.uber.hoodie.config.HoodieWriteConfig;
 import com.uber.hoodie.exception.HoodieIndexException;
@@ -35,7 +36,6 @@
 import java.util.List;
 import java.util.Set;
 import java.util.stream.Collectors;
-import org.apache.commons.lang3.tuple.Pair;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieCleanHelper.java
Patch:
@@ -27,6 +27,7 @@
 import com.uber.hoodie.common.table.TableFileSystemView;
 import com.uber.hoodie.common.table.timeline.HoodieInstant;
 import com.uber.hoodie.common.table.view.HoodieTableFileSystemView;
+import com.uber.hoodie.common.util.collection.Pair;
 import com.uber.hoodie.config.HoodieWriteConfig;
 import com.uber.hoodie.table.HoodieTable;
 import java.io.IOException;
@@ -36,7 +37,6 @@
 import java.util.Map;
 import java.util.Optional;
 import java.util.stream.Collectors;
-import org.apache.commons.lang3.tuple.Pair;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
@@ -260,4 +260,4 @@ private boolean isFileSliceNeededForPendingCompaction(FileSlice fileSlice) {
     }
     return false;
   }
-}
\ No newline at end of file
+}

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieCommitArchiveLog.java
Patch:
@@ -56,7 +56,6 @@
 import java.util.stream.Stream;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.IndexedRecord;
-import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
@@ -315,7 +314,7 @@ private com.uber.hoodie.avro.model.HoodieCommitMetadata commitMetadataConverter(
     com.uber.hoodie.avro.model.HoodieCommitMetadata avroMetaData = mapper
         .convertValue(hoodieCommitMetadata, com.uber.hoodie.avro.model.HoodieCommitMetadata.class);
     // Do not archive Rolling Stats, cannot set to null since AVRO will throw null pointer
-    avroMetaData.getExtraMetadata().put(HoodieRollingStatMetadata.ROLLING_STAT_METADATA_KEY, StringUtils.EMPTY);
+    avroMetaData.getExtraMetadata().put(HoodieRollingStatMetadata.ROLLING_STAT_METADATA_KEY, "");
     return avroMetaData;
   }
 }

File: hoodie-client/src/main/java/com/uber/hoodie/table/HoodieCopyOnWriteTable.java
Patch:
@@ -35,6 +35,7 @@
 import com.uber.hoodie.common.table.timeline.HoodieActiveTimeline;
 import com.uber.hoodie.common.table.timeline.HoodieInstant;
 import com.uber.hoodie.common.util.FSUtils;
+import com.uber.hoodie.common.util.collection.Pair;
 import com.uber.hoodie.common.util.queue.BoundedInMemoryExecutor;
 import com.uber.hoodie.common.util.queue.BoundedInMemoryQueueConsumer;
 import com.uber.hoodie.config.HoodieWriteConfig;
@@ -63,7 +64,6 @@
 import java.util.stream.Collectors;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
-import org.apache.commons.lang3.tuple.Pair;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -863,4 +863,4 @@ public int getPartition(Object key) {
   protected HoodieRollingStatMetadata getRollingStats() {
     return null;
   }
-}
\ No newline at end of file
+}

File: hoodie-client/src/main/java/com/uber/hoodie/table/WorkloadStat.java
Patch:
@@ -17,9 +17,9 @@
 package com.uber.hoodie.table;
 
 import com.uber.hoodie.common.model.HoodieRecordLocation;
+import com.uber.hoodie.common.util.collection.Pair;
 import java.io.Serializable;
 import java.util.HashMap;
-import org.apache.commons.lang3.tuple.Pair;
 
 /**
  * Wraps stats about a single partition path.

File: hoodie-client/src/test/java/com/uber/hoodie/io/strategy/TestHoodieCompactionStrategy.java
Patch:
@@ -25,6 +25,7 @@
 import com.uber.hoodie.avro.model.HoodieCompactionOperation;
 import com.uber.hoodie.common.model.HoodieDataFile;
 import com.uber.hoodie.common.model.HoodieLogFile;
+import com.uber.hoodie.common.util.collection.Pair;
 import com.uber.hoodie.config.HoodieCompactionConfig;
 import com.uber.hoodie.config.HoodieWriteConfig;
 import com.uber.hoodie.io.compact.strategy.BoundedIOCompactionStrategy;
@@ -37,7 +38,6 @@
 import java.util.Optional;
 import java.util.Random;
 import java.util.stream.Collectors;
-import org.apache.commons.lang3.tuple.Pair;
 import org.junit.Assert;
 import org.junit.Test;
 

File: hoodie-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/realtime/AbstractRealtimeRecordReader.java
Patch:
@@ -25,6 +25,7 @@
 import com.uber.hoodie.common.table.log.block.HoodieLogBlock;
 import com.uber.hoodie.common.util.FSUtils;
 import com.uber.hoodie.common.util.HoodieAvroUtils;
+import com.uber.hoodie.common.util.collection.Pair;
 import com.uber.hoodie.exception.HoodieException;
 import com.uber.hoodie.exception.HoodieIOException;
 import java.io.IOException;
@@ -40,7 +41,6 @@
 import org.apache.avro.generic.GenericArray;
 import org.apache.avro.generic.GenericFixed;
 import org.apache.avro.generic.GenericRecord;
-import org.apache.commons.lang3.tuple.Pair;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;

File: hoodie-hadoop-mr/src/test/java/com/uber/hoodie/hadoop/TestRecordReaderValueIterator.java
Patch:
@@ -18,11 +18,11 @@
 
 package com.uber.hoodie.hadoop;
 
+import com.uber.hoodie.common.util.collection.Pair;
 import java.io.IOException;
 import java.util.List;
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
-import org.apache.commons.lang3.tuple.Pair;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapred.RecordReader;

File: hoodie-hive/src/main/java/com/uber/hoodie/hive/HoodieHiveClient.java
Patch:
@@ -29,6 +29,7 @@
 import com.uber.hoodie.common.table.HoodieTimeline;
 import com.uber.hoodie.common.table.timeline.HoodieInstant;
 import com.uber.hoodie.common.util.FSUtils;
+import com.uber.hoodie.common.util.collection.Pair;
 import com.uber.hoodie.exception.HoodieIOException;
 import com.uber.hoodie.exception.InvalidDatasetException;
 import com.uber.hoodie.hive.util.SchemaUtil;
@@ -47,7 +48,6 @@
 import org.apache.commons.dbcp.BasicDataSource;
 import org.apache.commons.dbcp.ConnectionFactory;
 import org.apache.commons.dbcp.DriverConnectionFactory;
-import org.apache.commons.lang3.tuple.Pair;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/log/HoodieLogFormatReader.java
Patch:
@@ -94,4 +94,4 @@ public HoodieLogFile getLogFile() {
   public void remove() {
   }
 
-}
\ No newline at end of file
+}

File: hoodie-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/HoodieROTablePathFilter.java
Patch:
@@ -126,7 +126,7 @@ public boolean accept(Path path) {
           HoodieTableMetaClient metaClient = new HoodieTableMetaClient(fs.getConf(),
               baseDir.toString());
           HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,
-              metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),
+              metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants(),
               fs.listStatus(folder));
           List<HoodieDataFile> latestFiles = fsView.getLatestDataFiles()
               .collect(Collectors.toList());

File: hoodie-spark/src/main/java/com/uber/hoodie/KeyGenerator.java
Patch:
@@ -19,9 +19,9 @@
 package com.uber.hoodie;
 
 import com.uber.hoodie.common.model.HoodieKey;
+import com.uber.hoodie.common.util.TypedProperties;
 import java.io.Serializable;
 import org.apache.avro.generic.GenericRecord;
-import org.apache.commons.configuration.PropertiesConfiguration;
 
 /**
  * Abstract class to extend for plugging in extraction of
@@ -30,9 +30,9 @@
  */
 public abstract class KeyGenerator implements Serializable {
 
-  protected transient PropertiesConfiguration config;
+  protected transient TypedProperties config;
 
-  protected KeyGenerator(PropertiesConfiguration config) {
+  protected KeyGenerator(TypedProperties config) {
     this.config = config;
   }
 

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/HoodieCompactor.java
Patch:
@@ -80,7 +80,8 @@ public static void main(String[] args) throws Exception {
       System.exit(1);
     }
     HoodieCompactor compactor = new HoodieCompactor(cfg);
-    compactor.compact(UtilHelpers.buildSparkContext(cfg.tableName, cfg.sparkMaster, cfg.sparkMemory), cfg.retry);
+    compactor.compact(UtilHelpers.buildSparkContext("compactor-" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory),
+        cfg.retry);
   }
 
   public int compact(JavaSparkContext jsc, int retry) {
@@ -119,4 +120,4 @@ private int doSchedule(JavaSparkContext jsc) throws Exception {
     client.scheduleCompactionAtInstant(cfg.compactionInstantTime, Optional.empty());
     return 0;
   }
-}
\ No newline at end of file
+}

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/keygen/TimestampBasedKeyGenerator.java
Patch:
@@ -21,6 +21,7 @@
 import com.uber.hoodie.DataSourceUtils;
 import com.uber.hoodie.SimpleKeyGenerator;
 import com.uber.hoodie.common.model.HoodieKey;
+import com.uber.hoodie.common.util.TypedProperties;
 import com.uber.hoodie.exception.HoodieNotSupportedException;
 import com.uber.hoodie.utilities.exception.HoodieDeltaStreamerException;
 import java.io.Serializable;
@@ -30,7 +31,6 @@
 import java.util.Date;
 import java.util.TimeZone;
 import org.apache.avro.generic.GenericRecord;
-import org.apache.commons.configuration.PropertiesConfiguration;
 
 /**
  * Key generator, that relies on timestamps for partitioning field. Still picks record key by name.
@@ -64,7 +64,7 @@ static class Config {
         + ".dateformat";
   }
 
-  public TimestampBasedKeyGenerator(PropertiesConfiguration config) {
+  public TimestampBasedKeyGenerator(TypedProperties config) {
     super(config);
     DataSourceUtils.checkRequiredProperties(config,
         Arrays.asList(Config.TIMESTAMP_TYPE_FIELD_PROP, Config.TIMESTAMP_OUTPUT_DATE_FORMAT_PROP));

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieAppendHandle.java
Patch:
@@ -246,7 +246,6 @@ public WriteStatus close() {
       if (writer != null) {
         writer.close();
       }
-      writeStatus.getStat().setPrevCommit(commitTime);
       writeStatus.getStat().setFileId(this.fileId);
       writeStatus.getStat().setNumWrites(recordsWritten);
       writeStatus.getStat().setNumUpdateWrites(updatedRecordsWritten);

File: hoodie-client/src/test/java/com/uber/hoodie/table/TestMergeOnReadTable.java
Patch:
@@ -488,6 +488,7 @@ public void testRollbackWithDeltaAndCompactionCommit() throws Exception {
     recordsRead = HoodieMergeOnReadTestUtils.getRecordsUsingInputFormat(dataFiles, basePath);
     assertEquals(recordsRead.size(), 200);
 
+    writeRecords = jsc.parallelize(copyOfRecords, 1);
     writeStatusJavaRDD = client.upsert(writeRecords, commitTime2);
     client.commit(commitTime2, writeStatusJavaRDD);
     statuses = writeStatusJavaRDD.collect();

File: hoodie-client/src/main/java/com/uber/hoodie/table/HoodieMergeOnReadTable.java
Patch:
@@ -296,7 +296,9 @@ public List<HoodieRollbackStat> rollback(JavaSparkContext jsc, List<String> comm
                                 "Failed to rollback for commit " + commit, io);
                           } finally {
                             try {
-                              writer.close();
+                              if (writer != null) {
+                                writer.close();
+                              }
                             } catch (IOException io) {
                               throw new UncheckedIOException(io);
                             }

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/log/HoodieLogFormatWriter.java
Patch:
@@ -72,6 +72,7 @@ public class HoodieLogFormatWriter implements HoodieLogFormat.Writer {
       try {
         this.output = fs.append(path, bufferSize);
       } catch (RemoteException e) {
+        log.warn("Remote Exception, attempting to handle or recover lease", e);
         handleAppendExceptionOrRecoverLease(path, e);
       } catch (IOException ioe) {
         if (ioe.getMessage().equalsIgnoreCase("Not supported")) {

File: hoodie-cli/src/main/java/com/uber/hoodie/cli/Table.java
Patch:
@@ -144,7 +144,8 @@ private void sortAndLimit() {
         if (fieldNameToConverterMap.containsKey(fieldName)) {
           return fieldNameToConverterMap.get(fieldName).apply(row.get(idx));
         }
-        return row.get(idx).toString();
+        Object v = row.get(idx);
+        return v == null ? "null" : v.toString();
       }).collect(Collectors.toList());
     }).collect(Collectors.toList());
   }

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieCreateHandle.java
Patch:
@@ -75,7 +75,7 @@ public HoodieCreateHandle(HoodieWriteConfig config, String commitTime, HoodieTab
       throw new HoodieInsertException(
           "Failed to initialize HoodieStorageWriter for path " + getStorageWriterPath(), e);
     }
-    logger.info("New InsertHandle for partition :" + partitionPath);
+    logger.info("New InsertHandle for partition :" + partitionPath + " with fileId " + fileId);
   }
 
   public HoodieCreateHandle(HoodieWriteConfig config, String commitTime, HoodieTable<T> hoodieTable,

File: hoodie-common/src/main/java/com/uber/hoodie/common/util/AvroUtils.java
Patch:
@@ -143,7 +143,7 @@ public static HoodieSavepointMetadata convertSavepointMetadata(String user, Stri
         partitionMetadataBuilder.build());
   }
 
-  public static Optional<byte[]> serializeCompactionWorkload(HoodieCompactionPlan compactionWorkload)
+  public static Optional<byte[]> serializeCompactionPlan(HoodieCompactionPlan compactionWorkload)
       throws IOException {
     return serializeAvroMetadata(compactionWorkload, HoodieCompactionPlan.class);
   }

File: hoodie-common/src/main/java/com/uber/hoodie/common/util/CompactionUtils.java
Patch:
@@ -21,6 +21,7 @@
 import com.uber.hoodie.common.model.CompactionOperation;
 import com.uber.hoodie.common.model.FileSlice;
 import com.uber.hoodie.common.table.HoodieTableMetaClient;
+import com.uber.hoodie.common.table.HoodieTimeline;
 import com.uber.hoodie.common.table.timeline.HoodieInstant;
 import com.uber.hoodie.exception.HoodieException;
 import java.io.IOException;
@@ -114,7 +115,8 @@ public static List<Pair<HoodieInstant, HoodieCompactionPlan>> getAllPendingCompa
     return pendingCompactionInstants.stream().map(instant -> {
       try {
         HoodieCompactionPlan compactionPlan = AvroUtils.deserializeCompactionPlan(
-            metaClient.getActiveTimeline().getInstantDetails(instant).get());
+            metaClient.getActiveTimeline().getInstantAuxiliaryDetails(
+                HoodieTimeline.getCompactionRequestedInstant(instant.getTimestamp())).get());
         return Pair.of(instant, compactionPlan);
       } catch (IOException e) {
         throw new HoodieException(e);

File: hoodie-common/src/main/java/com/uber/hoodie/common/model/HoodieRecord.java
Patch:
@@ -102,7 +102,7 @@ public HoodieRecord setNewLocation(HoodieRecordLocation location) {
   }
 
   public Optional<HoodieRecordLocation> getNewLocation() {
-    return Optional.of(this.newLocation);
+    return Optional.ofNullable(this.newLocation);
   }
 
   public boolean isCurrentLocationKnown() {

File: hoodie-client/src/main/java/com/uber/hoodie/config/HoodieWriteConfig.java
Patch:
@@ -41,7 +41,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {
   public static final String TABLE_NAME = "hoodie.table.name";
   private static final String BASE_PATH_PROP = "hoodie.base.path";
   private static final String AVRO_SCHEMA = "hoodie.avro.schema";
-  private static final String DEFAULT_PARALLELISM = "200";
+  private static final String DEFAULT_PARALLELISM = "1500";
   private static final String INSERT_PARALLELISM = "hoodie.insert.shuffle.parallelism";
   private static final String BULKINSERT_PARALLELISM = "hoodie.bulkinsert.shuffle.parallelism";
   private static final String UPSERT_PARALLELISM = "hoodie.upsert.shuffle.parallelism";

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieMergeHandle.java
Patch:
@@ -141,8 +141,9 @@ private String init(String fileId, Iterator<HoodieRecord<T>> newRecordsItr) {
     try {
       // Load the new records in a map
       logger.info("MaxMemoryPerPartitionMerge => " + config.getMaxMemoryPerPartitionMerge());
-      this.keyToNewRecords = new ExternalSpillableMap<>(config.getMaxMemoryPerPartitionMerge(), Optional.empty(),
-          new StringConverter(), new HoodieRecordConverter(schema, config.getPayloadClass()));
+      this.keyToNewRecords = new ExternalSpillableMap<>(config.getMaxMemoryPerPartitionMerge(),
+          config.getSpillableMapBasePath(), new StringConverter(),
+          new HoodieRecordConverter(schema, config.getPayloadClass()));
     } catch (IOException io) {
       throw new HoodieIOException("Cannot instantiate an ExternalSpillableMap", io);
     }

File: hoodie-client/src/main/java/com/uber/hoodie/io/compact/HoodieRealtimeTableCompactor.java
Patch:
@@ -118,7 +118,8 @@ private List<WriteStatus> compact(HoodieTable hoodieTable, HoodieWriteConfig con
     HoodieCompactedLogRecordScanner scanner = new HoodieCompactedLogRecordScanner(fs,
         metaClient.getBasePath(), operation.getDeltaFilePaths(), readerSchema, maxInstantTime,
         config.getMaxMemoryPerCompaction(), config.getCompactionLazyBlockReadEnabled(),
-        config.getCompactionReverseLogReadEnabled(), config.getMaxDFSStreamBufferSize());
+        config.getCompactionReverseLogReadEnabled(), config.getMaxDFSStreamBufferSize(),
+        config.getSpillableMapBasePath());
     if (!scanner.iterator().hasNext()) {
       return Lists.<WriteStatus>newArrayList();
     }

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/log/HoodieCompactedLogRecordScanner.java
Patch:
@@ -42,7 +42,6 @@
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.Optional;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.stream.Collectors;
 import org.apache.avro.Schema;
@@ -98,7 +97,7 @@ public class HoodieCompactedLogRecordScanner implements
 
   public HoodieCompactedLogRecordScanner(FileSystem fs, String basePath, List<String> logFilePaths,
       Schema readerSchema, String latestInstantTime, Long maxMemorySizeInBytes,
-      boolean readBlocksLazily, boolean reverseReader, int bufferSize) {
+      boolean readBlocksLazily, boolean reverseReader, int bufferSize, String spillableMapBasePath) {
     this.readerSchema = readerSchema;
     this.latestInstantTime = latestInstantTime;
     this.hoodieTableMetaClient = new HoodieTableMetaClient(fs.getConf(), basePath);
@@ -109,7 +108,7 @@ public HoodieCompactedLogRecordScanner(FileSystem fs, String basePath, List<Stri
 
     try {
       // Store merged records for all versions for this log file, set the in-memory footprint to maxInMemoryMapSize
-      this.records = new ExternalSpillableMap<>(maxMemorySizeInBytes, Optional.empty(),
+      this.records = new ExternalSpillableMap<>(maxMemorySizeInBytes, spillableMapBasePath,
           new StringConverter(), new HoodieRecordConverter(readerSchema, payloadClassFQN));
       // iterate over the paths
       HoodieLogFormatReader logFormatReaderWrapper =

File: hoodie-common/src/main/java/com/uber/hoodie/common/util/collection/ExternalSpillableMap.java
Patch:
@@ -25,7 +25,6 @@
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.Map;
-import java.util.Optional;
 import java.util.Set;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
@@ -64,7 +63,7 @@ public class ExternalSpillableMap<T, R> implements Map<T, R> {
   // Flag to determine whether to stop re-estimating payload size
   private boolean shouldEstimatePayloadSize = true;
 
-  public ExternalSpillableMap(Long maxInMemorySizeInBytes, Optional<String> baseFilePath,
+  public ExternalSpillableMap(Long maxInMemorySizeInBytes, String baseFilePath,
       Converter<T> keyConverter, Converter<R> valueConverter) throws IOException {
     this.inMemoryMap = new HashMap<>();
     this.diskBasedMap = new DiskBasedMap<>(baseFilePath, keyConverter, valueConverter);

File: hoodie-client/src/test/java/com/uber/hoodie/common/HoodieClientTestUtils.java
Patch:
@@ -99,7 +99,6 @@ public static void fakeDataFile(String basePath, String partitionPath, String co
   }
 
   public static SparkConf getSparkConfForTest(String appName) {
-    System.out.println("HIII" + "HII2");
     SparkConf sparkConf = new SparkConf().setAppName(appName)
         .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
         .setMaster("local[1]");
@@ -125,6 +124,7 @@ public static Dataset<Row> readCommit(String basePath, SQLContext sqlContext, Ho
     try {
       HashMap<String, String> paths = getLatestFileIDsToFullPath(basePath, commitTimeline,
           Arrays.asList(commitInstant));
+      System.out.println("Path :" + paths.values());
       return sqlContext.read().parquet(paths.values().toArray(new String[paths.size()]))
           .filter(String.format("%s ='%s'", HoodieRecord.COMMIT_TIME_METADATA_FIELD, commitTime));
     } catch (Exception e) {

File: hoodie-cli/src/main/java/com/uber/hoodie/cli/commands/HoodieLogFileCommand.java
Patch:
@@ -31,6 +31,7 @@
 import com.uber.hoodie.common.table.log.block.HoodieLogBlock.HeaderMetadataType;
 import com.uber.hoodie.common.table.log.block.HoodieLogBlock.HoodieLogBlockType;
 import com.uber.hoodie.config.HoodieCompactionConfig;
+import com.uber.hoodie.config.HoodieMemoryConfig;
 import com.uber.hoodie.hive.util.SchemaUtil;
 import java.io.IOException;
 import java.util.ArrayList;
@@ -173,7 +174,7 @@ public String showLogFileRecords(
           HoodieCLI.tableMetadata.getBasePath(), logFilePaths, readerSchema,
           HoodieCLI.tableMetadata.getActiveTimeline().getCommitTimeline().lastInstant().get()
               .getTimestamp(),
-          Long.valueOf(HoodieCompactionConfig.DEFAULT_MAX_SIZE_IN_MEMORY_PER_COMPACTION_IN_BYTES),
+          Long.valueOf(HoodieMemoryConfig.DEFAULT_MAX_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES),
           Boolean.valueOf(HoodieCompactionConfig.DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED),
           Boolean.valueOf(HoodieCompactionConfig.DEFAULT_COMPACTION_REVERSE_LOG_READ_ENABLED));
       for (HoodieRecord<? extends HoodieRecordPayload> hoodieRecord : scanner) {

File: hoodie-client/src/main/java/com/uber/hoodie/io/compact/HoodieRealtimeTableCompactor.java
Patch:
@@ -98,11 +98,12 @@ private List<WriteStatus> compact(HoodieTable hoodieTable,
         .getTimelineOfActions(
             Sets.newHashSet(HoodieTimeline.COMMIT_ACTION, HoodieTimeline.ROLLBACK_ACTION,
                 HoodieTimeline.DELTA_COMMIT_ACTION))
-        .filterCompletedInstants().lastInstant().get().getTimestamp();
 
+        .filterCompletedInstants().lastInstant().get().getTimestamp();
+    log.info("MaxMemoryPerCompaction => " + config.getMaxMemoryPerCompaction());
     HoodieCompactedLogRecordScanner scanner = new HoodieCompactedLogRecordScanner(fs,
         metaClient.getBasePath(), operation.getDeltaFilePaths(), readerSchema, maxInstantTime,
-        config.getMaxMemorySizePerCompactionInBytes(), config.getCompactionLazyBlockReadEnabled(),
+        config.getMaxMemoryPerCompaction(), config.getCompactionLazyBlockReadEnabled(),
         config.getCompactionReverseLogReadEnabled());
     if (!scanner.iterator().hasNext()) {
       return Lists.<WriteStatus>newArrayList();

File: hoodie-common/src/test/java/com/uber/hoodie/common/util/SpillableMapTestUtils.java
Patch:
@@ -36,8 +36,8 @@ public static List<String> upsertRecords(List<IndexedRecord> iRecords,
     iRecords
         .stream()
         .forEach(r -> {
-          String key = ((GenericRecord)r).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();
-          String partitionPath = ((GenericRecord)r).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString();
+          String key = ((GenericRecord) r).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();
+          String partitionPath = ((GenericRecord) r).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString();
           recordKeys.add(key);
           records.put(key, new HoodieRecord<>(new HoodieKey(key, partitionPath),
               new HoodieAvroPayload(Optional.of((GenericRecord) r))));

File: hoodie-common/src/test/java/com/uber/hoodie/common/table/log/HoodieLogFormatTest.java
Patch:
@@ -374,13 +374,15 @@ public void testBasicAppendAndRead()
     assertEquals("Both records lists should be the same. (ordering guaranteed)", copyOfRecords1,
         dataBlockRead.getRecords());
 
+    reader.hasNext();
     nextBlock = reader.next();
     dataBlockRead = (HoodieAvroDataBlock) nextBlock;
     assertEquals("Read records size should be equal to the written records size",
         copyOfRecords2.size(), dataBlockRead.getRecords().size());
     assertEquals("Both records lists should be the same. (ordering guaranteed)", copyOfRecords2,
         dataBlockRead.getRecords());
 
+    reader.hasNext();
     nextBlock = reader.next();
     dataBlockRead = (HoodieAvroDataBlock) nextBlock;
     assertEquals("Read records size should be equal to the written records size",

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieAppendHandle.java
Patch:
@@ -47,6 +47,7 @@
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Comparator;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -108,8 +109,8 @@ private void init(String partitionPath) {
         this.writer = HoodieLogFormat.newWriterBuilder()
             .onParentPath(new Path(hoodieTable.getMetaClient().getBasePath(), partitionPath))
             .withFileId(fileId).overBaseCommit(baseCommitTime).withLogVersion(fileSlice.getLogFiles()
-                .max(HoodieLogFile.getLogVersionComparator().reversed()::compare)
-                .map(logFile -> logFile.getLogVersion()).orElse(HoodieLogFile.LOGFILE_BASE_VERSION))
+                .map(logFile -> logFile.getLogVersion())
+                .max(Comparator.naturalOrder()).orElse(HoodieLogFile.LOGFILE_BASE_VERSION))
             .withSizeThreshold(config.getLogFileMaxSize())
             .withFs(fs).withFileExtension(HoodieLogFile.DELTA_EXTENSION).build();
         this.currentLogFile = writer.getLogFile();

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieMergeHandle.java
Patch:
@@ -142,6 +142,7 @@ private void init(String fileId, String partitionPath) {
    */
   private String init(String fileId, Iterator<HoodieRecord<T>> newRecordsItr) {
     // Load the new records in a map
+    // TODO (NA) instantiate a ExternalSpillableMap
     this.keyToNewRecords = new HashMap<>();
     String partitionPath = null;
     while (newRecordsItr.hasNext()) {

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/timeline/HoodieActiveTimeline.java
Patch:
@@ -24,7 +24,6 @@
 import com.uber.hoodie.exception.HoodieIOException;
 import java.io.IOException;
 import java.io.Serializable;
-import java.text.SimpleDateFormat;
 import java.util.Arrays;
 import java.util.Comparator;
 import java.util.Date;
@@ -35,6 +34,7 @@
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 import org.apache.commons.io.IOUtils;
+import org.apache.commons.lang3.time.FastDateFormat;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.Path;
@@ -51,7 +51,7 @@
  */
 public class HoodieActiveTimeline extends HoodieDefaultTimeline {
 
-  public static final SimpleDateFormat COMMIT_FORMATTER = new SimpleDateFormat("yyyyMMddHHmmss");
+  public static final FastDateFormat COMMIT_FORMATTER = FastDateFormat.getInstance("yyyyMMddHHmmss");
 
   private final transient static Logger log = LogManager.getLogger(HoodieActiveTimeline.class);
   private HoodieTableMetaClient metaClient;

File: hoodie-client/src/main/java/com/uber/hoodie/HoodieWriteClient.java
Patch:
@@ -318,7 +318,8 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile,
                 partitionStat.getUpdateLocationToCount().entrySet().stream().forEach(entry -> {
                     HoodieWriteStat writeStat = new HoodieWriteStat();
                     writeStat.setFileId(entry.getKey());
-                    writeStat.setNumUpdateWrites(entry.getValue());
+                    writeStat.setPrevCommit(entry.getValue().getKey());
+                    writeStat.setNumUpdateWrites(entry.getValue().getValue());
                     metadata.addWriteStat(path.toString(), writeStat);
                 });
             });

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieAppendHandle.java
Patch:
@@ -187,7 +187,7 @@ public void doAppend() {
       }
     } catch (Exception e) {
       throw new HoodieAppendException(
-          "Failed while appeding records to " + currentLogFile.getPath(), e);
+          "Failed while appending records to " + currentLogFile.getPath(), e);
     }
   }
 

File: hoodie-client/src/main/java/com/uber/hoodie/table/HoodieCopyOnWriteTable.java
Patch:
@@ -54,6 +54,7 @@
 import java.util.stream.Collectors;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
+import org.apache.commons.lang3.tuple.Pair;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -199,7 +200,7 @@ class UpsertPartitioner extends Partitioner {
     private void assignUpdates(WorkloadProfile profile) {
       // each update location gets a partition
       WorkloadStat gStat = profile.getGlobalStat();
-      for (Map.Entry<String, Long> updateLocEntry : gStat.getUpdateLocationToCount().entrySet()) {
+      for (Map.Entry<String, Pair<String, Long>> updateLocEntry : gStat.getUpdateLocationToCount().entrySet()) {
         addUpdateBucket(updateLocEntry.getKey());
       }
     }

File: hoodie-client/src/test/java/com/uber/hoodie/table/TestMergeOnReadTable.java
Patch:
@@ -463,7 +463,7 @@ public void testRollbackWithDeltaAndCompactionCommit() throws Exception {
     newCommitTime = "002";
     client.startCommitWithTime(newCommitTime);
 
-    records = dataGen.generateUpdates(newCommitTime, 200);
+    records = dataGen.generateUpdates(newCommitTime, records);
 
     statuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();
     // Verify there are no errors
@@ -556,6 +556,7 @@ private HoodieWriteConfig.Builder getConfigBuilder(Boolean autoCommit) {
     return HoodieWriteConfig.newBuilder().withPath(basePath)
         .withSchema(TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
         .withAutoCommit(autoCommit)
+        .withAssumeDatePartitioning(true)
         .withCompactionConfig(
             HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024)
                 .withInlineCompaction(false).build())

File: hoodie-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/realtime/HoodieRealtimeInputFormat.java
Patch:
@@ -124,7 +124,7 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
               // Get the maxCommit from the last delta or compaction or commit - when bootstrapped from COW table
               String maxCommitTime = metaClient.getActiveTimeline()
                   .getTimelineOfActions(
-                      Sets.newHashSet(HoodieTimeline.COMMIT_ACTION,
+                      Sets.newHashSet(HoodieTimeline.COMMIT_ACTION, HoodieTimeline.ROLLBACK_ACTION,
                           HoodieTimeline.DELTA_COMMIT_ACTION))
                   .filterCompletedInstants().lastInstant().get().getTimestamp();
               rtSplits.add(

File: hoodie-cli/src/main/java/com/uber/hoodie/cli/HoodieCLI.java
Patch:
@@ -17,6 +17,7 @@
 package com.uber.hoodie.cli;
 
 import com.uber.hoodie.common.table.HoodieTableMetaClient;
+import com.uber.hoodie.common.util.FSUtils;
 import java.io.IOException;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
@@ -36,7 +37,7 @@ public enum CLIState {
 
   public static boolean initConf() {
     if (HoodieCLI.conf == null) {
-      HoodieCLI.conf = new Configuration();
+      HoodieCLI.conf = FSUtils.prepareHadoopConf(new Configuration());
       return true;
     }
     return false;

File: hoodie-cli/src/main/java/com/uber/hoodie/cli/commands/CleansCommand.java
Patch:
@@ -77,7 +77,7 @@ public String showCleans() throws IOException {
   @CliCommand(value = "cleans refresh", help = "Refresh the commits")
   public String refreshCleans() throws IOException {
     HoodieTableMetaClient metadata =
-        new HoodieTableMetaClient(HoodieCLI.fs, HoodieCLI.tableMetadata.getBasePath());
+        new HoodieTableMetaClient(HoodieCLI.conf, HoodieCLI.tableMetadata.getBasePath());
     HoodieCLI.setTableMetadata(metadata);
     return "Metadata for table " + metadata.getTableConfig().getTableName() + " refreshed.";
   }

File: hoodie-cli/src/main/java/com/uber/hoodie/cli/commands/DatasetsCommand.java
Patch:
@@ -33,7 +33,7 @@ public String connect(
       final String path) throws IOException {
     boolean initialized = HoodieCLI.initConf();
     HoodieCLI.initFS(initialized);
-    HoodieCLI.setTableMetadata(new HoodieTableMetaClient(HoodieCLI.fs, path));
+    HoodieCLI.setTableMetadata(new HoodieTableMetaClient(HoodieCLI.conf, path));
     HoodieCLI.state = HoodieCLI.CLIState.DATASET;
     return "Metadata for table " + HoodieCLI.tableMetadata.getTableConfig().getTableName()
         + " loaded";

File: hoodie-cli/src/main/java/com/uber/hoodie/cli/commands/SavepointsCommand.java
Patch:
@@ -138,7 +138,7 @@ public String rollbackToSavepoint(
   @CliCommand(value = "savepoints refresh", help = "Refresh the savepoints")
   public String refreshMetaClient() throws IOException {
     HoodieTableMetaClient metadata =
-        new HoodieTableMetaClient(HoodieCLI.fs, HoodieCLI.tableMetadata.getBasePath());
+        new HoodieTableMetaClient(HoodieCLI.conf, HoodieCLI.tableMetadata.getBasePath());
     HoodieCLI.setTableMetadata(metadata);
     return "Metadata for table " + metadata.getTableConfig().getTableName() + " refreshed.";
   }

File: hoodie-cli/src/main/java/com/uber/hoodie/cli/commands/SparkMain.java
Patch:
@@ -97,8 +97,8 @@ private static int deduplicatePartitionPath(JavaSparkContext jsc,
       String repairedOutputPath,
       String basePath)
       throws Exception {
-    DedupeSparkJob job = new DedupeSparkJob(basePath,
-        duplicatedPartitionPath, repairedOutputPath, new SQLContext(jsc), FSUtils.getFs());
+    DedupeSparkJob job = new DedupeSparkJob(basePath, duplicatedPartitionPath, repairedOutputPath,
+        new SQLContext(jsc), FSUtils.getFs(basePath, jsc.hadoopConfiguration()));
     job.fixDuplicates(true);
     return 0;
   }

File: hoodie-cli/src/main/java/com/uber/hoodie/cli/utils/SparkUtil.java
Patch:
@@ -18,6 +18,7 @@
 
 import com.uber.hoodie.HoodieWriteClient;
 import com.uber.hoodie.cli.commands.SparkMain;
+import com.uber.hoodie.common.util.FSUtils;
 import java.io.File;
 import java.net.URISyntaxException;
 import org.apache.log4j.Logger;
@@ -66,6 +67,7 @@ public static JavaSparkContext initJavaSparkConf(String name) {
     sparkConf = HoodieWriteClient.registerClasses(sparkConf);
     JavaSparkContext jsc = new JavaSparkContext(sparkConf);
     jsc.hadoopConfiguration().setBoolean("parquet.enable.summary-metadata", false);
+    FSUtils.prepareHadoopConf(jsc.hadoopConfiguration());
     return jsc;
   }
 }

File: hoodie-client/src/main/java/com/uber/hoodie/func/LazyInsertIterable.java
Patch:
@@ -72,7 +72,8 @@ protected List<WriteStatus> computeNext() {
         HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(hoodieConfig,
             commitTime,
             record.getPartitionPath(),
-            TaskContext.getPartitionId());
+            TaskContext.getPartitionId(),
+            hoodieTable);
         partitionsCleaned.add(record.getPartitionPath());
       }
 

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieCleanHelper.java
Patch:
@@ -33,7 +33,6 @@
 import java.util.List;
 import java.util.Optional;
 import java.util.stream.Collectors;
-import org.apache.hadoop.fs.FileSystem;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
@@ -51,14 +50,12 @@ public class HoodieCleanHelper<T extends HoodieRecordPayload<T>> {
   private final HoodieTimeline commitTimeline;
   private HoodieTable<T> hoodieTable;
   private HoodieWriteConfig config;
-  private FileSystem fs;
 
   public HoodieCleanHelper(HoodieTable<T> hoodieTable, HoodieWriteConfig config) {
     this.hoodieTable = hoodieTable;
     this.fileSystemView = hoodieTable.getCompletedFileSystemView();
     this.commitTimeline = hoodieTable.getCompletedCommitTimeline();
     this.config = config;
-    this.fs = hoodieTable.getFs();
   }
 
 

File: hoodie-client/src/test/java/HoodieClientExample.java
Patch:
@@ -81,7 +81,7 @@ public void run() throws Exception {
 
     // initialize the table, if not done already
     Path path = new Path(tablePath);
-    FileSystem fs = FSUtils.getFs();
+    FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());
     if (!fs.exists(path)) {
       HoodieTableMetaClient
           .initTableType(fs, tablePath, HoodieTableType.valueOf(tableType), tableName,

File: hoodie-client/src/test/java/com/uber/hoodie/common/HoodieClientTestUtils.java
Patch:
@@ -174,7 +174,7 @@ public static Dataset<Row> read(String basePath,
     List<String> filteredPaths = new ArrayList<>();
     try {
       HoodieTable hoodieTable = HoodieTable
-          .getHoodieTable(new HoodieTableMetaClient(fs, basePath, true), null);
+          .getHoodieTable(new HoodieTableMetaClient(fs.getConf(), basePath, true), null);
       for (String path : paths) {
         TableFileSystemView.ReadOptimizedView fileSystemView = new HoodieTableFileSystemView(
             hoodieTable.getMetaClient(),

File: hoodie-client/src/test/java/com/uber/hoodie/common/HoodieTestDataGenerator.java
Patch:
@@ -20,6 +20,7 @@
 import com.uber.hoodie.common.model.HoodieKey;
 import com.uber.hoodie.common.model.HoodiePartitionMetadata;
 import com.uber.hoodie.common.model.HoodieRecord;
+import com.uber.hoodie.common.model.HoodieTestUtils;
 import com.uber.hoodie.common.table.HoodieTableMetaClient;
 import com.uber.hoodie.common.table.HoodieTimeline;
 import com.uber.hoodie.common.util.FSUtils;
@@ -193,7 +194,7 @@ public static void createCommitFile(String basePath, String commitTime) throws I
     Path commitFile =
         new Path(basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/" + HoodieTimeline
             .makeCommitFileName(commitTime));
-    FileSystem fs = FSUtils.getFs();
+    FileSystem fs = FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf());
     FSDataOutputStream os = fs.create(commitFile, true);
     HoodieCommitMetadata commitMetadata = new HoodieCommitMetadata();
     try {
@@ -209,7 +210,7 @@ public static void createSavepointFile(String basePath, String commitTime) throw
     Path commitFile =
         new Path(basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/" + HoodieTimeline
             .makeSavePointFileName(commitTime));
-    FileSystem fs = FSUtils.getFs();
+    FileSystem fs = FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf());
     FSDataOutputStream os = fs.create(commitFile, true);
     HoodieCommitMetadata commitMetadata = new HoodieCommitMetadata();
     try {

File: hoodie-common/src/main/java/com/uber/hoodie/avro/MercifulJsonConverter.java
Patch:
@@ -77,7 +77,7 @@ private Object typeConvert(Object value, String name, Schema schema) throws IOEx
     switch (schema.getType()) {
       case BOOLEAN:
         if (value instanceof Boolean) {
-          return (Boolean) value;
+          return value;
         }
         break;
       case DOUBLE:

File: hoodie-common/src/main/java/com/uber/hoodie/common/model/ActionType.java
Patch:
@@ -17,5 +17,5 @@
 package com.uber.hoodie.common.model;
 
 public enum ActionType {
-  commit, savepoint, compaction, clean, rollback;
+  commit, savepoint, compaction, clean, rollback
 }

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/log/HoodieCompactedLogRecordScanner.java
Patch:
@@ -83,7 +83,7 @@ public HoodieCompactedLogRecordScanner(FileSystem fs, String basePath, List<Stri
       Schema readerSchema, String latestInstantTime) {
     this.readerSchema = readerSchema;
     this.latestInstantTime = latestInstantTime;
-    this.hoodieTableMetaClient = new HoodieTableMetaClient(fs, basePath);
+    this.hoodieTableMetaClient = new HoodieTableMetaClient(fs.getConf(), basePath);
     // load class from the payload fully qualified class name
     this.payloadClassFQN = this.hoodieTableMetaClient.getTableConfig().getPayloadClass();
 

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/log/HoodieLogFormat.java
Patch:
@@ -150,7 +150,7 @@ public WriterBuilder onParentPath(Path parentPath) {
     public Writer build() throws IOException, InterruptedException {
       log.info("Building HoodieLogFormat Writer");
       if (fs == null) {
-        fs = FSUtils.getFs();
+        throw new IllegalArgumentException("fs is not specified");
       }
       if (logFileId == null) {
         throw new IllegalArgumentException("FileID is not specified");

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/log/block/HoodieLogBlock.java
Patch:
@@ -58,7 +58,7 @@ public enum HoodieLogBlockType {
    */
   public enum LogMetadataType {
     INSTANT_TIME,
-    TARGET_INSTANT_TIME;
+    TARGET_INSTANT_TIME
   }
 
   public HoodieLogBlock(Map<LogMetadataType, String> logMetadata) {

File: hoodie-common/src/main/java/com/uber/hoodie/common/util/AvroUtils.java
Patch:
@@ -94,7 +94,7 @@ public static List<HoodieRecord<HoodieAvroPayload>> loadFromFile(FileSystem fs,
   public static HoodieCleanMetadata convertCleanMetadata(String startCleanTime,
       Optional<Long> durationInMs, List<HoodieCleanStat> cleanStats) {
     ImmutableMap.Builder<String, HoodieCleanPartitionMetadata> partitionMetadataBuilder =
-        ImmutableMap.<String, HoodieCleanPartitionMetadata>builder();
+        ImmutableMap.builder();
     int totalDeleted = 0;
     String earliestCommitToRetain = null;
     for (HoodieCleanStat stat : cleanStats) {
@@ -116,7 +116,7 @@ public static HoodieCleanMetadata convertCleanMetadata(String startCleanTime,
   public static HoodieRollbackMetadata convertRollbackMetadata(String startRollbackTime,
       Optional<Long> durationInMs, List<String> commits, List<HoodieRollbackStat> stats) {
     ImmutableMap.Builder<String, HoodieRollbackPartitionMetadata> partitionMetadataBuilder =
-        ImmutableMap.<String, HoodieRollbackPartitionMetadata>builder();
+        ImmutableMap.builder();
     int totalDeleted = 0;
     for (HoodieRollbackStat stat : stats) {
       HoodieRollbackPartitionMetadata metadata =
@@ -132,7 +132,7 @@ public static HoodieRollbackMetadata convertRollbackMetadata(String startRollbac
   public static HoodieSavepointMetadata convertSavepointMetadata(String user, String comment,
       Map<String, List<String>> latestFiles) {
     ImmutableMap.Builder<String, HoodieSavepointPartitionMetadata> partitionMetadataBuilder =
-        ImmutableMap.<String, HoodieSavepointPartitionMetadata>builder();
+        ImmutableMap.builder();
     for (Map.Entry<String, List<String>> stat : latestFiles.entrySet()) {
       HoodieSavepointPartitionMetadata metadata =
           new HoodieSavepointPartitionMetadata(stat.getKey(), stat.getValue());

File: hoodie-common/src/test/java/com/uber/hoodie/common/minicluster/HdfsTestService.java
Patch:
@@ -19,6 +19,7 @@
 
 import com.google.common.base.Preconditions;
 import com.google.common.io.Files;
+import com.uber.hoodie.common.model.HoodieTestUtils;
 import java.io.File;
 import java.io.IOException;
 import org.apache.commons.io.FileUtils;
@@ -54,7 +55,7 @@ public class HdfsTestService {
   private MiniDFSCluster miniDfsCluster;
 
   public HdfsTestService() {
-    hadoopConf = new Configuration();
+    hadoopConf = HoodieTestUtils.getDefaultHadoopConf();
     workDir = Files.createTempDir().getAbsolutePath();
   }
 
@@ -67,7 +68,7 @@ public MiniDFSCluster start(boolean format) throws IOException {
         .checkState(workDir != null, "The work dir must be set before starting cluster.");
 
     if (hadoopConf == null) {
-      hadoopConf = new Configuration();
+      hadoopConf = HoodieTestUtils.getDefaultHadoopConf();
     }
 
     // If clean, then remove the work dir so we can start fresh.

File: hoodie-common/src/test/java/com/uber/hoodie/common/table/string/HoodieActiveTimelineTest.java
Patch:
@@ -74,7 +74,7 @@ public void testLoadingInstantsFromFiles() throws IOException {
     HoodieInstant instant5 =
         new HoodieInstant(true, HoodieTimeline.COMMIT_ACTION, "9");
 
-    timeline = new HoodieActiveTimeline(HoodieTestUtils.fs, metaClient.getMetaPath());
+    timeline = new HoodieActiveTimeline(metaClient);
     timeline.saveAsComplete(instant1, Optional.empty());
     timeline.saveAsComplete(instant2, Optional.empty());
     timeline.saveAsComplete(instant3, Optional.empty());
@@ -98,7 +98,7 @@ public void testLoadingInstantsFromFiles() throws IOException {
 
   @Test
   public void testTimelineOperationsBasic() throws Exception {
-    timeline = new HoodieActiveTimeline(HoodieTestUtils.fs, metaClient.getMetaPath());
+    timeline = new HoodieActiveTimeline(metaClient);
     assertTrue(timeline.empty());
     assertEquals("", 0, timeline.countInstants());
     assertEquals("", Optional.empty(), timeline.firstInstant());

File: hoodie-common/src/test/java/com/uber/hoodie/common/table/view/HoodieTableFileSystemViewTest.java
Patch:
@@ -69,7 +69,7 @@ public void init() throws IOException {
   }
 
   private void refreshFsView(FileStatus[] statuses) {
-    metaClient = new HoodieTableMetaClient(HoodieTestUtils.fs, basePath, true);
+    metaClient = new HoodieTableMetaClient(HoodieTestUtils.fs.getConf(), basePath, true);
     if (statuses != null) {
       fsView = new HoodieTableFileSystemView(metaClient,
           metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),

File: hoodie-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/HoodieInputFormat.java
Patch:
@@ -295,6 +295,6 @@ protected static HoodieTableMetaClient getTableMetaClient(FileSystem fs, Path da
     }
     Path baseDir = HoodieHiveUtil.getNthParent(dataPath, levels);
     LOG.info("Reading hoodie metadata from path " + baseDir.toString());
-    return new HoodieTableMetaClient(fs, baseDir.toString());
+    return new HoodieTableMetaClient(fs.getConf(), baseDir.toString());
   }
 }

File: hoodie-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/HoodieROTablePathFilter.java
Patch:
@@ -19,6 +19,7 @@
 import com.uber.hoodie.common.model.HoodiePartitionMetadata;
 import com.uber.hoodie.common.table.HoodieTableMetaClient;
 import com.uber.hoodie.common.table.view.HoodieTableFileSystemView;
+import com.uber.hoodie.common.util.FSUtils;
 import com.uber.hoodie.exception.DatasetNotFoundException;
 import com.uber.hoodie.exception.HoodieException;
 import java.io.Serializable;
@@ -86,7 +87,7 @@ public boolean accept(Path path) {
     }
     Path folder = null;
     try {
-      FileSystem fs = path.getFileSystem(new Configuration());
+      FileSystem fs = path.getFileSystem(FSUtils.prepareHadoopConf(new Configuration()));
       if (fs.isDirectory(path)) {
         return true;
       }
@@ -123,7 +124,7 @@ public boolean accept(Path path) {
       if (baseDir != null) {
         try {
           HoodieTableMetaClient metaClient =
-              new HoodieTableMetaClient(fs, baseDir.toString());
+              new HoodieTableMetaClient(fs.getConf(), baseDir.toString());
           HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,
               metaClient.getActiveTimeline().getCommitTimeline()
                   .filterCompletedInstants(),

File: hoodie-hive/src/main/java/com/uber/hoodie/hive/HiveSyncTool.java
Patch:
@@ -30,6 +30,7 @@
 import java.util.Map;
 import java.util.Optional;
 import java.util.stream.Collectors;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.Partition;
@@ -183,7 +184,7 @@ public static void main(String[] args) throws Exception {
       cmd.usage();
       System.exit(1);
     }
-    FileSystem fs = FSUtils.getFs();
+    FileSystem fs = FSUtils.getFs(cfg.basePath, new Configuration());
     HiveConf hiveConf = new HiveConf();
     hiveConf.addResource(fs.getConf());
     new HiveSyncTool(cfg, hiveConf, fs).syncHoodieTable();

File: hoodie-hive/src/main/java/com/uber/hoodie/hive/HoodieHiveClient.java
Patch:
@@ -91,7 +91,7 @@ public class HoodieHiveClient {
   HoodieHiveClient(HiveSyncConfig cfg, HiveConf configuration, FileSystem fs) {
     this.syncConfig = cfg;
     this.fs = fs;
-    this.metaClient = new HoodieTableMetaClient(fs, cfg.basePath, true);
+    this.metaClient = new HoodieTableMetaClient(fs.getConf(), cfg.basePath, true);
     this.tableType = metaClient.getTableType();
 
     LOG.info("Creating hive connection " + cfg.jdbcUrl);

File: hoodie-hive/src/test/java/com/uber/hoodie/hive/util/HiveTestService.java
Patch:
@@ -20,6 +20,7 @@
 import com.google.common.base.Preconditions;
 import com.google.common.collect.Maps;
 import com.google.common.io.Files;
+import com.uber.hoodie.common.model.HoodieTestUtils;
 import java.io.File;
 import java.io.IOException;
 import java.net.InetSocketAddress;
@@ -87,7 +88,7 @@ public HiveServer2 start() throws IOException {
         .checkState(workDir != null, "The work dir must be set before starting cluster.");
 
     if (hadoopConf == null) {
-      hadoopConf = new Configuration();
+      hadoopConf = HoodieTestUtils.getDefaultHadoopConf();
     }
 
     String localHiveLocation = getHiveLocation(workDir);

File: hoodie-spark/src/main/java/com/uber/hoodie/HoodieDataSourceHelpers.java
Patch:
@@ -67,7 +67,7 @@ public static String latestCommit(FileSystem fs, String basePath) {
    */
   public static HoodieTimeline allCompletedCommitsCompactions(FileSystem fs, String basePath) {
     HoodieTable table = HoodieTable
-        .getHoodieTable(new HoodieTableMetaClient(fs, basePath, true), null);
+        .getHoodieTable(new HoodieTableMetaClient(fs.getConf(), basePath, true), null);
     if (table.getMetaClient().getTableType().equals(HoodieTableType.MERGE_ON_READ)) {
       return table.getActiveTimeline().getTimelineOfActions(
           Sets.newHashSet(HoodieActiveTimeline.COMMIT_ACTION,

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/HiveIncrementalPuller.java
Patch:
@@ -291,7 +291,7 @@ private String scanForCommitTime(FileSystem fs, String targetDataPath) throws IO
     if (!fs.exists(new Path(targetDataPath)) || !fs.exists(new Path(targetDataPath + "/.hoodie"))) {
       return "0";
     }
-    HoodieTableMetaClient metadata = new HoodieTableMetaClient(fs, targetDataPath);
+    HoodieTableMetaClient metadata = new HoodieTableMetaClient(fs.getConf(), targetDataPath);
 
     Optional<HoodieInstant>
         lastCommit = metadata.getActiveTimeline().getCommitsTimeline()
@@ -331,7 +331,7 @@ private boolean ensureTempPathExists(FileSystem fs, String lastCommitTime)
 
   private String getLastCommitTimePulled(FileSystem fs, String sourceTableLocation)
       throws IOException {
-    HoodieTableMetaClient metadata = new HoodieTableMetaClient(fs, sourceTableLocation);
+    HoodieTableMetaClient metadata = new HoodieTableMetaClient(fs.getConf(), sourceTableLocation);
     List<String> commitsToSync = metadata.getActiveTimeline().getCommitsTimeline()
         .filterCompletedInstants()
         .findInstantsAfter(config.fromCommitTime, config.maxCommits).getInstants()

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/schema/FilebasedSchemaProvider.java
Patch:
@@ -25,6 +25,7 @@
 import java.util.Arrays;
 import org.apache.avro.Schema;
 import org.apache.commons.configuration.PropertiesConfiguration;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 
@@ -50,7 +51,7 @@ static class Config {
 
   public FilebasedSchemaProvider(PropertiesConfiguration config) {
     super(config);
-    this.fs = FSUtils.getFs();
+    this.fs = FSUtils.getFs(config.getBasePath(), new Configuration());
 
     DataSourceUtils.checkRequiredProperties(config,
         Arrays.asList(Config.SOURCE_SCHEMA_FILE_PROP, Config.TARGET_SCHEMA_FILE_PROP));

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/sources/DFSSource.java
Patch:
@@ -65,7 +65,7 @@ static class Config {
   public DFSSource(PropertiesConfiguration config, JavaSparkContext sparkContext,
       SourceDataFormat dataFormat, SchemaProvider schemaProvider) {
     super(config, sparkContext, dataFormat, schemaProvider);
-    this.fs = FSUtils.getFs();
+    this.fs = FSUtils.getFs(config.getBasePath(), sparkContext.hadoopConfiguration());
     DataSourceUtils.checkRequiredProperties(config, Arrays.asList(Config.ROOT_INPUT_PATH_PROP));
   }
 

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/sources/HiveIncrPullSource.java
Patch:
@@ -72,7 +72,7 @@ static class Config {
   public HiveIncrPullSource(PropertiesConfiguration config, JavaSparkContext sparkContext,
       SourceDataFormat dataFormat, SchemaProvider schemaProvider) {
     super(config, sparkContext, dataFormat, schemaProvider);
-    this.fs = FSUtils.getFs();
+    this.fs = FSUtils.getFs(config.getBasePath(), sparkContext.hadoopConfiguration());
     DataSourceUtils.checkRequiredProperties(config, Arrays.asList(Config.ROOT_INPUT_PATH_PROP));
     this.incrPullRootPath = config.getString(Config.ROOT_INPUT_PATH_PROP);
   }

File: hoodie-utilities/src/test/java/com/uber/hoodie/utilities/TestHDFSParquetImporter.java
Patch:
@@ -24,6 +24,7 @@
 import com.uber.hoodie.HoodieWriteClient;
 import com.uber.hoodie.common.HoodieTestDataGenerator;
 import com.uber.hoodie.common.minicluster.HdfsTestService;
+import com.uber.hoodie.common.model.HoodieTestUtils;
 import com.uber.hoodie.common.table.HoodieTimeline;
 import com.uber.hoodie.common.table.timeline.HoodieActiveTimeline;
 import com.uber.hoodie.common.util.FSUtils;
@@ -38,7 +39,6 @@
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicInteger;
 import org.apache.avro.generic.GenericRecord;
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.LocatedFileStatus;
 import org.apache.hadoop.fs.Path;
@@ -174,7 +174,7 @@ private void createRecords(Path srcFolder) throws ParseException, IOException {
     ParquetWriter<GenericRecord> writer = AvroParquetWriter
         .<GenericRecord>builder(srcFile)
         .withSchema(HoodieTestDataGenerator.avroSchema)
-        .withConf(new Configuration())
+        .withConf(HoodieTestUtils.getDefaultHadoopConf())
         .build();
     for (GenericRecord record : records) {
       writer.write(record);

File: hoodie-cli/src/main/java/com/uber/hoodie/cli/commands/HoodieSyncCommand.java
Patch:
@@ -61,9 +61,9 @@ public String validateSync(
           "hivePass"}, mandatory = true, unspecifiedDefaultValue = "", help = "hive password to connect to")
       final String hivePass) throws Exception {
     HoodieTableMetaClient target = HoodieCLI.syncTableMetadata;
-    HoodieTimeline targetTimeline = target.getActiveTimeline().getCommitsAndCompactionsTimeline();
+    HoodieTimeline targetTimeline = target.getActiveTimeline().getCommitsTimeline();
     HoodieTableMetaClient source = HoodieCLI.tableMetadata;
-    HoodieTimeline sourceTimeline = source.getActiveTimeline().getCommitsAndCompactionsTimeline();
+    HoodieTimeline sourceTimeline = source.getActiveTimeline().getCommitsTimeline();
     long sourceCount = 0;
     long targetCount = 0;
     if ("complete".equals(mode)) {

File: hoodie-client/src/main/java/com/uber/hoodie/HoodieReadClient.java
Patch:
@@ -71,7 +71,7 @@ public HoodieReadClient(JavaSparkContext jsc, String basePath) {
     // Create a Hoodie table which encapsulated the commits and files visible
     this.hoodieTable = HoodieTable
         .getHoodieTable(new HoodieTableMetaClient(fs, basePath, true), null);
-    this.commitTimeline = hoodieTable.getCompletedCompactionCommitTimeline();
+    this.commitTimeline = hoodieTable.getCommitTimeline().filterCompletedInstants();
     this.index =
         new HoodieBloomIndex(HoodieWriteConfig.newBuilder().withPath(basePath).build(), jsc);
     this.sqlContextOpt = Optional.absent();

File: hoodie-client/src/main/java/com/uber/hoodie/index/bloom/HoodieBloomIndex.java
Patch:
@@ -239,7 +239,7 @@ List<Tuple2<String, BloomIndexFileInfo>> loadInvolvedFiles(List<String> partitio
         .parallelize(partitions, Math.max(partitions.size(), 1))
         .flatMapToPair(partitionPath -> {
           java.util.Optional<HoodieInstant> latestCommitTime =
-              hoodieTable.getCommitTimeline().filterCompletedInstants().lastInstant();
+              hoodieTable.getCommitsTimeline().filterCompletedInstants().lastInstant();
           List<Tuple2<String, HoodieDataFile>> filteredFiles = new ArrayList<>();
           if (latestCommitTime.isPresent()) {
             filteredFiles =

File: hoodie-client/src/main/java/com/uber/hoodie/table/HoodieCopyOnWriteTable.java
Patch:
@@ -22,7 +22,6 @@
 import com.uber.hoodie.common.HoodieCleanStat;
 import com.uber.hoodie.common.HoodieRollbackStat;
 import com.uber.hoodie.common.model.HoodieCommitMetadata;
-import com.uber.hoodie.common.model.HoodieCompactionMetadata;
 import com.uber.hoodie.common.model.HoodieDataFile;
 import com.uber.hoodie.common.model.HoodieKey;
 import com.uber.hoodie.common.model.HoodieRecord;
@@ -486,7 +485,7 @@ public Iterator<List<WriteStatus>> handleInsertPartition(String commitTime, Inte
   }
 
   @Override
-  public Optional<HoodieCompactionMetadata> compact(JavaSparkContext jsc, String commitCompactionTime) {
+  public Optional<HoodieCommitMetadata> compact(JavaSparkContext jsc, String commitCompactionTime) {
     logger.info("Nothing to compact in COW storage format");
     return Optional.empty();
   }
@@ -544,7 +543,7 @@ protected Map<FileStatus, Boolean> deleteCleanedFiles(String partitionPath, List
   @Override
   public List<HoodieRollbackStat> rollback(JavaSparkContext jsc, List<String> commits)
       throws IOException {
-    String actionType = this.getCompactedCommitActionType();
+    String actionType = this.getCommitActionType();
     HoodieActiveTimeline activeTimeline = this.getActiveTimeline();
     List<String> inflights = this.getInflightCommitTimeline().getInstants()
         .map(HoodieInstant::getTimestamp)

File: hoodie-client/src/test/java/com/uber/hoodie/TestHoodieClientOnCopyOnWriteStorage.java
Patch:
@@ -649,7 +649,7 @@ public void testInsertAndCleanByVersions() throws Exception {
 
       HoodieTableMetaClient metadata = new HoodieTableMetaClient(fs, basePath);
       table = HoodieTable.getHoodieTable(metadata, getConfig());
-      timeline = table.getCommitTimeline();
+      timeline = table.getCommitsTimeline();
 
       TableFileSystemView fsView = table.getFileSystemView();
       // Need to ensure the following
@@ -1493,10 +1493,10 @@ public void testCommitWritesRelativePaths() throws Exception {
         HoodieTestUtils.doesCommitExist(basePath, commitTime));
 
     // Get parquet file paths from commit metadata
-    String actionType = table.getCompactedCommitActionType();
+    String actionType = table.getCommitActionType();
     HoodieInstant commitInstant =
         new HoodieInstant(false, actionType, commitTime);
-    HoodieTimeline commitTimeline = table.getCompletedCompactionCommitTimeline();
+    HoodieTimeline commitTimeline = table.getCommitTimeline().filterCompletedInstants();
     HoodieCommitMetadata commitMetadata =
         HoodieCommitMetadata.fromBytes(commitTimeline.getInstantDetails(commitInstant).get());
     String basePath = table.getMetaClient().getBasePath();

File: hoodie-common/src/test/java/com/uber/hoodie/common/model/HoodieTestUtils.java
Patch:
@@ -158,7 +158,7 @@ public static final void createCompactionCommitFiles(String basePath, String...
     for (String commitTime : commitTimes) {
       boolean createFile = fs.createNewFile(new Path(
           basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/" + HoodieTimeline
-              .makeCompactionFileName(commitTime)));
+              .makeCommitFileName(commitTime)));
       if (!createFile) {
         throw new IOException("cannot create commit file for commit " + commitTime);
       }

File: hoodie-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/HoodieInputFormat.java
Patch:
@@ -100,7 +100,7 @@ public FileStatus[] listStatus(JobConf job) throws IOException {
       String tableName = metadata.getTableConfig().getTableName();
       String mode = HoodieHiveUtil.readMode(Job.getInstance(job), tableName);
       // Get all commits, delta commits, compactions, as all of them produce a base parquet file today
-      HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsAndCompactionsTimeline()
+      HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline()
           .filterCompletedInstants();
       TableFileSystemView.ReadOptimizedView roView = new HoodieTableFileSystemView(metadata,
           timeline, statuses);

File: hoodie-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/realtime/HoodieRealtimeInputFormat.java
Patch:
@@ -125,7 +125,6 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
               String maxCommitTime = metaClient.getActiveTimeline()
                   .getTimelineOfActions(
                       Sets.newHashSet(HoodieTimeline.COMMIT_ACTION,
-                          HoodieTimeline.COMPACTION_ACTION,
                           HoodieTimeline.DELTA_COMMIT_ACTION))
                   .filterCompletedInstants().lastInstant().get().getTimestamp();
               rtSplits.add(

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/HiveIncrementalPuller.java
Patch:
@@ -294,7 +294,7 @@ private String scanForCommitTime(FileSystem fs, String targetDataPath) throws IO
     HoodieTableMetaClient metadata = new HoodieTableMetaClient(fs, targetDataPath);
 
     Optional<HoodieInstant>
-        lastCommit = metadata.getActiveTimeline().getCommitsAndCompactionsTimeline()
+        lastCommit = metadata.getActiveTimeline().getCommitsTimeline()
         .filterCompletedInstants().lastInstant();
     if (lastCommit.isPresent()) {
       return lastCommit.get().getTimestamp();
@@ -332,14 +332,14 @@ private boolean ensureTempPathExists(FileSystem fs, String lastCommitTime)
   private String getLastCommitTimePulled(FileSystem fs, String sourceTableLocation)
       throws IOException {
     HoodieTableMetaClient metadata = new HoodieTableMetaClient(fs, sourceTableLocation);
-    List<String> commitsToSync = metadata.getActiveTimeline().getCommitsAndCompactionsTimeline()
+    List<String> commitsToSync = metadata.getActiveTimeline().getCommitsTimeline()
         .filterCompletedInstants()
         .findInstantsAfter(config.fromCommitTime, config.maxCommits).getInstants()
         .map(HoodieInstant::getTimestamp)
         .collect(Collectors.toList());
     if (commitsToSync.isEmpty()) {
       log.warn("Nothing to sync. All commits in " + config.sourceTable + " are " + metadata
-          .getActiveTimeline().getCommitsAndCompactionsTimeline().filterCompletedInstants()
+          .getActiveTimeline().getCommitsTimeline().filterCompletedInstants()
           .getInstants()
           .collect(Collectors.toList()) + " and from commit time is "
           + config.fromCommitTime);

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/HoodieSnapshotCopier.java
Patch:
@@ -74,11 +74,11 @@ public void snapshot(JavaSparkContext jsc, String baseDir, final String outputDi
     final HoodieTableMetaClient tableMetadata = new HoodieTableMetaClient(fs, baseDir);
     final TableFileSystemView.ReadOptimizedView fsView = new HoodieTableFileSystemView(
         tableMetadata,
-        tableMetadata.getActiveTimeline().getCommitsAndCompactionsTimeline()
+        tableMetadata.getActiveTimeline().getCommitsTimeline()
             .filterCompletedInstants());
     // Get the latest commit
     Optional<HoodieInstant> latestCommit = tableMetadata.getActiveTimeline()
-        .getCommitsAndCompactionsTimeline().filterCompletedInstants().lastInstant();
+        .getCommitsTimeline().filterCompletedInstants().lastInstant();
     if (!latestCommit.isPresent()) {
       logger.warn("No commits present. Nothing to snapshot");
       return;

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/deltastreamer/HoodieDeltaStreamer.java
Patch:
@@ -120,7 +120,7 @@ public HoodieDeltaStreamer(Config cfg) throws IOException {
     if (fs.exists(new Path(cfg.targetBasePath))) {
       HoodieTableMetaClient meta = new HoodieTableMetaClient(fs, cfg.targetBasePath);
       this.commitTimelineOpt = Optional
-          .of(meta.getActiveTimeline().getCommitsAndCompactionsTimeline()
+          .of(meta.getActiveTimeline().getCommitsTimeline()
               .filterCompletedInstants());
     } else {
       this.commitTimelineOpt = Optional.empty();

File: hoodie-client/src/test/java/com/uber/hoodie/common/HoodieClientTestUtils.java
Patch:
@@ -87,7 +87,7 @@ public static SparkConf getSparkConfForTest(String appName) {
         SparkConf sparkConf = new SparkConf()
                 .setAppName(appName)
                 .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
-                .setMaster("local[4]");
+                .setMaster("local[1]");
         return HoodieReadClient.addHoodieSupport(sparkConf);
     }
 }

File: hoodie-utilities/src/test/java/com/uber/hoodie/utilities/TestHDFSParquetImporter.java
Patch:
@@ -284,7 +284,7 @@ private HDFSParquetImporter.Config getHDFSParquetImporterConfig(String srcPath,
 
     private JavaSparkContext getJavaSparkContext() {
         // Initialize a local spark env
-        SparkConf sparkConf = new SparkConf().setAppName("TestConversionCommand").setMaster("local[4]");
+        SparkConf sparkConf = new SparkConf().setAppName("TestConversionCommand").setMaster("local[1]");
         sparkConf = HoodieWriteClient.registerClasses(sparkConf);
         return new JavaSparkContext(HoodieReadClient.addHoodieSupport(sparkConf));
     }

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/keygen/TimestampBasedKeyGenerator.java
Patch:
@@ -91,7 +91,7 @@ public HoodieKey getKey(GenericRecord record) {
             } else if (partitionVal instanceof Long) {
                 unixTime = (Long) partitionVal;
             } else if (partitionVal instanceof String) {
-                unixTime = inputDateFormat.parse(partitionVal.toString()).getTime();
+                unixTime = inputDateFormat.parse(partitionVal.toString()).getTime() / 1000;
             } else {
                 throw new HoodieNotSupportedException("Unexpected type for partition field: "+ partitionVal.getClass().getName());
             }

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieAppendHandle.java
Patch:
@@ -80,6 +80,8 @@ private void init(Iterator<HoodieRecord<T>> recordItr) {
                     fileSystemView.getLatestDataFilesForFileId(record.getPartitionPath(), fileId)
                         .findFirst().get().getFileName();
                 String baseCommitTime = FSUtils.getCommitTime(latestValidFilePath);
+                Path path = new Path(record.getPartitionPath(),
+                        FSUtils.makeDataFileName(commitTime, TaskContext.getPartitionId(), fileId));
                 writeStatus.getStat().setPrevCommit(baseCommitTime);
                 writeStatus.setFileId(fileId);
                 writeStatus.setPartitionPath(record.getPartitionPath());
@@ -103,7 +105,7 @@ private void init(Iterator<HoodieRecord<T>> recordItr) {
                             + " on commit " + commitTime + " on HDFS path " + hoodieTable
                             .getMetaClient().getBasePath() + partitionPath, e);
                 }
-                writeStatus.getStat().setFullPath(currentLogFile.getPath().toString());
+                writeStatus.getStat().setPath(path.toString());
             }
             // update the new location of the record, so we know where to find it next
             record.setNewLocation(new HoodieRecordLocation(commitTime, fileId));

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieInsertHandle.java
Patch:
@@ -123,12 +123,14 @@ public WriteStatus close() {
         try {
             storageWriter.close();
 
+            String relativePath = path.toString().replace(new Path(config.getBasePath()) + "/", "");
+
             HoodieWriteStat stat = new HoodieWriteStat();
             stat.setNumWrites(recordsWritten);
             stat.setNumDeletes(recordsDeleted);
             stat.setPrevCommit(HoodieWriteStat.NULL_COMMIT);
             stat.setFileId(status.getFileId());
-            stat.setFullPath(path.toString());
+            stat.setPath(relativePath);
             stat.setTotalWriteBytes(FSUtils.getFileSize(fs, path));
             stat.setTotalWriteErrors(status.getFailedRecords().size());
             status.setStat(stat);

File: hoodie-client/src/main/java/com/uber/hoodie/io/compact/HoodieRealtimeTableCompactor.java
Patch:
@@ -114,6 +114,7 @@ public Iterator<CompactionWriteStat> call(
     for (CompactionWriteStat stat : updateStatusMap) {
       metadata.addWriteStat(stat.getPartitionPath(), stat);
     }
+
     log.info("Compaction finished with result " + metadata);
 
     //noinspection ConstantConditions

File: hoodie-client/src/test/java/com/uber/hoodie/io/TestHoodieCompactor.java
Patch:
@@ -129,8 +129,9 @@ public void testCompactionEmpty() throws Exception {
 
         HoodieCompactionMetadata result =
             compactor.compact(jsc, getConfig(), table);
+        String basePath = table.getMetaClient().getBasePath();
         assertTrue("If there is nothing to compact, result will be empty",
-            result.getFileIdAndFullPaths().isEmpty());
+            result.getFileIdAndFullPaths(basePath).isEmpty());
     }
 
     @Test

File: hoodie-hive/src/main/java/com/uber/hoodie/hive/HoodieHiveClient.java
Patch:
@@ -313,7 +313,7 @@ public MessageType getDataSchema() {
               .orElseThrow(() -> new InvalidDatasetException(syncConfig.basePath));
           HoodieCommitMetadata commitMetadata = HoodieCommitMetadata
               .fromBytes(activeTimeline.getInstantDetails(lastCommit).get());
-          String filePath = commitMetadata.getFileIdAndFullPaths().values().stream().findAny()
+          String filePath = commitMetadata.getFileIdAndFullPaths(metaClient.getBasePath()).values().stream().findAny()
               .orElseThrow(() -> new IllegalArgumentException(
                   "Could not find any data file written for commit " + lastCommit
                       + ", could not get schema for dataset " + metaClient.getBasePath()));
@@ -340,7 +340,7 @@ public MessageType getDataSchema() {
             // read from the log file wrote
             commitMetadata = HoodieCommitMetadata
                 .fromBytes(activeTimeline.getInstantDetails(lastDeltaCommit).get());
-            filePath = commitMetadata.getFileIdAndFullPaths().values().stream().filter(s -> s.contains(
+            filePath = commitMetadata.getFileIdAndFullPaths(metaClient.getBasePath()).values().stream().filter(s -> s.contains(
                 HoodieLogFile.DELTA_EXTENSION)).findAny()
                 .orElseThrow(() -> new IllegalArgumentException(
                     "Could not find any data file written for commit " + lastDeltaCommit
@@ -377,7 +377,7 @@ private MessageType readSchemaFromLastCompaction(Optional<HoodieInstant> lastCom
     // Read from the compacted file wrote
     HoodieCompactionMetadata compactionMetadata = HoodieCompactionMetadata
         .fromBytes(activeTimeline.getInstantDetails(lastCompactionCommit).get());
-    String filePath = compactionMetadata.getFileIdAndFullPaths().values().stream().findAny()
+    String filePath = compactionMetadata.getFileIdAndFullPaths(metaClient.getBasePath()).values().stream().findAny()
         .orElseThrow(() -> new IllegalArgumentException(
             "Could not find any data file written for compaction " + lastCompactionCommit
                 + ", could not get schema for dataset " + metaClient.getBasePath()));

File: hoodie-hive/src/test/java/com/uber/hoodie/hive/TestUtil.java
Patch:
@@ -217,12 +217,12 @@ private static HoodieCommitMetadata createLogFiles(
     for (Entry<String, List<HoodieWriteStat>> wEntry : partitionWriteStats.entrySet()) {
       String partitionPath = wEntry.getKey();
       for (HoodieWriteStat wStat : wEntry.getValue()) {
-        Path path = new Path(wStat.getFullPath());
+        Path path = new Path(wStat.getPath());
         HoodieDataFile dataFile = new HoodieDataFile(fileSystem.getFileStatus(path));
         HoodieLogFile logFile = generateLogData(path, isLogSchemaSimple);
         HoodieDeltaWriteStat writeStat = new HoodieDeltaWriteStat();
         writeStat.setFileId(dataFile.getFileId());
-        writeStat.setFullPath(logFile.getPath().toString());
+        writeStat.setPath(logFile.getPath().toString());
         commitMetadata.addWriteStat(partitionPath, writeStat);
       }
     }
@@ -258,7 +258,7 @@ private static List<HoodieWriteStat> createTestData(Path partPath, boolean isPar
       generateParquetData(filePath, isParquetSchemaSimple);
       HoodieWriteStat writeStat = new HoodieWriteStat();
       writeStat.setFileId(fileId);
-      writeStat.setFullPath(filePath.toString());
+      writeStat.setPath(filePath.toString());
       writeStats.add(writeStat);
     }
     return writeStats;

File: hoodie-hive/src/main/java/com/uber/hoodie/hive/HiveSyncConfig.java
Patch:
@@ -49,7 +49,7 @@ public class HiveSyncConfig implements Serializable {
       "--base-path"}, description = "Basepath of hoodie dataset to sync", required = true)
   public String basePath;
 
-  @Parameter(names = "--partitioned-by", description = "Fields in the schema partitioned by")
+  @Parameter(names = "--partitioned-by", description = "Fields in the schema partitioned by", required = true)
   public List<String> partitionFields = new ArrayList<>();
 
   @Parameter(names = "-partition-value-extractor", description = "Class which implements PartitionValueExtractor to extract the partition values from HDFS path")

File: hoodie-client/src/main/java/com/uber/hoodie/config/HoodieCompactionConfig.java
Patch:
@@ -45,7 +45,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {
 
     // Run a compaction every N delta commits
     public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = "hoodie.compact.inline.max.delta.commits";
-    private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = "4";
+    private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = "10";
 
     public static final String CLEANER_FILE_VERSIONS_RETAINED_PROP =
         "hoodie.cleaner.fileversions.retained";

File: hoodie-common/src/main/java/com/uber/hoodie/common/model/HoodieFileFormat.java
Patch:
@@ -17,7 +17,7 @@
 package com.uber.hoodie.common.model;
 
 public enum HoodieFileFormat {
-    PARQUET(".parquet"), AVRO(".avro");
+    PARQUET(".parquet"), HOODIE_LOG(".log");
 
     private final String extension;
 

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/HoodieTableConfig.java
Patch:
@@ -52,7 +52,7 @@ public class HoodieTableConfig implements Serializable {
 
     public static final HoodieTableType DEFAULT_TABLE_TYPE = HoodieTableType.COPY_ON_WRITE;
     public static final HoodieFileFormat DEFAULT_RO_FILE_FORMAT = HoodieFileFormat.PARQUET;
-    public static final HoodieFileFormat DEFAULT_RT_FILE_FORMAT = HoodieFileFormat.AVRO;
+    public static final HoodieFileFormat DEFAULT_RT_FILE_FORMAT = HoodieFileFormat.HOODIE_LOG;
     private Properties props;
 
     public HoodieTableConfig(FileSystem fs, String metaPath) {

File: hoodie-hive/src/test/java/com/uber/hoodie/hive/HDroneDatasetTest.java
Patch:
@@ -55,10 +55,10 @@ public void testDatasetCreation() throws IOException, InitializationError {
 
         dataset = HoodieHiveDatasetSyncTask.newBuilder().withReference(metadata)
             .withConfiguration(TestUtil.hDroneConfiguration).build();
-        assertTrue("Table should exist after sync", hiveClient.checkTableExists(metadata));
-        assertEquals("After sync, There should not be any new partitions to sync", 0,
+        assertTrue("Table should exist after flush", hiveClient.checkTableExists(metadata));
+        assertEquals("After flush, There should not be any new partitions to flush", 0,
             dataset.getNewPartitions().size());
-        assertEquals("After sync, There should not be any modified partitions to sync", 0,
+        assertEquals("After flush, There should not be any modified partitions to flush", 0,
             dataset.getChangedPartitions().size());
 
         assertEquals("Table Schema should have 5 fields", 5,

File: hoodie-client/src/main/java/com/uber/hoodie/io/compact/HoodieCompactor.java
Patch:
@@ -36,7 +36,7 @@ public interface HoodieCompactor extends Serializable {
      * @throws Exception
      */
     HoodieCompactionMetadata compact(JavaSparkContext jsc, final HoodieWriteConfig config,
-        HoodieTable hoodieTable, CompactionFilter compactionFilter) throws Exception;
+        HoodieTable hoodieTable) throws Exception;
 
 
     // Helper methods

File: hoodie-client/src/main/java/com/uber/hoodie/io/compact/HoodieRealtimeTableCompactor.java
Patch:
@@ -64,7 +64,7 @@ public class HoodieRealtimeTableCompactor implements HoodieCompactor {
 
   @Override
   public HoodieCompactionMetadata compact(JavaSparkContext jsc, HoodieWriteConfig config,
-      HoodieTable hoodieTable, CompactionFilter compactionFilter) throws IOException {
+      HoodieTable hoodieTable) throws IOException {
     Preconditions.checkArgument(
         hoodieTable.getMetaClient().getTableType() == HoodieTableType.MERGE_ON_READ,
         "HoodieRealtimeTableCompactor can only compact table of type "
@@ -86,12 +86,12 @@ public HoodieCompactionMetadata compact(JavaSparkContext jsc, HoodieWriteConfig
                 .getFileSystemView()
                 .groupLatestDataFileWithLogFiles(partitionPath).entrySet()
                 .stream()
-                .map(s -> new CompactionOperation(s.getKey(), partitionPath, s.getValue()))
+                .map(s -> new CompactionOperation(s.getKey(), partitionPath, s.getValue(), config))
                 .collect(toList()).iterator()).collect();
     log.info("Total of " + operations.size() + " compactions are retrieved");
 
     // Filter the compactions with the passed in filter. This lets us choose most effective compactions only
-    operations = compactionFilter.filter(operations);
+    operations = config.getCompactionStrategy().orderAndFilter(config, operations);
     if (operations.isEmpty()) {
       log.warn("After filtering, Nothing to compact for " + metaClient.getBasePath());
       return null;

File: hoodie-client/src/main/java/com/uber/hoodie/table/HoodieMergeOnReadTable.java
Patch:
@@ -25,7 +25,6 @@
 import com.uber.hoodie.config.HoodieWriteConfig;
 import com.uber.hoodie.exception.HoodieCompactionException;
 import com.uber.hoodie.io.HoodieAppendHandle;
-import com.uber.hoodie.io.compact.CompactionFilter;
 import com.uber.hoodie.io.compact.HoodieRealtimeTableCompactor;
 import java.util.Optional;
 import org.apache.log4j.LogManager;
@@ -89,7 +88,7 @@ public Optional<HoodieCompactionMetadata> compact(JavaSparkContext jsc) {
         logger.info("Compacting merge on read table " + config.getBasePath());
         HoodieRealtimeTableCompactor compactor = new HoodieRealtimeTableCompactor();
         try {
-            return Optional.of(compactor.compact(jsc, config, this, CompactionFilter.allowAll()));
+            return Optional.of(compactor.compact(jsc, config, this));
         } catch (IOException e) {
             throw new HoodieCompactionException("Could not compact " + config.getBasePath(), e);
         }

File: hoodie-client/src/test/java/com/uber/hoodie/TestMergeOnReadTable.java
Patch:
@@ -32,7 +32,6 @@
 import com.uber.hoodie.config.HoodieStorageConfig;
 import com.uber.hoodie.config.HoodieWriteConfig;
 import com.uber.hoodie.index.HoodieIndex;
-import com.uber.hoodie.io.compact.CompactionFilter;
 import com.uber.hoodie.io.compact.HoodieCompactor;
 import com.uber.hoodie.io.compact.HoodieRealtimeTableCompactor;
 import com.uber.hoodie.table.HoodieTable;
@@ -165,7 +164,7 @@ public void testSimpleInsertAndUpdate() throws Exception {
         HoodieCompactor compactor = new HoodieRealtimeTableCompactor();
         HoodieTable table = HoodieTable.getHoodieTable(metaClient, getConfig());
 
-        compactor.compact(jsc, getConfig(), table, CompactionFilter.allowAll());
+        compactor.compact(jsc, getConfig(), table);
 
         metaClient = new HoodieTableMetaClient(fs, cfg.getBasePath());
         allFiles = HoodieTestUtils.listAllDataFilesInPath(fs, cfg.getBasePath());

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/log/HoodieLogFile.java
Patch:
@@ -75,6 +75,9 @@ public Optional<FileStatus> getFileStatus() {
         return fileStatus;
     }
 
+    public Optional<Long> getFileSize() {
+        return fileStatus.map(FileStatus::getLen);
+    }
 
     public HoodieLogFile rollOver(FileSystem fs) throws IOException {
         String fileId = getFileId();

File: hoodie-client/src/main/java/com/uber/hoodie/HoodieWriteClient.java
Patch:
@@ -476,7 +476,7 @@ public boolean savepoint(String commitTime, String user, String comment) {
                     + lastCommitRetained);
 
             Map<String, List<String>> latestFilesMap = jsc.parallelize(
-                FSUtils.getAllPartitionPaths(fs, table.getMetaClient().getBasePath()))
+                FSUtils.getAllPartitionPaths(fs, table.getMetaClient().getBasePath(), config.shouldAssumeDatePartitioning()))
                 .mapToPair((PairFunction<String, String, List<String>>) partitionPath -> {
                     // Scan all partitions files with this commit time
                     logger.info("Collecting latest files in partition path " + partitionPath);
@@ -650,7 +650,7 @@ private void rollback(List<String> commits) {
             logger.info("Clean out all parquet files generated for commits: " + commits);
             final LongAccumulator numFilesDeletedCounter = jsc.sc().longAccumulator();
             List<HoodieRollbackStat> stats = jsc.parallelize(
-                FSUtils.getAllPartitionPaths(fs, table.getMetaClient().getBasePath()))
+                FSUtils.getAllPartitionPaths(fs, table.getMetaClient().getBasePath(), config.shouldAssumeDatePartitioning()))
                 .map((Function<String, HoodieRollbackStat>) partitionPath -> {
                     // Scan all partitions files with this commit time
                     logger.info("Cleaning path " + partitionPath);
@@ -739,7 +739,7 @@ private void clean(String startCleanTime) throws HoodieIOException  {
                 .getHoodieTable(new HoodieTableMetaClient(fs, config.getBasePath(), true), config);
 
             List<String> partitionsToClean =
-                FSUtils.getAllPartitionPaths(fs, table.getMetaClient().getBasePath());
+                FSUtils.getAllPartitionPaths(fs, table.getMetaClient().getBasePath(), config.shouldAssumeDatePartitioning());
             // shuffle to distribute cleaning work across partitions evenly
             Collections.shuffle(partitionsToClean);
             logger.info("Partitions to clean up : " + partitionsToClean + ", with policy " + config

File: hoodie-client/src/main/java/com/uber/hoodie/io/compact/HoodieRealtimeTableCompactor.java
Patch:
@@ -77,7 +77,7 @@ public HoodieCompactionMetadata compact(JavaSparkContext jsc, HoodieWriteConfig
     String compactionCommit = startCompactionCommit(hoodieTable);
     log.info("Compacting " + metaClient.getBasePath() + " with commit " + compactionCommit);
     List<String> partitionPaths =
-        FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath());
+        FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath(), config.shouldAssumeDatePartitioning());
 
     log.info("Compaction looking for files to compact in " + partitionPaths + " partitions");
     List<CompactionOperation> operations =

File: hoodie-client/src/test/java/com/uber/hoodie/TestHoodieClient.java
Patch:
@@ -403,7 +403,7 @@ public void testCreateSavepoint() throws Exception {
         // Verify there are no errors
         assertNoWriteErrors(statuses);
 
-        List<String> partitionPaths = FSUtils.getAllPartitionPaths(fs, cfg.getBasePath());
+        List<String> partitionPaths = FSUtils.getAllPartitionPaths(fs, cfg.getBasePath(), getConfig().shouldAssumeDatePartitioning());
         HoodieTableMetaClient metaClient = new HoodieTableMetaClient(fs, basePath);
         HoodieTable table = HoodieTable.getHoodieTable(metaClient, getConfig());
         final TableFileSystemView view = table.getFileSystemView();
@@ -475,7 +475,7 @@ public void testRollbackToSavepoint() throws Exception {
         statuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();
         // Verify there are no errors
         assertNoWriteErrors(statuses);
-        List<String> partitionPaths = FSUtils.getAllPartitionPaths(fs, cfg.getBasePath());
+        List<String> partitionPaths = FSUtils.getAllPartitionPaths(fs, cfg.getBasePath(), getConfig().shouldAssumeDatePartitioning());
         HoodieTableMetaClient metaClient = new HoodieTableMetaClient(fs, basePath);
         HoodieTable table = HoodieTable.getHoodieTable(metaClient, getConfig());
         final TableFileSystemView view1 = table.getFileSystemView();

File: hoodie-utilities/src/test/java/com/uber/hoodie/utilities/TestHoodieSnapshotCopier.java
Patch:
@@ -63,7 +63,7 @@ public void testEmptySnapshotCopy() throws IOException {
 
         // Do the snapshot
         HoodieSnapshotCopier copier = new HoodieSnapshotCopier();
-        copier.snapshot(jsc, basePath, outputPath);
+        copier.snapshot(jsc, basePath, outputPath, true);
 
         // Nothing changed; we just bail out
         assertEquals(fs.listStatus(new Path(basePath)).length, 1);
@@ -117,7 +117,7 @@ public void testSnapshotCopy() throws Exception {
 
         // Do a snapshot copy
         HoodieSnapshotCopier copier = new HoodieSnapshotCopier();
-        copier.snapshot(jsc, basePath, outputPath);
+        copier.snapshot(jsc, basePath, outputPath, false);
 
         // Check results
         assertTrue(fs.exists(new Path(outputPath + "/2016/05/01/" + file11.getName())));

File: hoodie-cli/src/main/java/com/uber/hoodie/cli/commands/RepairsCommand.java
Patch:
@@ -80,7 +80,7 @@ public String addPartitionMeta(
             final boolean dryRun) throws IOException {
 
         String latestCommit  = HoodieCLI.tableMetadata.getActiveTimeline().getCommitTimeline().lastInstant().get().getTimestamp();
-        List<String> partitionPaths = FSUtils.getAllPartitionPaths(HoodieCLI.fs,
+        List<String> partitionPaths = FSUtils.getAllFoldersThreeLevelsDown(HoodieCLI.fs,
                 HoodieCLI.tableMetadata.getBasePath());
         Path basePath = new Path(HoodieCLI.tableMetadata.getBasePath());
         String[][] rows = new String[partitionPaths.size() + 1][];

File: hoodie-client/src/test/java/com/uber/hoodie/io/TestHoodieCompactor.java
Patch:
@@ -60,7 +60,6 @@
 
 public class TestHoodieCompactor {
     private transient JavaSparkContext jsc = null;
-    private transient SQLContext sqlContext;
     private String basePath = null;
     private HoodieCompactor compactor;
     private transient HoodieTestDataGenerator dataGen = null;

File: hoodie-common/src/main/java/com/uber/hoodie/common/model/HoodiePartitionMetadata.java
Patch:
@@ -71,7 +71,7 @@ public HoodiePartitionMetadata(FileSystem fs, String commitTime, Path basePath,
     }
 
     public int getPartitionDepth() {
-        if (!props.contains(PARTITION_DEPTH_KEY)) {
+        if (!props.containsKey(PARTITION_DEPTH_KEY)) {
             throw new HoodieException("Could not find partitionDepth in partition metafile");
         }
         return Integer.parseInt(props.getProperty(PARTITION_DEPTH_KEY));

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieAppendHandle.java
Patch:
@@ -101,7 +101,7 @@ private void init(Iterator<HoodieRecord<T>> recordItr) {
                     throw new HoodieUpsertException(
                         "Failed to initialize HoodieUpdateHandle for FileId: " + fileId
                             + " on commit " + commitTime + " on HDFS path " + hoodieTable
-                            .getMetaClient().getBasePath());
+                            .getMetaClient().getBasePath() + partitionPath, e);
                 }
                 writeStatus.getStat().setFullPath(currentLogFile.getPath().toString());
             }

File: hoodie-client/src/main/java/com/uber/hoodie/io/compact/HoodieCompactor.java
Patch:
@@ -16,6 +16,7 @@
 
 package com.uber.hoodie.io.compact;
 
+import com.uber.hoodie.common.model.HoodieCompactionMetadata;
 import com.uber.hoodie.common.table.HoodieTimeline;
 import com.uber.hoodie.common.table.timeline.HoodieActiveTimeline;
 import com.uber.hoodie.common.table.timeline.HoodieInstant;

File: hoodie-client/src/test/java/com/uber/hoodie/TestMergeOnReadTable.java
Patch:
@@ -189,7 +189,8 @@ private HoodieWriteConfig.Builder getConfigBuilder() {
         return HoodieWriteConfig.newBuilder().withPath(basePath)
             .withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)
             .withCompactionConfig(
-                HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024).build())
+                HoodieCompactionConfig.newBuilder().compactionSmallFileSize(1024 * 1024)
+                    .withInlineCompaction(false).build())
             .withStorageConfig(HoodieStorageConfig.newBuilder().limitFileSize(1024 * 1024).build())
 
             .forTable("test-trip-table").withIndexConfig(

File: hoodie-cli/src/main/java/com/uber/hoodie/cli/utils/SparkUtil.java
Patch:
@@ -30,6 +30,7 @@
 public class SparkUtil {
 
     public static Logger logger = Logger.getLogger(SparkUtil.class);
+    public static final String DEFUALT_SPARK_MASTER = "yarn-client";
 
     /**
      *
@@ -55,7 +56,7 @@ public static SparkLauncher initLauncher(String propertiesFile) throws URISyntax
 
     public static JavaSparkContext initJavaSparkConf(String name) {
         SparkConf sparkConf = new SparkConf().setAppName(name);
-        sparkConf.setMaster("yarn-client");
+        sparkConf.setMaster(DEFUALT_SPARK_MASTER);
         sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
         sparkConf.set("spark.driver.maxResultSize", "2g");
         sparkConf.set("spark.eventLog.overwrite", "true");

File: hoodie-common/src/main/java/com/uber/hoodie/avro/MercifulJsonConverter.java
Patch:
@@ -17,6 +17,7 @@
 package com.uber.hoodie.avro;
 
 import java.io.IOException;
+import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;

File: hoodie-common/src/main/java/com/uber/hoodie/common/model/HoodieAvroPayload.java
Patch:
@@ -29,6 +29,7 @@
 /**
  * This is a payload to wrap a existing Hoodie Avro Record.
  * Useful to create a HoodieRecord over existing GenericRecords in a hoodie datasets (useful in compactions)
+ *
  */
 public class HoodieAvroPayload implements HoodieRecordPayload<HoodieAvroPayload> {
     private final Optional<GenericRecord> record;

File: hoodie-client/src/main/java/com/uber/hoodie/config/HoodieWriteConfig.java
Patch:
@@ -36,7 +36,7 @@
 public class HoodieWriteConfig extends DefaultHoodieConfig {
     private static final String BASE_PATH_PROP = "hoodie.base.path";
     private static final String AVRO_SCHEMA = "hoodie.avro.schema";
-    private static final String TABLE_NAME = "hoodie.table.name";
+    public static final String TABLE_NAME = "hoodie.table.name";
     private static final String DEFAULT_PARALLELISM = "200";
     private static final String INSERT_PARALLELISM = "hoodie.insert.shuffle.parallelism";
     private static final String UPSERT_PARALLELISM = "hoodie.upsert.shuffle.parallelism";

File: hoodie-client/src/main/java/com/uber/hoodie/io/storage/HoodieStorageWriterFactory.java
Patch:
@@ -22,6 +22,7 @@
 import com.uber.hoodie.common.BloomFilter;
 import com.uber.hoodie.common.model.HoodieRecordPayload;
 import com.uber.hoodie.common.util.FSUtils;
+import com.uber.hoodie.table.HoodieTable;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.fs.Path;
@@ -32,7 +33,7 @@
 
 public class HoodieStorageWriterFactory {
     public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieStorageWriter<R> getStorageWriter(
-            String commitTime, Path path, HoodieTableMetaClient metaClient, HoodieWriteConfig config, Schema schema)
+            String commitTime, Path path, HoodieTable<T> hoodieTable, HoodieWriteConfig config, Schema schema)
         throws IOException {
         //TODO - based on the metadata choose the implementation of HoodieStorageWriter
         // Currently only parquet is supported

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/log/HoodieLogAppender.java
Patch:
@@ -20,6 +20,7 @@
 import com.uber.hoodie.common.table.log.avro.RollingAvroLogAppender;
 
 import java.io.IOException;
+import java.util.Iterator;
 import java.util.List;
 
 /**
@@ -36,7 +37,7 @@ public interface HoodieLogAppender<R> {
      * @param records
      * @throws IOException
      */
-    void append(List<R> records) throws IOException, InterruptedException;
+    void append(Iterator<R> records) throws IOException, InterruptedException;
 
     /**
      * Syncs the log manually if auto-flush is not set in HoodieLogAppendConfig. If auto-flush is set

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/log/avro/AvroLogAppender.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.log4j.Logger;
 
 import java.io.IOException;
+import java.util.Iterator;
 import java.util.List;
 
 /**
@@ -99,8 +100,8 @@ public AvroLogAppender(HoodieLogAppendConfig config) throws IOException, Interru
         }
     }
 
-    public void append(List<IndexedRecord> records) throws IOException {
-        records.forEach(r -> {
+    public void append(Iterator<IndexedRecord> records) throws IOException {
+        records.forEachRemaining(r -> {
             try {
                 writer.append(r);
             } catch (IOException e) {

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/log/avro/RollingAvroLogAppender.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.commons.logging.LogFactory;
 
 import java.io.IOException;
+import java.util.Iterator;
 import java.util.List;
 
 /**
@@ -64,8 +65,8 @@ public long getCurrentSize() throws IOException {
         return logWriter.getCurrentSize();
     }
 
-    public void append(List<IndexedRecord> records) throws IOException, InterruptedException {
-        LOG.info("Appending " + records.size() + " records to " + config.getLogFile());
+    public void append(Iterator<IndexedRecord> records) throws IOException, InterruptedException {
+        LOG.info("Appending records to " + config.getLogFile());
         rollOverIfNeeded();
         Preconditions.checkArgument(logWriter != null);
         logWriter.append(records);

File: hoodie-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/HoodieInputFormat.java
Patch:
@@ -22,8 +22,7 @@
 import com.uber.hoodie.common.table.HoodieTimeline;
 import com.uber.hoodie.common.table.TableFileSystemView;
 import com.uber.hoodie.common.table.timeline.HoodieInstant;
-import com.uber.hoodie.common.table.view.ReadOptimizedTableView;
-import com.uber.hoodie.common.util.FSUtils;
+import com.uber.hoodie.common.table.view.HoodieTableFileSystemView;
 import com.uber.hoodie.exception.InvalidDatasetException;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -95,8 +94,9 @@ public FileStatus[] listStatus(JobConf job) throws IOException {
             LOG.info("Hoodie Metadata initialized with completed commit Ts as :" + metadata);
             String tableName = metadata.getTableConfig().getTableName();
             String mode = HoodieHiveUtil.readMode(Job.getInstance(job), tableName);
-            TableFileSystemView fsView = new ReadOptimizedTableView(FSUtils.getFs(), metadata);
             HoodieTimeline timeline = metadata.getActiveTimeline().getCommitTimeline().filterCompletedInstants();
+            TableFileSystemView fsView = new HoodieTableFileSystemView(metadata, timeline);
+
             if (HoodieHiveUtil.INCREMENTAL_SCAN_MODE.equals(mode)) {
                 // this is of the form commitTs_partition_sequenceNumber
                 String lastIncrementalTs = HoodieHiveUtil.readStartCommitTime(Job.getInstance(job), tableName);

File: hoodie-hive/src/test/java/com/uber/hoodie/hive/util/TestUtil.java
Patch:
@@ -17,6 +17,8 @@
 package com.uber.hoodie.hive.util;
 
 import com.google.common.collect.Sets;
+import com.uber.hoodie.common.minicluster.HdfsTestService;
+import com.uber.hoodie.common.minicluster.ZookeeperTestService;
 import com.uber.hoodie.hive.HoodieHiveConfiguration;
 import com.uber.hoodie.hive.client.HoodieHiveClient;
 import com.uber.hoodie.hive.model.HoodieDatasetReference;

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieIOHandle.java
Patch:
@@ -50,7 +50,8 @@ public HoodieIOHandle(HoodieWriteConfig config, String commitTime,
         this.config = config;
         this.fs = FSUtils.getFs();
         this.metaClient = metaClient;
-        this.hoodieTimeline = metaClient.getActiveCommitTimeline();
+        this.hoodieTimeline =
+            metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants();
         this.fileSystemView = new ReadOptimizedTableView(fs, metaClient);
         this.schema =
             HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));

File: hoodie-client/src/test/java/com/uber/hoodie/common/HoodieClientTestUtils.java
Patch:
@@ -19,6 +19,7 @@
 import com.uber.hoodie.WriteStatus;
 import com.uber.hoodie.common.model.HoodieRecord;
 import com.uber.hoodie.common.table.HoodieTableMetaClient;
+import com.uber.hoodie.common.table.HoodieTimeline;
 import com.uber.hoodie.common.util.FSUtils;
 
 import java.io.File;
@@ -60,11 +61,11 @@ private static void fakeMetaFile(String basePath, String commitTime, String suff
 
 
     public static void fakeCommitFile(String basePath, String commitTime) throws IOException {
-        fakeMetaFile(basePath, commitTime, HoodieTableMetaClient.COMMIT_EXTENSION);
+        fakeMetaFile(basePath, commitTime, HoodieTimeline.COMMIT_EXTENSION);
     }
 
     public static void fakeInFlightFile(String basePath, String commitTime) throws IOException {
-        fakeMetaFile(basePath, commitTime, HoodieTableMetaClient.INFLIGHT_FILE_SUFFIX);
+        fakeMetaFile(basePath, commitTime, HoodieTimeline.INFLIGHT_EXTENSION);
     }
 
     public static void fakeDataFile(String basePath, String partitionPath, String commitTime, String fileId) throws Exception {

File: hoodie-client/src/test/java/com/uber/hoodie/common/HoodieTestDataGenerator.java
Patch:
@@ -20,6 +20,7 @@
 import com.uber.hoodie.common.model.HoodieKey;
 import com.uber.hoodie.common.model.HoodieRecord;
 import com.uber.hoodie.common.table.HoodieTableMetaClient;
+import com.uber.hoodie.common.table.HoodieTimeline;
 import com.uber.hoodie.common.util.FSUtils;
 import com.uber.hoodie.common.util.HoodieAvroUtils;
 
@@ -142,7 +143,7 @@ public static TestRawTripPayload generateRandomValue(HoodieKey key, String commi
 
     public static void createCommitFile(String basePath, String commitTime) throws IOException {
         Path commitFile =
-            new Path(basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/" + HoodieTableMetaClient.makeCommitFileName(commitTime));
+            new Path(basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/" + HoodieTimeline.makeCommitFileName(commitTime));
         FileSystem fs = FSUtils.getFs();
         FSDataOutputStream os = fs.create(commitFile, true);
         HoodieCommitMetadata commitMetadata = new HoodieCommitMetadata();

File: hoodie-client/src/test/java/com/uber/hoodie/func/TestUpdateMapFunction.java
Patch:
@@ -17,6 +17,7 @@
 package com.uber.hoodie.func;
 
 import com.uber.hoodie.common.table.HoodieTableMetaClient;
+import com.uber.hoodie.common.table.HoodieTimeline;
 import com.uber.hoodie.config.HoodieWriteConfig;
 import com.uber.hoodie.WriteStatus;
 import com.uber.hoodie.common.TestRawTripPayload;
@@ -79,7 +80,7 @@ public void testSchemaEvolutionOnUpdate() throws Exception {
                 rowChange3));
         Iterator<List<WriteStatus>> insertResult = table.handleInsert(records.iterator());
         Path commitFile =
-            new Path(config.getBasePath() + "/.hoodie/" + HoodieTableMetaClient.makeCommitFileName("100"));
+            new Path(config.getBasePath() + "/.hoodie/" + HoodieTimeline.makeCommitFileName("100"));
         FSUtils.getFs().create(commitFile);
 
         // Now try an update with an evolved schema

File: hoodie-client/src/test/java/com/uber/hoodie/table/TestCopyOnWriteTable.java
Patch:
@@ -184,8 +184,8 @@ public void testUpdateRecords() throws Exception {
             if (file.getName().endsWith(".parquet")) {
                 if (FSUtils.getFileId(file.getName())
                     .equals(FSUtils.getFileId(parquetFile.getName())) && metadata
-                    .getActiveCommitTimeline()
-                    .compareInstants(FSUtils.getCommitTime(file.getName()),
+                    .getActiveTimeline().getCommitTimeline()
+                    .compareTimestamps(FSUtils.getCommitTime(file.getName()),
                         FSUtils.getCommitTime(parquetFile.getName()), HoodieTimeline.GREATER)) {
                     updatedParquetFile = file;
                     break;

File: hoodie-common/src/main/java/com/uber/hoodie/common/model/HoodieFileFormat.java
Patch:
@@ -16,12 +16,12 @@
 
 package com.uber.hoodie.common.model;
 
-public enum HoodieStorageType {
+public enum HoodieFileFormat {
     PARQUET(".parquet");
 
     private final String extension;
 
-    HoodieStorageType(String extension) {
+    HoodieFileFormat(String extension) {
         this.extension = extension;
     }
 

File: hoodie-common/src/main/java/com/uber/hoodie/common/table/view/ReadOptimizedTableView.java
Patch:
@@ -36,7 +36,7 @@ protected FileStatus[] listDataFilesInPartition(String partitionPathStr) {
         Path partitionPath = new Path(metaClient.getBasePath(), partitionPathStr);
         try {
             return fs.listStatus(partitionPath, path -> path.getName()
-                .contains(metaClient.getTableConfig().getROStorageFormat().getFileExtension()));
+                .contains(metaClient.getTableConfig().getROFileFormat().getFileExtension()));
         } catch (IOException e) {
             throw new HoodieIOException(
                 "Failed to list data files in partition " + partitionPathStr, e);

File: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/HoodieDeltaStreamer.java
Patch:
@@ -66,11 +66,11 @@ private void sync() throws Exception {
         JavaSparkContext sc = getSparkContext(cfg);
         FileSystem fs = FSUtils.getFs();
         HoodieTableMetaClient targetHoodieMetadata = new HoodieTableMetaClient(fs, cfg.targetPath);
-        HoodieTimeline timeline = targetHoodieMetadata.getActiveCommitTimeline();
+        HoodieTimeline timeline = targetHoodieMetadata.getActiveTimeline().getCommitTimeline().filterCompletedInstants();
         String lastCommitPulled = findLastCommitPulled(fs, cfg.dataPath);
         log.info("Last commit pulled on the source dataset is " + lastCommitPulled);
         if (!timeline.getInstants().iterator().hasNext() && timeline
-            .compareInstants(timeline.lastInstant().get(), lastCommitPulled,
+            .compareTimestamps(timeline.lastInstant().get().getTimestamp(), lastCommitPulled,
                 HoodieTimeline.GREATER)) {
             // this should never be the case
             throw new IllegalStateException(

File: hoodie-client/src/main/java/com/uber/hoodie/HoodieReadClient.java
Patch:
@@ -70,7 +70,7 @@ public class HoodieReadClient implements Serializable {
 
     private transient final FileSystem fs;
     /**
-     * TODO: We need to persist the index type into hoodie.properties & be able to access the index
+     * TODO: We need to persist the index type into hoodie.properties and be able to access the index
      * just with a simple basepath pointing to the dataset. Until, then just always assume a
      * BloomIndex
      */

File: hoodie-client/src/main/java/com/uber/hoodie/index/HoodieBloomIndex.java
Patch:
@@ -323,7 +323,7 @@ private int determineParallelism(int inputParallelism, final Map<String, Long> s
     /**
      * Find out <RowKey, filename> pair. All workload grouped by file-level.
      *
-     *         // Join PairRDD(PartitionPath, RecordKey) and PairRDD(PartitionPath, File) & then repartition such that
+     *         // Join PairRDD(PartitionPath, RecordKey) and PairRDD(PartitionPath, File) and then repartition such that
      // each RDD partition is a file, then for each file, we do (1) load bloom filter, (2) load rowKeys, (3) Tag rowKey
      // Make sure the parallelism is atleast the groupby parallelism for tagging location
      */

File: hoodie-client/src/test/java/com/uber/hoodie/common/HoodieTestDataGenerator.java
Patch:
@@ -40,7 +40,7 @@
 import java.util.UUID;
 
 /**
- * Class to be used in tests to keep generating test inserts & updates against a corpus.
+ * Class to be used in tests to keep generating test inserts and updates against a corpus.
  *
  * Test data uses a toy Uber trips, data model.
  */

File: hoodie-client/src/main/java/com/uber/hoodie/HoodieWriteClient.java
Patch:
@@ -165,7 +165,7 @@ public JavaRDD<WriteStatus> upsert(JavaRDD<HoodieRecord<T>> records, final Strin
      * Inserts the given HoodieRecords, into the table. This API is intended to be used for normal
      * writes.
      *
-     * This implementation skips the index check & is able to leverage benefits such as
+     * This implementation skips the index check and is able to leverage benefits such as
      * small file handling/blocking alignment, as with upsert(), by profiling the workload
      *
      * @param records    HoodieRecords to insert
@@ -298,7 +298,7 @@ public Iterable<WriteStatus> call(List<WriteStatus> writeStatuses)
      * loads into a Hoodie table for the very first time (e.g: converting an existing dataset to
      * Hoodie).
      *
-     * This implementation uses sortBy (which does range partitioning based on reservoir sampling) &
+     * This implementation uses sortBy (which does range partitioning based on reservoir sampling) and
      * attempts to control the numbers of files with less memory compared to the {@link
      * HoodieWriteClient#insert(JavaRDD, String)}
      *

File: hoodie-client/src/main/java/com/uber/hoodie/HoodieReadClient.java
Patch:
@@ -60,7 +60,7 @@
  * Provides first class support for accessing Hoodie tables for data processing via Apache Spark.
  *
  *
- * TODO: Need to move all read operations here, since Hoodie is a single writer & multiple reader
+ * TODO: Need to move all read operations here, since Hoodie is a single writer and multiple reader
  */
 public class HoodieReadClient implements Serializable {
 
@@ -247,7 +247,7 @@ public DataFrame readCommit(String commitTime) {
 
     /**
      * Checks if the given [Keys] exists in the hoodie table and returns [Key,
-     * Optional<FullFilePath>] If the optional FullFilePath value is not present, then the key is
+     * Optional[FullFilePath]] If the optional FullFilePath value is not present, then the key is
      * not found. If the FullFilePath value is present, it is the path component (without scheme) of
      * the URI underlying file
      */

File: hoodie-client/src/main/java/com/uber/hoodie/HoodieWriteClient.java
Patch:
@@ -105,7 +105,6 @@ public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig clientConfig) t
      * @param jsc
      * @param clientConfig
      * @param rollbackInFlight
-     * @throws Exception
      */
     public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig clientConfig, boolean rollbackInFlight) {
         this.fs = FSUtils.getFs();
@@ -234,7 +233,7 @@ private JavaRDD<HoodieRecord<T>> combineOnCondition(boolean condition,
      *
      * @param records HoodieRecords to insert
      * @param commitTime Commit Time handle
-     * @return JavaRDD<WriteStatus> - RDD of WriteStatus to inspect errors and counts
+     * @return JavaRDD[WriteStatus] - RDD of WriteStatus to inspect errors and counts
      *
      */
     public JavaRDD<WriteStatus> insert(JavaRDD<HoodieRecord<T>> records, final String commitTime) {

File: hoodie-client/src/main/java/com/uber/hoodie/index/HoodieIndex.java
Patch:
@@ -33,7 +33,7 @@
 
 /**
  * Base class for different types of indexes to determine the mapping from uuid
- * <p/>
+ *
  * TODO(vc): need methods for recovery and rollback
  */
 public abstract class HoodieIndex<T extends HoodieRecordPayload> implements Serializable {
@@ -53,7 +53,7 @@ protected HoodieIndex(HoodieWriteConfig config, JavaSparkContext jsc) {
     }
 
     /**
-     * Checks if the given [Keys] exists in the hoodie table and returns [Key, Optional<FullFilePath>]
+     * Checks if the given [Keys] exists in the hoodie table and returns [Key, Optional[FullFilePath]]
      * If the optional FullFilePath value is not present, then the key is not found. If the FullFilePath
      * value is present, it is the path component (without scheme) of the URI underlying file
      *
@@ -74,7 +74,7 @@ public abstract JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> re
 
     /**
      * Extracts the location of written records, and updates the index.
-     * <p/>
+     *
      * TODO(vc): We may need to propagate the record as well in a WriteStatus class
      */
     public abstract JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD,

File: hoodie-client/src/main/java/com/uber/hoodie/io/HoodieInsertHandle.java
Patch:
@@ -64,9 +64,9 @@ public HoodieInsertHandle(HoodieWriteConfig config, String commitTime,
 
     /**
      * Determines whether we can accept the incoming records, into the current file, depending on
-     * <p/>
+     *
      * - Whether it belongs to the same partitionPath as existing records
-     * - Whether the current file written bytes < max file size
+     * - Whether the current file written bytes lt max file size
      *
      * @return
      */

File: hoodie-client/src/main/java/com/uber/hoodie/table/HoodieCopyOnWriteTable.java
Patch:
@@ -115,7 +115,7 @@ public String toString() {
     }
 
     /**
-     * Helper class for a bucket's type (INSERT & UPDATE) and its file location
+     * Helper class for a bucket's type (INSERT and UPDATE) and its file location
      */
     class BucketInfo implements Serializable {
         BucketType bucketType;

File: hoodie-common/src/main/java/com/uber/hoodie/common/model/HoodieCommits.java
Patch:
@@ -128,7 +128,7 @@ public String lastCommit() {
     }
 
     /**
-     * Returns the nth commit from the latest commit such that lastCommit(0) => lastCommit()
+     * Returns the nth commit from the latest commit such that lastCommit(0) gteq lastCommit()
      */
     public String lastCommit(int n) {
         if (commitList.size() < n + 1) {

File: hoodie-hive/src/main/java/com/uber/hoodie/hive/client/SchemaUtil.java
Patch:
@@ -138,7 +138,7 @@ private static boolean isFieldExistsInSchema(Map<String, String> newTableSchema,
      * Returns equivalent Hive table schema read from a parquet file
      *
      * @param messageType : Parquet Schema
-     * @return : Hive Table schema read from parquet file MAP<String,String>
+     * @return : Hive Table schema read from parquet file MAP[String,String]
      * @throws IOException
      */
     public static Map<String, String> convertParquetSchemaToHiveSchema(MessageType messageType)

File: hoodie-hive/src/main/java/com/uber/hoodie/hive/client/HoodieFSClient.java
Patch:
@@ -106,7 +106,9 @@ public Path lastDataFileForDataset(HoodieDatasetReference metadata,
                 Path path = files.next().getPath();
                 if (path.getName().endsWith(PARQUET_EXTENSION) || path.getName()
                     .endsWith(PARQUET_EXTENSION_ZIPPED)) {
-                    returnPath = path;
+                    if(returnPath == null || path.toString().compareTo(returnPath.toString()) > 0) {
+                        returnPath = path;
+                    }
                 }
             }
             if (returnPath != null) {

File: hoodie-hive/src/test/java/com/uber/hoodie/hive/HDroneDatasetTest.java
Patch:
@@ -92,7 +92,8 @@ public void testDatasetEvolution() throws IOException, InitializationError {
         assertEquals("Table schema should be evolved schema", expectedSchema, newDatasetSchema);
         assertEquals("Table schema should have 6 fields", 6,
             hiveClient.getTableSchema(metadata).size());
-        assertEquals("", "BIGINT", hiveClient.getTableSchema(metadata).get("region_key"));
+        assertEquals("Valid Evolution should be reflected", "BIGINT",
+            hiveClient.getTableSchema(metadata).get("region_key"));
     }
 
 }

